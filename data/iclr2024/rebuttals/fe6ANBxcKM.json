[
    {
        "title": "Federated Q-Learning: Linear Regret Speedup with Low Communication Cost"
    },
    {
        "review": {
            "id": "2MeDD0cmQk",
            "forum": "fe6ANBxcKM",
            "replyto": "fe6ANBxcKM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8542/Reviewer_dPut"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8542/Reviewer_dPut"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the federated RL and proposes two model-free algorithms---FedQ-Hoeffding and FedQ-Bernstein---that achieve regret speedup and low communication."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Overall, this paper has a fair contribution. It proposes two federated RL algorithms with a regret speed up and logarithmic communication."
                },
                "weaknesses": {
                    "value": "It would be great if the authors could provide some empirical validations for their algorithm. I understand this is a theoretical work, but it is always helpful to corroborate the theoretical results with some experiments."
                },
                "questions": {
                    "value": "- Why define $\\tilde{C}$ as it is used only once (above Eq. (2)) in the main paper? \n- Case 1 at Page 5, it should be $\\eqqcolon i_0$ instead of $\\coloneqq i_0$."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8542/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698332157395,
            "cdate": 1698332157395,
            "tmdate": 1699637068648,
            "mdate": 1699637068648,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jBN7YNkivv",
                "forum": "fe6ANBxcKM",
                "replyto": "2MeDD0cmQk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8542/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8542/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer dPut"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the careful reading and thoughtful comments. We address the reviewer's questions in the following and have revised the paper accordingly. The changes are marked in blue in our revision. We hope the responses below and changes in the paper address the reviewer's concerns. \n\n**Weakness:** It would be great if the authors could provide some empirical validations for their algorithm. I understand this is a theoretical work, but it is always helpful to corroborate the theoretical results with some experiments.\n\n**Response:** Thank you for the suggestion. We have performed some experiments and updated our draft to include the experimental results of FedQ-Hoeffding and the comparison with the single-agent Q-learning algorithm UCB-Hoeffding in Appendix F. The source code has been uploaded as a supplementary file. The results clearly exhibit the linear speedup of the learning regret and logarithmic communication cost, which corroborate our theoretical results. We will perform more experiments to thoroughly validate the performances of FedQ-Hoeffding and FedQ-Bernstein, and include the results in the next version of this paper. \n\n**Question 1:** Why define $\\tilde{C}$ as it is used only once (above Eq. (2)) in the main paper?\n\n**Response:** Thank you for the careful reading.  $\\tilde{C}$ has been used frequently in the proofs in the appendix. We have followed your suggestion to only keep this notation in the statements of the theorems in the main paper.\n\n**Question 2:** Case 1 at Page 5, it should be $=:i _0$ instead of $:=i _0$.\n\n**Response:** Thank you for your careful reading. We have revised it accordingly in the updated version of this paper. \n\n\n    \n-----\n\nWe thank the reviewer again for the helpful comments and suggestions for our work. If our response resolves your concerns to a satisfactory level, we kindly ask the reviewer to consider raising the rating of our work. Certainly, we are more than happy to address any further questions that you may have."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8542/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700279767303,
                "cdate": 1700279767303,
                "tmdate": 1700279767303,
                "mdate": 1700279767303,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Kx89m4tKb8",
                "forum": "fe6ANBxcKM",
                "replyto": "jBN7YNkivv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8542/Reviewer_dPut"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8542/Reviewer_dPut"
                ],
                "content": {
                    "comment": {
                        "value": "The reviewer thanks the author for their response. The reviewer will take these into consideration in the later discussion with AC."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8542/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700364210587,
                "cdate": 1700364210587,
                "tmdate": 1700364210587,
                "mdate": 1700364210587,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9IhVDzDqwJ",
                "forum": "fe6ANBxcKM",
                "replyto": "Kx89m4tKb8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8542/Reviewer_dPut"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8542/Reviewer_dPut"
                ],
                "content": {
                    "comment": {
                        "value": "The reviewer thinks the empirical simulation is unsatisfactory. For one thing, the mentioned baseline algorithms in Table 1 are not compared, and the author does not explain why not compare with them. Especially, while one key improvement in this paper is on low communication, the numerical results cannot support this advantage by only reporting the performance of the algorithm designed in this paper. For another thing, the authors may also consider comparing FedQ-Hoeffding and FedQ-Bernstein."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8542/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700364577089,
                "cdate": 1700364577089,
                "tmdate": 1700364577089,
                "mdate": 1700364577089,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zQTwg3q8f9",
                "forum": "fe6ANBxcKM",
                "replyto": "2MeDD0cmQk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8542/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8542/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the prompt reply. As acknowledged by the reviewer, this is a theoretical work, and we followed the reviewer's suggestion to conduct some experiments within the short rebuttal window in order to verify the linear speedup and logarithmic communication cost shown in our theoretical results. We have worked tirelessly to implement UCB-H and FedQ-Hoeffding within the short time window and obtained those experimental results, which indeed corroborate our theoretical results as we expected.\n\nWe agree with the reviewer that the experimental results are not comprehensive at this stage. This is because those works listed in Table 1 are all theoretical works, and they do not provide any experimental results or source code. It is thus impossible for us to implement those algorithms and perform the comparison given the short deadline we have. We are currently working on the implementation of FedQ-Beinstein, and we will include more comprehensive comparison with other works (if possible) in the next version of this work."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8542/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700367221460,
                "cdate": 1700367221460,
                "tmdate": 1700370691540,
                "mdate": 1700370691540,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "6m4e9XhuAQ",
            "forum": "fe6ANBxcKM",
            "replyto": "fe6ANBxcKM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8542/Reviewer_8uCJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8542/Reviewer_8uCJ"
            ],
            "content": {
                "summary": {
                    "value": "This work considers a federated Q-learning for tabular episodic MDP, where multiple agents collaboratively explore the environment and learn an optimal Q-value with the aid of a central server. They proposed two federated Q-learning algorithms (FedQ-Hoeffding, FedQ-Bernstein) with event-triggered policy switching and synchronization. The algorithms provably achieve linear regret speedup while requiring communication cost logarithmically scaling with the total number of samples (T)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- They proposed a federated Q-learning algorithm with event-triggered synchronization, which  guarantees logarithmic communication cost in terms of T.\n- They provided a finite-time regret analysis on the federated Q-learning algorithm with policy switching and proved linear regret speedup."
                },
                "weaknesses": {
                    "value": "* Although the algorithm considers a setting that agents can collaboratively explore by changing their policies, the algorithm requires all agents to use the same fixed policy during local iterations, which seems to be quite restrictive. It would be nice if you could elaborate on the necessity of these restrictions.\n* Although the paper claims that the event-triggered synchronization method is a key to reducing communication costs, the order of communication costs they showed in this paper seems to be larger than the one shown in [1], which uses just a fixed communication period. The communication cost shown in [1] not only logarithmically scales with T, but also is more efficient in terms of other factors (M: number of agents, $(1-\\gamma)^{-1} (\\approx H)$: length of horizon). I understand that direct comparison might be difficult given the settings are different, but I\u2019m still not convinced that the communication cost shown in this paper is especially low. It would be nice if you could provide more detailed comparisons with recent literature to help better understand on the communication efficiency.\n\n[1]: Jiin Woo, Gauri Joshi, and Yuejie Chi. The blessing of heterogeneity in federated q-learning: Linear speedup and beyond. In International Conference on Machine Learning, pp. 37157\u201337216, 2023."
                },
                "questions": {
                    "value": "* The previous federated Q-learning literature [1] already showed that communication cost logarithmically scaling with T is achievable without using event-triggered synchronization (with fixed communication period). Is there any reason to introduce event-triggered synchronization especially in this setting?\n* The algorithm seems to fix the Q-values and behavior policies to be the same for all agents during local iterations. However, a setting that agents can flexibly change their policy based on their local observations before the next synchronization seems more natural to me, especially in the federated setting. Would it hurt the performance if they can change their policies and Q-estimates locally during local updates? I wonder if letting agents to change their policies can introduce some diversity in their exploration, which might be an advantage in learning."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8542/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8542/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8542/Reviewer_8uCJ"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8542/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698600736866,
            "cdate": 1698600736866,
            "tmdate": 1699637068526,
            "mdate": 1699637068526,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gaScZP2yU8",
                "forum": "fe6ANBxcKM",
                "replyto": "6m4e9XhuAQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8542/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8542/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer 8uCJ (part one)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the careful reading and thoughtful comments. We address the reviewer's questions in the following and have revised the paper accordingly. The changes are marked in blue in our revision. We hope the responses below and changes in the paper address the reviewer's concerns. Due to character limits of openreview, our reply is splitted into three parts.\n\n\n**Weakness 1:** Although the algorithm considers a setting that agents can collaboratively explore by changing their policies, the algorithm requires all agents to use the same fixed policy during local iterations, which seems to be quite restrictive. It would be nice if you could elaborate on the necessity of these restrictions.\n\n**Response:** The main reason that our algorithm adopts the same policy for all agents is two-fold: *First, such a fixed policy design facilitates tractable global information aggregation in each round*. In federated RL, the distribution of the collected samples in each episode depends on the executed policy. Different exploration policy leads to different distribution of the data samples. If we allow local clients to adopt different policies, or allow the exploration policies to update locally in each episode, the data samples will no longer be IID across clients or episodes in the same round, leading to significant challenges when aggregating them for an analytically tractable global estimate.\n\n*Second, the fixed policy design naturally fits the homogeneous MDP setting and the low regret and communication cost requirements.* Given that the MDP environments encountered by the local clients are homogeneous, in order to achieve low learning regret across all clients, it is expected that the policies adopted by the clients will eventually converge to the same optimal policy. Meanwhile, in order to save the communication cost, it is desirable to keep the local policy fixed, such that the required information exchange is minimized.\n\nWe note that such a fixed exploration policy design is widely adopted in the study of federated RL and federated bandits, such as [3]-[6].\n\n[3] Yiding Chen, Xuezhou Zhang, Kaiqing Zhang, Mengdi Wang, and Xiaojin Zhu. Byzantine-robust online and offline distributed reinforcement learning. In International Conference on Artificial Intelligence and Statistics, pp. 3230\u20133269. PMLR, 2023.\n\n[4] Flint Xiaofeng Fan, Yining Ma, Zhongxiang Dai, Wei Jing, Cheston Tan, and Bryan Kian Hsiang Low. Fault-tolerant federated reinforcement learning with theoretical guarantee. Advances in Neural Information Processing Systems, 34:1007\u20131021, 2021.\n\n[5] Chengshuai Shi and Cong Shen. Federated multi-armed bandits. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 9603\u20139611, 2021.\n\n[6] Ruiquan Huang, Weiqiang Wu, Jing Yang, and Cong Shen. Federated linear contextual bandits. Advances in neural information processing systems 34 (2021): 27057-27068."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8542/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700279209125,
                "cdate": 1700279209125,
                "tmdate": 1700280064101,
                "mdate": 1700280064101,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "duZrmhQU1P",
                "forum": "fe6ANBxcKM",
                "replyto": "6m4e9XhuAQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8542/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8542/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer 8uCJ (part two)"
                    },
                    "comment": {
                        "value": "**Weakness 2:** Communication cost comparison with Woo et al (2023).\n\n**Response:** As pointed out by the reviewer, Woo et al (2023) considers a very different setting (infinite-horizon MDP with time-invariant transition kernels) with different learning objective (near-optimal Q-estimate at the central server). It also assumes the accessibility to a simulator for each agent in the synchronous setting, which generates a new sample\nfor *every state-action pair* independently at every iteration. Thus, it does not require any exploration policy, which is in stark contrast to our problem and cannot be compared fairly. \n\nFor the asynchronous setting, since the objective is to learn a near-optimal Q-estimate *at the central server*, the local clients adopt *fixed exploration policies* through the learning process to collect trajectories only. \nAs a result, it is able to achieve a communication round upper bound in $O(\\frac{ M}{\\mu(1-\\gamma)}\\log T)$ with a fixed communication period design. Here $\\mu = \\mu _{min}$ with equal weight assignment, and $\\mu = \\mu _{avg}$ with unequal weight assignment, where $\\mu _{min} = \\min _{x,a,m} \\mu^m(x,a)$ and $\\mu _{avg} = \\min _{x,a}\\frac{1}{M}\\sum _{m=1}^M\\mu^m(x,a)$, and $\\mu^m(x,a)$ is the visitation frequency for $(x,a)$ under the exploration policy at agent $m$.\n\nWe note that the total number of communication rounds in both our work and Woo et al (2023) scales linearly in $M$. As pointed out in our response to **Question 1 of reviewer tdCn**, the dependency on $M$ under our algorithm can be further reduced or even removed without affecting the regret if we relax the synchronization requirement.\n\nIn terms of the dependency $S,A,H$, we first note that $\\mu _{min} \\leq \\mu _{avg} \\leq \\frac{1}{SA}$. Besides, if we use the approximation $(1-\\gamma)^{-1} \\approx H$ as suggested by the reviewer, the bound in Woo et al (2023) becomes $O(MHSA\\log T)$. Compared with our bound $O(MH^3SA\\log T)$, we note that the only difference lies in the order of $H$. We emphasize that in the episodic MDP we consider, we have to handle *$H$ different transition kernels* for the $H$ steps in each episode. Roughly speaking, learning such MDPs is at least $H$ times harder than stationary infinite-horizon MDPs. Thus, it is no surprising that our dependency on $H$ has a higher order. Whether we can reduce the dependency on $H$ from $H^3$ to $H$ by assuming a constant transition kernel in each step requires some significant modification of the algorithm design and analysis, and we leave it to our future work. \n\n\n**Question 1:** The previous federated Q-learning literature Woo et al (2023) already showed that communication cost logarithmically scaling with $T$ is achievable without using event-triggered synchronization (with fixed communication period). Is there any reason to introduce event-triggered synchronization especially in this setting?\n\n**Response:** As pointed out in our response to **Weakness 2** above, Woo et al (2023) consider a very different setting (infinite-horizon MDP with time-invariant transition kernels) with different learning objective (near-optimal Q-estimate at the central server), which enables them to achieve logarithmic communication cost with fixed communication period. However, in our setting, we consider episodic MDPs with different transition kernels $P _h$ in each step $h$, and we aim to achieve low learning regret across all clients. Thus, in contrast to the fixed policy adopted by the clients throughout the learning process in Woo et al (2023), in our setting, it is necessary for the clients to adaptively change their local exploration policy and eventually converge to the optimal policy. On the other hand, in order to reduce the communication cost, it is also desirable to have the local exploration policy stays fixed for a period of time. It thus requires a sophisticated design to achieve low regret and low communication cost at the same time, **which in general cannot be achieved by a fixed communication period design**. \n\nMotivated by the event-triggered policy switching in the single-agent setting, which has been shown to achieve low regret with logarithmic policy switching cost, we adopt the event-triggered synchronization in our setting. As we have explained in our response to **Question 1 of reviewer tdCn**, such event-triggered synchronization essentially guarantees **exponentially increasing round lengths**, which is critical to guarantee the logarithmic communication cost. Besides, when combining this technique with the new equal weight assignment in global aggregation, we are able to bound the estimation error in each round using the novel round-wise approximation technique, and obtain low learning regret at the same time."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8542/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700279355245,
                "cdate": 1700279355245,
                "tmdate": 1700281637931,
                "mdate": 1700281637931,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "x0Xl1Hc9iG",
                "forum": "fe6ANBxcKM",
                "replyto": "DMNzjLsAqe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8542/Reviewer_8uCJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8542/Reviewer_8uCJ"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to the authors for the detailed responses. I have several follow-up questions for further clarification.\n\n1. Now I understand that the fixed policy design is necessary for generating i.i.d. data and the tractability of global estimates in analysis. However, I am still confused if the fixed policy design choice is fundamentally necessary in practice or if this is a simplification for analysis. From the \"exploration\" perspective, making all agents choose different actions rather than a fixed common action seems more efficient, especially when round lengths are very long. Could you elaborate on some simple intuitive examples or empirical demonstrations where adaptively updating local policy and Q-values using newly obtained local observation before synchronization can harm convergence?\u00a0\n\u2028\n2. I am still not convinced that the communication efficiency shown in this work is especially novel. As the authors explained, the communication cost shown in this paper is comparable to, or worse than, the one suggested in Woo et al (2023). Although there exist differences in analyzing episodic MDP and infinite MDP, I am not convinced how the difference causes some special challenges in the \"communication\" perspective. Could you provide some intuitive examples or empirical demonstrations that explain the communication challenges stemming from finite-horizon or transition kernels for $h$, other than general differences or challenges previously discussed in the single-agent setting?"
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8542/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700553753679,
                "cdate": 1700553753679,
                "tmdate": 1700553753679,
                "mdate": 1700553753679,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "uEBesOygZC",
            "forum": "fe6ANBxcKM",
            "replyto": "fe6ANBxcKM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8542/Reviewer_62fC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8542/Reviewer_62fC"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies federated reinforcement learning for the tabular episodic Markov Decision Process (MDP). In the model, there are $M$ agents that play in a given tabular episodic MDP and aim to minimize total regret. More formally, a tabular episodic MDP consists of $J$ episodes, where in each episode, the agents are given an adversarial picked initial state and everyone will keep picking actions till an absorbing state is reached. Each time an agent picks an action, it receives a reward that can help it update the strategy. An agent's regret for one episode is defined to be the difference between the rewards obtained by the optimal strategy and its strategy, and the goal is to minimize the sum of regrets for each agent in each episode. \n\nThe authors first propose a federated Q-Learning algorithm and show that with a communication cost of $O(M^2H^4S^2A\\log(T/M))$, the algorithm obtains a regret of $\\tilde{O}(\\sqrt{H^4SAMT})$, where $H$ is the number of steps per episode, $T$ is the total number of steps, $S$ is the number of states in MDP, $A$ is the number of actions and $M$ is the number of agents. Further, using a higher upper confidence bound, the regret can be improved to $\\tilde{O}(\\sqrt{H^3SAMT})$ under the same communication cost."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Federated reinforcement learning is a very interesting topic and the paper makes theoretical contributions in this direction. They prove that there exists a federated and model-free algorithm that achieves linear regret speedup (compared with the single-agent setting) with a relatively low communication cost.\n\n- The paper is well-stated. In addition to describing the algorithm, some intuitions behind the algorithm design are provided in the paper."
                },
                "weaknesses": {
                    "value": "- The main weakness is that the experimental evaluation is missing. The paper would be strengthened if an experimental section were added."
                },
                "questions": {
                    "value": "(1) Are there any simple experimental figures for the proposed algorithms?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8542/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8542/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8542/Reviewer_62fC"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8542/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698674534395,
            "cdate": 1698674534395,
            "tmdate": 1699637068409,
            "mdate": 1699637068409,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "P0SaqQ1cQx",
                "forum": "fe6ANBxcKM",
                "replyto": "uEBesOygZC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8542/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8542/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer 62fC"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the careful reading and thoughtful comments. We address the reviewer's questions in the following and have revised the paper accordingly. The changes are marked in blue in our revision. We hope the responses below and changes in the paper address the reviewer's concerns. \n\n**Q1:** The main weakness is that the experimental evaluation is missing. The paper would be strengthened if an experimental section were added.\n\n\n**A1:** Thank you for the suggestion. We have performed some experiments and updated our draft to include the experimental results of FedQ-Hoeffding and the comparison with the single-agent Q-learning algorithm UCB-Hoeffding in Appendix F. The source code has been uploaded as a supplementary file. The results clearly exhibit the linear speedup of the learning regret and logarithmic communication cost, which corroborate our theoretical results. We will perform more experiments to thoroughly validate the performances of FedQ-Hoeffding and FedQ-Bernstein, and include the results in the next version of this paper. \n\n\n    \n-----\n\nWe thank the reviewer again for the helpful comments and suggestions for our work. If our response resolves your concerns to a satisfactory level, we kindly ask the reviewer to consider raising the rating of our work. Certainly, we are more than happy to address any further questions that you may have."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8542/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700278934878,
                "cdate": 1700278934878,
                "tmdate": 1700278934878,
                "mdate": 1700278934878,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IODuRjPsTD",
                "forum": "fe6ANBxcKM",
                "replyto": "3QCxQuHnnL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8542/Reviewer_62fC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8542/Reviewer_62fC"
                ],
                "content": {
                    "comment": {
                        "value": "The reviewer thanks the author for their response. Given the new experimental results, the paper looks more complete now."
                    }
                },
                "number": 27,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8542/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700667646819,
                "cdate": 1700667646819,
                "tmdate": 1700667646819,
                "mdate": 1700667646819,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "acXJaMjC5F",
            "forum": "fe6ANBxcKM",
            "replyto": "fe6ANBxcKM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8542/Reviewer_tdCn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8542/Reviewer_tdCn"
            ],
            "content": {
                "summary": {
                    "value": "The algorithm proposes a federated-style Q-learning for tabular MDPs and shows the algorithm achieves linear-speedup in terms of regret, only requiring $\\log(T)$ communication rounds. Two types of uncertainty bonuses, Hoeffding and Berstein, are considered."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper's theoretical analysis is comprehensive, comprising of both Hoeffding-style and Berstein-style bonuses.\n- The paper does not analyze the UCB-style algorithms typically considered in distributed/federated RL/bandit, and instead focuses on the harder to analyze Q-learning style algorithms."
                },
                "weaknesses": {
                    "value": "1. The paper slightly over-claims its results. The regret speedup is nearly linear, but not exactly linear, due to the overhead terms that are linear in $M$, the number of machines, that appears in both Thm 4.1 and 5.1. Immediately after Thm 4.1, the paper also states that the algorithm enjoys a linear speedup in terms of $M$ in the general FL setting, despite the presence of the overhead terms. \n2. While the typical martingale-style concentration analysis in the single agent RL setting cannot be directly applied, bounding each local term's \"drift\" from some \"averaged parameter update path\" is a commonly used technique in federated learning. \n3. Compared with contemporary or prior works on federated RL, the paper focuses on a more \"vanilla\" setting, where updates are allowed to be adversarial or asynchronous, the setting considered in this paper is a fairly simplified, bare bones version of federated RL.\n\nMinor Comments\n1. The term $\\beta$ in eq (3) and (4) should be (at least) indexed to reflect the fact that it changes over time. Preferably, it should also indicate that the term changes with $(x, a, h)$. Otherwise, as of now, it appears that a constant is added to the Q-function estimate at every single round and is misleading.\n2. It might be useful to rename paragraph \"New Weight Assignment Approach\" on page 7 to \"Equal Weight Assignment Approach\", or something similar. Currently, the phrase \"equal weight assignment\" is claimed to be a major contribution, but from the manuscript it is not immediately obvious what this procedure means mathematically.\n3. Please see the questions below on the tightness of the technical results.\n4. Due to the overall similarity between RL and bandits, some literature survey on federated/distributed bandit would be a welcomed addition to Appendix A."
                },
                "questions": {
                    "value": "1. It is a bit surprising that the number of communication rounds is linear in $M$. Can the authors intuitively explain why this must be the case, or if this term could be removed by more involved technical analysis?\n2. Can the authors provide either some lower bound or some additional justification for the overhead terms? \n3. For theorem 4.1, why can we ignore the overhead terms that are on the order of $O(HSAM\\sqrt{H^3\\iota} + H^4SAM)$ in the general FL setting?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8542/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698688755148,
            "cdate": 1698688755148,
            "tmdate": 1699637068265,
            "mdate": 1699637068265,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DLKdXLpfjV",
                "forum": "fe6ANBxcKM",
                "replyto": "acXJaMjC5F",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8542/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8542/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer tdCn (part one)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the careful reading and thoughtful comments. We address the reviewer's questions in the following and have revised the paper accordingly. The changes are marked in blue in our revision. We hope the responses below and the changes in the paper address the reviewer's concerns. Due to character limits of openreview, our reply is splitted into five parts.\n\n**Weakness 1**: The paper slightly over-claims its results. The regret speedup is nearly linear, but not exactly linear, due to the overhead terms that are linear in $M$, the number of machines, that appears in both Theorem 4.1 and 5.1. Immediately after Theorem 4.1, the paper also states that the algorithm enjoys a linear speedup in terms of $M$ in the general FL setting, despite the presence of the overhead terms.\n\n**Response**:  Thank you for your comment. In Theorem 4.1, we claim that with high probability, the cumulative regret over all clients can be upper bounded as\n    $$\\mbox{Regret}(T)\\leq O\\left(\\sqrt{H^4\\iota MTSA} + M\\mbox{poly}(H,S,A)\\sqrt{\\iota}\\right),$$\nwhere $H, S, A$ are episode length, total number of states, and total number of actions, respectively, $T$ is the time horizon, and $\\iota$ is a $\\log$ factor.\n\nCorrespondingly, the per-client regret under our algorithm scales in \n$$ O\\left(\\sqrt{\\frac{H^4SAT\\iota}{M}} + \\mbox{poly}(H,S,A)\\sqrt{\\iota}\\right),$$ in which **the overhead term no longer depends on $M$**.\n    \nThus, when we consider the regret speedup, we ignore the overhead and just focus on the impact of $M$ on the dominating term $\\sqrt{\\frac{H^4SAT\\iota}{M}} $. This is why we claim \"linear\" regret speedup when $T$ is sufficiently large.\n\nWe note that \"linear speedup\" is a commonly adopted terminology to describe the impact of multiple agents on various performance metrics in the federated setting, even though similar overhead terms exist and linear speedup is only observed in the dominant term. For example, Theorem 2 of [1] shows the sample complexity for FedAsynQ-EqAvg required to find an $\\varepsilon$-optimal Q function scales in  $ \\tilde{O}\\left(\\frac{1}{M\\varepsilon^2} + M\\right)$, and they claim it achieves a linear speedup in the sample complexity with respect to $M$, even though an overhead term $M$ exists.  Similar examples can be found in Theorem 4.1 of [2].\n\n[1] Jiin Woo, Gauri Joshi, and Yuejie Chi. The blessing of heterogeneity in federated Q-learning: Linear speedup and beyond. In International Conference on Machine Learning, pp. 37157\u201337216, 2023.\n\n[2] Kumar Kshitij Patel, Lingxiao Wang, Aadirupa Saha, and Nathan Srebro. Federated online and bandit convex optimization. In International Conference on Machine Learning, pp. 27439\u201327460, 2023."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8542/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700273604182,
                "cdate": 1700273604182,
                "tmdate": 1700281752722,
                "mdate": 1700281752722,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9kTHD9iXCc",
                "forum": "fe6ANBxcKM",
                "replyto": "acXJaMjC5F",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8542/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8542/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer tdCn (part four)"
                    },
                    "comment": {
                        "value": "**Question 1**: It is a bit surprising that the number of communication rounds is linear in $M$. Can the authors intuitively explain why this must be the case, or if this term could be removed by more involved technical analysis?\n\n**Response**: \n The current linear dependency of the number of communication rounds on $M$ is due to the event-triggered synchronization design in our algorithm. Specifically, if in round $k$, the visitation to any $(x,a,h)$ tuple at any agent $m$ equals $\\max\\left(1,\\left\\lfloor\\frac{N _h^k(x,a)}{MH(H+1)}\\right\\rfloor\\right)$, the current round ends and communications happen.\nIntuitively, as more agents are involved in this procedure, such an event happens more frequently, triggering more communication rounds.\n\nMathematically,  under this synchronization protocol, for each round $k$, we can only claim that there exists one $(x,a,h,m)$ tuple such that the equality holds: \n$$n _h^{m,k}(x,a)= \\max\\left(1,\\left\\lfloor\\frac{N _h^k(x,a)}{MH(H+1)}\\right\\rfloor\\right).$$\nFor the $(x,a,h,m)$ that meets this equality, after the early sequential updating stage, $n_h^{m,k}(x,a)$ is at least $\\frac{N _h^k(x,a)}{MH(H+1)}$, which implies that $N _h^{k+1}(x,a)$ is at least $(1+\\frac{1}{MH(H+1)}) N _h^{k}(x,a)$, i.e., increasing exponentially from a round to the next round.\n \nOn the other hand, by the pigeonhole principle, if in total there are $K$ communication rounds over time $T$, there must exist one $(x,a,h)$ tuple such that the above equality holds for at least $\\left\\lceil\\frac{K}{HSA}\\right\\rceil$ rounds. Combining with the exponential increase in the visiting number $n_h^{m,k}(x,a)$ during those rounds, we have that\n $$MT\\geq \\Omega\\left(\\left[1+\\Omega\\left(\\frac{1}{MH(H+1)}\\right)\\right]^{\\left\\lceil K/(HSA)\\right\\rceil}\\right).$$\n We note that $M$ appears in the denominator of the base on the RHS but is absent from the exponent, leading to the linear dependency on $M$ in the communication cost.\n\n We point out that such a linear dependency on $M$ in the communication round upper bound commonly exists in federated RL algorithms, such as [1], [3].\n\n\n**The linear dependency in $M$ can be improved by removing the synchronization design.** Specifically, we can let each agent $m$ continue its current exploration round until the above equation is met for an $(x,a,h)$ tuple locally. Thus, out of $K$ rounds, the equality must hold for at least $KM$ times. By the pigeonhole principle, there exists one $(x,a,h)$ such that the above equality holds for at least $\\left\\lceil\\frac{KM}{HSA}\\right\\rceil$ times. Intuitively, since now the length of each round is determined locally, the total number of communication rounds does not depend on $M$. A detailed analysis can be performed to show that the communication round scales in $O(\\log T)$. We note that the implicit price for the improved dependency in $M$ is that the clients who finish their exploration in the current round earlier need to wait until all clients are done before proceeding to the next round. \n\n \n[1] Jiin Woo, Gauri Joshi, and Yuejie Chi. The blessing of heterogeneity in federated Q-learning: Linear speedup and beyond. In International Conference on Machine Learning, pp. 37157\u201337216, 2023.\n\n\n[3] Yiding Chen, Xuezhou Zhang, Kaiqing Zhang, Mengdi Wang, and Xiaojin Zhu. Byzantine-robust online and offline distributed reinforcement learning. In International Conference on Artificial Intelligence and Statistics, pp. 3230\u20133269. PMLR, 2023."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8542/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700278667454,
                "cdate": 1700278667454,
                "tmdate": 1700281384533,
                "mdate": 1700281384533,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zM4jmFm6Y9",
                "forum": "fe6ANBxcKM",
                "replyto": "acXJaMjC5F",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8542/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8542/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer tdCn (part five)"
                    },
                    "comment": {
                        "value": "**Question 2**: Can the authors provide either some lower bound or some additional justification for the overhead terms?\n\n**Response**: Thank you for the question. We explain why those overhead terms linear in $M$ appear in the regret upper bound as follows. \n\nIn our paper, $N _h^k(x,a)$ is the total visiting number of $(x,a,h)$ before starting round $k$ and $n _h^{m,k}(x,a)$ is the visiting number of $(x,a,h)$ of agent $m$ in round $k$. Our design for both the Hoeffding-type and the Bernstein-type algorithms guarantees that\n$$n _h^{m,k}(x,a)\\leq \\max \\left(1,\\left\\lfloor\\frac{N _h^k(x,a)}{MH(H+1)}\\right\\rfloor\\right).$$\nDuring the early stage of the algorithms when $N _h^k(x,a)$ is small, we have $n _h^{m,k}(x,a)\\leq 1$. Therefore, the total number of visits during round $k$ satisfies that\n$$n _h^k(x,a) = \\sum _{m \\in [M]} n _h^{m,k}(x,a)\\leq M .$$\nOnly when $N _h^k(x,a)$ becomes sufficiently large, the algorithms enter the second stage and the statistics start to kick in.\n  \nIn other words, the overhead terms is contributed by the $O(M)$ samples collected in the first stage, i.e., the burn-in cost. Such a burn-in cost is arguably inevitable in the federated setting. Intuitively, the learning agent needs to collect at least one sample for each $(x,a,h)$ tuple for a proper initialization. In the federated setting where all local clients collect their samples *in parallel*, it is hard to coordinate their exploration to reduce the dependency on $M$. We note that such linear overhead terms exist in [1] above as well.\n\n\n**Question 3**: For theorem 4.1, why can we ignore the overhead terms that are on the order of  in the general FL setting?\n\n**Response**: Please refer to our response to **Weakness 1**.\n\n-----\n\nWe thank the reviewer again for the helpful comments and suggestions for our work. If our response resolves your concerns to a satisfactory level, we kindly ask the reviewer to consider raising the rating of our work. Certainly, we are more than happy to address any further questions that you may have."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8542/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700278733817,
                "cdate": 1700278733817,
                "tmdate": 1700281419428,
                "mdate": 1700281419428,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pfYhT9CAIc",
                "forum": "fe6ANBxcKM",
                "replyto": "acXJaMjC5F",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8542/Reviewer_tdCn"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8542/Reviewer_tdCn"
                ],
                "content": {
                    "comment": {
                        "value": "I am not convinced by the comment on the \"linear speedup\".\n\n- The overhead no longer depends on $M$ because an average is taken over all the machines. On a per machine basis, that means there is at least a part of the regret that does not benefit from the distributed setup whatsoever.\n\n- Moreover, in the discussion for Theorem 4.1 in [2], an example provided here, the authors of [2] explicitly mentions that we see linear speedup only for certain problem settings, but not overall. The referenced work clearly discusses when linear speedup is possible and when it is not by deriving a condition for $d$ under which the overhead term is dominated. Similar discussions are missing in the submission.\n\n- For [1], they claim linear speedup because in Theorem 1, the number of rounds required $T$ itself is shown to be $1/K$. In other words, they have achieved linear speedup because there are no additional terms that are not improved by having more machines, which is not the case for the regret bound in this paper. In later theorems, [1] also discusses the implications of $T_0$ and $\\tilde{T}$ in the form of burn-in costs. For Theorems 2 and 3, [1] explicitly said that the burn-in costs need to be amortized over time in order to achieve some notion of linear speedup. Their burn-in costs are also slightly more appealing, as they are instance dependent and are not directly affected by $|S|$, $|A|$, and $H$.\n\nI think the paper would benefit greatly if it could incorporate these discussions, including\n1. The reasons behind the burn-in cost and a brief discussion on when is the regret bound dominated by the term that is $O(\\sqrt{M})$ rather than the terms that are $O(M)$.\n2. Challenges caused by the RL setting that are not found in federated convex optimization."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8542/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700365574859,
                "cdate": 1700365574859,
                "tmdate": 1700366309736,
                "mdate": 1700366309736,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PPhZBq25ke",
                "forum": "fe6ANBxcKM",
                "replyto": "acXJaMjC5F",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8542/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8542/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer tdCn on new comments"
                    },
                    "comment": {
                        "value": "We thank the reviewer for carefully checking our responses and providing prompt feedback and insightful suggestions. We address the reviewer's comments in the following and have revised the paper accordingly. We hope the responses below and the changes in the paper address the reviewer's concerns. We would be more than happy to address any further concerns that you may have.\n\n**Q1:** Condition for linear regret speedup in our work.\n\n**A1:** We thank the reviewer for the helpful suggestion about the conditions under which the linear regret speedup holds for the dominating term.  Rigorously speaking, the linear regret speedup can be guaranteed when the time steps $T$ is sufficiently large. Specifically, in Theorem 4.1, we show that with high probability, \n$$\\mbox{Regret}(T)\\leq \\tilde{O}(1)\\times O(\\sqrt{H^4MTSA} + M\\mbox{poly}(H,S,A)).$$\nThus, when $T\\geq \\Omega(M\\mbox{poly}(H,S,A)),$\nwith high probability, FedQ-Hoeffding enjoys a linear speedup as follows\n$$\\mbox{Reget}(T)\\leq \\tilde{O}(\\sqrt{H^4MTSA}).$$\n\nSimilar arguments can be provided for FedQ-Bernstein. When $T\\geq \\tilde{\\Omega}(M\\mbox{poly}(H,S,A))$ where $\\tilde{\\Omega}$ hides a log factor that takes the form $\\log^2 (MT\\mbox{poly}(H,S,A))$), the term $\\tilde{O}(\\sqrt{H^3SAMT})$ in Theorem 5.1 becomes the dominating term, thus a linear regret speedup can be achieved.\n\nWe have added those conditions in the remarks after the theorems to make our statements more rigorous. We have also made according changes throughout the paper to clarify that the claimed linear speedup only holds when $T$ is sufficiently large.\n\n**Q2:** Discussions on the burn-in costs.\n\n**A2:** Thank you for your valuable suggestion! We have followed your advice to incorporate a brief discussion on the burn-in cost in the remark after Theorem 4.1. \n\nWe would like to clarify that the instance-dependent burn-in cost in [1] also depends on $H,S,A$ in the asynchronous setting, and thus is of the same flavor as in our work.\n\nFor asynchronous setting with equal weights (Theorem 2 in [1]), the burn in cost is\n    $$\\tilde{O}\\left(\\frac{t _{min}^{max}\\max (\\frac{1}{1-\\gamma}, M)}{\\mu _{min}^2(1-\\gamma)}\\right).$$\nFor asynchronous setting with unequal weights (Theorem 3 in [1]), it is \n    $$\\tilde{O}\\left(\\frac{\\max(t _{min}^{max},\\frac{1}{1-\\gamma}, M)}{\\mu _{avg}(1-\\gamma)}\\right).$$\nHere $\\mu _{min} = \\min _{x,a,m} \\mu^m(x,a)$ and $\\mu _{avg} = \\min _{x,a}\\frac{1}{M}\\sum _{m=1}^M\\mu^m(x,a)$ in which $\\mu^m(x,a)$ is the stationary visitation frequency for $(x,a)$ under the exploration policy at agent $m$, and constant $t _{min}^{max}\\geq 1$ is the required time for reaching the stationary frequency. \n\nWe first note that $\\mu _{min} \\leq \\mu _{avg} \\leq \\frac{1}{SA}$. Besides, if we use the approximation $(1-\\gamma)^{-1} \\approx H$ to convert the results from infinite-horizon MDPs to those under episodic MDPs, both of the bounds now explicitly depend on $M,S,A,H$.\n\n**Q3:** Discussion on the challenges caused by the RL setting that are not found in federated convex optimization.\n\n**A3:** Thank you for this great suggestion. We have included a brief discussion on the unique challenges in federated RL that are not found in FL in the Proof Sketch of Theorem 4.1. Specifically, we add the following paragragh:\n\n*We would like to emphasize that the techniques required to bound those non-martingale differences are fundamentally different from the commonly used techniques in federated learning (FL), which usually construct an \"averaged parameter update path'' and then bound each local term's \"drift'' from it. This is because such bounding techniques in FL rely on certain assumptions that do not exist in federated RL. Due to the inherent randomness in the environment, even if the same policy is taken at all local agents, it may result in very different trajectories. Thus, it is hard to obtain an easy-to-track \"averaged parameter update path'' in federated RL, or a tight bound on the local terms' drifts from such averaged parameter update path.  We overcome this challenge by relating $\\{\\tilde{\\theta} _{t^k}^i\\} _i$ with $\\{{\\theta} _{t^k}^i\\} _i$. Instead of bounding the local drift $\\tilde{\\theta} _{t^k}^i-{\\theta} _{t^k}^i$ in each time step, we choose to group the ``drift'' terms based on the corresponding rounds and then leverage the round-wise equal weight assignment adopted in our algorithm to obtain a tight bound.*\n\n----\n\nWe thank the reviewer again for your prompt reply and invaluable suggestions, which have helped us greatly to enhance the rigor and presentation of our work. If you have any further comments, questions or suggestions, we will be more than happy to address them. Meanwhile, if our responses have addressed your concerns satisfactorily, we kindly request that you re-evaluate our work based on the revision and raise the rating of our work."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8542/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700411505808,
                "cdate": 1700411505808,
                "tmdate": 1700412253262,
                "mdate": 1700412253262,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]