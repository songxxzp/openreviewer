[
    {
        "title": "Learning From Simplicial Data Based on Random Walks and 1D Convolutions"
    },
    {
        "review": {
            "id": "72Ffj20HA4",
            "forum": "OsGUnYOzii",
            "replyto": "OsGUnYOzii",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9185/Reviewer_vWSA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9185/Reviewer_vWSA"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a novel algorithm for learning on simplicial\ncomplexes, i.e. generalisations of graphs. As opposed to relying on the\nmessage passing paradigm, the paper uses random walks (or rather,\n*sampled* random walks) to featurise the input data, subsequently\nemploying 1D convolutions as a type of local update mechanism.\nExperiments on several data sets demonstrate the efficacy of the method."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The paper is exceptionally well written and describes highly-original\nresearch. Given the recent interest in extending graph learning to\nhigher-order structures, the work is also timely and highly-relevant.\n\nMoreover, I appreciate that this paper presents alternatives to the\nprevailing message passing paradigm. As amply demonstrated in the\nmanuscript, this leads to a different class of expressivity, thus\nopening the door to further explorations.\n\nThe current write-up is sufficiently detailed to be reproducible, and\nall concepts are explained sufficiently well. This paper was truly\na pleasure to read and review."
                },
                "weaknesses": {
                    "value": "Only minor weaknesses cropped up, which can be easily addressed in\na revision (please also see my questions below).\n\n- The impact of parameter choices could be studied more carefully.\n\n- A discussion on the utility of a simplicial perspective might enrich\n  the paper. As it stands now, the experiments deal with simplicial\n  data. The scope of the paper could be improved substantially if the\n  simplicial perspective was shown to be *crucial* for good performance.\n  I want to stress that I consider this to be a minor weakness since the\n  paper as such already makes a strong contribution under the assumption\n  that simplicial complexes are the 'right' thing to model the data.\n  Showing this empirically would just be a cherry on top of this cake.\n\n  (the experiment on social contact networks is a good start for this,\n  but I would appreciate a more in-depth discussion **if possible**)\n\n- Unless I am mistaken, the proper verb form should be 'convolved' as\n  a opposed to 'convoluted' (in Figure 2).\n\n- As a minor style issue: please use `\\citep` and `\\citet` (when using\n  `natbib`) consistently. The former is meant for parenthetical\n  citations, the latter for in-text citations.\n\n- There are some inconsistencies (mostly capitalisation / venues) in the\n  references.\n\n- When it comes to explaining expressivity in the context of WL,\n  a [recent survey](https://arxiv.org/abs/2112.09992) might be useful as\n  an additional reference. (The related work covered at the moment is\n  sufficient; this is just a suggestion. The review form unfortunately\n  lacks a field for 'additional comments')"
                },
                "questions": {
                    "value": "1. How are the random walk parameters (length / number of walks)\n   affecting performance? Could you show predictive performance as\n   a function of these parameters (or, even better, provide theoretical\n   guarantees)?\n\n2. Would the proposed method also extend to *hypergraphs* or other\n   combinatorial complexes? This seems to be the case, given that merely\n   a notion of a random walk is required.\n\n3. Given the performance considerations, are there any practical\n   limitations in terms of data set size that could be given? This might\n   be useful to assess the suitability for certain data sets in advance."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9185/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698082524867,
            "cdate": 1698082524867,
            "tmdate": 1699637155877,
            "mdate": 1699637155877,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PIMNgycg3z",
                "forum": "OsGUnYOzii",
                "replyto": "72Ffj20HA4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9185/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9185/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your detailed response. Let us address each point separately:\n\n**Parameter Choice**\n\nWe added an ablation study to the paper that explores the influence of considered adjacency, the size of the local window, and the length of the random walks on the performance on the social contact datasets.\n\n**Utility of Simplicial Perspective**\n\nWe agree that highlighting the importance a simplicial perspective on datasets would be benefitial.\nHowever due to space constraints we considered this outside the scope of our paper.\nThe comparison between SCRaWl and CRaWl (Figure 7) provides an initial indication towards this topic and other authors considered this comparison as well, e.g. Bodnar et al, 2021.\n\nWe argue that this topic should be dealt with in a more dedicated work and on a more abstract level not focused on one particular neural network architecture but rather with different graph neural networks and simplicial neural networks in mind.\n\n**Application to other Complexes**\n\nThe approach should also work on combinatorial complexes, provided an analogous notion of a random walk is defined. \nThis is similar to the case of cell complexes as explained in the conclusion.\nFor hypergraphs, the notion of upper and lower adjacency is not well defined, which is why we did not consider them in our work.\nOf course, ignoring this (e.g., by merging the respective columns into one) would yield a valid approach, but we did not investigate the performance of such a method in detail here.\n\n**Dataset Size**\n\nFor larger datasets, reducing the number of random walks is a viable option to reduce complexity.\nChoosing one random walk per simplex makes is very likely to cover each simplex sufficiently often as a center simplex in a local window, but this property is neither ensured nor necessary.\nIt is also worth considering to use smaller parameters during training and larger parameters during inference.\n\n**Minor Things**\n\nWe added the suggested survey paper as reference for expressivity in context with WL and fixed the minor style issues."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9185/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700680257040,
                "cdate": 1700680257040,
                "tmdate": 1700731585815,
                "mdate": 1700731585815,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qB1zSRSVat",
                "forum": "OsGUnYOzii",
                "replyto": "PIMNgycg3z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9185/Reviewer_vWSA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9185/Reviewer_vWSA"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks a lot\u2014I'm very excited about your work and believe it will have substantial impact!"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9185/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700731472684,
                "cdate": 1700731472684,
                "tmdate": 1700731472684,
                "mdate": 1700731472684,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PaVadeR7qQ",
            "forum": "OsGUnYOzii",
            "replyto": "OsGUnYOzii",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9185/Reviewer_Wmrd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9185/Reviewer_Wmrd"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents SCRaWI, a neural network model to encode simplicial complexes based on higher-order random walks on simplices. SCRaWI samples several higher-order random walks followed by fast 1D convolutions to learn the structural properties of the Simplicial Data."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is well-written and easy to follow. Also, it is well-organized with enough illustrative figures. The proposed method is simple, yet potentially powerful, which makes it feasible for large and real-world cases. More, specifically, the model design allows choosing the number of sampled random walks as a hyperparameter, which provides a trade-off between computational cost and the expressivity of the model. Also, using fast Fourier transforms to perform convolutions on sampled walks is an interesting idea, which potentially can improve efficiency and training time."
                },
                "weaknesses": {
                    "value": "The paper has missed some important related studies or detailed discussions about them. As an example, there are several random walk-based methods [1, 2] in a simple graph, which uses a similar idea. while they have been briefly mentioned, there are some important questions about them that are not discussed in the paper. Is there any connection between SCRaWl and these methods? Or SCRaWl is the extended version of these studies to higher-order data? A very similar study on higher-order graphs is CATWALK [3], which is a hypergraph learning method that similarly uses higher-order random walks to learn higher-order patterns. I think given the fact that hypergraphs can be seen as a general case of simplicial complexes, their walk sampling is exactly the same as this work, which limits the novelty of this paper's idea and its model design.\n\nThe proposed method is simple, which is a desirable property **if** the effectiveness of the method is theoretically or experimentally shown. That is, the novelty of the model architecture is limited and so it is expected that extensive experimental or theoretical results support the method performance. However, in the paper, the experimental study is very limited, e.g.:\n\n1. There are only three datasets from two different domains. Please note that co-authorship networks and social networks usually have similar properties so it would be much better if the authors could provide more datasets with different domains, specifically in drug-drug or chemical networks (NDC, and/or NDC Substances), and communication networks (Email Enron). \n \n2. There is a lack of ablation study on the method architecture. Accordingly, it is not clear what is the contribution of each component of SCRaWl. How using a simple random walk instead of a higher-order random walk can affect the performance? How does each of the six features contribute to the performance of SCRaWl?\n\n3. The main motivations and claims in the paper are not supported by experiments. For example, when the main motivation is to address the inefficiency of the existing methods, I think it is needed to see how SCRaWl performs compared to existing methods in terms of time and how proposed components are improving the efficiency. \n\n4. Hypergraphs are another paradigm to represent higher-order interactions. There is a lack of comparison with state-of-the-art hypergraph learning methods [3, 4, 5]. \n\n\n\nIn addition to the above points, as discussed in the \"Introduction\" section, there is a trade-off between computational cost and the power of SCRaWl. It would be great if you could show this trade-off in the experiments as well. \n\n  \n  $$ $$\n  $$ $$\n\n\n[1] Random walk graph neural networks. NeurIPS 2020.\n\n[2] Walking out of the Weisfeiler Leman hierarchy: Graph learning beyond message passing. TMLR 2023. \n\n[3] CAT-Walk: Inductive Hypergraph Learning via Set Walks. NeurIPS 2023.\n\n[4] Teasing out missing reactions in genome-scale metabolic networks through hypergraph learning. Nature Communications 2023.\n\n[5] You are allset: A multiset function framework for hypergraph neural networks. ICLR 2022."
                },
                "questions": {
                    "value": "Please see the Weaknesses. In summary, my suggestions are here: Please add \n\n1. more discussions about existing methods [1, 2]. Also please discuss [3] and its differences with your method.\n2. more datasets, different experimental settings, and hypergraph learning-based baselines. \n3. ablation studies to show the contribution of each component. \n4. scalability and efficiency evaluation."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9185/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9185/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9185/Reviewer_Wmrd"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9185/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698796524638,
            "cdate": 1698796524638,
            "tmdate": 1699637155766,
            "mdate": 1699637155766,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lnwxqetPd4",
                "forum": "OsGUnYOzii",
                "replyto": "PaVadeR7qQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9185/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9185/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**References**\n\nWe incorporated the suggested references into our work.\nReferences [1] and [2] have already been cited in our related work section before.\nIn particular, [2] is the primary work we built upon by extending their approach to simplicial complexes.\nWe believe this is made clear in the current version of the paper already, e.g. in the introduction: \"[...] here we take inspiration from random walk-based graph neural networks on graphs (T\u00f6nshoff et al., 2023) and explore the use of a random walk-based learning architecture for simplicial complexes.\"\n\nIt is our understanding that [4] focuses on hyperedge link prediction and their proposed method is therefore not (directly) applicable to the tasks considered in our paper.\nSimilarly, [3] deals with temporal data, which is not the focus of our paper, though they also explain how their approach can be applied to node classification on static hypergraphs.\nWe however want to point out that this work was published after the submission deadline of our paper.\n\nWe added a paragraph to our related works section that mentions hypergraphs as another paradigm to represent higher-order interactions and reference to some noticeable hypergraph neural networks, including the proposed references [3] and [5].\nWe further extended the comparison on the social contact datasets with hypergraph neural networks (Table 3 in the appendix).\n\n**Ablation Study**\n\nWe added an ablation study on the social contact datasets to the appendix where we compare the influence of the window size $s$, the walk length $\\ell$, and of lower and upper adjacency when sampling the walks."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9185/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700680237663,
                "cdate": 1700680237663,
                "tmdate": 1700680237663,
                "mdate": 1700680237663,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "uaXHP5F7kc",
            "forum": "OsGUnYOzii",
            "replyto": "OsGUnYOzii",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9185/Reviewer_WBPx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9185/Reviewer_WBPx"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a method called SCRAWL which explores simplicial complexes-based learning representations for graph datasets using random walks and 1D convolution on simplices. This work is built on top of the recent work CRAWL which uses random walks and 1D conv directly on graphs. The authors performed two tasks vertex classification and missing citation counts on two benchmark datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is well presented and motivated. \n2. Additional information from simplices seems to improve embeddings. The results are interesting. \n3. An ablation study is provided to show the robustness and compare it against existing works. \n4. The expressivity is equivalent to CRAWL. So, the theorem by CRAWL holds for this too."
                },
                "weaknesses": {
                    "value": "1. The major concern I could see is memory and computation as it contains the matrices for k levels of simplices. And, each walk consists of six matrices. \n2. It is not clear how long the walk is and how many walks are computed for each of the k-simplices. Assuming m is for the collection of all walks from all k-simplices. \n3. Why are walks passed in every layer? Is it not enough to use it only at the input? \n4. How are the output from k-simplices combined for the final output? I could see three different outputs in Fig. 3. \n5. Fig 6. results are on par with MPSN. Justification of why it works differently on two different kinds of datasets would be much appreciated. Results are improved significantly for social networks although it."
                },
                "questions": {
                    "value": "Why are walks computed on the fly or they are just sampled on the fly? Can\u2019t we precompute the walks which is just one time processing before training?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9185/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698819594376,
            "cdate": 1698819594376,
            "tmdate": 1699637155641,
            "mdate": 1699637155641,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "V9AOYQqbjf",
                "forum": "OsGUnYOzii",
                "replyto": "uaXHP5F7kc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9185/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9185/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "1. We agree that the memory footprint is considerable, especially for large datasets.\n   However, this is generally the case for simplicial complexes as the simplicial property forces exponentially many sub-simplices for any higher order simplex.\n   The feature matrices themselves can be computed on the fly and are very sparse in the later columns (containing the structural information about connectivity and identity).\n   The first columns contain features that any reasonable approach needs to store as well.\n   An advantage of our approach is, however, that we can vary the scalability, e.g., by selecting the window sizes and walk lengths, which is not possible for classical message passing architectures. Investigating such trade-offs in more detail (more broadly: how to optimally leverage higher-order information, while keeping computations tractable) is an interesting topic we intend to explore in future work in more detail.\n2. In our experiments, we sampled one random walk per simplex.\n   Walk lengths are set to 5 for the semantic scholar dataset and 50 for the other datasets.\n   These parameters are mention in the text of section 5 and Table 1 in the appendix.\n   These parameters are of course hyperparameters and can be tuned for the respective dataset.\n3. The feature matrices change in every layer (because the hidden states are updated), thus the random walks are passed to every layer.\n4. This depends on the dataset at hand and the considered targets.\n   For the semantic scholar dataset, the outputs $y_0, ..., y_5$ are compared as is with the targets.\n   For the social contact dataset, only the outputs $y_0$ are considered.\n5. For the semantic scholar dataset, the results of SCRaWl and MPSN are close to perfect, so we cannot expect any strong improvement beyond MPSN.\n   The social contact datasets show that SCRaWl is able to outperform MPSN.\n\nRegarding your questions:\nYes, the walks can be precomputed as well. This is actually also implemented in our code.\nHowever, this does not have any influence on imputation accuracy, but only on training time, which is why we did not include it in the paper.\nNotice that the same random walks should not be reused in every epoch to avoid overfitting to these specific walks."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9185/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700680217181,
                "cdate": 1700680217181,
                "tmdate": 1700680217181,
                "mdate": 1700680217181,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7LlZwVbfmd",
                "forum": "OsGUnYOzii",
                "replyto": "V9AOYQqbjf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9185/Reviewer_WBPx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9185/Reviewer_WBPx"
                ],
                "content": {
                    "title": {
                        "value": "Thanks"
                    },
                    "comment": {
                        "value": "Thanks for the feedback. I have read the responses to my concerns. Most of the concerns are addressed. I will consolidate everything in the final comments and rating."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9185/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700717375122,
                "cdate": 1700717375122,
                "tmdate": 1700717375122,
                "mdate": 1700717375122,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "AOixfP3lBq",
            "forum": "OsGUnYOzii",
            "replyto": "OsGUnYOzii",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9185/Reviewer_6GSS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9185/Reviewer_6GSS"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new learning method on simplicial complexes based on random walks and 1-D convolution encodings."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The idea of using random walks to tackle the high complexity of learning on simplicial complexes is a natural idea. The overall design also looks reasonable to me.\n2. Experiments show that the proposed method outperforms some of the most recent methods."
                },
                "weaknesses": {
                    "value": "1. Compared to this work, a recent work [1] seems to provide more principled insights into exactly the same topic. Can the authors discuss more of its edge over this work?\n\n2. The experiments are conducted on a relatively limited number of datasets. I would hope to see more datasets from diverse domains being used.\n\n[1] Facilitating Graph Neural Networks with Random Walk on Simplicial Complexes, NeurIPS 2023"
                },
                "questions": {
                    "value": "See Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9185/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699112684195,
            "cdate": 1699112684195,
            "tmdate": 1699637155513,
            "mdate": 1699637155513,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8IRDTn9dmu",
                "forum": "OsGUnYOzii",
                "replyto": "AOixfP3lBq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9185/Reviewer_6GSS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9185/Reviewer_6GSS"
                ],
                "content": {
                    "comment": {
                        "value": "The rebuttal period is approaching the end. Can the authors please respond to my review? Also, I have read Reviewer Wmrd's review, and would like to echo with their concern about comparison with CATWALK and lack of experiment on more diverse dataset. Can the authors please also address these points?"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9185/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700674445477,
                "cdate": 1700674445477,
                "tmdate": 1700674555815,
                "mdate": 1700674555815,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "c1rSZGuKpr",
                "forum": "OsGUnYOzii",
                "replyto": "AOixfP3lBq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9185/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9185/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their feedback.\n\nWe would like to point out that the mentioned paper was published after the submission deadline of this paper.\nHowever, we agree that the paper provides interesting results and discuss it now in our work.\nNotice that the paper is similar to RWNN (already in our related work) in that the authors employ random walk probabilities as additional node and edge features, which enhance the main graph neural network.\nDifferent to RWNN, they use random walks on simplicial complexes (e.g. by lifting).\nIn contrast, in our work, the concrete random walks are not used as enhancement, but as the main building block of the network."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9185/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700680194277,
                "cdate": 1700680194277,
                "tmdate": 1700680194277,
                "mdate": 1700680194277,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]