[
    {
        "title": "Expressivity of ReLU-Networks under Convex Relaxations"
    },
    {
        "review": {
            "id": "JptOWTIcPP",
            "forum": "awHTL3Hpto",
            "replyto": "awHTL3Hpto",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7513/Reviewer_pT5c"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7513/Reviewer_pT5c"
            ],
            "content": {
                "summary": {
                    "value": "This work studies the expressive power of ReLU neural networks under various convex relaxations, by measuring their ability to represent (certain subclass of) CPWL functions. On the positive side, for univariate CPWL functions, most convex relaxation methods are shown to be able to express monotone or convex CPWL functions, and Multi-Neuron can even represent all CPWL functions. However, it's shown that all these methods fail in the multivariate case: they can't even represent the simple $\\max$ function in $\\mathbb{R}^2$."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "**Novel direction:** the expressive power of neural networks is an interesting and important topic. This work is the first to consider such expressive power in the precise representation setting.\n\n**Wide coverage:** in the univariate setting, this work discusses a broad range of convex relaxation methods, which covers most popular ones in practice."
                },
                "weaknesses": {
                    "value": "**Univariate is restricted:** the main weakness I found is the restricted univariate assumption. All the positive results for the expressive power of convex relaxation methods hold for univariate functions, which is restricted in two ways: (1) in practice, almost all functions of interest are multivariate (2) in theory, the case of univariate functions is too special, which often times avoids the general difficulty in high dimensions and thereby hard to generalize.\n\nOn the other hand, the negative multivariate result only holds for $\\triangle$, and the precision gap can be arbitrarily small. It's not clear whether multivariate methods can express multivariate CPWL functions precisely."
                },
                "questions": {
                    "value": "**Motivation for precise analysis:** it's mentioned in this work if we allow approximate analysis, for any approximation error $\\epsilon>0$ and general multivariate continuous function on $\\mathbb{R}^n$, IBP can express the function up to $\\epsilon$ error (Baader et al 20). This seems a strong enough guarantee. What's the significance of considering precise analysis beyond pure theoretical interest?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7513/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7513/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7513/Reviewer_pT5c"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7513/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697680466710,
            "cdate": 1697680466710,
            "tmdate": 1700658816912,
            "mdate": 1700658816912,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RQRFzJ000x",
                "forum": "awHTL3Hpto",
                "replyto": "JptOWTIcPP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7513/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7513/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer pT5c Part I"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their high-quality review and the interesting questions they raised. We are happy to hear that they find our approach novel and research question both important and interesting. Below we address their remaining questions.\n\n**Q1: Can you discuss the relevance of your results in the heavily restricted univariate domain? What are the implications of your negative results for the $\\Delta$-relaxation in the multivariate setting?**  \n\nFirst, we want to clarify that the $\\Delta$-relaxation is strictly more precise than the DeepPoly and IBP relaxations. Thus, our negative result for the $\\Delta$-relaxation in the multivariate setting directly implies negative results for the other, less precise relaxations (DeepPoly-0, DeepPoly-1, and IBP), even for extremely simple functions. We believe this result to be of great importance as the majority of neural network verifiers are upper bounded in their analysis precision by the so-called \u201cconvex relaxation barrier\u201d, realized by the $\\Delta$-relaxation [1]. Only a few, very recent methods use multi-neuron-relaxations (at great computational cost) to break this barrier [2,3]. This highlights that even extremely simple functions can not be encoded as ReLU networks such that these simpler verifiers can analyze them precisely and agrees well with the fact that the latter methods have recently dominated the international neural network verification competition [4].\n\nHowever, as even simple multivariate functions can not be encoded such that their analysis is precise using any single neuron domain $D$ ($D$ is IBP, DeepPoly-0, DeepPoly-1 or $\\Delta$), we can not separate them with respect to their expressivity in this setting. To do so and investigate if the more complex domains improve expressivity, we study the univariate case, and indeed show positive results and separation between methods, both in terms of expressivity and size of resulting solution spaces.\n\n**Q2: Why do you not discuss multi-neuron relaxations in the multi-variate setting?**  \nWe agree with the reviewer that this is an interesting question and have given it some thought, but remain unable to prove either a positive or negative result. We note that the first (to the best of our knowledge) constructive proof by He et al (2020) [9] showing that arbitrary CPWL functions can be encoded by (finite) ReLU networks was only published recently, highlighting the difficulty of this question even in the non-robust setting. Unfortunately, a multi-neuron analysis of the networks constructed in this proof is not precise, thus an altogether different construction would be required which would be out of scope for this already dense work. To show a negative result with our technique, we would require a network simplification strategy (and counterexample) for multi-neuron relaxations. However, simple functions with one non-linearity (such as the max function) can be analyzed precisely using multi-neuron relaxations and more complex functions/subnetworks with two or more non-linearities can not generally be simplified to fewer layers, necessitating a completely different approach. Thus we believe this to be out of the scope of this work. \n\nFurther, to provide some context to the contributions made in this work, we want to again highlight that prior work [6] considered only a single relaxation method (the simple IBP), while we consider four relaxations, the univariate and multivariate setting, and four function classes.\n\n**References:**   \n[1] Salman, et al. \"A convex relaxation barrier to tight robustness verification of neural networks.\" NeurIPS 2019  \n[2] Ferrari et al. \"Complete verification via multi-neuron relaxation guided branch-and-bound.\" ICLR 2022  \n[3] Zhang et al. \"General cutting planes for bound-propagation-based neural network verification.\" NeurIPS 2022  \n[4] M\u00fcller et al. \"The third international verification of neural networks competition (VNN-COMP 2022): summary and results.\" arXiv 2022  \n[6] Mirman et al. \u201cThe fundamental limits of neural networks for interval certified robustness\u201d, TMLR 2022  \n[7] Baader et al. \u201cUniversal approximation with certified networks\u201d, ICLR 2020  \n[8] Wang et al. \u201cInterval universal approximation for neural networks\u201d, POPL 2022  \n[9] He, et al. \"ReLU deep neural networks and linear finite elements.\" JCM 2020  \n[10] M\u00fcller et al. \"Certified training: Small boxes are all you need.\"ICLR 2023  \n[11] Mao et al. \"Connecting Certified and Adversarial Training.\" NeurIPS 2023  \n[12] De Palma et al. \"Expressive Losses for Verified Robustness via Convex Combinations.\" arXiv 2023"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7513/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699963499503,
                "cdate": 1699963499503,
                "tmdate": 1699963571420,
                "mdate": 1699963571420,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "H5agyyrIIU",
                "forum": "awHTL3Hpto",
                "replyto": "RQRFzJ000x",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7513/Reviewer_pT5c"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7513/Reviewer_pT5c"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the reply. Although I still think univariate is a big restriction, given the lower bounds in the multivariate setting and the fact that this work is the first result of the kind, considering univariate is forgivable and I have raised the score accordingly."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7513/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700658800517,
                "cdate": 1700658800517,
                "tmdate": 1700658800517,
                "mdate": 1700658800517,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FBYcztHQI7",
            "forum": "awHTL3Hpto",
            "replyto": "awHTL3Hpto",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7513/Reviewer_y6LY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7513/Reviewer_y6LY"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the expressive power of ReLU neural networks under different convex relaxations that are commonly used for neural network certification. The key findings are:\n\n* For univariate functions, more precise convex relaxations like \u0394 and DeepPoly allow expressing larger classes of continuous piecewise linear (CPWL) functions precisely compared to the simple interval bound propagation (IBP).\n* IBP can precisely express monotone CPWL functions, while \u0394 and DeepPoly can also express convex CPWL functions.\n* Multi-neuron relaxations allow single-layer networks to express all univariate CPWL functions precisely.\n* For multivariate functions, even the most precise single-neuron relaxation (\u0394) cannot precisely express simple classes like multivariate monotone convex CPWL functions.\n* This suggests single-neuron convex relaxations are fundamentally limited for multivariate functions, highlighting the need for more precise analysis methods like multi-neuron relaxations.\n* The results have implications for certified training, suggesting more precise relaxations could yield larger effective hypothesis spaces and higher performance if optimization challenges can be overcome.\n\nIn summary, the paper provides an in-depth analysis of the expressive power of ReLU networks under different convex relaxations, showing more precise relaxations increase expressivity for univariate functions but are still fundamentally limited for multivariate functions. The results motivate developing more advanced analysis techniques and studying their potential benefits for certified training."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Here are some strengths of this paper:\n\n* It provides the first in-depth, systematic study on the expressive power of ReLU networks under a wide range of convex relaxations commonly used in neural network certification.\n* The analysis covers univariate and multivariate functions and simple as well as more complex function classes like (monotone) convex CPWL.\n* It clearly differentiates the capabilities of different relaxations through precise mathematical results and constructive proofs.\n* The paper relates the theoretical results back to certified training, drawing interesting hypotheses about the potential benefits of more advanced relaxations.\n* The paper is clearly structured and provides detailed mathematical proofs for all results."
                },
                "weaknesses": {
                    "value": "Some potential weaknesses of this paper:\n\n* The focus is exclusively on ReLU networks, not covering other activation functions commonly used like sigmoid or tanh.\n* Only fully-connected feedforward networks are considered, not convolutional or residual architectures widely used in practice.\n* The analysis is limited to deterministic networks, not touching on stochastic networks.\n* Only standardized datasets and perturbation sets are studied; results may not generalize to other domains.\n* While hypotheses are provided for certified training, no experiments are conducted to validate the conjectured benefits.\n* The writing is quite dense and mathematical, which could make it less accessible to a general AI audience.\n* Aside from certified training, implications for other applications of neural network analysis are not discussed much."
                },
                "questions": {
                    "value": "Here are my questions:\n\n* There is recent literature on the convex optimization of ReLU network, e.g., [1] and several other follow-up papers by the same authors extending this work to various neural network architectures. Can authors comment on their contributions over this work and explain how their paper supports/refutes the claim there? For example, the very first question to comment on would be: What is the point of having convex relaxations if the ReLU networks can already be trained using convex optimization?\n\n* Can authors also briefly comment on the issues raised in the weaknesses part?\n\n[1] Pilanci and Ergen, Neural Networks are Convex Regularizers: Exact Polynomial-time Convex Optimization Formulations for Two-layer Networks, ICML 2020"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7513/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7513/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7513/Reviewer_y6LY"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7513/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698443367858,
            "cdate": 1698443367858,
            "tmdate": 1700588065010,
            "mdate": 1700588065010,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4o6vEcPBVY",
                "forum": "awHTL3Hpto",
                "replyto": "FBYcztHQI7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7513/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7513/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer y6LY Part I"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their review, high-quality summary, and great outline of the strengths of our paper. However, we have identified some misconceptions and factual errors in the listed weaknesses which we address below while answering the remaining questions.\n\n**Q1: Can you discuss why you focus exclusively on ReLU networks instead of considering sigmoid or tanh activations?**  \nWe decided to focus on ReLU networks for two key reasons. \nFirstly, they are much more widespread and thus relevant in the certified robustness community, e.g., the international verification of neural networks competition last year considered ReLU activations in all 12/12 benchmark classes, sigmoids in only one, and tanh in none at all [1].\nSecondly, finite sigmoid and tanh networks can not exactly encode (to the best of our knowledge) any commonly used function class, making the question of exact analysis less relevant and interesting. Given the limited space and already \u201cdense and mathematical\u201d writing of our work, we decided to focus on the most relevant class of networks.\n\n**Q2: You seem to only consider fully connected feedforward networks. Can you discuss the extensibility of your results to other architectures?**  \nWe already consider arbitrary ReLU networks with any architecture including arbitrary skip connections, which include both CNNs and ResNets (see the first paragraph in Section 2 and Theorem 17). We have made this more clear.\n\n**Q3: Why do you only consider deterministic neural networks and not stochastic networks?**  \nAs our work is motivated by (deterministic) neural network verification (where all state-of-the-art methods are based on convex relaxations), we focus on the general type of network typically considered in the field, i.e., deterministic networks. In particular, we are not aware of any such method considering stochastic networks. Perhaps the reviewer can point to a specific class of stochastic networks that would be relevant in this setting.\n\n**Q4: You only seem to study standardized datasets and perturbation sets. Can you discuss to what extent your results generalize beyond that?**  \nAs we do not study any specific data(set) anywhere in the paper but argue about general function classes, our results will generalize to any applicable dataset.\nFor univariate functions, we show a positive result for the general case of continuous bounded perturbation sets, subsuming all $\\ell_p$-norm bounded perturbations, typically studied in the field. In the multivariate case, we show a negative result even for the more restrictive (but very commonly used) $\\ell_\\infty$-norm bounded perturbations. That is, even extremely simple functions can not be expressed for box input regions. This entails that these functions can also not be expressed for more general classes of perturbation sets. We note that Theorem 19 and thus 20 can be trivially extended to other $\\ell_p$-norms and have added a corresponding Theorem (Thm. 24) to Appendix A.\n\n**Q5: While hypotheses are provided for certified training, no experiments are conducted to validate the conjectured benefits.**  \nFirst, we want to highlight that our general results agree well with multiple recent empirical works as discussed in \u201cImplications of our Results for Certified Training\u201d (Section 1). \nWhile validating our conjecture of more precise relaxations being beneficial for certified training is an interesting item for future work, it is beyond the scope of this theoretical work, as no current certified training method supports more precise relaxations, as current approaches are non-differentiable.\n\n**Q6: The writing seems to be quite dense and mathematical - how accessible is it to a general AI audience?**  \nAs work on certified adversarial robustness [2,3,4], in general, and closely related prior theoretical work [5,6], specifically, is frequently published at general AI venues (including ICLR), we believe our work to also be accessible to the audience of these venues, given that multiple reviewers highlighted the good presentation. \n\n**References:**  \n[1] M\u00fcller et al. \"The third international verification of neural networks competition (VNN-COMP 2022): summary and results.\" arXiv 2022  \n[2] Xu et al. \u201cFast and Complete: Enabling Complete Neural Network Verification with Rapid and Massively Parallel Incomplete Verifiers\u201d ICLR 2021 \n[3] Ferrari et al. \"Complete verification via multi-neuron relaxation guided branch-and-bound.\" ICLR 2022  \n[4] M\u00fcller et al. \u201cCertified training: Small boxes are all you need\u201d ICLR 2023  \n[5] Baader et al. \u201cUniversal approximation with certified networks\u201d, ICLR 2020  \n[6] Mirman et al. \u201cThe fundamental limits of neural networks for interval certified robustness\u201d, TMLR 2022  \n[7] Pilanci and Ergen, Neural Networks are Convex Regularizers: Exact Polynomial-time Convex Optimization Formulations for Two-layer Networks, ICML 2020"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7513/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699963365305,
                "cdate": 1699963365305,
                "tmdate": 1699963365305,
                "mdate": 1699963365305,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tEdlxofmAA",
                "forum": "awHTL3Hpto",
                "replyto": "FBYcztHQI7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7513/Reviewer_y6LY"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7513/Reviewer_y6LY"
                ],
                "content": {
                    "title": {
                        "value": "Response to rebuttal"
                    },
                    "comment": {
                        "value": "I'd like to thank the authors for their detailed responses. The rebuttal clarified some of my concerns therefore I increase my score."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7513/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700588025128,
                "cdate": 1700588025128,
                "tmdate": 1700588041949,
                "mdate": 1700588041949,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wZuy1jbGnJ",
            "forum": "awHTL3Hpto",
            "replyto": "awHTL3Hpto",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7513/Reviewer_FMX3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7513/Reviewer_FMX3"
            ],
            "content": {
                "summary": {
                    "value": "The authors presented an analysis for special classes of convex relaxations of neural networks and their expressivity. Unfortunately, given that I am not an expert with the subject, and that the background in the paper is limited, I have trouble understanding the high level ideas of the paper. I would encourage the authors to engage with discussion, so that I can provide a proper review of this work."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "I will re-evaluate the strengths after a discussion with the authors to understand the paper."
                },
                "weaknesses": {
                    "value": "Similar to strengths, I will re-evaluate after discussion."
                },
                "questions": {
                    "value": "As mentioned earlier, I have many basic questions about this work. Perhaps let me start from the basics about the background of this work. \n\n1. Can the authors elaborate on how convex relaxations are helping with formal robustness guarantees? This was quickly glossed over, but I would hope to get a better explanation. \n\n2. Can the authors explain exactly what is being relaxed into a convex function? Section 2.1 reads quite confusingly for me, as all I can discern are inequalities, and I do not see which parts are being relaxed. \n\n3. When the authors write a vector is less than or equal to another vector, is this inequality entrywise? \n\n4. There are a lot of definitions in section 2.2, can the authors provide a simpler explanation of these definitions and what they are trying to capture? In particular, why is it that we care about $D$-analysis in the definition of expressivity? I would have expected expressivity of a network architecture to be about function approximation. \n\n5. At the end, it seems like the authors are investigating whether or not ReLU networks can express certain functions. I thought this was already an answered question with respect to universal approximation, but perhaps I am missing something. Can the authors explain why we need to analyze the expressivity results for these certain class of functions? \n\nPerhaps let's start here. Once we go into discussion and have a better understanding, I can follow up with more questions regarding the actual technical contributions of this work."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7513/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7513/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7513/Reviewer_FMX3"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7513/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699564113080,
            "cdate": 1699564113080,
            "tmdate": 1700498756075,
            "mdate": 1700498756075,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UlBLLazbTX",
                "forum": "awHTL3Hpto",
                "replyto": "wZuy1jbGnJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7513/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7513/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer FMX3"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their candid admission that they are not an expert in the field and are happy to provide a significantly extended version of the background, which we believe addresses all the reviewer's questions. We have incorporated some of these extensions (including a better intuition for Definition 3) into Section 2 in an effort to make our work more accessible.  \n\n## Extended Background  \n**Problem Setting**  \nFor clarity of exposition, we focus on the most common case of adversarially robust classification. \nWe call a classifier $h: \\mathcal{X} \\to \\mathcal{Y}$ locally robust around some input $x$ if it predicts the same, correct class for all similar inputs $\\mathcal{B}^\\epsilon_p(x) := \\\\{ x' \\in \\mathcal{X} \\mid \\| x - x' \\|_p \\leq \\epsilon \\\\}$. Thus, to prove that a classifier is locally robust, we have to show  $\\forall x' \\in \\mathcal{B},  h(x') = h(x) = y$.\nFor a neural network that predicts the class $ h(x) \\:= argmax_i f(x)_i $, we can equivalently show that the logit of the target class is always greater than that of all other classes, by solving the following optimization problem:  \n\n$0 < \\min_{x' \\in \\mathcal{B},  i \\neq y}  f(x')_y - f(x')_i $  \n\n**Convex Relaxations for Efficient Solution**  \nUnfortunately, solving this problem exactly is NP-complete [1] as the neural network $f$ is highly non-linear and non-convex. Thus, state-of-the-art neural network verifiers use convex relaxations (combined with a branch-and-bound approach), to relax the above non-convex optimization problem to a (convex) linearly constrained linear program which can be solved much more efficiently [2]. More concretely, the non-linear activation layers (ReLUs), which can be seen as non-linear constraints in the original problem, are replaced with a set of linear constraints (the element-wise inequalities) in their input-output space. Intuitively, we relax the non-convex graph of the ReLU function with the convex over-approximations illustrated in Figures 1-4. However, as these convex-relaxations introduce an (over)-approximation error, these relaxed problems might be too imprecise to prove robustness even when the network is robust. In this work, we study for which classes of functions there exist ReLU networks such that this analysis does not lose precision depending on the relaxation method used.\n\n**Relevance of a $D$-Analysis**  \nSolving the above optimization problem can geometrically be interpreted as linearly projecting the graph of the network (set of input-output tuples) for inputs in $\\mathcal{B}$ to the 1d-line $f_y - f_i$ corresponding to the output difference for every $i \\neq y$. While this projection is computationally efficient, computing the exact graph is still NP-complete. Thus, we can instead use convex relaxations to compute and then project (often in one step) an over-approximation of this graph and then check whether all points in this over-approximation are classified correctly. We call this over-approximate graph the $D$-analysis depending on the convex relaxation $D$ that was used. As the analysis with relaxation $D$ is precise if and only if the 1d projection of the exact graph and the $D$-analysis agree, this is a very natural and intuitive perspective on the problem.\n\nWe can now reformulate our above question as \u201cFor which function classes exist ReLU networks such that the 1d projection of their $D$-analysis (over-approximation of the graph) is precise, i.e., agrees with the projection of the exact graph?\u201d. For \u201cexpressivity under convex relaxation\u201d we thus do not only require that a given function can be encoded by a network (the default notion of expressivity and already shown for CPWL functions and ReLU networks by prior work [3]) but additionally require the analysis of the resulting network using the $D$ relaxation to be precise. Thus, $D$-expressivity depends on both the relaxation $D$ and the function class $F$.\n\nFor a more in-depth explanation of (certified) adversarial robustness including examples and visualizations, we recommend the excellent blog post by Eric Wong: https://locuslab.github.io/2019-03-12-provable/\n\n**Notation**  \nAll inequalities between vectors are element-wise. We have added a corresponding note to the Notation paragraph. Thanks for the pointer.\n\nWe are looking forward to the reviewer\u2019s reply and are happy to provide further explanations or references and answer any follow-up questions.\n\n**References:**  \n[1] Katz et al. \"Reluplex: An efficient SMT solver for verifying deep neural networks.\" CAV 2017  \n[2] Salman, et al. \"A convex relaxation barrier to tight robustness verification of neural networks.\" NeurIPS 2019  \n[3] He, et al. \"ReLU deep neural networks and linear finite elements.\" JCM 2020"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7513/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699963229388,
                "cdate": 1699963229388,
                "tmdate": 1699963229388,
                "mdate": 1699963229388,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "n5NRhuhP1I",
                "forum": "awHTL3Hpto",
                "replyto": "UlBLLazbTX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7513/Reviewer_FMX3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7513/Reviewer_FMX3"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "I want to start by thanking the authors for the detailed reply and the updated draft. I believe the context is a lot clearer to me now, and I think I understand the problem of interest. At this point, I do think the results are interesting, but I would like clarify a few more things, with the current work. \n\nFirstly, I want to understand the negative result in two dimensions a bit better. Is the issue of expressivity a problem with the convex relaxation, or is it something more basic about ReLU networks? What I mean is, can f(x,y) = max(x,y) be expressed by a finite sized ReLU network at all? \n\nIf the answer is no, then in some sense, the convex relaxation of this problem isn't very interesting, since in this case the definition of \"precise\" is clearly too strong. I think a better approach would be studying the $\\epsilon$-precise version of this expressivity problem. \n\nSecondly, I want to understand the impact on training actual networks in practice better. If I was a practitioner who only cares about training adversarially robust networks, can the authors explain how I might have to modify my training methods given a potential future result from this line of work?"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7513/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700077631457,
                "cdate": 1700077631457,
                "tmdate": 1700077631457,
                "mdate": 1700077631457,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OdWMikwq9e",
                "forum": "awHTL3Hpto",
                "replyto": "2lKn6xlCm5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7513/Reviewer_FMX3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7513/Reviewer_FMX3"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thanks for the detailed reply. I think I understand the problem and contributions much better now. I will raise my score to accept. \n\nOn the other hand, while I cannot make the other reviewers engage and raise their scores, I hope the discussion has helped the authors improve the draft."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7513/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700498738326,
                "cdate": 1700498738326,
                "tmdate": 1700498738326,
                "mdate": 1700498738326,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]