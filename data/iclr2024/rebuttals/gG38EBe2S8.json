[
    {
        "title": "IMPUS: Image Morphing with Perceptually-Uniform Sampling Using Diffusion Models"
    },
    {
        "review": {
            "id": "Vsy5b91FNP",
            "forum": "gG38EBe2S8",
            "replyto": "gG38EBe2S8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1019/Reviewer_UYJ7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1019/Reviewer_UYJ7"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a diffusion-based image morphing approach aiming to produce smooth, direct, and realistic interpolations given an image pair.  The authors interpolate in the locally linear and continuous text embedding space and Gaussian latent space. An \"adaptive\" bottleneck constraint based on a relative perceptual path diversity score that automatically controls the bottleneck size and balances the diversity along the path with its directness. Experiments verify the effectiveness to some extent. Though the method is clearly described, there exist some issues with both motivations and experiments."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* Image morphing in computer graphics is useful. Image morphing of nature images seems visually interesting to me.\n* The method of image morphing with perceptually-uniform sampling using diffusion models is clearly described.\n* The paper is OK to read."
                },
                "weaknesses": {
                    "value": "* The motivation for image morphing of conceptually different nature images is not clear to me. Why should we study the image morphing of natural images without any human involvement? To my understanding, it may be reasonable to morph images for a certain purpose, e.g., continuously changing the poses or colors, via human involvement. However, how can we guarantee the generated images fit our purpose without any human involvement?\n* The perceptually-uniform sampling is not reasonable to me. If I am right, morphing should be that given a uniform sequence in time-space, e.g., [0,1], and generating images \"uniformly\" changing between start and end images. In perceptually-uniform sampling, can we obtain the images corresponding to a given middle time?\n* The authors argue \"adaptive\" bottleneck constraint. However, I can not see how the rank is adaptively learned.\n* More discussion on the perceptual path diversity is encouraged. For example, why should we use this to measure the diversity? Why use the formulation in Eq. (8)? Does large $\\gamma$ imply higher diversity?\n* It would be better to provide an algorithm for both fine-tuning and inference.\n* To show the superiority of the proposed method, more baseline approaches should be compared in experiments, such as the image morphing approaches in the related works."
                },
                "questions": {
                    "value": "* In Eq. (5), $\\nu$ is a distribution, why it should belong to the data manifold $\\mathcal{M}$ which should be the set of samples?\n* The embedding space of CLIP is locally linear. Since the two semantically distinct text embeddings are not in the global linear space, does it contradict the linear interpolation between the optimized embedding spaces?\n* What is the logic for \"the diffusion process between $\\bf x_0$ and ${\\bf x}_T$ can be seen as an optimal transport mapping, therefore, we apply spherical linear interpolation\"?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1019/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1019/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1019/Reviewer_UYJ7"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1019/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698498523251,
            "cdate": 1698498523251,
            "tmdate": 1700751819921,
            "mdate": 1700751819921,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "c3jQspqnN7",
                "forum": "gG38EBe2S8",
                "replyto": "Vsy5b91FNP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1019/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1019/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer UYJ7 - Part 1/2"
                    },
                    "comment": {
                        "value": "Thanks for your constructive feedbacks. Following are some responses to your questions. \n\n>**W1: The motivation for image morphing of conceptually different nature images, Why should we study the image morphing of natural images without any human involvement? It may be reasonable to morph images for a certain purpose, e.g., continuously changing the poses or colors, via human involvement. However, how can we guarantee the generated images fit our purpose without any human involvement?** \n\nThank you for your thought-provoking question. There is actually one *crucial element of human involvement*, namely the **selection of the endpoint images**. Direct, smooth and realistic image morphing can be used for different downstream purposes depending on the choice of selected images. \n\nIf the selected endpoints directly reflect a certain factor variation, e.g., an object attribute, pose or style, then our method will provide a sequence where this factor is varied continuously, since other variations would hinder the directness of the path. This can be highly beneficial for numerous applications. For example, in controllable generation, as shown in Fig.6 of the revised appendix, we integrate our method with an existing image editing pipeline [1], where our method generates a gradual transition between the original image and its edited version according to certain prompt. With our proposed model adaptation strategy, the transition can reveal a direct & high-fidelity variation that aligns with the editing direction. Thus we believe **our work can also contribute to controllable generation applications in terms of enabling users to control the magnitude of editing or translation**.\n\nMorphing two conceptually different images also has many important usages: \n* **Novel content creation:** Using image morphing techniques to create novel content has been studied for many years [2]. For large-scale generative models, one of the important purposes is to achieve creation of customized novel content that is difficult to see in real world or would require specialized and extensive human involvement [3]. There are two major solutions to achieve novel content generation: a) to design a compositional generative model [4], i.e., combining visual concepts or attributes that exist in a training dataset to generate novel examples, which has been studied by numerous existing works works (e.g., [4,5,6]); and b) to design an interpolative generative model, e.g., the example of interpolating a \"lion\" and an \"eagle\" in our work. The former can exponentially expand the control space as more concepts are learned by the generative model, while the latter can potentially provide infinite use of learned concepts. Thus we believe that morphing between different concepts is a highly useful task, and our work is the one of the first attempts.\n* **Data augmentation:** As our proposed method can generate **perceptually direct transitions** between endpoint images, for a given dataset, the interpolated samples with our method could be considered as in-distribution images, which can be used as a powerful tool to achieve **semantically meaningful and diverse data augmentation**.\n* **Model robustness analysis:** Our work could also be used to generate conceptually-ambiguous samples (e.g., the morph between a \"cat\" and a \"dog\") to study the robustness and uncertainty awareness of computer vision models."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1019/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700205608067,
                "cdate": 1700205608067,
                "tmdate": 1700411533182,
                "mdate": 1700411533182,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gQLhpPbF4g",
                "forum": "gG38EBe2S8",
                "replyto": "Vsy5b91FNP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1019/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1019/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer UYJ7 - Part 2/2"
                    },
                    "comment": {
                        "value": ">**W2: The perceptually-uniform sampling is not reasonable to me. If I am right, morphing should be that given a uniform sequence in time-space, e.g., [0,1], and generating images \"uniformly\" changing between start and end images. In perceptually-uniform sampling, can we obtain the images corresponding to a given middle time?**\n\nPerceptually-uniform sampling just finds a continuous mapping from $[0,1] \\to [0,1]$ such that the perceptual gap between images $t$ and $t+\\delta$ is roughly the same as that of images $t+\\delta$ and $t+2\\delta$. A middle time (0.5) is still meaningfully defined, and an image can be generated at this time.\nIn this context, time-uniformity would only be more meaningful than perceptual-uniformity if the underlying space was uniform in some well-defined way.\n\nWe provide feedbacks based on our understanding of this question, and more discussion is encouraged.\n\n>**W3: The authors argue \"adaptive\" bottleneck constraint. However, I can not see how the rank is adaptively learned.**\n\nThe rank is adaptively calculated based on the input image pair.\n\n>**W4: More discussion on the perceptual path diversity is encouraged.**\n\nPlease refer to general comment 3.\n\n>**W5: Provide an algorithm for both fine-tuning and inference.**\n\nThanks for the suggestion. Please refer to Algorithm 1 in the revised appendix.\n\n>**W6: More baselines.**\n\nPlease refer to general comment 1 for additional comparison study.\n\n>**Q1: In Eq. (5), $v$ is a distribution, why it should belong to the data manifold $\\mathcal{M}$ which should be the set of samples.**\n\nThanks for pointing out the typo. We define $\\nu$ as probability measure on manifold $\\mathcal{M}$. \n\n>**Q2: Since the two semantically distinct text embeddings are not in the global linear space, does it contradict the linear interpolation between the optimized embedding spaces?**\n\nThanks for your concern. We avoid this problem by our textual inversion method. Firstly, we set a common prompt **<An image of [token]>** for the inputs no matter their semantic gap, where for semantically distinct pairs, [token] is either a common root class (e.g., \"animal\") or a concatenation of the pair (e.g., \"poker man\"). Secondly, we optimize the initial prompt embeddings w.r.t. to the input images. Since two prompt embeddings are initialized with the same common prompt, they stay closed after optimization. We find that this simple method can avoid the nonlinear \\& discontinuous issue in CLIP embedding space to some extent, where the usage of a common prompt is a crucial step which is illustrated in Fig.7 of the revised appendix. \n\n>**Q3: What is the logic for \"the diffusion process between $x_0$ and $x_T$ can be seen as an optimal transport mapping, therefore, we apply spherical linear interpolation\"?**\n\nPlease refer to general comment 2.\n\n**References:**\n\n[1] Tim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 18392\u201318402, 2023.\n\n[2] Wolberg, George. \"Image morphing: a survey.\" The visual computer 14.8-9 (1998): 360-372.\n\n[3] Nichol, Alex, et al. \"Glide: Towards photorealistic image generation and editing with text-guided diffusion models.\" arXiv preprint arXiv:2112.10741 (2021).\n\n[4] Liu, Nan, et al. \"Compositional visual generation with composable diffusion models.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022.\n\n[5] Kumari, Nupur, et al. \"Multi-concept customization of text-to-image diffusion.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\n[6] Nie, Weili, Arash Vahdat, and Anima Anandkumar. \"Controllable and compositional generation with latent-space energy-based models.\" Advances in Neural Information Processing Systems 34 (2021): 13497-13510."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1019/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700206456010,
                "cdate": 1700206456010,
                "tmdate": 1700659090305,
                "mdate": 1700659090305,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ElFlqVQuyR",
                "forum": "gG38EBe2S8",
                "replyto": "Vsy5b91FNP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1019/Reviewer_UYJ7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1019/Reviewer_UYJ7"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for the response. Some of my concerns are addressed but others are not. \n\nRegarding the usefulness of natural image morphing, I am still not convinced that natural image morphing is important. I do believe that image morphing is useful in some applications. For example, in computer graphics, morphing between two facial expressions of one person is useful, which requires preserving the identity of the person. However, the importance of morphing between two natural images given only the start and end images is not clear. [1] and [3-6] studies controllable image generation or image editing rather than image morphing. \"Novel content creation\", \"data augmentation\", and \"model robustness analysis\" mainly increase the diversity of datasets, for which image morphing may not be appropriate. Because there seems to be no need for directness. I suggest that the authors focus on the morphing of certain images instead of arbitrary natural images.\n\nRegarding the adaptiveness of the rank, the paper empirically sets it using an empirically designed function. I would not say that it is adaptive because why to choose this formulation and how to set the hyper-parameters in the function are not clear.\n\nMeanwhile, a probability measure $\\nu$ on $\\mathcal{M}$ is function of a subset of $\\mathcal{M}$, i.e., $\\nu:\\sigma(\\mathcal{M})\\rightarrow R$, where $\\sigma(\\mathcal{M})$ a set of subsets of $\\mathcal{M}$ (called $\\sigma$-algebra)."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1019/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700640001158,
                "cdate": 1700640001158,
                "tmdate": 1700640303478,
                "mdate": 1700640303478,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cxtLY8frDj",
                "forum": "gG38EBe2S8",
                "replyto": "Vsy5b91FNP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1019/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1019/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your valuable suggestions! Following are responses to your concerns.\n\n>**The importance of morphing between two natural images given only the start and end images is not clear. I suggest that the authors focus on the morphing of certain images instead of arbitrary natural images**\n\nWe would like to note that **morphing between arbitrary image pairs** is not the main focus of our work. Instead, we assume that **the selected endpoint images should have certain perceptual connection for the interpolated images between them to be meaningful**, which is reflected from our sample selection strategy (as shown in sec 5.3 Appendix). Specifically,\n* For inter-class image pairs, we select benchmark images according to some LPIPS threshold. \n* For real-world images, the manually selected samples also have a certain perceptual connection. \n* In Fig.10 of the revised appendix, we provide a counterexample, showing that morphs between a pair of perceptually unrelated images can be meaningless.\n* Given the perceptual connection, we've also shown some examples where the morphs can be meaningful in novel content creation for different visual concepts, e.g., the morph between 'beetle' and 'car'. \n* Meanwhile, morphing between images belong to different categories has been studied for decades where morphing between human faces and animal faces are commonly applied image pairs. The second animation from [1] also shows an example of morphing between an ape and a bird. There are numerous morphing applications in entertainment industry, digital art, social media, etc.\n\nThus we believe the perceptual connection between endpoint images (either decided by LPIPS threshold or users) can be a meaningful constraint to relate the endpoint images, which is a relaxation of traditional methods which use corresponding points to relate two images.\n\n>**[1] and [3-6] studies controllable image generation or image editing rather than image morphing.**\n\n[1] and [3-6] indeed deal with controllable generation or image editing.  We find that our image morphing method can be integrated with these methods to further strengthen their controllability. As shown in Fig.6 Appendix, given an image editing pipeline, our method can be used to allow users to control magnitude of editing strength, which is a desirable property in controllable generation and has been studied by previous works, e.g., [2].\n\n>**\"Novel content creation\", \"data augmentation\", and \"model robustness analysis\" mainly increase the diversity of datasets, for which image morphing may not be appropriate. Because there seems to be no need for directness.**\n\n* For novel content creation, directness can lead to higher level controllability, i.e., the user can control the morphing scale to generate the desired content based on selected end points while avoiding unexpected results. A direct morph also more aligns with human preference.\n* For data augmentation and model robustness analysis, given limited computational and labor resources, the expectation would be that diversity and relevance are both important. Lets consider a classification problem where majority of data stay away from the decision boundary while minority of data stay closed to the decision boundary. Creating a larger and more diverse dataset could generate more samples near the decision boundary to enhance the model robustness, but the cost is large in terms of computation and labelling. With our morphing method, we are able to generate more samples near the decision boundary with less cost because of directness.\n\n>**Regarding the adaptiveness of the rank, the paper empirically sets it using an empirically designed function. I would not say that it is adaptive because why to choose this formulation and how to set the hyper-parameters in the function are not clear.**\n\n\nThanks for your suggestion, the rank selection equation is a heuristic we inferred from real-world data, where we find that the desired rank can vary from different image pairs. Empirically, we find that the proposed heuristic can provide an effective trade-off between directness (usually benefit from larger rank) and fidelity (usually benefit from smaller rank) for these samples. \n\n\nWe acknowledge that this is heuristic. To avoid confusion, we are updating the term \u2018adaptive strategy\u2019 to \u2018heuristic strategy\u2019 in the main text. We will further investigate into the rank selection strategy and an interpretation of this problem in our future work.\n\n>**Probability measure on a manifold**\n\nThank you for the correction, we have rectified this in the main text and response.\n\nThank you once again for your valuable feedback. We hope the above responses have addressed your concerns. \n \n**References**\n\n[1] Wikipedia contributors. \"Morphing.\" https://en.wikipedia.org/wiki/Morphing\n\n[2] Kwon, Mingi, Jaeseok Jeong, and Youngjung Uh. \"Diffusion Models Already Have A Semantic Latent Space.\" The Eleventh International Conference on Learning Representations. 2022."
                    },
                    "title": {
                        "value": "Follow Up Response to Reviewer UYJ7"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1019/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700659683200,
                "cdate": 1700659683200,
                "tmdate": 1700685952781,
                "mdate": 1700685952781,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FIqFF34H6R",
            "forum": "gG38EBe2S8",
            "replyto": "gG38EBe2S8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1019/Reviewer_meGk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1019/Reviewer_meGk"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents an image morphing approach using pre-trained text-to-image diffusion models. The key idea is to interpolate text embeddings and latent states of two images following low-rank adaptation of model weights. The intuition is that the fine-tuned model exposes a more direct morphing path, yielding more perceptually convincing interpolations. Qualitative and quantitative experiments are performed to assess several key aspects of the proposed method. Overall, the method achieves superior morphing results compared to baselines and further supports interesting applications."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The method makes use of a pre-trained text-to-image diffusion model for image morphing. The data prior captured by the diffusion model naturally constrains the morphing path on the image manifold, allowing perceptually convincing interpolations.\n\n- The paper provides a helpful discussion on the tradeoffs in model adaptation. It discovers that LoRA creates an effective bottleneck to reduce overfitting, and further devises a simple strategy to calculate LoRA ranks based on sample diversity along the morphing path.\n\n- The paper introduces an effective approach for perceptually uniform sampling. It is based on adaptive step size selection and yields smooth and visually pleasing morphing results. \n\n- Several metrics are defined to evaluate different aspects of image morphing, namely smoothness, realism and directness. The method outperforms baselines in qualitative and quantitative experiments, and shows potential for several interesting applications."
                },
                "weaknesses": {
                    "value": "- Despite the impressive results, there are some vague claims between the lines which need further clarification.\n\na) Equation 5 formulates image morphing as walking on the geodesic path in the 2-Wasserstein sense. However, the actual implementation amounts to linear interpolation of CLIP text embeddings, which in no way reflects the theoretical formulation.\n\nb) For the interpolation of latent states, the paper cites an early work saying x0 and xT forms an optimal transport mapping. How does this justify the slerp interpolation between xT(0) and xT(1)?\n\n- The method only compares to Wang and Golland (2023), another diffusion-based method for image morphing, yet there are numerous methods that are not based on diffusion (or even not learning-based). I am curious about how the method compares to, for example, frequency domain morphing methods, and what are their typical failure modes."
                },
                "questions": {
                    "value": "- It would be helpful if the authors can comment on the robustness of their method relative to the hyper-parameters in model adaptation and sampling.\n\n- Most of the image pairs used for experiments seem to have certain degree of spatial alignment. It would be helpful to know the rule of thumb for the selection of these image pairs."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1019/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698717105764,
            "cdate": 1698717105764,
            "tmdate": 1699636027806,
            "mdate": 1699636027806,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "a9Ns0VCiB8",
                "forum": "gG38EBe2S8",
                "replyto": "FIqFF34H6R",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1019/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1019/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer meGK regarding theoritical formulation, slerp, comparison, robustness and endpoint image selection."
                    },
                    "comment": {
                        "value": "Thanks for your concerns and instructive feedbacks. Following are some responses to your questions and concerns. \n\n>**W1: Equation 5 formulates image morphing as walking on the geodesic path in the 2-Wasserstein sense. However, the actual implementation amounts to linear interpolation of CLIP text embeddings, which in no way reflects the theoretical formulation.**\n\nEquation 5 is formulated following the previous work [1], which is used to reflect the desiderata of image morphing, i.e., the Wasserstein Barycenter problem leads to smoothness and directness of the transition between two images, while the constraint that the morphs should stay on a target data manifold leads to the realism of the morphs. However, it is extremely challenging to apply such formulation to high dimensional real world image data due to computational complexity as well as modeling of manifold; thus, previous work [1] restricts themselves to image data with simple background. We instead utilize alternative, tractable methods, without losing track of the desirable characteristics suggested by the theoretical problem. That is, based on the local linearity of the CLIP embedding space, we perform textual inversion and linear interpolation between inverted text embeddings and latent states which ensures realism and smoothness of interpolation. Our model adaptation strategy then enhances the directness of the morphing process while largely preserve the manifold geometry. Thus, although Equation 5 is not directly solved, we find that our work addresses the same criteria in a more tractable way.\n\n>**W2: The paper cites an early work saying x0 and xT forms an optimal transport mapping. How does this justify the slerp interpolation between xT(0) and xT(1)?**\n\nThanks for your concern. Please refer to the general comment 2.\n\n>**W3: How the method compares to non-diffusion methods?**\n\nPlease refer to the general comment 1.\n\n>**Q1: Robustness of method relative to the hyper-parameters in model adaptation and sampling.**\n\nFigure 4 of the main paper shows results of different LoRA ranks. In general, our method achieves reasonable performance across different LoRA ranks, while some ranks generate more visual appealing results than other ranks. If two samples are significantly different in visual appearance, model adaptation with large rank may lead to blurry intermediate samples.\n\nIn terms of conditional guidance scales, we discover that, given fixed LPIPS difference for perceptually-uniform sampling, larger guidance scales are likely to generate longer sequences (less direct morphs). The reconstruction quality also decreases for large guidance scales, but overall the differences are not visually significant. We show an example of sequence generated from different guidance scale in Figure 14 of the revised appendix. Detailed results also report within tables in our appendix.\n\n>**Q2: The rule of thumb for the selection of these image pairs**\n\nThanks for your question. Image pairs selected in our works are collected from three sources: (1) Example image pairs provided in Wang and Golland 2023; (2) Additional images that are manually collected from Internet; (3) Benchmark data for computer vision research. In terms of benchmark evaluation, we select the image pairs based on a simple protocol, i.e., for a given benchmark dataset, we randomly select image pairs whose similarity is under a certain LPIPS threshold, for details, please refer to sec 5.3 in the revised appendix. The intuition behind this process is that, the endpoint images should have certain perceptual connection for the interpolated images between them to be meaningful. In Fig.10 of the revised appendix, we provide a counterexample, showing that morphs between a pair of perceptually unrelated images can be meaningless and benchmark performances on these image pairs are not reliable.\n\n**References:**\n\n[1] Dror Simon and Aviad Aberdam. Barycenters of natural images constrained wasserstein barycenters for image morphing. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 7910\u20137919, 2020."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1019/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700204518492,
                "cdate": 1700204518492,
                "tmdate": 1700660517128,
                "mdate": 1700660517128,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vaosWWdiEq",
                "forum": "gG38EBe2S8",
                "replyto": "a9Ns0VCiB8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1019/Reviewer_meGk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1019/Reviewer_meGk"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for addressing my comments. Overall, the paper looks good to me, and I would like to keep my rating."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1019/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700709230869,
                "cdate": 1700709230869,
                "tmdate": 1700709230869,
                "mdate": 1700709230869,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jCXVci6X5y",
                "forum": "gG38EBe2S8",
                "replyto": "FIqFF34H6R",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1019/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1019/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your efforts"
                    },
                    "comment": {
                        "value": "Thank you for your positive feedbacks and your valuable effort in reviewing our paper! Your comments helped clarify and improve our work."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1019/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700710628969,
                "cdate": 1700710628969,
                "tmdate": 1700710650188,
                "mdate": 1700710650188,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qwgNd0jkzq",
            "forum": "gG38EBe2S8",
            "replyto": "gG38EBe2S8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1019/Reviewer_zxkV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1019/Reviewer_zxkV"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes IMPUS, a diffusion-based image morphing method that can synthesize smooth and realistic transitions between two images. It employs a latent diffusion model with distinct conditional distributions for each image and interpolates in the locally linear and continuous text embedding space and Gaussian latent space. The method employs an adaptive bottleneck constraint based on a novel relative perceptual path diversity score to yield a direct path and suppresses ghosting artifacts in the interpolated images. Extensive experiments confirm IMPUS's effectiveness in image morphing and its potential for other image generation tasks."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The idea of applying diffusion models on image morphing task is interesting and promising;\nThe proposed evaluation criterion Smoothness,  Realism and Directness are reasonable and effective;\nThe presented techniques are effective and the overall method is able to generate high qulity results;\nAblation studies are thorough and abundant, which fully reveals the function and influence of each components of the model"
                },
                "weaknesses": {
                    "value": "In the comparison section, the proposed method is only compared with one previous method, which seems to be not comprehensive.  Also, there's only qualitative comparison, while quantitative criterion and user study would better convey the difference.\nIn the realism criteria and section3, the authors discussed about manifold and distribution several times, but gave no illustration or discussion in the experiment section."
                },
                "questions": {
                    "value": "Why is data manifold and data distribution discuessed in methodologies but not covered in experiments? Would it be possible to visualize the data manifold or distribution with figures or give quantitative experiment resutls?\nWhy is the proposed method compared with only one existing diffusion based image interpolation method? Are there other similar works, can they be compared in fair experiment settings and criteria?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1019/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1019/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1019/Reviewer_zxkV"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1019/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698807703486,
            "cdate": 1698807703486,
            "tmdate": 1699636027733,
            "mdate": 1699636027733,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "AqIkaGxYSg",
                "forum": "gG38EBe2S8",
                "replyto": "qwgNd0jkzq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1019/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1019/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer zxkV regarding comparison study, quantitative analysis and data manifold"
                    },
                    "comment": {
                        "value": "Thanks for the valuable suggestions and feedbacks. Following are some responses for your questions.\n\n>**W1: Only compared with one previous method, which seems to be not comprehensive**\n\nPlease refer to general comment 1 for additional comparison with previous methods. Existing automatic morphing tools either create blurred and unrealistic intermediate images or cannot generalize to unseen data, while our work can generate smooth, direct and photorealistic morphs on real world images with large gaps.\n\n>**W2: There\u2019s only qualitative comparison, while quantitative criterion and user study would better\nconvey the difference**\n\nWe thank the reviewer for the suggestion. We provide some quantitative comparison with Wang and Golland 2023 in Table 1 of the main paper (with their metrics and hyperparameters). We will defer the user study to future work.\n\n\n>**Q1: Data manifold and data distribution discussed in methodologies but not covered in experiments**\n\nWe discuss the data manifold in the background section since many of the existing morphing works in the literature, which we mention in related work and background, are based upon ideas from manifold geometry and optimal transport to achieve smooth and realistic morphs. However, modeling manifold of high dimensional real world images are challenging, thus, these works restrict themselves to relatively simple image data. Some metrics reported in these works also not applicable for high dimensional image data, therefore, we propose some new metrics. The evaluation metrics proposed in paper are inspired by these works; thus, we mention these concepts to provide some intuition as to the selection of these metrics.\n\nIn addition, the design of our method is heavily inspired by the hypothesis that high dimensional image data lie on low dimension manifolds. As pointed out by reviewer meGk, the data prior captured by the pre-trained diffusion model naturally constrains the morphing path on the image manifold and we apply low rank adaptation instead of standard finetuning to preserve the manifold learnt by the pre-trained diffusion model.\n\nTo the best of our knowledge, it is challenging to obtain meaningful quantitative measurements related to the manifold or distribution of high dimensional images, so we defer this to future work.\n\n>**Q2: Why is the proposed method compared with only one existing diffusion based image interpolation method? Are there other similar works, can they be compared in fair experiment settings and criteria?**\n\nWe note that the compared method is only existing diffusion-based work that is relavent to our targeted task. We provide additional comparison with some non-diffusion methods as illustrated in general comment 1."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1019/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700203319283,
                "cdate": 1700203319283,
                "tmdate": 1700411356475,
                "mdate": 1700411356475,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "2DQUGdGuSo",
            "forum": "gG38EBe2S8",
            "replyto": "gG38EBe2S8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1019/Reviewer_NAVh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1019/Reviewer_NAVh"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new method to generate smooth and realistic image morphing results using diffusion models. The method starts with a baseline approach based on textural embedding interpolation and latent interpolation. To advocate direct and plausible interpolation results, authors fine-tune the diffusion model based on the image pairs with a bottleneck constraint implemented by LoRA to retrain the mode coverage along the morphing path. The rank is selected adaptively based on a metric that measures the density along the morphing path. To produce a smooth morphing, interpolation parameters are adaptively determined to ensure uniform perceptual transition. The proposed method demonstrates improved image morphing results compared to the prior work, especially when the semantics of the image pairs differ a lot."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The method proposes several effective techniques to improve the prior diffusion-based baseline, yielding smooth, realistic and direct morphing results. The method works well on challenging cases as shown in Figure 1 and Figure 2 in the main text and Figure 2 in the Appendix, which are indeed impressive. Notably, these are achieved without auxiliary knowledge such as pose guidance as used in prior the method.\n\n- The proposed techniques are simple yet effective. The LoRA finetuning effectively improves the plausibility of the interpolation results without mode collapse to training data. The perceptually uniform sampling based on the LPIPS difference is a quite useful technique and will benefit the following works in this area.\n\n- The paper proposes novel metrics to measure the morphing quality in terms of directness, realism and smoothness. The quantitative comparisons are solid and sufficiently prove the effectiveness of the method."
                },
                "weaknesses": {
                    "value": "- While the proposed method is effective, I think the contribution of the paper is relatively incremental. The framework is mostly built upon Wang and Golland 2023. Using LoRA to derive an input-aware diffusion model is relatively straightforward. And the proposed metrics are mostly based on prior work. Also, the LoRA rank adaption is heuristic and cannot be regarded as a major contribution.\n\n- The physical meaning of the relative perceptual path diversity (rPPD) score is not clear. Why the score is a ratio that divides the\nperceptual difference of the endpoint images? Moreover, the accumulated LPIPS difference cannot properly reflect the semantic difference of the input image pair. For example, the morphing for two bears that change a lot in location may be much easier than the morphing for two objects that differ significantly in semantics. To properly choose the LoRA rank, a much simpler metric that depends on CLIP score difference seems to suffice.\n\n- Some of the proposed techniques only bring marginal improvement. As shown in Table 2, the best model is more likely not to be the one with adaptive rank. And in Figure 4, the difference between (n) and (o) is not significant.\n\n- There is no quantitative study for the effectiveness of separate LoRA for unconditional score estimation. Also, there is no smoothness measure for perceptually uniform sampling."
                },
                "questions": {
                    "value": "The rank adaption based on relative perceptual path diversity (rPPD) seems not effective enough. Is it possible to use much simpler heuristics based on CLIP score difference?\n\nThe techniques proposed in the paper do not make a significant difference when used independently, but the visual difference relative to the prior work (Figure 2) is impressive. It will be useful to show the qualitative results that show how the result progressively evolves by adding each technique one by one. Such analysis will let the readers understand which component plays the role at most."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1019/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1019/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1019/Reviewer_NAVh"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1019/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698812426729,
            "cdate": 1698812426729,
            "tmdate": 1699636027654,
            "mdate": 1699636027654,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Ts2qJNMvKe",
                "forum": "gG38EBe2S8",
                "replyto": "2DQUGdGuSo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1019/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1019/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the insightful comments. Following are responses for your questions.\n\n>**W1: Contribution of the paper is relatively incremental. The framework is mostly built upon Wang and Golland 2023**\n\nThe major differences are as follows.\n\nFirstly, see general comment 4 for difference in task definition.\n\nSecondly, we are the first to propose that model adaptation is the key component for morphing. Despite its simplicity, this technique has been shown in our work to provide extraordinary improvements in both sample fidelity and directness. Meanwhile, unlike Wang and Golland 2023, the superior performance does not depend on careful selection for prompts, hyperparameters or random seeds. It also enables our method to morph inputs with pose inconsistency, while Wang and Golland 2023 relies on an auxiliary pose estimation model to handle this problem. \n  \nThirdly, the flexibility and robust performance shows the potential of our method to be adopted to a wide range of applications, e.g., in Sec. 5.3 of the main context, we show that our work can be adopted for model evaluation and video interpolation. We provide further examples in Fig.6 of the revised appendix showing that our work can be seamlessly incorporated with an existing state-of-the-art image editing \\& translation pipeline to enable users for controlling the magnitude of change. Wang and Golland 2023 does not have such flexibility.\n\nLastly, our sampling strategy is also different. We propose perceptually uniform sampling while Wang and Golland 2023 use standard uniform sampling. This has a significant impact on the quality of the results.\n\nA comparison with Wang and Golland 2023 is provided in Sec. 2 of the appendix. Given these comparisons, we believe that our work has significant improvement over Wang and Golland 2023 in terms of clearer task definition, more robust performance and more significant downstream benefits.\n\n>**W2: Using LoRA to derive an input-aware diffusion model is relatively straightforward. And the proposed metrics are mostly based on prior work. Also, the LoRA rank adaption is heuristic and cannot be regarded as a major contribution.**\n\nOur contributions can be summarized as follows: (1) we propose a robust open world automatic image morphing framework with diffusion models (see appendix for comparison with other existing methods); (2) we discover that low-rank adaptation contributes significantly to morphing task; (3) we present an algorithm for perceptually-uniform sampling that encourages the interpolated images to have visually smooth changes; (4) we propose three evaluation criteria, namely smoothness, realism, and directness, with associated metrics to measure the quality of the morphing sequence; and (5) we explore potential applications of morphing in broader domains such as video interpolation, model evaluation as well as data augmentation.\n\nTraditional image morphing is a labor intensive task as it requires involvement from human experts. Thus, creating a morphing sequence with good image quality is costly which restricts the applications of morphing in fewer domains. Some efforts for automatic image morphing have been performed; however, existing tools are only able to handle image pairs with small gaps. We are the first work which can robustly and automatically generate morphing sequences for image pairs with large gaps in semantics, pose, etc. Thus, we believe the technical contributions of our work are significant, as well as its ability to be used easily in applications.\n\n>**W3: The physical meaning of the relative perceptual path diversity (rPPD) score is not clear.**\n\nPlease refer to general comment 3."
                    },
                    "title": {
                        "value": "Response to reviewer NAVh - Part 1/3"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1019/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700201470075,
                "cdate": 1700201470075,
                "tmdate": 1700547005932,
                "mdate": 1700547005932,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wSLgOizCKR",
                "forum": "gG38EBe2S8",
                "replyto": "2DQUGdGuSo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1019/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1019/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer NAVh - Part 2/3"
                    },
                    "comment": {
                        "value": ">**W4: No quantitative study for separate LoRA for unconditional score estimation**\n\nOur main contribution is not applying separate LoRA for unconditional score estimation; thus, we do not perform comprehensive evaluation on that. During our experiments, we observed that using separate LoRA for unconditional score estimation performed more stable (compared with randomly discarded conditioning during finetuning) for image inversion, e.g., on CelebA (female-male), maximum end point reconstructed error in terms of LPIPS is **0.137** for separate LoRA unconditional score estimates, the value for randomly discarded conditioning during finetuning is **0.211**. A poor reconstruction may lead to inferior quality; thus, we select separate LoRA for unconditional score estimation. We show two examples of reconstructed images in Fig.13 of the revised appendix. To enhance the difference, we blend real as well as reconstructed images, i.e., $0.5 \\times real + 0.5 \\times recon$ in Fig. 13 (b). Other approaches could improve reconstruction quality as well, but applying separate LoRA for unconditional score estimates is simple yet effective enough for our task.\n\n>**W5: In Table 2, the best model is more likely not to be the one with adaptive rank. In Fig 4, the difference between (n) and (o) is not significant.**\n\nThe adaptive rank equation is a heuristic inferred from real data. These images are quite different in terms of semantic, pose, etc., compared with the benchmark dataset. Most of benchmark datasets do not have such complexity, and are already partially aligned in pose and similar in semantics, or have already been seen by the pretrained diffusion model. Thus, issues such as mode collapse and blurred images may not be a large concern, which leads to a preference of higher rank. This observation is mentioned in the main paper.\n\nFor Figure 4 (n)-(o) of the main paper, separate LoRA for unconditional score estimation leads to slightly more natural and balanced color in cheeks compared to randomly discard conditioning during finetuning. This phenomenon is related to the reconstruction quality. We show examples of reconstructed images from both methods in Fig.13 of the revised appendix.\n\n>**W6: No smoothness measure for perceptually uniform sampling**\n\nThe total LPIPS ($LPIPS_T$) can serve as an indicator for the smoothness of perceptually uniform sampling. We provide some rationale for how the total LPIPS relates to (1) the rate of change between consecutive images and (2) the perceptual path length (PPL) below.  \n\nGiven two images $x^{(0)}$ and $x^{(1)}$, a diffusion model $x^{(\\alpha_i)} = \\mathcal{F}(\\alpha_i, x^{(0)}, x^{(1)})$ generates an interpolated sample (between $x^{(0)}$ and $x^{(1)}$) based on the interpolation parameter $\\alpha_i$. For a specified small delta LPIPS value $\\Delta_{LPIPS}$ (which is almost imperceptible by human), perceptually uniform sampling ensures that $LPIPS(x^{(\\alpha_i)}, x^{(\\alpha_{i+1})})=\\Delta_{LPIPS}$ and the morphing process requires $N$ interpolated samples to start from $x^{(0)}$ to $x^{(1)}$.\n\nWe define this set of $N$ (non-uniform) interpolation scale as $A=\\\\{\\alpha_1,...,\\alpha_{N}\\\\}$, and a set of $N$ uniform interpolation scale as $B=\\\\{\\frac{1}{N+1}, \\frac{2}{N+1},...,\\frac{N}{N+1}\\\\}$, then perceptually uniform sampling can be considered as a function $\\mathcal{G}:B\\to A$ such that $\\mathcal{G}(\\frac{i}{N+1})=\\alpha_i$, and the generation of interpolated samples (given perceptually uniform sampling) can be written as $x^{(\\alpha_i)} = \\mathcal{F}(\\mathcal{G}(\\beta_i), x^{(0)}, x^{(1)})$ where $\\beta_i \\in B$. Thus, **rate of change** $\\frac{\\Delta_{LPIPS}}{\\Delta_{\\beta}}$ is always $(N+1)\\cdot \\Delta_{LPIPS}$ which is the **total LPIPS** along the morphing sequence. One noteworthy observation is that $N$ and $\\Delta_{LPIPS}$ are not independent: as $N$ increase, $\\Delta_{LPIPS}$ will decrease. The intuition behind is that for a fixed number of $N$ interpolated samples (from $x^{(0)}$ to $x^{(1)}$), models with larger total LPIPS will have a larger rate of change (in term of LPIPS) between two consecutive samples (less smooth given fixed number of interpolated points).\n\nAnother metric for smoothness is **PPL**: $PPL_\\epsilon = E_{\\alpha \\sim U(0,1)}[\\frac{1}{\\epsilon^2} \\operatorname{LPIPS}(x^{(\\alpha)}, x^{(\\alpha+\\varepsilon)})]$. Following notations from the previous paragraph, if we let $\\varepsilon=\\frac{1}{N+1}$, PPL for perceptually uniform sampling is $PPL_\\epsilon = E_{\\alpha \\sim U(0,1)}[(N+1)^2 \\cdot \\Delta_{LPIPS}] = (N+1)^2 \\cdot \\Delta_{LPIPS} = \\frac{(total LPIPS)^2}{\\Delta_{LPIPS}}$. Standard PPL calculation requires $\\varepsilon=\\frac{1}{N+1} \\ll 1$. In terms of perceptually-uniform sampling, $N$ should be defined in a manner such that $\\Delta_{LPIPS}$ are almost imperceptible by human. Using total LPIPS to approximate PPL may not be precise due to numerical approximation, but the trend should be similar (larger total LPIPS indicates larger PPL)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1019/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700201930910,
                "cdate": 1700201930910,
                "tmdate": 1700410944914,
                "mdate": 1700410944914,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pZoE9fKQgk",
                "forum": "gG38EBe2S8",
                "replyto": "2DQUGdGuSo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1019/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1019/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer NAVh - Part 3/3"
                    },
                    "comment": {
                        "value": ">**Q1: The accumulated LPIPS difference cannot properly reflect the semantic difference of the input image pair. CLIP score instead?**\n\nThe accumulated LPIPS is not a measurement for semantic similarity, instead it measures the **sample diversity** of morphs which reflects the **perceptual directness** of a morphing result, which is the main criteria for selection of different ranks. The sample diversity is not solely decided by the gap in semantic information, it also depends on the gap in structural information, pose differences, overall style differences, etc., which are intractable to be directly measured by a metric. Thus, instead of measuring this information from the endpoints, we take a more flexible strategy by calculating the total perceptual change from a preliminary morphing result based on uniform sampling. \n\nTo further clarify, we provide the cosine distance (1.0 - cosine\\_similarity) in CLIP embedding space for samples in Fig.4 (a)-(l) of the revised main paper. For example, the pair of semantically similar samples in Fig.4 (a)-(f) (the flower images) with small cosine distance (0.15) have a more diverse morphing path than a pair of semantically dissimilar samples in Fig.4 (g)-(i) (the beetle car images) with large cosine distance (0.37). As can be seen in these cases, our proposed rPPD is more informative in selection of LoRA ranks than CLIP score. Please refer to general comment 3 for more details as well.\n\n>**Q2: Qualitative results that show how the result progressively evolves by adding each\ntechnique one by one**\n\nThanks for the suggestion. In Fig.5 of the revised appendix, we provide the results of progressively adding proposed components in our work to the baseline of a pre-trained Stable Diffusion model, which uses user-provided prompts and discards the textual inversion and model adaptation: 1) Baseline, 2) Textual Inversion, 3) Textual Inversion + Vanilla Adaptation, 4) Textual Inversion + LoRA Adaptation (rank=4), 5) Textual Inversion + LoRA Adaptation (adaptive rank), 6) Textual Inversion + LoRA Adaptation (adaptive rank) + Unconditional Bias Correction.\n\nWe can see that the baseline model fails to reconstruct one of the endpoint images and produces low fidelity samples. Textual inversion ensures a faithful reconstruction of the input endpoints. Model adaptation with LoRA can improve the perceptual directness of morphing path without suffering from the severe artifacts of vanilla model adaptation. An adaptively selected rank using the proposed rPPD metric shows improved directness than a commonly used rank, avoiding unnatural factor variations, e.g., using rank 4, the brightness of the left man's shirt first increases then decreases, while when adaptive rank is used, the brightness varies naturally. Results with the proposed unconditioned bias correction strategy shows finer details (such as hands and clothes). We've added discussion of this part in Sec. 3 of the appendix."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1019/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700202409302,
                "cdate": 1700202409302,
                "tmdate": 1700439592602,
                "mdate": 1700439592602,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]