[
    {
        "title": "Towards Analyzing Self-attention via Linear Neural Network"
    },
    {
        "review": {
            "id": "E7xBgiAIJO",
            "forum": "4fVuBf5HE9",
            "replyto": "4fVuBf5HE9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9108/Reviewer_7ZKc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9108/Reviewer_7ZKc"
            ],
            "content": {
                "summary": {
                    "value": "The paper analyzes the gradient flow training dynamics of a simplified linear transformer on the histogram task. The method reduces the training of a simplified transformer to that of a linear neural network with two layers where the first layer is a diagonal matrix. The theoretical results of the paper are based on one assumption, which is experimentally justified."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proofs of the theorems and lemmas are provided and detailed.\n\n2. Assumption 3.1 for the theoretical results seems reasonable and experimentally justified. \n\n3. The paper is well-motivated."
                },
                "weaknesses": {
                    "value": "1. The paper only deals with a very simple attention layer with only a single linear layer and lacks the components of the transformer model. This setting is not practical in real-world applications and thus limits the scope of the paper\u2019s results.\n\n2. The paper only considers the histogram tasks, which is rather limited in the context of Transformers. \n\n3. The paper lacks experiment results to demonstrate the theoretical results."
                },
                "questions": {
                    "value": "1. Can the results of the paper be extended to other common machine learning tasks where Transformers succeed such as language modeling or machine translation, rather than just the histogram tasks?\n\n2. It would be helpful to show the decay behavior of the loss function $l$ in Theorem 1 under random initialization. Additionally, the authors should demonstrate the behavior of $l$ under bad initialization."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9108/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698828856825,
            "cdate": 1698828856825,
            "tmdate": 1699637146196,
            "mdate": 1699637146196,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "l90wF9PmCl",
                "forum": "4fVuBf5HE9",
                "replyto": "E7xBgiAIJO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9108/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9108/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your suggestions. We have added additional some additional experiments demonstrating our theoretical results, and the behavior of the loss function under random and bad initializations."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9108/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700475250318,
                "cdate": 1700475250318,
                "tmdate": 1700475250318,
                "mdate": 1700475250318,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "LdyHMV3Pz1",
            "forum": "4fVuBf5HE9",
            "replyto": "4fVuBf5HE9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9108/Reviewer_nN7u"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9108/Reviewer_nN7u"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to understand the training dynamics of self-attention networks. In particular, the paper focuses on a simplified single-layer self-attention network without softmax, MLP, layer normalization, and positional embeddings. It restricts itself to a specific class of learning tasks, namely histogram-like tasks. In its simplest form, given an $N$ length sequence, this task requires the network to produce an $N$ length output sequence where $i$th output element contains the frequency of the input element at the $i$th position in the input sequence. The paper reduces the problem of learning the simplified self-attention model to learning a two-layer linear network. Subsequently, the paper analyzes the gradient flow for learning the two-layer linear network."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1) The paper considers multiple variants of the histogram-like learning tasks for a single-layer self-attention model. \n2) The paper explores a connection between learning self-attention models and linear networks.\n3) The paper exploits the structure of the underlying problem and breaks down the gradient flow analysis of learning linear networks to multiple one-dimensional problems."
                },
                "weaknesses": {
                    "value": "1) The main setup studied in the paper is not well motivated. Why should one care about the histogram-like learning tasks? By construction, self-attention can easily model this task. But that alone does not justify the importance of such tasks. \n2) The paper considers a very simplified setup, e.g., single-layer, no positional embeddings, and the size of the alphabet equal to the embedding dimension. How the findings of this paper affect the practice is not at all clear from the current version of the paper.\n3) While discussing the prior works, the paper states \"...While insightful, these papers generally involve stylized assumptions and this makes it difficult to compare the results.\" However, this paper goes on to study a completely new problem (again with various assumptions); hence does not provide any comparison with prior art. \n4) There is significant scope for improvement in the presentation of the paper. For example, one can improve the flow of the paper by better organizing the key contributions and the discussion of prior work. Similarly, there is room for improvement in the presentation of the technical content. Section 4 repeatedly mentions Eq. (2) which is only introduced later in Section 5. Similarly, Theorem 2 is mentioned multiple times before being formally introduced or informally discussed. How do various points in Remark 1 constitute an as \"extensions of theorem 2\"?"
                },
                "questions": {
                    "value": "See the comments under the weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9108/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699243030984,
            "cdate": 1699243030984,
            "tmdate": 1699637146088,
            "mdate": 1699637146088,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KAQsRA1dCh",
                "forum": "4fVuBf5HE9",
                "replyto": "LdyHMV3Pz1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9108/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9108/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your suggestions. The remarks are better called corollaries than extensions. We will incorporate your suggestions in our revision."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9108/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700238766333,
                "cdate": 1700238766333,
                "tmdate": 1700238766333,
                "mdate": 1700238766333,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BogcH2xp6O",
                "forum": "4fVuBf5HE9",
                "replyto": "KAQsRA1dCh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9108/Reviewer_nN7u"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9108/Reviewer_nN7u"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response."
                    },
                    "comment": {
                        "value": "Thank you for your response. I have decided to keep my original score. I would encourage the authors to submit a revised version to a future venue."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9108/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700691853143,
                "cdate": 1700691853143,
                "tmdate": 1700691853143,
                "mdate": 1700691853143,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ngnGDX5uND",
            "forum": "4fVuBf5HE9",
            "replyto": "4fVuBf5HE9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9108/Reviewer_4ez9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9108/Reviewer_4ez9"
            ],
            "content": {
                "summary": {
                    "value": "This paper simplifies the training dynamics problem of a 1-layer linear self-attention layer into the joint optimization problem with two matrix variables that minimize loss like l_{df}(Q, v) = 0.5 * |Q*Diag(v) - M|_F^2. They show that this loss will decrease in exponential speed. They also try histogram tasks and show that the learned attention maps match their expectation."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. The writing is clear, and the main result is easy to understand.\n2. I think the construction of s for solving dynamics systems in theorem 2 is skillful."
                },
                "weaknesses": {
                    "value": "1. I think the total contribution of this work is not enough to be accepted by ICLR. Although this paper solves a particular dynamical system in detail with some clever construction of $r$ and $s$, which I think is the only novel contribution of this paper, the problem setting is too simple (linear attention without softmax layer, l2 loss, adding assumption 3.1 to connect l_{tf} and l_{df}, histogram tasks experiment, etc.) to give much insight to understand the true mechanism of the transformer. Several recent works(for example, [1,2,3]) mentioned in the related work part have shown that the training dynamics of the 1-layer transformer with softmax layer will let the attention map show some sparsity pattern and focus on particular topics of the input data. And their results may have covered more insightful results\n\n[1] Yuchen Li, Yuanzhi Li, and Andrej Risteski. How do transformers learn topic structure: Towards a mechanistic understanding, 2023.\n[2] Yuandong Tian, Yiping Wang, Beidi Chen, and Simon Du. Scan and snap: Understanding training dynamics and token composition in 1-layer transformer, 2023.\n[3] Samet Oymak, Ankit Singh Rawat, Mahdi Soltanolkotabi, and Christos Thrampoulidis. On\nthe role of attention in prompt-tuning. arXiv preprint arXiv:2306.03435, 2023."
                },
                "questions": {
                    "value": "The same as weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9108/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9108/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9108/Reviewer_4ez9"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9108/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699254822741,
            "cdate": 1699254822741,
            "tmdate": 1699637145977,
            "mdate": 1699637145977,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]