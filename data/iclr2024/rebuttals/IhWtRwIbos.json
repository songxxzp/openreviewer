[
    {
        "title": "Discovering Environments with XRM"
    },
    {
        "review": {
            "id": "xfnLORP8ma",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2424/Reviewer_KDkA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2424/Reviewer_KDkA"
            ],
            "forum": "IhWtRwIbos",
            "replyto": "IhWtRwIbos",
            "content": {
                "summary": {
                    "value": "The paper addresses OOD generalization by discovering latent environments (partitions) of the training data that are beneficial when used subsequently with standard methods (GroupDRO, reweighting, or resampling to equalize groups during training). The method proceeds by training a pair of models (details discussed below)."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Thorough evaluation on multiple standard datasets.\n\n- Good empirical results."
                },
                "weaknesses": {
                    "value": "W1. If I understand correctly, the method seems to rely on the fact that misclassified examples are such because they do not contain a \"spurious correlation\" that a model would learn by default. The twin training serves to reinforce the tendency of one of the trained models to capture this spurious correlation. If this is indeed the case, then the overall methods seems to depend on the (common) heuristic that models learn spurious correlations by default (a.k.a. shortcut learning). I think this is the same heuristic that is used in the existing methods criticised in Section 3. Critically, this heuristic relies on the fact that we know that the chosen architecture/training set lead to learnin undesirable spurious correlations by default. What if one applies the method to a situation where a perfectly-fine, \"robust\" model is learned by default? I'm guessing that the method would then be detrimental.\n\nI'm not suggesting that we should be able to do better without additional knowledge (in fact [1] seems to show it's not possible) but the authors here do claim to do so, hence the need to point out this possible limitation (see also W5).\n\nIf my understanding of the method is correct, the method is also very similar to the following.\n- Works in the debiasing literature (e.g. LfF) that train a pair of models that respectively rely/do not rely on spurious features. These works are discussed in Sect. 3, and I do understand that they rely to some extent to the tuning of model capacity to ensure that it captures the spurious feature, but I am not convinced that the proposed parallel training (which seem to be the essential difference) leads to the discovery of something fundamentally different.\n- Works in the \"model diversity\" literature that train a pair of models that differ in their predictions [5,6]. These also proceed to train models in parallel, in a was that seems conceptually very similar to the step 1 proposed here (implementation details aside).\n\n--------\n\nW2. The proposed method only partitions the data into 2 \"environments\". I don't think this is really in line with the literature on DG (with which this work is supposed to connect) that are mostly based on invariance learning and need a large number of training environments (e.g. IRM). This work therefore seems much more related to the simpler setting of \"debiasing\" methods (e.g. LfF) that aim at removing the reliance of a model from one precise biased feature.\n\nThe methods used for the second phase are indeed simple baselines for debiasing, and not really DG methods. These are very strong baselines in these settings (and with the datasets considered), but I'm not sure this is what the reader would expect given all the mentions about DG.\n\n--------\n\nW3. Absence of a comprehensive review of related work. Some directly-related methods are correctly cited/discussed throughout the paper, but there are other connected areas that are not really discussed (examples below).\n\n[1,2] discuss conditions under which environment discovery is possible. I think the theoretical statements in [1] are particularly important to discuss (I am not sure how the proposed method overcomes the impossibility stated in that paper; see also W5).\n[3] was an early method that also proposed to \"unshuffle\" data (a term used in this paper) by simply clustering the data. Looking at the visualizations of discovered \"environments\" in Fig. 3, one wonders if these could also be discovered with such as simple clustering baseline.\n[4] is another recent method that also seems to claim discovering partitions in the data (I suspect it has similar flaws to those discussed in the paper; it has appeared at ICCV 2023 after the ICLR deadline so it's totally fine to dismiss it though).\n\n[1] [ZIN When and How to Learn Invariance Without Environment Partition](https://arxiv.org/abs/2203.05818)\n\n[2] [Provably Invariance Learning without Domain Information](https://proceedings.mlr.press/v202/tan23b/tan23b.pdf)\n\n[3] [Unshuffling data for improved generalization](https://arxiv.org/abs/2002.11894)\n\n[4] [Invariant Feature Regularization for Fair Face Recognition](https://openaccess.thecvf.com/content/ICCV2023/papers/Ma_Invariant_Feature_Regularization_for_Fair_Face_Recognition_ICCV_2023_paper.pdf)\n\n[5] [Agree to Disagree: Diversity through Disagreement for Better Transferability](https://arxiv.org/abs/2202.04414)\n\n[6] [Diversify and Disambiguate: Learning From Underspecified Data](https://arxiv.org/abs/2202.03418)\n\n--------\n\nW4: No discussion or empirical exploration of the limitations of the methods. No precise statement of the assumptions on which the method relies.\n\n--------\n\nMinor comments (no need to comment in the rebuttal; these do not affect my rating of the paper)\n\n- W-minor 1. The writing style is unusual for a technical paper. There are many verbose statements, emotional words, exclamation marks, etc. This is actually a great writing style in other circumstances, but it does not maximize the clarity and efficiency of communication. This does not directly affect my rating of the paper, but it made the reading more tedious. I would suggest using a more concise style and neutral tone for the benefit of the readers.\n\n- W-minor 2. The existing methods for environment discovery based on 2 phases is described twice in sections 1 and 3. It could be clearer to merge these.\nSection 3 is a mix of review/background material/motivation/related work. It's not bad at all in its contents, but it could be easier for the readers to stick with common sections like \"related work\", \"background\", etc.\n\n- W-minor 3. Note that the initial premise stated in the very first sentence of the abstract is not really correct (although it does not really affect the rest of the paper):\n\"Successful out-of-distribution generalization requires environment annotations (...) therefore (...) we must develop algorithms to automatically discover environments\"\nUsing multiple training environments/domains is only one approach to improve OOD generalization."
                },
                "questions": {
                    "value": "Please comment on W1-W4 above.\n\nTo summarize, the main reasons for my negative rating are the absence of precise statements about limitations/assumptions of the method, and the missing discussion of links with the existing literature. Therefore, I am not sure this is really a work about DG (but rather the simpler setting of single-bias), and the core of the method may be very similar to existing work [5,6] (although presented in very different terms).\n\n--------\n\nIn the spirit of constructive feedback, I would suggest that theses issues are fixable (in a future version) with:\n\n(1) a proper review of the existing work, how/if it relates to this work (e.g. what is the connection with invariance learning? how do the many-environment methods related to this one? how to understand the claims made here in relation to the impossibility theorem in [1] mentioned below?)\n\n(2) a better discussion why/how the proposed method work. The current text is mostly hand waving. Even if a complete theory is out of reach, perhaps a concrete example could help (conceptual, or with a toy example)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2424/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697189229654,
            "cdate": 1697189229654,
            "tmdate": 1699636177872,
            "mdate": 1699636177872,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WwYzBqN8wb",
                "forum": "IhWtRwIbos",
                "replyto": "xfnLORP8ma",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2424/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2424/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Addressing Review Feedback"
                    },
                    "comment": {
                        "value": "Thank you so much for your valuable feedback.\n\n> What if one applies the method to a situation where a perfectly-fine, \"robust\" model is learned by default?\n\nAppendix B now contains a tour on four versions of the ColoredMNIST dataset, providing intuition about those cases where XRM works, and those instances where it may fail. As you suggest, XRM is very helpful when the spurious correlation is learned first, whereas ERM is a better alternative in those cases where the invariant correlation is learned first. In sum, we recommend running both ERM and XRM when the spurious correlation type is unknown.\n\n> [Comparison to LfF]\n\nWe now clarify (in blue). The crucial ingredient in XRM is the mechanism of label-flipping, which allows us to (i) run our environment discovery method safely until convergence, and (ii) count label flips as a model selection criteria. This is a major advancement with respect to previous methods, which (i) required surgical early stopping, and (ii) didn't provide a model selection criteria. Consequently, previous work resorted to a validation set with environment annotations, the very information subject to discovery.\n\n> [Comparison to model diversity]\n\nWe now clarify (in blue). Once again, the difference is in the label-flipping mechanism, which implements an \"echo chamber\" that biases the twins to increasingly rely on spurious correlations, and *agree* on their predictions. This is in fact the opposite of what is pursued in the model diversity literature\u2014training multiple accurate models that differ in their predictions.\n\n> The proposed method only partitions the data into 2 \"environments\"\n\nWhether two environments suffice\u2014when constructed appropriately\u2014is in fact a fascinating question that we have pondered about for a while. We do not believe that XRM is identifying \"one spurious correlation\" but, more generally, \"the direction of spuriousness\u201d. We lack the theory we would love to have here, but we suspect that an Oracle with knowledge about what the test domain will be would be able to split the training data as to maximally reveal the train-test discrepancy. Thank you for pointing this out, we now discuss this stimulating topic (in blue).\n\n> I'm not sure this is what the reader would expect given all the mentions about DG\n\nWe are currently conducting experiments on DomainBed, a key benchmark in domain generalization. We will include the findings in Appendix C in coming hours.\n\n> Absence of a comprehensive review of related work\n\nAppendix A now contains an exhaustive literature review on domain generalization, subpopulation shift, and their relations to invariance and causal inference. We survey methods for environment discovery, as well as the relevant impossibility results and other interesting pieces of work you kindly mention.\n\n> No discussion or empirical exploration of the limitations of the methods\n\nAgain, Appendix B now presents a ColoredMNIST dataset analysis, showing XRM's effectiveness and limitations in certain scenarios and recommending a combined use of ERM and XRM when the correlation type is unclear.\n\n---\n\nWe've done our best to address your concerns. If you need more clarification, please let us know. If you find our response satisfactory, we would appreciate it if you could consider increasing your score."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2424/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700596039014,
                "cdate": 1700596039014,
                "tmdate": 1700596039014,
                "mdate": 1700596039014,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "17J3u0aPjT",
                "forum": "IhWtRwIbos",
                "replyto": "WwYzBqN8wb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2424/Reviewer_KDkA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2424/Reviewer_KDkA"
                ],
                "content": {
                    "title": {
                        "value": "Response to the authors"
                    },
                    "comment": {
                        "value": "I appreciate the efforts of the authors in patching the issues pointed out in the review. However I still think that the whole story about environment discovery does not match with the actual contributions centered on a \"debiasing\" (2-environment setting). Therefore I still find the paper misleading and a disservice for the readers in its current form. The relation with the existing literature should be more precisely and clearly laid out in the text, not just as an afterthought in the appendix.\n\nSimilarly the discussion about the impossibility of solving the problem being addressed in its generality, which the authors added as Appendix B, should also be part of the main story (at least in a short form, not just as a reference to Appendix B). The bottom line seems simply that the method should be tried empirically to check whether its heuristics are appropriate for the task. This is fine since this seems to be a fundamental limitation, but it doesn't match the story and claims in the main text of a method that comes with batteries included.\n\nThis is still a clear reject for me but I look forward to seeing another version of this paper submitted at a future venue."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2424/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700647046414,
                "cdate": 1700647046414,
                "tmdate": 1700647046414,
                "mdate": 1700647046414,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QZ7kLDcXKl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2424/Reviewer_f39H"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2424/Reviewer_f39H"
            ],
            "forum": "IhWtRwIbos",
            "replyto": "IhWtRwIbos",
            "content": {
                "summary": {
                    "value": "The paper introduces CROSS-RISK MINIMIZATION (XRM), a method for achieving robust out-of-distribution generalization without relying on resource-intensive environment annotations. By training twin networks to imitate confident mistakes made by each other, XRM enables automatic discovery of relevant environments for training and validation data. The proposed approach addresses the challenge of hyper-parameter tuning and achieves oracle worst-group accuracy, offering a promising solution for broad generalization in AI systems."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The problem of learning OOD robust model without manual domain partition is a very important task, which might has great impact on real-world applications.\n* The proposed method has a clear advantage over existing methods such as EIIL and JTT, that they does not need to explicitly tune the hyperparameter for early stopping. Since hyper-parameter tuning is a crucial challenge, the proposed method would be of interest to many.\n* The empirical performance is strong."
                },
                "weaknesses": {
                    "value": "I have several concerns as follows:\n\n1. The paper should provide a clear discussion on the identifiability challenges presented in [1], which demonstrate that learning invariance without domain partition can be generally impossible. It is crucial to address the need for imposing inductive bias, additional assumptions, conditions, or auxiliary information to ensure the effectiveness of the proposed method. A thorough exploration of these aspects would enhance the paper's theoretical foundation and its practical applicability.\n\n2. According to [2, 3], spurious features are defined as any nodes in the causal graph other than the direct causes of the label. However, I have concerns about the evaluations conducted on datasets like waterbird, which explicitly contain only one dominating spurious feature. These datasets may not fully reflect the implications of the proposed methods on more realistic and high-dimensional datasets such as ImageNet variants. Moreover, the paper relies on the assumption that Empirical Risk Minimization (ERM) learns spurious features first, but it may not hold true for all types of spurious features as discussed in [2, 3]. It would be valuable to address these concerns and provide further insights into the generalizability of the method to diverse real-world datasets.   \n\n3. If a large number of spurious features are present, [1] demonstrates that there are necessary and sufficient conditions for learning invariance without explicit domain partitions, which can be quite restrictive. I have concerns about whether the proposed two-stage method can effectively address this problem given the limitations imposed by these conditions. It would be valuable for the authors to discuss how their method overcomes or accommodates these restrictions and whether it can achieve satisfactory results in scenarios with a significant number of spurious features.  \n\n4.  Several studies [5, 6] have highlighted the challenges associated with learning invariance in the presence of many spurious features. In a recent paper, [4] discovered that when dealing with a large number of spurious features, each ERM model tends to learn a subset of these features. [4] further demonstrates that rather than exclusively focusing on learning invariant features, it is beneficial for OOD performance to diversify the learned spurious features (referred to as spurious feature diversification). Spurious feature diversification is shown to explain the effectiveness of empirically strong methods like SWAD and Model soup. It would be valuable to investigate whether the proposed method (XRM) can enhance spurious feature diversification and demonstrate effective performance on a broader range of real-world datasets, such as PACS, OfficeHome, DomainNet, or ImageNet variants.\n\nCorrect me if I was wrong. I would increase the score if (part of) my concerns were addressed. \n\n[1] Yong Lin et.al., ZIN: When and how to learn invariance without domain partition.\n\n[2] Martin Arjovsky et.al., Invariant Risk Minimization \n\n[3] Jonas Peters, et.al.,. Causal inference using invariant prediction: identification and confidence intervals\n\n[4] Yong Lin et.al., Spurious Feature Diversification Improves Out-of-distribution Generalization\n\n[5] Ishaan Gulrajani et.al., In Search of Lost Domain Generalization\n\n[6] Elan Rosenfeld et.al., The Risks of Invariant Risk Minimization\n\n[7] Junbum Cha et.al., SWAD: Domain Generalization by Seeking Flat Minima\n\n[8] Mitchell Wortsman et.al., Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time"
                },
                "questions": {
                    "value": "See weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2424/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2424/Reviewer_f39H",
                        "ICLR.cc/2024/Conference/Submission2424/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2424/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697363256818,
            "cdate": 1697363256818,
            "tmdate": 1700636274232,
            "mdate": 1700636274232,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2FIlhoi32R",
                "forum": "IhWtRwIbos",
                "replyto": "QZ7kLDcXKl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2424/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2424/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Addressing Review Feedback"
                    },
                    "comment": {
                        "value": "Thank you so much for your valuable feedback.\n\n> The paper should provide a clear discussion on the identifiability challenges presented in [ZIN]\n\nThank you for the very interesting reference [ZIN]. Appendix B now contains a tour on four versions of the ColoredMNIST dataset, providing intuition about those cases where XRM works, and those instances where it may fail. We frame this analysis within the impossibility results described in the ZIN paper.\n\n> I have concerns about the evaluations conducted on datasets like waterbirds\n\nOur main evaluation was carried out on seven datasets, where we would argue that Metashift and ImageNetBG are \"ImageNet-like\". We added as many datasets as we could within our constraints in an attempt to cover a wide range of spurious correlations.\n\n> discuss how their method overcomes or accommodates these [ZIN] restrictions\n\nAppendix B now contains a tour on four versions of the ColoredMNIST dataset, providing intuition about those cases where XRM works, and those instances where it may fail. As you suggest, XRM is very helpful when the spurious correlation is learned first, whereas ERM is a better alternative in those cases where the invariant correlation is learned first. In sum, we recommend running both ERM and XRM when the spurious correlation type is unknown.\n\n> investigate whether the proposed method (XRM) can enhance spurious feature diversification and demonstrate effective performance on [...] PACS, OfficeHome, DomainNet, [...]\n\nThank you for pointing us to feature diversification, we now discuss it in our manuscript (in blue).\nAdditionally, per your suggestion, we are currently conducting experiments on DomainBed, and we will include the findings in Appendix C in coming hours.\n\n---\n\nWe've done our best to address your concerns. If you need more clarification, please let us know. If you find our response satisfactory, we would appreciate it if you could consider increasing your score."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2424/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700595597383,
                "cdate": 1700595597383,
                "tmdate": 1700595597383,
                "mdate": 1700595597383,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "di2Xfqr0xe",
                "forum": "IhWtRwIbos",
                "replyto": "2FIlhoi32R",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2424/Reviewer_f39H"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2424/Reviewer_f39H"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the effort in addressing the concerns. I think Appendix B is important and I would appreciate it if the authors could move it to the main part in the final version. I raise my score to 6 to acknowledge this. \n\nAs for the concerns on multiple spurious features, I think it would be more convincing to conduct experiments on MultiColorMNIST (or its variants) in [1] to see how XRM performs. \n\n[1] Yong Lin et.al., Spurious Feature Diversification Improves Out-of-distribution Generalization"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2424/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700636248010,
                "cdate": 1700636248010,
                "tmdate": 1700636248010,
                "mdate": 1700636248010,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xxFjM77Im1",
                "forum": "IhWtRwIbos",
                "replyto": "QZ7kLDcXKl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2424/Reviewer_f39H"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2424/Reviewer_f39H"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the additional experiments. \n\nI would be very interested in seeing future works  which is designed for multi and even high-dimensional spurious features. The spurious features are natrually high dimensional."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2424/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700701094688,
                "cdate": 1700701094688,
                "tmdate": 1700701239269,
                "mdate": 1700701239269,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "D1zaYXQauT",
            "forum": "IhWtRwIbos",
            "replyto": "IhWtRwIbos",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2424/Reviewer_fj8c"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2424/Reviewer_fj8c"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the challenge of achieving robust out-of-distribution generalization without relying on resource-intensive environment annotations. The authors propose Cross-Risk Minimization (XRM), a novel approach that trains twin networks to learn from random halves of the training data while imitating confident mistakes made by their counterparts. XRM enables automatic discovery of environments for both training and validation data. The authors demonstrate the effectiveness of XRM by building domain generalization algorithms based on the discovered environments, achieving oracle worst-group-accuracy."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is well-organized and easy to understand.\n2. This paper addresses a crucial challenge in Domain Generalization (DG) tasks, which is the data-splitting process without relying on human annotations.\n3. The authors provide strong empirical evidence through extensive experiments to substantiate the effectiveness of their proposed XRM method."
                },
                "weaknesses": {
                    "value": "1. The paper's claims may be slightly overstated. While the focus on subpopulation shift in distribution shift is indeed important, it might be more appropriate to avoid claiming to solve a long-standing problem in out-of-distribution generalization without further empirical studies on widely recognized DG benchmarks such as DomainBed and Wilds. These additional experiments could provide more convincing evidence of the proposed approach's effectiveness.\n2. The paper lacks a comprehensive discussion of important related works concerning data splitting strategies for improved DG performance and subpopulation shift, such as references [1], [2], [3] and [4]. Notably, in [1], the authors have theoretically demonstrated the challenges of learning the invariant correlation between samples and labels in the absence of prior information. Including these relevant works would enhance the paper's literature review and contextualize the proposed approach.\n\nTypo:\n\nIn the first sentence of the paragraph above section 4.2, it appears that the authors have inadvertently added a redundant \"we.\u201d\n\n[1] ZIN: When and How to Learn Invariance Without Environment Partition?\n[2] Provably Invariant Learning without Domain Information.\n[3] Rethinking Invariant Graph Representation Learning without Environment Partitions.\n[4] Just Mix Once: Worst-group Generalization by Group Interpolation."
                },
                "questions": {
                    "value": "1. As highlighted in [1], it is crucial to understand the specific scenarios where XRM is expected to be effective. Therefore, it would be beneficial for the authors to provide further insights into the data distribution settings in which XRM is likely to perform well. Alternatively, the authors could explore providing theoretical guarantees to enhance the understanding of XRM's strengths and limitations.\n2. The observation that XRM outperforms Human-annotation methods is intriguing and warrants further explanation."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2424/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2424/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2424/Reviewer_fj8c"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2424/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698549604766,
            "cdate": 1698549604766,
            "tmdate": 1699636177730,
            "mdate": 1699636177730,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "medaDhXRzr",
                "forum": "IhWtRwIbos",
                "replyto": "D1zaYXQauT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2424/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2424/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Addressing Review Feedback"
                    },
                    "comment": {
                        "value": "Thank you so much for your valuable feedback.\n\n> The paper's claims may be slightly overstated [...] without further empirical studies on widely recognized DG benchmarks such as DomainBed\n\nIn line with your suggestion, we are currently conducting experiments on DomainBed. We will share the findings in Appendix C in coming hours.\n\n> The paper lacks a comprehensive discussion of important related works.\n\nAppendix A now contains an exhaustive literature review on domain generalization, subpopulation shift, and their relations to invariance and causal inference. We also survey methods for environment discovery, as well as the relevant impossibility results you kindly mentioned.\n\n> It is crucial to understand the specific scenarios where XRM is expected to be effective\n\nAppendix B now contains a tour on four versions of the ColoredMNIST dataset, providing intuition about those cases where XRM works, and those instances where it may fail.\n\n> The observation that XRM outperforms Human-annotation methods [...] warrants further explanation\n\nOur experiments on Waterbirds, CelebA, MultiNLI, and CivilComments show that environment annotations discovered by XRM afford the same or better state-of-the-art test worst-group-accuracy than those environment annotations included in the original datasets by humans. Our hypothesis is that the biases identified by humans may not always align with the biases that models learn, and vice versa [1].\n\n---\n\nWe've done our best to address your concerns. If you need more clarification, please let us know. If you find our response satisfactory, we would appreciate it if you could consider increasing your score.\n\n[1] Bell, S.J. and Sagun, L., 2023, June. Simplicity Bias Leads to Amplified Performance Disparities."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2424/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700595438323,
                "cdate": 1700595438323,
                "tmdate": 1700595438323,
                "mdate": 1700595438323,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]