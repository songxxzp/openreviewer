[
    {
        "title": "Democratizing Fine-grained Visual Recognition with Large Language Models"
    },
    {
        "review": {
            "id": "5gpcq1k4Ev",
            "forum": "c7DND1iIgb",
            "replyto": "c7DND1iIgb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2720/Reviewer_WYuk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2720/Reviewer_WYuk"
            ],
            "content": {
                "summary": {
                    "value": "- This paper proposes a novel training-free FGVC pipeline.\n\n- The key idea is to use multiple LLMs to generate the visual concepts to represent the imae of a fine-grained category, which allows not only trainin-free but also zero-shot inference.\n\n- Besides, a novel Pokemon dataset is proposed to further foster FGVC.\n\n- Extensive experiments on multiple standard FGVC datasets and the proposed dataset show the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This paper is well-written, easy-to-follow and well-presented.\n\n- A novel Pokemon dataset is proposed to benefit FGVC.\n\n- The proposed method is novel and interesting, which provides a new way to do training-free FGVC based on LLM.\n\n- The proposed method is experimentally better than existing state-of-the-art methods."
                },
                "weaknesses": {
                    "value": "- Although the novelty and technique score is satisfactory for ICLR, a major issue is whether it is necessary to only leverage LLM for FGVC under the proposed setting. In fact, as far as the reviewer concerns, some other only learning prompt on VLM can already achieves more than 90% accuracy for zero-shot FGVC. For example:\n\n[1] Conditional Prompt Learning for Vision-Language Models. CVPR 2022.\n\n[2] Learning to Prompt for Vision-Language Models. IJCV 2022.\n\nIn fact, this issue is critical, at least from the vision community. What should be the correct role of LLM for FGVC? Should only the concepts from LLM be used for FGVC? Or it should be jointly used with visual representation to aid FGVC? Soley relying on LLM for vision tasks is in a way that the reviewer does not appreciate.\n\n- The idea to use LLM to generate concepts for FGVC, is already not new now. Some recent works implement idea:\n\n[3] Learning concise and descriptive attributes for visual recognition. ICCV 2023.\n\n- The proposed dataset also has multiple issues for publication. For example:\n\n(1) The exact number of samples, how the training and testing set is divided, and etc. Many details are missing in the main submission and the supplementary materials.\n\n(2) As the dataset is Pokemon, not real-world FGVC categories, some further questions raise: Is it able to describle the fine-grained patterns? Besides, is the LLM leart from real-world able to differentiate the fine-grained patterns on Pokemon. These issues make the contribution of FGVC to be doubted."
                },
                "questions": {
                    "value": "My concerns on Q1, 2, 3 have been well addressed. Although I still have some minor concerns, it does not impede that this work has reached the accept threshold. So, I decide to improve my rating to borderline accept.\n\n%%%%%%% before rebuttal\n\nAlthough the strength seems to overwhlem the weakness, as an expert in vision and FGVC, the reviewer still holds a critical view towards this LLM based work, which may be good enough for pulibcation. Some specific questions:\n\nQ1: As there is already some good VLM solution to do zero-shot FGVC and achives nearly 90% accuracy, is it necessary to solely address FGVC by pure concepts from LLM? Besides, is it necessary to solely rely on LLM instead of VLM or visual representation for FGVC?\n\nQ2: The idea of this paper is actually not very new now. The ICCV paper [3] implements similary idea. The difference and comparison should be made and clarified.\n\nQ3: So many details of the proposed dataset are missing. The specific comments have been listed in the weakness.\n\nQ4: Is the proposed Pokemon dataset can really fit the fine-grained patterns? Or, the LLM and VLM learnt from real-world data, can really discriminate the fine-grained pokemon? These issues may make its contribution actually limited for FGVC community."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2720/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2720/Reviewer_WYuk",
                        "ICLR.cc/2024/Conference/Submission2720/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2720/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697634735417,
            "cdate": 1697634735417,
            "tmdate": 1700329978828,
            "mdate": 1700329978828,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dH4giNzXFt",
                "forum": "c7DND1iIgb",
                "replyto": "5gpcq1k4Ev",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2720/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2720/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer WYuk (1/4)"
                    },
                    "comment": {
                        "value": "Thank you for your positive, attentive and constructive feedbacks! We are encouraged that you, as an expert in vision and FGVC, recognized our method as \"**novel and interesting**\" and \"experimentally better than existing state-of-the-art methods\" which provides \"a new way to do training-free FGVC based on LLM\". We address your thoughts point by point below.\n\n>**Q1:** As there is already some good VLM solution to do zero-shot FGVC and achives nearly 90% accuracy, is it necessary to solely address FGVC by pure concepts from LLM? Besides, is it necessary to solely rely on LLM instead of VLM or visual representation for FGVC?\n>\n>**A1:** Great question! Thanks for recoginizing the novelty and technique score of our work are statisfactory for ICLR. Following your suggestion, we have now also added a discussion section in **Appendix K.4** of the revision to carefully compare our setting and method with respect to VLM zero-shot transfer and prompt enrichment/tuning methods. Here, we breakdown this question into three subsequent questions and address them below:\n>\n>1. **Setting perspective:** In fact, as recoginized by ```Reviewer oRxX```, we would like to emphasize and clarify that the novel task we propose and study in this work is **vocabulary-free FGVC with only few unlabeled samples as natural observation**. We compare our setting and existing VLM solutions apple-to-apple in the **Table 1 and Table 2 (in Response 2/4)**. To be more specific, under vocabulary-free FGVC setting, there is no target class names (vocabulary) pre-defined by domain experts (*e.g.*, ornithologist) for VLM-based zero-shot transfer methods or VLM prompt tuning methods to construct classifiers. This makes existing VLM solutions (*e.g.,* CLIP, CoOp[2], CoCoOp[1]) that rely on the pre-defined vocabulary **fail to function** in this novel task, although VLM solutions such as [1] and [2] can achieves good performance for standard zero-shot FGVC task where the class names are already known. To be best of our knowledge, vocabulary-free FGVC task is underexplored in the existing literature. We believe this is an important yet realistic novel task for the community to explore. This newly proposed vocabulary-free FGVC task is inspired by the natural case:\n>\n> > \"An bird lover gathered several unlabeled images from a webcam set up in the Amazon jungle. Confronted with the task of identifying the fine-grained bird species existing the the region from these few unlabeled observations, and without any pre-defined species names as prior knowledge from ornithologists, the bird lover encountered a significant challenge. In this context, our proposed FineR system is functional, enabling the bird lover to not only discover the bird species but also to classify them effectively in the incoming data stream from the webcam.\u201c\n>\n>2. **Method perspective:** Our proposed method **is not** solely rely on LLM. In fact, the proposed FineR system jointly integrates the visual information translation capability of the **VQA model**, the reasoning capability of the **LLM**, and the zero-shot discrimination capability of **contrastive VLM** to tackle the proposed vocabulary-free FGVR task. To further clarify, FineR primarily operates in 3 steps: (i) first uses BLIP-2 to **translate** visual information (super-category and attribute descriptions) from the few (3 per class) unlabeled images to natural language; (ii) then uses the visual cues in text to elicit the reasoning capability of ChatGPT for **discovering fine-grained categories**; (iii) lastly uses CLIP to classify/recognize fine-grained categories in the test data stream based on the discovered concetps.\n>\n>3. **The necessity of using LLMs:** Insightful question! In our work, **the role of LLM is a Reasoning Engine**, just like a human expert would reason fine-grained categories based on the visual input. Furthermore, to further study \"Is it necessary to use LLMs?\", we constructed strong baseslines that adopt different solutions: (i) Learning-based Sinkhorn; (ii) Knowledge base based WordNet (use CLIP to retrive categories from WordNet); (iii) and BLIP-2 (VQA), as well as compared  with SOTA methods (SCD and CASED) based on large corpus knowledge bases. As shown in the quantitative results in Tab. 1 and qualitative results in Fig. 6 of the paper, our FineR system not only performs better but also makes more plausible mistakes. This demonstrates the advantages of using LLMs to **reason** in vocabulary-free FGVC task.\n\n[1] Conditional Prompt Learning for Vision-Language Models. CVPR 2022.\n[2] Learning to Prompt for Vision-Language Models. IJCV 2022."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2720/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700328740674,
                "cdate": 1700328740674,
                "tmdate": 1700328740674,
                "mdate": 1700328740674,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "s2XfUX9izb",
                "forum": "c7DND1iIgb",
                "replyto": "5gpcq1k4Ev",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2720/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2720/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer WYuk (2/4)"
                    },
                    "comment": {
                        "value": "**Table 1: *Training* setting comparison between our novel vocabulary-free FGVR task and other related machine learning settings.**\n|                                     | **Model**               | **Annotation**            | Data    | Data       |\n| ----------------------------------- |:------------------:| :--------------: | :------: | :-------------: |\n| **Settings**                        | **Training** | **Supervision**  | **Base Classes** | **Novel Classes** |\n| Standard FGVR Classification                    | &#10004;  |  Class Names + Auxilliary attributes    | Fully Labeled    | &#10006;     |\n| VLM Zero-shot transfer              | &#10006;  | &#10006;     | &#10006;     | &#10006;     |\n| VLM Prompt Enrichment [3]           | &#10006;  | &#10006;     | &#10006;     | &#10006;     |\n| VLM Prompt Tuning ([1], [2])        | &#10004;  | Class Names   | Few (e.g., 16) Shots Labeled    | &#10006;     |\n| **Vocabulary-free FGVR (Ours)**     | &#10006;  | &#10006;       | &#10006;     | **Few (e.g., 3) Shots unlabeled** |\n\n**Table 2: *Inference* setting comparison between our novel vocabulary-free FGVR task and other related machine learning settings.**\n\n| Setting                         | Base Classes     | Novel Classes        | Output                         | Prior Knowledge                |\n| ------------------------------- | ---------------- | -------------------- | ------------------------------ |------------------------------- |\n| Standard FGVR Classification                    | &#10004;  |  Class Names    | Full Labeled    | &#10006;     |\n| VLM Zero-Shot Transfer          | None             | Classify             | Names                          | Class Names                    |\n| VLM Prompt Enrichment [3]       | None             | Classify             | Names                          | Class Names                    |\n| VLM Prompt Tuning ([1], [2])    | Classify         | Classify             | Names                          | Class Names                    |\n| **Vocabulary-free FGVC (Ours)** | **None**         | **Classify**         | **Names**                      | **None**                       |"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2720/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700328805909,
                "cdate": 1700328805909,
                "tmdate": 1700328805909,
                "mdate": 1700328805909,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Rntt0FfD7w",
                "forum": "c7DND1iIgb",
                "replyto": "5gpcq1k4Ev",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2720/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2720/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer WYuk (3/4)"
                    },
                    "comment": {
                        "value": ">-----\n>**Q2:** The idea of this paper is actually not very new now. The ICCV paper [3] implements similary idea. The difference and comparison should be made and clarified.\n>\n>**A2:** We would like to clarify that the settings and methods of our paper and [3] are very different.\n> * **Setting**. The work in [3] assumes the fine-grained categories are **known**. On the contrary, we do not assume any knowledge about the class names.\n> * **Implementation**. In [3], a LLM is queried to simply enhance the descriptions of the already known concepts (or class names) to improve classification. Instead, we use VQA model to first identify super-category (e.g., \"Bird\") from an image and then acquire useful attribute names (no description) for distinguishing the fine-grained sub-categories. Then we use a VQA model to describe the attribute values in the image. The acquired attribute-description paris are then used to invoke LLM reasoning for discovering the fine-grained categories that are **unknown**. **In a nutshell, [3] only uses LLM to describes known classes, while our method uses the interaction between LLM and VLM to reason unknown classes.**\n>\n>In fact, [3] falls into the same group of LLM-based VLM prompt enrichment methods as the ICLR 2023 work [4] that we discussed in the second paragraph of Section 4. We have now also included [3] in the second paragraph Line#3 of Section 4 in the revision. Thanks for the suggestion!\n\n[3] Learning concise and descriptive attributes for visual recognition. ICCV 2023.\n[4] Visual Classification via Description from Large Language Models. ICLR 2023.\n\n>-----\n\n> **Q3:** So many details of the proposed dataset are missing. The specific comments have been listed in the weakness.\n>\n> **A3:** In fact, **we have included the exact number of samples, train/test splits, and statistics of the proposed Pokemon dataset in Appendix A.2**. Furthermore, due to the small scale of the proposed Pokemon dataset, which consists of 10 categories with 3 images each for discovery (D_train) and 10 images per category for testing (D_test), we **direcly present the entire dataset and its splits in Appendix A.2, Fig. 9**. It is important to note that the Pokemon dataset was designed to delve deeper into the capabilities of vocabulary-free FGVC in the wild. Despite its small scale, it poses significant challenges for many strong methods we compared. We will elaborate on these challenges in the next question."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2720/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700329136031,
                "cdate": 1700329136031,
                "tmdate": 1700329136031,
                "mdate": 1700329136031,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QUcdu9cvMV",
                "forum": "c7DND1iIgb",
                "replyto": "5gpcq1k4Ev",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2720/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2720/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer WYuk (4/4)"
                    },
                    "comment": {
                        "value": ">**Q4:** Is the proposed Pokemon dataset can really fit the fine-grained patterns? Or, the LLM and VLM learnt from real-world data, can really discriminate the fine-grained pokemon? These issues may make its contribution actually limited for FGVC community.\n>\n>**A4:** Excellent question! The second part of your question precisely captures the motivation behind creating this dataset. Our intent was exactly to further examine the ability of our method, as well as the compared methods, to discover and differentiate virtual and wild FGVC categories effectively. Therefore, we proposed this wild and virtual Pokemon dataset, which is relatively novel and challenging for foundation models. We breakdown this question into two subsequent questions and address them below:\n>* **Fine-grained Patterns:** We mannuly selected 10 highly visually similar Pokemon categories to capture the fine-grained patterns. As shown in Appendix A.2, Fig. 9, since two Pokemon categories evolved from each other, their inter-class variance is low (*e.g.*, the subtle differences between \"Pidgey\" and \"Pidgeotto\" are only at the crown and tail.) In addition, since Pokemons are cartoon characters, they by default have high inner-class variance (*e.g.*, different facial expressions or body decorations). Additionally, the proposed Pokemon dataset presents an extra intriguing challenge due to the close resemblance of Pokemon characters to their real-world animal counterparts (e.g., \"Squirtle\" closely resembles a \"Turtle\" and \"Pidgey\" a \"Bird\"). This cross-domain similarity introduces an additional domain gap challenge in FGVC discovery. As illustrated in Fig.7, methods based on knowledge bases often incorrectly identify the real-world counterpart of the Pokemon (e.g., identified 'Turtle' instead of 'Squirtle'). In stark contrast, our FineR system successfully discovered 7/10 ground-truth Pokemon categories.\n>\n>* **the LLM and VLM learnt from real-world data, can really discriminate the fine-grained Pokemons?**: Yes! As demonstrated in Fig.7, our FineR system successfully reasoned and discovered 7/10 ground-truth and subsequently achieved upper-bound clustering accuracy and high semantic similarity accuracy. This suggests that LLMs and VLMs trained with Internet-scale data are able to discriminate the fine-grained virtual concepts such as Pokemons. We found the results of this experiment fascinating!\n\nWe hope our answers have adequately addressed your concerns. We hope you might consider raising your score for our pioneering work in the vocabulary-free FGVC task. Should you have any additional questions, please don't hesitate to let use know. We would be more than happy to engage in further discussion with you. Thanks again for your insightful feedbacks strengthening our paper and stimulating our thinking."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2720/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700329215177,
                "cdate": 1700329215177,
                "tmdate": 1700329215177,
                "mdate": 1700329215177,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZbV7XxctLu",
                "forum": "c7DND1iIgb",
                "replyto": "QUcdu9cvMV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2720/Reviewer_WYuk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2720/Reviewer_WYuk"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors  from Reviewer WYuk"
                    },
                    "comment": {
                        "value": "Thanks for the authors for providing such a detailed rebuttal.\n\nYes, my concerns on: 1) its necessaity when compared with existing zero-shot inference by VLM such as CLIP, BLIP; 2) its difference and insights against the recent ICCV 2023 paper have been well clarified.\nSo, after the clarification, I think this work has already meet the accept threshold.\n\nAlthough I am still not fully convinced that the Pokemon dataset may have a significant contribution to real FGVC tasks, and think that leveraging the concepts from LLM to improve the visual representation may be more pratical for FGVC, these concerns (more personal) do not necessarily impede the objective rating for the work itself.\n\nThus, I would like to improve my rating to above the accept threshold."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2720/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700329820286,
                "cdate": 1700329820286,
                "tmdate": 1700329820286,
                "mdate": 1700329820286,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "aedDAZRLDB",
            "forum": "c7DND1iIgb",
            "replyto": "c7DND1iIgb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2720/Reviewer_3hkC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2720/Reviewer_3hkC"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a fine-grained semantic category reasoning method for fine-grained visual recognition, which attempts to leverage the knowledge of large language models (LLMs) to reason about fine-grained category names. It alleviates the need of high-quality paried expert annotations in fine-grained recognition."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tThe presentation of this paper is clear and the proposed method is easy to follow. \n2.\tThis paper proposes to extract visual attributes from images into the large language models (LLM) for reasoning the fine-grained subcategory names, which is a promising way to alleviate the high need for expert annotations in fine-grained recognition."
                },
                "weaknesses": {
                    "value": "The weaknesses are as follows\uff1a\n\n1.\tThe novelty of this paper should be further demonstrated. The proposed method seems an intuitive combination of existing large-scale models, such as the visual question answering model, large-language model and vision-language model, etc. Besides, extracting visual attributes from images for recognition is widely used in generalized zero-shot learning methods such as [a]. \n\n2.\tThe effectiveness of the proposed method should be further verified. The recognition accuracy on the CUB dataset is relatively low. Besides, existing generalized zero-shot learning methods such as [a] should be compared and more corresponding analyses should be added.\n\n3.\tThere lacks a complete recognition example for explaining the whole procedure, which should be added for better understanding.\n\n[a] Progressive Semantic-Visual Mutual Adaption for Generalized Zero-Shot Learning. CVPR 2023"
                },
                "questions": {
                    "value": "The novelty should be clarified and the difference from existing generalized zero-shot learning methods using visual attributes for recognition should be explained. More corresponding experiments and analyses should be added."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2720/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698117215443,
            "cdate": 1698117215443,
            "tmdate": 1699636214313,
            "mdate": 1699636214313,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "V81bhIjXyT",
                "forum": "c7DND1iIgb",
                "replyto": "aedDAZRLDB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2720/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2720/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 3hkC (1/3)"
                    },
                    "comment": {
                        "value": "Thank you for your feedback. We are particularly encouraged with your comment about our method: \"is a promising way to alleviate the high need for expert annotations in fine-grained recognition\". We address your feedback point by point below.\n\n>**Q1.1:** The novelty should be clarified.\n>\n>**A1.1:** Thanks for the suggestions. Our work, we believe, is novel in several key aspects:\n>* **Innovative Application/Task:** To the best of our knowledge, our work is the first to tackle the novel task of **vocabulary-free FGVR with only a few unlabeled samples as natural observation**. To tackle this novel task we harness the reasoning capabilities of LLMs and leverage the Internet-scale expert knowledge encoded in their weights to first discover the fine-grained categories that are present in a dataset, which is otherwise not know a priori. While CaSED (Conti et al., 2023) previously addressed vocabulary-free classification by retrieving concepts from a knowledge-base, our method is tailored specifically for FGVR by leveraging the knowledge in LLM. This also resonates with the comments from `Reviewer 3hkC` and `Reviewer WYuk` who commented that utilization of LLM for FGVR as \"**a new way**\" and \"**a promising way**\", respectively.\n>\n>* **Methodological Advancements:** We introduced **FineR**, a novel system that integrates the visual information translation capability of VQA models, the reasoning capability of LLMs, and the zero-shot discrimination capability of contrastive VLMs to address the vocabulary-free FGVR task. This approach is recognized by `Reviewer WYuk` as \"**novel**\" and \"**interesting**\". Furthermore, as shown in the preliminary experiments in Fig. 2, we observed that other LLM-based methods requiring training alignments (such as BLIP-2 learning Q-former, LLaVA, and MiniGPT-4 learning a projector) struggle to discover fine-grained categories effectively due to the limitations of alignment data. These training-based methods cannot fully utilize the reasoning capabilities of LLMs. In contrast, our FineR system translates visual information from unlabeled images into natural language in order to fully harness the reasoning power of LLMs for fine-grained concepts. Thus, we showed that visual and textual concepts can be aligned *without* the need of expensive training.\n>\n>* **Resource Efficiency and Accessibility:** Our approach is both training-free and vocabulary-free, significantly reducing the computational and data resources typically required for vision model and LLM alignments. This enhances the accessibility and feasibility of our method for a broader range of users, potentially democratizing the application of FGVR across various sub-fields.\n>\n>* **Scalability and Generalization:** Being training-free enhances the scalability and generalizability of our methods. As demonstrated in Fig. 7 (Section 3.2, page 8) and Fig. 24 (Appendix K.3, page 31), our method can effectively discover wild and novel fine-grained Pok\u00e9mon categories, outperforming other methods that fail in these wild scenario.\n>\n> > We have summarized our novelty and contributions at the end of the Introduction section (Sec. 1) in the revision.\n>-----"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2720/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700328070414,
                "cdate": 1700328070414,
                "tmdate": 1700328070414,
                "mdate": 1700328070414,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2uM08ULRZg",
                "forum": "c7DND1iIgb",
                "replyto": "aedDAZRLDB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2720/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2720/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 3hkC (2/3)"
                    },
                    "comment": {
                        "value": ">**Q1.2:** The difference from existing generalized zero-shot learning methods using visual attributes for recognition should be explained.\n>\n>**A1.2:** **In Table 1 and Table 2 below**, we have compared our vocabulary-free FGVR setting with Generalized Zero-Shot Learning (GZSL) at training and inference time, respectively. Furthermore, we have now also added a discussion section in **Appendix K.4** of the revision to carefully compare our setting and method with respect to GZSL.\n>\n**Table 1: *Training* setting comparison between our novel vocabulary-free FGVR task and other related machine learning settings.**\n|                                     | **Model**               | **Annotation**            | Data    | Data       |\n| ----------------------------------- |:------------------:| :--------------: | :------: | :-------------: |\n| **Settings**                        | **Training** | **Supervision**  | **Base Classes** | **Novel Classes** |\n| Generalized Zero-shot Learning ([a])      | &#10004;  | Class Names + Attributes    | Full Labeled    | &#10006;     |\n| **Vocabulary-free FGVR (Ours)**     | &#10006;  | &#10006;       | &#10006;     | **Few (e.g., 3) Shots unlabeled** |\n>\n**Table 2: *Inference* setting comparison between our novel vocabulary-free FGVR task and other related machine learning settings.**\n\n| Setting                         | Base Classes     | Novel Classes        | Output                         | Prior Knowledge                |\n| ------------------------------- | ---------------- | -------------------- | ------------------------------ |------------------------------- |\n| Generalized Zero-Shot Learning ([a])  | Classify         | Classify             | Names or Cluster Index (novel) | Class Names and Attributes     |\n| **Vocabulary-free FGVR (Ours)** | **None**         | **Classify**         | **Names**                      | **None**                       |\n\n> Generalized Zero-Shot Learning (GZSL) methods, like [a], rely on **pre-existing ground-truth attribute-description pairs** for both seen and unseen classes. GZSL methods use these attributes and seen (base) classes to train a vision model that can generalize to unseen classes based on shared attributes. Our method diverges significantly from GZSL in several ways: \n>* **Attribute and Description Availability**: As shown in **Table 1 and Table 2 above**, in our approach, **neither ground-truth attribute names nor descriptions are available**; thus, we do not use any predefined attribute information as prior knowledge, which requires expert annotations.\n> * **Attribute and Description Acquisition**: Initially, our method identifies super-category names (e.g., \"Bird\") using a VQA model, followed by querying an LLM to **generate relevant attribute names** for further distinguishing fine-grained sub-categories. **The attribute descriptions are then extracted from a few unlabeled images using the VQA model**.\n>* **Attribute and Description Utilization**: The generated **attribute-description pairs serve as high-level semantic cues in natural language**, invoking LLM reasoning in identifying fine-grained categories. This approach grants our method semantic awareness, enabling it to provide useful semantic information even in cases of incorrect predictions, as demonstrated in Fig.6.\n>-----"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2720/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700328146019,
                "cdate": 1700328146019,
                "tmdate": 1700328146019,
                "mdate": 1700328146019,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dYowIby3Xk",
                "forum": "c7DND1iIgb",
                "replyto": "aedDAZRLDB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2720/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2720/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 3hkC (3/3)"
                    },
                    "comment": {
                        "value": ">**Q2:** The effectiveness of the proposed method should be further verified. The recognition accuracy on the CUB dataset is relatively low. Besides, existing generalized zero-shot learning methods such as [a] should be compared and more corresponding analyses should be added. More corresponding experiments and analyses should be added.\n>\n>**A2:** We first would like to clarify that it is not fair to directly compare between our approach with generalized zero-shot learning methods. It's crucial to highlight that our approach is both **vocabulary-free** and **training-free**, which fundamentally differs from GZSL methods. Directly comparing our method's recognition accuracy with that of GZSL approaches, which are **supervised** on numerous base seen classes with class name and attribute annotation and jointly evaluated on both seen and unseen classes, would be inappropriate. For instance, GZSL methods' accuracy on the CUB-200 dataset is jointly evaluated on 150 seen and 50 unseen classes. In contrast, all 200 classes of CUB are unseen in our evaluation. Moreover, as outlined in the setting comparison in **Table 1 and Table 2 above**, all the prerequisites required by GZSL during training and inference, including class names, annotated base class data, and ground-truth attribute-description pairs are not available in our setting. In our vocabulary-free FGVR task, both GZSL and standard VLM zero-shot transfer methods, which depend on predefined vocabulary, **fail to function** due to the absence of class names. A thorough review of [a] leads us to conclude that a comparison with **GZSL approaches is not feasible in our setting due to the absence of class names**. In fact, standard VLM zero-shot transfer method (CLIP) was considered and present as the upper bound in our experiments. Notably, we have achieved a significant performance improvement on the CUB-200 dataset compared to the methods we evaluated against. Nevertheless, **we appreciate your suggestion and have added a discussion section in Appendix K.4** of the revised manuscript to specifically clarify the differences between our vocabulary-free FGVR setting and GZSL, along with comparisons to other related settings.\n\n>-----\n\n>**Q3:** There lacks a complete recognition example for explaining the whole procedure, which should be added for better understanding.\n>\n>**A3:** Thank you for your valuable suggestions! Indeed, **we have demonstrated the entire recognition process using an unlabeled \"Italian Greyhound\" image as an example in Appendix B.2, from Fig.12 to Fig.15**. This includes the visual input, the identified super-category ('Dog'), attribute names obtained from the LLM, attribute descriptions extracted by the VQA model, the complete LLM reasoning prompt, and the LLM's output results. In response to your advice, we have added a new figure in **Appendix B.2 Fig.11** of the revised manuscript to more clearly illustrate the full pipeline with actual inputs and outputs.\n\nWe hope our responses have sufficiently addressed your concerns. With these clarifications and changes in the revision based on your suggestion, we kindly hope you might be convinced to raise your score for our pioneering work in the vocabulary-free FGVC task. Your attentive feedback has been helpful in enhancing our paper in the revision, and for that, we are truly grateful.\n\n[a] Progressive Semantic-Visual Mutual Adaption for Generalized Zero-Shot Learning. CVPR 2023."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2720/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700328218297,
                "cdate": 1700328218297,
                "tmdate": 1700328218297,
                "mdate": 1700328218297,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "roV0RW2Edb",
                "forum": "c7DND1iIgb",
                "replyto": "aedDAZRLDB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2720/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2720/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking Forward to Your Reply"
                    },
                    "comment": {
                        "value": "Dear Reviewer 3hkC,\n\nWe genuinely appreciate the valuable feedback you have provided.\n\nAs the deadline for the Author-Reviewer Discussion Phase approaches, we are reaching out to inquire if you might have any remaining questions or concerns regarding our submission.\n\nWe wholeheartedly thank you for your dedication and effort in evaluating our submission. Please do not hesitate to let us know if you need any clarification or have additional suggestions. Your input is highly valued.\n\nBest regards,\n\nAuthors of Submission #2720"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2720/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700609809967,
                "cdate": 1700609809967,
                "tmdate": 1700609840799,
                "mdate": 1700609840799,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "sq0zJfeGHh",
            "forum": "c7DND1iIgb",
            "replyto": "c7DND1iIgb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2720/Reviewer_oRxX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2720/Reviewer_oRxX"
            ],
            "content": {
                "summary": {
                    "value": "FineR leverages large language models to identify fine-grained image categories without expert annotations, by interpreting visual attributes as text. This allows it to reason about subtle differences between species or objects, outperforming current FGVR methods. It shows potential for real-world applications where expert data is scarce or hard to obtain."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This is a good paper, and the advantages are:\n1. The utilization of LLM to vocabulary-free FGVR tasks is novel;\n2. The paper is generally well-written and easy to follow;\n3. Good performance in Table 1 \n4. The well utilization of LLM."
                },
                "weaknesses": {
                    "value": "Advice:\n1. Add the citation of some highly related missing works:\n(1) Transhp: image classification with hierarchical prompting; it also focuses on the fine-grained image classification task. Also, it takes advantage of the recently proposed prompting technique in CV. Is your used LLM better or prompting better?\n(2) V2L: Leveraging Vision and Vision-language Models into Large-scale Product Retrieval; it also focuses on the vision language model and fine-grained visual recognition. The paper is the champion of FGVC 9. A discussion of comparison seems necessary.\n2. Could you show more examples of Fig. 2? Or any failure cases of Fig. 2?\n3. What is your view of NOVELTY of a work using LLMs without training anything?"
                },
                "questions": {
                    "value": "I want to discuss with the authors that:\nWhat is your view of NOVELTY of a work using LLMs without training anything?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2720/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2720/Reviewer_oRxX",
                        "ICLR.cc/2024/Conference/Submission2720/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2720/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699103089112,
            "cdate": 1699103089112,
            "tmdate": 1700351634904,
            "mdate": 1700351634904,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Z5nv04J622",
                "forum": "c7DND1iIgb",
                "replyto": "sq0zJfeGHh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2720/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2720/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer oRxX (1/2)"
                    },
                    "comment": {
                        "value": "We sincerely thank you for your positive feedback and helpful suggestions! We are glad that you recognize our method, which utilizes LLM for tackling **vocabulary-free FGVR tasks**, both \"novel\" and demonstrating \"good performance\", as well as representing a \"well utilization of LLM\". Furthermore, we appreciate that you found that our proposed method has *potential to work in real-world applications*. We address your questions below.\n\n\n>**Q1:** Add the citation of some highly related missing works.\n>\n>**A1:** Thanks you for your helpful advice. In the revision, we have now included these two highly related papers in the second paragraph of Related Work section (Sec.4) of the main paper. Furthermore, we have now also added a discussion section in Appendix K.4 of the revision to carefully compare our setting and method with respect to related works. In response to your question, we discuss the main differences between our work and TransHP and V2L below:\n>* **TransHP**: We note that the very recent NeurIPS'23 FGVR work TransHP (arXived on 2023-October-23, which is after ICLR'24 deadline) is related to our study. Our method differs from TransHP in two ways: (i) We study the task of **vocabulary-free** FGVR, where neither coarse-grained class names nor the target fine-grained class names are known, unlike TransHP where coarse grained classes are known; (ii) Our method is also **training-free**. The only resources our method has access are few unlabeled images (3 per-class or [1,10] per class) as an observation for discovering the fine-grained class names. Under this realistic vocabulary-free FGVR setting TransHP *cannot operate* due to the lack of class names.\n>\n>\n>* **V2L**: V2L learns and complementarily ensembles vision (ResNeSt, ResNeXt, CotNet, HS-ResNet, NAT, ViT, and ViT) and vision-language models (BLIP, ALBEF, XVLM, METER and SLIP) with the supervision from coarse-grained labels and textual description. Similar to TransHP, the lack of class names and supervisory signals makes V2L inoperable in our vocabulary-free scenario.\n\n\n>-----\n\n>**Q2:** Could you show more examples of Fig. 2? Or any failure cases of Fig. 2?\n>\n>**A2:** In the Appendix Fig. 20 we have provided more examples of Fig. 2. In Fig. 20 we have compared our method with several baselines. We observe that our FineR predicts the correct class more number of times than the baselines.\n>\n>In addition, following your suggestion we have added a thorough qualitative failure case analysis in **Appendix I, Fig. 21**. Specifically, Fig. 21 qualitatively presents the failure cases of our method across each dataset (including five fine-grained datasets and the proposed Pok\u00e9mon dataset) and compares these with vision-language models (BLIP-2, LENS, LLaVA, MiniGPT-4). We observe that most of the baselines fail to correctly identify the finegrained categories. On the contrary our FineR, albeit not being able to predict the exact fine-grained category, it can still capture some fine-grained attributes. For eg., it predicts the dog breed *Brabancon Griffon* as *Brussels Griffon*, demonstrating that it can capture the characteristics of a *Griffon* dog. This suggests that our FineR makes *less severe* mistakes than the baselines. \n>\n>Intriguingly, on the novel Pok\u00e9mon dataset, almost all compared methods predict only the real-world counterparts of the Pok\u00e9mon characters (e.g., 'Squirtle' as 'Turtle' and 'Pidgey' as 'Bird'), highlighting a domain gap between real-world and virtual categories. In contrast, although not completely accurate, our method offers predictions that are closer to the ground truth (evolutionized Pokemon characters, 'Pidgey' as 'Pidgeotto' and 'Squirtle' as 'Wartortle') for the two Pok\u00e9mon objects. This failure case analysis further confirms that our system effectively captures fine-grained visual details from images and leverages them for reasoning. It demonstrates that FineR not only generates more precise and fine-grained predictions when correct but also displays high semantic awareness when it errs."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2720/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700327729027,
                "cdate": 1700327729027,
                "tmdate": 1700327729027,
                "mdate": 1700327729027,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BsVnIVpYLe",
                "forum": "c7DND1iIgb",
                "replyto": "sq0zJfeGHh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2720/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2720/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer oRxX (2/2)"
                    },
                    "comment": {
                        "value": ">-----\n>**Q3:** What is your view of NOVELTY of a work using LLMs without training anything?\n>\n>**A3:** Thank you for your insightful question! Following your suggestion, we have outlined the novel aspects of our work at the end of the Introduction section (Sec. 1) in the revision. Our work, we believe, is novel in several key aspects:\n>* **Innovative Application/Task:** To the best of our knowledge, our work is the first to tackle the novel task of **vocabulary-free FGVR with only a few unlabeled samples as natural observation**. To tackle this novel task we harness the reasoning capabilities of LLMs and leverage the Internet-scale expert knowledge encoded in their weights to first discover the fine-grained categories that are present in a dataset, which is otherwise not know a priori. While CaSED (Conti et al., 2023) previously addressed vocabulary-free classification by retrieving concepts from a knowledge-base, our method is tailored specifically for FGVR by leveraging the knowledge in LLM. This also resonates with the comments from `Reviewer 3hkC` and `Reviewer WYuk` who commented that utilization of LLM for FGVR as \"**a new way**\" and \"**a promising way**\", respectively.\n>\n>* **Methodological Advancements:** We introduced **FineR**, a novel system that integrates the visual information translation capability of VQA models, the reasoning capability of LLMs, and the zero-shot discrimination capability of contrastive VLMs to address the vocabulary-free FGVR task. This approach is recognized by `Reviewer WYuk` as \"**novel**\" and \"**interesting**\". Furthermore, as shown in the preliminary experiments in Fig. 2, we observed that other LLM-based methods requiring training alignments (such as BLIP-2 learning Q-former, LLaVA, and MiniGPT-4 learning a projector) struggle to discover fine-grained categories effectively due to the limitations of alignment data. These training-based methods cannot fully utilize the reasoning capabilities of LLMs. In contrast, our FineR system translates visual information from unlabeled images into natural language in order to fully harness the reasoning power of LLMs for fine-grained concepts. Thus, we showed that visual and textual concepts can be aligned *without* the need of expensive training.\n>\n>* **Resource Efficiency and Accessibility:** Our approach is both training-free and vocabulary-free, significantly reducing the computational and data resources typically required for vision model and LLM alignments. This enhances the accessibility and feasibility of our method for a broader range of users, potentially democratizing the application of FGVR across various sub-fields.\n>\n>* **Scalability and Generalization:** Being training-free enhances the scalability and generalizability of our methods. As demonstrated in Fig. 7 (Section 3.2, page 8) and Fig. 24 (Appendix K.3, page 31), our method can effectively discover wild and novel fine-grained Pok\u00e9mon categories, outperforming other methods that fail in these wild scenario.\n> > We have summarized our novelty and contributions at the end of the Introduction section (Sec. 1) in the revision.\n\nThank you once again for your insightful and helpful advice. We have integrated your suggestions into the revised manuscript, which we believe has led to an improved submission. With the specific changes made in response to your suggestions, such as \"Adding citations of some key related works\" and \"Providing more examples or failure cases for Fig. 2\", we hope these improvements in the revision will encourage you to raise your confidence."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2720/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700327800555,
                "cdate": 1700327800555,
                "tmdate": 1700327800555,
                "mdate": 1700327800555,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wKDNCoxCG4",
                "forum": "c7DND1iIgb",
                "replyto": "BsVnIVpYLe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2720/Reviewer_oRxX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2720/Reviewer_oRxX"
                ],
                "content": {
                    "title": {
                        "value": "Good paper"
                    },
                    "comment": {
                        "value": "I have checked the reviews of all my felllow reviewers and the corresponding rebuttals. The concerns seem to be solved in a satisfactory way. My own concern is also well-addressed. By the way, the last question \u201cWhat is your view of NOVELTY of a work using LLMs without training anything?\u201d is not doubt your novelty, instead, I just want to discuss with you about this aspect, and I appreciate your novelty.\n\nIn conclusion, I agree with the WYuk\u2019s acceptance recommendation, insist my 8 score, and raise the confidence to 5."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2720/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700351609738,
                "cdate": 1700351609738,
                "tmdate": 1700351609738,
                "mdate": 1700351609738,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]