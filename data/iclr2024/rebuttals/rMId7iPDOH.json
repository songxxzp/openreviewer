[
    {
        "title": "Stylist: Style-Driven Feature Ranking for Robust Novelty Detection"
    },
    {
        "review": {
            "id": "MXWhEBAwZN",
            "forum": "rMId7iPDOH",
            "replyto": "rMId7iPDOH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2713/Reviewer_6JHA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2713/Reviewer_6JHA"
            ],
            "content": {
                "summary": {
                    "value": "Novelty detection is a nuanced problem, as one cannot distinguish between novel styles to be generalized and novel contents to be filtered. To distinguish these two cases, this paper considers a multi-environment setup in which the user is aware of multiple datasets with the same content but different styles. Given a pre-trained backbone, the paper introduces a feature selection method called Stylist, which selects the more env-invariant features by computing perturbations across environments. By focusing solely on the env-invariant features, Stylist improves upon previous feature-based novelty detection methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Distinguishing novel styles and contents is an important problem in the literature on uncertainty and robustness. Prior works have mostly focused on a single task, either generalizing to novel styles (i.e., domain generalization) or filtering novel contents (i.e., novelty detection). Bridging the gap between both fields is an important research direction."
                },
                "weaknesses": {
                    "value": "**Limited scope**\n\nThe paper addresses a minor tweak of an existing problem. In doing so, it proposes an intuitive method that can be integrated with existing techniques, resulting in a clear improvement over the baseline.\nWhile tackling a niche problem can be an easy way to write a paper, I believe that in representative venues like ICLR, the focus should be on addressing the core of the problem rather than opting for low-hanging fruit.\n\nThe authors could explore more impactful problems. For example, a single scalar uncertainty metric fails to differentiate between novel styles and contents. The proposed Stylist can be viewed as introducing a two-dimensional uncertainty to address this limitation.\nBuilding on this concept, the authors might consider developing a unified framework that addresses both domain generalization and novelty detection in real-world situations.\n\n---\n**Comparison with invariant learning**\n\nThe paper aims to select domain-invariant features in a post-hoc manner. However, features can also be trained to be invariant using methods such as IRM.\nA comparison between the proposed post-hoc feature selection and invariant learning is necessary, considering both domain generalization (tables in IRM) and novelty detection (tables in this paper).\n\n---\n**CLIP should be the default backbone choice**\n\nIn the methods section, the paper states, \"our approach leverages pretrained embeddings with extensive coverage across various content and style categories.\"\nHowever, the paper uses ResNet-18 pretrained on ImageNet for the main results, which does not align with the statement.\n\nThe paper should primarily consider models like CLIP, which are trained on diverse datasets and known to be robust to domain shifts.\nThe paper presents the CLIP results in Table 2, and the performance gap is significantly lower than that of the (domain-variant) ResNet.\nThus, the overall benefits of the proposed method may be exaggerated, and it would be more appropriate to conduct the main results using the less domain-variant models like CLIP.\n\n---\n**Hyperparameter selection**\n\nThe OOD detection competitors considered in the paper apply simple methods, such as OCSVM or kNN, on top of a fixed representation.\nGiven that this paper refines the representation, it is unsurprising that the proposed method enhances the original representation when appropriate hyperparameters are selected.\n\nFurthermore, how were the hyperparameters selected? It seems they were chosen for optimal performance in the reported table.\nHowever, (1) hyperparameters should not be spoiled by the test cases, and (2) the OOD detection method should be robust to unseen samples.\nThe correct approach is to select hyperparameters in a validation domain and evaluate their performance on novel test domains.\n\n---\n**Dataset contribution is marginal**\n\n(1) The necessity of COCOShift95 over prior works is unclear.\\\nThe paper introduces a new benchmark called COCOShift95, where objects are cut and shifted to different backgrounds.\nConsequently, this benchmark is artificial and does not demand substantial effort, unlike previous works such as fMoW and DomainNet, which require considerable effort to collect realistic images.\nWhat new insights can be gained from COCOShift95, aside from adding yet another column to the table?\n\n(2) The cut and paste strategy has been considered in prior works.\\\nThe approach of creating an artificial dataset through cut and paste has been proposed in previous works, such as Waterbirds and Background Challenge.\nAdditionally, why not use more natural benchmarks, such as MetaShift, which also include multi-domain images from COCO?\nTherefore, the technical contribution of the proposed dataset collection strategy is also not convincing.\n\n---\n**Nitpicks**\n\n- \"+0.0\" should not be highlighted in green in Table 3. It exaggerates the benefit of the proposed method.\n- \"Stylist\" seems to be too ambiguous as a name for the feature selection method in multi-domain novelty detection."
                },
                "questions": {
                    "value": "1. Why posthoc feature selection instead of using an invariant backbone like IRM or CLIP?\n2. Were the hyperparameters chosen from the val set? Are they generalizable to unseen domains?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2713/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698253430913,
            "cdate": 1698253430913,
            "tmdate": 1699636213610,
            "mdate": 1699636213610,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "t8HgOAFSwB",
                "forum": "rMId7iPDOH",
                "replyto": "MXWhEBAwZN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2713/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2713/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 6JHA - part 1"
                    },
                    "comment": {
                        "value": "Thank you for your detailed comments and helpful insights. We have addressed the raised points in the comments below. \n\n**W1. Limited scope**\nRespectfully, we don\u2019t consider the Stylist approach or its setup a minor tweak or a low-hanging fruit, but on the contrary, it could help address really impactful problems. \n\nFirst, If the reviewer referred to the downstream task impact:\n\n***a) Robust Novelty Detection (ND) task***: Going from ND towards Robust ND does not create a niche, but quite the contrary, extending the solutions that work only in isolated/well-controlled lab scenarios (where the style distribution remains stable at test time) towards real-life scenarios ( where this is almost never the case). Our solution is particularly suitable for this task because it allows us to define what a novelty is, in an implicit way, by excluding what it is not (the style), in a very generic way and weakly supervised manner. As opposed to this, for instance, a supervised downstream task would focus on defining the content, rather than defining the non-content (the style). Such an approach requires lots of supervision and focus on covering all the variations for the content aspects.\n\nSecondly, if the reviewer referred to the main contribution impact, there is a large applicability to the idea:\n\n***b) Biases***: Using Stylist, one can identify biases in a dataset, given a pre-trained representation by interpreting the environment-biased features. This could lead to better datasets in the field, and better understanding and discovery of the existing biases from commonly used data.\n\n***c) Implicit Robustness***: Using Stylist, one can define the \u201cspace\u201d for robustness in an implicit way, through environments (rather than doing it explicitly, with augmentations for instance). This leads to far more general and robust algorithms.\n\nFor both b) and c) scenarios, you need a way to define (label) or find (maybe in an unsupervised way) intrinsic environments from the dataset.\n\n**W2 and Q1. Comparison with invariant learning**\n\n***[Comparison with invariant learning]*** This is a very good point raised by the reviewer. There is a very important difference in the setup between the existing methods for learning features that are invariant to the environment (like IRM) and our Stylist:\n\n***a)*** For computing the invariant features, all the others need both semantic (content) labels (namely content classes) and environment labels, while our Stylist only needs the environment labels. We argue that it is a huge effort to have those semantic labels, similar to the supervised vs unsupervised differences.\n\n***b)*** Indeed, to continue the argument, one may say that we use pre-trained models. And this means we also need labels at some point. So, to balance the need for semantic labels, we can make the IRM also use pre-trained models this way: one can learn IRM to predict labels that a pre-trained method gives for the current dataset. But this is a tricky step, because it might result in a lot of knowledge loss, since for training the IRM, you will go beyond the pre-trained features, to a final target class that is not necessarily mapped over the new dataset. This way, the final supervision will greatly influence the representations, especially if there is not a 100% alignment between your new dataset and the pre-trained model training dataset (and this is a common case).\n\n***c)*** Nevertheless, since semantic labels are available in our used datasets, we did an experiment that focuses on the complementarity between the methods, not on their differences (we explained before why this direct comparison is not fair): We first trained IRM on those semantic labels. Next, we applied Stylist on top of them. Interestingly, we see a 1% increase over using 100% of the features, showing that the two methods are somehow complementary.\n\n***IRM*** - Invariant Risk Minimization: Martin Arjovsky, L\u00e9on Bottou, Ishaan Gulrajani, David Lopez-Paz"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2713/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700578501916,
                "cdate": 1700578501916,
                "tmdate": 1700578776318,
                "mdate": 1700578776318,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "XRqWYGuQHq",
            "forum": "rMId7iPDOH",
            "replyto": "rMId7iPDOH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2713/Reviewer_gWKH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2713/Reviewer_gWKH"
            ],
            "content": {
                "summary": {
                    "value": "This paper considers a robust novelty detection problem of finding semantic changes while being robust to style distributional shifts. The authors propose a feature selection method, which ranks the features by evaluating the differences of features across domains to achieve invariance, thereby removing the influence of spurious correlations. In the experiments, the effectiveness of the proposal is validated where the feature selection improves the subsequent novel detection algorithms under distribution shift."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The problem presented is important and highly relevant to the machine learning community. The proposal is straightforward and technologically feasible. \n2. The proposed feature selection can be combined with different novel detection algorithms and improve their robustness when facing distribution shifts. The authors also demonstrate that the proposal can effectively select environment-invariant features.\n3. The overall presentation is well-organized and easy to follow."
                },
                "weaknesses": {
                    "value": "Important references are missing. There have been some works on how to achieve novelty detection in the case of domain shift, which is the same as this paper. For example:\n1. Open domain generalization with domain-augmented meta-learning. CVPR 2021\n2. Open-set learning under covariate shift. ML 2022\n3. Open-Set Single Domain Generalization. ICLR 2022  \nThese works all consider open-set learning/novelty detection under distribution shift. Moreover, the ML'22 and ICLR\u201922 works could improve cross-domain generalization without the requirement of multiple domains.  \n\nThe previous works mainly focus on improving model robustness from the perspective of representation learning, while the current work assumes that the representation has already been well learned and mainly focuses on feature selection. \n\nHow does the author consider the sufficiency of representation capacity? Can we assume that the representation has been sufficiently learned after thorough across-domain representation learning? Will it still learn spurious correlations? Comparison with these works, especially the specific clarification of the applicable scope for this work, will further improve this manuscript."
                },
                "questions": {
                    "value": "1. Differences with existing work and the rationale behind the assumptions made in the current work. [See weakness part]\n2. In practice, how to determine the proportion of selected features? As shown in Figure 3. An inappropriate number of selected features can lead to a drop in ROC-AUC."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2713/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2713/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2713/Reviewer_gWKH"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2713/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698586733387,
            "cdate": 1698586733387,
            "tmdate": 1699636213542,
            "mdate": 1699636213542,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mZOK23nGpc",
                "forum": "rMId7iPDOH",
                "replyto": "XRqWYGuQHq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2713/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2713/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer gWKH - part 1"
                    },
                    "comment": {
                        "value": "Thank you for the detailed analysis of our work. Below we address each of the raised concerns. \n\n**Q1 - W1.  Important references are missing. There have been some works on how to achieve novelty detection in the case of domain shift, which is the same as this paper. For example:**\n\n* **Open domain generalization with domain-augmented meta-learning. CVPR 2021**\n\n* **Open-set learning under covariate shift. ML 2022**\n\n* **Open-Set Single Domain Generalization. ICLR 2022**\n\n**These works all consider open-set learning/novelty detection under distribution shift. Moreover, the ML\u201922 and ICLR\u201922 works could improve cross-domain generalization without the requirement of multiple domains**\n\nThis is a very important observation, we address it below (we refer to those methods in the revised paper).\n\n***[open set vs novelty detection]*** Open-set (OS) and novelty detection (ND) are strongly related, but also different in some key aspects. OS requires more supervision than ND, as during training, OS has access to the semantic labels of the normal/known data. Meanwhile, ND observes during training the set of normal classes, without having access to their specific content labels. OS is typically approached in a supervised learning context, while ND methods are often employed in an unsupervised context.  The indicated papers are not an exception, requiring annotated semantic labels for the training set. CVPR\u201921 optimizes a meta-learning objective that requires semantic labels. ML\u201922 requires the target concepts in order to identify the semantic and environmental parts of the image for the EnvMix step. ICLR\u201922 employs semantic labels for the minimization stage and implicitly they are used for increasing the diversity of the source domain and the generation of auxiliary unknown class samples. \n\n***[requirement of multiple domains]*** Our Stylist employs the multi-environment setup as a way of defining the style of the images, namely the covariate shifts that should be ignored. Our algorithm is flexible and not restricted to a fixed definition of style and content. Content should not be understood as elements of WordNet hierarchy, but rather as what is constant between environments. Similarly, we define the style as the elements that vary between environments. In this setup, the absence of multiple training domains means the absence of a defined problem. \n\n***CVPR\u201921*** Open Domain Generalization with Domain-Augmented Meta-Learning. Yang Shu, Zhangjie Cao, Chenyu Wang, Jianmin Wang, Mingsheng Long, CVPR 2021\n\n***ML\u201922*** Open-set learning under covariate shift. Jie-Jing Shao, Xiao-Wen Wang, Lan-Zhe Guo, Machine Learning 2022\n\n***ICLR\u201922*** CrossMatch: Cross-Classifier Consistency Regularization for Open-Set Single Domain Generalization. Ronghang Zhu, Sheng Li, ICLR 2022\n\n**Q1 (and W2). The previous works mainly focus on improving model robustness from the perspective of representation learning, while the current work assumes that the representation has already been well learned and mainly focuses on feature selection.  How does the author consider the sufficiency of representation capacity? Can we assume that the representation has been sufficiently learned after thorough across-domain representation learning? Will it still learn spurious correlations? Comparison with these works, especially the specific clarification of the applicable scope for this work, will further improve this manuscript.**\n\n***[sufficiency of representation capacity]*** We rely on the existence of a pre-trained representation that captures relevant features for both known and unknown content and style. Considering the availability of heavily pre-trained models and scenarios like the use case illustrated below, such a representation can be easily accessible. As we are focusing on a flexible definition of style and content, an ideal representation will evenly capture elements of style and content.\n\n***[will it still learn spurious correlations]*** In the unsupervised setup of novelty detection, during training, we observe normal samples in a few training environments. Assuming a perfectly balanced dataset, the ND algorithm will immediately correlate the normal samples with the seen training environments as it does not have access to semantic labels, as in the case of open-set recognition, to guide the learning process towards semantic features. Also, we can deal with biased datasets, which can further enhance this effect. In this context, the feature ranking strategy employed by Stylist will ensure that the novelty detection algorithm only has access to the relevant features, while ignoring environment-biased ones."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2713/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700577422213,
                "cdate": 1700577422213,
                "tmdate": 1700577446586,
                "mdate": 1700577446586,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "6z8oKRWqve",
            "forum": "rMId7iPDOH",
            "replyto": "rMId7iPDOH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2713/Reviewer_Diep"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2713/Reviewer_Diep"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses the problem of detecting novel environments in machine learning models, which is important for ensuring their robustness and reliability. The authors note that existing methods for novelty detection often rely on the assumption that the training and testing data come from the same distribution, which is not always the case in real-world scenarios. They also point out that previous work has focused on either semantic or style changes, but not both, and that there is a need for a method that can separate these two types of changes and detect novel environments based on semantic changes while ignoring style changes.\n\nTo address this problem, the authors propose a novel method called Stylist, which leverages pretrained embeddings to separate semantic and style changes and rank features based on their relevance for detecting novel environments. The authors note that their approach is different from previous work in that it focuses on feature ranking rather than domain adaptation or anomaly detection. They also highlight the importance of identifying environment-biased features that contain spurious correlations and should be ignored for novelty detection.\n\nThe authors' contributions include a formalization of the problem of detecting novel environments based on semantic and style changes, a feature ranking approach that focuses on dropping environment-biased features, and an evaluation of their method on several benchmark datasets. They also provide insights into the impact of pretrained feature extractors and the potential applications of their method beyond novelty detection."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. One of the strengths of Stylist is its ability to identify environment-biased features that contain spurious correlations and should be ignored for novelty detection. This is an important contribution, as spurious correlations can lead to false positives and negatively impact the reliability of machine learning models. By removing these features, Stylist can improve the generalization performance of novelty detection algorithms and make them more robust to changes in the environment.\n\n2. Another strength of Stylist is its potential for interpretability. By ranking features based on their relevance for detecting novel environments, Stylist can provide insights into which features are important for the task at hand and which ones are not. This can be useful for understanding the behavior of machine learning models and for identifying areas for improvement."
                },
                "weaknesses": {
                    "value": "1.  While the authors' approach of leveraging pretrained embeddings to separate semantic and style changes is innovative, the overall contribution of the paper may not be novel enough to warrant publication in a top-tier conference or journal. The authors could strengthen their contribution by providing more evidence of the novelty of their approach and by comparing it with other state-of-the-art methods for novelty detection.\n\n2.  While the authors have provided some results on synthetic and real-world datasets, the evaluation of their method could be more comprehensive. Specifically, the authors could provide more details on the experimental setup, such as the choice of hyperparameters and the number of trials, to ensure that their results are reproducible. Additionally, the authors could compare their method with other state-of-the-art methods for novelty detection to better understand its strengths and weaknesses.\n\n3. The paper focuses on the problem of detecting novel environments based on semantic and style changes, but it does not address other important aspects of novelty detection, such as temporal changes or changes in the data distribution over time. The authors could expand the scope of their work to address these other aspects of novelty detection and provide a more comprehensive solution to the problem."
                },
                "questions": {
                    "value": "1. The authors mention that their method focuses on feature ranking rather than domain adaptation or anomaly detection, but it is not clear how this approach is fundamentally different from other methods. Could the authors provide more details on how their method differs from existing methods and what makes it unique?\n\n2. The authors could provide more details on the experimental setup, such as the choice of hyperparameters and the number of trials, to ensure that their results are reproducible. Additionally, the authors could compare their method with other state-of-the-art methods for novelty detection to better understand its strengths and weaknesses. Could the authors provide more details on the experimental setup and the comparison with other methods?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2713/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698720013386,
            "cdate": 1698720013386,
            "tmdate": 1699636213468,
            "mdate": 1699636213468,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2zQrvbpii8",
                "forum": "rMId7iPDOH",
                "replyto": "6z8oKRWqve",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2713/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2713/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Diep - part 1"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the observations and for pointing out ways to improve the analysis of our work. Below we address each of your comments and we have included the additional ablations in the supplementary material.\n\n**W3. The paper focuses on the problem of detecting novel environments based on semantic and style changes, but it does not address other important aspects of novelty detection, such as temporal changes or changes in the data distribution over time. The authors could expand the scope of their work to address these other aspects of novelty detection and provide a more comprehensive solution to the problem.**\n\nThis is a very interesting point raised by the reviewer. The proposed approach is flexible to the definition of style and content. Style should not be understood solely as characteristics that separate for example sketches, real images, or paintings (as in DomainNet), or specific background characteristics like water, sand, or forest (as in Waterbirds, or fMoW with regions environments), but rather as the general covariate shift that we wish to be robust to (as temporal shifts that you mentioned).\n\nOur Stylist method does not make any other assumption on what the style is, the only way to provide a definition for what is the style, is the multi-environment setup considered in our work: what is changing between environments is irrelevant for the final task, and we call it style. So, if our training environments are composed of aerial images of a city observed over a long period of time, the style will capture those changes that occur over time. \nMoreover, we did a test to emphasize your proposed temporal shift use-case. First, we applied Stylist for the first 5 years of fMoW. Next, we observed the ND performance using our solution on a later year (with a visible temporal shift). The ROC-AUC improvement is around 1%, when compared with using all the features, emphasizing that our method works for different types of styles, specified and embedded as supervision signals from the multi-environment labels setup.\n\n\n**Q1 (and W1) - The authors mention that their method focuses on feature ranking rather than domain adaptation or anomaly detection, but it is not clear how this approach is fundamentally different from other methods. Could the authors provide more details on how their method differs from existing methods and what makes it unique?**\n\nWe have completed the comparative analysis from our manuscript tackling the following points:\n\n***[Robust Novelty Detection (our task) vs. Anomaly Detection]*** While anomaly detection is focused on identifying any abnormal or previously unseen elements, robust novelty detection focuses on identifying content distribution changes, while ignoring style changes that may appear. Considering the first example from Figure 1 of our manuscript, in anomaly detection, the sketch of the banana should be identified as an anomaly, because the style change is still considered an anomaly. In Robust Novelty Detection, the banana is classified as a normal class, because we should be robust to style changes.\n\n***[Robust Novelty Detection (our task) vs. Open Set Recognition]*** Open-set (OS) and novelty detection (ND) are strongly related, but also different in some key aspects. OS requires more supervision than ND, as during training, OS has access to the semantic labels of the normal / known data. Meanwhile, ND observes during training the set of normal classes, without having access to their specific content labels. OS is typically approached in a supervised learning context, while ND methods are often employed in an unsupervised context.\n\n***[Stylist (ours) vs. Domain Adaptation]*** Stylist is rather a domain generalization technique, removing environment-biased features and in consequence building representations that are robust to covariate shifts. Different from a domain adaptation technique (like TENT or TTT-MAE), Stylist does not require access to unlabeled target environment (test) data in order to compute the feature ranking."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2713/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700576796381,
                "cdate": 1700576796381,
                "tmdate": 1700576796381,
                "mdate": 1700576796381,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3BoalhdaMu",
                "forum": "rMId7iPDOH",
                "replyto": "6z8oKRWqve",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2713/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2713/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Diep - part 2"
                    },
                    "comment": {
                        "value": "***[Stylist (ours) vs. Domain Generalization Approaches]***\n***Similar*** to domain generalization techniques, we work in a multi-environment setup. During training, we observe data from at least two different environments and we implicitly define the style of our data as the covariate shift observed between those training environments. This can be seen as a way of defining the shifts that we want to be robust to. This way we give flexibility to our algorithm, providing the signal for robustness only from the environment labels. ***Different*** from other generalization approaches like IRM and LISA, we do not require access to both environments and semantic labels for our in-distribution training data. Our Stylist only requires style labels in order to compute the feature ranking. \n\n***TENT*** - Fully Test-Time Adaptation by Entropy Minimization. Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, Trevor Darell,. ICLR 2021 \n\n***TTT-MAE*** - Test-Time Training with Masked Autoencoders. Yossi Gandelsman, Yu Sun, Xinlei Chen, Alexei Efros, NeurIPS 2022\n\n***IRM*** - Invariant Risk Minimization: Martin Arjovsky, L\u00e9on Bottou, Ishaan Gulrajani, David Lopez-Paz\n\n***LISA*** - Improving Out-of-Distribution Robustness via Selective Augmentation: Huaxiu Yao, Yu Wang, Sai Li, Linjun Zhang, Weixin Liang, James Zou, Chelsea Finn, ICML 2022\n\n**Q2 (and W2) - The authors could provide more details on the experimental setup, such as the choice of hyperparameters and the number of trials, to ensure that their results are reproducible... Could the authors provide more details on the experimental setup and the comparison with other methods?**\n\n***[experimental setup - number of trials]*** Stylist does not involve a random component and its results are reproducible given the same set of hyperparameters, as discussed below. For clarity, we have attached the code.\n\n***[experimental setup - hyperparameters]*** Stylist is robust to the choice of hyperparameters. We emphasize the constant improvement illustrated below. \n\nHyperparameters:\n\n* ***The feature extractor:***\nIn Table 2 of our manuscript we provide an analysis of the performance w.r.t. the considered feature extractor. Our Stylist, improves the performance regardless of the selected feature extractor. \n\n* ***The distance metric*** and the ***combination between pairwise distances*** employed in Step1 of our algorithm:\nIn Figure 7 of our manuscript we provide an ablation study regarding this matter. We highlight the performance improvement of our Stylist, irrespective of the considered configuration. \n\n* ***The percent of selected features***, for Step2 of our algorithm:\nAs highlighted in Figure 3, Stylist consistently improves over the baseline w.r.t. the percent of considered features, proving that the provided feature ranking is relevant for the novelty detection problem. To select an optimal percent of features per setup, we employ a validation step, analyzing either the performance on an ID validation set or the performance on an OOD test set. As highlighted in the table below, there is a very small performance variance between those techniques (< 0.07 for ResNet-18 features, and even 0 for CLIP features), highlighting that the performance is stable. This is an important property of our algorithm, managing to improve OOD performance, with ID chosen hyperparameters."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2713/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700576894212,
                "cdate": 1700576894212,
                "tmdate": 1700577535039,
                "mdate": 1700577535039,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "x9JTV1DdQJ",
                "forum": "rMId7iPDOH",
                "replyto": "6z8oKRWqve",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2713/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2713/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Diep - part 3"
                    },
                    "comment": {
                        "value": "* ***Novelty detection algorithm***\nTable 1 shows the impact of our Stylist on different novelty detection algorithms. It showcases a consistent improvement over baselines in almost all scenarios.  \nFor our default novelty detection algorithm, which is kNN, we provide below an ablation regarding the choice of the number of neighbors (k). We highlight the robustness of the approach w.r.t. k, as the variance is in range [0.002, 0.004] for CLIP and in range [0.013, 0.160] for ResNet-18 features.\n\n\n|Dataset | Features | Optimal nr. of features selection approach | k | ROC-AUC all feat. | ROC-AUC Stylist feat. | % selected feat. |\n|--|--|--|--|--|--|--|\n|COCOShift_balanced | CLIP | Based on ID val set | 5 | 94.96 | 95.29 (+0.33) | 95  |\n| | | | 10 | 95.07 | 95.38 (+0.31) | 95 |\n| | | | 20 | 95.08 | 95.41 (+0.33) | 95 |\n| | | | 30 | 95.06 | 95.40 (+0.34) | 95 |\n| | | | **Variance w.r.t. k** | **0.003** | **0.003** | - |\n| | | Based on OOD test set | 5 | 94.96 | 95.29 (+0.33) | 95  |\n| | | | 10 | 95.07 | 95.38 (+0.31) | 95 |\n| | | | 20 | 95.08 | 95.41 (+0.33) | 95 |\n| | | | 30 | 95.06 | 95.40 (+0.34) | 95 |\n| | | | **Variance w.r.t. k** | **0.003** | **0.003** | - |\n| | | **Variance w.r.t. features selection approach** | - | -| **0** | - |\n| | ResNet-18 | Based on ID val set | 5 | 81.51 | 83.58 (+2.07) | 10  |\n| | | | 10 | 81.36 | 83.51 (+2.15) | 10 |\n| | | | 20 | 81.12 | 83.36 (+2.24) | 10 |\n| | | | 30 | 80.93 | 83.24 (+2.31) | 10 |\n| | | | **Variance w.r.t. k** | **0.065** | **0.023**  | - |\n| | | Based on OOD test set | 5 | 81.51 | 84.08 (+2.57)  | 40  |\n| | | | 10 | 81.36 | 83.92 (+2.56) | 40 |\n| | | | 20 | 81.12 | 83.68 (+2.56) | 40 |\n| | | | 30 | 80.93 | 83.49 (+2.56) | 40 |\n| | | | **Variance w.r.t. k** | **0.065** | **0.069** | - |\n| | | **Variance w.r.t. features selection approach** | - |- |**0.068** |- |\n|COCOShift75 | CLIP | Based on ID val set | 5 | 94.77 | 95.11 (+0.34) | 95  |\n| | | | 10 | 94.88 | 95.22 (+0.34) | 95 |\n| | | | 20 | 94.91 | 95.25 (+0.34) | 95 |\n| | | | 30 | 94.88 | 95.21 (+0.33) | 95 |\n| | | | **Variance w.r.t. k** | **0.004** | **0.003** | - |\n| | | Based on OOD test set | 5 | 94.77 | 95.11 (+0.34) | 95  |\n| | | | 10 | 94.88 | 95.22 (+0.34) | 95 |\n| | | | 20 | 94.91 | 95.25 (+0.34) | 95 |\n| | | | 30 | 94.88 | 95.21 (+0.33) | 95 |\n| | | | **Variance w.r.t. k** | **0.004** | **0.003** | - |\n| | | **Variance w.r.t. features selection approach** | - | - |**0** |- |\n| | ResNet-18 | Based on ID val set | 5 | 81.10 | 84.96 (+3.86) | 10  |\n| | | | 10 | 80.91 | 84.90 (+3.99) | 10 |\n| | | | 20 | 80.62 | 84.79 (+4.17) | 10 |\n| | | | 30 | 80.40 | 84.71 (+4.31) | 10 |\n| | | | **Variance w.r.t. k** | **0.096** | **0.013** | - |\n| | | Based on OOD test set | 5 | 81.10 | 84.96 (+3.86) | 10  |\n| | | | 10 | 80.91 | 84.90 (+3.99) | 10 |\n| | | | 20 | 80.62 | 84.79 (+4.17) | 10 |\n| | | | 30 | 80.40 | 84.71 (+4.31) | 10 |\n| | | | **Variance w.r.t. k** | **0.096** | **0.013** | - |\n| | | **Variance w.r.t. features selection approach** | - | - | **0** | - |\n|COCOShift95 | CLIP | Based on ID val set | 5 | 94.59 | 94.97 (+0.38) | 95  |\n| | | | 10 | 94.65 | 95.02 (+0.37) | 95 |\n| | | | 20 | 94.61 | 94.99 (+0.38) | 95 |\n| | | | 30 | 94.53 | 94.92 (+0.39) | 95 |\n| | | | **Variance w.r.t. k** | **0.002** | **0.002** | - |\n| | | Based on OOD test set | 5 | 94.59 | 94.97 (+0.38) | 95  |\n| | | | 10 | 94.65 | 95.02 (+0.37) | 95 |\n| | | | 20 | 94.61 | 94.99 (+0.38) | 95 |\n| | | | 30 | 94.53 | 94.92 (+0.39) | 95 |\n| | | | **Variance w.r.t. k** | **0.002** | **0.002** | - |\n| | | **Variance w.r.t. features selection approach** | - |- |**0** |- |\n| | ResNet-18 | Based on ID val set | 5 | 80.72 | 85.53 (+4.81) | 25  |\n| | | | 10 | 80.48 | 85.38 (+4.90) | 25 |\n| | | | 20 | 80.11 | 85.16 (+5.05) | 25 |\n| | | | 30 | 79.82 | 85.01 (+5.19) | 25 |\n| | | | **Variance w.r.t. k** | **0.160** | **0.052** | - |\n| | | Based on OOD test set | 5 | 80.72 | 85.66 (+4.94) | 30  |\n| | | | 10 | 80.48 | 85.51 (+5.03) | 30 |\n| | | | 20 | 80.11 | 85.29 (+5.18) | 30 |\n| | | | 30 | 79.82 | 85.13 (+5.31) | 30 |\n| | | | **Variance w.r.t. k** | **0.160** | **0.054** | - |\n| | | **Variance w.r.t. features selection approach** | - | - | **0.008** | - |"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2713/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700576933145,
                "cdate": 1700576933145,
                "tmdate": 1700577553887,
                "mdate": 1700577553887,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "41qvr5M81g",
            "forum": "rMId7iPDOH",
            "replyto": "rMId7iPDOH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2713/Reviewer_guaR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2713/Reviewer_guaR"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on novelty detection studies. Specifically, the authors want to improve the robustness of the novelty detection method to the spurious relations. To this end, the authors a new feature selection method called Stylist. The method ranks features according to the distance between different environments and selects features according to the rank."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1) The idea is technically sound. The proposal first rank features according to their distance between environments and remove features responsible for spurious correlations to improve the robustness."
                },
                "weaknesses": {
                    "value": "1) It is not clear how to compute the Eq.(1) and Eq.(2). For example, how to construct different environments? How to split the features extracted from a pre-trained model into $N$ dimensions? It seems that if the pre-trained feature is very large, the computational cost is expensive.\n2) In the experiments, the authors simply compare the proposal with the feature selection method and novelty detection method. However, there are some algorithms related to spurious relations that can be easily adapted to solve the novelty detection problem. Moreover, the adopted novelty detection methods are not SOTA. So, the experiment results are not convincing. More related methods should be discussed.\n3) In my view, the spurious problems can be easily addressed with data augmentation. So I think maybe there exist easier methods to solve the problem in this paper."
                },
                "questions": {
                    "value": "As discussed above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2713/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2713/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2713/Reviewer_guaR"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2713/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698831104200,
            "cdate": 1698831104200,
            "tmdate": 1699636213262,
            "mdate": 1699636213262,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3LSMcgDmX2",
                "forum": "rMId7iPDOH",
                "replyto": "41qvr5M81g",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2713/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2713/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer guaR - part 1"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the observations, we tried to better explain the raised points and will update them in the paper to make it easier to follow.\n\n**W1. How to construct different environments? How to split the extracted features into N dimensions? The computational cost is expensive.**\n\n***[environments]*** For the current version of our method, we need to have labels for the environments, so we don\u2019t build them, we consider them as given. In the generalization field, there are a lot of works that are based on having annotations for multiple training distributions (=our environments), like IRM, LISA, or all the other methods that report on the WILDS benchmark.\n\n***[extracted features]*** We think there is a misunderstanding here, we don\u2019t split the features into N dimensions, but simply take the entire pre-trained features (e.g. ResNet18, with 512 features) and decide for each feature (of all 512 features), with our method, how much it varies between different environments.\n\n***[computational cost]*** Quite the opposite, the computational impact is really low. For each feature, we only need to compute the inter-environments Wasserstein distances, which is extremely fast (for instance, for 10k samples, split into 5 environments, it takes ~ 4 seconds on a single CPU, with no optimizations whatsoever, since this was not our focus). All those operations can also be easily parallelized on CPUs. For more details on the implementation, we also attach the code to the submission.\n\n***IRM*** - Invariant Risk Minimization: Martin Arjovsky, L\u00e9on Bottou, Ishaan Gulrajani, David Lopez-Paz\n\n***LISA*** - Improving Out-of-Distribution Robustness via Selective Augmentation: Huaxiu Yao, Yu Wang, Sai Li, Linjun Zhang, Weixin Liang, James Zou, Chelsea Finn\n\n***WILDS*** - https://wilds.stanford.edu/datasets/ \n\n&nbsp;\n\n**W2. Comparison with other spurious algorithms and SOTA for Novelty Detection. More related methods should be discussed.**\n\n***[Comparison with invariant learning (or other spurious-related algorithms)]***\nThere is a very important difference in the setup between the existing methods for learning features that are invariant to the environment (like IRM) and our Stylist:\n\n***a)*** For computing the invariant features, all the others need both semantic (content) labels (namely content classes) and environment labels, while our Stylist only needs the environment labels. We argue that it is a huge effort to have those semantic labels, similar to the supervised vs unsupervised differences.\n\n***b)*** Indeed, to continue the argument, one may say that we use pre-trained models. And this means we also need labels at some point. So, to balance the need for semantic labels, we can make the IRM also use pre-trained models this way: one can learn IRM to predict labels that a pre-trained method gives for the current dataset. But this is a tricky step, because it might result in a lot of knowledge loss, since for training the IRM, you will go beyond the pre-trained features, to a final target class that is not necessarily mapped over the new dataset. This way, the final supervision will greatly influence the representations, especially if there is not a 100% alignment between your new dataset and the pre-trained model training dataset (and this is a common case).\n\n***c)*** Nevertheless, since semantic labels are available in our used datasets, we did an experiment that focuses on the complementarity between the methods, not on their differences (we explained before why this direct comparison is not fair): We first trained IRM on those semantic labels. Next, we applied Stylist on top of them. Interestingly, we see a 1% increase over using 100% of the features, showing that the two methods are somehow complementary.\n\n***IRM*** - Invariant Risk Minimization: Martin Arjovsky, L\u00e9on Bottou, Ishaan Gulrajani, David Lopez-Paz"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2713/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700575503238,
                "cdate": 1700575503238,
                "tmdate": 1700576686301,
                "mdate": 1700576686301,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RbUCCSgIyh",
                "forum": "rMId7iPDOH",
                "replyto": "41qvr5M81g",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2713/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2713/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer guaR - part 2"
                    },
                    "comment": {
                        "value": "***[SOTA for Novelty Detection]*** Our method is for ***a) feature selection***, while the downstream task is ***b) Novelty Detection***, analysed in the setup where the data is shifting.\n\n***b)*** Indeed, we do not compare against SOTA solutions for Novelty Detection, since they have no focus on the distribution shift of the input data, unlike in our case, and the comparison would be pointless. The most similar setup we found for ND in out-of-distribution, is the Out-of-distribution detection, because there is a shift in data and for a good performance, the model should be able to detect it. So we compare against the SOTA for this task (Out-of-distribution detection), called kNN+. We also add to the comparison with classic Novelty Detection solutions, that could be easily adapted for our setup (see Table 1).\n\n***a)*** Consequently, we also compared against other feature selection approaches, basic and adapted for multiple-environments (env-aware) (see Figure 6).\n\n***kNN+*** - Out-of-Distribution Detection with Deep Nearest Neighbors: Yiyou Sun, Yifei Ming, Xiaojin Zhu, Yixuan Li\n\n\n***[More related methods]*** The reviewer does not specify exactly the related methods to be discussed. We add in the revised paper **Open-Set methods** and **Domain adaptation** ones (see related work). We have discussed already in the experimental section **Feature selection** and **Novelty detection** algorithms, and in the related work, **spurious/invariant learning** methods. \n\n&nbsp;\n\n**W3. The spurious problems can be easily addressed with data augmentation**\nSince it might be true that with the proper augmentations for each task and dataset, you might address some of the bias problems (like ImageNet has a bias that the main objects are always in the center), others can\u2019t be addressed by this, e.g.:\n\n* If cows are always on the grass background in training, how can one augment the data to prepare it for other kinds of backgrounds? Just by first identifying the exact problem and building the exact custom augmentations on synthetic data (if you can for instance do a proper instance segmentation). But this is way more complicated and hand-designed than our proposed solution.\n\n* Another case is when the bias is distributed across the image, like CT image devices in hospitals that have different acquisition parameters that influence the entire image. This makes it impossible to augment in a generic way, since you don\u2019t know how data will look for the next hospital. So you will need to see the \u201ctest\u201d data. With our Stylist approach, you might figure out the features that are responsible for the acquisition parameters beforehand.\n\nMore, please note that the best augmentations to make for a scenario, are task and data-dependent, as shown in ERM w/ targeted aug. Also, please take into account that there is a whole subfield of ML research regarding spurious correlations and biases.\n\n***ERM w/ targeted aug***: Out-of-Distribution Robustness via Targeted Augmentations: Irena Gao, Shiori Sagawa, Pang Wei Koh, Tatsunori Hashimoto, Percy Liang"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2713/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700575650626,
                "cdate": 1700575650626,
                "tmdate": 1700577054441,
                "mdate": 1700577054441,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]