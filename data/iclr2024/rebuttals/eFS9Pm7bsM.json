[
    {
        "title": "Adversarial Latent Feature Augmentation for Fairness"
    },
    {
        "review": {
            "id": "aOGlIE82dC",
            "forum": "eFS9Pm7bsM",
            "replyto": "eFS9Pm7bsM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7792/Reviewer_CWiz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7792/Reviewer_CWiz"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes the Adversarial Latent Feature Augmentation (ALFA) which merges adversarial attacks into data augmentation in the latent space to promote fairness. The paper points out that adversarial perturbation can in fact be used to augment fairness. Through covariance-based fairness constraint, this paper unveils a counter-intuitive relationship between adversarial attacks against fairness and enhanced model fairness upon training with the resultant perturbed latent features by hyperplane rotation. The paper theoretically proves that the proposed adversarial fairness objective assuredly generates biased feature perturbation, and empirically validate that training with adversarial features significantly improve fairness."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper has good originality, high quality and clear expression.  The paper combines adversarial attacks into data augmentation in the latent space to promote fairness and proposes a new method."
                },
                "weaknesses": {
                    "value": "More complex dataset is better to be verified the effectiveness of the proposed method."
                },
                "questions": {
                    "value": "1.Adversarial perturbations suffer from robust overfitting[1] when used as data augmentation in adversarial training, when used as data augmentation in fairness, is there exits some overfitting in promoting fairness?\n2.The rotation of the decision boundary is because of the exists of adversarial pertuabations in the proposed method,will different forms of perturbations such as L2 perturbation or L-infinity effect the rotation of decision boundary?\n3.The proposed method has a relatively large number of hyperparameters, will this affect its efficiency when applied to more complex datasets?\n\n\n\n[1]Leslie Rice, Eric Wong, and Zico Kolter. Overfitting in adversarially robust deep learning. In International Conference on Machine Learning, pp. 8093\u20138104. PMLR, 2020."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7792/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698810406187,
            "cdate": 1698810406187,
            "tmdate": 1699636952876,
            "mdate": 1699636952876,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YuZ6mfDUtM",
                "forum": "eFS9Pm7bsM",
                "replyto": "aOGlIE82dC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7792/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7792/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### More complex dataset\nPlease see the global response, addressing more general scenarios beyond binary classification with binary-sensitive attributes.\n\n### Overfitting problem in data augmentation with adversarial training\nIn the proposed framework, overfitting is not a matter, because unlikely typical adversarial training, we use different objective functions in the attacking step and training step. Moreover, we can interpret the 'unfair region' described in Figure 1(a) as an overfitted or underfitted area. We directly recover the unfair region making the misclassification rates for each subgroup fair, and alleviate the overfitting and underfitting of original prediction.\n\n### About the L2 perturbation and hyperparameters\nIn fact, our framework is not limited to $L_\\infty$ perturbation. $L2$ perturbation is also applicable because the perturbation vector $\\delta$ is trained by the fairness attack objective function Eq.(5). Especially, the Sinkhorn distance regularizes the perturbated dataset has a similar distribution with the original dataset.\n\nThe reason we adapt $L_\\infty$ is the hyperparameter $\\epsilon$ is not straightforward if we use $L2$ perturbation. However, we may be able to omit the hyperparameter $\\epsilon$ by computing the proper $\\epsilon$ given dataset and decision boundary. In detail, we can measure the Euclidean distance between each subgroup and the pre-traind decision boundary. And we can set the mean distance as $\\epsilon$ for the $L2$ perturbation. \n\nFor example, if we use $L2$ perturbation by setting $\\epsilon$ as the measured distance for COMPAS dataset and MLP with $\\alpha=1.0$ and $\\lambda = 0.5$, we obtain the range of $\\delta\\in[-0.2722,0.2552]$. Following the measured perturbation, we obtain a similar result with $L_\\infty$ by setting $\\epsilon=0.27$. Therefore, we can conclude that the choice of $L2$ and $L_\\infty$ doesn't affect the performance. If a practitioner wants to control the hyperparameter $\\epsilon$, $L_\\infty$ is suitable. On the other hand, if one wants to simplify the setting, the $\\epsilon$ measurement with $L2$ perturbation could be a wise choice. We will add this ablation study in the revision.\n\n\n| COMPAS | magnitude of $\\delta$| |Accuracy |  | $\\Delta DP$ |  | $\\Delta EOd$ |  |\n| --- | --- | --- | --- | --- | --- | --- |--- | --- |\n| Method |min $\\delta$ | max $\\delta$ | mean | std. | mean | std. | mean | std. |\n| $L2$-perturbation |-0.2722|0.2552| 0.6664 | 0.0029 | 0.0108 | 0.0056 | 0.0663 | 0.0215 |\n| $L_\\infty$-perturbation |-0.2700|0.2700| 0.6613| 0.0033| 0.0126| 0.0065| 0.0686| 0.0212|\n\nWhen it comes to $\\lambda$, the selection of $\\lambda$ is significant to the accuracy-fairness trade-off since it works as the weight between the original and perturbed dataset. Therefore, we vary $\\lambda$ and report the 'line' of the performances in the Pareto Frontier as well as varying $\\alpha$ in Eq.(5). We suggest using $\\lambda=0.5$ since it empirically shows the best accuracy-fairness trade-off for all datasets. Still, the hyperparameter tuning is needed for $\\alpha$. Here we report the training time for experiments for each combination of hyperparameters. Each fine-tuning is conducted in 50 epochs. We measure the training time for a single run for baseline, and the average of 10 runs for fine-tuning. \n\n| Dataset | Baseline | Fine-tuning|\n| --- | --- | --- |\n| Adult | 40.046s| 37.018s |\n| COMPAS |9.118s| 6.166s|\n| German |3.934s |1.177s| \n| Drug| 4.748s | 1.838s|\n\nTherefore, we can omit the hyperparameter $\\epsilon$ and $\\lambda$ by using $L2$ perturbation taking the mean Euclidean distance between the latent feature and the decision boundary as $\\epsilon$ and setting $\\lambda=0.5$ to make the original dataset and perturbed dataset equally contribute. However, even though only one hyperparameter, $\\alpha$, is need to be tuned, the hyperparameter tunig itself takes less time compared to the original ERM because fine-tuning trains only the last layer of the model."
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7792/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700285629303,
                "cdate": 1700285629303,
                "tmdate": 1700285629303,
                "mdate": 1700285629303,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OzBcbiSY7C",
                "forum": "eFS9Pm7bsM",
                "replyto": "YuZ6mfDUtM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7792/Reviewer_CWiz"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7792/Reviewer_CWiz"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to the authors for their reply, I have no more questions."
                    }
                },
                "number": 28,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7792/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700620698852,
                "cdate": 1700620698852,
                "tmdate": 1700620698852,
                "mdate": 1700620698852,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xV2yzMCY8N",
            "forum": "eFS9Pm7bsM",
            "replyto": "eFS9Pm7bsM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7792/Reviewer_XHAN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7792/Reviewer_XHAN"
            ],
            "content": {
                "summary": {
                    "value": "The paper discusses a novel approach called Adversarial Latent Feature Augmentation (ALFA) to address bias in machine learning models. This approach involves perturbing latent features with the aim of mitigating bias and achieving fairness. The authors define a fairness attack as a feature-level perturbation to maximize a covariance-based fairness constraint, which can lead to biased latent features. They argue that fine-tuning these perturbed features can rotate the decision boundary, covering unfair regions and achieving group fairness. The method is tested on various datasets, including Adult, COMPAS, German, and Drug datasets, as well as image classification on CelebA, demonstrating accuracy while achieving group fairness."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is clearly presented and easy to follow.\n2. It provides some theory analysis.\n3. Empirical studies sound."
                },
                "weaknesses": {
                    "value": "1. Why \"GAN-based perturbation might yield unsuitable generated features not aligning on the sensitive hyperplane\" for tabular datasets? The intuition is unclear to me. It would be better to explain using concrete examples and provide citations.\n2. Can the proposed method be applied to non-tabular datasets, such as vision datasets (i.e., images and videos)? If not, the scope of the method is limited. If so, why does W1 matter?\n3. In Eq.(1), the perturbation \\delta should be bolded. What is the \\bar{a} in Eq.(1)? Is this the mean of all a_i? The notation of \\bar{a} is not defined before using.\n4. Adversal fairness learning is not novel. The earliest work dates back to 2018 [1]. I doubt the novelty of the proposed method in 3.3.\n5. According to the setting and empirical studies, the proposed method can be applied only when there is only one sensitive attribute with binary values. \n\n\n[1] Christina Wadsworth, Francesca Vera, Chris Piech. Achieving Fairness through Adversarial Learning: an Application to\nRecidivism Prediction. FAT/ML Workshop, 2018"
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7792/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698823770511,
            "cdate": 1698823770511,
            "tmdate": 1699636952774,
            "mdate": 1699636952774,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fChfIUf4EM",
                "forum": "eFS9Pm7bsM",
                "replyto": "xV2yzMCY8N",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7792/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7792/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### The statement about GAN in tabular dataset\nThe statement in the paper reviewer mentioned is based on our empirical observation of the existing work, FAAP [1] which is applicable only to the image dataset. As shown in Figures 1 and 2 in the paper demonstrating the experimental result on tabular datasets, FAAP shows poor performance in tabular datasets and we suspect it is because the GAN-based perturbation might not be converged well.\n\n\n### Application to vision dataset\nWe reported the experimental results for the image dataset, CelebA as shown in Figure 4. In this case, FAAP shows comparable results to ours, and W1 is not a matter in this case. Therefore, it shows that FAAP works only for image datasets, while ALFA (ours) works for both tabular and image datasets. Moreover, we conduct additional experiments applying ALFA to the NLP dataset, as shown in the global response in this rebuttal.\n\n### Missing notations\nThanks for pointing out the typos. We will revise them and screen again the entire notations thoroughly.\n\n\n### The novelty of adversarial fairness\nALFA and [2] have different perspectives on adversarial training. [2] uses an adversary to predict sensitive attributes, while the classifier predicts the label. The objective function for the classifier is designed to minimize the binary cross-entropy (BCE) loss on the label while maximizing BCE loss on the sensitive attribute, which makes the classifier not use the demographic information during the prediction. However, no mathematical derivation is introduced as to how maximizing BCE on the sensitive attribute makes the classifier fair. Moreover, [2] is designed only for the COMPAS dataset. \n\nWe contain recent adversarial training methods in the paper such as [1] and [3], and describe how they are different from ours, in Section 1 and Section 2, respectively.\n\n\n### Beyond binary labels and sensitive attributes\nPlease see the global response, addressing more general scenarios beyond binary classification with binary-sensitive attributes.\n\n\n\n\n[1] Zhibo Wang, Xiaowei Dong, Henry Xue, Zhifei Zhang, Weifeng Chiu, Tao Wei, and Kui Ren. Fairness-aware adversarial perturbation towards bias mitigation for deployed deep models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10379-10388, 2022b\n\n[2] Christina Wadsworth, Francesca Vera, Chris Piech. Achieving Fairness through Adversarial Learning: an Application to Recidivism Prediction. FAT/ML Workshop, 2018 \n\n[3] Peizhao Li, Ethan Xia, and Hongfu Liu. Learning antidote data to individual unfairness. In International Conference on Machine Learning, pp. 20168\u201320181. PMLR, 2023."
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7792/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700285573912,
                "cdate": 1700285573912,
                "tmdate": 1700285573912,
                "mdate": 1700285573912,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "uqTgS5f3mK",
            "forum": "eFS9Pm7bsM",
            "replyto": "eFS9Pm7bsM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7792/Reviewer_KRtS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7792/Reviewer_KRtS"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes using adversarial attack on fairness of a Deep network to generate more synthetic samples that could be used to fix the bias present in the model. The authors show through a small example how generated adversarial samples would be overlapping with the region of unfairness. Hence using such samples in the training set rotates the classifier boundary in a way the misclassification regions are now correctly predicted. The idea is quite novel application of adversarial attack on a neural network. The paper also provides theory on why optimizing over covariance is relevant to impacting the fairness constraints like DP and Equality of Opportunity. The experiment results on standard datasets show better pareto frontier for proposed method for most cases."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper provides insights on one of the reasons of unfairness due to data imbalance and tackles the problem through data augmentation. I like the analysis presented in the paper.\n \n1. The proposed data augmentation intelligently utilizes the adversarial attack to identify the unfairness regions. The samples from unfairness region are then used to teach the model to fix its linear decision boundary.\n2. The optimization objective for adversarial attack are tied to the fairness objectives under certain conditions.\n3. The pareto frontier for proposed technique is better than baselines."
                },
                "weaknesses": {
                    "value": "There are some minor weaknesses of the proposed approach:\n1. Storage cost increase due to dataset augmentation. Not necessarily a flaw.\n2. Limitation: Paper only deals with binary classification and two protected classes. \n3. The current solution of adjusting rotation of the linear classifier would only work when the two classes are separated in their latent space. For more complicated cases like lesser data, higher dimensional data etc. the latent space might not be separated and intuition would not hold. However this would be future direction to look into."
                },
                "questions": {
                    "value": "I find the proposed method quite interesting. However I have some clarifying questions:\n\nQ1. One of the comparisons that could be done would be to generate samples using directly optimizing empirical EOd or DP, i.e finding adversarial examples using maximizing False positive error or False negative error. This could be done by flipping the labels for the dataset. Why choose to attack L_fair vs False positive rate?\n\nQ2. In theorem 3.2, $N_p = 4\\times \\max(n_{i,j})$. How is this maintained? Does this mean some samples would be repeated before adversarial attack?\n\nQ3. The final loss function is a convex combination of true and perturbed dataset. I wanted to understand the weight attributed to the perturbed dataset. What values of $\\lambda$ are used in different cases? \n\nQ4. How can linear decision boundary handle the case of multiple classes in protected attribute? A rotation would not be able to fix the bias in the dataset.\n\nI will adjust my score after these concerns are addressed."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7792/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7792/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7792/Reviewer_KRtS"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7792/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698831658355,
            "cdate": 1698831658355,
            "tmdate": 1699636952650,
            "mdate": 1699636952650,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YUDvc4AkkS",
                "forum": "eFS9Pm7bsM",
                "replyto": "uqTgS5f3mK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7792/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7792/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### Storage cost\nALFA is effective in terms of storage or memory cost. In detail, we can think about two types of augmentation, offline augmentation (storing augmented data in the drive) and online augmentation (augmentation on the data loader, stored in the memory temporarily).\n\nFor the offline augmentation, more storage cost may be required compared to the baseline ALFA. However, it requires less storage cost compared to the data augmentation on the input space. For example, for $N$ samples, the storage cost for $N$ images might be huge, while ours requires a single $N \\times d$ matrix where $d$ is the dimension of the latent feature.\n\nFor the online augmentation, the data augmentation on the input space may require more training time and memory space. However, as ours is a fine-tuning method using a latent feature from original images and a perturbed latent feature, we only require two $N \\times d$ matrices in the memory space removing the dataloader from the input images in the memory.\n\n\n### Beyond binary $Y$ and $A$\nPlease see the global response, addressing more general scenarios beyond binary classification with binary-sensitive attributes.\n\n### Linear separability assumption\nWe agree with the reviewer's point that the linear separability assumption is needed in our framework. However, linear separability is a widespread assumption [1] even though the size of the dataset is small or the number of features is large even in the input space. Moreover, deep neural networks transform the input data into well-separated latent features, making the classification easier.\n\n[1] Moshkovitz, Michal, Yao-Yuan Yang, and Kamalika Chaudhuri. \"Connecting interpretability and robustness in decision trees through separation.\" International Conference on Machine Learning. PMLR, 2021.\n\n### Label flipping strategy\nAlthough flipping labels might generate samples in which $\\Delta EOd$ and $\\Delta DP$ are worse, training on them doesn't necessarily improve fairness. For example, we consider the latent distribution in Figure 1 in the paper. The augmented dataset may have the same distribution as the original data but flipped labels. Training on the original and augmented (flipped) dataset together may produce poor decision boundaries, exacerbating the accuracy. \n\nOn the other hand, ALFA aims to produce a biased dataset while maintaining the data distribution of the original feature, using Sinkhorn distance, which ensures the generated data has a similar data distribution to the original one and is useful to improve fairness while maintaining accuracy.\n\n### Sample repetition\nYes, the data sampling is repeated to maintain the size of each subgroup to be the same. Although this repetition is not a mandatory condition empirically, we make the subgroup size equal to ensure the efficiency of the fairness attack as shown in Theorem 3.2.\n\n### Contribution of $\\lambda$\nThe selection of $\\lambda$ is significant to the accuracy-fairness trade-off since it works as the weight between the original and perturbed dataset. Therefore, we vary $\\lambda$ and report the 'line' of the performances in the Pareto Frontier in Figures 2 and 3 in the paper, as well as varying $\\alpha$ in Eq.(5).\n\n\n### Multiple protected attributes\nPlease see the global response, addressing more general scenarios beyond binary classification with binary-sensitive attributes."
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7792/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700285546166,
                "cdate": 1700285546166,
                "tmdate": 1700285546166,
                "mdate": 1700285546166,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DP7ZYd3kiN",
                "forum": "eFS9Pm7bsM",
                "replyto": "uqTgS5f3mK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7792/Reviewer_KRtS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7792/Reviewer_KRtS"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the rebuttal"
                    },
                    "comment": {
                        "value": "I would like to thank the authors for their rebuttal work. For most parts, my questions have been answered. \nHowever, I do have some additional comments on them which seems to be important -\n1. **Linear separability**: The original comment should be read as separation of data wrt different sensitive attributes which seems to be essential to be explanation that rotation of linear boundary would help cover unfair regions. Even though data with different classes (labels) would be linearly separable [1], it is not guaranteed the same for data with different sensitive attributes. Consider Figure 1, if the two sensitive attributes were inter-mixed in latent space, I think a rotation would be insufficient to fix unfairness. This issue even worsens when there are more classes of sensitive attributes.\n\n 2. **Label flipping** - the original question Q1 was to generate augmented samples using maximizing FPR for one of the classes. This could in theory generate samples to cover the unfair region as well. How would this direct approach do in comparison to covariance loss used in the paper.\n\n3. **Contribution of $\\lambda$** - I was of the understanding that the figures 2, 3 are plots between EOd or DP vs Accuracy. Q3 was regarding the absolute value of $\\lambda$ vs EOd and Accuracy.\n\n4. **New results on multi-class classification** - The new results seem to indicate that proposed method leads to significant drop in acc with not much gain in DP. Also why are EOd not reported for this experiment?\n\nMinor typo in definition Differential Fairness DF in the rebuttal. Please check indices i,j."
                    }
                },
                "number": 30,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7792/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700646950826,
                "cdate": 1700646950826,
                "tmdate": 1700647335605,
                "mdate": 1700647335605,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YSEX6i3M5O",
                "forum": "eFS9Pm7bsM",
                "replyto": "CTe0ayZe5k",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7792/Reviewer_KRtS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7792/Reviewer_KRtS"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for detailed reponse"
                    },
                    "comment": {
                        "value": "Thank you for the additional responses. They address my concerns sufficiently."
                    }
                },
                "number": 34,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7792/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700738926991,
                "cdate": 1700738926991,
                "tmdate": 1700738926991,
                "mdate": 1700738926991,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "eODHbpAwav",
            "forum": "eFS9Pm7bsM",
            "replyto": "eFS9Pm7bsM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7792/Reviewer_J4SN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7792/Reviewer_J4SN"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a data augmentation technique to improve fairness, based on adversarial perturbations in the latent space. This technique leverages a covariance based constraint to to generate features which are maximally separated i.e. biased - followed by a training procedure to minimize the distance between the perturbed and original features - hoping to enforce some \u201cinvariance\u201d over the true features i.e. not relying on the spurious / sensitive features. Experiments are demonstrated across multiple fairness datasets and vis-a-vis different methods, demonstrating the competitive fairness-accuracy tradeoff of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper addresses an important challenge of fairness by leveraging an adversarial latent space approach - which is interesting as most previous works demonstrate the opposite i.e. adversarial robustness maybe at odds with fairness. Further,  it is appreciated that detailed analysis is also shown on a synthetic dataset to better understand the hyperplane rotation i.e. how the boundary is encouraged to be changed. For the experiments, comparison is performed with many other methods."
                },
                "weaknesses": {
                    "value": "Please see below:\n1.  It is is unclear how this approach could be better than simple reweighting of the demographic groups, given that the knowledge is already available. Why would one like to leverage the adversarial data augmentation?\n2. It is hard to infer the experimental conclusions from the many plots which are represented - is there any reason for not observing a non decreasing or non increasing trend in most of the experiments?\n3. There seems to be no comparison with baselines such as reweighting and using only ERM features (no data augmentation) - how does the performance compare?\n4. The motivation for choosing a covariance based approach is unclear - is there any specific intuition behind this choice? \n5. Overall, the conciseness and coherence of the paper could be improved - e.g. :During the adversarial training, we ensure that the semantic essence of the perturbed features is preserved \u201c -> semantic essence could be replaced with a more technical term w.r.t the true invariant / sensitive attributes."
                },
                "questions": {
                    "value": "Questions:\nApart from the questions mentioned in the above section:\n1. Is there any underlying assumption on the structure of the space? Perhaps, it would be helpful to have a causal model explaining the structure of the input space (e.g. to depict the spurious correlation between Y and A). \n2. Does this method scale beyond binary Y and A?\n3. Could you compare your intuitions with the following adversarial group robustness technique? \u2028Paranjape, Bhargavi, et al. \"AGRO: Adversarial Discovery of Error-prone groups for Robust Optimization.\"\u00a0arXiv preprint arXiv:2212.00921\u00a0(2022)\n\nSuggestions:\n1. I would recommend adding a concise summary of the plots (e.g. a table) to show the demonstrated improvements.\n2. \"Hyperplane rotation\" may be hard to understand in the first read, could we replaced with a more common term like decision boundary."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7792/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7792/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7792/Reviewer_J4SN"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7792/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699036445209,
            "cdate": 1699036445209,
            "tmdate": 1699636952533,
            "mdate": 1699636952533,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ERDbK2HSV8",
                "forum": "eFS9Pm7bsM",
                "replyto": "eODHbpAwav",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7792/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7792/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### The benefit of the proposed method compared to data reweighting.\nExisting data reweighting methods [1], and [2] assign weights for all samples\naccording to their importance, and re-train the entire model using the weighted data. On the other hand, ALFA freezes the encoder and fine-tunes the last layer only using the augmented data. Therefore, ALFA is more effective than existing reweighing methods in terms of time and memory cost. Moreover, according to [3] data reweighting always overfit, so there's no guarantee that the worst-group test performance to be better than ERM. In contrast, we directly consider both the overestimated (high false positive rate) and underestimated group (high false negative rate), seeking a fair misclassification rate for them. \n\n\n### Description of the Pareto Frontier\nThanks for pointing out. We will revise the explanation for the figure. In detail, the Pareto Frontier shows the trade-off between accuracy and fairness metric, while the black solid line shows the optimal trade-off. The closer a point to the black line, the better the accuracy-fairness trade-off is. In most cases, ours is located in the upper-right of the figure, which means it can achieve fairness while minimizing the compromising of accuracy. \n\nIn fact, most of the existing frameworks shown in the figure are designed to improve fairness while maintaining accuracy. And only a few cases the fairness frameworks can improve accuracy too. It is hard to observe either decreasing or increasing accuracy in the experimental result. Moreover, we exclude some experimental results if they do not converge well, or are worse than the base rate.\n\n### Comparison to reweighting and baseline\nWe mainly focus on augmentation-based methods or some methods conducted in the latent space. However, as the reviewer asked, we can further compare the baseline and reweighting methods. In particular, we implement the state-of-the-art reweighting methods [2]. The revised Pareto Frontier, Figures 2 and 3 in revised paper's PDF file shows that ALFA outperforms the baseline and reweighting method. Moreover, the improvement in [2] is not consistent, whereas ALFA shows promising improvement."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7792/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700285302150,
                "cdate": 1700285302150,
                "tmdate": 1700285302150,
                "mdate": 1700285302150,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "m4OOxBJV1q",
            "forum": "eFS9Pm7bsM",
            "replyto": "eFS9Pm7bsM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7792/Reviewer_VbNu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7792/Reviewer_VbNu"
            ],
            "content": {
                "summary": {
                    "value": "The paper considers fairness in binary classification with binary protected features. In particular, focusing on DP and EOdds notions of group-level fairness, the paper proposes adversarial latent feature augmentation (ALFA), which considers the adversarial attack together with data augmentation. Experimental results are also provided."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The strength of the paper comes from the attempt to address adversarial attack and (latent space) data augmentation at the same time, for certain group-level fairness notions."
                },
                "weaknesses": {
                    "value": "The weakness of the paper comes from the relatively unclear presentation of the material, the worry of the limited scope of application, and therefore, the unclear takeaway message. It would be helpful if points in __Questions__ below can be clarified."
                },
                "questions": {
                    "value": "__Q1__: regarding \"data augmentation in latent space\"\n\nThe paper sets a goal of addressing adversarial attack and data augmentation at the same time for fair classification. However, it is not clear to me what exactly is being considered in the \"latent space\". The theoretical derivation closely follows the previous work Zafar et al., (2017), but with a different loss function (Equation 1). The \"latent feature\" $\\mathbf{z}$ is utilized to attack the classifier. What is the relation between the original input features and the latent feature after attack? I am having difficulty understanding the motivation behind introducing an alteration in latent feature space for the purpose of both augmenting data and improving fairness.\n\n__Q2__: what is \"unfair region\"\n\nIn Figure 1a, \"unfair regions\" are highlighted near the decision boundary (hyperplane). While I understand the fact that the groups are imbalanced, and that the proposed approach yields a \"rotated\" decision boundary, I am not sure how to parse the goal of finding a different slope (i.e., rotating) for the hyperplane in terms of data augmentation for fair classification. Please clarify.\n\n__Q3__: more general scenarios beyond binary classification with binary protected feature\n\nCan the proposed framework handle more complicated settings in practical scenarios? What is the takeaway message the paper would like to convey?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7792/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699230963920,
            "cdate": 1699230963920,
            "tmdate": 1699636952385,
            "mdate": 1699636952385,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "U8adw1u7tm",
                "forum": "eFS9Pm7bsM",
                "replyto": "m4OOxBJV1q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7792/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7792/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### The details about \"data augmentation in latent space\" and \"unfair region\"\nIn the binary classification scenario, a model might be biasedly trained due to data imbalance in the demographic group, producing inconsistent misclassification rates across the subgroup, which we define as an 'unfair region' in the paper. \n\n***Please see the Appendix K in the paper for the figures.***      \nHere's an example of synthetic data explaining the concept of unfair region and how the decision boundary is rotated. We simply the binary classification task with a 2D Gaussian mixture data, consisting of two classes $y \\in \\\\{0, 1\\\\}$ and two sensitive attributes, $A \\in \\\\{0, 1\\\\}$ (indicating unprivileged and privileged groups.) \n\\begin{align}\n    x \\sim \\begin{cases}\n        group1: \\textbf{N} (\\begin{bmatrix}\n           \\mu \\\\\\\\\n           \\mu\n         \\end{bmatrix} , \\sigma^2 )& \\text{if} \\\\: y=1, a=1 \\\\\\\\\n        group2: \\textbf{N} (\\begin{bmatrix}\n           \\mu \\\\\\\\\n           \\mu^\\prime \n         \\end{bmatrix} , \\sigma^2)& \\text{if} \\\\: y=0, a=1 \\\\\\\\\n        group3: \\textbf{N} (\\begin{bmatrix}\n           0 \\\\\n        \\mu\n         \\end{bmatrix} ,(K\\sigma)^2) & \\text{if}\\\\: y=1, a=0 \\\\\\\\\n        group4: \\textbf{N} (\\begin{bmatrix}\n           0 \\\\\\\\\n           0\n         \\end{bmatrix} , (K\\sigma)^2)& \\text{if}\\\\: y=0, a=0\n    \\end{cases}\n\\end{align}\nwhere $\\mu^\\prime =r\\mu, 0<r<1$ and $K>1$, where the number of samples in each group is $N_1 : N_2: N_3: N_4$.  We arbitrarily set $K=3$, $r=0.7$, $\\mu = 1$, $N_1 = N_2 = 100$, and $N_3=N_4=400$.\n\nFrom the given synthetic data, we have a decision boundary like Figure 1 in the rebuttal pdf. Due to the imbalance in the dataset, the subgroup $a=1,y=0$ is overestimated as label $y=1$, and the subgroup $a=0, y=1$ is underestimated as label $y=0$. The difference in misclassification rate is shown in Figure 2. We define them as 'unfair regions' where the misclassification rate is disproportionately high. \n\nOur fairness attack shifts the data distribution in a more biased way. In the synthetic data, as the demographic group $a=1$ is privileged to be predicted as $y=1$, group $a=1$ shifts towards the positive area, while group $a=0$ towards the negative area. Although this perturbation is counter-intuitive, the perturbed data directly recover the unfair region, producing a new decision boundary. \n\nFigure 2 shows that when we vary the amount of perturbation $\\delta$ from 0 to 0.2, the misclassification rate for each group changes accordingly. As the fairness evaluation metric $\\Delta EOd$ is defined as the summation of the True Positive Rate (TPR) gap and False Positive Rate (FPR) gap between the demographic groups, we plot the TPR gap, FPR gap, $\\Delta EOd$, and the overall misclassification rate. The lower subfigure in Figure 2 shows that the TPR gap and FPR gap drop significantly achieving fairness while maintaining the overall misclassification rate.\n\n### More general scenarios beyond binary labels and sensitive attributes\nPlease see the global response, addressing more general scenarios beyond binary classification with binary-sensitive attributes."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7792/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700285191983,
                "cdate": 1700285191983,
                "tmdate": 1700285191983,
                "mdate": 1700285191983,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "x5yztaFKuu",
                "forum": "eFS9Pm7bsM",
                "replyto": "U8adw1u7tm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7792/Reviewer_VbNu"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7792/Reviewer_VbNu"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up Questions"
                    },
                    "comment": {
                        "value": "Thank authors for the additional experimental results and responses. My original questions and concerns are still not resolved.\n\nThe term \"(un)fair\" and \"bias\" are heavily overloaded in the paper and the response. The data imbalance is irrelevant to what predictor one would like to use, and what fairness notion on prediction one would like to apply. By \"fairness attack shifts the data distribution in a more biased way\", the data imbalance (which may be problematic in itself and may be not) is mixed together with the DP violation. What exactly is the goal of ALFA? Is it intended to be a general strategy of data augmentation for different fairness notions? Or, is it specifically designed to deal with group-level DP notion, which happen to be closely related to \"imbalance\" in prediction $\\hat{Y}$ (instead of the data $Y$)?\n\nIn addition, I am still having difficulty parsing the connection between \"data augmentation in latent space\" and the characterization of \"unfair region\". While I understand the highlighted region in the figure corresponds to \"unfair region\", the characterization is too general to be informative. What is the \"unfair region\" in cases beyond binary classifications? What is the relation between rotating (in a general sense) the classification hyperplane and augmenting the data in latent space?"
                    }
                },
                "number": 27,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7792/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700597976708,
                "cdate": 1700597976708,
                "tmdate": 1700597976708,
                "mdate": 1700597976708,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "LEmswxMxG3",
            "forum": "eFS9Pm7bsM",
            "replyto": "eFS9Pm7bsM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7792/Reviewer_qEGE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7792/Reviewer_qEGE"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes ALFA as a new method to enhance the fairness of classification models. It introduces the concept of adversarial perturbation in latent variable space, and demonstrates that fine-tuning on these perturbed features can lead to rotated decision boundaries, covering unfair regions. Experiments verify the effectiveness of ALFA in achieving group fairness while maintaining accuracy."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "*Quality*\n- This paper proposed a new approach to enhance fairness in machine learning models was proposed. This approach effectively merged data augmentation and adversarial attacks in the latent space to promote fairness. It revealed a counter-intuitive relationship between adversarial attacks and enhanced model fairness upon training with the resultant perturbed latent features. \n- The theoretical and experimental results are correct to the best of my knowledge. Theoretical proof showed that the approach could generate biased feature perturbation, and experiments validated that training with adversarial features can improve fairness.\n\n*Relevance*\n\nThe topic of fairness is important in machine learning."
                },
                "weaknesses": {
                    "value": "*Novelty*\n\n- Although the proposed method is new, the idea of using adversarial training and data augmentation for fairness enhancement is not novel. Adversarial training has been widely used to improve model robustness and generalization, while data augmentation is a common technique to enrich training datasets and reduce overfitting. Integrating these two concepts to address fairness issues has already been studied. Thus, further research is needed to explore the boundaries and limitations of this approach, as well as its applications in different domains.\n\n*Algorithmic Analysis*\n\n- One of the key aspects of ALFA's design is its ability to generate biased feature perturbations during training. This is achieved by introducing a novel loss function that encourages the model to focus on features that are particularly sensitive to small perturbations in the input space. While this approach can effectively rotate the decision boundary to cover previously unfavorable regions, it may also introduce computational complexity and training time challenges. ALFA's computational requirements are not explicitly stated in the article, but it's essential to consider them in order to evaluate its practicality for large-scale datasets and complex models.\n\n- Additionally, while ALFA may improve fairness measures for certain groups, it's important to consider its overall impact on model stability and robustness. Adversarial loss can make training process vulnerable to small perturbations in the input space, potentially leading to increased numerical instability. This issue is not addressed in the article, and further research is needed to investigate these potential trade-offs when employing ALFA in practice.\n\n- Moreover, while the experimental results provided in the article are encouraging, they only cover a limited number of datasets and model types. It remains to be seen how ALFA performs on other datasets that may exhibit different characteristics and challenges. Extensive validation and comparison with other state-of-the-art methods are necessary to assess ALFA's overall effectiveness and superiority."
                },
                "questions": {
                    "value": "- To what extent is the proposed ALFA method innovative? Is it based on novel ideas or techniques that have not been previously explored in the field of fairness-enhancing machine learning? ALFA's algorithm design incorporates adversarial training and data augmentation. What are the novel aspects of this integration, and how does it lead to improved fairness?\n\n- The use of adversarial training can potentially lead to instability. How does ALFA address this issue, and what measures are taken to ensure algorithm stability during training and deployment?\n\n- ALFA's design involves a novel loss function that encourages the model to focus on features that are particularly sensitive to small perturbations in the input space. How does this loss function compare to traditional loss functions used in fairness-enhancing machine learning?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7792/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7792/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7792/Reviewer_qEGE"
                    ]
                }
            },
            "number": 6,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7792/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699513883750,
            "cdate": 1699513883750,
            "tmdate": 1699636952158,
            "mdate": 1699636952158,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DRxR2FV4tt",
                "forum": "eFS9Pm7bsM",
                "replyto": "LEmswxMxG3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7792/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7792/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### Novelty of the proposed method\n\nExisting data augmentation methods are conducted on the input space. Although some of them utilize adversarial training to generate augmented data, the transparency and efficacy of transformation in the input space are not obvious because of the non-linearity of the deep models. Moreover, the objectives of adversarial training in the existing methods are different from ours. In detail, FAAP [1] adopts adversarial training to align the data to the auxiliary decision boundary predicting sensitive attributes rather than labels. AntiDRO [2] generates antidote data produced by Distributionally Robust Optimization (DRO) to select the worst-performing data. The generated data is used as data augmentation while having analogous features from the original dataset but different sensitive attributes. However, AntiDRO is designed only for individual fairness, and training on these antidote data cannot improve group fairness. In contrast, our combination of data augmentation and adversarial training is not trivial. In detail, we define 'unfair regions' where a demographic group is disproportionately overestimated (high false positive rate) and underestimated data (high false negative rate). The fairness attack automatically finds the unfair regions and directly makes the perturbed data recover the unfair regions. In this step, the perturbed data itself could be a biased dataset, but we utilize them as data augmentation which is a counter-intuitive way to mitigate bias by making the misclassification rates for each subgroup fair.\n\n    \n    \n###  Computational complexity and training time\n\nWhen it comes to computational complexity, ALFA requires the same memory space as the original training during the attacking step. In the fine-tuning step, as it stores perturbed latent features as a matrix form and is directly applied to the final classifier, only the final classifier needs the additional computational cost for the single latent feature matrix $Z^\\prime \\in \\mathbb{R}^{N \\times d}$ where $N$ is the number of samples and $d$ is the dimension of the latent feature. \nFor example, let's assume we use $L$ layers of MLP, and each layer has the same dimension $d$ for simplicity. Then, the computational complexity is $\\mathcal{O}(pdN + (L-1) d^2N + dN) )$ where $p$ is the number of features of the input. During the attacking step, we freeze the encoder and classifier, only computing the covariance and the perturbation update, where each of them takes $\\mathcal{O}(dN)$ complexity. Moreover, during the fine-tuning step, it also takes complexity of $\\mathcal{O}(dN)$. Therefore, the additional computational complexity for an iteration is $\\mathcal{O}(3dN)$ including covariance calculation, perturbation update, and fine-tuning, which is significantly light compared to the ERM.\n\n\n### Stability of the proposed method\n\nIn typical adversarial training, the min-max optimization or bi-level (alternative) optimization may not be stable. However, it doesn't happen in ALFA since it requires a one-time attacking step to maximize the fairness constraint, and the augmented data is used in the training to minimize binary cross-entropy loss. Therefore, our framework has the same stability as traditional empirical risk minimization (ERM).\n\n### Extension to more complex dataset and model\nPlease see the global response addressing more complex situations."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7792/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700284886891,
                "cdate": 1700284886891,
                "tmdate": 1700284886891,
                "mdate": 1700284886891,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PxP6A6Bm2X",
                "forum": "eFS9Pm7bsM",
                "replyto": "bfp4NVJBMe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7792/Reviewer_qEGE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7792/Reviewer_qEGE"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the authors feedback. I do not have further questions."
                    }
                },
                "number": 29,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7792/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631363664,
                "cdate": 1700631363664,
                "tmdate": 1700631363664,
                "mdate": 1700631363664,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ZDXbo8zT2o",
            "forum": "eFS9Pm7bsM",
            "replyto": "eFS9Pm7bsM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7792/Reviewer_92u3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7792/Reviewer_92u3"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors propose an adversarial data augmentation technique based on maximizing the covariance of sensitive attributes and signed distance from the decision boundary. They then incorporate the augmented samples into the training of the classifier layer. Overall, the authors demonstrate that their method improves fairness metrics over the considered baselines."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The proposed method is simple and effective \n- The presentation of the paper is good."
                },
                "weaknesses": {
                    "value": "- The paper considers fine-tuning only the classifier part of the network (i.e., the last layer) and freezes the encoder part of the network. While this method works for simple tabular-like datasets, I have concerns about how this method performs in the case of datasets with a large label space, like ImageNet.\n\n- In general, the idea is not novel as the perturbation in the latent space is already explored by the earlier methods"
                },
                "questions": {
                    "value": "Please see the weakness section above and in addition to that:\n\n- How is the perturbation $\\epsilon$ set for different feature encoding layers in different ML models? This is an additional hyperparameter that needs to be carefully tuned. Moreover, the authors mention hyperparameters in Table 2 in the appendix. It would be fair if the authors mentioned the cost of hyperparameter tuning in terms of time for the proposed method compared to the baselines.\n\n- How is the performance in datasets with large label space?\n\n- What is the performance of the adversarial training by perturbing the input space instead of latent space with similar attack objective?\n\n- In section 4.4.1, authors could quantitatively discuss the  improvements in terms of performance and fairness metrics. It is not clear to me from the graphs about the comparison of different methods."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7792/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7792/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7792/Reviewer_92u3"
                    ]
                }
            },
            "number": 7,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7792/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699858027472,
            "cdate": 1699858027472,
            "tmdate": 1699865330691,
            "mdate": 1699865330691,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0LyvQXz2Hd",
                "forum": "eFS9Pm7bsM",
                "replyto": "ZDXbo8zT2o",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7792/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7792/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### Extension to diverse data types and scenarios\nWe adopt the fine-tuning strategy of freezing the encoder part, our experimental results show that this strategy effectively achieves fairness even for the image dataset (CelebA) and text dataset (Wiki), as shown in the manuscript and the global response. \n\nWe also further research the usage of ALFA in the multi-class classification, as it can be conceptualized as multiple One-Vs-All binary classifications as presented in the global response. Though ALFA can address multi-class classification, the improvement is not significant, and the dataset used in the experiment is not large enough in terms of its size and label space.\n\nWe summarize our future works here. First, we will find or construct a large dataset having fairness concerns since the existing benchmark datasets for fairness research are relatively small in terms of size and label space. As we have verified the effectiveness of finding and directly recovering unfair regions, we expect that ALFA can be applied to large label space. Second, we are interested in the long-tail classification scenario, where the label space is large and extremely imbalanced. Although this topic is not directly related to the fairness problem, we motivate this scenario because it might suffer from highly overestimated or underestimated classes, which our framework can address.\n\n### Comparison to existing perturbation methods in latent space\nWe have investigated the literature about latent perturbation methods for fairness and identified two existing works [1,2]. Both use GAN for the perturbation, and the direction of the perturbation is regarding the auxiliary decision boundary, produced by the sensitive attribute. In detail, [1] generates a perturbed sample having opposite sensitive attributes but the same target labels locating it in a symmetric point. Therefore, the role of perturbation is to make the number of samples and distribution similar across the subgroups. However, [1] is based on the strong assumption that each subgroup's data distribution should be symmetric about the decision boundaries. [2] also uses the sensitive attribute decision boundary and generates the perturbation towards that decision boundary. We show the conceptual image showing that this perturbation doesn't necessarily lead to the fair decision boundary, as shown in Figure 5(b) in the paper. Moreover, both [1] and [2] are designed only for the image dataset. In our paper, we empirically show that FAAP [2] produces poor results in tabular datasets as shown in Figure 2 and 3.\n\n**The novelty of our method can be summarized:** \n1) the adaptiveness to diverse types of data, \n2) defining unfair regions where a demographic group is disproportionately overestimated (high false positive rate) and underestimated (high false negative rate), \n3) directly recovering the unfair regions using fairness attack which is counter-intuitive usage of fairness constraint, generating biased dataset to mitigate bias.\n\nAlthough we have identified only two existing methods, we are open to further discussion about the perturbation method in latent space for fairness. In particular, we will study further how to build interpretable latent perturbation for fairness."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7792/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700284809254,
                "cdate": 1700284809254,
                "tmdate": 1700284809254,
                "mdate": 1700284809254,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]