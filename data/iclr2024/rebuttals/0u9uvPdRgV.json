[
    {
        "title": "Semi-supervised Diffusion Solver for Travelling Salesman Problem"
    },
    {
        "review": {
            "id": "tLJLAqJqpt",
            "forum": "0u9uvPdRgV",
            "replyto": "0u9uvPdRgV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4355/Reviewer_FvPj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4355/Reviewer_FvPj"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a semi-supervised diffusion solver for TSP. Technically, it combines DIFUSCO [1] (i.e., supervised discrete diffusion model) and UTSP [2] (i.e., unsupervised loss) to achieve a semi-supervised objective function. To ease the decoding, a proxy of heatmap - transition matrix $T$ is used as the initial state for the forward pass. Experimental results on large-scale TSP demonstrate its superiority over several baselines."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* This paper is well-written. The empirical results look good.\n* The studied semi-supervised setting is new and practical for COPs.\n* The idea of modeling the generative process upon a transition matrix is interesting."
                },
                "weaknesses": {
                    "value": "* The scope of this paper is limited to TSP, while DIFUSCO [1] could solve MIS and TSP. It is suggested to include another problem, such as MIS or CVRP.\n* The technical contribution seems to be limited since this work mostly builds upon DIFUSCO [1] and UTSP [2].\n* What are the pros of your method and the proposed semi-supervised setting compared with UTSP [2] if only TSP is solved? The training of UTSP seems to be very efficient, i.e., ~2 hours on TSP1000 without any labels. Could you report your training overhead? Moreover, UTSP should be another baseline, while I did not find it in Tables 1 and 2.\n* Recent studies [3] find data augmentations could greatly ease the labeled burden of supervised learning (SL) methods in neural combinatorial optimization. Is it possible that (SL) DIFUSCO with data augmentations is comparable to (Semi-SL) your method, or even more powerful?\n* Lack of analyses, any visualization (e.g., of the denoising process or obtained solution) would be better.\n* The results on TSPLIB should be reported.\n* Minor:\n  * Better to use some figures for illustration (e.g., overview).\n  * The citation format is not correct (e.g., `~\\citep` and `~\\citet`).\n  * The reproducibility is not clear. No source code is provided.\n\n[1] DIFUSCO: Graph-based Diffusion Solvers for Combinatorial Optimization. In NeurIPS 2023.  \n[2] Unsupervised Learning for Solving the Travelling Salesman Problem. In NeurIPS 2023.  \n[3] Data-efficient Supervised Learning is Powerful for Neural Combinatorial Optimization. Submitted to ICLR 2023.\n\n----\n\n**Overall,** I don't see the benefits of combining supervised DIFUSCO with unsupervised UTSP. To be more clear, (1) compared with DIFUSCO, the proposed method cannot solve other COPs; (2) compared with UTSP, your training overhead seems to be much higher. Although with slight performance improvement, I think the current contribution is not enough for ICLR. Providing further analyses, highlighting the advantage of your method and setting, and adding another COP may significantly enhance the paper's strength. I vote for rejection."
                },
                "questions": {
                    "value": "* Due to the usage of $T$, could the denoising of your method directly output a feasible solution (without search)? If this is the case, could this strength help the denoising process, such as reducing the total inference time?\n* What is the total run time (i.e., TIME) on TSP50 and TSP100?\n* For Table 3, why does your method still outperform DIFUSCO at the ratio of 1:0?\n* Since generating unlabeled TSP instances is extremely easy, it is interesting to see your performance when the ratio scales up to 1:100 or even larger.\n* In each batch of the training, assume you have some labeled and unlabeled instances. So for the unlabeled instances, only the second part of Eq. (3) is activated? Could you add a pseudo-code?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4355/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697612513975,
            "cdate": 1697612513975,
            "tmdate": 1699636406833,
            "mdate": 1699636406833,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "FiA4GBKqOU",
            "forum": "0u9uvPdRgV",
            "replyto": "0u9uvPdRgV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4355/Reviewer_JPUZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4355/Reviewer_JPUZ"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a semi-supervised learning method for TSP. \n\nThey use discrete diffusion to build the transition matrix $\\mathbb{T}$ and build the heatmap using the $\\mathbb{T} \\rightarrow \\mathcal{H}$ proposed in Min et al.\n\nCompared with DIFUSCO (which is a SL model),  they show they achieve better performance by adding the UL loss. The ablation study further show that the UL loss can significantly improve the performance.\n\nReference:\n\nMin, Yimeng, Yiwei Bai, and Carla P. Gomes. \"Unsupervised Learning for Solving the Travelling Salesman Problem.\" arXiv preprint arXiv:2303.10538 (2023).\n\nSun, Zhiqing, and Yiming Yang. \"Difusco: Graph-based diffusion solvers for combinatorial optimization.\" arXiv preprint arXiv:2302.08224 (2023)."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "TSP holds a pivotal position in the realms of computer science, operations research, and combinatorial optimization. Investigating methodologies for constructing data-driven solvers for TSP is important for the Machine Learning for Combinatorial Optimization (ML4CO) community.\n\nThis paper studies SSL for TSP,  it combine the diffusion model in Graikos et al. [1] and unsupervised TSP in Min et al. [2].  \n\n\nNumerous articles have been published on the topic of supervised learning (SL)-based TSP solvers. Semi-supervised learning (SSL) can be viewed as an augmentation of these SL-based methodologies. Contrarily, unsupervised learning (UL)-based TSP solvers represent a new area of research.\n\nThis manuscript integrates both SL and UL approaches, demonstrating that the introduction of UL loss significantly enhances performance. Given that the UL loss concept was recently introduced by [2], this represents the (first?) application of SSL techniques to the TSP domain.\n\n\nReference\n\n[1]: Graikos, Alexandros, et al. \"Diffusion models as plug-and-play priors.\" Advances in Neural Information Processing Systems 35 (2022): 14715-14728.\n\n[2]: Min, Yimeng, Yiwei Bai, and Carla P. Gomes. \"Unsupervised Learning for Solving the Travelling Salesman Problem.\" arXiv preprint arXiv:2303.10538 (2023)."
                },
                "weaknesses": {
                    "value": "1. The novelty is mediocre. It's a combination of graph diffusion and UTSP.\n2. only limited to TSP, but this is not a big issue, as TSP is one of the most studied problems in optimization and has various real-world applications."
                },
                "questions": {
                    "value": "I am open to raise my rating if some of the following concerns are addressed:\n\n1. In Table 3,4,5, TSP-100 results (the distance) does not match with Table 1, 5.69 should correspond to TSP-50.\n\n2. In table 3, the authors achieve 16.55 on TSP-500 using ratio 1:10, however, in Table 2, it reports 16.72, why different?\n\nMinor:\n\n1. Table 2, DIFUSCO's time for TSP 500 should be 10.13m instead of 10.13h?\n2. In abstract, the author use Travelling Salesman at first and Traveling Salesman and end, Travelling/Traveling should be consistent."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4355/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698268743854,
            "cdate": 1698268743854,
            "tmdate": 1699636406766,
            "mdate": 1699636406766,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "EfOwD9PA34",
            "forum": "0u9uvPdRgV",
            "replyto": "0u9uvPdRgV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4355/Reviewer_9n2o"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4355/Reviewer_9n2o"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel semi-supervised diffusion solver for the TSP problem. The semi-supervised training method tackles the limitations of previous methods, including reinforcement learning, supervised learning and unsupervised learning methods. It achieves a clear improvement on large-scale TSP problems against the SOTA neural method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The problem addressed by this paper is significant, combining both the labeled and unlabeled data is promising in creating a commercial-level neural solver.\n2. The way to train the diffusion model in an unsupervised way is inspiring, which looks to me a promising direction in the future.\n3. The proposed method can show a consistent improvement under different search algorithms."
                },
                "weaknesses": {
                    "value": "1. Though the problem studied by this paper is significant, I'm afraid the proposed method makes a quite incremental technical contribution. Equation 8 is the only novelty in this paper and the overall framework simply combines the method of DIFUSCO and UTSP without any logic behind it. It is probably fine to call the proposed method a semi-supervised learning method, but simply combining a supervised learning objective and an unsupervised learning objective does not reveal any nature of the problem. I hope the authors to answer why they choose DIFUSCO + UTSP rather than other combinations (for example, Att-GCN + UTSP, DIFUSCO + Erdos goes neural) from the perspective of CO or TSP.\n2. The experimental results are not convincing, with actually very small improvements. In both Table 1 and Table 2, it is hard for me to say the proposed method shows a significant advantage against DIFUSCO, considering the more training samples used, which I believe will unavoidably lead to a longer training time. \n3. The proposed method still uses lots of labeled samples in the experiment, and this is in conflict with the claimed limitation of the supervised learning methods `These methods typically suffer from large expenditure in computation of the pre-computed optimal or high-quality approximate TSP solutions`. Now the question is, does your method really resolve this limitation of supervised learning methods? I think authors should set a goal to use less than 10% labeled instances to achieve a comparable performance with DIFUSCO trained with 100% labeled instances and much better performance than DIFUSCO trained with 10% labeled instances as well.\n4. The presentation is also bad, there are many grammar errors and typos, for example, the time for DIFUSCO on TSP-500 in Table 2 should be in minutes rather than in hours. Also, I cannot find the results of DIMES and UTSP in Table 1, which is confusing."
                },
                "questions": {
                    "value": "See the questions raised in the weakness session. Besides, I think [1] is just a sampling-based method without any training, so it should not be categorized as an unsupervised method. And it definitely shall not share the same limitations with other unsupervised learning method.\n\n[1] Haoran Sun, Katayoon Goshvadi, Azade Nova, Dale Schuurmans, and Hanjun Dai. Revisiting sampling for combinatorial optimization. 2023."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4355/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698553613320,
            "cdate": 1698553613320,
            "tmdate": 1699636406690,
            "mdate": 1699636406690,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "rmOBzi7pkJ",
            "forum": "0u9uvPdRgV",
            "replyto": "0u9uvPdRgV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4355/Reviewer_r8xE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4355/Reviewer_r8xE"
            ],
            "content": {
                "summary": {
                    "value": "Authors propose semi-supervised diffusion solver for traveling salesman problem (TSP). Mainly, there are three technical contributions. First, authors adopt discrete diffusion model from DIFUSCO (Sun and Yang). The choice of diffusion process, and decoding strategies are mostly followed from DIFUSCO. Second, authors Adopt transition matrix-based heatmap formulation from UTSP (Min et al). This heatmap-based formulation enabled UTSP to be very sample-efficient and work with unsupervised learning. This sample-efficiency advantage seems to transfer to semi-supervised setting. Third, authors apply the combination of these techniques into semi-supervised setting. For unlabeled instances, authors propose a formulation where two reverse processes are used. On stand benchmark configurations, the proposed method demonstrates consistent improvement over baselines."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Quality: The proposed method combines most recent techniques for Neural TSP solvers. Experimental results show quite consistent improvement over baseline methods; gap reduction is often reduced to 1/2~1/3 from a strong baseline method DIFUSCO. Ablation studies provide good justification for both of technical innovations: semi-supervised learning, and heatmap-based formulation on top of DIFUSCO.\n\nOriginality & Significance: While authors build upon DIFUSCO and UTSP, it has been unclear whether the combination of these techniques, combined with semi-supervised learning, actually works. Hence, the paper provides readers a convincing recipe for building state-of-the-art neural TSP solver. This will help the research community to establish a best practice. It is also good to confirm that Discrete Diffusion model from DIFUSCO continues to work well on another heatmap-based formulation. This helps the community to be more confident about discrete diffusion-based methods, and motivate people to follow-up.\n\nClarity: Overall technical strategy of the paper is easy to understand."
                },
                "weaknesses": {
                    "value": "While one of the key contributions of this paper is the use of semi-supervised learning, I find its formulation is not sufficiently explained. The key equation (8) for semi-supervised loss is presented without much motivation or justification. It seems like there are two diffusion processes, one for prediction and the other for training, but I wasn't sure. \n\nMuch of the training & inference recipe comes from Sun & Yang. In order to highlight authors' contribution, I would suggest to focus on one setting which worked best (or reasonably) in Sun & Yang (say, MCTS), and spend more pages on deeper analysis of semi-supervised learning, which is the main contribution of the paper. Table 3 does show that semi-supervised learning improves upon supervised baseline,  but more could've been explore to strengthen the unique contribution of the paper. For example, how does semi-supervised learning help in generalization to larger instances? For ex, can we use supervised labels of TSP-50 to generalize to TSP-1000?"
                },
                "questions": {
                    "value": "For semi-supervised instances, is $x_T$ sampled from a uniform distribution? Is the model trained to predict its own output?\n\nIn Section 3.2, \"T[i,j] indicates the probability of the j-th in a tour sequence is the ith node\" seems to be a grammatical error? Overall, I hope authors explained the heatmap and transition matrix better, at least in the appendix, because it is an important component of this work."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4355/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699168866920,
            "cdate": 1699168866920,
            "tmdate": 1699636406632,
            "mdate": 1699636406632,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]