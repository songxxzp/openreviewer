[
    {
        "title": "MVSFormer++: Revealing the Devil in the Transformer's Details for Multi-View Stereo"
    },
    {
        "review": {
            "id": "AcRTD8A0zU",
            "forum": "wXWfvSpYHh",
            "replyto": "wXWfvSpYHh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission639/Reviewer_Xp5d"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission639/Reviewer_Xp5d"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes MVSFormer++, which is an extended/enhanced version of the previous work MVSFormer. The authors have well-studied the usage of the transformer at different stages of the learning-based MVS pipeline, and have demonstrated the effectiveness of the proposed components by extensive experiments. The proposed pipeline achieves SOTA results on several MVS datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The method achieves SOTA results on DTU, Tanks and Temples, and ETH3D datasets. I believe currently it is one of the best-performing MVS approaches.\n\n- The authors have conducted extensive experiments to demonstrate the effectiveness of the proposed components. I can find ablation studies on each proposed component in the experimental section."
                },
                "weaknesses": {
                    "value": "- The paper proposed a bunch of small components/tricks over the previous MVSFormer. I acknowledge that these tricks might be useful, however, I would feel like each of them is a bit incremental and the whole story is not that interconnected. For the conference paper, I prefer a neat idea/key component that can bring strong improvements. The paper looks more like an extended journal version paper of the previous one.\n\n- ETH3D evaluation: the proposed method does not perform well on ETH3D even compared with other learning-based approach (e.g., Vis-MVSNet, EPP-MVSNet). Could the authors explain potential causes?\n\n- These is no a limitation section. I would like know the scalability of the proposed method, will the memory cost dramatically increased compared with CNN based approaches (e.g., CasMVSNet) when the image size/depth sample number increase?"
                },
                "questions": {
                    "value": "See above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission639/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698595606449,
            "cdate": 1698595606449,
            "tmdate": 1699635991800,
            "mdate": 1699635991800,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oTUA9MRvYN",
                "forum": "wXWfvSpYHh",
                "replyto": "AcRTD8A0zU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission639/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission639/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Xp5d"
                    },
                    "comment": {
                        "value": "Thanks for the valuable feedback on our work. We have thoroughly investigated that MVSFormer++ enjoys good scalability. And we would like to engage in further discussions to address any confusion.\n\n**1.Incremental ideas rather than neat ones with strong improvements?**\n\nThanks for this comment. We would like to clarify that our contributions are not merely incremental. The decision to build upon MVSFormer is strategic, as it serves as a simple yet highly effective baseline with strong scalability and significant potential. With the development of pre-trained ViTs and the widespread adoption of transformers in feature-matching tasks, as highlighted in the second paragraph of the introduction, exploring the reasonable usage of transformers in MVS becomes very important. MVSFormer++ systematically addressed three key challenges that hinder effective transformer-based learning in MVS (already previously outlined in the third paragraph of the introduction):\n\n1) *Tailored attention mechanisms for different MVS modules.* We provide insight indicating that linear attention is well-suited for feature encoding with superior robustness in terms of sequential length, while cost volume regularization benefits more from spatial relations, favoring vanilla and window-based attention mechanisms.\n\n2) *Incorporating cross-view information into Pre-trained ViTs.* Our Side View Attention (SVA) efficiently learns cross-view information through frozen DINOv2.\n\n3) *Enhancing Transformer\u2019s Length Extrapolation Capability in MVS.* We effectively address the length extrapolation issue through normalized 2D-PE, Frustoconical Positional Encoding (FPE), and Adaptive Attention Scaling (AAS) as verified in Table12.\n\nMoreover, our neat and simple combination of Cost Volume Transformer (CVT) enhanced with FPE and AAS achieves substantial improvements based on not only MVSFormer but also CasMVSNet as shown in Table13 of Appendix A.5.\nWe anticipate that our work could benefit the community by bridging the gap between MVS learning and reliable transformer utilization.\n\n**2.ETH3D evaluation is not competitive enough when compared to Vis-MVSNet and EPP-MVSNet.**\n\nThanks for this point. We need to clarify that our ETH3D results are all achieved with the same threshold (0.5) of depth confidence filter and default settings of dynamic point cloud fusion (DPCD)[1] without any cherry-pick hyper-parameter adjusting. This approach ensures a fair and unbiased evaluation. Though improvements may not be very significant, they are consistent and indicative of the robustness and competitiveness of MVSFormer++.  We have claimed these points in the paper revision. For the results of Vis-MVSNet and EPP-MVSNet, unfortunately, we could not find their detailed hyper-parameter settings for ETH3D to re-implement their results. Since our goal is to propose a generalized and powerful MVS method, we focus on the intrinsical depth estimation quality rather than sophisticated post-precessing tricks. Our codes and evaluation scripts will be fully open-released if accepted.\n\n[1] Yan J, Wei Z, Yi H, et al. Dense hybrid recurrent multi-view stereo net with dynamic consistency checking. ECCV2020.\n\n**3.Scalability: how does the memory cost increase when the image/depth sample number increases?**\n\nThanks for this insightful comment. We have investigated the scalability in both image and depth sizes and compared MVSFormer++ to CasMVSNet in Table11. Benefiting from an efficient attention implementation[2] and a more reasonable 4-stage depth hypothesis setting, MVSFormer++ demonstrates robust scalability for dense and precise depth estimation. The development of the highly optimized attention mechanism also improves the significance of our work in advancing the effective utilization of transformers in MVS. Moreover, our CVT enhanced by FPE and AAS could also be well extended into large-scale images, which overcomes the key challenge of previous transformer-based manners.\n\n[2] Dao T. Flashattention-2: Faster attention with better parallelism and work partitioning[J]. arXiv preprint arXiv:2307.08691, 2023.\n\n**4.Missing discussion about limitation**\n\nThanks for this point. We have provided related discussions in Appendix A.8. Though MVSFormer++ enjoys powerful MVS capability as well verified in our experiments, it still suffers from similar limitations as other coarse-to-fine MVS models.\nSpecifically, the coarse stage struggles for inevitable error estimations for tiny foregrounds in low-resolution inputs, resulting in error accumulations for the following stages as shown in Figure12. Designing a novel dynamic depth interval selection strategy would be a potential solution to handle this problem, which can be seen as interesting future work."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission639/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700143075233,
                "cdate": 1700143075233,
                "tmdate": 1700146190162,
                "mdate": 1700146190162,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Y6Z4lzW6kK",
            "forum": "wXWfvSpYHh",
            "replyto": "wXWfvSpYHh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission639/Reviewer_4iWa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission639/Reviewer_4iWa"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces MVSFormer++, a learning-based Multi-View Stereo (MVS) method that leverages pre-trained models to enhance depth estimation in MVS. The study tackles a crucial gap in existing research by exploring the impact of transformers on various MVS modules. The paper's motivation is clear. However, there are notable areas that require attention for improvement."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper innovatively introduces transformer-based models and attention mechanisms to address a vital issue in MVS. The novelty lies in the thorough exploration of different transformer attention mechanisms across diverse MVS modules. The paper provides hypotheses and experimental evidence supporting the use of different attention mechanisms in the feature encoder and cost volume regularization.\n\nThe authors conducted experiments across multiple benchmarks, including DTU, Tanks-and-Temples, BlendedMVS, and ETH3D, showcasing MVSFormer++'s state-of-the-art performance on challenging benchmarks (DTU and Tanks-and-Temples). This highlights the practical significance of the proposed approach.\n\nThe paper includes well-executed ablation studies, comparing the impacts of different attention mechanisms on various MVS modules."
                },
                "weaknesses": {
                    "value": "(1) Clarity and Detail:\nThe paper lacks detailed explanations of specific design choices, such as the rationale behind selecting DINOv2 as the base model. Additionally, the utilization of different levels of DINOv2 features is not clearly elucidated. It is recommended to include these details to enhance the manuscript's clarity and independence.\n\n(2) Experiments:\nWhile the incorporation of DINOv2 in the feature extraction stage significantly enhances performance, it is crucial to clarify that this improvement is not solely due to the increase in the number of parameters. The improvement in point cloud evaluation metrics by the proposed module during ablation experiments appears subtle. To bolster the paper's experimental support, I recommend validating the proposed module's effectiveness by integrating it into baseline methods and conducting a comparative analysis. This would provide a clearer understanding of the module's actual contribution.\n\nAdditionally, it is advisable to explore more pre-trained models and conduct ablation experiments without pre-trained models. Given DINOv2's frozen state during training, fine-tuning it serves as a pivotal baseline.\n\nFurthermore, the paper should include visual comparisons of depth maps to visually demonstrate the accuracy advantages of the estimated depth maps.\n\n(3) Discussion of Limitations:\nThe paper lacks a discussion of the limitations and failure cases of MVSFormer++. Understanding the method's limitations is crucial for evaluating its real-world applicability."
                },
                "questions": {
                    "value": "(1) DINOv2 Pre-training Choice:\nWhat motivated the decision to freeze DINOv2 during pre-training? How does it uniquely contribute to your method? Would including experiments with different pre-trained models or fine-tuning DINOv2 serve as valuable comparisons?\n\n(2) Cost-Benefit Analysis of DINOv2:\nConsidering the marginal improvement in point cloud metrics, is the increase in network parameters due to adopting DINOv2 justified? How can you demonstrate that the metric enhancements stem from the introduced module's contribution rather than a mere increase in parameters?\n\n(3) Discussion of MVSFormer++ Limitations:\nCould you briefly discuss MVSFormer++'s limitations, especially in scenarios where it might underperform?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission639/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission639/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission639/Reviewer_4iWa"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission639/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698656161312,
            "cdate": 1698656161312,
            "tmdate": 1699635991717,
            "mdate": 1699635991717,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "d8sde7CdTK",
                "forum": "wXWfvSpYHh",
                "replyto": "Y6Z4lzW6kK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission639/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission639/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 4iWa (part1/2)"
                    },
                    "comment": {
                        "value": "Thanks for the valuable feedback on our work. We would like to clarify that our proposed components enjoy substantial improvements upon other baselines. And we would like to engage in further discussions to address any confusion.\n\n**1.More elucidation about the selection of DINOv2 and the utilization of different levels of DINOv2.**\n\nThanks for your insightful comments.  We apologize for the insufficient discussion about DINOv2. We have provided more discussions about the usage of DINOv2 and its feature layers in Sec3 Preliminary and Appendix A.1 in our paper revision.\nOur choice of DINOv2[1] as the backbone of MVSFormer++ is attributed to its robust zero-shot feature-matching capability and impressive performance in dense vision downstream tasks. Compared to other ViTs, DINOv2 was trained on large-scale curated data, unifying both image-level (contrastive learning) and patch-level (masked image prediction) objectives, as well as high-resolution adaption. Hence DINOv2 enjoys remarkably stable feature correlation across different domains and better performance in dense vision benchmarks (segmentation and monocular depth estimation). Our experiments in Table10 (listed below) show the efficacy of DINOv2 in MVS.\n\n| ViT backbone | Frozen backbone | Accuracy$\\downarrow$ | Completeness$\\downarrow$ | Overall$\\downarrow$ |\n|--------------|-----------------|----------------------|--------------------------|------------------------------------|\n| DINO-small   | $\\checkmark$    | 0.327                | 0.265                    | 0.296               |\n| DINO-base    | $\\checkmark$    | 0.334                | 0.268                    | 0.301               |\n| Twins-small  | $\\times$        | 0.327                | 0.251                    | 0.289               |\n| Twins-base   | $\\times$        | 0.326                | 0.252                    | 0.289               |\n| DINOv2-base  | $\\checkmark$    | **0.3198**     | 0.2549                   | 0.2875              |\n| DINOv2-base  | LoRA (rank=16)  | 0.3239               | **0.2501**          | **0.2870**     |\n\nFor the selection of DINOv2 layers, we have carefully analyzed the feature characteristics in Figure6 and Table6 of the Appendix. Here we provide more detailed discussions as follows. We show the WTA depth estimations of DINOv2 in Figure6(b), which indicate middle layers are more useful to MVS. So we empirically compare the layer ablations in Table6 to verify our claims. Generally, our final combination (3,7,11) is a good selection. Though the 5th layer shows better zero-shot WTA feature correlations in Figure6(b), the gap between (3,5,11) and (3,7,11) is not obvious. As shown in Figure6(a), layers from 8 to 10 are unstable, which would cause inferior results to the 4-layer setting. More details are discussed in Appendix A.1.\n\nWe should clarify that the usage of DINOv2 and other ViT backbones are not the main contributions of this paper. Our improvements are not solely dependent on the increased capacity of pre-trained ViTs. We will explain this in the next question.\n\n[1] Oquab M, Darcet T, Moutakanni T, et al. Dinov2: Learning robust visual features without supervision[J]. arXiv preprint arXiv:2304.07193, 2023.\n\n**2.The improvement is subtle when compared to the usage of DINOv2.**\n\nThanks for this point.  Although MVSFormer+DINOv2 is a strong baseline (Table10), we need to clarify that our work is orthogonal to the DINOv2. DINOv2 just improved the overall distance from 0.289 to 0.2875 with less computation (frozen backbone compared with Twins), while we further improved the overall distance from 0.2875 to 0.2805 with our novel designs of SVA, CVT, and image scaling adaption.\n\n**3.DINOv2 fine-tuning result as the pivotal baseline.**\n\nThanks. As discussed in MVSFormer[2], fine-tuning a Plain ViT (DINO, DINOv2) is very costly, especially for high-resolution images. Thus we try to optimize the low-rank fine-tuning (LoRA[3]) rank=16 as a new baseline to verify whether more trainable parameters of the ViT backbone result in prominent improvements (Table10).\nThe result indicates that more trainable parameters in the backbone only achieve minor improvements. This result also proves the effectiveness of our proposed components, which further push the strong baseline to the limit. \n\n[2] Cao C, Ren X, Fu Y. MVSFormer: Multi-View Stereo by Learning Robust Image Features and Temperature-based Depth. TMLR2022.\n[3] Hu E J, Shen Y, Wallis P, et al. Lora: Low-rank adaptation of large language models[J]. arXiv preprint arXiv:2106.09685, 2021."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission639/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700141772785,
                "cdate": 1700141772785,
                "tmdate": 1700152492072,
                "mdate": 1700152492072,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "y84vstu9t0",
                "forum": "wXWfvSpYHh",
                "replyto": "Y6Z4lzW6kK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission639/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission639/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 4iWa (part2/2)"
                    },
                    "comment": {
                        "value": "**4.Validating the proposed modules into other baseline methods (with more pre-trained models or without pre-training).**\n\nThanks. We should clarify that our work just focuses on transformer usage in MVS rather than pre-training for MVS. \nMVSFormer[2] has thoroughly discussed the effect of different types of pre-training on MVS. \nTo further prove the effectiveness of our proposed method, we provide more detailed experiments in the revision in Table13 of Appendix A.5. \n\nIn Table13, we add our contributions (CVT and SVA) to DINOv1 pre-training based MVSFormer (MVSFormer-P) and CasMVSNet (without pre-training).\nWe re-train CasMVSNet (CasMVSNet*) as an intermediate baseline for a fair comparison, which contains a 4-stage depth hypothesis (32-16-8-4) and cross-entropy loss, sharing the same setting with MVSFormer and MVSFormer++. \nSince the proposed SVA is a side-tuning module specifically designed for pre-trained models, we only evaluate the effect of SVA on MVSFormer-P. From Table.13, our CVT demonstrates substantial improvements for both CasMVSNet* and MVSFormer-P, and our SVA further enhances the results of MVSFormer-P with CVT. \nThese results demonstrate the generalization of all proposed components. We added these discussions to the paper revision.\n\n**5.Missing visual comparisons of depth maps of MVSFormer and MVSFormer++.**\n\nThanks. We added qualitative comparisons and related discussions for the depth map in Figure9 of Appendix, Our MVSformer++ can estimate more precise depth maps even in challenging scenes. \n\n**6.Lacks a discussion of the limitations and failure cases.**\n\nThanks for this point. We have provided related discussions in Appendix A.8. Though MVSFormer++ enjoys powerful MVS capability as well verified in our experiments, it still suffers from similar limitations as other coarse-to-fine MVS models.\nSpecifically, the coarse stage struggles for inevitable error estimations for tiny foregrounds in low-resolution inputs, resulting in error accumulations for the following stages as shown in Figure12. Designing a novel dynamic depth interval selection strategy would be a potential solution to handle this problem, which can be seen as interesting future work."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission639/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700142465664,
                "cdate": 1700142465664,
                "tmdate": 1700142465664,
                "mdate": 1700142465664,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EmIchBApOw",
                "forum": "wXWfvSpYHh",
                "replyto": "Y6Z4lzW6kK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission639/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission639/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 4iWa (more discussion about fine-tuned DINOv2 and cost-benefit analysis)"
                    },
                    "comment": {
                        "value": "**1.Supplemental result of DINOv2 full-model fine-tuning.**\n\nWe apologize for the delay in submitting the full-model fine-tuning result of DINOv2, as it requires a significant amount of computation. As mentioned in MVSFormer[2], we carefully set the learning rate of DINOv2 backbone to 3e-5, while all other learning rates for MVS modules are still 1e-3.\nAll results of DINOv2-based MVS are compared as follows:\n\n| Exp      | Acc    | Comp   | Overall |\n|----------|--------|--------|---------|\n| DINOv2 (frozen)   | **0.3198** | 0.2549 | 0.2875  |\n| DINOv2 (LoRA) | 0.3239 | **0.2501** | **0.2870**  |\n| DINOv2 (fine-tuned) | 0.3244 | 0.2566 | 0.2905  |\n\nWe would clarify that fine-tuning the whole DINOv2 is costly and achieves inferior performance compared with the frozen one. This phenomenon makes sense. Since DINOv2 is a robust model pre-trained on large-scale curated data, the MVS fine-tuning is based on the limited DTU, which degrades the generalization of DINOv2\u2019s matching capability. \nWe added related results and discussion to the Appendix.\nBased on these results, our usage of frozen DINOv2 is convincing.\n\n**2.Cost-benefit analysis of DINOv2. Parameters increasing or module improving?**\n\nThanks. Since we have accomplished all related experiments of DINOv2, we can prove the improvement does not rely on the cost of more trainable parameters of DINOv2. More importantly, fine-tuning DINOv2 with more trainable parameters could not achieve equivalent improvements as our proposed modules (Overall 0.2905 vs 0.2805). \n\nOn the other hand, our proposed SVA is simplified with fewer self-attention blocks, while normalized 2D-PE, FPE (3D-PE), AAS, adaptive layer scaling and Pre-LN introduce substantial improvements for both depth and point clouds with negligible parameters.\nThe additional scalability study in Appendix A.2 (Table11) also demonstrates the good efficiency of our model.\nSo the superior performance of MVSFormer++ relies on reasonable model designs rather than simply using more parameters."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission639/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700450867145,
                "cdate": 1700450867145,
                "tmdate": 1700452578446,
                "mdate": 1700452578446,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "P0DyXTl8qj",
            "forum": "wXWfvSpYHh",
            "replyto": "wXWfvSpYHh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission639/Reviewer_Wn6x"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission639/Reviewer_Wn6x"
            ],
            "content": {
                "summary": {
                    "value": "This paper enhances MVSFormer by infusing cross-view information into the pre-trained DINOv2 model and exploring different attention methods in both feature encoder and cost volume regularization. It also dives into the detailed designs of the transformer in MVS, such as the positional encoding, attention scaling, and position of LayerNorm."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper explores the detailed designs of attention methods in the context of MVSNet. \n2. It exploits the pre-trained DINOv2 in the feature encoder and merges the information of source views by cross-view attention.\n3. It designs a 3D Frustoconical Positional Encoding on the normalized 3D position, which is interesting and shows good improvements in depth map accuracy.\n4. It validates that attention scaling helps the scaling of the transformer to different resolutions, and the position of LayerNorm can affect the final accuracy."
                },
                "weaknesses": {
                    "value": "Although the MVSFormer++ modifies the base model MVSFormer by DINOv2, SVA, Norm& ALS, FPE, etc, the core contributions share similar designs with other MVS methods.\n\n1. In the feature encoder, the Side View Attention is similar to the Intra-attention and Inter-attention in Transmvsnet. The main differences are that this paper uses a pre-trained DINOv2 as input and removes the self-attention for source features.\n2. The use of linear attention in the feature encoder has already been proposed in Transmvsnet.\n3. In Table 9, although with a larger network, the MVSFormer++ only improves on MVSFormer by a small margin, which can not fully support the claim of the effectiveness of 2D-PE and AAS.\n4. The FPE in Table 4 shows good improvement on CVT. The detailed network structure should be made more clear. Please see the questions. \n5. The evidence for the minor changes such as the LN and AAS is not strong with experiments on only DTU. They are more intuitive and may need more experiments to prove whether they are generalizable designs. For example, Table 9 on ETH3D actually cannot fully support AAS."
                },
                "questions": {
                    "value": "1. I would to know the detailed structure differences between CVT and CVT+FPE. CVT is only used in the first coarse stage so how many stages use the CVT+FPE in Table 4? What are the results when CVT and CVT+FPE are both used in all stages or only the first coarse stage?\n2. The paper can be improved by focusing more on the novel and interesting designs such as the FPE and analyzing more on it."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission639/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698744276042,
            "cdate": 1698744276042,
            "tmdate": 1699635991649,
            "mdate": 1699635991649,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RYessGiWbj",
                "forum": "wXWfvSpYHh",
                "replyto": "P0DyXTl8qj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission639/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission639/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Wn6x (part1/2)"
                    },
                    "comment": {
                        "value": "Thanks for the valuable feedback on our work. We would like to clarify some misunderstandings from the reviewer about the contribution of our work. And we would like to engage in further discussions to address any confusion.\n\n**1.Side View Attention (SVA) is similar to the Intra-attention and Inter-attention in Transmvsnet.**\n\nThanks for this comment. Besides the pre-trained DINOv2 features and highly simplified architecture (only self-attention for reference features), SVA makes a substantial contribution as a \"side-tuning module\" [1] based on a frozen pre-trained model (refer to Sec 3.1.1, sentence 2). It's crucial to highlight that no gradients are propagated through the frozen DINOv2+SVA, eliminating the need to store the middle tensor of DINOv2 (achieved with torch.no_grad), resulting in significant GPU memory savings. This way is very efficient to incorporate cross-view information into monocular pre-trained vision models. To the best of our knowledge, we are the first to learn cross-view information for DINOv2 with efficient side-tuning. Moreover, we carefully design the normalized 2D-PE for the robustness of various image scales, the position of layer norm (Norm), and adaptive layer scaling (ALS) for stable and better training convergence, pushing the overall performance to the limit (0.2847->0.2805). As the development of large pre-trained models and transformers for 3D vision, our contributions built upon incrementally learning cross-view information for pre-trained ViTs, and detailed transformer designs for feature encoding are beneficial to the MVS community.\n\n[1] Zhang J O, Sax A, Zamir A, et al. Side-tuning: a baseline for network adaptation via additive side networks. ECCV2020.\n\n**2.Linear attention has been proposed in Transmvsnet.**\n\nThanks for this point. Although Transmvsnet leveraged linear attention for feature learning, they involved a compromise to save the attention computation, which leaks in-depth discussion about transformer usage in MVS. In our study, we thoroughly investigate the impact of various attention mechanisms for different MVS parts, as outlined in Table 5. As shown in Table5(right), linear attention became the best choice of **feature encoding** because of the global respective fields and image scaling robustness as discussed in Sec.3.1.1 and Sec4.2.3. We have also revealed an insightful finding that **linear attention enjoys good performance in feature encoding, but it performs very terribly in cost volume (Table5 left)**. Since features in cost volume are built upon feature similarity, leaking the essentially informative feature presentation, feature aggregating-based linear attention cannot handle the cost volume regularization compared with spatial aggregating-based attention (discussed in Sec3.2.1 paragraph2). We hope that such an insightful contribution could benefit the community for a better combination of transformer and MVS learning.\n\n**3.Improvements of MVSFormer++ compared to MVSFormer are marginal in Table9.**\n\nThanks. We need to clarify that our ETH3D results are all achieved with the same threshold (0.5) of depth confidence filter and default settings of dynamic point cloud fusion (DPCD)[2] without any cherry-pick hyper-parameter adjusting. This approach ensures a fair and unbiased evaluation. Though improvements may not be very significant, they are consistent and indicative of the robustness and competitiveness of MVSFormer++.  We have claimed these points in the paper revision. \n\n[2] Yan J, Wei Z, Yi H, et al. Dense hybrid recurrent multi-view stereo net with dynamic consistency checking. ECCV2020.\n\n**4.More experiments to prove the effectiveness of the proposed components, such as Normalization 2D-PE and AAS.**\n\nThanks for this valuable feedback. We appreciate the reviewer's suggestion for additional experiments to further establish the effect of the proposed components, including Normalized 2D-PE, FPE, and AAS. To further prove the effect of our proposed methods for handling high-resolution scenes, we detail additional ablation studies to Table12 in Appendix A.3 with different image sizes and discuss their influence on MVS depth estimation. \nBoth high-resolution depth and point cloud results outperform the low-resolution ones, which proves the importance of adaptability for high-resolution images. Moreover, FPE and AAS improve the results under both 576\\*768 and 1152\\*1536 images. However, AAS is more effective for depth estimation in high-resolution cases, while the depth gap in low-resolution ones is not obvious. The normalized 2D-PE plays a very important role in high-resolution feature encoding, contributing substantial improvements in both depth and point clouds.\nWe have also added these results and discussions to the paper revision.\nFor the LayerNorm and adaptive layer scaling, we clarify that these techniques are important to stabilize the SVA training based on frozen DINOv2, which has already been verified clearly in Table4 and Table6."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission639/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700140321555,
                "cdate": 1700140321555,
                "tmdate": 1700140757681,
                "mdate": 1700140757681,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eCN6tX4htH",
                "forum": "wXWfvSpYHh",
                "replyto": "P0DyXTl8qj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission639/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission639/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Wn6x (part2/2)"
                    },
                    "comment": {
                        "value": "**5.More details about CVT and FPE; results of CVT+FPE in all stages.**\n\nThanks for this valuable comment. We have to clarify both CVT and FPE are only used in the first stage (1/8 resolution) as introduced in Sec3.2.1 paragraph2. So CVT+FPE is only applied to the first stage in Table4. As discussed in Sec.3.2.1, we find that CVT for other stages would cause inferior performance. Because only the first stage in a cascade model enjoys a complete and continuous 3D scene. Therefore, the integrality and continuity of the cost volume are very important for CVT. Although FPE is the key to unlocking the capacity of CVT (like position encoding for Transformer), for the 3DCNN-based cost volume regularization of other stages(>1), FPE is not necessary. Because CNN with zero padding could provide sufficient position clues[3]. We further clarify this in the paper revision.\n\n[3] Islam M A, Jia S, Bruce N D B. How much position information do convolutional neural networks encode? ICLR2020."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission639/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700140417805,
                "cdate": 1700140417805,
                "tmdate": 1700140417805,
                "mdate": 1700140417805,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qNCIm9JTAY",
                "forum": "wXWfvSpYHh",
                "replyto": "P0DyXTl8qj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission639/Reviewer_Wn6x"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission639/Reviewer_Wn6x"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response and further questions."
                    },
                    "comment": {
                        "value": "Thanks for the detailed response from the authors. I have further questions about the rebuttal and the revision.\n\n1. The SVA and the Intra-attention and Inter-attention in Transmvsnet.\n\nThe rebuttal has explained the detailed differences between the SVA and the Intra-attention and Inter-attention in Transmvsnet. The contribution of this part should focus more on introducing the pre-trained ViT into the existing Intra-attention and Inter-attention framework. The paper must be revised to discuss more on the existing Intra-attention and Inter-attention methods instead of referring to the \"Side-tuning\" method because we need to focus more on the network differences in the context of the practical operations of MVS. With this discussion, this part can make the new contributions such as the DINOv2 more clear.\n\n2. The linear attention.\n\nThe explanation of \"in-depth discussion and more experiments\" is not very strong because from Table 5, the improvement of linear attention v.s Vanilla is minor according to the $e_2$ and $e_4$. The previous motivation of saving memory makes more sense. The paper revision should also discuss more on the usage of linear attention in existing MVS methods.\n\n\n3. Improvements of MVSFormer++ compared to MVSFormer are marginal in Table9.\n\nThe MVSFormer also used dynamic point cloud fusion (DPCD) for ETH3D. Therefore, this part of the rebuttal does not provide more information about the marginal improvement. \n\n4. The LayerNorm and adaptive layer scaling.\n\nThanks for the more experiments on the different image scales. For the LayerNorm and adaptive layer scaling, the results in Table 4 and Table 6 are rather minor: 0.2815 v.s 0.2805 in Table 4, and 0.2827 v.s 0.2850 in Table 6. Without Norm&ALS, the method performs even better on $e_2$ and $e_4$ in Table 4. The depth error is a more direct evaluation metric so I think the strength of Norm&ALS are not clearly supported by Table 4, although this paper discusses a lot about it. From my opinion, since it is not fully supported by the experiments, this paper may degrade this contribution.\n\nThanks again for the authors' efforts of providing more experiments and discussions."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission639/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700455140377,
                "cdate": 1700455140377,
                "tmdate": 1700455408347,
                "mdate": 1700455408347,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ydFKaYYbwc",
            "forum": "wXWfvSpYHh",
            "replyto": "wXWfvSpYHh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission639/Reviewer_3kfC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission639/Reviewer_3kfC"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents an enhanced iteration of MVSFormer named as MVSFormer++. The method utilizes the Side View Attention (SVA) to empower the cross-view learning ability of DINOv2. It prudently maximizes the inherent characteristics of attention to enhance various components of the MVS pipeline. The results MVSFormer++ achieves on the DTU and Tanks-and-Temples benchmarks show the model works quite well."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The design of Side View Attention (SVA) is effective.\n2. Compared to other models, MVSFormer++ has better performance.\n3. The FPE and AAS are used efficiently to generalize high-resolution images.\n4. The paper is well written, and one can easily grasp the main idea."
                },
                "weaknesses": {
                    "value": "1. In the ablation study, the results of Norm&ALS under the depth error ratios of 2mm and 4mm are slightly inferior.\n2. A discussion regarding the limitations is missing. \n3. Minor: Section 4.1 Experimental performance, mean F-score is 41.75 on the Advanced sets in the text while in Tab.3 it is 41.70."
                },
                "questions": {
                    "value": "Please refer to the weaknesses above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission639/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission639/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission639/Reviewer_3kfC"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission639/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698748766639,
            "cdate": 1698748766639,
            "tmdate": 1699635991575,
            "mdate": 1699635991575,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ml4mIHK2sU",
                "forum": "wXWfvSpYHh",
                "replyto": "ydFKaYYbwc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission639/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission639/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 3kfC"
                    },
                    "comment": {
                        "value": "Dear reviewer 3kfC,\n\nThanks for the valuable feedback on our work. We provide more details about the experiments and limitations. And we would like to engage in further discussions to address any confusion.\n\n**1.Depth results of 2mm and 4mm are inferior with Norm&ALS.**\n\nThanks for this point. We have to clarify that depth annotations of DTU are not complete and absolutely correct as discussed in [1]. Thus the results of the point cloud should be more convincing. Moreover, Norm&ALS is mainly working for more stable feature learning in low-resolution (DINOv2 feature), so it results in fewer large-depth errors. This result conforms to the expectation of DINOv2+SVA which learns more robust low-resolution depth, while precise high-resolution depth should be learned by convolutions. Better results of point clouds also prove our aforementioned opinion.\n\n[1] Luo K, Guan T, Ju L, et al. Attention-aware multi-view stereo CVPR2020.\n\n**2.Missing limitation discussion.**\n\nThanks for this point. We have provided related discussions in Appendix A.8. Though MVSFormer++ enjoys powerful MVS capability as well verified in our experiments, it still suffers from similar limitations as other coarse-to-fine MVS models.\nSpecifically, the coarse stage struggles for inevitable error estimations for tiny foregrounds, resulting in error accumulations for the following stages as shown in Figure12. Designing a novel dynamic depth interval selection strategy would be a potential solution to handle this problem, which can be seen as interesting future work.\n\n**3.Wrong text description.**\n\nWe apologize for the typo here. We revised the result to 41.70 in the revision."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission639/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700139588378,
                "cdate": 1700139588378,
                "tmdate": 1700139588378,
                "mdate": 1700139588378,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "vvsXgP1OEs",
            "forum": "wXWfvSpYHh",
            "replyto": "wXWfvSpYHh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission639/Reviewer_B1hX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission639/Reviewer_B1hX"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes an enhanced version of MVSFormer. In particular, it specifically addressed three challenges that remained in previous works: tailored attention mechanisms for different MVS modules, incorporating cross-view information into pre-trained ViTs, and enhancing Transformer's length extrapolation capability. Experimental results demonstrated the proposed MVSFormer++ attains state-of-the-art results across multiple benchmark datasets, including DTU, Tanks-and-Temples, BlendedNVS, and ETH3D."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ The contributions of this work are solid and well address the limitations of previous MVS methods. For example, introducing side view attention significantly elevates depth estimation accuracy, resulting in substantially improved MVS results.\n+ The combination of frustoconical positional encoding and adaptive attention scaling is interesting. It enhances the model's ability to generalize across a variety of image resolutions while avoiding attention dilution issues.\n+ The experiments are comprehensive and promising. Almost all classical and SOTA methods are considered in the comparison experiments, which are evaluated on various datasets. For visual comparisons, the proposed method significantly outperforms other competitive methods, showing more complete structure and fewer geometric distortions."
                },
                "weaknesses": {
                    "value": "- Except for the customized designs beyond the MVSFormer, this work leverages DINOv2 as a new backbone (compared to DINO used in the MVSFormer). It would be interesting to see how the performance of MVSFormer++ changes when it keeps the same backbone as that of MVSFormer.\n- MVSFormer and MVSFormer++ show different reconstruction performances regarding different cases on Tanks-and-Temples (Table 3). The authors are suggested to provide more discussions on how the qualitative results differ (like local details and global distributions) and why the degenerations happen.\n- The performance of the complete version of this work in the ablation study is different from the quantitative results reported in Table 2. Please elaborate on this inconsistency in metrics.\n- The baseline version of MVSFormer (without CVT, FPE, AAS, SVA, Norm&ALS) seems kind of strong already. Does it gain from the strong backbone? Moreover, the qualitative results of the ablation study are expected to be provided.\n- The description of Normalization and Adaptive Layer Scaling is ambiguous and unclear. More details about the motivation and implementation would be helpful to understand this part."
                },
                "questions": {
                    "value": "Please refer to the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission639/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission639/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission639/Reviewer_B1hX"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission639/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698765292720,
            "cdate": 1698765292720,
            "tmdate": 1699635991482,
            "mdate": 1699635991482,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Knge2QXt46",
                "forum": "wXWfvSpYHh",
                "replyto": "vvsXgP1OEs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission639/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission639/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer B1hX (part1/2)"
                    },
                    "comment": {
                        "value": "Dear reviewer B1hX,\n\nThanks for the valuable feedback on our work. We provide more discussions about the backbone selection of MVSFormer++ and other aspects to address your confusion. And we would like to engage in further discussions to address any confusion.\n\n**1.Performance changing of DINOv2 backbone in MVSFormer++; does the improvement gain from a strong backbone?**\n\nThanks for this comment. The baseline result of MVSFormer enhanced by frozen DINOv2 is listed in the first row of Table4. We further compare the baseline of MVSFormer+DINOv2 with MVSFormer as follows (Table10 of the revision):\n\n| ViT backbone | Frozen backbone | Accuracy$\\downarrow$ | Completeness$\\downarrow$ | Overall$\\downarrow$ |\n|--------------|-----------------|----------------------|--------------------------|------------------------------------|\n| DINO-small   | $\\checkmark$    | 0.327                | 0.265                    | 0.296               |\n| DINO-base    | $\\checkmark$    | 0.334                | 0.268                    | 0.301               |\n| Twins-small  | $\\times$        | 0.327                | 0.251                    | 0.289               |\n| Twins-base   | $\\times$        | 0.326                | 0.252                    | 0.289               |\n| DINOv2-base  | $\\checkmark$    | **0.3198**     | 0.2549                   | 0.2875              |\n| DINOv2-base  | LoRA (rank=16)  | 0.3239               | **0.2501**          | **0.2870**     |\n\nCompared with DINO and Twins, MVSFormer based on DINOv2 enjoys slightly better results with a frozen ViT backbone. This is strengthened by robust visual features from DINOv2 verified as zero-shot cross-domain feature matching experiments in the DINOv2 paper[1]. We further clarify the motivation for selecting DINOv2 in the last of Appendix A.1. Although MVSFormer+DINOv2 is a strong baseline, we need to clarify that our work is orthogonal to the DINOv2. DINOv2 just improved the overall distance from 0.289 to 0.2875 with less computation (frozen backbone compared with Twins), while we further improved the overall distance from 0.2875 to 0.2805 with our novel designs of SVA, CVT, and image scaling adaption. To verify the effectiveness of our contribution, we further show the improvements of the proposed components based on MVSFormer-P (DINOv1) and CasMVSNet as in Tab.13 of Appendix A.5.\nOur CVT demonstrates substantial improvements for both CasMVSNet* and MVSFormer-P, and our SVA further enhances the results of MVSFormer-P with CVT.\nWe added these discussions to the paper revision.\n\n[1] Oquab M, Darcet T, Moutakanni T, et al. Dinov2: Learning robust visual features without supervision[J]. arXiv preprint arXiv:2304.07193, 2023.\n\n**2.Qualitative results of the ablation study**\n\nThanks for this point. We have compared the qualitative ablation study in Appendix Figure8.  From Figure8, CVT and AAS could effectively eliminate the outliers, while SVA with normalized 2D-PE is also critical for precise point clouds.\n\n**3.Discuss the qualitative results on Tanks-and-Temples.**\n\nThanks. We have compared more qualitative results between MVSFormer and MVSFormer++  in Figure7 of the Appendix. In general, MVSFormer++ achieves much more precise geometric reconstruction (better precision), while MVSFormer shows more complete results in some scenes, such as the \u201cRecall\u201d subfigure of \u201cPlayground\u201d. For the trading-off between precision and recall, MVSFormer++ obviously enjoys a superior balance.\nWe added these discussions to the paper revision Appendix A.7.\n\n**4.Different results of the full model in Table2 and ablation study.**\n\nWe apologize for the confusing results. Both results of MVSFormer++ in the last row of Table2 and ablations share the same model setting, i.e., our full model. The performance difference is just randomness from two different training results. Because of the server allocation issue, we re-trained the model for the ablation, and subsequently fine-tuned it for additional experiments such as Tanks-and-Temples and ETH3D. The outcomes of them are detailed below. The differences are minimal and can be attributed to the inherent randomness in the training process. \n\n| Exp      | Acc    | Comp   | Overall |\n|----------|--------|--------|---------|\n| Table2(old)   | 0.3105 | 0.2503 | 0.2804  |\n| Ablation(new) | 0.3090 | 0.2521 | 0.2805  |\n\nWe have revised the results in Table2, and appreciate your careful reading."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission639/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700139120513,
                "cdate": 1700139120513,
                "tmdate": 1700139120513,
                "mdate": 1700139120513,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PHVnZZOcyX",
                "forum": "wXWfvSpYHh",
                "replyto": "vvsXgP1OEs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission639/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission639/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer B1hX (part2/2)"
                    },
                    "comment": {
                        "value": "**5.More details about motivation and implementation of Normalization and Adaptive Layer Scaling.**\n\nThanks for this comment. For the normalization, we have discussed Pre-LN and Post-LN in the related work of \"LN and PE in Transformers''. We provide more details about them in Appendix A.1. Post-LN is the regular setting of transformer, which normalizes features after the residual addition, while Pre-LN normalizes them before the attention and Feed Forward Network (FFN).\n\nPost-LN: x=LN(x+attn(x)), x=LN(x+ffn(x)).\n\nPre-LN: x=x+attn(LN(x)), x=x+ffn(LN(x)).\n\nFor the MVS encoding, we found that Pre-LN enjoys more significant gradient updates, especially when being trained for multi-layer attention blocks (as discussed in [2]). In Table6, Pre-LN enjoys superior performance, while we find that Post-LN usually results in slower convergence.\nFor Adaptive Layer Scaling (ALS), we have analyzed the high variance issue in Figure6(a) of the Appendix, and multiplied learnable coefficients to different layers' features of frozen DINOv2. Thus our model could adaptively learn the significance of different DINOv2 layers, which stabilize the MVS training. \nWe have added all related presentations in the paper revision to make our idea clearer for readers.\n\n[2] Wang Q, Li B, Xiao T, et al. Learning deep transformer models for machine translation. ACL2019."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission639/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700139380694,
                "cdate": 1700139380694,
                "tmdate": 1700139380694,
                "mdate": 1700139380694,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Vj1adNuiKa",
                "forum": "wXWfvSpYHh",
                "replyto": "PHVnZZOcyX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission639/Reviewer_B1hX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission639/Reviewer_B1hX"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the Response"
                    },
                    "comment": {
                        "value": "Thanks for the detailed response. It addressed most of my concerns, and I am satisfied with the updated experiment as well as its analysis. Thus, I am inclined to recommend accepting this work."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission639/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700659411567,
                "cdate": 1700659411567,
                "tmdate": 1700659411567,
                "mdate": 1700659411567,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]