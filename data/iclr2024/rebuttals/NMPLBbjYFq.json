[
    {
        "title": "Large Language Models as Rational Players in Competitive Economics Games"
    },
    {
        "review": {
            "id": "d7dx0snd8Z",
            "forum": "NMPLBbjYFq",
            "replyto": "NMPLBbjYFq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2817/Reviewer_xNRn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2817/Reviewer_xNRn"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors propose to evaluate LLM using two standard games: second-price auctions and beauty games. The author measure the performances using the distance to their canonical Nash equilibria. The author also provide experimental results."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The idea of using games to using LLMs is promising."
                },
                "weaknesses": {
                    "value": "There are certain flaws of the approach. Particularly on the point of using stage Nash equilibrium to evaluate LLMs in a repeated game setting."
                },
                "questions": {
                    "value": "While I like the idea of using game to evaluate LLMs, I think there are certain flaws of this paper's approach. Particularly, the author claim the distances to Nash equilibria in these games are reasonable measures. While for a one-shot version of these game it does make sense to some extent, I believe this is no longer true for repeated version of these games. The folk theorem in repeated games indicates the set of Nash may far more larger than stage-Nash. For example in the iterated Prisoners' dillemma, the stage-Nash is (defect, defect), but people may usually regard tit-for-tat as a rational strategy. So for repeated games, the problem of equilibrium selection becomes harder.\n\n\nAnother question is about the choices of games. While these two games are very classical, this definitely limited the settings. Furthermore, isn't it possible that the LLMs may already knows like the optimal strategies in these games, given that they are very famous?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2817/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2817/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2817/Reviewer_xNRn"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2817/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698185691633,
            "cdate": 1698185691633,
            "tmdate": 1699636225076,
            "mdate": 1699636225076,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Cda3eqF6KG",
                "forum": "NMPLBbjYFq",
                "replyto": "d7dx0snd8Z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2817/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2817/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for recognising the potential of this work. Regarding the reviewer's concerns, we reply as follows.\n\n1. Measure based on NEs\n    \n> While I like the idea of using game to evaluate LLMs, I think there are certain flaws of this paper's approach. Particularly, the author claim the distances to Nash equilibria in these games are reasonable measures. While for a one-shot version of these game it does make sense to some extent, I believe this is no longer true for repeated version of these games. The folk theorem in repeated games indicates the set of Nash may far more larger than stage-Nash. For example in the iterated Prisoners' dillemma, the stage-Nash is (defect, defect), but people may usually regard tit-for-tat as a rational strategy. So for repeated games, the problem of equilibrium selection becomes harder.\n    \nThanks for highlighting this issue. For most part of the paper, we are doing multiple sessions of one-shot games, since an agent\u2019s current period action has no implication on future actions, we can use deviation away from NE as one of the measures. In the case of revelation of historical information, we understand that there could be some influence across different runs, though not in the traditional sense as in repeated games. The main focus here would be the convergence of strategies to demonstrate that agents are able to reason about other players\u2019 strategies and learn from past information. While we loosely define rational strategy in this case to be the same as NE strategy in a stage game, the best strategy that generates the highest payoffs may not be that strategy, therefore, the combination of metrics highlighted in Section 3.2 are needed to evaluate the LLMs.\n    \n2. Choice of games\n    \n> Another question is about the choices of games. While these two games are very classical, this definitely limited the settings. Furthermore, isn't it possible that the LLMs may already knows like the optimal strategies in these games, given that they are very famous?\n\nThanks for proposing the question. As shown in our updated results, if LLMs already know the optimal strategies, they shouldn\u2019t deviate from NEs when the prompt clearly states that the opponents are rational and clearly asks them to behave in the most rational manner.\n    \nA very nice feature of our EconArena is that the environment can be dynamicise in several different dimensions, e.g. the configurations of the games, the setup of the players, and the overall rationality degree of the players. Compared with static benchmarks, such dynamic setups are very unlikely to be seen during the training of LLMs, especially considering that new LLMs are rolled out these days."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2817/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700602818620,
                "cdate": 1700602818620,
                "tmdate": 1700602818620,
                "mdate": 1700602818620,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "UUm3zEpRXv",
            "forum": "NMPLBbjYFq",
            "replyto": "NMPLBbjYFq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2817/Reviewer_xpxk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2817/Reviewer_xpxk"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to assess the strategic reasoning and rationality of large language models (LLMs) in competitive games, specifically focusing on the second price auction and beauty contest game. The paper compares the performance of various versions of GPT-4 and GPT-3.5."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. The paper is well-written and organized, making it easy to follow.\n2. Investigating the capability of strategic reasoning in LLMs is important.\n3. The choice of using the second price auction and beauty contest game as an evaluation arena for LLMs is novel."
                },
                "weaknesses": {
                    "value": "1. The significance of this paper appears to fall short of the standard of ICLR. As acknowledged by the authors, there are already a few related works that explore the performance of LLMs in economic games with different settings, including various game classes and information provided to LLMs. It is already known that LLMs can exhibit some degree of rationality and strategic reasoning. Although this paper evaluates LLMs under specific new settings, it does not significantly advance our understanding of LLMs in economic games.\n\n2. As a paper that selects \"dataset and benchmark\" as the primary area, the experiments are not thorough enough to support the main claims and reproducibility is questionable. Most importantly, the prompts which are central for reproducibility are not presented.  Moreover, only multiple versions of GPT are considered, neglecting other commonly used LLMs (e.g., Claude 2 or LLama 2) which have been shown to have quite different performances in games compared to GPT, and only the results of average values are reported (e.g., Figure 2 and Figure 4).  Additionally, the paper claims that they have varied the prompts to show the robustness of their results, but these experiments are not presented."
                },
                "questions": {
                    "value": "What is the reason for choosing the second-price auction and beauty contest game rather than other zero-sum games, such as matching pennies and rock paper scissors?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2817/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698467189594,
            "cdate": 1698467189594,
            "tmdate": 1699636224880,
            "mdate": 1699636224880,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "boh32O1ejj",
                "forum": "NMPLBbjYFq",
                "replyto": "UUm3zEpRXv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2817/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2817/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for identifying the novelty of this work. We reply to the reviewer's concerns one-by-one below.\n\n1. Significance concern\n    \n > The significance of this paper appears to fall short of the standard of ICLR. As acknowledged by the authors, there are already a few related works that explore the performance of LLMs in economic games with different settings, including various game classes and information provided to LLMs. It is already known that LLMs can exhibit some degree of rationality and strategic reasoning. Although this paper evaluates LLMs under specific new settings, it does not significantly advance our understanding of LLMs in economic games.\n    \nWe have updated the methodology and experiment sections, and they have fixed the reviewer\u2019s concerns about the significance of this work.\n    \n2. Dataset and benchmark\n    \n> As a paper that selects \"dataset and benchmark\" as the primary area, the experiments are not thorough enough to support the main claims and reproducibility is questionable. Most importantly, the prompts which are central for reproducibility are not presented. Moreover, only multiple versions of GPT are considered, neglecting other commonly used LLMs (e.g., Claude 2 or LLama 2) which have been shown to have quite different performances in games compared to GPT, and only the results of average values are reported (e.g., Figure 2 and Figure 4). Additionally, the paper claims that they have varied the prompts to show the robustness of their results, but these experiments are not presented.\n    \nWe believe the updated version fixed all these concerns from the reviewer."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2817/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700602734763,
                "cdate": 1700602734763,
                "tmdate": 1700602734763,
                "mdate": 1700602734763,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "APQU7InhH6",
            "forum": "NMPLBbjYFq",
            "replyto": "NMPLBbjYFq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2817/Reviewer_8dxE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2817/Reviewer_8dxE"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed to explore competitive games as an evaluation of the rationality and strategic reasoning ability of LLMs. By varying the game history revealed to LLMs-based players, they found that most LLMs are rational in the sense of playing strategies that can increase their payoffs, but not the most rational strategies, i.e., Nash Equilibria (NEs). Moreover, when game history is available, certain types of LLMs can converge faster to the NE strategies. Other abilities of LLMs were tested."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "This paper proposed to explore competitive games as an evaluation of the rationality and strategic reasoning ability of LLMs. They can be used to test the abilities of LLMs, i.e., rationality, strategic reasoning ability, and instruction-following capability."
                },
                "weaknesses": {
                    "value": "Competitive games exist in the literature. This paper just shows how to design experiments to test the ability of LLMs. That is, this paper did not provide any dataset or benchmark, and then it should not be added to the primary area of datasets and benchmarks as well. \n\nThis paper is not a technical paper on learning representation. Thus, it is not related to ICLR."
                },
                "questions": {
                    "value": "."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2817/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698660112572,
            "cdate": 1698660112572,
            "tmdate": 1699636224650,
            "mdate": 1699636224650,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ntj66CS6Lv",
                "forum": "NMPLBbjYFq",
                "replyto": "APQU7InhH6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2817/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2817/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Competitive games exist in the literature. This paper just shows how to design experiments to test the ability of LLMs. That is, this paper did not provide any dataset or benchmark, and then it should not be added to the primary area of datasets and benchmarks as well.\n> \n> This paper is not a technical paper on learning representation. Thus, it is not related to ICLR.\n\nWe want to kindly remind the reviewer that \u201capplications in economics\u201d is part of the subject areas of ICLR, and evaluation methods are definitely of the interest of ICLR audience. \n\nRegarding the existence of competitive games, we know there is concurrent work on auction arena, but this is no pre-existing literature on evaluating LLMs with competitive economics games. Can the reviewer provide a reference to back up the claim \u201ccompetitive games exist in the literature\u201d?\n\nMoreover, as we stated in our updated paper, we will release the EconArena as both a Python package publicly available on PyPI and a website where users can config and run games with GUI operations. This kind of dynamic \u201cbenchmark\u201d is definitely a kind of dataset/benchmark."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2817/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700602546417,
                "cdate": 1700602546417,
                "tmdate": 1700602546417,
                "mdate": 1700602546417,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "glv7J3ivSt",
            "forum": "NMPLBbjYFq",
            "replyto": "NMPLBbjYFq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2817/Reviewer_HCDQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2817/Reviewer_HCDQ"
            ],
            "content": {
                "summary": {
                    "value": "The paper investigates the strategic reasoning abilities of Large Language Models (LLMs) by employing them as rational agents in competitive economic games. The study focuses on the second-price auction and beauty-contest games. It also includes a variation, the self-computing beauty contest game, to analyze the LLMs' ability to adapt strategies based on other agents' behaviors when those agents are instances of the same model. GPT-4, among other models, is highlighted for its capacity to quickly converge to the Nash Equilibrium, demonstrating a strong capability for strategic adjustment and reasoning in this specific case."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* Arguing about metrics that associate with the Nash Gap is a systematic approach. This is indeed a straightforward metric to evaluate rationality.\n* The use of self-computing beauty contest games to assess LLMs is an interesting one, compared to having different models in the same auction.\n* The methodology includes running multiple runs and account for the LLMs not responding correctly, an inherent pitfall of their architecture."
                },
                "weaknesses": {
                    "value": "* The paper acknowledges the importance of prompt sensitivity but fails to provide a detailed account of prompt structures, limiting the reproducibility of the experiments. It does not investigate different methods of prompting or incorporating historical data into prompts.\n* There is no discussion on whether the stability of outputs in the homogeneous model setting correlates with a consistent strategy distribution between all agents, nor is there an exploration of how the model's temperature setting influences strategy uniformity.\n* The methodology is unclear on whether different rounds were obtained by a chat-based model instantiation, where there are affecting subsequent decisions, or if the models were independently assessed in varied historical contexts, simulating different rounds.\n* The experiments lack scenarios where LLMs interact with actual strategic agents, which would test the models in more realistic strategic environments.\n\nMiscellaneous:\n* The font size in Figure 1 is difficult to read."
                },
                "questions": {
                    "value": "* How does the variation in prompt structure affect the strategic decision-making of LLMs, and what would be a good prompt structure to accurately assess their performance?\n* Is the sample size of rounds (e.g., 10 rounds) statistically significant enough to establish the rate at which GPT-4 converges to the optimal strategy? Could one argue that gpt-4 is learning with some arbitrary learning rate over time?\n* What role did the temperature parameter play in the stability of output in games where all models were the same? The paper argues about gpt-4 facing the inconsistency other models. For a given temperature parameter there would also be self-inconsistency. Also, as LLMs are rolled out into ever more realistic scenarios, the assumption of competing with the same agent is not realistic.\n* How were the answers considered? Was it the next token in the generation process, or a number found after allowing the LLM to complete the sentence until reaching the <eos> token?\n* What are the effective ways to prompt LLMs to account for historical information, and how can this be standardized to assess performance sensitivity to prompt phrasing? Since there is sensitivity in the prompt and as the history becomes larger, how is the information of the history compressed into the prompt?\n* Can the strategic behaviors observed in LLMs within the confines of the study be generalized to other forms of competitive games or economic models?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2817/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699053771260,
            "cdate": 1699053771260,
            "tmdate": 1699636224576,
            "mdate": 1699636224576,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "u5FyQZSAMg",
                "forum": "NMPLBbjYFq",
                "replyto": "glv7J3ivSt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2817/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2817/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer HCDQ (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for pointing out the interesting question this work tries to answer. We reply the reviewer\u2019s concerns one-by-one below.\n\n1. Details about prompts\n    \n> The paper acknowledges the importance of prompt sensitivity but fails to provide a detailed account of prompt structures, limiting the reproducibility of the experiments. It does not investigate different methods of prompting or incorporating historical data into prompts.\n    \nThanks for brining this up, we provided all the prompts we used in the updated Appendix B.\n    \n2. Stability and uniformity of LLMs\u2019 strategies\n    \n> There is no discussion on whether the stability of outputs in the homogeneous model setting correlates with a consistent strategy distribution between all agents, nor is there an exploration of how the model's temperature setting influences strategy uniformity.\nWhat role did the temperature parameter play in the stability of output in games where all models were the same? The paper argues about gpt-4 facing the inconsistency other models. For a given temperature parameter there would also be self-inconsistency. Also, as LLMs are rolled out into ever more realistic scenarios, the assumption of competing with the same agent is not realistic.\n\nThanks for pointing this out. In our updated version, we use the self-compete set up for sanity check to verify consistency in strategies.\n    \n3. Instantiation of the LLM backbones\n    \n> The methodology is unclear on whether different rounds were obtained by a chat-based model instantiation, where there are affecting subsequent decisions, or if the models were independently assessed in varied historical contexts, simulating different rounds.\n\nThanks for bringing this up. We\u2019ve fixed this problem in the updated Section 4. In short, yes, for each independent run and thus session of the same game, we re-instantiate a new interface with the LLM.\n    \n4. Strategic agents\n    \n> The experiments lack scenarios where LLMs interact with actual strategic agents, which would test the models in more realistic strategic environments.\n    \nThanks for the suggestions. In our updated version, we established several agents with hard-coded rational or irrational strategies, and showed the results of LLMs competing with these different baselines. Other pre-defined strategies were not included at the moment. Since the paper mainly focused on exploring competition between different LLM-based agents, actual strategic agents were not considered currently, but could serve as interesting extension.\n    \n5. Effect of prompt engineering on the performance of LLMs\n    \n> How does the variation in prompt structure affect the strategic decision-making of LLMs, and what would be a good prompt structure to accurately assess their performance?\n    \nThanks for this good question! We agree that the prompt structure may affect the strategic decision-making of LLM, thus we decided to introduce the least amount of prompt engineering into this work. In the meantime, we have also set up the APIs for future users of EconArena to customise their own prompts for all games. A detailed discussion about the optimal prompt structure is not the focus of this work, as our aim is to provide the community a public economics environment for evaluating LLMs.\n    \n6. Effect of the Number of rounds on the convergence rate\n\n> Is the sample size of rounds (e.g., 10 rounds) statistically significant enough to establish the rate at which GPT-4 converges to the optimal strategy? Could one argue that gpt-4 is learning with some arbitrary learning rate over time?\n\nThanks for pointing out this ambiguity. We further clarify the convergence experiment setup in the updated Section 4.\n    \n7. Answer post-processing\n    \n> How were the answers considered? Was it the next token in the generation process, or a number found after allowing the LLM to complete the sentence until reaching the <eos> token?\n    \nThanks for the question. We didn't post-process the answers returned from LLMs, as we specified the format that LLMs should follow in the prompts.\n    \n8. Prompt engineering for game history\n    \n> What are the effective ways to prompt LLMs to account for historical information, and how can this be standardized to assess performance sensitivity to prompt phrasing? Since there is sensitivity in the prompt and as the history becomes larger, how is the information of the history compressed into the prompt?\n    \nThanks for bringing up this point. As we stated in our updated paper, the prioritised objective of this work is to provide the community a dynamic economics environments for evaluating LLMs. In the meantime, we tried to introduce the least amount of prompt engineering. Explore the optimal prompt structure for game history is beyond the scope of this work.\n    \n(Continued below)"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2817/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700602458433,
                "cdate": 1700602458433,
                "tmdate": 1700602507230,
                "mdate": 1700602507230,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bFjqRGvQht",
                "forum": "NMPLBbjYFq",
                "replyto": "glv7J3ivSt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2817/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2817/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Replyt to Reviewer HCDQ"
                    },
                    "comment": {
                        "value": "(Continued)\n\n9. Generalisation of strategic behaviours\n    \n> Can the strategic behaviors observed in LLMs within the confines of the study be generalized to other forms of competitive games or economic models?\n\nThanks for this great question. The convergence rate we proposed in this work can be easily generalised to any economics games with unique pure NEs."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2817/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700602479290,
                "cdate": 1700602479290,
                "tmdate": 1700602516523,
                "mdate": 1700602516523,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8GcX7szGS6",
            "forum": "NMPLBbjYFq",
            "replyto": "NMPLBbjYFq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2817/Reviewer_JmK9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2817/Reviewer_JmK9"
            ],
            "content": {
                "summary": {
                    "value": "The authors consider the problem of quantifying the degree of rationality that Large Language Models can exhibit in economic settings. To this end, the authors present a set of economic games in which the authors measure the ratio of the payoff achieved by the LLMs with Nash equilibrium payoffs."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "This is a really great direction, and I am very excited about the direction of this paper which seeks to bring game-theoretic tools to the study of LLMs. I hope the authors pursue these directions further!"
                },
                "weaknesses": {
                    "value": "The main assumption of the paper, namely that rational agents play a Nash equilibrium, is in my opinion incorrect. More generally, rational agents play a rationalizable action, i.e., the solution concept of interest is rationalizability [1]. In fact, rational agents might not be compelled to play a Nash equilibrium as summarized by following quote from Luce and Raiffa [2, page 63] regarding Nash equilibrium and rationality here is relevant:\n\n> Even if we were tempted at first to call a (Nash) non-conformist 'irrational', we would have to admit that (his opponent) might be 'irrational' in which case it would be 'rational' for (him) to be 'irrational'-to be a (Nash) non-conformist. \n\nThe set of experimental settings are relatively standard, and do not add significantly to the existing literature."
                },
                "questions": {
                    "value": "Comments and questions: The measure of rationality provided by the authors seems to become ill-defined if Nash equilibria are not unique, since the equilibrium payoffs are not unique, what do the authors do in such settings?\n\nThe definition of the rationality metric involves time, should I be thinking of this as time-step of a repeated-game? \n\n\n[1] Bernheim, B. Douglas. \"Rationalizable strategic behavior.\" Econometrica: Journal of the Econometric Society (1984): 1007-1028.\n\n[2] Luce, R. Duncan, and Howard Raiffa. Games and decisions: Introduction and critical survey. Courier Corporation, 1989."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2817/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2817/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2817/Reviewer_JmK9"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2817/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699170202061,
            "cdate": 1699170202061,
            "tmdate": 1699636224508,
            "mdate": 1699636224508,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cYi8hq3cMf",
                "forum": "NMPLBbjYFq",
                "replyto": "8GcX7szGS6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2817/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2817/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for pointing out the interesting question this work tries to answer. We reply the reviewer\u2019s concerns one-by-one below.\n\n1. NE Rationalisability in the games\n    \n > The main assumption of the paper, namely that rational agents play a Nash equilibrium, is in my opinion incorrect. More generally, rational agents play a rationalizable action, i.e., the solution concept of interest is rationalizability [1].\n    \nWe thank the reviewer for the reference. In the updated Section 4.1, we mentioned that the agents\u2019 behaviour could be a result of themselves being not maximally rational, or that they believe their opponents are not rational, thus resulting in them playing a rationalizable strategy. We attempt to disentangle the two effect by introducing a rational environment, where agents face hard-coded rational players and were informed in prompt that their opponents are rational.\n    \n2. Non-unique NEs in the games\n    \n> The measure of rationality provided by the authors seems to become ill-defined if Nash equilibria are not unique, since the equilibrium payoffs are not unique, what do the authors do in such settings?\n \n    \nWe thank the reviewer for proposing this concern. As we stated in the updated Section 3.1, all the current games in our EconArena have only a unique pure NE. Such implementation guarantees the objectives of LLMs conflict with each other, and it is more straightforward to compare the strategies of LLMs with the NE strategy.\n    \n3. Repeated games\n    \n > The definition of the rationality metric involves time, should I be thinking of this as time-step of a repeated-game?\n\nWe thank the reviewer for proposing this concern. As we stated in the updated Section 3.1, all the current games in our EconArena are of single-round to ensure that LLMs can be used in an \u201coff-the-shelf\u201d fashion, and no additional prompt engineering of history management is necessary. In the meantime, we keep this practice to make sure we introduce the least amount of influence from prompt engineering onto the performance of LLMs in EconArena."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2817/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700601954266,
                "cdate": 1700601954266,
                "tmdate": 1700601954266,
                "mdate": 1700601954266,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]