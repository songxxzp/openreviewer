[
    {
        "title": "Non-Redundant Graph Neural Networks with Improved Expressiveness"
    },
    {
        "review": {
            "id": "mVWcWNTbHZ",
            "forum": "AlkANue4lm",
            "replyto": "AlkANue4lm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5380/Reviewer_h7VD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5380/Reviewer_h7VD"
            ],
            "content": {
                "summary": {
                    "value": "This paper points that the redundancy, i.e., repeated exchange and encoding of identical information, in the message passing framework amplifies the over-squashing. To resolve the redundancy, the authors propose an aggregation scheme based on `neighborhood trees', which control redundancy by pruning branches. Authors have theoretically proved that reducing redundancy improves the expressivity, and experimentally showed it can alleviate over-squashing."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper has pointed out the inherent problem of message passing problem, the \u201crepeated exchange and encoding of identical information\u201d amplifying over-squashing."
                },
                "weaknesses": {
                    "value": "1. The necessity of k-redundant Neighborhood Tree (k-NT) seems week. In Table 2, experiments on EXP-class, the performance seems to be always higher when k is smaller. Removing all redundant nodes seems to be the best choice, why use k as an selection?\n2. Experiments seems to be not sufficient enough to support the authors claim. For example in the abstract, authors claimed that the paper experimentally shows the method alleviates over-squashing. They have shown the results for synthetic datas in Table 2, but they are no experiments for real-world datasets to show this (such as experiments on long-range graph benchmark). \n3. In the introduction section, the authors mentioned PathNNs and RFGNN as closely related works. Also in table 3, the authors highlighted the best results from polynomial time complexity in bold. However, it seems that they are no comparison with any methods having polynomial time complexity other than linear."
                },
                "questions": {
                    "value": "1. For experiment results in Table 1, 3, authors highlighted the best results with polynomial time complexity methods, emphasizing that DAG-MLP has advantages in time. What is the time complexity of DAG-MLP in terms of big-O notation? Also, is there any inference time comparison for the inference time of each method (GIN, SPN, PathNN, DAG-MLP)? \n2. Following weakness #4, is there are more baselines to compare with the paper method having a polynomial time complexity? What about the results of RFGNN mentioned for related works?\n3. In Table 3, the performance IMDB-B and IMDB-M datasets are said to not applicable. However, in the Michel et al.$^{[1]}$, they do report the performance of PathNN-SP+(K=2) for datasets IMDB-D and IMDB-M. What do the authors mean by not applicable? Also, what path length K did the authors use for PathNN networks in Table 3?\n\n[1] Michel et al., Path neural networks: Expressive and accurate graph neural networks, ICML 2023"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5380/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5380/Reviewer_h7VD",
                        "ICLR.cc/2024/Conference/Submission5380/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5380/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698310992600,
            "cdate": 1698310992600,
            "tmdate": 1700541008571,
            "mdate": 1700541008571,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wu4ENKUFXe",
                "forum": "AlkANue4lm",
                "replyto": "mVWcWNTbHZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5380/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5380/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for your review. \n\n**W1** Since Theorem 1 applies only to $1$-NTs, it is currently conjectured that on graph level $0$-NTs are as expressive as $1$-NTs. However, since it has not been proven yet, the claim of being strictly more expressive than $1$-WL holds only for $1$-NTs. We show that it does not make sense to choose a very high $k$, because this introduces redundancy again, up until the point where the $k$-NTs are equal to the unfolding trees.\n\n**W2** Please refer to the global reply regarding further experiments and the LRGB.\n\n**W3** Please refer to the answer to **Q2** below.\n\n**Q1** Please refer to the global reply regarding the time complexity of DAG-MLP.\n\n**Q2** RFGNN does not have a polynomial but exponential time complexity. To the best of our knowledge, no methods currently exist that address the problem of reducing redundancy in graph neural networks and running in polynomial time.\n\n**Q3** As stated in our paper, for the comparison methods we used the results reported in [1]. We only reported the best results from this method and hence chose PathNN-P(K = 1) and PathNN-SP+(K = 3). We will add the parameter choice to the final version to clarify this. While results for PathNN-SP+(K=2) are reported, the performance is worse than that of PathNN-P(K = 1).\n\n**Presentation score** \nAs you rated the presentation of our article with 1 but did not mention any points of criticism regarding the presentation in the review, we would like to ask for specific comments to help us improve the presentation of the manuscript.\n\n[1] Michel et al., Path neural networks: Expressive and accurate graph neural networks, ICML 2023"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5380/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700173807220,
                "cdate": 1700173807220,
                "tmdate": 1700173807220,
                "mdate": 1700173807220,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LLHiMC2WqW",
                "forum": "AlkANue4lm",
                "replyto": "wu4ENKUFXe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5380/Reviewer_h7VD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5380/Reviewer_h7VD"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their comprehensive response. I don't have any additional queries, and adjusted the presentation score.\n\nHowever, despite recognizing the constraints of limited time, I find it challenging to fully agree with the claim that DAG-MLP performs well in long-range tasks. The reported performance of GCN in LRGB$^{[1]}$ is approximately 0.593. I believe the authors' message would have been more clear, if they had compared one instance of DAG-MLP directly for the entire dataset, rather than testing many variants for sampled data.\n\n[1] Dwivedi et al., Long-range graph benchmark, 2022"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5380/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700540987478,
                "cdate": 1700540987478,
                "tmdate": 1700540987478,
                "mdate": 1700540987478,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "RXt8P6pKlq",
            "forum": "AlkANue4lm",
            "replyto": "AlkANue4lm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5380/Reviewer_e2WG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5380/Reviewer_e2WG"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new graph neural network architecture that alleviates the redundancy in the message-passing structure. The authors (1) prove that the expressive power of the new GNN architecture improves over the 1-WL test and (2) the new GNN architecture alleviates the over-squashing issue. The proposed architecture is evaluated on the synthetic datasets and the TUDataset."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper proposes a new GNN architecture to alleviate the redundancy of message passing. The proposed architecture is sound. The figures are helpful for understanding the paper."
                },
                "weaknesses": {
                    "value": "My main concern is on the positioning of the paper with respect to similar work on alleviating GNN redundancy, i.e., RFGNN (Chen et al., 2022), and the weak experimental results. \n\n### Comparison with RFGNN \n\n- This work argues to alleviate over-squashing based on the results from RFGNN (Chen et al., 2022). However, as the authors argue, their proposed GNN architecture is different from RFGNN. Hence the logic is incomplete, i.e., it is not clear whether the proposed architecture alleviates over-squashing based on the same logic as RFGNN. \n-  Upon reading Appendix A, the authors seem to claim that RFGNN introduces more redundancy compared to the proposed work. Since there is no clear explanation of how redundancy is harmful to GNN tasks, it is hard for me to understand the benefit of the proposed DAG-MLP. \n- Furthermore, the authors do not compare the expressive power of DAG-MLP compared to RFGNN. One might argue that RFGNN might be more expressive than the proposed DAG-MLP at the cost of introducing more redundancy. \n- In addition, the authors claim speed-up of RFGNN as another benefit. I wonder if the authors could empirically show this in a meaningful scenario, e.g., large-scale graphs.\n\n### Weak experiments (TUDataset)\n- Overall, I think TUDataset is not good enough for evaluating the performance of DAG-MLP in practical scenarios. Especially, to validate the ability of DAG-MLP to alleviate over-squashing, I strongly suggest the long-range graph benchmark (Dwivedi et al., 2022) to run the proposed DAG-MLP.\n- The proposed work underperforms compared to the PathGNN. While the authors argue that PathGNN takes exponential running time, the actual running time is not reported. Hence it is hard to tell whether the issue is practically relevant. \n- The statistical box plot in Appendix F should be similarly drawn for the baselines to make a fair comparison.\n- The authors use four versions of DAG-MLP (0/1-NT, fixed single height/combined heights) while the relevant baselines have usually one or two versions (PathGNN has three versions, but DAG-MLP is not directly compared due to computational complexity). This makes the comparison unfair especially for TUDataset with high variance scores. \n- The list of baselines is not comprehensive enough to check whether if performance improvement of the proposed DAG-MLP is practically relevant."
                },
                "questions": {
                    "value": "How does the actual running time of DAG-MLP compare with the baselines in the considered experiments? I think this is an important criterion since the main (and possibly the only) benefit of DAG-MLP over RFGNN is the computational complexity."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5380/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698727901812,
            "cdate": 1698727901812,
            "tmdate": 1699636543877,
            "mdate": 1699636543877,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0nYIUkw8AV",
                "forum": "AlkANue4lm",
                "replyto": "RXt8P6pKlq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5380/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5380/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for your review and remarks, which we are happy to discuss.\n\n**Comparison to RFGNN**\n  * We elaborate on the relation to RFGNN regarding the ability to avoid over-squashing in the global reply. Our analysis also shows how redundancy is harmful to GNN tasks and that our approach provides advantages in this respect compared to MPNNs and RFGNN. We believe that these arguments lift your concerns.\n  * RFGNN claims to be fully expressive (solve graph isomorphism) and has exponential runtime. This is a huge drawback. Additionally, it does not address the second type of redundancy (computational redundancy), which we also address in our submission.\n  * Please refer to our global reply regarding time complexity.\n\n**Weak experiments**\n  * Please refer to the global reply regarding experiments on LRGB.\n  * Since we do not possess the same hardware as [1], comparing the running times of our method and PathNN is not feasible. We think that the theoretical time complexity is more meaningful, since it is independent of the hardware used and the optimization of the implementations.\n  * The statistical plot for the baselines can be found in Appendix 1, Figure 1 in [1]. In their plot, we only consider the test set (square symbol)\n  * We report the results for each of our variants separately, so a fair comparison is possible. It is often the case that only the best parameter choice of each model is shown and mixed throughout datasets. We did not do that to ensure a fair comparison.\n  * Please refer to our global reply regarding additional experiments.\n\n[1] Errica, F., Podda, M., Bacciu, D., & Micheli, A Fair Comparison of Graph Neural Networks for Graph Classification, ICLR 2020.\n\n**Questions**\nSee answer to point 2 in \u201cWeak experiments\u201d. We also address this in our global reply. We will state the difference in computational complexity more clearly in the final version."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5380/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700173544554,
                "cdate": 1700173544554,
                "tmdate": 1700173544554,
                "mdate": 1700173544554,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MbM614a8cH",
                "forum": "AlkANue4lm",
                "replyto": "RXt8P6pKlq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5380/Reviewer_e2WG"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5380/Reviewer_e2WG"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the response. I still find some of my concerns are not well resolved. \n\nI still think the actual running time should be reported. Empirical running time is important since it matters in practice. Since the PathNN has a public repository, the authors could simply reproduce the results and compare the runtime. \n\nI also find the newly reported results not very significant. There exist other baselines like DReW that perform better than the proposed method."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5380/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740689096,
                "cdate": 1700740689096,
                "tmdate": 1700740716619,
                "mdate": 1700740716619,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5GEd4tE7id",
            "forum": "AlkANue4lm",
            "replyto": "AlkANue4lm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5380/Reviewer_8Vsr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5380/Reviewer_8Vsr"
            ],
            "content": {
                "summary": {
                    "value": "A compact representation of neighborhood trees is proposed, from which node and graph embeddings via a neural tree canonization technique are computed. The main goal is reduce redundancy in message passing GNNs to address oversquashing. The resulting message passing GNN is provably more expressive than the Weisfeiler-Leman test."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Good literature discussion with details what distinguishes the different approaches.\n- The basic concepts are introduced in detail.\n- The proposed 1-NT isomorphism test is provably more powerful than the Weifeiler-Leman test.\n- Experiments verify that a reduction in redundancy could help address oversquashing."
                },
                "weaknesses": {
                    "value": "- k seems to be a hyper-parameter that would need to be tuned in practice.\n- Even though DAG-MLPs are provably more expressive than the Weisfeller-Lehmann method, they are not proven to be fully expressive (i.e. distinguish any non-isomorphic graphs).\n- The computational complexity of the proposed architecture and algorithms are not analysed but form an integral part of the contribution.\n- PathNN-P seems stronger on the Enzymes and Proteins dataset but also suffers from exponential computational time complexity."
                },
                "questions": {
                    "value": "- How expressive are the proposed DAG-MLPs? It seems like there could exist non-isomorphic graphs that cannot be distinguished by DAG-MLP. What would be an example?\n- How does the expressive power compare with baseline methods?\n- What is the computational complexity of building and evaluation DAG-MLPs? What are their memory requirements?\n-> It would be helpful to add measurements of time complexity in the tables of the experiments.\n- What could be other explanations why k-NTs perform less well for higher $k$ in Table 3? Does the explanation have to be over-squashing?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5380/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698792057616,
            "cdate": 1698792057616,
            "tmdate": 1699636543775,
            "mdate": 1699636543775,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9LAwcmwXv1",
                "forum": "AlkANue4lm",
                "replyto": "5GEd4tE7id",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5380/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5380/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for the review, the remarks and the questions, which we are happy to discuss. Since the weaknesses and questions overlap, we discuss them jointly if possible. Please let us know, if anything is not adequately addressed.\n\n**Weaknesses and Questions**\n  * $k$ as a hyper-parameter: Setting $k$ to $1$ (or even $0$) should in general be the best solution. With higher $k$ values, redundancy is introduced again, up until the point where the $k$-NTs are equal to the unfolding trees. We do not expect NTs with $k$ greater than $1$ to be more expressive than $1$-NTs, therefore this parameter does not have to be optimized (other than maybe choosing between $0$ and $1$). If we could prove that $0$-NTs are as expressive on graph-level as $1$-NTs, then $0$-NTs could be used in any case for graph-level tasks.\n  * Not fully expressive/Example: Indeed, DAG-MLP is not fully expressive. There are non-isomorphic graphs that it cannot distinguish (for example the $4\\times 4$-Rooks and Shrikhande graph). There is always a trade-off between running time and expressivity. Currently, no polynomial time algorithms for solving graph isomorphism are known.\n  * We analyzed the size of the DAG, as well as the time to build it in our submission in Section 4.2. This makes up the crucial part of the running time for our method.  Please also refer to the global reply regarding the computational complexity.\n  * Please refer to the global reply regarding the expressivity of the baseline methods.\n  * We believe that over-squashing is a suitable explanation for our experimental findings. We have formally related our method to over-squashing in the global reply. Using the notation introduced there, we have $I_{i\\text{-NT}}(v,u) \\leq I_{j\\text{-NT}}(v,u)$ for $i>j$ supporting this hypothesis."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5380/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700173440569,
                "cdate": 1700173440569,
                "tmdate": 1700173440569,
                "mdate": 1700173440569,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ey84cvlgBc",
                "forum": "AlkANue4lm",
                "replyto": "9LAwcmwXv1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5380/Reviewer_8Vsr"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5380/Reviewer_8Vsr"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their response and do not have any further open questions.\nAs my main concerns have been addressed, I will keep my score, but will be open to adapting it during the discussion with other reviewers."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5380/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700691700999,
                "cdate": 1700691700999,
                "tmdate": 1700691700999,
                "mdate": 1700691700999,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Uy2X9Jq1jp",
            "forum": "AlkANue4lm",
            "replyto": "AlkANue4lm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5380/Reviewer_zntd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5380/Reviewer_zntd"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel aggregation scheme based on neighborhood trees to control redundancy in message-passing graph neural networks (MPNNs). The authors show that reducing redundancy improves expressivity and experimentally show that it alleviates over squashing."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1) The paper introduces a novel aggregation scheme based on neighborhood trees, which allows for controlling redundancy in message passing MPNNs.\n2) The authors provide a theoretical analysis of expressivity that shows the proposed method is more expressive than the Weisfeiler-Leman method."
                },
                "weaknesses": {
                    "value": "1) The main weakness is the computational cost, which requires O(nm) space where n is the number of nodes and m is the number of edges. This brings a significant limitation to the applicability of the proposed method, even for moderate-sized graphs.\n\n2) The experimental result only shows occasional marginal improvements over some baselines and only on a few datasets. This is not enough to demonstrate the effectiveness of the proposed method.\n\n3) One main motivation for the proposed method is to address over squashing, but there is no theoretical analysis of the proposed method to address it."
                },
                "questions": {
                    "value": "1) What is the largetst graph size that the proposed method can handle?\n2) What is the preprocessing time for the proposed method?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5380/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5380/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5380/Reviewer_zntd"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5380/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698823300831,
            "cdate": 1698823300831,
            "tmdate": 1699636543673,
            "mdate": 1699636543673,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IpQtuNCV6E",
                "forum": "AlkANue4lm",
                "replyto": "Uy2X9Jq1jp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5380/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5380/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for the review. We address some of your remarks in our global reply to all reviewers and look forward to discussing further comments.\n\n**W1** We agree that the increase in running time is a drawback of our approach. However, our method has significantly lower computational complexity than comparable approaches. We elaborate on this further in the global reply.\n\n**W2** We compare our approach to the most recent state-of-the-art methods in Table 3. Since these methods have been shown to outperform the baselines, we believe that this is appropriate. In [1] there are results for other methods, but Path-NN outperforms them in most cases. We provide additional experimental results supporting the advantages of our method in the global reply.\n\n**W3** Thank you for asking how our approach formally relates to over-squashing. We referred to the analysis in [2] without making the relation explicit in our manuscript. We have elaborated this in the global reply showing that our approach can formally be related to over-squashing and improves in this respect over existing methods. We believe that including these arguments will strengthen our manuscript.\n\n**Q1** This depends on the properties of the graphs and on the hardware used. Giving a general answer to this question is generally difficult (also for standard GNNs).\n\n**Q2** In practice, this again depends on the hardware used. On a 64 CPU our non-optimized implementation performs the preprocessing for the 600 protein graphs of the dataset ENZYMES (33 nodes and 62 on an average) in 7.8 seconds for 1-NTs. We believe that the theoretical time complexity is more meaningful (although it does not take our compression techniques into account) and refer to the global reply.\n\n[1] Michel et al., Path neural networks: Expressive and accurate graph neural networks, ICML 2023\n\n[2] Rongqin Chen, Shenghui Zhang, Leong Hou U, Ye Li: Redundancy-Free Message Passing for Graph Neural Networks. NeurIPS 2022"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5380/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700173204687,
                "cdate": 1700173204687,
                "tmdate": 1700173204687,
                "mdate": 1700173204687,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]