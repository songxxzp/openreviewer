[
    {
        "title": "Malcom-PSGD: Inexact Proximal Stochastic Gradient Descent for Communication Efficient Decentralized Machine Learning"
    },
    {
        "review": {
            "id": "gZI8f7aSLl",
            "forum": "t8D9JxEn0J",
            "replyto": "t8D9JxEn0J",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8432/Reviewer_ckRP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8432/Reviewer_ckRP"
            ],
            "content": {
                "summary": {
                    "value": "The paper focused on the communication bottleneck in decentralized ML problems. The authors provided a method named MALCOM-PSGD to reduce communication cost, which integrates the model sparsification and gradient compression. An $O(1/\\sqrt{t})$ rate was established to guarantee the convergence of the proposed algorithm. Also the authors used numerical experiments to validate the performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper addresses the challenge of overcoming communication bottlenecks in the context of decentralized machine learning, with a particular focus on training large-scale, over-parameterized neural network models. The authors provided theoretical analysis and numerical experiments to validate the performance of the proposed algorithm."
                },
                "weaknesses": {
                    "value": "The paper studied the traditional communication bottleneck issue in decentralized ML problems. The proposed algorithm appears to amalgamate two pre-existing techniques, exhibiting rather modest advancements. Moreover, the theoretical analysis appears to follow a standard course, and there is room for enhancing the experimental aspects."
                },
                "questions": {
                    "value": "1. Can authors provide more explanation for the reason of combining model sparisificaion and gradient compression? Though both of them can improve communication efficiency, can the combination achieve `1+1>2` improvement? Also, is there any additional challenge from either implementation or theoretical analysis caused by the combination? \n\n2. If we look at the convergence rate in Theorem 2, it is w.r.t. to $\\mathcal{F}$ instead of $F$. To give a fair comparison, can the result be extended to  $F$?\n\n3. After adding the model sparisificaion,  how will the model generalization be changed (gain or loss) in either theoretical way or numerical experiments? This part remains very unclear.\n\n4. There are many other communication efficient methods, such as local updates/signSGD, why don't compare with them at least in the numerical experiments? I feel the numerical experiments can include many other communication efficient decentralized optimization methods.\n\n5. From the experiments, the improvement w.r.t. test accuracy seems not very signification. Any analysis on it?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8432/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8432/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8432/Reviewer_ckRP"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8432/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697912749325,
            "cdate": 1697912749325,
            "tmdate": 1700591042298,
            "mdate": 1700591042298,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Ig1RUHNrrn",
                "forum": "t8D9JxEn0J",
                "replyto": "gZI8f7aSLl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8432/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8432/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response Pt 1"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the comments. We have organized our response as follows: We have  responded to your comments in the order they were given starting with the weaknesses comment. \n\n1.  \n    **Reply**: In this revision, we have provided a more detailed\n    exposition on the combined effects of model sparsification and\n    gradient compression. Additionally, we have delved deeper into the\n    challenges in our analysis and algorithm design. Furthermore, the\n    numerical results have been enriched with comparisons of our method\n    against an additional baseline algorithm. We have also incorporated\n    extra simulation results to illustrate the enhancement of the\n    communication bit rate from model sparsification. Please refer to\n    the replies below for more details.\n\n2.  \n    **Reply**: In our design, model sparsification is anticipated to\n    yield sparse model residuals as training progresses. This fact,\n    combined with the convergence of our algorithm that naturally\n    reduces the entropy of the quantization output, will reduce the\n    number of bits required for representing the input values, as long\n    as the compression scheme takes advantage of this. To specifically\n    highlight the improvement, we have added the following changes to\n    the manuscript:\n\n    -   We specialize the entropy rate with a concrete example, where\n        the model residuals have an i.i.d. Laplace distribution. Under\n        this assumption and a constant quantization precision, we show\n        that the communication bits decrease to a constant with the\n        training iteration $t$ on the order of $\\mathcal{O}(-d\\log t)$;\n        see Section 5 and Appendix A.3.\n\n    -   In Appendix A.3, we have added a table to compare the\n        communication bit rate of our method with QSGD \\[Ref 5\\] and\n        signSGD \\[Ref 3\\]. We show analytically that our method archives\n        compression that scales with the entropy of the quantization\n        output and the order of the model.\n\n    -   Section 6 includes a new plot, Fig. 3, which examines training\n        performance and total communication bits across 300 training\n        iterations for a fixed compression scheme and varying levels of\n        the $\\ell_1$-regularization penalty $\\mu$ defined in Eq. (2).\n        Here, a larger $\\mu$ leads to sparser local models, while\n        $\\mu=0$ indicates no sparsification. The orange curve in Fig.\n        3(c) shows that the optimal communication cost is achieved with\n        a non-zero $\\mu=10^{-6}$. This setting also maintains comparable\n        training accuracy to the unsparsified model. The observation\n        underscores the importance of model sparsification in\n        decentralized learning: *when combined with gradient\n        compression, it further boosts communication efficiency without\n        compromising model accuracy.*\n\n    Although combining model sparsification and gradient compression\n    yields favorable improvements, it significantly complicates the\n    theoretical analysis on the convergence of the proposed algorithm.\n    The $\\ell_1$-regularization for sparsification results in a\n    non-smooth objective, and the quantization errors that accumulate\n    over training iterations need precise quantification in the\n    convergence proof. Specifically, the non-smoothness precludes the\n    directly application of existing analyses, such as that in \\[Ref\n    4\\], which rely on leveraging the smooth properties to expand the\n    consensus objective value in each iteration. Moreover, the\n    intertwined quantization error, compounded by the non-convex nature\n    of the loss function, hinders straightforward computation of the\n    consensus gap in terms of the overall objective, which is commonly\n    achieved by using convexity properties in the existing work such as\n    \\[Ref 6\\]. Finally, the random quantization error in each training\n    iteration results in a generally non-monotonic objective value. This\n    contrasts with the scenario in error-free decentralized proximal\n    SGD, where the boundedness of the solution can be established, as\n    demonstrated in \\[Ref 6, Lemma 17\\]. To effectively address this\n    challenge, we have to incorporate an additional assumption as in\n    Assumption 4.\n\n    Our work provides a novel analysis for the convergence of\n    decentralized proximal SGD in the context of non-convex non-smooth\n    optimization. We believe that our findings advance the theoretical\n    understanding in this area, not to mention that empirically it shows\n    a $75\\%$ gain in communication cost over the state-of-the-art."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8432/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700562850707,
                "cdate": 1700562850707,
                "tmdate": 1700562850707,
                "mdate": 1700562850707,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "R3NLWhP6cE",
                "forum": "t8D9JxEn0J",
                "replyto": "ab0lSJHWPH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8432/Reviewer_ckRP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8432/Reviewer_ckRP"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for authors' response!"
                    },
                    "comment": {
                        "value": "I would like to thank authors for their detailed response.\n\nI appreciate the authors includes more experimental comparison in the revised paper. In Figure 1-3, the authors compared the methods w.r.t. iteration. But I think the main advantage of this work is communication efficiency. So it would be better to compare the methods w.r.t. communicated bits. \n\nFor the theoretical part, I would like to hold my opinion that it would be better to provide a convergence on the unregularized loss so that it is comparable with other algorithms, which will definitely make the work more completed. \n\nI appreciate the authors effort and raise my score from 3 to 5."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8432/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700591025037,
                "cdate": 1700591025037,
                "tmdate": 1700591025037,
                "mdate": 1700591025037,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eDiOBJS1DA",
                "forum": "t8D9JxEn0J",
                "replyto": "gZI8f7aSLl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8432/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8432/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow up Response"
                    },
                    "comment": {
                        "value": "Thank you for reading our response and responding so quickly and for re-evaluating your score.  We feel your comments have improved the manuscript.  We have a short follow-up to your reply. \n\n1. We have added this Figure for the Ring and Fully connected topology to the Appendix(Appendix B.1 Figure 10). Additionally, there are 4 plots in Appendix B.7 (Figure 9), which show the effects of fixing the number of bits per iteration. Under this setting, we demonstrate that our algorithm still achieves good accuracy while Choco-SGD diverges. We should also acknowledge that in Figures 1-2 we have already taken Choco-SGD to its limits. That additional bit reductions cause substantial performance degradation. \n\n2. We recognize the reviewer\u2019s interest in understanding how our approach theoretically behaves with respect to an unregularized loss, as considered in the baseline algorithms. However, it is important to note that our methodology diverges fundamentally in this aspect. We are minimizing a different problem than the state of the art, and thus our convergence analysis should be with respect to our objective function. Furthermore, it is a standard practice in the literature to provide convergence in regard to the regularized objective value. For example [Ref 6],[Ref 7], and [Ref 8] all prove convergence with regard to their composite objective.   \n\nNew References: \n\n[Ref 7]: Xiao, Lin and Zhang, Tong. A Proximal Stochastic Gradient Method with Progressive Variance Reduction. SIAM Journal on Optimization (2014)\n\n[Ref 8]: Zhize Li and Jian Li. A simple proximal stochastic gradient method for nonsmooth nonconvex optimization. In Proceedings of the 32nd International Conference on Neural Information Processing Systems (NIPS'18) (2018)"
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8432/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700604868322,
                "cdate": 1700604868322,
                "tmdate": 1700605103497,
                "mdate": 1700605103497,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "pJkyyGMPD7",
            "forum": "t8D9JxEn0J",
            "replyto": "t8D9JxEn0J",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8432/Reviewer_4xpx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8432/Reviewer_4xpx"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes MALCOM-PSGD, a communication efficient decentralized optimization algorithm for smooth and non-convex objectives that combines several existing techniques for communication cost reduction.  The first one is to promote the sparsity of node model parameters using $\\ell_1$ regularization and MALCOM-PSGD optimizes the resulting non-smooth objective using proximal SGD.  Furthermore, MALCOM-PSGD applies residual quantization and source coding techniques to reduce the communication cost between decentralized nodes at each communication round. This work gives detailed analysis of the communication cost and convergence rate of MALCOM-PSGD in the synchronous setting and, specifically, shows with properly chosen learning rate, MALCOM-PSGD is able to achieve a convergence rate of $O(\\ln t / \\sqrt{t})$ with $t$ iterations. Finally, experiments on optimizing DNNs in a federated learning setting demonstrate the fast convergence rate and low communication cost of MALCOM-PSGD compared to the SOTA decentralized baseline, in both synchronous and asynchronous settings."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This work considers the important and interesting problem of communication efficient decentralized optimization. While communication efficient distributed and centralized optimization is well understood, this work explores the relatively less well understood decentralized settings.\n\n- The paper presents MALCOM-PSGD in a well-organized and easy-to-follow way."
                },
                "weaknesses": {
                    "value": "- In Section 1 the 3rd point under \u201cour contributions\u201d, it is stated \u201cOur use of compressed model updates and differential encoding allows us to reasonably assume we are creating a structure within our updates that this encoding scheme is most advantageous under.\u201d This sentence seems a bit confusing and it is unclear why this encoding scheme the most advantageous. \n\n- The experiments section in the draft only considers the federated learning (FL) setting, which is essentially a distributed and centralized optimization, with a single choice of the mixing matrix W. Since the focus of the paper is in decentralized settings, it\u2019d be better to present more results of MALCOM-PSGD under different mixing matrices W.\n\n- Minor issue: in Eq.10, it should be $\\sum_{i=1}^{n} \\mathbb{E}[...] \\leq \\sum_{i=1}^{n} 2\\mathbb{E}[...] + 2 ... $\n\n- Minor issue: in Eq.16, it should be $\\eta_k$ instead of $\\eta_t$."
                },
                "questions": {
                    "value": "- How does the mixing matrix W affect the convergence rate of MALCOM-SGD?\n\n- Is $\\omega$ in Theorem 2 the same as the one defined in Theorem 1?\n\n- In Assumption 4, is $|| \\mathbf{X}^{(t)}||$ the operator norm of $\\mathbf{X}^{(t)}$ ? (unclear notation)\n\n- Just as this work mentions in the introduction, sparsification and quantization are two major techniques for reducing communication cost in distributed optimization algorithms. If instead of quantizing the model residual (aka., line 3 in Algorithm 1) in MALCOM-PSGD, one sparsifies the model residual by applying, e.g., Rand-$k$ sparsification, can the current analysis of MALCOM-PSGD be extended to this new method?\n\n- Is Eq.8 in Section 4 \"bit rate analysis\" used to compute the communication cost in the experiments?\n\n- In Section 4 \"bit rate analysis\", it is stated \"As the training converges, we expect a diminishing residual to be quantized in Step 3 of Alg. 1, resulting in a sparse quantized vector. \" Does this imply as the training proceeds, the communication cost of the nodes at each round decreases? However, from the experiment results, e.g., plot (c) in Figure 1/2, the communication cost of MALCOM-PSGD remains the same across the communication rounds. Any comments on this?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8432/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8432/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8432/Reviewer_4xpx"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8432/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698723598660,
            "cdate": 1698723598660,
            "tmdate": 1699637051051,
            "mdate": 1699637051051,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cxoZ99Zh0t",
                "forum": "t8D9JxEn0J",
                "replyto": "pJkyyGMPD7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8432/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8432/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response Pt 1"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the comments. Each number corresponds to your comment in the order it was originally made. \n\n1.  \n    **Reply**: The superiority of our compression scheme results from\n    the exploitation of model sparsification and the shrinkage of model\n    residual values due to the convergence of our learning algorithm.\n    Specifically, our model sparsification set yields sparse model\n    residuals as training progresses. This fact, combined with the\n    convergence of our algorithm that naturally reduces the entropy of\n    the quantization output, reduces the number of bits required for\n    representing the input values in our compression scheme\n\n    We have updated the description in the Introduction for clarity.\n    Moreover, in this revised version, we have made the following\n    changes to demonstrate the improvement in communication efficiency\n    resulting from model sparsification and the low-entropy update.\n\n    -   We specialize the entropy rate with a concrete example, where\n        the model residuals have an i.i.d. Laplace distribution. Under\n        this assumption and a constant quantization precision, we\n        establish, based on our convergence result, that the\n        communication bits decrease to a constant with the training\n        iteration $t$ on the order of $\\mathcal{O}(-d\\log t)$; see\n        Section 5 and Appendix A.3.\n\n    -   In Appendix A.3, we have added a table to compare the\n        communication bit rate of our method with QSGD \\[Ref 5\\] and\n        signSGD \\[Ref 3\\]. We show analytically that our method archives\n        the compression scales with the entropy of the quantization\n        output and the order of the model.\n\n    -   Section 6 includes a new plot, Fig. 3, which examines training\n        performance and total communication bits across 300 training\n        iterations for a fixed compression scheme and varying levels of\n        the $\\ell_1$-regularization penalty $\\mu$ defined in Eq. (2).\n        Here, a larger $\\mu$ leads to sparser local models, while\n        $\\mu=0$ indicates no sparsification. The orange curve in Fig.\n        3(c) shows that the optimal communication cost is achieved with\n        a non-zero $\\mu=10^{-6}$. This setting also maintains comparable\n        training accuracy to the unsparsified model. The observation\n        underscores the importance of model sparsification in\n        decentralized learning: *when combined with gradient\n        compression, it further boosts communication efficiency without\n        compromising model accuracy.*\n\n2.  \n    **Reply**: We have added additional numerical results with a\n    different choice of $\\bf W$ in Fig. 2 of the manuscript. This\n    experiment considers a different graph topology shown in Fig. 4 from\n    Appendix B. Moreover, in Fig. 6 of Appendix B, we present another\n    simulation with a time-varying mixing matrix ${\\bf W}_t$ over the\n    training iteration $t$, where each ${\\bf W}_t$ represents a\n    sub-sampled partially connected network. In these experiments, we\n    have observed consistent results with the FL case: Our method\n    outperforms the state-of-the-art baselines in terms of communication\n    efficiency.\n3.  **Reply**: We have corrected the minor issues in Eqs. (10) and (16).\n\n4.  \n    **Reply**: The second largest absolute eigenvalue of $\\bf W$, i.e.,\n    $|\\lambda_2|$, determines the value of the key parameter\n    $\\omega=\\frac{(1-|\\lambda_2|)^2}{8\\tau}$ in Theorem 1. In scenarios\n    where the graph is more connected, $|\\lambda_2|$ tends to be small,\n    leading to a larger $\\omega$ and thus a reduced consensus error\n    bound in Theorem 1. Furthermore, a larger $\\omega$ also contributes\n    to a smaller optimality gap in Theorem 2, thereby leading to faster\n    convergence.\n\n    In the revised manuscript, we have explained this point under\n    Theorem 2, as\n\n    \"For a decentralized learning network exhibiting higher\n    connectivity, the eigenvalue $|\\lambda_2|$ of the mixing matrix\n    $\\bf W$ is generally smaller. This results in a larger $\\omega$ as\n    per Theorem 1, consequently leading to improved consensus and\n    convergence rates according to Theorems 1 and 2.\\\"\n5.  \n    **Reply**: Yes. We have clarified this in Theorem 2.\n6.  \n    **Reply**: The norm $||{\\bf X}^{(t)}||$ refers to the Frobenius norm\n    $||{\\bf X}^{(t)}||_F$. We have clarified this in Assumption 4.\n\n7.  \n    **Reply**: Yes. As shown in \\[Ref 1, Lemma A.1\\], Rand-$k$\n    sparsification satisfies Assumption 2 on Page 4, with the parameter\n    $\\tau=d/k$, where $d$ is the size of the model. This implies that\n    our analysis is directly applicable to Rand-$k$. We have mentioned\n    this point under Assumption 2 in the revised manuscript.\n\n    [Ref 1]: Sebastian U Stich. Local SGD converges fast and communicates\n    little. In *International Conference on Learning Representations*,\n    2019."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8432/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700563647343,
                "cdate": 1700563647343,
                "tmdate": 1700563647343,
                "mdate": 1700563647343,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Jm95wwEaum",
            "forum": "t8D9JxEn0J",
            "replyto": "t8D9JxEn0J",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8432/Reviewer_zv9X"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8432/Reviewer_zv9X"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to improve the communication efficiency of decentralized nonconvex optimization. In addition to compression that is heavily used recently, the authors also suggest to add an $\\ell_1$ regularization to encourage model sparsity to help communication efficiency. The authors show that the convergence rate for consensus and objective convergence is matching the state-of-the-art. Moreover, the authors show in experiments that new method provides approximately 75% improvement in terms of communication costs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The motivation of the paper, improving the communication efficiency for decentralized optimization and ML is definitely important. Moreover, the paper combines techniques from different areas including not only stochastic and decentralized optimization but also coding theory such as Elias coding and Golomb coding which is a positive. The paper does a decent job of comparing with the existing works (even though there are some unclear points about this that I touch on later). The experimental validation is helpful to show that the provided algorithm is a good candidate for achieving the main goal of the paper. The paper is also well-written and mostly polished."
                },
                "weaknesses": {
                    "value": "Since the paper brings together many ideas, like stated in the beginning of Sect 3: \"SGD, residual compression, communication and encoding, consensus aggregation and proximal optimization\", it's at times difficult to distinguish in which aspects the work is different from the existing methods since sometimes both assumptions and problems are different than previous works. It is also not clear how significant the contribution is in addition to the existing techniques (which is not necessarily a reason for rejection since combination of existing tools can be interesting if the result is interesting enough, but this should be clarified much more).\n\nFor example, there is a decent comparison to Zeng & Yin (2018b) where the difference is handling the stochastic case. However, the comparison with more recent works, for example in Koloskova et al. 2021 is less clear. Is the main difference handling the $\\ell_1$ regularizer? The authors mention a couple of times, for example right after Theorem 1 that \"there are two key differences with Koloskova et al. 2021 for consensus bound: (1) decreasing step size instead of constant one (2) tighter upper bound in terms of C/\\omega^3\", with no further explanation. It is not clear why a decreasing step size is better or what the precise \"tighter upper bound\" is compared to previous work: what is the exact improvement in the bound? After I go look at Koloskova et al. 2021, I see that the \"constant step size\" in the existing paper depends on the final iterate $T$ whereas the decreasing step size in this paper depends on $t$, which I assume is the difference. Apart from not having to set $T$ in advance, what is the advantage of this? For the second part, tighter upper bound, it was less clear even after I had a quick look at Koloskova et al. (2021) (quick look since I ideally would prefer not to review other papers to be able to review one paper especially in such tight timelines as usual for ML conferences), the comparison is unclear because the bounds have different dependences. What corresponds to $C$ and $\\omega$ in Koloskova et al. 2021 bound so that I can see what the improvement is?\n\nEven more confusing is the difference on the assumptions. Rate of Koloskova et al. 2021 is on the gradient norm, which is standard for nonconvex optimization. I see that since the problem in this paper is regularized, gradient norm itself is not enough, but one can instead look at the prox-mapping norm. However, also surprisingly the rate in this paper in Theorem 2 is on the objective value, which is not standard for a nonconvex optimization problem. This points out to the difference on the assumptions, this current paper assumes coercivity, whereas Koloskova et al. 2021 does not (from my quick look again, please correct me if I am wrong). Unfortunately, there is no comment about this in the paper after Theorem 2, how come we can now get a guarantee on the objective value (a very strong guarantee for nonconvex optimization requiring very strong additional assumptions) whereas Koloskova et al. 2021 only gets gradient norm. What would we get by only assuming the standard assumptions such as Koloskova et al. 2021? Can we get a similar rate for the prox mapping?\n\nOn the same topic, I also had to have a quick look at Zeng & Yin (2018b) trying to understand the difference on assumptions. The eqs. (88), (89) that this paper uses in the middle of page 17, is from Lemma 21 in Zeng & Yin (2018b) that additionally assumes that the objective function is convex. This would explain the resulting rate in the objective value, whereas the current submission does not mention this. Can the authors explain if the estimations they use from Zeng & Yin (2018b) use convexity of the objective or not? As a result, does the current submissions use convexity (or any other additional assumptions) in some way or not?\n\nMoreover, the paper mentioned in page 2 \"our contributions\" paragraph: \"Our findings suggest that the gradient quantization process introduces notable aggregation errors ..... Making the conventional analytical tools on exact PGD inapplicable\". Can you please clarify more? In my reading of the paper and the proofs, the analysis looks like a combination of Zeng & Yin (2018b) with Koloskova et al. (2021) and some tools from Alistarh et al. 2017 with Woldemariam et al. 2023 for encoding/decoding. If the authors claim that there are difficulties in combining these techniques, they should clearly state that. Even if there are not difficulties in combining techniques, this can also be fine if the result is strong enough. But this should be clarified by the authors.\n\nThe paper argues there is 75% communication improvement in practice, what about theory? Do the bounds predict any improvement? Moreover, what is the main sources of improvement in practice compared to choco-SGD? Is it using $\\ell_1$ regularization leading to sparse solutions? If so, how to quantify this? Page 5 in paragraph \"Source coding\" mentions \"intuitively\" regularized problem produces sparse solutions, but can the authors provide a precise theoretical evidence for this? Moreover, the authors mention that the consensus aggregation  is different from Koloskova et al. 2021, since they use a scheme from Dimakis et al. 2010, is this also helping to improve the communication efficiency? These really need to be clarified.\n\nNumerical results are a bit confusing. The paper solves the regularized problem whereas choco-SGD solves the unconstrained problem. How do the authors compare these two different methods solving different problems? Moreover, the authors say they use constant step size which is a bit disconnected from theory."
                },
                "questions": {
                    "value": "- eq. (55) please provide a pointer to the definition of $\\Phi^{(t+1)}$ as the sub gradient, for example after eq. (30). Also, after eq. (30) it calls $\\Phi^{(t+1)}$ Subdifferential whereas it should be subgradient.\n\n- Can you describe clearly Elias coding and Golomb coding used in Algorithm 2 (unfortunately many readers might be not familiar with coding theory) even if they are not essential for the purpose of the paper? Where do they come in to play in the analysis? Is it only the eq. (8) that is derived in the paper of Woldemariam et al. 2023? Also, for eq. (8) in Sect. 4 please provide a precise pointer in Woldemariam et al. 2023 where this result is proven so that a reader would know where to look. Also, please show clearly how Algorithm 2 fits within the main algorithm. In the \"encoding\" step of Algorithm 1, you may mention you call Algorithm 2 explicitly.\n\n- Assumptions 3i is written in a bit confusing way, please consider writing it like $x_i \\mapsto F_i(x_i)$ has $L_i$  as the Lipschitz constant for gradient so then it will be clear the sum is Lipschitz gradient  with the max of $L_i$. Please also provide more commentary about the coercivity in Assumption 3ii since it is not standard for nonconvex optimization and also different from existing works for example Koloskova et al. 2021.\n\n- eq. (4) please explain better the difference of this scheme with Koloskova et al. 2021. Especially since the notations in the two papers are different, it is not easy for the reader to compare.\n\n- footnote in page 4: What about theory? Does the theoretical results go through with asynchronous and time-varying network? If so, more justifications are needed.\n\n- Second page says that Nesterov proposed proximal gradient descent whereas Nesterov in this paper points to earlier work (including a paper from 1981) for this \"unaccelerated\" PGD. Can you please correct the reference for proximal gradient descent?\n\n- page.3 states \"all prior analysis on convergence of quantized models rely on smoothness\" as if this paper does not. But this paper also does, since all the proximal gradient methods do. They still use smoothness in a very central way and handle structural nonsmoothness with proximal operator. Better to be not misleading on this.\n\n- It is not clear to me what is \"inexact proximal gradient\" referring to here. For example in the paper of Schmidt et al. 2011, inexactness is both on the gradient computation and proximal operator. Here the proximal operator seems to be exact, am I missing something? Is the inexactness due to compression and other techniques used for improving communication efficiency?\n\n- Theorem 1: please point out to the definition of $\\tau$ from Assumption 2 in Theorem 1 for improving readability.\n\n- Right before eq. (58) the authors refer to some calculations in Koloskova et al. 2019 (eqs. (20)-(24)). Can you explicitly write these steps in the paper so that the reader will not have to go to another paper, get familiarized with their notation to come back, recall the notation of the current paper and then understand the steps?\n\n- What can we get with the same set of assumptions as Koloskova et al. 2021 by not introducing more assumptions? This would probably be a rate on the prox-mapping."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8432/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698948797651,
            "cdate": 1698948797651,
            "tmdate": 1699637050934,
            "mdate": 1699637050934,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "l634PlvMN1",
                "forum": "t8D9JxEn0J",
                "replyto": "Jm95wwEaum",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8432/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8432/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response Pt 1"
                    },
                    "comment": {
                        "value": "We want to thank the review for their detailed comments on the\nmanuscript. We understand the time commitment and difficulty in\nreviewing our paper within the time constraints. We have found your\ncomments to be constructive and insightful and we have done our best to\naddress them. We consider each of your paragraphs a single question for the formatting of the response.  \n\n1.  \n    **Reply**: Thank you for highlighting the ambiguity. We have added a\n    few statements throughout the paper to, hopefully, clarify where we\n    are different and where we have added a contribution. Analytically\n    there was difficulty in combing the non-smoothness and compression\n    errors for convergence. We have edited the contribution section\n    stating that our contributions are as follows:\n\n    1.  Our most novel contribution is combining sparsifcation,\n        communicating the compressed residual and adopting the novel\n        compression scheme proposed in Woldemariam (2023), that is best\n        suited to leverage the low entropy of the resulting updates. The\n        whole is greater than the sum of its parts. On the other hand,\n        from the and analytical standpoint the heavy lifting comes from\n        the non-smooth and non-convex nature of our algorithm which\n        required a non-trivial convergence analysis. Numerically, we\n        demonstrate a 75% reduction in communication costs when compare\n        to the state of the art.\n\n    2.  Analytically We prove that MALCOM-PSGD converges in the\n        objective value for non-convex and smooth loss functions with a\n        given compression error. MALCOM-PSGD exhibits a convergence rate\n        of $\\mathcal{O}(\\ln{t}/\\sqrt{t})$ and a consensus rate of\n        $\\mathcal{O}(1/t)$ with diminishing learning rates and consensus\n        stepsizes, where $t$ represents the number of training rounds.\n        This analysis was complicated by the fact that our aggregation\n        scheme does not preserve the average of the iterates meaning we\n        could not directly apply Koloskova et al. (2021) and Zeng & Yin\n        (2018b).\n\n    Additionally, when we introduce our aggregation scheme (equation 4)\n    we have added the following sentence to clarify this is one of our\n    contributions: \" This aggregation protocol is non-standard and is\n    thus one of our contributions.\\\" Additionally we add the following\n    line to Consensus Aggregation on page 5 \"Even though it does not\n    preserve the average of the iterates and complicates the analysis,\n    this aggregation scheme was chosen because it reduces the error\n    accumulation caused by $Q(\\bf{x})$\\\"\n\n    In regards to the compression operator we adopt the same assumptions\n    that Koloskova et al. (2021) uses. Meaning Assumption 2 is now:\\\n    For any input ${\\bf x}$,$Q({\\bf x}):\\mathbb{R}^d\\rightarrow\\mathbb{R}^{d}$\n    satisfies that $$\\mathbb{E}[\\|{Q({\\bf x})-\\bf x\\|}^2]\\leq (1-\\frac{1}{\\tau})\\|{\\bf x}\\|^2, \\qquad Q({\\bf 0})={\\bf 0},$$ where $\\tau\\in (0,1)$ is a constant representing the\n    expected compression error.\\\n    Finally, in section 4 where we provide theorem 1 and 2, we have\n    added the following remark to clarify the differences in assumptions\n    between Koloskova et al. (2021), Zeng & Yin(2018b), and our own. We\n    have added: \"Assumption 3(ii) is consistent with Zeng & Yin (2018b)\n    and under this condition, Assumption 4 requires that the loss values\n    produced in training iterations of our algorithm are finite. This\n    assumption can be reliably met with practical machine learning\n    solvers. We note that Assumption 4 is not required in Koloskova et\n    al. (2021) and Zeng & Yin (2018b), but is critical for our analysis\n    since we need it to bound the optimality gap. Koloskova et\n    al. (2021) is able to avoid this assumption by assuming a smooth\n    objective function while Zeng & Yin (2018b) avoids it by assuming\n    loss-less communication\\\".\n2.  \n    **Reply**:\n    Thank you for pointing out the inconsistency here, we agree that\n    this is an oranges to apples comparison so we have decided to remove\n    it from the manuscript. Since we have a non-smooth objective the\n    diminishing learning rate is required and thus makes a direct\n    comparison challenging. Importantly we have the same asymptotic\n    bound as Koloskova et al. (2021)."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8432/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700564440667,
                "cdate": 1700564440667,
                "tmdate": 1700564440667,
                "mdate": 1700564440667,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NPkO0gD7Fi",
                "forum": "t8D9JxEn0J",
                "replyto": "Jm95wwEaum",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8432/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8432/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response Pt 5"
                    },
                    "comment": {
                        "value": "17. \n    **Reply**: We will update this shortly. We want to get the key\n    points from the review out, but will update this before the rebuttal\n    period has concluded.\n\n18. \n    **Reply**: Unfortunately, to the best of our knowledge, we cannot\n    get anything if we do not have coercivity and Assumption 4 since\n    again this is required for analysis to hold. Furthermore, because we\n    do not preserve the average of the iterates so it is unclear how one\n    would do the analysis only using the assumptions in Koloskova et\n    al. 2021. We initially tried it this way and were unable to make any\n    progress."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8432/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700566713450,
                "cdate": 1700566713450,
                "tmdate": 1700566735775,
                "mdate": 1700566735775,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IYa4KRchTk",
                "forum": "t8D9JxEn0J",
                "replyto": "NPkO0gD7Fi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8432/Reviewer_zv9X"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8432/Reviewer_zv9X"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up"
                    },
                    "comment": {
                        "value": "Thank you for your rebuttal, I appreciate your attempt to address my concerns. Unfortunately, it's still difficult to be convinced of improvements in the paper. As the authors mention, in Table 1, in the worst case, there is no improvement. Sure, compared QSGD, their communication loss is smaller, is the factor significant enough? What is the maximum improvement and under which case it occurs? Compared to signSGD, there is no improvement. A bigger question is how realistic the considered setting is for the comparison? \n\nMoreover, the comparisons are also not always clear due to the difference in settings. signSGD and QSGD apply to general nonconvex setting (by just assuming smoothness) but in this paper there is additional assumptions (of course this paper focuses on a regularized problem whereas the previous ones focus on an unconstrained case). \n\nFor this reason, it is also not convincing for the authors to say \"whether or not $\\ell_1$ regularization promotes sparsity is out of the scope\" since this is one of the main ingredients that the authors say they use for improving communication complexity, this is the reason that the paper is focusing on regluarized problem and not unconstrained. Page 2 states for example \"Our algorithm leverages model sparsification to reduce the model dimension by incorporating l1 regularization during the SGD update\". App A.3 seems to assume a prior by assuming that $\\ell_1$ norm will produce the required sparsity. This is not proving that $\\ell_1$ regularization is doing what the authors claim it is doing, this looks more like assuming what they claim. This is what I understand in the paragraph \"Bit Analysis Under Laplace Inputs\", you can correct me if I am wrong. But in either case, it seems that the improvement is either quite small or there is no improvement compared to signSGD. I can see that experiments somewhat suggest some improvement, but they seem to be limited. \n\n> \"Eq. (51) of our manuscript. We tackle this challenge by bounding the additional error terms using the result in Theorem 1 of the manuscript.\"\n\nClaims like this are difficult to verify. How do you bound these additional terms? Theorem 1 that uses 2 other lemmas, and it is complicated for a reader to parse the \"main idea\" from these and the information the authors provide. If the additional terms that do not appear in previous work are bounded by using the additional assumption (Assumption 4), this is not \"tackling a challenge\" but more of adding an assumption so that the challenge goes away. For example the rebuttal states \"We cannot prove this lemma because we have the compression errors and hence why we introduce Assumption 4.\" If I am wrong and there are key ideas for tackling the new difficulties, these should be fleshed out.\n\nThe last point is perhaps less important because as I said before, the theoretical contribution is difficult to quantify due to the lack of improvements in the bounds and assumptions being different from the previous work. As such, unfortunately, I cannot suggest acceptance at this stage. Going forward, I really recommend the authors to make the writing much clearer. This includes explaining every time a new assumption made, \"why is this assumption reasonable\" by giving proper references and real-world example settings (referring to Assumption 4 or coercivity). The same for results, they should make a clear case why the result is interesting and is better in some way than previous results. If the main improvement is illustrated empirically, then this should be clearly stated that the aim of the theory is to show there is some guarantees, but the real improvements are shown empirically. Then, the paper will attract more empirically-minded reviewers who can judge better the rigor of the experimental comparisons."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8432/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700706694921,
                "cdate": 1700706694921,
                "tmdate": 1700706694921,
                "mdate": 1700706694921,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YrbqJuc1od",
            "forum": "t8D9JxEn0J",
            "replyto": "t8D9JxEn0J",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8432/Reviewer_aRgR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8432/Reviewer_aRgR"
            ],
            "content": {
                "summary": {
                    "value": "The paper considers the non-convex decentralized learning problems with finite-sum, smooth, and non-convex loss functions with L1 regularization. The authors propose MALCOM-PSGD algorithm that strategically integrates gradient compression techniques with model sparsification. The proposed algorithm is guaranteed to converge at a sublinear rate with a diminishing stepsize. Numerical results are provided to show the advantages of the algorithm on saving of communication."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed algorithm employs the residual compression via quantization and source coding methods to encode model sparsity, which can efficiently reduce the communication cost.\n2. They provide a comprehensive convergence analysis of the MALCOM-PSGD algorithm, and its performance is substantiated by suitable experimental evidence."
                },
                "weaknesses": {
                    "value": "1. The idea for model sparsification with L1 regularized training loss function and presented non-smooth problem are not surprising in distributed learning, which has been widely studied in the literature. \n2. The communication complexity is not provided; the authors is suggested to compare the communication complexity with related works.\n3. The theoretical results fail to demonstrate the existence of a linear-speedup case in decentralized training.\n4. Assumption 4 is directly imposed on the sequence generated by the algorithm, which is not well justified and appears to be stringent.\n5. The proof techniques used in the paper are standard in decentralized learning and source code methods is not original as well. \n6. The algorithm design should be further clarified. Additionally, there is an abuse of notations, e.g. the constant $L$."
                },
                "questions": {
                    "value": "refer to Weaknesses part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N.A."
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8432/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699358444082,
            "cdate": 1699358444082,
            "tmdate": 1699637050827,
            "mdate": 1699637050827,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vDOmJYMTCA",
                "forum": "t8D9JxEn0J",
                "replyto": "YrbqJuc1od",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8432/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8432/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response Pt 1"
                    },
                    "comment": {
                        "value": "we thank the reviewer for the comments.\n\n1.  *The idea for model sparsification with L1 regularized training loss\n    function and presented non-smooth problem are not surprising in\n    distributed learning, which has been widely studied in the\n    literature.*\n\n    **Reply**: We agree that model sparsification and proximal methods\n    have been extensively studied in distributed learning. However, it\n    is important to note that most of the existing research primarily\n    examines the influence of model sparsification for robustness or\n    dimension reduction without incorporating bit analysis of\n    compression.\n\n    Our research, in contrast, takes a distinctive approach of\n    *integrating* model sparsification with quantization and compression\n    techniques in local model communication. This approach significantly\n    enhances the communication efficiency in decentralized learning,\n    achieving a non-trivial $75$% gain compared to the state-of-the-art\n    method published in this conference \\[Ref 3\\]. Note this reference\n    also has established a \"non-surprising\\\" result in the convergence\n    analysis. However, it is non-trivial to show that the same\n    convergence rate can be maintained with a non-smooth proximal\n    optimization step. Note that this also means their proof can not be\n    directly used in our case. To summarize, we demonstrate that 1)\n    model sparsification and quantization/compression interactively\n    impact the convergence in decentralized learning, and 2) model\n    sparsification can significantly improve the communication rate\n    during model compression when the low-entropy nature of model\n    updates has been taken into account.\n\n    In the revised Introduction, we have highlighted the above\n    contributions and insights, particularly emphasizing the coupled\n    effect of model sparsification and compressed model communication,\n    done with a vector quantization scheme specifically designed for\n    inputs with a fast-decaying probability for large values and sparse\n    support. This quantization scheme itself significantly helps even\n    other learning methods, such as \\[Ref 3\\], although it clearly\n    performs better when model sparsification is incorporated.\n\n2.  *The communication complexity is not provided; the authors is\n    suggested to compare the communication complexity with related\n    works.*\n\n    **Reply**: In Section 5, we have expanded the discussion on the\n    communication complexity of the compression scheme under a concrete\n    example. Equation (18) shows that the communication bits required\n    for each local communication converge to\n    $\\mathcal{O}(de^{-1/\\Gamma})$ at the rate of\n    $\\mathcal{O}(-d\\log t)$. Here, $d$ represents the size of the model\n    parameters, $\\Gamma$ represents the quantization levels, and $t$\n    denotes the training iteration. The decrease in communication bits\n    over iterations results from the decreasing of the model residual\n    values, which is an effect of model sparsification and the\n    convergence of the algorithm, making the residual smaller and\n    smaller.\n\n    In addition, we have included in Appendix A.3 a detailed comparison\n    of our compression scheme with other methods. The following table\n    shows the comparison in terms of the *worst-case* communication rate\n    with existing compressed schemes \\[Ref 1\\] and \\[Ref 2\\].\n| Compression method | \\# of bits per local communication                                 |\n|--------------------|--------------------------------------------------------------------|\n| Our scheme         | Eq. (17) $< d(H(f^t)+\t\\text{const})\\leq d(\\log \\Gamma+\\text{const})$ |\n| QSGD [Ref 1]       | $d (\\log \\Gamma +\\text{const})$                                    |\n| signSGD [Ref 2]    | $d$                                                                |\n\nWe see from the table that the communication cost of our method\n    attains less bits compared to the method in \\[Ref 1\\]. This aligns\n    with our numerical findings in Section 6, which shows that our\n    approach consistently outperforms \\[Ref 1\\] in reducing\n    communication costs, exemplified by Fig. 1(c). When comparing our\n    method with signSGD from \\[Ref 2\\], with a small $\\Gamma$, our\n    method exhibits comparable communication costs. Nevertheless, the\n    method in \\[Ref 2\\] is limited to one-bit quantization only, which\n    incurs substantial quantization errors and subsequently slower\n    training convergence. As demonstrated in Section 6, our method not\n    only maintains similar communication costs but also significantly\n    enhances training performance compared to \\[Ref 2\\]."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8432/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700562213891,
                "cdate": 1700562213891,
                "tmdate": 1700664275426,
                "mdate": 1700664275426,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wLf5ykJ0Tl",
                "forum": "t8D9JxEn0J",
                "replyto": "0wiCG1akpg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8432/Reviewer_aRgR"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8432/Reviewer_aRgR"
                ],
                "content": {
                    "title": {
                        "value": "Response to the Rebuttal"
                    },
                    "comment": {
                        "value": "The reviewer thank the authors's effort in the reply which have partially addressed the reviewer's concerns. The reviewer would thus maintain the score."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8432/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700729344992,
                "cdate": 1700729344992,
                "tmdate": 1700729344992,
                "mdate": 1700729344992,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]