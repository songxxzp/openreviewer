[
    {
        "title": "Minimum width for universal approximation using ReLU networks on compact domain"
    },
    {
        "review": {
            "id": "QKHXkw7MiL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3196/Reviewer_FUZf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3196/Reviewer_FUZf"
            ],
            "forum": "dpDw5U04SU",
            "replyto": "dpDw5U04SU",
            "content": {
                "summary": {
                    "value": "This work studies the minimum width required for universal approximation ability of neural networks, under general settings with varying norm, input/output dimensions and activation functions. In particular, this work generalizes the result of Cai 2023, showing that the minimum width is exactly $\\max(d_x,d_y,2)$ for neural networks with ReLU-class activations to approximate $L^p$ functions from a compact set of $\\mathbb{R}^{d_x}$ to $\\mathbb{R}^{d_y}$. Then it's shown that when uniform approximation is considered, the minimum width is at least $d_y+1$ when $d_x\\le d_y\\le 2d_x$, implying a dichotomy between $L^p$ and uniform approximation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "**Solid results:** the theoretical results make a solid contribution to the understanding of the universal approximation ability of neural networks.\n\n**Wide coverage:** Theorem 2 holds for a wide class of ReLU-like activations, while previous works mostly consider only the most representative ReLU. Though such result is well expected due to the similarity between these activations, and indeed the proof is based on a simple reduction, it is non-trivial and such generality is valuable.\n\n**Tight bounds:** since the initial work of Lu et al 2017 which achieves the $d_x+4$ bound in the particular setting with $L_1$ norm, $d_y=1$ and ReLU activation, there has been a line of works on sharpening the bound itself, and generalizing the setting to other norms and activations. This work finally presents an exact characterization of the minimum width for general $L^p$ norm and a wide class of ReLU-like activations."
                },
                "weaknesses": {
                    "value": "**Questionable significance:** though this work makes a solid contribution to a tight characterization of the minimum width for universal approximation which is certainly valuable for our theoretical understanding, in my opinion, the mission itself to improve upon previous results is not so significant. The gap between known upper and lower bounds is merely an additive constant, and a similar tight result was achieved in Cai 2023 for the special case of Leaky-ReLU."
                },
                "questions": {
                    "value": "As a side note, the separation between the whole Euclidean space and compact subset (equivalently, $L^p$ versus uniform) was noticed even before Wang and Qu 2022. In a technical report of Lu [1], it's shown that any two-layer ReLU network must incur $\\int_{\\mathbb{R}^d}|f|$ error to approximate an integrable function $f$, a sharp contrast to the case when the integral is deployed on a compact set. Their argument is very simple and may be potentially helpful for explaining the intuition of such separation results.\n\n[1], A note on the representation power of ghhs, Zhou Lu, arXiv preprint 2101.11286"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3196/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3196/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3196/Reviewer_FUZf"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3196/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697136507585,
            "cdate": 1697136507585,
            "tmdate": 1700658382340,
            "mdate": 1700658382340,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YHzmnmF8bz",
                "forum": "dpDw5U04SU",
                "replyto": "QKHXkw7MiL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3196/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3196/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer FUZf"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their positive evaluation and valuable feedback. We address all comments of the reviewer, and provide pointers to the corresponding updates in the revised draft. All the updates are color-coded in the revised version.\n\n\n**A similar tight result was achieved in Cai 2023 for the special case of Leaky-ReLU.**\n\nWe would like to emphasize that the construction for achieving the tight minimum width of Leaky-ReLU networks for $L^p$ approximation by Cai (2023) does not generalize to ReLU networks. Cai (2023) constructed a Leaky-ReLU networks of width $\\max\\\\{d_x,d_y,2\\\\}$ by combining two observations: (1) $L^p$ functions can be approximated by neural ODEs (Li et al., 2022) and (2) neural ODEs can be approximated by Leaky-RELU networks of width $\\max\\\\{d_x,d_y,2\\\\}$ (Duan et al., 2022). However, the second observation requires the strict monotonicity of Leaky-ReLU networks, which does not extend to ReLU ones. To bypass this issue and show the tight minimum width for ReLU networks, we propose a completely different approach based on the characteristics of ReLU networks (Lemmas 5 and 7), which further extends to other (possibly non-monotone) ReLU-Like activation functions. We have clarified this difference in the revised draft (page 4 in Section 3).\n\n**The gap between known upper and lower bounds is merely an additive constant.** \n\nAlthough our results improve constant factors of existing bounds on the minimum width, we believe that our results are important contributions to the expressive power of neural networks. As the reviewer pointed out, our Theorems 1 and 2 provide a tight minimum width of networks using various activation functions for $L^p$ approximation. We think such tight characterization enables us to understand how the expressive power of neural networks can be affected by problem setups: for example, our Theorems 1 and 2 show dichotomy between the minimum widths for $L^p$ approximation on the whole Euclidean space and compact sets. To our knowledge, such a separation result was previously unknown in terms of the minimum width.\n\nTheorem 3 also generalizes dichotomy between $L^p$ and uniform approximation to general activation functions and general input/output dimensions. Furthermore, we recently found that *Theorem 3 is tight when $2d_x=d_y$* for Leaky-ReLU networks, together with the matching upper bound $\\max\\\\{2d_x+1,d_y\\\\}$ [Hwang, 2023]. We think our tight minimum width results can help better understand the universal approximability of deep and narrow neural networks.\n\nWe have added the new tight minimum width result using Theorem 3 to the revised draft (page 3 in Section 1.2 and page 5 in Section 3).\n\n[Hwang, 2023] Minimum Width for Deep, Narrow MLP: A Diffeomorphism Approach, arXiv preprint 2308.15873 \n\n**On separation between the whole Euclidean space and compact subset.**\n\nWe thank the reviewer for providing a reference [Lu, 2021] that we were not aware of. We have cited this work in the discussion about the separation between the whole Euclidean space and compact subset in our revised draft (page 4 in Section 3).\n\n[Lu, 2021] A note on the representation power of ghhs, Zhou Lu, arXiv preprint 2101.11286\n\n\nWe would be happy to clarify any concerns or answer any questions that may come up during the discussion period."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3196/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700366605177,
                "cdate": 1700366605177,
                "tmdate": 1700366605177,
                "mdate": 1700366605177,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2JJllDAtwK",
                "forum": "dpDw5U04SU",
                "replyto": "YHzmnmF8bz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3196/Reviewer_FUZf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3196/Reviewer_FUZf"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your reply. Now I see the technical novelty over Cai (2023), and since ReLU is a more fundamental case than Leaky-ReLU, I have increased the score accordingly."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3196/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700658369306,
                "cdate": 1700658369306,
                "tmdate": 1700658369306,
                "mdate": 1700658369306,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Tjaya5FjCG",
            "forum": "dpDw5U04SU",
            "replyto": "dpDw5U04SU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3196/Reviewer_fZsP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3196/Reviewer_fZsP"
            ],
            "content": {
                "summary": {
                    "value": "The authors, in a sense, improve on the available results quantifying the minimal widths required for a class of deep but narrow MLPs to be universal in $C([0,1]^{d_X},\\mathbb{R}^{d_Y})$.    The result is very interesting, and of a technical nature; namely, they show that minimal width can (but surprisingly and not shockingly) be improved when only considering approximation of functions on the cube and not the entire domain $\\mathbb{R}^{d_X}$."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The results are interesting, especially for those studying the fundamental limits of MLPs and their approximation theory. The presentation is clear and the proofs are mathematically correct.  \n\nThough one may foreseeable argue that the contribution is of a technical nature, these results answer fundamental questions at the core of the validity of practical MLP implementations. \n\nIn short, I think these results should definitely be published :)"
                },
                "weaknesses": {
                    "value": "The paper is rigorous and I do not see any weaknesses; with one exception:\n\n- Can the authors please add more details and rigor in the construction in Lemma 6's proof.  I know a lot of it is drawn from another paper's lemmata but it would be better to have it explicit and self contained.  Right now it is not even a proof but a proof sketch/recipe."
                },
                "questions": {
                    "value": "** 1) Impact of metric entropy on minimal width?**\n\nFix a compact subset $X$ of $\\mathbb{R}^{d_X}$, non-empty.  Suppose that we are looking for universal approximators in $C(X,\\mathbb{R}^{d_Y})$  implementable by MLPs with ReLU-Like activation function and of bounded widths.  How do you expect that the metric entropy/capacity of $X$ will impact the minimum width?\n\n\nFor instance, if $X=\\{x_0\\}$ is a singleton and $d_Y=1$, then clearly the width $\\min\\{d_X,d_Y,2\\}$ is suboptimal since the class since the class\n$$\n\\{ x\\mapsto a\\operatorname{ReLU}(1\\cdot (x+b)):\\, a\\in \\mathbb{R} ,\\, b:= -x_0 + 1\\}\n$$\nis universal in $C(X,\\mathbb{R})$.  So I guess there is room for improvement for general $X$.  (The same question applies to the case where $d_X=0$ and $d_Y=1$, in which case the minimum width is\n$$\n1 < \\max\\{d_X,d_Y,2\\}=\\max\\{0,1,2\\}=2.\n$$\n\n\n----\n\nWhat's your intuition on how the metric entropy of $X$ appears into the estimate?  \n\n\nI thought about the case where $X=\\{-1,1\\}$ but minimal width seems to apply there also.  What am I missing?\n\n\n\n----\n\n** 2) Why not note more general implications?**\n\nPerhaps I missed it, but it could be worth noting that your results also imply the minimal widths for universality/density in $C(\\mathbb{R}^{d_X},\\mathbb{R}^{d_Y})$ in the topology of uniform convergence on compact sets.  This is because of the extension and normalization arguments as in the proof of Proposition 3.10 [1] or in the proof of Proposition 53 [2], which allows one reduce the problem of universality in $C([0,1]^{d_X},\\mathbb{R}^{d_Y})$.  I.e. using either of the Tiezte or McShane extension theorems\n\n\n** 3) Improving Minimal Width Estimates for general nonlinearities**\n\nIn [5], the authors just recently showed that most MLPs with standard and continuous activation functions can approximately implement and MLP with ReLU activation function using roughly the same depth, width, and number of parameters.  I was wondering, unless I am missing something, why not use their results to sharpen your statement for general continuous activation functions?\n\n- References -\n\n[1] Acciaio, Beatrice, Anastasis Kratsios, and Gudmund Pammer. \"Designing universal causal deep learning models: The geometric (Hyper) transformer.\" Mathematical Finance (2023).\n\n[2] Kratsios, Anastasis, and L\u00e9onie Papon. \"Universal approximation theorems for differentiable geometric deep learning.\" The Journal of Machine Learning Research 23, no. 1 (2022): 8896-8968.\n\n[3] Arenas, Francisco Garc\u00eda, and Mar\u00eda Luz Puertas. \"Tietze's extension theorem.\" Divulgaciones Matem\u00e1ticas 10, no. 1 (2002): 63-78.\n\n[4] Beer, Gerald. \"McShane\u2019s extension theorem revisited.\" Vietnam Journal of Mathematics 48, no. 2 (2020): 237-246.\n\n[5] Zhang, Shijun, Jianfeng Lu, and Hongkai Zhao. \"Deep Network Approximation: Beyond ReLU to Diverse Activation Functions.\" arXiv preprint arXiv:2307.06555 (2023)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3196/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3196/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3196/Reviewer_fZsP"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3196/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698513575117,
            "cdate": 1698513575117,
            "tmdate": 1699636267475,
            "mdate": 1699636267475,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "afiVsPRiT5",
                "forum": "dpDw5U04SU",
                "replyto": "Tjaya5FjCG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3196/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3196/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer fZsP"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their positive evaluation and thoughtful feedback. We address all comments of the reviewer, and provide pointers to the corresponding updates in the revised draft. All the updates are color-coded in the revised version.\n\n**More details and rigor in the construction in Lemma 6's proof.**\n\nWe thank the reviewer for pointing this out. Following the reviewer\u2019s comment, we have added details to the proof of Lemma 6 in the revised draft (pages 14-16 in Appendix B.4).\n\n**Impact of metric entropy on minimal width.**\n\nWe are not sure if we correctly understand the definition of the metric entropy that you were considering, but we try to answer with the following definition, which appears in the covering argument in statistics: $\\log N(X,\\delta)$ where $N(X,\\delta)$ denotes the minimum number of $\\delta$-balls that can cover $X$ (i.e., the logarithm of the covering number). We consider $L^p$ approximation in this answer. If this problem setup is not what the reviewer was considering, please let us know.\n\nAs the reviewer pointed out, any map from a single point (of the zero metric entropy) to a real number can be exactly represented by a ReLU network of width one. We think this observation extends to any domain of zero Lebesgue measure (including any finite sets) since we can ignore such sets in $L^p$ approximation; ignoring them incurs zero $L^p$ error. Under this observation, one interesting question we think is that \u201cwhat if our error use a different measure, other than the Lebesgue one?\u201d For example, the Cantor set $\\mathcal C$ has zero Lebesgue measure but can have non-zero $d$-dimensional Hausdorff measure for $d<\\log(2)/\\log(3)$. In such a case, for universal approximation, we expect that ReLU networks of width one are insufficient since they can only represent monotone functions. Here, since the value $\\log(2)/\\log(3)$ is the Minkowski dimension of the Cantor set defined as $d_M(\\mathcal C):=\\lim_{\\delta\\to0^+} (\\log N(\\mathcal C,\\delta))/(\\log(1/\\delta))$, one may find a connection between the metric entropy and the minimum width for universal approximation under a Hausdorff measure. We believe investigating the universal approximation property of neural networks under various measures is an interesting future research direction.\n\n**More general implications.**\n\nWe thank the reviewer for this suggestion. As the reviewer pointed out, all of our results hold for an arbitrary compact domain in the Euclidean space. Using the extension lemmas provided by the reviewer, we have made this point clearer in the revised draft (page 5 in Section 3).\n\n**Improving minimal width estimates for general nonlinearities.**\n\nWe appreciate the reviewer for this suggestion. [Zhang et al., 2023] approximate ReLU networks using a network using a class of activation functions by scaling the width and depth of a network with multiplicative factors 3 and 2. Applying this result to our Theorem 1 gives us an upper bound $3\\max\\\\{d_x,d_y,2\\\\}$ on the minimum width. However, we found that this bound exceeds the known upper bound $\\max\\\\{d_x+2,d_y+1\\\\}$ in Theorem 4 by Park et al. (2021).\n\nOn the other hand, our results easily extend to other network architectures: recurrent neural networks (RNNs) and bidirectional recurrent neural networks (BRNNs). For these networks, we consider universally approximating the space of $L^p$ functions that maps a length $T$ sequence of $d_x$-dimensional vectors to a length $T$ sequence of $d_y$-dimensional vectors, denoted by $L^p([0,1]^{d_x\\times T},\\mathbb R^{d_y\\times T})$. Since the $t$-th output of RNN is always a function of the first to $t$-th inputs, for RNNs, we consider universally approximating the space of such functions, called *past-dependent* $L^p([0,1]^{d_x\\times T},\\mathbb R^{d_y\\times T})$, while we consider $L^p([0,1]^{d_x\\times T},\\mathbb R^{d_y\\times T})$ for BRNNs.\n\nWe recently showed that the minimum width of RNNs to be dense in past-dependent $L^p([0,1]^{d_x\\times T},\\mathbb R^{d_y\\times T})$ is exactly $\\max\\\\{d_x,d_y,2\\\\}$ if the activation function is one of ReLU, SOFTPLUS, Leaky-RELU, ELU, CELU, SELU, GELU, SILU, and MISH; here GELU, SILU, and MISH require $d_x+d_y\\ge3$. In addition, we also show that the minimum width of BRNNs to be dense in $L^p([0,1]^{d_x\\times T},\\mathbb R^{d_y\\times T})$ is upper bounded by $\\max\\\\{d_x,d_y,2\\\\}$ if the activation function is ReLU or in ReLU-Like. We have added these results with formal problem setups and proofs in the revised draft (pages 27-34 in Appendix F).\n\n[Zhang et al., 2023] Deep Network Approximation: Beyond ReLU to Diverse Activation Functions, arXiv preprint arXiv:2307.06555, 2023\n\n\nWe would be happy to clarify any concerns or answer any questions that may come up during the discussion period."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3196/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700366247973,
                "cdate": 1700366247973,
                "tmdate": 1700366613461,
                "mdate": 1700366613461,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eGEUYYAsjc",
                "forum": "dpDw5U04SU",
                "replyto": "afiVsPRiT5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3196/Reviewer_fZsP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3196/Reviewer_fZsP"
                ],
                "content": {
                    "title": {
                        "value": "Thanks"
                    },
                    "comment": {
                        "value": "Dear Authors thank you for the clarifications, adding details to Lemma 6's proof, and for the very interesting discussion.  I think this is a very nice paper."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3196/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700692761650,
                "cdate": 1700692761650,
                "tmdate": 1700692761650,
                "mdate": 1700692761650,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "jqElg5Vu2T",
            "forum": "dpDw5U04SU",
            "replyto": "dpDw5U04SU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3196/Reviewer_4yPi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3196/Reviewer_4yPi"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors study the universal approximation problem of deep neural networks with unlimited depth. The main contribution of this paper is to derive that when the input domain and output domain are $[0,1]^{d_x}$ and $\\mathbb R^{d_y}$ respectively, the minimum width of the universal approximation of neural networks for $L^p$ functions is equal to $\\max(d_x,d_y,2)$, when the activation function is similar to RELU (e.g., RELU, GELU, SOFTPLUS). The authors also show that if the activation function is a continuous function that can be uniformly approximated by a sequence of continuous one-to-one functions, then the minimum width of the universal approximation of neural networks for continuous functions is at least $d_y+1$ if $d_x<d_y \\leq 2d_x$."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Originality: The related works are adequately cited. The main results in this paper will certainly help us have a better understanding of the universal approximation property of deep neural networks from a theoretical way. I have checked the technique parts and found that the proofs are solid. One of the main results, which shows that there is a dichotomy between $L^p$ and uniform approximations for general activation functions and input/output dimensions, is a non-trivial extension of previous results in this field.\n\nQuality: This paper is technically sound.\n\nClarity: This paper is clearly written and well organized. I find it easy to follow.\n\nSignificance: I think the results in this paper are not very significant, as explained below."
                },
                "weaknesses": {
                    "value": "However, I have several concerns about the contribution of this paper. Firstly, the paper (Cai, 2023) already proved that the minimum width of the universal approximation of neural networks for $L^p$ functions is equal to $\\max(d_x,d_y,2)$, when the activation function is Leaky-RELU. This paper only generalizes Leaky-RELU to RELU-LIKE activations (e.g., RELU, GELU, SOFTPLUS), and derives the same result. I think this makes the contribution of this paper incremental. Also, It would be more interesting if the authors could study the exact minimum width for more architectures used in practice. Furthermore, the technical part is not very deep and mostly based on the technical results from previous papers such as (Cai, 2023). In summary, I think this paper is a decent paper with some good results, but may not be suitable for the top conferences such as ICLR."
                },
                "questions": {
                    "value": "As explained above, It would be more interesting if the authors could study the exact minimum width for more architectures used in practice."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3196/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698581583440,
            "cdate": 1698581583440,
            "tmdate": 1699636267380,
            "mdate": 1699636267380,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wyf8kMe6Nj",
                "forum": "dpDw5U04SU",
                "replyto": "jqElg5Vu2T",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3196/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3196/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 4yPi"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer for their time and effort to provide valuable comments. We address all comments of the reviewer, and provide pointers to the corresponding updates in the revised draft. All the updates are color-coded in the revised version.\n\n**This paper only generalizes Leaky-RELU to RELU-LIKE activations, and derives the same result. I think this makes the contribution of this paper incremental.**\n\nWe believe that our contribution extending Leaky-ReLU to ReLU-Like activation is non-trivial: the existing proof for Leaky-ReLU networks does not extend to ReLU and ReLU-like activation functions (our Theorems 1 and 2). The proof of Cai (2023) for showing the upper bound on the minimum width $\\max\\\\{d_x,d_y,2\\\\}$ consists of two parts: they (1) approximate $L^p$ functions via neural ODEs following (Li et al., 2022) and (2) approximate neural ODEs via Leaky-RELU networks of width $\\max\\\\{d_x,d_y,2\\\\}$ by using results in (Duan et al., 2022). Here, the second part heavily relies on the strict monotonicity of Leaky-ReLU, and hence, does not generalize to ReLU. To bypass this issue and show the tight minimum width for ReLU networks, we propose a completely different proof technique (not using neural ODEs) utilizing properties of ReLU networks (Lemmas 5 and 7), which generalizes to (possibly non-monotone) ReLU-Like activation functions. We also note that our Theorems 1 and 2 first show the separation between the whole Euclidean space and compact subset for $L^p$ approximation, which was unknown up to our knowledge.\n\nFurthermore, we would like to emphasize that our Theorem 3 is also an important contribution. The best known lower bound on the minimum width for uniform approximation under general $d_x,d_y$ and general activation functions was $\\max\\\\{d_x+1,d_y\\\\}$ (Johnson, 2019; Park et al., 2021). However, a few exceptional cases indicate that this bound can be improved: $d_y+1 > \\max\\\\{d_x+1,d_y\\\\}$ is a tight lower bound for ReLU/Leaky-ReLU networks when $d_x=1$ and $d_y=2$ (Park et al., 2021; Cai, 2023). Our Theorem 3 improves existing lower bounds by showing that the minimum width for uniform approximation is at least $d_y+1$ if $d_x < d_y \\le 2d_x$ for continuous activation functions that can be uniformly approximated by a sequence of continuous injections. As the reviewer pointed out, Theorem 3 also extends the dichotomy between and uniform approximations to general activation functions and input/output dimensions. In addition, we recently found that *Theorem 3 is tight when $d_y=2d_x$* for Leaky-ReLU networks, together with the matching upper bound $\\max\\\\{2d_x+1,d_y\\\\}$ in [Hwang, 2023]. We think our tight minimum width results can help better understand universal approximability of deep and narrow neural networks.\n\nIn the revised draft, we have clarified the difference between the existing Leaky-ReLU network result (Cai, 2023) and ours (page 4 in Section 3), and added the new tight minimum width result for uniform approximation using Theorem 3 (page 3 in Section 1.2 and page 5 in Section 3).\n\n[Hwang, 2023] Minimum Width for Deep, Narrow MLP: A Diffeomorphism Approach, arXiv preprint 2308.15873, 2023"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3196/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700365607609,
                "cdate": 1700365607609,
                "tmdate": 1700366622466,
                "mdate": 1700366622466,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5eUuhTR6wV",
                "forum": "dpDw5U04SU",
                "replyto": "jqElg5Vu2T",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3196/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3196/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 4yPi"
                    },
                    "comment": {
                        "value": "**The technical part is not very deep and mostly based on the technical results from previous papers such as (Cai, 2023).**\n\nWe believe that our technical results are not mostly based on previous papers such as (Cai, 2023). Theorem 1 (and Theorem 2) uses the lower bound $\\max\\\\{d_x,d_y\\\\}$ from (Cai, 2023); however, this bound can be shown using rather straightforward arguments. If a network has width $d_x-1$, then it must have the form $g(Mx)$ for some continuous function $g:\\mathbb R^{d_x-1}\\to\\mathbb R^{d_y}$ and $M \\in \\mathbb{R}^{(d_x-1) \\times d_x}$. Hence, the network cannot use the full information of inputs and cannot universally approximate, e.g., consider approximating $\\\\|x\\\\|_2^2$. Likewise, if a network has width $d_y-1$, then it must have the form $Nh(x)$ for some continuous function $h:\\mathbb R^{d_x}\\to\\mathbb R^{d_y-1}$ and $N\\in\\mathbb{R}^{d_y\\times (d_y-1)}$ and cannot universally approximate, e.g., consider approximating a path that visits all vertices of a $d_y$-dimensional standard simplex along its edges. Combining these two arguments gives us the lower bound $\\max\\\\{d_x,d_y\\\\}$.\n\nOn the other hand, we believe our proof of the upper bound $\\max\\\\{d_x,d_y,2\\\\}$ in Theorem 1, which we think the most critical part for showing the tight minimum width, is non-trivial and has technical novelty, especially compared to the lower bound. As we previously answered, prior result in (Cai, 2023) does not extend to ReLU (and ReLU-Like activation functions). We use a coding scheme; however, the existing coding-based ReLU network construction achieving the tight minimum width (Park et al., 2021) considers the whole Euclidean space and requires width at least $d_x+1$. Namely, existing results cannot directly show the tight minimum width in our problem setups: ReLU or ReLU-Like activation functions and a compact domain. We would like to also note that Theorem 3, which is also an important contribution of our submission, does not rely on any of previous technical results up to our knowledge. \n\nWe have added this discussion to the revised draft (page 5 in Section 3).\n\n**Exact minimum width for more architectures used in practice.**\n\nWe thank the reviewer for this interesting comment. Following the reviewer\u2019s comment, we have explored other network architectures and found that our proof techniques can be used for bounding the minimum width of  recurrent neural networks (RNNs) and bidirectional RNNs (BRNNs) for $L^p$ approximation. Specifically, we consider universally approximating the space of $L^p$ functions that maps a length $T$ sequence of $d_x$-dimensional vectors to a length $T$ sequence of $d_y$-dimensional vectors, denoted by $L^p([0,1]^{d_x\\times T},\\mathbb R^{d_y\\times T})$. Since the $t$-th output of RNN is always a function of the first to $t$-th inputs, for RNNs, we consider universally approximating the space of such functions, called *past-dependent* $L^p([0,1]^{d_x\\times T},\\mathbb R^{d_y\\times T})$, while we consider $L^p([0,1]^{d_x\\times T},\\mathbb R^{d_y\\times T})$ for BRNNs.\n\nWe proved that the minimum width of RNNs to be dense in past-dependent $L^p([0,1]^{d_x\\times T},\\mathbb R^{d_y\\times T})$ is exactly $\\max\\\\{d_x,d_y,2\\\\}$ if the activation function is one of ReLU, SOFTPLUS, Leaky-RELU, ELU, CELU, SELU, GELU, SILU, and MISH; here GELU, SILU, and MISH require $d_x+d_y\\ge3$ as in Theorem 2. Furthermore, we also show that the minimum width of BRNNs to be dense in $L^p([0,1]^{d_x\\times T},\\mathbb R^{d_y\\times T})$ is upper bounded by $\\max\\\\{d_x,d_y,2\\\\}$ if the activation function is ReLU or in ReLU-Like.\n\nWe have added these new results with corresponding proofs and formal problem setups in the revised draft (pages 27-34 in Appendix F).\n\nWe would be happy to clarify any concerns or answer any questions that may come up during the discussion period."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3196/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700365890211,
                "cdate": 1700365890211,
                "tmdate": 1700366324178,
                "mdate": 1700366324178,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]