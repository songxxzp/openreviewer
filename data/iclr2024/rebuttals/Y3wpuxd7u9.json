[
    {
        "title": "GoLLIE: Annotation Guidelines improve Zero-Shot Information-Extraction"
    },
    {
        "review": {
            "id": "uDRvnLLGEG",
            "forum": "Y3wpuxd7u9",
            "replyto": "Y3wpuxd7u9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7332/Reviewer_zWZU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7332/Reviewer_zWZU"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes GoLLIE, a large language model fine-tuned to follow label descriptions and few-shot examples to improve zero-shot information extraction. Key ideas include formulating tasks as Python code, regularizing training, and comprehensive evaluation on diverse IE tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* LLMs have excelled at numerous complex tasks, but underperformed in IE tasks. This paper addresses this gap, diving deep into the annotation guidelines and error analysis using an LLM approach: zero-shot learning, natural language instructions, programming language output format, and leveraging one model to tackle multiple tasks.\n* The study's thorough experiments span various IE tasks, coupled with an extensive error analysis when applying the unified model across these diverse tasks. Such detailed error analysis offers valuable insights into the model's strengths and limitations.\n* Another significant contribution is crafting prompts for intricate IE tasks. Demonstrating the model's ability to adhere to these instructions, especially given the recognized challenges of current LLMs in following complex instructions, signifies substantial effort and expertise from the IE domain."
                },
                "weaknesses": {
                    "value": "* The claim of making use of \u201cannotation guideline\u201d may be an overstatement - this paper only considered label name, label description and few-shot examples, however, annotation guideline in IE domain are very complicated and was curated by linguists. E.g., For TACRED slot filling (https://tac.nist.gov/2015/KBP/ColdStart/guidelines/TAC_KBP_2015_Slot_Descriptions_V1.0.pdf), section 3.6 per:city_of_birth, they use \u201cGPEs below the city level (e.g. 5 boroughs of New York City) are not valid fillers.\u201c as an example rule to guide annotators. The prompts proposed by this paper might not fully capture the depth of true guideline understanding.\n* The paper omits key references from the era before LLMs that discuss label descriptions and verbalization. Examples include:\n    * Zero-Shot Relation Extraction via Reading Comprehension\n    * An Empirical Study on Multiple Information Sources for Zero-Shot Fine-Grained Entity Typing\n    * Label Verbalization and Entailment for Effective Zero- and Few-Shot Relation Extraction\n\nSmall formatting issues:\n* Use ``\u2019\u2019 instead of \u2018\u2019\u2019\u2019.\n* Table 6 could be more user-friendly. Presenting exact numbers (like 10/10,000) would be clearer than percentages.\n\nPlease see more in the question section."
                },
                "questions": {
                    "value": "* How does the prompt length vary in the proposed method? Translating IE tasks into Python classes might make them more accessible for LLMs, but could also significantly inflate the token count. For instance, a fine-grained entity type classification task with over 1000 entity types would result in a considerable token overhead.\n* The paper doesn't detail the human (or domain expert) effort required to craft the prompts. Given the variety of labels and tasks covered in the study, it would be beneficial to track this for subsequent research.\n\nThese are important questions that need to be answered in the updated paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7332/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7332/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7332/Reviewer_zWZU"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7332/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698542820186,
            "cdate": 1698542820186,
            "tmdate": 1699636876910,
            "mdate": 1699636876910,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Jc7h4ElU82",
                "forum": "Y3wpuxd7u9",
                "replyto": "uDRvnLLGEG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7332/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7332/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Regarding the first weakness, yes we completely agree on that. It is something that we definitely want to try. However those guidelines are very long, and considering the amount of labels we have per example, it is intractable with our current models/infrastructure setup. With recent efficient methods and models for larger sequences we hope we could soon explore the use of more detailed and longer guidelines in our experiments. We believe that the insights from this paper will motivate further research in this direction.\n\nRegarding the second weakness, we will happily add the missing references (see paper changes). In fact, one of them was already in the paper. Thank you for your suggestions.\n\nThank you for your recommendations, we will update the paper accordingly.\n\nFor the first question, we have added to the updated paper a new discussion section (Appendix E) related to the prompt length and scalability. However, answering your question, the amount of tokens required to represent the code structures is very small in contrast with the tokens used for the guidelines. This depends on how detailed the guidelines are and on the specific tokenizer. But as a reference, for representing the class \u201cActor\u201d of the MitMovie named entity dataset, the code structure accounts for 52 characters, while the actual guideline requires 350 characters. In the HarverNER Point entity, which has a more extensive and detailed guideline, the code structure accounts for only 8% of the total characters of the guideline. While there is a small overhead of tokens, when dealing with a dataset with 1000 entity types, the main limitation would be fitting into the model input 1000 guideline definitions, not the Python code structure.\n\nRegarding your second question, the human effort involved was very little in the case of freely available guidelines. We just needed to copy and paste, and if they were very long and detailed (the case of ACE and TACRED for example), simplify. When no guidelines are provided for a given dataset, we either reuse some templates from previous works (for WikiEvents for example) or follow other dataset styles to generate very simple guidelines. The human effort here was also very little. The more important point is that no domain expert was required. We have added a new section in the updated paper (Appendix F) with a more detailed disclosure of the human effort involved to generate the inputs."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7332/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700237973918,
                "cdate": 1700237973918,
                "tmdate": 1700237973918,
                "mdate": 1700237973918,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "n9C8rkXe5L",
                "forum": "Y3wpuxd7u9",
                "replyto": "Jc7h4ElU82",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7332/Reviewer_zWZU"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7332/Reviewer_zWZU"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for addressing my concerns in the revised version of your paper.\n\nI would like to emphasize a particularly unique and impactful aspect of your work: the error analysis concerning label definition conflicts. This analysis is especially enlightening and holds significant implications for future research. In the current era of LLMs, resolving conflicts that arise from training a single model on multiple existing datasets represents a critical challenge. Your paper makes a substantial contribution in this area.\n\nI will maintain my original score for the paper."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7332/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700466646832,
                "cdate": 1700466646832,
                "tmdate": 1700466646832,
                "mdate": 1700466646832,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "yEtUdh7S1w",
            "forum": "Y3wpuxd7u9",
            "replyto": "Y3wpuxd7u9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7332/Reviewer_xrh5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7332/Reviewer_xrh5"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces GoLLIE (Guideline-following Large Language Model for IE), a model fine-tuned to effectively utilize annotation guidelines for enhanced zero-shot results in Information Extraction tasks. A primary distinction of GoLLIE is its use of a Python code-based representation for both input and output. This method provides a clear, human-readable format, unifying the representation of various IE tasks and addressing challenges associated with traditional natural language instructions. Empirical evaluations validate GoLLIE's superior capabilities in leveraging guidelines, indicating promising directions for future research and model enhancements, including the exploration of more extensive pre-training datasets and refining its response to ambiguous labels."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper presents a novel solution to the long-standing challenge where large models struggled to leverage intricate annotation guidelines inherently. Through targeted fine-tuning and a unified Python code-based representation for both input and output, it significantly improves zero-shot outcomes in IE tasks.\n\n2. The detailed error analysis provides invaluable insights into the challenges of zero-shot IE, especially when leveraging guidelines. The study methodically identifies specific issues like the ambiguity of labels, conflicts between fine-grained and coarse entities, and the repercussions of strong label preconceptions. These findings offer a valuable roadmap for subsequent research in the domain, emphasizing the need for clearer guidelines, broader fine-tuning datasets, and more specific instruction-following capabilities.\n\n3. The paper is well-written and easy to follow."
                },
                "weaknesses": {
                    "value": "1. The paper's novelty appears somewhat limited as similar approaches have been explored in past research. [1] proposes a zero-shot entity typing approach that utilizes the type description available from Wikipedia to build a distributed semantic representation of the types.\n\n2. The choice of code-based prompting can potentially hinder usability for general users. Those unfamiliar with coding might find it challenging to interact with or fully leverage the model, thereby narrowing its applicability.\n\n3. The method's efficacy remains unclear due to incomplete experimental validation. Specific concerns include:\n\n   a. **Prompt Style Impact**: The rationale for using a Python code style prompt is not experimentally validated. It's untested whether natural language prompts with guidelines might yield similar outcomes. Although CodeIE validated code style prompts for OpenAI models, the necessity of this design for open-source LLMs during instruction tuning remains unproven.\n\n   b.**Prompt Sensitivity**: The paper omits experiments examining the sensitivity of prompts, leading to concerns about the method's stability. The model's performance under varying definitions, code structures, and code comment styles remains unexplored, making its robustness questionable.\n\n4. The paper lacks experiments analyzing the impact of training dataset diversity on the LLM's ability to follow unseen guidelines. It remains unclear how much data is required to achieve satisfactory performance, leaving questions about the scalability and efficiency of the approach.\n\n\n\n[1] Description-Based Zero-shot Fine-Grained Entity Typing"
                },
                "questions": {
                    "value": "1. **Sampling Strategy**: Datasets differ in scale, and a naive mixture could introduce imbalance. How did the authors handle the sampling process for the datasets used in this study? Were experiments conducted to assess the effects of different sampling techniques on the results? This aspect wasn't addressed in the paper and could provide clarity on the method's robustness across varied data distributions.\n2. **Scalability with Numerous Labels**: For datasets with a large number of labels, say in the order of hundreds, the input length for your method could become substantially lengthy, leading to efficiency concerns. In such complex scenarios with multiple labels, how does the inclusion of guidelines impact the performance? This wasn't touched upon in the paper but would provide deeper insights into the method's scalability and effectiveness in real-world applications.\n3. **QLoRA vs. Full Model Fine-tuning**: The authors mentioned employing QLoRA for training, citing its superior performance over fine-tuning the entire model on zero-shot tasks and faster training speed. However, the paper lacks empirical evidence to support this. What are the relative impacts of full-parameter SFT and techniques like LoRA and QLoRA on zero-shot IE performance?\n4. **Benchmark Selection**: Many of the state-of-the-art methods used for comparison in the paper, such as UIE, appear outdated. Why weren't more recent and potentially stronger methods like USM, InstructUIE, and GPT4 considered for benchmarking?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7332/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7332/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7332/Reviewer_xrh5"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7332/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698564997526,
            "cdate": 1698564997526,
            "tmdate": 1700449612895,
            "mdate": 1700449612895,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SPB0fDgkYB",
                "forum": "Y3wpuxd7u9",
                "replyto": "yEtUdh7S1w",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7332/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7332/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Before answering your concerns, we would like to emphasize that using code style prompts for IE is not one of our contributions. It was just a development decision due to its practical properties. \n\nRegarding the first weakness, our main contribution is the use of annotation guidelines. The idea for this came from the weaknesses of previous instruction-tuned approaches. We go beyond instruction tuning, as we explore techniques to gear the model towards focusing on the guidelines. \n\nFor the second weakness, GoLLIE is not a product. Therefore, it is not intended nor designed to be used by general users. Our goal building GoLLIE was to show that teaching a model how to follow guidelines is beneficial, independently of the prompting strategy choice. In any case, coding skills are not needed to just interpret Python class definitions. Alternatively, it is straightforward to build a user friendly interface on top of it, but this is out of the research scope.\n\nFor the third weakness, we will address each point at a time:\n* __Backbone influence:__ Our goal is to demonstrate that training LLMs to adhere to annotation guidelines improves the performance of IE models in unseen tasks.  We demonstrate this by building GoLLIE and a baseline. The baseline shares the same model, same data, same environment but uses no guidelines. InstructUIE is comparable to our baseline, as both receive the same amount of information in the input. The main difference is the backbone LLM , InstructUIE uses FlanT5 instead of LLaMA/CodeLLaMA. Note that [1] indicates that FlanT5 is superior to LLaMA on instruction following tasks, so our backbone starts from a weaker position. As you suggest, we could have added guidelines to InstructUIE. Unfortunately, this is not possible, as Flan-T5 is limited to only 512 tokens in the input. Our research shows that doing this would have improved the performance of InstructUIE in unseen-tasks.  In any case, we do not claim that our choice of backbone and code-style prompting is superior to the framework presented in InstructUIE. Our main claim is that including guidelines in the training and inference of information extraction models improves the performance of the system on unseen tasks. \n* __Prompt style impact:__ Although not rigorously tested, the Code style-syntax worked well in our preliminary experiments with LLaMA (completely zero-shot without any fine-tuning), better than actual natural language prompts. Note that our method involves fine-tuning, and in this case the prompt style loses relevance, as the model rapidly learns to adapt to it (the first checkpoint in the training already showed very few malformed outputs and hallucinations). We expect that Code-style prompts and natural language instructions would perform similarly after fine tuning. Although this would be an interesting experiment,  it would involve an amount of human labor and compute resources that we don't have (implement another system, designing a set of different prompts and running the experiments). Using Code-style syntax is a convenience, not a contribution of our research. Regarding \u201cthe necessity of this design for open-source LLMs during instruction tuning remains unproven\u201d , we do not claim that this style is necessary.\n\nFor the fourth weakness, under this scenario, the prompt variations are very restricted. All the inputs should follow the template illustrated in Figure 2.  However, we do expect the users to test different class definitions, or guidelines styles. The purpose of the evaluation on un-seen tasks is designed to evaluate how the model performs in this scenario. Each dataset used in the zero-shot evaluation has its own definitions written in its own styles, which allows us to indirectly verify the robustness of our method. \n\nRegarding the last weakness, we plan to explore how the model scales with data in the future. However, the efficiency of the approach is proved, as GoLLIE 7B outperforms 11B Instruct-UIE on zero-shot IE tasks with much less training data: 9 IE datasets on our side vs 1.8k FLAN tasks and 32 IE datasets for Instruct-UIE.\n\n[1] Chia, Y. K., Hong, P., Bing, L., & Poria, S. (2023). INSTRUCTEVAL: Towards Holistic Evaluation of Instruction-Tuned Large Language Models. arXiv preprint arXiv:2306.04757."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7332/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700237816398,
                "cdate": 1700237816398,
                "tmdate": 1700237816398,
                "mdate": 1700237816398,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aO4Ymv3IJg",
                "forum": "Y3wpuxd7u9",
                "replyto": "yEtUdh7S1w",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7332/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7332/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Regarding the questions"
                    },
                    "comment": {
                        "value": "* __Sampling strategy:__  The only training dataset that has a size that significantly differs from the others is OntoNotes. This dataset is generated semi-automatically and is orders of magnitude larger than the other ones. Therefore, we sampled 30.000 random examples from the training set. The imbalance on dataset size for the rest of the datasets is small. As it can be seen in the supervised evaluation result, our model does not overfit to a single dataset/task. Therefore, we do not consider it necessary to implement any additional sampling strategies. \n* __Scalability with Numerous Labels:__ The inclusion of guidelines in datasets with hundreds of labels will make inputs extremely long, and they would not fit in the context size of current LLMs. This is an already known constraint in LLMs, and recently a lot of research effort is being directed to algorithms that efficiently increase the context window size. We expect that in the future, LLMs will have a context window large enough to fit  not only more labels, but also more detailed guidelines as well. For now, the problem could be solved by batching the labels into multiple input examples. That is, instead of prompting the model with, for example, 100 labels in a single input, it is possible to prompt the model with 10 inputs that incorporate 10 labels each and combine all the outputs into a single one. In any case, we will mention in the paper that this is a limitation for our model. We have added a discussion about this limitation to the updated paper (Appendix E). \n* __QLoRa vs Full-Model finetuning:__ we did provide information about our preliminary analysis in Appendix C.3 (also referred to in section 4.2 \u201cTraining details\u201d). We have tested multiple hyperparameter configuration for Full-Model finetuning (we have tested the same hyperparameters used by previous work) and none of it was successful. This doesn\u2019t mean that such a configuration doesn\u2019t exist. What we claim in our paper is that LoRA achieves very good performance and requires much less compute resources than Full-Fine Tuning. Which is supported by previous research and demonstrated in Appendix C.3. We will clarify this in the paper. \n* __Benchmark selection:__ Instruct-UIE is our main baseline (See Section 5.2), which is  the current state-of-the-art Information Extraction model in zero-shot settings. In the Instruct-UIE paper, they already demonstrate that their method is superior to USM, so we didn\u2019t use it in our evaluation (Although we discuss USM in the Related Work Section). Regarding closed-source models such as GPT-4, our objective is to test the hypothesis that training Large Language Models (LLMs) to adhere to annotation guidelines can enhance their zero-shot performance in Information Extraction tasks. Validating this hypothesis involves building a baseline not trained to follow annotation guidelines and performing multiple ablation studies. This is not possible with closed-source models such as GPT-4. Moreover, we know almost nothing from the underlying model(s) powering this commercial product or the data used for training. On top of that, this product is regularly updated, and previous versions are not accessible anymore, therefore, experiments with GPT-4 are not reproducible. Consequently, evaluating GPT-4, or any other closed-source modes, would not add any additional scientific insight in this context of this paper."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7332/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700237902717,
                "cdate": 1700237902717,
                "tmdate": 1700237902717,
                "mdate": 1700237902717,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WD3KewS5bO",
                "forum": "Y3wpuxd7u9",
                "replyto": "SPB0fDgkYB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7332/Reviewer_xrh5"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7332/Reviewer_xrh5"
                ],
                "content": {
                    "title": {
                        "value": "Further comments on specific points"
                    },
                    "comment": {
                        "value": "Thank you for your detailed response, which has clarified some of my concerns. However, I would like to offer further comments on specific points.\n\n1. **On the Novelty of Using Guidelines**: While the concept of using guidelines as a novel approach in your paper is appreciated, it appears somewhat limited in its novelty. Similar approaches have been explored in past research, notably in a 2019 ACL paper which also enhanced model capabilities through guidelines. This category of work, involving the incorporation of additional knowledge to boost model performance, seems to be under-cited and discussed in your paper.\n2. **Backbone Influence**: I mostly agree with the points made in your rebuttal. However, I would like to challenge the assertion that \u201cFlanT5 is superior to LLaMA on instruction following tasks, so our backbone starts from a weaker position.\u201d In the domain of Information Extraction (IE), this conclusion may not be consistently valid unless it is supported by targeted experimental data.\n3. **Prompt Style Impact**: I understand the constraints you faced, but if it were possible to supplement your paper with relevant experiments comparing different prompt styles, it would significantly enhance the robustness and credibility of your findings.\n\n[1] Description-Based Zero-shot Fine-Grained Entity Typing\n\n\n\nReviewer xrh5"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7332/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700448417431,
                "cdate": 1700448417431,
                "tmdate": 1700448417431,
                "mdate": 1700448417431,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AJhIpjX74O",
                "forum": "Y3wpuxd7u9",
                "replyto": "aO4Ymv3IJg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7332/Reviewer_xrh5"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7332/Reviewer_xrh5"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for addressing my questions. With these additional response, it looks more solid.\n\nThanks,\n\nReviewer xrh5"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7332/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700448462593,
                "cdate": 1700448462593,
                "tmdate": 1700448462593,
                "mdate": 1700448462593,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4RvXjBIyqs",
            "forum": "Y3wpuxd7u9",
            "replyto": "Y3wpuxd7u9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7332/Reviewer_bJoP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7332/Reviewer_bJoP"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an LLM fintuned to comply with annotation guidelines for information extraction tasks. Specifically, with a focus on NER, EE, and EAE (19 datasets in total), the task format is unified into a code style: the label class definitions as class docstrings and the candidates as a comment for the arguments.  Some regularization techniques are proposed to ensure that the model follows the guidelines and does not just learn to identify specific datasets. Experiments show that a finetuned version of Code-LLaMA achieves comparable performance compared with a naive baseline finetuned without guidelines, and beats the baseline in zero-shot settings by a large margin."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper proposes to utilize the annotation guidelines to boost the performance of zero-shot information extraction. The idea is intuitive and compelling.\n2. The experimental results properly demonstrate the effectiveness of the proposed method and the bonus of the annotation guidelines.\n3. The presentation of this paper is clear, with few typos."
                },
                "weaknesses": {
                    "value": "1. The motivation behind the code style and its effects are unclear. Note that incorporating the annotation guidelines in a natural language style rather than the code style is also feasible. The LLMs pretrained on codes may lack natural language understanding capabilities that are crucial to conducting IE tasks. This may result in the low performance of the baseline. The advantages may lie in the capability of generating outputs following code grammar, which mitigates the need for parsing outputs. The authors should justify the necessity of the code style and its underlying effects compared to the natural language style.\n2. The ablation studies of class order shuffling and guideline paraphrasing are missing."
                },
                "questions": {
                    "value": "1. How many samples are used to supervise finetuning LLaMA?\n2. The guidelines used in this paper are specifically the basic definitions of entity/relation/event types, which should have been provided to LLMs. However, the full annotation guidelines usually cover many edge cases and may contain tens of pages. Have the authors considered incorporating more fine-grained guidelines?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7332/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7332/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7332/Reviewer_bJoP"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7332/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698718049329,
            "cdate": 1698718049329,
            "tmdate": 1699636876671,
            "mdate": 1699636876671,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HtBy6xvurF",
                "forum": "Y3wpuxd7u9",
                "replyto": "4RvXjBIyqs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7332/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7332/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "The use of code (more precisely Python syntax) as an underlying format for the input and output was primarily motivated by previous research works [1, 2]. Prompting IE tasks requires some kind of structured format for the input and output. As you mention, we could also have prompted the model with more natural style language. However, we decided to use a format that is familiar for both the human and the model, in order to represent the input-output. Previous work has demonstrated that LLMs trained on code are capable of performing  IE [1, 2]. Furthermore, most LLMs fine-tuned for code generation, including Codellama, are fine-tuned on top of a general purpose LLM and retain most of its natural language understanding capabilities. Additionally, our method outperforms Instruct-UIE [3] on zero-shot, a model prompted with natural language.\n\nThat being said, using code-style format is not strictly our contribution; so we did not dig deeper on the matter. It is more of a convenience for easily parsing the input and output, than a decision that contributes to the model performance. We believe that, since fine-tuning is involved, LLMs will easily adapt to any input structure, that being code-style format, natural language instructions or any other alternative. While performing this evaluation would have great interest, we currently lack the human and computer resources for such experiments. \n\nRegarding the \u201cThis may result in the low performance of the baseline.\u201d, both the baseline and GoLLIE share the same conditions, except for the mentioned contributions. \n\nWith respect to the lack of ablation of guideline shuffling and paraphrasing, we have updated the paper to include both. \n\nRegarding your question about how many samples we have used to train the model, the models were trained for around 15k steps (3 epochs of our entire dataset with a batch size of 32). Note that for a single dataset a single sample might appear several times as we generate different inputs per task. We have updated the paper to include this information in Appendix D.2. \n\nRegarding your question about the full annotation guidelines, it is something that we definitely want to try. However those guidelines are very long, and considering the amount of labels we have per example, we found them intractable with our current models/infrastructure setup. With the recent efficient methods and models for larger sequences we believe we could soon explore the use of more detailed and longer guidelines in our experiments. However, it is important to note that in the paper we use some of the more basic guidelines to illustrate our method, since we consider that they were the easiest to understand. We have included a new figure (Figure 8, in Appendix E) that shows the guideline for the entity Point in the HarveyNer dataset. This guideline is very descriptive including edge cases and exceptions. All the guidelines we use will be made publicly available and we will add a link to them in the paper. \n\n\n[1] Xingyao Wang, Sha Li, and Heng Ji. 2023. Code4Struct: Code Generation for Few-Shot Event Structure Prediction. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3640\u20133663, Toronto, Canada. Association for Computational Linguistics.\n\n[2] Peng Li, Tianxiang Sun, Qiong Tang, Hang Yan, Yuanbin Wu, Xuanjing Huang, and Xipeng Qiu. 2023. CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 15339\u201315353, Toronto, Canada. Association for Computational Linguistics.\n\n[1] Wang, X., Zhou, W., Zu, C., Xia, H., Chen, T., Zhang, Y., ... & Du, C. (2023). InstructUIE: Multi-task Instruction Tuning for Unified Information Extraction. arXiv preprint arXiv:2304.08085."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7332/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700237555163,
                "cdate": 1700237555163,
                "tmdate": 1700237555163,
                "mdate": 1700237555163,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cwThssJWSb",
                "forum": "Y3wpuxd7u9",
                "replyto": "4RvXjBIyqs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7332/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7332/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Ablation updated"
                    },
                    "comment": {
                        "value": "In the last update of the paper we have added the missing ablation studies. As expected, the shuffle and paraphrase did not have a significant effect."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7332/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700562218987,
                "cdate": 1700562218987,
                "tmdate": 1700562218987,
                "mdate": 1700562218987,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1gg9Ia7iti",
                "forum": "Y3wpuxd7u9",
                "replyto": "cwThssJWSb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7332/Reviewer_bJoP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7332/Reviewer_bJoP"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your reply."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7332/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700705379367,
                "cdate": 1700705379367,
                "tmdate": 1700705379367,
                "mdate": 1700705379367,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xYwjxxfwLU",
            "forum": "Y3wpuxd7u9",
            "replyto": "Y3wpuxd7u9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7332/Reviewer_y8gw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7332/Reviewer_y8gw"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on zero-shot information extraction. Their goal is to train a large model that can generalize well to unseen domains and datasets. They main idea is using code as the intermediate layer for all the information extraction tasks and considering annotation guidelines or additional information during pre-training or testing. This additional information makes the model to understand tasks better and therefore be able to generalize. Experiments demonstrate the potential of the proposed method."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The performance is impressive.\n- The pre-trained model is valuable."
                },
                "weaknesses": {
                    "value": "- My main concern is as follows. Although the authors try to split the datasets for training and testing based on domains. I believe that there is still a large overlap among their output spaces. For example, RAMS, which is considered as one of the training tasks, contains role label *transporter*, *vehicle*, and *place*, while WikiEvents, which is consider as one of the testing tasks, also contains those role labels. In NER tasks, I believe the same situation happens as well. This makes the setting not *truly* zero-shot because the model has already learned those concepts. The model can get improvements just because including more *in-domain* supervised training signals.\n- I think the authors should be careful when referring the state-of-the-art models. In fact, in Table 2, the reported SOTA numbers and models are not SOTA anymore."
                },
                "questions": {
                    "value": "- See above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7332/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698809514827,
            "cdate": 1698809514827,
            "tmdate": 1699636876538,
            "mdate": 1699636876538,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oLPpaCiHlo",
                "forum": "Y3wpuxd7u9",
                "replyto": "xYwjxxfwLU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7332/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7332/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "About the zero-shot evaluation: In order to compare GoLLIE with previous work, we decided to use as zero-shot datasets the same datasets that have been used for this purpose in previous works, such as a InstructUIE [1], USM [2] or UniNER [3] among others. These methods use an even larger set of pre-training datasets for their models, so more entities in the zero-shot datasets overlap with the ones in the training dataset. Thus, even with less overlap,, GoLLIE outperforms all of them. It is also noteworthy that, while some datasets share similar entities, the annotation guidelines often vary between them. For example, ACE05  annotates pronouns as persons, while, CoNLL03 does not. In any case, we agree with your comment: as we also commented in the paper (first paragraph of Section 5.2), such overlap could make both the baseline and CoLLIE perform better in the evaluation. We believe that understanding the model performance with totally unseen labels is very important. Therefore, we have updated the paper to include an analysis regarding seen and unseen label evaluation in Section 5.2. This analysis is further extended in the Appendix B. As a short summary, the baseline model achieves significantly lower F1 scores on unseen labels than on seen labels. In contrast, GoLLIE demonstrates a more robust performance, showing a smaller gap in F1 scores between the seen and unseen labels.\n\nRegarding the SoTA models in Table 2 (supervised results), the main purpose of this paper is not to improve the supervised SoTA, but to focus on the zero-shot generalization. Our results on Table 2 are shown for completeness and to show that the use of guidelines does not hurt performance when compared to the baseline. The SoTa results reported there correspond to systems that share the most comparable settings to us. In order to be clear we have updated the text to make it more explicit. The final version will include results for the best SoTA models. \n\n[1] Wang, X., Zhou, W., Zu, C., Xia, H., Chen, T., Zhang, Y., ... & Du, C. (2023). InstructUIE: Multi-task Instruction Tuning for Unified Information Extraction. arXiv preprint arXiv:2304.08085.\n\n[2] Lou, J., Lu, Y., Dai, D., Jia, W., Lin, H., Han, X., ... & Wu, H. (2023). Universal Information Extraction as Unified Semantic Matching. arXiv preprint arXiv:2301.03282.\n\n[3] Zhou, W., Zhang, S., Gu, Y., Chen, M., & Poon, H. (2023). Universalner: Targeted distillation from large language models for open named entity recognition. arXiv preprint arXiv:2308.03279."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7332/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700237431886,
                "cdate": 1700237431886,
                "tmdate": 1700237431886,
                "mdate": 1700237431886,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6Mpq1kw7MQ",
                "forum": "Y3wpuxd7u9",
                "replyto": "oLPpaCiHlo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7332/Reviewer_y8gw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7332/Reviewer_y8gw"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your response."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7332/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700376631715,
                "cdate": 1700376631715,
                "tmdate": 1700376631715,
                "mdate": 1700376631715,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]