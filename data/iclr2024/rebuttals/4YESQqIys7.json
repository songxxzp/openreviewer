[
    {
        "title": "NfgTransformer: Equivariant Representation Learning for Normal-form Games"
    },
    {
        "review": {
            "id": "YgiSxoMBkR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1759/Reviewer_6Avg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1759/Reviewer_6Avg"
            ],
            "forum": "4YESQqIys7",
            "replyto": "4YESQqIys7",
            "content": {
                "summary": {
                    "value": "In this paper, the authors aim to learn the representations of each action of every player in normal-form games. They propose a transformer-based architecture called NfgTransformer, which takes the payoffs and zero-initialized action representations as input and outputs the action embeddings. These output embeddings can be used for many downstream tasks.\n\nIn experiments, authors verify the effectiveness of NfgTransformer in Nash equilibrium solving under full-observed payoffs, and in payoff prediction when some payoffs are unobserved. Furthermore, they also conduct case studies of attention weights in small bimatrix games to demonstrate the interpretability of NfgTransformer."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. The paper introduces a novel task: learning game representations. The acquired embeddings have the potential to enhance numerous downstream applications, making them particularly valuable in empirical game theory.\n2. The attention-based architecture offers a degree of interpretability, and I found the case studies presented in Figure 5 to be particularly insightful.\n3. NfgTransformer exhibits scalability as its parameters remain independent of the number of actions"
                },
                "weaknesses": {
                    "value": "The presentation can be further improved:\n\n(a) The description of the three attention mechanisms in Section 4.2 is too brief. I believe it would be beneficial to introduce more mathematical expressions.\n\n(b) Also, in Section 4.2, I find the name of the first attention mechanism to be misleading. I would suggest using \"joint-action-to-player self-attention\" rather than the original \"player-to-coplayer self-attention.\"\n\n(c) I'm having difficulty discerning the results presented in Figure 3. Perhaps using a table would be clearer than the loss curves."
                },
                "questions": {
                    "value": "1. At the end of Section 4.4, you mentioned, \"During training, invalid joint-actions are simply masked out from the set of values...\" How are these invalid joint-actions masked out? Is this achieved by reducing the corresponding pre-softmax scores significantly?\n2. In Section 5.2, how are the instances of DISC games generated? How do you train your model?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1759/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1759/Reviewer_6Avg",
                        "ICLR.cc/2024/Conference/Submission1759/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1759/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697456482702,
            "cdate": 1697456482702,
            "tmdate": 1700532038658,
            "mdate": 1700532038658,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ubDds5YOdL",
                "forum": "4YESQqIys7",
                "replyto": "YgiSxoMBkR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1759/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1759/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for recognising the significance and innovation presented in our work. We hope our significant revision to the main text and figures following several of your suggestions will further improve clarity. \n\n**Q1: clarity of presentation in the method section as well as Figure 3**\nWe fully agree with your assessment and we took on board several of your suggestions in our revision (see updated PDF, with major changes highlighted in blue). We have fully reworked Figure 1 and Figure 2 as well. \n\nSpecifically, \n* We introduced precise mathematical notations where possible in describing our method (in Figures and in text). \n* We renamed \u201cplayer-to-coplayer self-attention\u201d to \u201caction-to-joint-action self-attention\u201d. This is slightly different from your suggestion but it reflects the fact that in the self-attention module, queries are the action of each player, and the key-values attended over are the joint-action (and their respective payoffs). \n\nRegarding Figure 3, we tried to provide thorough ablation results here as they differ based on the nature of the tasks, the game sizes and the model hyperparameters. We are actively trying out other ways of presenting these results and perhaps a table would indeed work best. We could still provide the learning curves (showing the rate of convergence) in an appendix section for interested readers. We will update with revision when ready. \n\n**Q2: How are these invalid joint-actions masked out?**\nWe unfortunately glossed over this detail in our initial submission. We have now included the precise implementation of this operation in Equation 4. \n\nIndeed, the implementation amounts to enforcing invalid combinations of query-key-values to have negative infinity in their pre-softmax logits, effectively removing their contribution in the outputs. \n\n**Q3: how are the DISC games sampled? And how is the model trained?**\n\nWe provided technical details in how the DISC games are generated in Appendix B.2. but we failed to signpost it in the main text clearly. We do so in our revision now. \n\nIn short, we sample latent vectors (with different dimension Z) underlying a DISC game from a standard normal distribution and add to them a noise vector sampled from a uniform distribution. \n\nFor each sampled instance of the DISC game, we additionally sample a mask (such that each joint-action is kept with probability $p$) and we pass both the sampled game payoff tensor and the sampled mask to the NfgTransformer model, which returns a set of action embeddings, without observing the outcomes of invalid joint-actions (we ensure this via Equation 4). We then decode from action embeddings predicted payoff values for every joint-action and simply minimize the reconstruction loss given the ground truth full-payoff (as described in Figure 2 (Right)). The loss is minimised end-to-end via gradient descent. \n\nWe have added a description of the above training procedure in Appendix B to clarify our settings for future readers."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1759/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700176537023,
                "cdate": 1700176537023,
                "tmdate": 1700176537023,
                "mdate": 1700176537023,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Cpg3NhFvuv",
                "forum": "4YESQqIys7",
                "replyto": "ubDds5YOdL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1759/Reviewer_6Avg"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1759/Reviewer_6Avg"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response and the significant revision. The presentation is now clearer than before, and the clarifications have addressed my questions. I will maintain my score.\n\nRegarding the paper's presentation, I have some further personal suggestions:\n1. It seems that the Primary Area could be 'unsupervised, self-supervised, semi-supervised, and supervised representation learning' or 'algorithmic game theory' to better characterize your paper.\n2. The key contribution of your paper should be learning the game representation in an **explicit** way. This is because, in previous works on learning equilibrium solvers using neural networks (Feng et al., 2021; Marris et al., 2022; Duan et al., 2023a;b), they already obtain game representations **implicitly**. The intermediate result of their architecture before the last layer can be seen as game representations.\n3. Considering that previous works have already learned game representations implicitly, you can describe the advantages of learning game representations in a more convincing way. Afterward, you can delve into a more detailed explanation of why we need to learn the representations explicitly."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1759/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700451853374,
                "cdate": 1700451853374,
                "tmdate": 1700451853374,
                "mdate": 1700451853374,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fgdIzmkhEv",
                "forum": "4YESQqIys7",
                "replyto": "YgiSxoMBkR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1759/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1759/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your quick response and for reconfirming your positive assessment of our work. A few quick responses below: \n\n**Q1. choice of primary area**\n\nWe have indeed hesitated between the two choices. As we study the representation learning for a *novel* data modality previously under discussed, we tried to be non-committal in our choice. The \"representation learning\" category indeed seems more appropriate. \"Algorithmic game theory\" perhaps hints at solving specific game theory tasks which is a secondary effect of our paper not a primary goal. \n\nWe can't seem to make the change now to \"representation learning\", but we will follow up with ICLR to make the switch if possible if/when the paper gets accepted. \n\n**Q2/Q3: explicit vs. implicit representation learning**\n\nThis is an excellent remark and we will rephrase this comparison in our introductory paragraphs to make this comparison stand out more. \n\n*A few quick remarks on this point:*\n\nIt is true that prior works *implicitly* relied on representation learning, however, they are either lossy (e.g. Feng et al., 2021; Marris et al., 2022) due to the use of order-invariant pooling functions (see Marris et al., 2022 Appendix C.1 for a list of candidate pooling functions) or inefficient (e.g. Duan et al., 2023a;b), using MLPs which do not exploit the permutation equivariance properties of NFGs. \n\nIn fact, for the specialist pooling-based architectures, one could simply handcraft pooling functions *for some tasks* such that no representation *learning* is needed at all. For `max-deviation-gain` prediction, one could imagine designing pooling function that directly extract max-deviation-gain from the payoff tensor and expose them in the penultimate layer of the network (indeed, without such customisation, NES seemed to struggle in the `max-deviation-gain` prediction task more than it struggled in the NE-solving task). This would not in our view qualify as representation learning. However, the boundary is blurred across tasks: for equilibrium-solving, design of such pooling functions wouldn't be possible and representation learning would be implicit indeed as noted in your comment."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1759/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700469811796,
                "cdate": 1700469811796,
                "tmdate": 1700469968723,
                "mdate": 1700469968723,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dwf9FRjsPq",
            "forum": "4YESQqIys7",
            "replyto": "4YESQqIys7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1759/Reviewer_s4as"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1759/Reviewer_s4as"
            ],
            "content": {
                "summary": {
                    "value": "This paper considers the permutation equivariance in normal-form games (NFGs) in terms of player action and players. The authors claim that there is inherent equivariance in NFGs \u2013 \u201cany permutation of strategies describes an equivalent game.\u201d Then they design an NfgTransformer to leverage this equivariance. They conduct comprehensive experiments show their method significantly outperformes strong baselines in a range of game-theoretic applications."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper proposes to discover the \"inherent symmetry\" in game theoretical problems using equivariant neural networks. This idea is impressive, timely, and promising.\n\nThe authors design an NfgTransformer to meet their aim. I find using transformer reasonable.\n\nThey also conducted experiments covering a wide range of cases."
                },
                "weaknesses": {
                    "value": "My concerns are as follows:\n\n- I am worried about the claim that permutation equivariance is \"inherent\" in normal-form games either in terms of action or player. This is not obvious to me. Please prove this. Also, such permutation equivariance is a quite strong assumption, and leads to a significantly restrictive application domain.\n\n- It is not clear to what extent the proposed NfgTransformer is different from a vanilla one which has naturally been permutation equivariant. The authors did not clearly present the architecture of NfgTransformer - I could not validate the novelty in terms of model design.\n\n- The authors did not give any theoretical guarantee - whether the proposed method secure the desired permutation equivariance, whether the method can reach the desired Nash equilibrium, how fast would the algorithm converge, whether the generalisability of the proposed method (as a learning algorithm) is satisfiable, whether the proposed method has any theoretical advances, etc. Before answering this question, I was quite hesitant about the proposed algorithm.\n\n- I appreciate the empirical results. However, game theory has a very wide range of applications - I do not think experiments in a few applications could justify an algorithm which is claimed to have advances in general cases."
                },
                "questions": {
                    "value": "Please address the above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1759/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1759/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1759/Reviewer_s4as"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1759/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699278933501,
            "cdate": 1699278933501,
            "tmdate": 1700668438815,
            "mdate": 1700668438815,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "t6uhuDvUDd",
                "forum": "4YESQqIys7",
                "replyto": "dwf9FRjsPq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1759/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1759/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewers for pointing out potential sources of confusion in our presentation. We hope our revision to the main text would help clarify our arguments. \n\n**Q1: Is permutation equivariance inherent to normal-form games?**\n\nBy \u201cinherent equivariance of NFGs\u201d we mean that action or player permutations applied to the payoff tensor of an NFG do not fundamentally \u201cchange\u201d the game. Note that this general property of NFGs has been observed and formally characterised in [1-2]. \n\nThis is *not* an assumption that we make, or a property discovered in our work but a general observation that holds for all NFGs and this is why we can leverage this property in our design without loss of generality. \n\nAs a simple example, consider $G$, a payoff matrix that describes a prisoner\u2019s dilemma. I could permute the two actions for the row-player and obtain $G\u2019$, which is \u201cstrategically equivalent\u201d or strongly isomorphic to $G$. \n\n| $G$   \t| $a^1_2$  \t| $a^2_2$  \t|\n|:--:|:--:|:--:|\n| $a^1_1$ \t| (+1, +1) \t| (-1, +2) \t|\n| $a^2_1$ \t| (+2, -1) \t| (-1, -1) \t|\n\n| $G\u2019$   \t| $a\u2019^1_2$  \t| $a\u2019^2_2$  \t|\n|:--:|:--:|:--:|\n| $a\u2019^1_1$ \t| (+2, -1) \t| (-1, -1) \t|\n| $a\u2019^2_1$ \t| (+1, +1) \t| (-1, +2) \t|\n\nIf I were to encode G1 and G2 using NfgTransformer, I would expect the following identity in the action embeddings: $\\mathbf{a\u2019}^2_1 = \\mathbf{a}^1_1$. If I were to use a MLP network then I would not have this guarantee. \n\n**Q2: how does NfgTransformer differ from vanilla transformer?**\n\nWe have significantly reworked our method description as well as Figure 1 and Figure 2 that describe the model architecture in specifics. The only commonality between our proposed model and vanilla transformer is that both architectures use self-/cross-attention operations as fundamental building blocks. The NfgTransformer is otherwise distinct from vanilla Transformer in how these components are applied and used. \n\nWe hope our revision to the main text and figures could help verify the novelty of our model design. \n\n**Q3: Is permutation equivariance guaranteed?**\n\nThe permutation equivariance property is guaranteed by construction. In our revision, we have slightly expanded on the background section on the attention mechanism which is equivariant with respect to queries (and order-invariant with respect to key-values). In the revised Figure 1 (Bottom), we make clear what are the queries and key-values to each attention operation, and show how permutation equivariance is guaranteed by careful design. \n\nEmpirically, our interpretability results (Figure 5, action embedding visualisation) also confirm that Proposition 3.2 holds, which is a property for any permutation equivariant encoding function (which we proved in Appendix A).\n\n**Q4: Are there any theoretical guarantees to the proposed architecture?**\n\nWe do not have any bounds on the rate of convergence of our model as it\u2019s based on deep learning neural networks architecture and such guarantees are difficult to prove in general. However, we do show significantly better empirical results than several comparable baseline methods, and our Figure 3 thoroughly ablates different model hyper-parameters, showing which parameters affect the rate of convergence in different tasks at each game size. \n\nIn terms of theoretical guarantees, we did prove (in Appendix A) the precise conditions under which one could expect identities in the action embeddings, for any deterministic and equivariant embedding function. NfgTransformer is deterministic and equivariant and these identities must hold. Two straightforward corollaries of this theorem are stated in Section 3. \n\n**Q5: are the empirical results sufficient in making the generality claim?**\n\nWe think the best way to demonstrate the generality of any representation learning technique is to demonstrate that the same representation learning approach can enable strong results in a number of downstream tasks. In domains such as computer vision one would be expected to provide results on reconstruction, classification among other benchmark tasks. \n\nHere, we showed strong empirical results in key tasks such as equilibrium-solving, deviation gain estimation and payoff prediction. We chose these tasks because they operate at different decoding granularities (see our updated Figure 2), and cover many of the interesting use-cases of normal-form game theory. We believe these support our claim on the generality of our approach to representing NFGs. \n\n\n[1] J. C. C. McKinsey.11. ISOMORPHISM OF GAMES, AND STRATEGIC EQUIVALENCE, pp. 117\u2013130.   Princeton University Press, Princeton,  1951.\n\n[2] Joaquim Gabarr\u0301o, Alina Garc\u0131a, and Maria Serna. The complexity of game isomorphism. Theoreti-\ncal Computer Science, 412(48):6675\u20136695, 2011."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1759/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700176376869,
                "cdate": 1700176376869,
                "tmdate": 1700213504247,
                "mdate": 1700213504247,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ENCo0L4Ltn",
                "forum": "4YESQqIys7",
                "replyto": "rQaTZImg3f",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1759/Reviewer_s4as"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1759/Reviewer_s4as"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your response.\n\nMy first concern is cleared (about whether normal-form games are permutation equivariant). \n\nI still see weaknesses in the novelty of architecture design, theoretical analysis, and empirical verification.\n\nI now recognise the contribution in proposing to learn game representation, but the development/verification of the idea is not in the perfect form.\n\nI will increase my score to 5."
                    }
                },
                "number": 29,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1759/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700668423721,
                "cdate": 1700668423721,
                "tmdate": 1700668423721,
                "mdate": 1700668423721,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "SBHxhvZBD2",
            "forum": "4YESQqIys7",
            "replyto": "4YESQqIys7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1759/Reviewer_pter"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1759/Reviewer_pter"
            ],
            "content": {
                "summary": {
                    "value": "The submission proposes a transformer architecture for learning normal-form games. They evaluate this architecture on a variety of game-related tasks, including solving for Nash equilibria and predicting deviation payoffs."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "Regarding originality, I don't know of any paper that has done what the submission seems to be doing."
                },
                "weaknesses": {
                    "value": "### First sentence\n\nI'm going to start off by discussing the first sentence of the introduction:\n\n> A number of celebrated results in artificial intelligence (AI) have appealed to game-theoretic principles during training, evaluation and ranking (Silver et al., 2016; Lanctot et al., 2017; Vinyals et al., 2019; Omidshafiei et al., 2019; Liu et al., 2022b; Perolat et al., 2022).\n\nThe way this sentence is written leaves the reader with the impression that the citations are examples of celebrated results that have appealed to game-theoretic principles. Let's go through them and examine whether this claim is true. I agree that Silver et al. (2016) is a celebrated result. However, it is not clear to me that it uses any game-theoretic principles during training (it uses supervised learning + self-play) or evaluation/ranking (it uses elo rankings). Lanctot et al. (2017) is not a result at all -- it is a paper that advocates for combining double oracle and deep RL. Vinyals et al. (2019), Omidshafiei et al. (2019), Liu et al. (2022b), Perolat et al. (2022) all use some level of game-theoretic justification, but none live up to the very high standard implied by the term \"celebrated\". One also cannot help but notice that all six of the cited papers are DeepMind papers. While it is certainly the case that DeepMind has made important contributions to RL in games (and perhaps more so than any other group), citing only DeepMind papers here is completely unjustifiable. Arguably, the two most significant sets of successes that directly appeal to game-theoretic principles are those concerning poker (DeepStack/Libratus/Pluribus/ReBeL) and No-Press Diplomacy (Diplodocus). That the submission choses to cite DeepMind papers as opposed to more-relevant non-DeepMind papers to support their claim gives the impression that it is attempting to push a narrative rather than give an honest perspective (or, alternatively, that the submission is simply uninformed). One might argue here that I am being too pedantic about a somewhat boilerplate sentence in the introduction that is not closely related to the submission's contributions. But I think it is important that the submission not mislead the reader.\n\n---\n\n### Motivation of paper\n\nAfter reading the introduction, I am not quite sure I fully grasp the problem that the submission is trying to solve. The submission abstractly states \"We consider the problem of bringing game-theoretic reasoning to deep learning systems and, conversely, using deep learning techniques to solve challenges in game theory\" but does not immediately clarify why would would want bring game-theoretic reasoning to deep learning systems or what challenges the submission is going to solve in game theory. It further states \"In its most basic form, strategic interactions between players are formulated as NFGs where players simultaneously select actions and receive payoffs subject to the joint action. Strategic interactions are therefore presented as payoff tensors, where values to each player are tabulated under every joint action. This tabular view of strategic interactions presents its own challenges to representation learning.\" Under the presumption that doing representation learning on a normal form game is important, I agree there are some representation learning-related challenges, but the submission hasn't really explained why we would want to be doing this in the first place. The submission then lists some game-theory related questions \"Examples of such enquiries permeate different communities within game theory: given sets of actions, what would be\nan equilibrium strategy (Greenwald et al., 2003; Marris et al., 2022; Duan et al., 2023a;b)? How does the efficiency of the system degrade due to individual selfishness (Koutsoupias & Papadimitriou,1999)? How might we cluster actions considering transitivity and strategy cycles (Czarnecki et al., 2020)? Given outcomes for some joint actions, can one predict payoffs for the others (Balduzzi et al., 2018; Bertrand et al., 2023; Vadori & Savani, 2023)? To what extent can we reduce the dimensionality of a class of NFGs (Marris et al., 2023)?\" This gives the reader the impression that these are some of the questions the submission is interested in. But the big question of *Why we need to learn a representation of the game to do this* remains unaddressed -- it is unclear to the why problems such as \"what would be an equilibrium strategy\" ought not to be addressed from a normal-form representation. The submission proceeds to list desiderata for a unified representation for solving these tasks.\n\n---\n\n### Description of methodology\n\nThe submission refers the reader to Figure 1 to understand the architecture that they're proposing, but I find the figure not very easy to grok. What does it mean when two arrows point at the same block?\n\nThe submission describes the architecture in Section 4, but I could not figure out what the architecture was being trained to do. I also still do not understand what the downstream use case of this architecture is.\n\n---\n\n### Experiments\n\nIn Section 5.1.1, the submission says this \"For equilibrium solving, we optimise variants of the NfgTransformer to minimise the NE GAP\n() = maxp p() directly.\" How is the submission performing this optimization? How are we recovering a NE strategy from the action embeddings? These questions are central to the paper, but seem to be unanswered in the text. Furthermore, why should we want to solve NFGs with this architecture instead of using a classical method? Without discussing this question, the reader is left wondering why they should care about these experiments.\n\nIn Section 5.1.2, the submission says this \"We optimise a NfgTransformer network to regress towards the maximum deviation-gain () for every joint pure-strategy (deterministic) , using a per joint-action decoder architecture (Figure 2).\" Again, it is totally not obvious how the submission is doing this based on the text in the paper. Furthermore, the submission again has no discussion about why we should want to use an architecture instead of a classical method for this task (which in this case just amounts to matrix multiplications).\n\nSection 5.2 studies a kind of payoff table completion task. Here it seems reasonable that one might actually want to use an architecture of the kind the submission describes. However, the submission still neglects to disclose how the the architecture was trained."
                },
                "questions": {
                    "value": "What's a regular isomorphism as opposed to strong isomorphism in the context of games?\n\n---\n\nOverall, I think the submission needs to be re-written to make it more clear what problem it is trying to solve and how it is training its architecture."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1759/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1759/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1759/Reviewer_pter"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1759/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699312500948,
            "cdate": 1699312500948,
            "tmdate": 1700808930805,
            "mdate": 1700808930805,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oW1QApNiup",
                "forum": "4YESQqIys7",
                "replyto": "SBHxhvZBD2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1759/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1759/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q1: choice of citations in the introduction and motivations behind the work.**\n\nYou are right (and not being pedantic) to point out the issues with the initial motivations and citations used. We regret the boilerplate sentences used and have reworked the entire introduction to have a clearer motivation and have updated citations to not omit important work in the field.\n\nWe hope our revision and more relevant citations would help clarify the motivation behind our work. \n\n**Q2: Lack of clarity of the method description.**\n\nWe took on board these constructive criticisms and have revised Figure 1 and Figure 2 entirely, with precise notations throughout. We have also updated our method description in text accordingly, preferring precise notations where possible. In particular, the three downstream applications that we studied are now clearly described, with specific loss functions shown in Figure 2 and in text. \n\nWe hope our revision brings clarity to our method descriptions.  \n\n**Q3: The \u201chow\u201d and \u201cwhy\u201d behind the experiments.**\n\nWe hope our earlier responses and our revision to the main paper provides sufficient clarity now. All three empirical experiments are now described more precisely in Figure 2. We train these models end-to-end via gradient descent, minimising the loss functions described in Figure 2 (Top) and in text. Equation 4 now describes precisely how masking is implemented in our experiment. \n\n**Q4: regular isomorphism vs strong isomorphism for NFGs.**\n\n[1] offers more precision definition of isomorphic games and strongly isomorphic NFGs. In short, isomorphic games would preserve the \u201cpreference ranking\u201d of actions when players and actions are permuted. Strongly isomorphic games preserve the exact payoff value before and after the permutation.\n\nFor example, rock-paper-scissors and paper-scissors-rock are strongly isomorphic because I can permute the former to recover the exact payoff matrix of the latter (i.e. payoff values match, not just in the preference ranking of the actions). \n\nWe provide additional details in Appendix A, Figure 6 on the notion of strong isomorphism.\n\n\n[1] Joaquim Gabarr\u0301o, Alina Garc\u0131a, and Maria Serna. The complexity of game isomorphism. Theoreti-\ncal Computer Science, 412(48):6675\u20136695, 2011."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1759/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700175852995,
                "cdate": 1700175852995,
                "tmdate": 1700503033734,
                "mdate": 1700503033734,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZFQKzXLYpi",
                "forum": "4YESQqIys7",
                "replyto": "oW1QApNiup",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1759/Reviewer_pter"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1759/Reviewer_pter"
                ],
                "content": {
                    "title": {
                        "value": "Message to Authors"
                    },
                    "comment": {
                        "value": "Thanks so much for your response and revision! I'm sorry my response has been so delayed. It is regrettable that the discussion period is so short. I will be sure to respond in detail in the next day or two."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1759/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700500015364,
                "cdate": 1700500015364,
                "tmdate": 1700500015364,
                "mdate": 1700500015364,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PZq6OFIPzT",
                "forum": "4YESQqIys7",
                "replyto": "rpgypKiuTm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1759/Reviewer_pter"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1759/Reviewer_pter"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Authors"
                    },
                    "comment": {
                        "value": "I read through the revised version of the text. I appreciate the changes to the introduction and the more detailed description of the training process. One thing I feel like I'm still not quite understanding is the motivation of the problem. In a previous version of their response, the authors stated:\n\n> In short, NfgTransformer is preferable to classical solvers as they can be used as another neural network component within a deep learning system that\u2019s fully differentiable, can be parallelised and takes deterministic time. This property can unlock lines of research that could benefit from baking in equilibrium concepts in the agent learning-rules [1-3]. For ranking, as suggested by the reviewer, NfgTransformer provides a much richer space of representation for ranking players and actions compared to Elo, a benefit we demonstrate empirically in our results.\n\nCan the authors elaborate on this? What are agent learning rules [1-3]?\n\nIn the submission, the authors make an analogy to RGB pixels. However, it's not clear to me that this analogy holds up well. If I'm asked to solve a zero-sum normal-form game, I can run regret matching (among other classical algorithms) and get a good approximation in a relatively short amount of time. On the other hand, if you ask me to segment an image, there's really nothing else I can do besides deep learning.\n\nI agree that the elo task is compelling compared to the others. Maybe this should play a more central role in the motivation?"
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1759/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700591646144,
                "cdate": 1700591646144,
                "tmdate": 1700591646144,
                "mdate": 1700591646144,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WvjyrzDxnB",
                "forum": "4YESQqIys7",
                "replyto": "SBHxhvZBD2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1759/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1759/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Another quick comment regarding our analogy to RGB pixels and the fact that one could solve for NE to reasonable approximation in zero-sum NFGs: \n\nIn our experiments, we showed empirical results on solving for NE in 2 and **3-player** **general**-sum NFGs. The latter being more difficult but we could nonetheless approximate an NE solution to acceptable accuracy with our larger models. We can trivially extend our model to work with 4 player games too perhaps with further architectural innovations. Using classical solvers, with each generalisation of the game class (2-player 0-sum --> 2 player general-sum ---> 3 player zero-sum ...) we might need specialised tools and innovations, we think our results provide another promising approach to tackle such approximate equilibrium solving problems in a rather general fashion. \n\nNN-based solvers also have other practical benefits such as their differentiability and ease to parallelise taking advantage of accelerators to solve a large number of games in short amount of time. \n\nWe think our analogy to RGB remains apt: before deep representation learning techniques revolutionised computer vision, many representation techniques have been developed too, even for tasks like segmentation or classification (e.g. Laplacian Pyramid, wavelet transform). Granted they do not achieve similar levels of performance as we can do today. Similarly, **a)** no off-the-shelf classical NE solver can solve thousands or millions of 3-player general-sum NFGs to reasonable accuracy in seconds; and **b)** we show better results on equilibrium solving than NES which relied on handcrafted feature vectors (not dissimilar to how learned representation surpassed the performance offered by laplacian pyramid/wavelet transform). \n\nOur work is only the first (principled) representation learning technique for NFGs too and our hope is that even better performance will follow with further architectural innovation as is the tradition of this conference."
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1759/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700605315467,
                "cdate": 1700605315467,
                "tmdate": 1700605742754,
                "mdate": 1700605742754,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hpbUBH9ADP",
                "forum": "4YESQqIys7",
                "replyto": "LSsjpiIBGg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1759/Reviewer_pter"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1759/Reviewer_pter"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "> In our initial response we referred to [1-3] which is a line of earlier works in tabular RL learning literature where at each policy update, the agent internally constructs an NFG and solves for its equilibrium (Nash or Correlated depending on the algorithm). These NFGs correspond to Q-tables learned by the RL agents (i.e. if I were to play one action and you were to play another action, what would be our expected payoffs)? The RL policy update step then rely on the equilibria solved from such NFGs. One could note that this line of work stopped at 2003 yet there has been major advances in deep RL over the past two decades. We believe a major reason for the lack of follow-up of similar ideas, and we alluded to it in the revised introduction using NES [4] as an example, is that it is infeasible to bake in such equilibrium-solving procedure in deep RL agents' update rules --- the deep (RL) learning paradigm very much rely on the loss function being end-to-end differentiable, yet classical equilibrium solvers (or a regret matching procedure as suggested) are not end-to-end differentiable therefore one cannot use these subroutines in deep RL agents. This problem is made even worse as off-the-shelf classical solvers can fail, converge in non-deterministic time, and do not parallelise well --- in stark contrast to deep RL agents's gradient-based updates that finish in deterministic time, run on accelerator hardware with massive parallelism and must not fail. We believe our work on NfgTransformer offers a principled approach that can revive similar ideas in the age of deep (RL) learning.\n\nThis motivation seems reasonable. Is there a reason it was not communicated in the submissions itself?\n\n> As you can see these tabular RL examples [1-3] are relevant but perhaps a bit more difficult to follow for an introductory paragraph. We have therefore decided to move them to the Related Works section with a brief description and used NES as an easier-to-understand example for the introduction.\n\nI see, I guess this is why.\n\n---\n\nI re-read the introduction again and I think it could be improved. As is, the introduction goes (roughly):\n1. Representation learning is important\n2. Hasn't made impact in game theory\n3. Payoff tables are like pixels\n4. We do something more general than existing works\n5. Equivariance in normal-form games\n6. Equivariance desiderata\n7. We propose ...\n\nI feel like this is too abstract/meandering and doesn't directly communicate the most important thing for the introduction to communicate: the motivation for the paper. As far as revisions, to make space for this, I think (1-4) aren't that important and could be shortened or removed to make space for more direct motivations (in particular, I think that the elo experiments and the Nash Q-learning aspiration are the submission's strongest motivators).\n\n---\n\n Will the authors commit to releasing documented code if the submission is accepted? I think that would be helpful."
                    }
                },
                "number": 26,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1759/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700611657146,
                "cdate": 1700611657146,
                "tmdate": 1700611657146,
                "mdate": 1700611657146,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "o3Jq3gcWdK",
                "forum": "4YESQqIys7",
                "replyto": "fr93W0psNx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1759/Reviewer_pter"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1759/Reviewer_pter"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "> Given the upcoming deadline, we are wondering if our revision, which has significantly improved clarity and motivation compared to our initial submission, have (in full or at least in part) addressed your main concerns and if you would be willing to reconsider your initial assessment of our work?\n\nYes, the discussion period has improved my appreciation of the value of the submission. I will update my evaluation at the end of the discussion period. \n\nRe: 1-3, I think it's fine to include some version of the content in the submission. But I think it's a problem that a reader whose expertise is as close as mine to the subject matter of the submission could read through the introduction without actually understanding what specific problems the submission is trying to address."
                    }
                },
                "number": 31,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1759/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700676863164,
                "cdate": 1700676863164,
                "tmdate": 1700676863164,
                "mdate": 1700676863164,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LX6Fph9tBa",
                "forum": "4YESQqIys7",
                "replyto": "Ad6DwgPJZ0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1759/Reviewer_pter"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1759/Reviewer_pter"
                ],
                "content": {
                    "title": {
                        "value": "Response to Revision"
                    },
                    "comment": {
                        "value": "I like the revision better, but I think still more could be done to clarify the motivation of the submission. As is, the reader is supposed to infer the Elo application from this sentence: \"Vadori & Savani (2023) augmented this representation with learned features, enriching the player representation for much improved prediction.\" I feel like this could really be fleshed out into three or more sentences. I don't think I would be able to understand exactly what is being said here until after reading the Elo experiments. Also, the way the sentence is written, it gives the reader the impression that the problem has already been \"solved\" by Vadori & Savani (2023), rather than that it is a main motivator of the paper."
                    }
                },
                "number": 32,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1759/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700677466379,
                "cdate": 1700677466379,
                "tmdate": 1700677466379,
                "mdate": 1700677466379,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "P8yiz0ekdl",
            "forum": "4YESQqIys7",
            "replyto": "4YESQqIys7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1759/Reviewer_6Yok"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1759/Reviewer_6Yok"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes an action-payoff encoder architecture for normal-form games (NFGs), which can be used to predict the payoffs, and maximal deviation of a player. The main contribution of this work is NfgTransformer, which is an action-payoff encoder that leverages the permutation-invariance of transformer architectures. The proposed architecture reportedly outperforms baselines in various game-theoretic toy examples."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "# Strengths\n- The empirical results of the proposed architecture outperforms the baselines in several toy benchmarks."
                },
                "weaknesses": {
                    "value": "# Weakness\n- [Presentation]\n    - It is hard to parse the main goal of this work at the first glance; the main contribution of this work is a game encoder architecture that exhibits a baked-in equivariance (which is a direct consequence of transformer architecture)\n    - I think it would have been much better if the authors emphasized why we need such a game encoder, and why it is important; for example, the authors state \u201cFor practical applications such as ranking in Go and Chess, it is infeasible to evaluate all pairs of players yet we may wish to make predictions about the game based on incomplete information from a subset of the matchups.\u201d In Sec. 1 \u2014 more concrete examples would help the readers to understand the motivation of this work. \n    - In Sec. 1, the authors vaguely describe the main contribution of this work as: \n        - \u201c\u2026 we consider the problem of bringing game-theoretic reasoning to deep learning systems and, conversely, using deep learning techniques to solve challenges in game theory.\u201d \n        - \u201c\u2026 NFGs are canonical descriptions of strategic interaction between players that allows one to ask a variety of questions.\u201d\n        - But none of the descriptions clearly states that the actual contribution lies in an encoder architecture that encodes action & payoffs.\n    - Hard to understand the illustration of the proposed architecture in Fig. 1 and Fig.2\n        - The illustration and caption are not self-contained; readers need to resort to Sec. 4 to actually grasp the implementation \n        - What is the difference between light colors and bold colors in Fig.1 ?\n        - It is hard to parse the meaning of the annotations attached to the tokens in Fig. 1; for example, in \u201cPayoffs\u201d of Fig. 1, different lengths are identically annotated as \u201cT\u201d \n    - Some of internal links are broken\n        - Pointers to Fig. 5\n        - Pointers to Proposition 3.2\n        - Pointer to Sec. 5.1.\n        - Pointers to Sec 4.4\n        - Some links to the references\n- [Technical Novelty]\n    - If I understood correctly, it seems like the equivariance of NfgTransformer is a direct consequence of transformer architecture, and therefore, hard to consider it as a novel technical contribution.\n- [Experiment]\n    - The games presented in the experiments are rather toy-ish \u2014 given that the main contribution of this work lies in an empirical architecture, I think the efficiency of the proposed architecture should be validated on a real-world games, e.g., Go and Chess, which the authors listed as possible practical applications in Sec. 1."
                },
                "questions": {
                    "value": "From my understanding, the \"equivariance\" property of NfgTransformer architecture is a direct consequence of transformer architecture itself -- which is OK, but hard to be considered as a significant technical contribution; it would be appreciated if the authors could clarify this point."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1759/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699633067782,
            "cdate": 1699633067782,
            "tmdate": 1699636104779,
            "mdate": 1699636104779,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XSgn3Bl0ik",
                "forum": "4YESQqIys7",
                "replyto": "P8yiz0ekdl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1759/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1759/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their constructive feedback and we preface our responses by stating that our initial submission has not been clear which could have added to the confusion in reviewer\u2019s questions. We hope our revision will bring clarity to many of these questions. \n\n**Q1: permutation equivariance follows directly from the transformer architecture, please clarify your contribution.**\n\nWe define permutation equivariance to be: permutation in the strategies and players of the input, results in identical permutation of the output (with *no* change to network parameters). This definition is made precise in our background section.\n\nTraditional language transformers do NOT have this property: they are causal and position aware, with appropriate masking and position embeddings. The representation after seeing the first token \u201cdog\u201d in \u201cdog bites man\u201d should be very different from the representation after seeing the last token \u201cdog\u201d in \u201cman bites dog\u201d even though we simply permuted \u201cdog\u201d and \u201cman\u201d in the input.\n\nThe word \"transformer\" is used in NfgTransformer due to the use of self-attention and cross-attention operations, which have been popularised by the Transformer architecture. The attention operation treats key-value inputs as an unordered set; they are additionally equivariant with respect to the queries. These are the properties NfgTransformer leverages in its architecture. We now call them out explicitly in the background section on Multi-headed Attention in our revision.\n\nIn short, the equivariance property of NfgTransformer is through careful design of our novel architecture proposal that is catered to the structure of NFGs, not a direct result of existing transformer architecture.\n\n**Q2: the contribution and motivation of the method should be clarified.**\n\nWe agree with the reviewer that our initial introduction paragraphs were not clear and we have significantly revised our introduction paragraphs to better reflect our motivation. \n\n**Q3: presentation of the method is not clear (e.g. Fig1 and Fig2).**\n\nWe recognise these issues as a major source of confusion and have revised these two figures significantly with precise notations. We have updated our description of the architecture in the text too. Please do let us know if there are other changes that would improve the presentation upon reviewing our changes. \n\n**Q4: broken links.**\nWe have verified that links are working as intended in our revised PDF. Please let us know if this is not the case. \n\n**Q5: evaluation domains are toy\u2019ish.**\nWe evaluate our model on synthetic games that cover the entire equilibrium-invariant space of NFGs in Figure 3. Games sampled from this space have the property that they cover all interesting strategic interactions that could change the equilibrium solution of the NFGs at a specific game size. This is consistent with earlier work such as NES [1]. For ranking, we studied DISC games which are consistent with prior works in the ranking literature [2-3]. Finally, our interpretability results are done in GAMUT, which is the standard set of NFGs that capture relevant strategic dynamics in the economics/game theory literature. \n\nWe believe our choice of test domains is principled and well-grounded in the literature targeting normal-form game theory. \n\nThe normal-form payoff matrix of Go and Chess are known to be highly transitive [4] and we would expect them to tend to have dominant actions (players) and therefore make for less comprehensive evaluation.\n\n[1] Marris, Luke, et al. \"Turbocharging solution concepts: Solving NEs, CEs and CCEs with neural equilibrium solvers.\" Advances in Neural Information Processing Systems 35 (2022): 5586-5600.\n\n[2] Bertrand, Quentin, Wojciech Marian Czarnecki, and Gauthier Gidel. \"On the Limitations of the Elo, Real-World Games are Transitive, not Additive.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2023.\n\n[3] Vadori, Nelson, and Rahul Savani. \"Ordinal Potential-based Player Rating.\" arXiv preprint arXiv:2306.05366 (2023).\n\n[4] Czarnecki, Wojciech M., et al. \"Real world games look like spinning tops.\" Advances in Neural Information Processing Systems 33 (2020): 17443-17454."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1759/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700175648572,
                "cdate": 1700175648572,
                "tmdate": 1700175912486,
                "mdate": 1700175912486,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]