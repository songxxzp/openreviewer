[
    {
        "title": "Only Pay for What Is Uncertain: Variance-Adaptive Thompson Sampling"
    },
    {
        "review": {
            "id": "y5P1mfpOye",
            "forum": "p8ujRTjEf3",
            "replyto": "p8ujRTjEf3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1076/Reviewer_oe9F"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1076/Reviewer_oe9F"
            ],
            "content": {
                "summary": {
                    "value": "The submission studied multi-armed bandits. The reward-generating process of each arm follows a different Gaussian-Gamma distribution. The submission proposed two Bayesian methods (Algorithms 1 and 2) and provided two prior-dependent regret bounds (Theorems 1 and 2). The proposed method (VarTS) is compared with seven existing methods in experiments."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The submission addresses the unknown reward variance. The regret is a finite-time analysis, and the bounds show how the reward variance affects the regret."
                },
                "weaknesses": {
                    "value": "(a) Assuming Gaussian bandits would be impractical when facing realistic applications. As a complement, the analysis should discuss the cost of mis-modeling and show how to control this additional cost to obtain a meaningful regret bound. (In contrast, the experiments indeed consider distributions other than Gaussian.)\n\n(b) The submission lacks a clarification that connects the novelty claimed in Contribution-(3) to the analysis in the appendix. This information would help evaluate and understand the technical contribution(s) of this submission.\n\n(c) The variance-dependent bounds (Theorems 1 and 2) would be better justified if we could see curves of the proposed methods that vary according to the change of the unknown parameters in experiments."
                },
                "questions": {
                    "value": "(d) The main paper categorizes the methods using Bayesian/frequentist approaches or the identical $\\sigma$/distinct $\\sigma_i$ settings. Could you please also reflect on these different baselines in the experiments, helping the reader to compare and contrast the differences between considering/not considering the prior information?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1076/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698027726736,
            "cdate": 1698027726736,
            "tmdate": 1699636034334,
            "mdate": 1699636034334,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XUThhKep5L",
                "forum": "p8ujRTjEf3",
                "replyto": "y5P1mfpOye",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1076/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1076/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal for Reviewer oe9F"
                    },
                    "comment": {
                        "value": "Thank you for the detailed review. Our responses to your pointed-out weaknesses are below. We are happy to discuss any additional concerns that you may have.\n\n**W1: Theory beyond Gaussian bandits**\n\nA great question. One paper that analyzes the error of using Gaussian posteriors for other distributions is [Liu et al. (2022)](https://arxiv.org/pdf/2201.01902.pdf). Their results are quite pessimistic and lead to an additional $\\varepsilon n$ term, where $\\varepsilon$ is a measure of model misspecification. To make this term $O(\\sqrt{n})$, an $O(1 / \\sqrt{n})$ model misspecification would be needed. Since this is not practical, we opted for an empirical validation in Bernoulli and beta bandits.\n\n**W2: Novelty**\n\nThe novelty is in analyzing a variance-adaptive algorithm. We derive the first Bayes regret bound for such an algorithm. The bound captures the benefit of informative priors and is finite time. None of the prior bounds have either of these properties. This is discussed in the last paragraph of Section 5.2. We discuss the technical novelty in Q3 in **Common response**.\n\n**W3: Plot the variance-dependent bound to validate it**\n\n***In Appendix D of the updated paper, we plot the regret of VarTS, the regret bound in Theorem 2, and also frequentist regret bounds.*** We observe three trends:\n\n* The regret of VarTS decreases as the prior becomes more informative.\n* The regret bound of VarTS decreases as the prior becomes more informative.\n* Our regret bound becomes tighter than the frequentist bounds as the prior becomes more informative. This is because it depends on how informative the prior is. The expectation of the frequentist bounds does not depend on how informative the prior is.\n\n**Q1: Categorization of the baselines**\n\nWe tried to cover all combinations of variance adaptive / not adaptive and Bayesian / frequentist bandit algorithms:\n\n* Adapts to unknown variance: UCB1-Tuned, UCB-V, TS20, TS14\n* Does not adapt to unknown variance: UCB1, Bernoulli TS, Gaussian TS\n* Bayesian algorithm: Bernoulli TS, Gaussian TS, TS20, TS14\n* Frequentist algorithm: UCB1, UCB1-Tuned, UCB-V"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1076/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700299679228,
                "cdate": 1700299679228,
                "tmdate": 1700299679228,
                "mdate": 1700299679228,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zxqexc0FlL",
                "forum": "p8ujRTjEf3",
                "replyto": "y5P1mfpOye",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1076/Reviewer_oe9F"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1076/Reviewer_oe9F"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your reply."
                    },
                    "comment": {
                        "value": "On W1: Based on the feedback, maybe adding a discussion similar to the following provides a general picture of the bandit problem. When one cannot control the misspecification, a frequentist approach is preferred, as an $\\varepsilon n$ is a huge cost to pay. On the other hand, if one is confident with the underlying distribution (to be Gaussian), the Bayesian approach proposed in this submission would be the preferred one.\n\nOn W2: Thank you for the feedback in Common Response. But I am looking forward to the discussion between you and reviewer Lfwz. (Although Lfwz decided to fix his/her score, there is no restriction to continue to defend your work.)"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1076/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700533553774,
                "cdate": 1700533553774,
                "tmdate": 1700533591114,
                "mdate": 1700533591114,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MIho9Exwcm",
                "forum": "p8ujRTjEf3",
                "replyto": "y5P1mfpOye",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1076/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1076/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the follow-up comments.\n\n**Re. W1:** We totally agree with you on W1 and we will add this discussion as a remark in the updated version of our paper. \n\n**Re. W2:** We have further elaborated on our technical novelties and challenges addressed in the proof of Theorem 2 in Q3 in **Common response**, and also responded to Reviewer Lfwz on the follow-up concerns. Please go over them. \n\nThanks again for your suggestions. Please let us know if we can clarify anything further."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1076/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700556542994,
                "cdate": 1700556542994,
                "tmdate": 1700557200396,
                "mdate": 1700557200396,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PZAgtrElSE",
            "forum": "p8ujRTjEf3",
            "replyto": "p8ujRTjEf3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1076/Reviewer_tUNz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1076/Reviewer_tUNz"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the challenge of optimizing bandit algorithms in the context of varying reward variances. Most bandit algorithms assume fixed reward variances or upper bounds on them, leading to suboptimal performance due to the inaccurate estimation of these variances. In this work, the authors lay the foundation for a Bayesian approach that incorporates prior knowledge about reward variances, resulting in lower regret and improved performance. They specifically focus on Gaussian bandits with unknown heterogeneous reward variances and develop a Thompson sampling algorithm with prior-dependent Bayes regret bounds."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "They introduce a Thompson sampling algorithm for Gaussian bandits with known heterogeneous reward variances and provide regret bounds that decrease as reward variances decrease. This is a significant advancement in the context of bandit algorithms.\n\nThey propose a Thompson sampling algorithm (VarTS) for Gaussian bandits with unknown heterogeneous reward variances. VarTS maintains a joint Gaussian-Gamma posterior for mean rewards and precision, resulting in Bayes regret bounds that decrease with lower reward variances and more informative priors.\n\nThe paper thoroughly evaluates VarTS on various reward distributions, demonstrating its superiority over existing baselines. The results highlight the generality and robustness of the proposed algorithm.\n\nThe work distinguishes itself from prior research by providing strong finite-time regret guarantees, as opposed to asymptotic bounds.\n\nThey discuss the differences between their Bayesian approach and frequentist algorithms, highlighting the ability to leverage more informative priors in their design"
                },
                "weaknesses": {
                    "value": "The paper primarily focuses on Gaussian bandits with heterogeneous reward variances. While this is a significant step, the approach may not be directly applicable to other types of bandit problems with different reward distributions. The generalizability of the proposed method to various scenarios is not thoroughly explored.\n\nBayesian regret is often considered as easier than frequentist regret.\n\nYou may need to cite https://arxiv.org/abs/2006.06613 and https://arxiv.org/abs/2302.11182 as they analysed a Gaussian thompson sampling policy in a quite general setting which is related and relevant to the present paper.\n\nI was thinking the Gaussian-Gamma prior was already used for the analysis of TS, but it seems that I am wrong since I cound not find back the ref."
                },
                "questions": {
                    "value": "I am not convinced that this is a fundamental step in dealing with the problem of the general case of unknown variance in the case of non-Gaussian reward. Can you give more details on this ?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1076/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698416809796,
            "cdate": 1698416809796,
            "tmdate": 1699636034225,
            "mdate": 1699636034225,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fAbssQNoX9",
                "forum": "p8ujRTjEf3",
                "replyto": "PZAgtrElSE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1076/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1076/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal for Reviewer tUNz"
                    },
                    "comment": {
                        "value": "Thank you for the detailed review. Our responses to your pointed-out weaknesses are below. We are happy to discuss any additional concerns that you may have.\n\n**W1: The approach is limited to Gaussian distributions with unknown variances**\n\nPlease see Q2 in **Common response**.\n\n**W2: Bayes regret versus frequentist regret**\n\nPlease see Q1 in **Common response**.\n\n**W3: Additional references**\n\nThank you for the references. ***We included both [Perrault et al. (2020)](https://arxiv.org/abs/2006.06613) and [Perrault (2023)](https://arxiv.org/abs/2302.11182) in related works in the updated paper.*** However, note that these works focus on extending Thompson sampling to combinatorial bandits rather than variance adaptivity in our work. In a sense, this is an orthogonal structure.\n\n**Q1: Extension beyond Gaussian bandits**\n\nWe discuss an extension beyond Gaussian bandits in Q2 in **Common response**. The general case is hard to handle in theory, and this is one limitation of Bayesian algorithms and their analyses. We discuss this in Q1 in **Common response**. The second best thing is an empirical validation and this is why we experiment with two types of sub-Gaussian noise in Section 6: Bernoulli and beta."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1076/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700299632990,
                "cdate": 1700299632990,
                "tmdate": 1700327191168,
                "mdate": 1700327191168,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cDW3sRNU4T",
                "forum": "p8ujRTjEf3",
                "replyto": "fAbssQNoX9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1076/Reviewer_tUNz"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1076/Reviewer_tUNz"
                ],
                "content": {
                    "comment": {
                        "value": "I acknowledge having read the rebuttal. I would like to thank the authors for their response to my questions. I will consider increasing my score."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1076/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700635299977,
                "cdate": 1700635299977,
                "tmdate": 1700635299977,
                "mdate": 1700635299977,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "gV8mttTGLW",
            "forum": "p8ujRTjEf3",
            "replyto": "p8ujRTjEf3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1076/Reviewer_RHSZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1076/Reviewer_RHSZ"
            ],
            "content": {
                "summary": {
                    "value": "The authors investigate Gaussian bandits with heterogeneous reward variances, both known and unknown. For known variances, they introduce a Thompson sampling algorithm, achieving a Bayes regret bound that decreases as variances decrease. When variances are unknown, they present the VarTS algorithm, which employs a joint Gaussian-Gamma posterior for the mean and precision of all arm rewards. They establish a Bayes regret bound for VarTS that decreases with decreasing variances and stronger priors. Extensive experiments confirm VarTS's efficacy across different reward distributions."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1) This paper pioneers the study of Gaussian bandits with unknown heterogeneous reward variances and introduces the VarTS algorithm, leveraging a Gaussian-Gamma posterior for the task.\n2) The authors provide Bayes regret bounds that captures the effect of the prior on learning reward variances for Gaussian bandits with both known and unknown heterogeneous reward variances. The regret analysis for the unknown variance scenario is novel.\n3) Numerical experiments encompass not just Gaussian but also Bernoulli and beta distributions, demonstrating the generality and robustness of the proposed method."
                },
                "weaknesses": {
                    "value": "1) As mentioned on page 5, the finite-time Bayes regret lower bound for Gaussian bandits remains unresolved, making the optimality of the Bayes regret bounds in Theorems 1 and 2 ambiguous. Furthermore, the current optimality discussion focuses on the order of $K, n$, but there should also be a discussion on its dependency on the prior and reward variances.\n2) The algorithm design for Gaussian bandits with unknown heterogeneous rewards seems as standard as the typical TS algorithm."
                },
                "questions": {
                    "value": "1) The detailed proof of Theorem 2 stands out as the main technical contribution in the paper. However, the current discussion on page 6 is somewhat succinct. A more in-depth exploration of the core ideas behind the proof would greatly benefit readers in grasping its implications.\n2) Is it possible to derive finite-time prior-dependent regret bound for Bernoulli TS or Beta TS?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1076/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698922621170,
            "cdate": 1698922621170,
            "tmdate": 1699636034151,
            "mdate": 1699636034151,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3sHGUxzXjZ",
                "forum": "p8ujRTjEf3",
                "replyto": "gV8mttTGLW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1076/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1076/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal for Reviewer RHSZ"
                    },
                    "comment": {
                        "value": "Thank you for the detailed review. Our responses to your pointed-out weaknesses are below. We are happy to discuss any additional concerns that you may have.\n\n**W1: Dependencies on prior and reward variances are not discussed**\n\nThey should be. For Theorem 1, read starting from \"Second, it increases with variances\" on page 5. For Theorem 2, read starting from \"Further, a closer examination of $C$ reveals many similarities:\" on page 7. Please let us know if you would like to discuss anything else.\n\n**W2: Algorithm seems standard**\n\nAlgorithm 2 is standard posterior sampling for Gaussian-Gamma distribution, based on well-known formulas in [Murphy (2007)](https://www.cs.ubc.ca/~murphyk/Papers/bayesGauss.pdf). The novelty is in analyzing a variance-adaptive algorithm. We derive the first Bayes regret bound for such an algorithm. The bound captures the benefit of informative priors and is finite time. None of the prior bounds have either of these properties. This is discussed in the last paragraph of Section 5.2.\n\n**Q1: Theorem 2 does not stand out as the main contribution**\n\nWe will certainly elaborate on this in the main paper and include a detailed sketch. For the rebuttal, we discuss the technical novelty in Q3 in **Common response**.\n\n**Q2: Do Bernoulli or beta TS have finite-time prior-dependent regret bounds?**\n\nBernoulli TS was analyzed in ranking problems by [Kveton et al. (2022)](https://proceedings.mlr.press/v151/kveton22a/kveton22a.pdf). Note that the Bernoulli distribution has a single parameter, which determines both its mean and variance. Beta TS is problematic because the beta distribution does not have a computationally tractable prior."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1076/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700299404648,
                "cdate": 1700299404648,
                "tmdate": 1700299404648,
                "mdate": 1700299404648,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bieMpifOKX",
                "forum": "p8ujRTjEf3",
                "replyto": "3sHGUxzXjZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1076/Reviewer_RHSZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1076/Reviewer_RHSZ"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the response. To clarify my question about W1, I was asking whether the dependencies on prior and reward variances are tight, i.e., any lower bound result regarding them."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1076/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700308310354,
                "cdate": 1700308310354,
                "tmdate": 1700308310354,
                "mdate": 1700308310354,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0QRGdukePl",
            "forum": "p8ujRTjEf3",
            "replyto": "p8ujRTjEf3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1076/Reviewer_u1uV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1076/Reviewer_u1uV"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the two-parameter Thompson Sampling with Bayesian regret. When the variance is known, the regret bound presented in this paper scales as $\\sqrt{n\\sum_{i=1}^{K}\\sigma_i^2}\\cdot \\log n$, which is optimal up to a $\\log n$ factor. In the case of unknown variance, the regret bound resembles that of the known variance scenario. Experimental findings indicate that the Thompson Sampling algorithm introduced in this study outperforms other baseline methods, including UCB and traditional Thompson Sampling."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "See summary."
                },
                "weaknesses": {
                    "value": "However, there are certain limitations to this paper:\n\n1. The techniques used to prove Bayesian regret have been extensively explored in previous literature and are considerably more straightforward than those for frequentist regret. More crucially, Bayesian regret is a subset of frequentist regret. This is because the latter always implies Bayesian regret, but the reverse is not true. \n2. The regret bound presented is not particularly stringent. It is a log n factor away from the optimal regret.\n3. The paper omits some crucial related work such as: \n    (1) Minimax policies for adversarial and stochastic bandits;\n    (2) Mots: minimax optimal thompson sampling;\n    (3) A minimax and asymptotically optimal algorithm for stochastic bandits;\n    (4) Prior-free and prior-dependent regret bounds for thompson sampling\n    (5) Thompson Sampling with less exploration is fast and optimal. It would be beneficial to see the baselines (2) (3) and (5) incorporated into the experiments.\n4. The experimental framework is anchored in the Bayesian context. It would be enlightening to witness experiments in the general setting, i.e., no prior on the means and variance of arms."
                },
                "questions": {
                    "value": "In Bayesian analysis, regret bounds are typically derived using the Upper Confidence Bound (UCB) method, as illustrated in works like \"Prior-free and prior-dependent regret bounds for Thompson Sampling.\" However, this paper presents an alternative approach. It raises the question of whether it is possible to obtain the regret bound through a UCB method."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1076/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1076/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1076/Reviewer_u1uV"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1076/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699022823449,
            "cdate": 1699022823449,
            "tmdate": 1700564515243,
            "mdate": 1700564515243,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kBR7OiFGh3",
                "forum": "p8ujRTjEf3",
                "replyto": "0QRGdukePl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1076/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1076/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal for Reviewer u1uV"
                    },
                    "comment": {
                        "value": "Thank you for the detailed review. Our responses to your pointed-out weaknesses are below. We are happy to discuss any additional concerns that you may have.\n\n**W1: Bayes regret is weaker than frequentist regret**\n\nPlease see Q1 and Q3 in **Common response**. In addition, note that we derive the first Bayes regret bound for variance-adaptive Thompson sampling. The bound reflects the benefit of informative priors and is finite time. None of the prior bounds have either of these properties. This is discussed in the last paragraph of Section 5.2.\n\n**W2: Presented bounds are not very stringent**\n\nNote that $\\sqrt{\\log n}$ grows much slower than $\\sqrt{n}$. Therefore, it is customary to hide these factors in $\\tilde O(\\cdot)$ in bandit analyses. The elimination of the extra $\\sqrt{\\log n}$ is an interesting open question, and would be needed to close the gap between the regret upper and lower bounds.\n\n**W3: Paper omits some crucial related works**\n\nWe respectfully disagree. The objective of our paper was to design, analyze, and empirically evaluate an algorithm that can adapt to unknown reward variances. Therefore, we included all relevant baselines to variance adaptation. That being said, we agree that we should use the latest improvements of Thompson sampling. We reviewed (2), (3), and (5). (5) compared their algorithm to (2) and (3), and showed that it is generally superior. ***Therefore, we implemented (5) for both Bernoulli and Gaussian Thompson sampling (Section 6 of the updated paper).*** This significantly reduces the regret of Gaussian TS while keeping the regret of Bernoulli TS comparable. Apart from this, all other conclusions in Section 6 remain the same.\n\n**W4: Bayesian framework in experiments**\n\nBayesian algorithms are quite robust to prior and model misspecification. This can be seen in our experiments with Bernoulli and beta bandits, where VarTS (a Gaussian bandit algorithm) performs well. When the prior or model misspecification is major, Bayesian algorithms can fail miserably. We will comment on this in the paper.\n\n**Q1: Can we obtain a regret bound through a UCB-like analysis?**\n\nWe follow the classic Thompson sampling analysis of [Russo and Van Roy (2014)](https://djrusso.github.io/docs/Learning_to_Optimize.pdf). The key idea is to bound the per-round regret conditioned on history by high-probability confidence intervals. Then we add them up and get an upper bound on the $n$-round regret. This style of an analysis is also common in linear bandits."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1076/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700299359487,
                "cdate": 1700299359487,
                "tmdate": 1700299359487,
                "mdate": 1700299359487,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NYevNkfPIO",
                "forum": "p8ujRTjEf3",
                "replyto": "kBR7OiFGh3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1076/Reviewer_u1uV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1076/Reviewer_u1uV"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your reply. I appreciate that you have addressed many of my concerns. However, there remains an issue regarding the related work section. While it is acceptable to incorporate recent studies in your experiments, it is also important to discuss other seminal papers in the field. This is particularly relevant since your paper considers the known variance setting\u2014a scenario these previous works have significantly contributed to. Additionally, I recommend that you delineate between the modified Thompson Sampling, referred to as (5), and the original Thompson Sampling in your figures. This clarification is important because (5) represents a variation of the standard Thompson Sampling algorithm, and not the traditional version itself."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1076/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700528632984,
                "cdate": 1700528632984,
                "tmdate": 1700528632984,
                "mdate": 1700528632984,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Qa53UL4egt",
                "forum": "p8ujRTjEf3",
                "replyto": "0QRGdukePl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1076/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1076/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the follow-up comments.\n\n**Re. Related Works:** We completely agree with you that we should add a discussion of all the 5 (seminal) papers pointed out above in our related works as well. We were planning to add that in the updated submission, but now we will make sure to add these discussions in the related work section of our Openreview submission before November 22 and upload the draft again.  \n\n**Re. (5):** Additionally, we will also update the plots to distinguish between (modified) $\\epsilon$-TS (5) and standard (classic) TS. We will update them in the draft as well. Hope this will address all your concerns, please let us know if we can clarify anything further in the meantime."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1076/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700556247164,
                "cdate": 1700556247164,
                "tmdate": 1700557281405,
                "mdate": 1700557281405,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Ai6Zc50IQc",
            "forum": "p8ujRTjEf3",
            "replyto": "p8ujRTjEf3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1076/Reviewer_Lfwz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1076/Reviewer_Lfwz"
            ],
            "content": {
                "summary": {
                    "value": "This work develops a new Thompson sampling algorithm and proves prior-dependent Bayes regret bounds for K-armed Gaussian bandits. It studies the problem in both settings where the reward variances are known and unknown. The algorithm can achieve lower regret when the\nreward variances are low, which indicates the trade-off of the learner\u2019s performance (regret) versus the prior parameters and reward\nvariances. The authors evaluate their new algorithms with numerical experiments comparing with other baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is well organized. The presentation is unambiguous.\n2. The paper provides a new regret bound dependent on reward variances and informative priors.\n3. The paper conducts numerical experiments to justify the main results."
                },
                "weaknesses": {
                    "value": "1. The variance aware regret bound depends on the summation of the variance across all the arms $\\sum_{i=1}^K \\sigma_i^2$. This can \n reduce the dependency of the number of arms $K$ to a variance-dependent result. However, the standard method to deal with large $K$ is to use function approximation. I feel confused about the relationship of these two methods. Can the result in this work be generalized to the bandit problem with function approximation?\n2. The Bayesian regret studied in this work is usually weaker than the frequentist regret.\n3. Algorithm 2 is under the standard Thompson sampling framework, and the posterior sampling updates seem similar to previous works, for example, [Zhu & Tan., 2020].  Are there any novel techniques in the algorithm design, or do the new results come from the analysis?\n4. There are some mistakes in the literature review. In the study of $d$-dimensional linear contextual bandits, they did not always keep the fixed variance across the arms. For example, in [Kim et al., 2022] and  [Zhao et al.,2023] mentioned in the paper, the stochastic noise at time step $k$, $\\epsilon_k$, is a random variable dependent on $\\mathcal F_k = \\sigma(x_1,\\epsilon_1,\\ldots,x_{k-1},\\epsilon_{k-1}, x_k)$ with $\\mathbb{E}[\\epsilon_k | \\mathcal F_k] = 0$ and $\\mathbb{E}[\\epsilon_k^2 | \\mathcal F_k] = \\sigma_k^2$. They do not assume the variances are fixed across arms.\n5. Some typos: should the bound in page 7 be $\\tilde O(C\\sqrt{n})$?"
                },
                "questions": {
                    "value": "1. Is it possible to deal with distributions other than the Gaussian-Gamma prior distribution?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1076/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1076/Reviewer_Lfwz",
                        "ICLR.cc/2024/Conference/Submission1076/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1076/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699671197573,
            "cdate": 1699671197573,
            "tmdate": 1699671329603,
            "mdate": 1699671329603,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6ILryZpLI0",
                "forum": "p8ujRTjEf3",
                "replyto": "Ai6Zc50IQc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1076/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1076/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal for Reviewer Lfwz"
                    },
                    "comment": {
                        "value": "Thank you for the detailed review. Our responses to your pointed-out weaknesses are below. We are happy to discuss any additional concerns that you may have.\n\n**W1: Function approximations**\n\nPlease note that the goal of this paper was to design, analyze, and empirically evaluate an algorithm that can adapt to unknown reward variances in the standard bandit setting. Our work does not focus on a large number of arms $K$. Therefore, no function approximation is used. Please see Q2 in **Common response** for a generalization beyond Gaussian bandits.\n\n**W2: Bayes regret is weaker than frequentist regret**\n\nPlease see Q1 in **Common response**.\n\n**W3: Algorithmic or analysis novelty?**\n\nAlgorithm 2 is standard posterior sampling for Gaussian-Gamma distribution, based on well-known formulas in [Murphy (2007)](https://www.cs.ubc.ca/~murphyk/Papers/bayesGauss.pdf). The novelty is in analyzing a variance-adaptive algorithm. We derive the first Bayes regret bound for such an algorithm. The bound captures the benefit of informative priors and is finite time. None of the prior bounds have either of these properties. This is discussed in the last paragraph of Section 5.2.\n\n**W4: Discussion of Kim et al. (2022) and Zhao et al. (2023)**\n\nThere seems to be a misunderstanding of what we wrote. We say on page 2 that \"$\\sigma_s^2$ is an unknown reward variance **in round $s$**\". We mean that the reward variance depends on round $s$. However, it is the same for all arms in round $s$. In our work, each arm has its own reward variance, and this one is the same for all rounds. We are happy to incorporate any additional changes if the reviewer has a different opinion and can further elaborate.\n\n**W5: Some typos**\n\n$\\tilde O(\\sqrt{Cn})$ should be $\\tilde O(C \\sqrt n)$. Thanks for pointing this out.\n\n**Q1: Other reward distributions than Gaussian-Gamma**\n\nOur algorithm design and analysis are structured enough to incorporate other models. We discuss this in Q2 in **Common response**."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1076/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700299278328,
                "cdate": 1700299278328,
                "tmdate": 1700299278328,
                "mdate": 1700299278328,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uoSuscHQ5E",
                "forum": "p8ujRTjEf3",
                "replyto": "6ILryZpLI0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1076/Reviewer_Lfwz"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1076/Reviewer_Lfwz"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. Please inform me if I have any misunderstanding, but I do not see any assumption that the variance is the same for all arms in round $s$, for example, in Kim et al. (2022). Could you provide any reference on such an assumption?"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1076/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700515038587,
                "cdate": 1700515038587,
                "tmdate": 1700515038587,
                "mdate": 1700515038587,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "En3xJQ2voW",
                "forum": "p8ujRTjEf3",
                "replyto": "6ILryZpLI0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1076/Reviewer_Lfwz"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1076/Reviewer_Lfwz"
                ],
                "content": {
                    "comment": {
                        "value": "While awaiting further clarification from the authors regarding the previous question on the variance assumption, upon reviewing the authors\u2019 response on the novelty of the results, I have come to the realization that the algorithm seems rather conventional, and the analysis does not present any noteworthy novelty. The paper doesn't sufficiently discuss the novel techniques used in the analysis to establish the variance-dependent regret bounds.  The only discussion I can find is behind Theorem 2, which says \u201cTo overcome these difficulties, we carefully condition random variables on each other together with appropriate histories, and combine these using Jensen\u2019s and Cauchy-Schwarz inequalities. \u201d To me, this is not novel. I have decided to maintain my current score and refrain from supporting its acceptance."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1076/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700531313838,
                "cdate": 1700531313838,
                "tmdate": 1700531313838,
                "mdate": 1700531313838,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "q1quUkGXjt",
                "forum": "p8ujRTjEf3",
                "replyto": "Ai6Zc50IQc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1076/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1076/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the follow-up comments.\n\n**Re. Further clarification on the discussion of Kim et al. (2022) and Zhao et al. (2023):** The main confusion could be in the notations used in their papers. Note that in these papers\n\n* K is the time horizon and $k \\in [K]$ denotes the $k$-th time step in the time horizon K. \n* We on the other hand use $K$ to denote the total number of arms and $k \\in [K]$ is the indexing used to denote the $k$-th arm among the $[K]$ arms.\n\nNow please read:\n\n* Page 2 last paragraph of [Kim et al. (2023)](https://arxiv.org/pdf/2111.03289.pdf): \"Let K be the total number of rounds ..... till Equation (2.1)\". \n\n* Section 2.1 of [Zhao et al. (2023)](https://proceedings.mlr.press/v195/zhao23a/zhao23a.pdf): \"Linear bandits. The linear bandit problem has ..... till the last sentence of Page 2\".\n\nThus, in their notation $\\sigma_k^2$ is the noise variance at time step $k$, which is same across all the arms, no matter which arm the algorithm selects in time step $k \\in [K]$. To emphasize on our previous claim again,  these papers do not consider heterogeneous reward variance across arms -- unlike our setting -- where we assume distinct prior variance $\\sigma_{0,i}$ as well as the distinct reward variance $\\sigma_i$ for each distinct $i \\in [K]$. Consequently, the nature of the results of these works are very different than ours, as they can not bring out the tradeoff between regret vs informative priors and variances (across the K arms), unlike our regret bounds in Theorem 1 and 2. Please let us know if you still have any confusion.\n\n**Re. Novelty:** We have further elaborated on our technical novelties and challenges addressed in the proof of Theorem 2 in Q3 in **Common response**, please go over it and also our detailed proof of Theorem 2 in the Appendix (Page 15-20). Hope this clarifies the novelties of our analysis of Theorem 2.\n\n* As promised, we will add a concise proof sketch of Theorem 2 in the updated version of our paper.  \n* To emphasize again, as also clarified in Q3 in **Common response** above, our novelty not only lies in the proof of Theorem 2 but also in the implication of the final bound it reveals: We precisely obtained the ***First finite time regret bound for Bayesian bandits with unknown heterogeneous reward variances which captures the dependence of the regret on the degree of prior information and reward variances, unlike any other existing work.***\n\nPlease let us know if we can clarify anything further."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1076/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700558187186,
                "cdate": 1700558187186,
                "tmdate": 1700559276391,
                "mdate": 1700559276391,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]