[
    {
        "title": "Structural Knowledge Informed Continual Multivariate Time Series Forecasting"
    },
    {
        "review": {
            "id": "YH1J4dW7B2",
            "forum": "URCfZ2NgaR",
            "replyto": "URCfZ2NgaR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4554/Reviewer_1Nyy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4554/Reviewer_1Nyy"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a novel framework called Structural Knowledge Informed Continual Multivariate Time Series Forecasting (SKI-CL) that leverages structural knowledge to improve MTS forecasting under the continual learning setting. The proposed framework consists of a deep forecasting model that incorporates a graph learner to capture the variable dependencies and a regularization scheme to ensure consistency between learned variable dependencies and structural knowledge. The authors address the challenge of modeling variable dependencies across different regimes while maintaining forecasting performance. The paper presents experimental results on several real-world datasets, demonstrating the effectiveness of the proposed framework in improving forecasting performance and maintaining consistency with structural knowledge."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper proposes a novel framework, SKI-CL, that leverages structural knowledge to improve MTS forecasting under the continual learning setting. \n2. The proposed framework consists of a deep forecasting model that incorporates a graph learner to capture the variable dependencies and a regularization scheme to ensure consistency between learned variable dependencies and structural knowledge. \n3. The paper presents experimental results on several real-world datasets, demonstrating the effectiveness of the proposed framework in improving forecasting performance and maintaining consistency with structural knowledge."
                },
                "weaknesses": {
                    "value": "1. This paper is written in a way that takes time to understand the term \"regime\", so it is not easy to follow.\n2. This paper omits related work on continual learning in time series and graph domains.\n3. The hyperparameter analysis used in the sensitivity analysis is unclear, which reduces confidence in the experimental results.\n4. There is no comparison of model complexity or training time with other baselines.\n5. This paper can be read as an incremental work that brings the existing graph continual learning problem to the MTS domain. While the authors define the catastrophic forgetting problem in different regime settings in this paper, they overshadow all other similar problems in the time series domain. For example, this paper almost entirely ignores concept/temporal drift, dynamic graph learning, and continual learning on traditional time series. In this paper, it is necessary to clarify the similarities and differences with these fields and show the relative position in these research lines."
                },
                "questions": {
                    "value": "1. The definition of regime written by the authors in the introduction is unclear, so readers may have difficulty understanding it. The motivation to solve problems caused by different regimes is good, but if readers do not understand regimes, the motivation of this paper may be meaningless. Readers may want to look at Figure 1 for simple examples of problems caused by different regimes instead of SKI-CL's framework.\n2. The introduction on page 2 of this paper explains that the catastrophic forgetting problem in the MTS task is \"the model performance will deteriorate over existing regimes as their associated structural knowledge cannot be maintained.\" Compared to the definitions of catastrophic forgetting in other existing domains, this is a setting in which no tasks or classes are added/incremented. Is this description appropriate compared to definitions in other domains?\n3. This paper needs to add related research to papers that solve concept drift and temporal drift, where the distribution changes with time in a time series. In this paper, there is a need to add related research to the paper that addresses concept drift and temporal drift, where distributions change over time in time-series data. The authors also need to compare the models and experiments in these papers.\n4. This paper needs to describe the position in the research of continuous learning, incremental learning, concept/temporal drift, and dynamic graph learning in each domain. Can the authors explain what the similarities and differences are with these research topics?\n5. As written in the last paragraph of Section 2.2, the authors are aware that FSNet [1] uses MTS data, but do not consider it in the baseline due to the online learning setting. However, the experimental baseline of FSNet also performs including **DER++** and **ER**. This appears to be possible in the experimental settings of this paper, and comparative experiments are needed. If you can compare FSNet, compare the performance difference compared to the model proposed by the author.\n6. Additionally, the paper makes no mention of the MIR [2] method. This should be considered similarly to FSNet above.\n7. The authors need to mention that papers [3-5] dealing with temporal drift are either experimental baselines or related work.\n8. Can you explain how the dependency structure learning proposed by the author differs from the graph structure parameterization in the GTS paper [6]? If there is a similarity, it is necessary to cite the GTS paper.\n9. Section 4.4 of the paper explains as follows: \u201cWe perform experiments on the Traffic-CL dataset to validate the effectiveness and sensitivity of two key hyperparameters in SKI-CL, the weight of structure regularizer $\\lambda$ (1 by default) and the memory budget (sampling ratio) at each regime (0.01 by default).\u201d However, the results for $\\lambda$ of 1 are not shown in Table 3. And the results in Table 3 and Table 4 do not show any settings that match the results in Table 2, so the reliability of the experimental results is reduced.\n10. From the experimental results in Table 2, the AF of SKI-CL of the proposed model does not result in the lowest error compared to all baselines. For example, $\\text{Autoformer}_{\\text{der++}}$ has the lowest MAE for AF in Traffic-CL. There seems to be a lack of explanation as to why other baselines have lower errors.\n11. There is a grammatical error in the sentence \"given the a collection\" above Eq.(1) on page 5.\n\n\n> [1]: Learning Fast and Slow for Online Time Series Forecasting, ICLR 2023\n> [2]: Online continual learning with maximal interfered retrieval, NeurIPS 2019\n> [3]: AdaRNN: Adaptive learning and forecasting of time series, CIKM 2021\n> [4]: Reversible Instance Normalization for Accurate Time-Series Forecasting against Distribution Shift, ICLR 2022\n> [5]: Time Series Forecasting with Hypernetworks Generating Parameters in Advance, arXiv preprint arXiv:2211.12034, 2022\n> [6]: Discrete Graph Structure Learning for Forecasting Multiple Time Series, ICLR, 2021"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4554/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4554/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4554/Reviewer_1Nyy"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4554/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698118659026,
            "cdate": 1698118659026,
            "tmdate": 1699636433057,
            "mdate": 1699636433057,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Eyxxn14Cdn",
                "forum": "URCfZ2NgaR",
                "replyto": "YH1J4dW7B2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4554/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4554/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 1Nyy (1)"
                    },
                    "comment": {
                        "value": "We sincerely thank the reviewer for recognizing our contribution and the constructive comments. The answers to the concerns and questions are listed below:\n\n**Weakness 1 and Question 1: Concerns regarding the definition of regime**\n\nWe sincerely appreciate the reviewer for recognizing our contributions and raising concerns in terms of the definition of regimes. In the abstract and introduction, we referred \u201cregime\u201d to \u201cstage\u201d and indicated the underlying temporal dynamics and dependency structures of multivariate time series data for each regime is different (**abstract, the end of the first paragraph and the beginning of the second paragraph in introduction, page 1**). We also discussed this term multiple times under the context of continual learning across the introduction. As shown in Figure 1, we defined different regimes with raw multivariate time series and the corresponding structural knowledge, which are marked and annotated using different colors. Besides, in the beginning of the methodology, we gave the mathematical formulation of our continual forecasting task with a clear notion of regime. Again, we appreciate the reviewer for raising this concern and would emphasize the definition of regime at the beginning of our final manuscript and add another real-world example for demonstration.  \n\n**Weakness 2: Paper omits related work on continual learning in time series and graph domains**\n\nWe explicitly discussed the methods of continual learning in time series in our related work, **section 2.2**, where we also explicitly mentioned methods of continual learning on graphs.\n\n**Weakness 3 and Question 9:  Concerns of ablation study results with main experiment results**\n\nMain experiment results and ablation study for memory budgets are average experiment results that run by using different random seeds. The results have small differences due to randomness but similar trends are observed. We will release our code and models in the final version.\n\n**Weakness 4: Lack of comparison of model complexity or training time**\n\nWe compare the number of parameters for each backbone models as following table:\n| Model | Parameter Number | Rank |\n| :--- | :---: | :---: |\n| LSTNet | 53253| 12 |\n| STGCN| 96606 | 11 |\n| MTGNN| 139990 | 10 |\n| AGCRN| 252130 | 8 |\n| GTS | 14647763 | 2 |\n| ESG | 5999516 | 4 |\n| TCN | 170886 | 9 |\n| StemGNN | 1060802 | 6 |\n| Autoformer | 10612758 | 3 |\n| PatchTST | 3226124 | 5 |\n| Dlinear | 49168 | 13 |\n| TimesNet | 36849590 | 1 |\n| SKI-CL | 614731 | 7 |\n\nThe number of our models' parameters is significantly smaller than the SOTA baseline models, including Autoformer, PatchTST, TimesNet and can achieve the best performance for continual learning tasks.\n\n**Question 2 Concerns of the description of catastrophic forgetting with other domain**\n\nWe appreciate the reviewer for raising this question. Yes, our description of catastrophic forgetting is appropriate. We would like to emphasize the **full context** of catastrophic forgetting that the reviewer refers to, in the first paragraph of page 2, is about modeling variable dependencies of MTS data for forecasting tasks (which is our research focus, and this problem is still underexplored as we discussed in the literature review, **the first paragraph of section 2.2 in page 3**).  \nWe refer to the definition of catastrophic forgetting in one of the most cited paper in continual learning [1],  *\u201cContinual learning poses particular challenges for artificial neural networks due to the tendency for knowledge of the previously learned task(s) (e.g., task A) to be abruptly lost as information relevant to the current task (e.g., task B) is incorporated. This phenomenon, termed catastrophic forgetting, occurs specifically when the network is trained sequentially on multiple tasks because the weights in the network that are important for task A are changed to meet the objectives of task B.\u201d*. Our description fits this definition, as the so-called \u201ctask\u201d means regime in our case, and the \u201cknowledge\u201d within catastrophic forgetting involves the structural knowledge of MTS data learned by dependency modeling methods. \nThe reviewer mentions that this is a setting in which no tasks or classes are added/incremented, which is under the context of classification tasks. In essence, our continual forecasting is a regime incremental setting in regression tasks, which is analogous to class-incremental tasks in classification tasks.  \nTherefore, the description demonstrates the catastrophic forgetting problem under our multivariate time series forecasting task and is appropriate compared with continual learning in other domains. \n\n[1] Kirkpatrick, James, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan et al. \"Overcoming catastrophic forgetting in neural networks.\" Proceedings of the national academy of sciences 114, no. 13 (2017): 3521-3526."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4554/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700020973321,
                "cdate": 1700020973321,
                "tmdate": 1700160066678,
                "mdate": 1700160066678,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xJZdYILW7S",
                "forum": "URCfZ2NgaR",
                "replyto": "YH1J4dW7B2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4554/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4554/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 1Nyy (2)"
                    },
                    "comment": {
                        "value": "**Weakness 5 and Question 4: The discussions of concept/temporal drift, dynamic graph learning and continual learning on traditional time series. Research positions**\n\nWe appreciate the reviewer for raising this concern. Firstly, we would like to clarify that the scope of our paper is different from that of graph continual learning. Graph continual learning aims to maintain the model performance of all previous tasks on a sequence of explicit topological structures that is potentially evolving. However, in our work, we resort to modeling the dependency structure of MTS data that is coupled with the temporal dynamics, and leverage it for downstream forecasting tasks. This means that the dependency structure is not fixed and implicit within each regime.\nIn addition to that, several terms are mentioned in this comment: concept/temporal drift, dynamic graph learning, and continual learning on traditional time series. For continual learning on traditional time series, we have discussed the continual learning methods for MTS data in the **section 2.2**, and we are not sure which reference the reviewer refers to here. It would be helpful to facilitate the discussion if specific literature can be pointed out.\n**Dynamic graph learning:** Next, we discuss the main differences between dynamic graph learning and our method. While both dynamic graph learning and our method deal with the evolving data, dynamic graph learning focuses on adapting to the changes in the explicit graph structure and possibly features over time. Like graph continual learning, dynamic graph learning uses the explicitly given graph structure for message passing and performs downstream tasks like node classification, link prediction, or community detection. While our proposed continual multivariate time series forecasting method needs to discover underlying dependencies structure of variables over different regimes, by capturing the temporal dynamics of MTS data. This also leads to the other key difference between dynamic graph learning and our method: the former always adapts its model to cope with the change (perform well on the new data), while the primary goal of the latter is the model's capability to continually learn without forgetting previous knowledge (i.e., addressing catastrophic forgetting). The focus of methodology can be very different due to the different learning objectives. We have similar discussion regarding the differences between online learning and our setting, in the literature review, **the second paragraph of section 2.2 in page 3**.  \n**Concept drift/temporal drift:** Then we would like to discuss the relationship between our research scope and concept drift/temporal drift, and explain why these methods cannot serve as baselines for comparison(as the reviewer mentioned in the other question). Concept/temporal drift of MTS represents the phenomenon where the data distribution of the target MTS changes over time in unforeseen ways. Based on this notion, it is important to emphasize the continual adaptation nature of continual learning setting regarding the concept/temporal drifts, where the catastrophic forgetting is introduced due to the shifts over multiple regimes. That being said, continual MTS forecasters need to handle a variety of regimes, not just adapt to changes in data distribution for a single one, which is to some degree more realistic and challenging. Concept/temporal drift represents a specific challenge within this broader goal. \nMoreover, the learning objectives between concept/temporal drift methods and continual learning methods are very different. For the concept/temporal drift methods, the objective is to adapt the forecaster to these new MTS patterns by effectively detecting, responding to, and learning from these changes. In contrast, as we emphasized before, the main focus of continual learning methods is to prevent the model from forgetting previously learned knowledge (coupled temporal dynamics and variable dependencies in our scope) when adapting to new MTS distribution. Due to the aforementioned differences in adaptation nature and learning objectives, it is very difficult to make fair experimental comparisons under the same setting. \nFinally, regarding the reviewer\u2019s concern, we would like to restate the position/scope of our research compared to the fields mentioned above. We start from the continual learning perspective, explore and address the challenges of MTS forecasting problem under this setting, where we resort to variable dependency modeling and characterization aided by structural knowledge. As we discussed in **section 2.2**, continual learning on MTS data and especially continual forecasting is still an underexplored problem, and we proposed a solution connecting both research areas. Even if the mentioned research fields and our work all tackle the evolving data, the problem formulation, challenges and learning objectives are significantly different as we discussed above."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4554/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700021082691,
                "cdate": 1700021082691,
                "tmdate": 1700021094851,
                "mdate": 1700021094851,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3YEEgPe7Ow",
                "forum": "URCfZ2NgaR",
                "replyto": "YH1J4dW7B2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4554/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4554/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 1Nyy (3)"
                    },
                    "comment": {
                        "value": "**Question 3 and Question 7: The discussions of concept/temporal drift methods are needed.**\n\nAs we discussed in our response to weakness 5 and question 4, while both concept drift methods and continual learning strategies deal with changing data environments, concept/temporal drift approaches are often more reactive and focused on adapting to changes in data distribution for a specific task. In contrast, continual learning involves a broader spectrum of adaptation, focusing on learning from new data and preventing forgetting over existing data. The continual adaptation nature of continual learning and very different objectives between both fields make it very difficult to include the concept/temporal drift methods as baselines of continual learning and make fair comparisons under the same setting.  \nWe agree with the reviewer to some extent that the concept/temporal drift and continual learning in MTS both tackle the evolving environment, and some ideas from one field can be shared to facilitate the advancement of each field. **Regarding the reviewer\u2019s concern, we have discussed the aforementioned connections in our updated manuscript.** We would like to thank the reviewer for these references of temporal drifts and will try to involve more recent methods.\n\n**Question 5 and Question 6: Include the FSNet as a backbone baseline, and MIR as a continual learning baseline.**\n\nWe sincerely appreciate the reviewer\u2019s advice. However, FSNet is designed for online forecasting, which focuses on fast adaptation to newly incoming data and uses accumulated error on the training data as evaluation protocols (there is no notion of testing data to evaluate model performance at each regime). The objective of continual learning is to maintain the learned knowledge and model performance over all seen data when adapting to new data (as we discussed in **the second paragraph of section 2,2, page 3**). As such, the problem formulation, learning objective and evaluation protocols are very different to the continual learning setting, making it not very meaningful for comparison. DER++ and ER are commonly used experience-replayed methods for continual learning, so it is natural for us to include them as our baselines. \nThe MIR method is also originally designed for online continual learning setting, which uses an in-batch virtual updated model to select samples that cause the highest forgetting to perform experience replay. To enable the fair comparison, we use the aforementioned MIR criteria to select a number of samples from the fixed budget memory per regime.  We report the other SOTA model results below and our proposed method consistently achieves the best performance.     \n\nTraffic-CL:\n| Model | |  |  |  |\n| :---: | :---: | :---: | :---: | :---: |\n|  | AP_MAE | AP_RMSE | AF_MAE | AF_RMSE |\n| PatchTST_seq |19.11 | 32.50  | 2.34  | 2.97 |\n| PatchTST_mir | 19.04 | 32.23  | 2.28 | 2.79 |\n| PatchTST_herd| 18.96 | 32.10  | 2.21  | 2.67 |\n| PatchTST_er|   18.77 |  31.50 | 1.98 | 2.01 |\n| PatchTST_der++| 18.53 |  31.34  | 1.75  | 1.98 |\n| DLinear_seq| 19.69 | 32.75  |2.91  |2.83  |\n| DLinear_mir| 19.37 | 32.25  |2.17  |2.59 |\n| DLinear_herd| 19.53|  32.40 | 2.25 | 2.68 |\n| DLinear_er| 19.19 |  32.30  |1.73  |2.14 |\n| DLinear_der++| 19.02|  31.97  | 1.75  | 1.93 |\n| TimesNet_seq| 17.77 | 29.91 | 3.13 | 6.93 |\n| TimesNet_mir| 17.53 | 29.61 | 2.44 | 5.32 |\n| TimesNet_herd| 17.38 | 29.53 | 2.56 | 5.83 |\n| TimesNet_er| 17.25 | 29.33 | 1.97 | 4.19 |\n| TimesNet_der++| 17.13 | 29.28 | 1.56 | 4.02 |\n| SKI-CL| **15.24** | **25.33** | 1.50 | 2.71|\n\n\n\nSolar-CL:\n| Model | |  |  |  |\n| :---: | :---: | :---: | :---: | :---: |\n|  | AP_MAE | AP_RMSE | AF_MAE | AF_RMSE |\n| PatchTST_seq   | 2.64 | 5.32 | 0.72 | 0.43 |\n| PatchTST_mir   | 2.61 | 5.30 | 0.70 | 0.40 |\n| PatchTST_herd  | 2.60 | 5.30 | 0.68 | 0.35 |\n| PatchTST_er   | 2.57 | 5.27 | 0.47 | 0.30 |\n| PatchTST_der++ | 2.53 | 5.17 | 0.43 | 0.28 |\n| DLinear_seq    | 3.47 | 6.56 | 1.17 | 1.12 |\n| DLinear_mir    | 3.45 | 6.51 | 1.02 | 1.01 |\n| DLinear_herd   | 3.41 | 6.50 | 1.03 | 1.00 |\n| DLinear_er     | 3.37 | 6.43 | 0.93 | 0.98 |\n| DLinear_der++  | 3.25 | 6.37 | 0.83 | 0.79 |\n| TimesNet_seq   | 3.92 | 7.18 | 1.46 | 2.51 |\n| TimesNet_mir   | 3.77 | 7.15 | 1.22 | 1.57 |\n| TimesNet_herd  | 3.83 | 7.10 | 1.03 | 1.44 |\n| TimesNet_er    | 3.55 | 7.02 | 0.42 | 0.91 |\n| TimesNet_der++ | 3.45 | 6.55 | 0.37 | 0.90 |\n| SKI-CL                   | **1.75** | **4.46** | 0.09 | 0.06 |"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4554/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700021171016,
                "cdate": 1700021171016,
                "tmdate": 1700021199990,
                "mdate": 1700021199990,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cBSvrBUGfG",
                "forum": "URCfZ2NgaR",
                "replyto": "YH1J4dW7B2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4554/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4554/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 1Nyy (5)"
                    },
                    "comment": {
                        "value": "**Question 8: Explain the difference of dependency structure learning between the proposed method and GTS**\n\nWe have cited the GTS paper in **introduction(page 2)**, **section 2.1 in literature review(page 3)** and the beginning of our dynamic graph inference module in **section 3.2 (page 4)**. Our dependency structure learning method has multiple important differences from GTS. Firstly, GTS learns a fixed discrete edge distribution based on Gumbel reparameterization trick from the whole training dataset, and samples static dependency structures for each time series window. However, our graph inference module learns a dynamic dependency structure based on the context of each time series window, which models the structure in a more fine-grained manner and copes with the dependencies variations within each regime. Furthermore, this leads to an important difference when doing continual learning, as we emphasized in **section 3.4 (page 6), Figure 1(page 2)** and the second paragraph in **Appendix C (page 19)**, our method is able to dynamically infer faithful dependency structures for existing and current regimes without accessing the memory buffer, while *GTS has to access the training data and memory buffer when inferring graphs at the testing stage, which is not realistic for practical model deployment in real-world applications*. Secondly, our method considers both discrete and continuous edge variable formulations with a flexible learning component and objective, while GTS focuses on the discrete edge scenario with a specialized Gumbel design.  Nevertheless, we agree with the reviewer that GTS and our method share the design choice of modeling an edge for all variable pairs instead of directly modeling the whole graph\u2019s adjacency matrix.\n\n**Question 10: The concern regarding average forgetting in the performance evaluation**\n\nAs we discussed in **Appendix B.3**, we emphasized the superiority of average performance over average forgetting in continual learning (**page 18**):\n*\u201cEven if we provide both metrics for performance evaluation, we need to **emphasize the superiority of average performance over average forgetting** in continual learning. Average performance provides a direct measure of how well a learning system is performing on a task or set of tasks. It reflects the system\u2019s ability to retain previously learned knowledge over past regimes while adapting to new information. While average forgetting is a relevant metric in assessing the memory capabilities of a learning system, it does not provide a complete picture of the learning system\u2019s retention abilities. The average performance takes into account both the retention of old knowledge and the acquisition of new knowledge, providing a more comprehensive evaluation of the learning system\u2019s performance. Therefore, we **use average performance as the main evaluation metric** and **average forgetting as an auxiliary metric** to measure knowledge retention and model adaptivity.\u201d*\nTherefore, we are supposed to firstly look at the average performance. If the average performance is very close, then we pay attention to the average forgetting to further compare the model superiority. In the aforementioned example, autoformer-der++ does not yield satisfactory average performance over past regimes, thus it is less meaningful to directly evaluate the model solely via average forgetting. \n\n**Question 11: A typo in the sentence \"given the a collection\" above Eq.(1) on page 5**\n\nWe sincerely appreciate the reviewer for pointing out the typo here, we corrected it in our updated manuscript."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4554/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700021306017,
                "cdate": 1700021306017,
                "tmdate": 1700168772568,
                "mdate": 1700168772568,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "avxUpac2h3",
                "forum": "URCfZ2NgaR",
                "replyto": "YH1J4dW7B2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4554/Reviewer_1Nyy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4554/Reviewer_1Nyy"
                ],
                "content": {
                    "title": {
                        "value": "Additional questions"
                    },
                    "comment": {
                        "value": "The authors have addressed my concerns well. However, there are still some issues that remain. While I understand that our time is limited, I will consider this in my decision-making process.\n\n------\n\n## Questions on Traffic-CL dataset\nFirstly, I appreciate the authors' clarification regarding the definition of regimes, which has greatly aided my understanding. I have a question about the Traffic-CL dataset. I have a question about how the Traffic-CL dataset was used, as shown below:\n\n> Based on the constructed PEMSD3-Stream dataset, we make the following modifications to further simulate distinct regimes in the setting of continual forecasting. For each year, we rank and select the top 100 traffic sensors with the largest node degrees, based on which we randomly select 22 sensors as a set representing a part of the major traffic.\n\nFurthermore, the authors have sampled using 22 sensors to create the different regimes for each year. My concern arises from the choice of selecting sensor nodes based on the **largest node degree**. As you know, the graph structure for PeMSD3 is typically created based on the distance between sensors. In the PeMSD3-Stream dataset [1], *can we assume that selecting nodes based on node degree will generate distinct regimes?*\n\nThis doubt leads me to suspect that the efficacy of the method proposed by the authors for Traffic-CL may be due to non-distinctive regimes.\nSince the nodes with the largest node degree are sampled, the dataset is already concentrated around the central nodes or hubs of the network in all regimes. For instance, in a road network, nodes with the largest degrees are likely to be located at points with heavy traffic (e.g., intersections). This could mean that areas with relatively less traffic might be overlooked in your Traffic-CL dataset. This suggests that the traffic volume at each node in each regime is likely to be already high relative to the traffic volume of others not selected. Alternatively, because nodes with high connectivity are selected, the traffic volume of nodes is likely to be similar across regimes.\n\nI believe this is a different issue from what is visualized in Figure 3. Could you provide information on how the traffic volume distribution varies across each regime in Traffic-CL to clarify this issue?\n\nIn other words, it is necessary to reconsider whether the Traffic-CL dataset in which changes in structural dependency do not affect temporal dependency is suitable for continuous learning using structural knowledge for MTS.\n\n\n## Appendix B\nMy concerns have been somewhat addressed in Appendix B. While the explanation in Appendix B highlights the key differences between temporal/concept drift and continual learning, it might slightly oversimplify the relationship between these two areas. As you know, the temporal and concept drift methods can be an essential part of continual learning strategies. \n\nAdditionally, the distinction between dynamic graph learning and the method proposed by the authors is still not readily apparent to the reader. I understand the authors' motivation and the differences between your method and others, but I suggest that elaborating on these differences more clearly in the introduction of the paper would be beneficial. Researchers in the dynamic graph learning and concept drift communities are likely to be particularly interested in the positioning of this paper.\n\n\n\n## Minor comment\n\nRegarding citations in the paper, the use of \\cite{} makes it difficult to distinguish between \\cite{} and \\citet{}. It would be more reader-friendly if the authors could change \\cite{} to \\citep{}.\n\n> [1] Xu Chen, Junshan Wang, and Kunqing Xie. \"TrafficStream: A Streaming Traffic Flow Forecasting Framework Based on Graph Neural Networks and Continual Learning,\u201d IJCAI, 2021. https://www.ijcai.org/proceedings/2021/0498.pdf"
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4554/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700549084486,
                "cdate": 1700549084486,
                "tmdate": 1700549084486,
                "mdate": 1700549084486,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ncjyrFh3Qv",
                "forum": "URCfZ2NgaR",
                "replyto": "YH1J4dW7B2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4554/Reviewer_1Nyy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4554/Reviewer_1Nyy"
                ],
                "content": {
                    "title": {
                        "value": "Maintaing original score"
                    },
                    "comment": {
                        "value": "I maintain my score due to concern regarding the regimes in the Traffic-CL dataset.\n\nAs the authors say, in graphs where local clusters are formed, selecting the top 100 nodes with the highest node degree likely results in a high probability of choosing duplicate nodes. The selected nodes still represent structural dependency within the original networks, which calls into question the claim of different regimes.\n\nThe visualizations in the paper for Traffic-CL only show the 22 nodes for each regime. From these visualizations, I cannot discern how different sensor nodes are positioned across each regime. Additionally, referring to the statistics in the Traffic-Stream paper, the rate of expansion in the graphs is not very fast. In fact, the expansion from 2014 to 2015 involves the addition of only about 12 nodes, suggesting that these added nodes are still likely to fall within local clusters.\n\nMy continued skepticism about the \u201cdistinct regimes\u201d claimed by the authors remains one of the most disappointing aspects of the paper. I find the methods used to form distinct regimes in Solar-CL and HAR-CL to be more convincing. In Solar-CL, different 5 states are used as distinct 5 regimes, whereas the distinct regimes in Traffic-CL seem less well-defined."
                    }
                },
                "number": 27,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4554/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700728644956,
                "cdate": 1700728644956,
                "tmdate": 1700728644956,
                "mdate": 1700728644956,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HgCQx2XR65",
                "forum": "URCfZ2NgaR",
                "replyto": "YH1J4dW7B2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4554/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4554/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would really appreciate if the reviewer could check our earlier responses and the experimental results in the paper carefully, rather than misinterpreting it. \n\n**\u201cAs the authors say, in graphs where local clusters are formed, selecting the top 100 nodes with the highest node degree likely results in a high probability of choosing duplicate nodes.\u201d**  This comment is completely opposite to our responses and experimental results, as we said **\u201cit is not the case\u201d**. \n\nMoreover, we need to remind the reviewer that the performance evaluation part is very strong evidence showing regime differences. Specifically, on Traffic-CL (as shown in Tables 2, 8, and 9), we have observed that *_seq* based methods perform significantly worse than other continual learning methods. This observation is consistent with the results (_seq) on other datasets, i.e., Solar-CL and HAR-CL (for which the reviewer has admitted the validity). This is strong evidence to demonstrate the validity of our setting and the potential distribution disparity."
                    }
                },
                "number": 28,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4554/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740704294,
                "cdate": 1700740704294,
                "tmdate": 1700742584487,
                "mdate": 1700742584487,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "k4kbxLy9ey",
            "forum": "URCfZ2NgaR",
            "replyto": "URCfZ2NgaR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4554/Reviewer_RPp8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4554/Reviewer_RPp8"
            ],
            "content": {
                "summary": {
                    "value": "This paper develops a novel structural knowledge informed continual learning framework to perform multivariate time series forecasting under continual learning setting. The key idea is to exploit structural knowledge to characterize the variable dependencies within each different regime and leverage a representation matching sample selection technique to construct memory buffer for replay. The experiment results on 4 public datasets showed the effectiveness of the proposed SKI-CL."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "+ This paper is well written and organized. The structural knowledge informed continual learning framework is well-motivated and a comprehensive overview of related research is provided.\n+ This paper introduces a novel SKI-CL framework for MTS forecasting and dependency structure inference under continual learning. This is an interesting, new, and practical MTS forecasting scenario to explore. \n+ The proposed dynamic graph structure learning module to capture temporal dependencies and infer dependency structures are elegantly articulated and technically sound. \n+ The idea to incorporate structural knowledge via adaptive regularization over the parameterized graph can enable the proposed model to infer the structural knowledge from all learned scenarios (regimes).\n+ An innovative representation-matching memory replay scheme is proposed to maximize temporal data coverage and preserve dynamics and structures.\n+ The experiment results are comprehensive and solid. Ablation studies of different components and some case studies are also provided."
                },
                "weaknesses": {
                    "value": "- This paper studies MTS forecasting under continual learning setting, however, some datasets used for evaluation is standard benchmark. How do you define different regimes in this case?\n- I notice that Autoformer which focuses on long term forecasting has been compared here. I wonder what\u2019s the performance of PatchTST here, although it is also originally designed for long term forecasting."
                },
                "questions": {
                    "value": "Please find the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4554/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698756540555,
            "cdate": 1698756540555,
            "tmdate": 1699636432968,
            "mdate": 1699636432968,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EVqHN9eGwE",
                "forum": "URCfZ2NgaR",
                "replyto": "k4kbxLy9ey",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4554/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4554/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We sincerely thank the reviewer for recognizing our contribution and the constructive comments. The answers to the concerns and questions are listed below:\n\n**Weakness 1: How do you define different regimes for some benchmark data?**\nThanks for raising this question! Two datasets are related to the standard benchmarks (PEMS and Solar), but extended by fetching more data under identical settings from the same databases. To be specific, the Traffic-CL dataset is based on an existing PEMSD3-Stream dataset [1] which uses the traffic data (Performance Measurement System in California, District 3) from the year 2011 to 2017. The standard PEMS benchmarks for general multivariate time series forecasting is a subset from this database. Based on the PEMSD3-Stream dataset, we further simulate distinct regimes represented/defined by a different portion of a temporally expanding traffic network (sensors with different geo-locations) from different years.  \nFor Solar-CL, we use the database of NREL\u2019s Solar Power Data for Integration Studies, which contains solar power data in the United States for the year 2006. The standard benchmark uses the Alabama subset of this database for general multivariate time series forecasting. We construct different regimes by states (spatial locations) with different average annual sunlight levels based on the statistics. \nThe detailed description of how we generate datasets with different regimes is provided in **appendix B.1 (pages 14-16)**.\n[1] Chen, Xu, Junshan Wang, and Kunqing Xie. \"TrafficStream: A Streaming Traffic Flow Forecasting Framework Based on Graph Neural Networks and Continual Learning.\" Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence (IJCAI-21)\n\n**Weakness 2: Comparison with PatchTST baseline**\n\nWe agree with the reviewer that some long-term forecasting methods are beyond the scope of our research focus. The performance of PatchTST is shown as follows. (We also provide the evaluation results for Dlinear and TimesNet). Our proposed method consistently achieves the best performance. \n\nTraffic-CL:\n| Model | |  |  |  |\n| :---: | :---: | :---: | :---: | :---: |\n|  | AP_MAE | AP_RMSE | AF_MAE | AF_RMSE |\n| PatchTST_seq |19.11 | 32.50  | 2.34  | 2.97 |\n| PatchTST_mir | 19.04 | 32.23  | 2.28 | 2.79 |\n| PatchTST_herd| 18.96 | 32.10  | 2.21  | 2.67 |\n| PatchTST_er|   18.77 |  31.50 | 1.98 | 2.01 |\n| PatchTST_der++| 18.53 |  31.34  | 1.75  | 1.98 |\n| DLinear_seq| 19.69 | 32.75  |2.91  |2.83  |\n| DLinear_mir| 19.37 | 32.25  |2.17  |2.59 |\n| DLinear_herd| 19.53|  32.40 | 2.25 | 2.68 |\n| DLinear_er| 19.19 |  32.30  |1.73  |2.14 |\n| DLinear_der++| 19.02|  31.97  | 1.75  | 1.93 |\n| TimesNet_seq| 17.77 | 29.91 | 3.13 | 6.93 |\n| TimesNet_mir| 17.53 | 29.61 | 2.44 | 5.32 |\n| TimesNet_herd| 17.38 | 29.53 | 2.56 | 5.83 |\n| TimesNet_er| 17.25 | 29.33 | 1.97 | 4.19 |\n| TimesNet_der++| 17.13 | 29.28 | 1.56 | 4.02 |\n| SKI-CL| **15.24** | **25.33** | 1.50 | 2.71|\n\nSolar-CL:\n| Model | |  |  |  |\n| :---: | :---: | :---: | :---: | :---: |\n|  | AP_MAE | AP_RMSE | AF_MAE | AF_RMSE |\n| PatchTST_seq   | 2.64 | 5.32 | 0.72 | 0.43 |\n| PatchTST_mir   | 2.61 | 5.30 | 0.70 | 0.40 |\n| PatchTST_herd  | 2.60 | 5.30 | 0.68 | 0.35 |\n| PatchTST_er   | 2.57 | 5.27 | 0.47 | 0.30 |\n| PatchTST_der++ | 2.53 | 5.17 | 0.43 | 0.28 |\n| DLinear_seq    | 3.47 | 6.56 | 1.17 | 1.12 |\n| DLinear_mir    | 3.45 | 6.51 | 1.02 | 1.01 |\n| DLinear_herd   | 3.41 | 6.50 | 1.03 | 1.00 |\n| DLinear_er     | 3.37 | 6.43 | 0.93 | 0.98 |\n| DLinear_der++  | 3.25 | 6.37 | 0.83 | 0.79 |\n| TimesNet_seq   | 3.92 | 7.18 | 1.46 | 2.51 |\n| TimesNet_mir   | 3.77 | 7.15 | 1.22 | 1.57 |\n| TimesNet_herd  | 3.83 | 7.10 | 1.03 | 1.44 |\n| TimesNet_er    | 3.55 | 7.02 | 0.42 | 0.91 |\n| TimesNet_der++ | 3.45 | 6.55 | 0.37 | 0.90 |\n| SKI-CL                   | **1.75** | **4.46** | 0.09 | 0.06 |\n\n\nHAR-CL:\n\n| Model | |  |  |  |\n| :---: | :---: | :---: | :---: | :---: |\n|  | AP_MAE | AP_RMSE | AF_MAE | AF_RMSE |\n| PatchTST_seq   | 17.91 | 27.13 | 7.18 | 6.88 |\n| PatchTST_mir  | 17.82 | 26.89 | 6.82 | 4.81 |\n| PatchTST_herd>  | 17.73 | 26.84 | 6.62 | 4.79 |\n| PatchTST_er   | 17.57 | 26.40 | 6.02 | 4.69 |\n| PatchTST_der++ | 17.12 | 26.13 | 5.79 | 4.32 |\n| DLinear_seq    | 17.32 | 26.31 | 2.71 | 3.43 |\n| DLinear_mir    | 16.87 | 26.12 | 2.67 | 3.01 |\n| DLinear_herd   | 16.83 | 25.81 | 2.57 | 2.91 |\n| DLinear_er    | 16.71 | 25.75 | 2.13 | 2.85 |\n| DLinear_der++  | 16.58 | 25.47 | 1.92 | 2.77 |\n| TimesNet_seq   | 18.38 | 27.61 | 4.33 | 5.15 |\n| TimesNet_mir   | 18.27 | 27.59 | 4.01 | 5.08 |\n| TimesNet_herd  | 18.01 | 27.53 | 3.46 | 5.03 |\n| TimesNet_er | 17.84 | 27.07 | 3.28 | 4.01 |\n| TimesNet_der++ | 17.73 | 26.86 | 3.11 | 3.87 |\n| SKI-CL                   | **13.41** | **21.30** | 1.64 | 2.08 |\n\nPlease see the next cell for experiment results for Synthetic-CL dataset"
                    },
                    "title": {
                        "value": "Response to Reviewer RPp8 (1)"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4554/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700013439773,
                "cdate": 1700013439773,
                "tmdate": 1700019177085,
                "mdate": 1700019177085,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Zk0w9B0GOo",
                "forum": "URCfZ2NgaR",
                "replyto": "k4kbxLy9ey",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4554/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4554/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Synthetic-CL:\n\n| Model | |  |  |  |\n| :---: | :---: | :---: | :---: | :---: |\n|  | AP_MAE | AP_RMSE | AF_MAE | AF_RMSE |\n| PatchTST_seq   | 4.85 | 5.93 | 1.59 | 1.78 |\n| PatchTST_mir   | 4.83 | 5.86 | 1.55 | 1.72 |\n| PatchTST_herd  | 4.80 | 5.79 | 1.43 | 1.68 |\n| PatchTST_er    | 4.72 | 5.26 | 1.03 | 1.54 |\n| PatchTST_der++ | 4.64 | 5.13 | 0.83 | 0.88 |\n| DLinear_seq    | 4.81 | 5.81 | 1.64 | 1.57 |\n| DLinear_mir    | 4.79 | 5.73 | 1.47 | 1.40 |\n| DLinear_herd   | 4.77 | 5.70 | 1.59 | 1.46 |\n| DLinear_er     | 4.74 | 5.20 | 1.23 | 1.43 |\n| DLinear_der++  | 4.21 | 4.88 | 1.12 | 1.13 |\n| TimesNet_seq   | 5.18 | 6.13 | 1.72 | 2.03 |\n| TimesNet_mir   | 5.12 | 6.05 | 1.69 | 1.97 |\n| TimesNet_herd  | 5.10 | 6.03 | 1.68 | 1.95 |\n| TimesNet_er.   | 4.93 | 5.90 | 1.42 | 1.90 |\n| TimesNet_der++ | 4.81 | 5.88 | 1.32 | 1.78 |\n| SKI-CL                   | **3.24** | **4.24** | 0.15 | 0.23 |"
                    },
                    "title": {
                        "value": "Response to Reviewer RPp8 (2)"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4554/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700013463004,
                "cdate": 1700013463004,
                "tmdate": 1700019188766,
                "mdate": 1700019188766,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2OvOwECqGA",
                "forum": "URCfZ2NgaR",
                "replyto": "5bnyu4iAPC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4554/Reviewer_RPp8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4554/Reviewer_RPp8"
                ],
                "content": {
                    "title": {
                        "value": "Maintain original score"
                    },
                    "comment": {
                        "value": "All of my concerns have been addressed, and I maintain my original score."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4554/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700361462644,
                "cdate": 1700361462644,
                "tmdate": 1700361462644,
                "mdate": 1700361462644,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "s4ZODokOmV",
            "forum": "URCfZ2NgaR",
            "replyto": "URCfZ2NgaR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4554/Reviewer_YCYx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4554/Reviewer_YCYx"
            ],
            "content": {
                "summary": {
                    "value": "This paper propose a novel Structural Knowledge Informed Continual Learning (SKI-CL) framework to perform MTS forecasting under the\ncontinual learning setting, which leverages the structural knowledge to characterize the dynamic variable dependencies within each regime.\n\n\n\nIn my opinion, the proposed dynamic graph learning module is not very novel, and many papers have used this structure, such as adaptive GCN, for forecasting. And the main contribution is applying your model in a continuous learning setting, which is not enough for ICLR."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.  The paper is well written and easy to understand\n\n2. The paper proposes incorporating knowledge for Dependencies Characterization."
                },
                "weaknesses": {
                    "value": "1. My main concern is: why not compare it with a series of Sota time series models? And why not compare it with other continuing learning models?\n\n2. The dynamic structure learning is not novel."
                },
                "questions": {
                    "value": "What is structural knowledge? prior knowledge? or learned knowledge?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4554/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699443828333,
            "cdate": 1699443828333,
            "tmdate": 1699636432880,
            "mdate": 1699636432880,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Vd2SNTlJDO",
                "forum": "URCfZ2NgaR",
                "replyto": "s4ZODokOmV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4554/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4554/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We sincerely thank the reviewer for recognizing our contribution and the constructive comments. The answers to the concerns and questions are listed below:\n\n**Weakness1 : Concerns about comparisons with other SOTA baselines**\n\nThank you for the comments, this paper mainly focuses on continual learning setting of multivariate short-term forecasting and graph inference. We have compared our method with the multiple state-of-the-art graph learning backbones (that model variable dependencies for forecasting). We included results of autoformer, which currently is the third place of short-term forecasting tasks according to TSLib[1]. Regarding the reviewer\u2019s concern, here we present more state-of-the-art model results, including PatchTST[2], Dlinear[3] and TimesNet[4] as below. As for the continual learning method, we have already included commonly used baselines for comparison. Now we also adapt and use MIR[5] for memory-replay continual learning baselines. Our proposed method consistently achieves best performance and outperform all baselines. \n\nTraffic-CL:\n| Model | |  |  |  |\n| :---: | :---: | :---: | :---: | :---: |\n|  | AP_MAE | AP_RMSE | AF_MAE | AF_RMSE |\n| PatchTST_seq |19.11 | 32.50  | 2.34  | 2.97 |\n| PatchTST_mir | 19.04 | 32.23  | 2.28 | 2.79 |\n| PatchTST_herd| 18.96 | 32.10  | 2.21  | 2.67 |\n| PatchTST_er|   18.77 |  31.50 | 1.98 | 2.01 |\n| PatchTST_der++| 18.53 |  31.34  | 1.75  | 1.98 |\n| DLinear_seq| 19.69 | 32.75  |2.91  |2.83  |\n| DLinear_mir| 19.37 | 32.25  |2.17  |2.59 |\n| DLinear_herd| 19.53|  32.40 | 2.25 | 2.68 |\n| DLinear_er| 19.19 |  32.30  |1.73  |2.14 |\n| DLinear_der++| 19.02|  31.97  | 1.75  | 1.93 |\n| TimesNet_seq| 17.77 | 29.91 | 3.13 | 6.93 |\n| TimesNet_mir| 17.53 | 29.61 | 2.44 | 5.32 |\n| TimesNet_herd| 17.38 | 29.53 | 2.56 | 5.83 |\n| TimesNet_er| 17.25 | 29.33 | 1.97 | 4.19 |\n| TimesNet_der++| 17.13 | 29.28 | 1.56 | 4.02 |\n| SKI-CL| **15.24** | **25.33** | 1.50 | 2.71|\n\n\nSolar-CL:\n| Model | |  |  |  |\n| :---: | :---: | :---: | :---: | :---: |\n|  | AP_MAE | AP_RMSE | AF_MAE | AF_RMSE |\n| PatchTST_seq   | 2.64 | 5.32 | 0.72 | 0.43 |\n| PatchTST_mir   | 2.61 | 5.30 | 0.70 | 0.40 |\n| PatchTST_herd  | 2.60 | 5.30 | 0.68 | 0.35 |\n| PatchTST_er   | 2.57 | 5.27 | 0.47 | 0.30 |\n| PatchTST_der++ | 2.53 | 5.17 | 0.43 | 0.28 |\n| DLinear_seq    | 3.47 | 6.56 | 1.17 | 1.12 |\n| DLinear_mir    | 3.45 | 6.51 | 1.02 | 1.01 |\n| DLinear_herd   | 3.41 | 6.50 | 1.03 | 1.00 |\n| DLinear_er     | 3.37 | 6.43 | 0.93 | 0.98 |\n| DLinear_der++  | 3.25 | 6.37 | 0.83 | 0.79 |\n| TimesNet_seq   | 3.92 | 7.18 | 1.46 | 2.51 |\n| TimesNet_mir   | 3.77 | 7.15 | 1.22 | 1.57 |\n| TimesNet_herd  | 3.83 | 7.10 | 1.03 | 1.44 |\n| TimesNet_er    | 3.55 | 7.02 | 0.42 | 0.91 |\n| TimesNet_der++ | 3.45 | 6.55 | 0.37 | 0.90 |\n| SKI-CL                   | **1.75** | **4.46** | 0.09 | 0.06 |\n\nHAR-CL:\n\n| Model | |  |  |  |\n| :---: | :---: | :---: | :---: | :---: |\n|  | AP_MAE | AP_RMSE | AF_MAE | AF_RMSE |\n| PatchTST_seq   | 17.91 | 27.13 | 7.18 | 6.88 |\n| PatchTST_mir  | 17.82 | 26.89 | 6.82 | 4.81 |\n| PatchTST_herd>  | 17.73 | 26.84 | 6.62 | 4.79 |\n| PatchTST_er   | 17.57 | 26.40 | 6.02 | 4.69 |\n| PatchTST_der++ | 17.12 | 26.13 | 5.79 | 4.32 |\n| DLinear_seq    | 17.32 | 26.31 | 2.71 | 3.43 |\n| DLinear_mir    | 16.87 | 26.12 | 2.67 | 3.01 |\n| DLinear_herd   | 16.83 | 25.81 | 2.57 | 2.91 |\n| DLinear_er    | 16.71 | 25.75 | 2.13 | 2.85 |\n| DLinear_der++  | 16.58 | 25.47 | 1.92 | 2.77 |\n| TimesNet_seq   | 18.38 | 27.61 | 4.33 | 5.15 |\n| TimesNet_mir   | 18.27 | 27.59 | 4.01 | 5.08 |\n| TimesNet_herd  | 18.01 | 27.53 | 3.46 | 5.03 |\n| TimesNet_er | 17.84 | 27.07 | 3.28 | 4.01 |\n| TimesNet_der++ | 17.73 | 26.86 | 3.11 | 3.87 |\n| SKI-CL                   | **13.41** | **21.30** | 1.64 | 2.08 |\n\nSynthetic-CL:\n\n| Model | |  |  |  |\n| :---: | :---: | :---: | :---: | :---: |\n|  | AP_MAE | AP_RMSE | AF_MAE | AF_RMSE |\n| PatchTST_seq   | 4.85 | 5.93 | 1.59 | 1.78 |\n| PatchTST_mir   | 4.83 | 5.86 | 1.55 | 1.72 |\n| PatchTST_herd  | 4.80 | 5.79 | 1.43 | 1.68 |\n| PatchTST_er    | 4.72 | 5.26 | 1.03 | 1.54 |\n| PatchTST_der++ | 4.64 | 5.13 | 0.83 | 0.88 |\n| DLinear_seq    | 4.81 | 5.81 | 1.64 | 1.57 |\n| DLinear_mir    | 4.79 | 5.73 | 1.47 | 1.40 |\n| DLinear_herd   | 4.77 | 5.70 | 1.59 | 1.46 |\n| DLinear_er     | 4.74 | 5.20 | 1.23 | 1.43 |\n| DLinear_der++  | 4.21 | 4.88 | 1.12 | 1.13 |\n| TimesNet_seq   | 5.18 | 6.13 | 1.72 | 2.03 |\n| TimesNet_mir   | 5.12 | 6.05 | 1.69 | 1.97 |\n| TimesNet_herd  | 5.10 | 6.03 | 1.68 | 1.95 |\n| TimesNet_er.   | 4.93 | 5.90 | 1.42 | 1.90 |\n| TimesNet_der++ | 4.81 | 5.88 | 1.32 | 1.78 |\n| SKI-CL                   | **3.24** | **4.24** | 0.15 | 0.23 |"
                    },
                    "title": {
                        "value": "Response to Reviewer YCYx (1)"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4554/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700013598472,
                "cdate": 1700013598472,
                "tmdate": 1700018887367,
                "mdate": 1700018887367,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gRfnmOXm1k",
                "forum": "URCfZ2NgaR",
                "replyto": "s4ZODokOmV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4554/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4554/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Weakness2: The dynamic structure learning is not novel.**\n\nThe existing literature either learns the structure via training parameters or representations. Our novelty lies in the role of dynamic structure learning for regime characterization, rather than an individual structure learning component. It is important that we impose the consistency regularization in the dynamic structure learning process, which aligns the learned structure with the universal and task-irrelevant structural knowledge to characterize a specific regime. The joint structure modeling based on both components leads to an capability that the existing backbones cannot achieve, i.e., **during the inference stage, our model can automatically infer a consistent structure solely based on the time series data, without knowing the regime and accessing the memory buffer**, as shown in **Figure 1: SKI-CL: Testing (page 2) and validated in Figure 4 (page 9)**. Furthermore, we are the first to explore and test different structure learning scenarios regarding the formulation of edge variables (discrete or continuous) and availability of structural knowledge (completed or partial) for multivariate time series forecasting, with visualizations in **Figures 5-8 in appendix (page 19-21)**.\n\n\n**Question 1: Questions about structure knowledge**\n\nThanks for raising this question. As we discussed in the introduction (**the last three lines in page 1 and the first six lines in page 2**) and literature review(**section 2.1 lines 10-17**), the structural knowledge can be prior knowledge (e.g., physical constraints like traffic network and power grids, domain knowledge of application scenarios like Mel-frequency cepstral coefficients) or the so-called \u2018learned\u2019 knowledge if the structure is directly inferred from data by traditional statistical methods(e.g., correlation, k-NN), or learned and inferred individually (e.g., transfer entropy/granger causality). \n\n\n[1]THUML. \"Time-Series-Library.\" GitHub repository, URL: https://github.com/thuml/Time-Series-Library. Accessed on November 12, 2023\n\n[2]Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, Jayant Kalagnanam \"A time series is worth 64 words: Long-term forecasting with transformers.\" in Proceedings of the International Conference on Learning Representations (2023)\n\n[3]Zeng, Ailing, Muxi Chen, Lei Zhang, and Qiang Xu. \"Are transformers effective for time series forecasting?.\" In Proceedings of the AAAI conference on artificial intelligence(2023)\n\n[4]Haixu Wu and Tengge Hu and Yong Liu and Hang Zhou and Jianmin Wang and Mingsheng Long. \u201dTimesNet: Temporal 2D-Variation Modeling for General Time Series Analysis.\u201d in Proceedings of the International Conference on Learning Representations (2023)\n\n[5]Rahaf, Aljundi, and Caccia Lucas. \"Online Continual Learning with Maximally Interfered Retrieval.\" Advances in neural information processing systems 32 (2019)."
                    },
                    "title": {
                        "value": "Response to Reviewer YCYx (2)"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4554/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700013685643,
                "cdate": 1700013685643,
                "tmdate": 1700018999449,
                "mdate": 1700018999449,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]