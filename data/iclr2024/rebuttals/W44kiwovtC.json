[
    {
        "title": "FastDCFlow: Fast and Diverse Counterfactual Explanations Using Normalizing Flows"
    },
    {
        "review": {
            "id": "SI7a3frWiD",
            "forum": "W44kiwovtC",
            "replyto": "W44kiwovtC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2663/Reviewer_uvnA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2663/Reviewer_uvnA"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a methodology for computing counterfactual explanations using normalizing flows."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Very interesting and novel idea of using normalizing flows\n- Paper is well structured and mostly well written -- see Section \"Weaknesses\" for some criticism"
                },
                "weaknesses": {
                    "value": "- Experimental results do not look that convincing to me -- often other methods outperform the proposed method. Not sure how much sense the corrected scores make -- I think it would be better to compare different aspects directly, instead of merging them all into a single score\n- Training a generation mechanism (e.g. the proposed method) might not be possible or difficult if only very few training samples are available. This might hinder the use of the proposed method in case of high-dimensional data space where only few samples are available\n- Access to model internals are needed -- otherwise a gradient-based optimization method can not be applied to the proposed loss function for training the model. To me this looks as a another limitation of the proposed method\n-  In general, I miss a discussion of limitations and drawbacks of the proposed method"
                },
                "questions": {
                    "value": "-  I am not sure how fair/appropriate the runtime comparison is: Other methods (non-generative methods) where never designed to be trained on data, so it is somewhat clear that these will not be able to compete with a trained generative method. On the other hand, building/training a generative method requires a lot of training data which poses a major disadvantage compared to other non-generative methods. I think a comparison of these very different methods is challenging -- maybe the authors can elaborate on this a bit more."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2663/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698215929078,
            "cdate": 1698215929078,
            "tmdate": 1699636206952,
            "mdate": 1699636206952,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fE0397wnO4",
                "forum": "W44kiwovtC",
                "replyto": "SI7a3frWiD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2663/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2663/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your valuable review"
                    },
                    "comment": {
                        "value": "Dear Reviewer uvnA,\n\nThank you for your valuable feedback and suggestions on improving our paper. Here we address responses to your questions.\n\nAt the beginning, we summarize significant revisions to this paper.\n\n**1) Modification in Implementation**\n\nUpon reviewing our source code, we discovered that for all model-based methods (CF_VAE, CF_CVAE, CF_DUVAE, FastDCFlow), the training data used was limited to samples where the ML prediction was < 0.5. However, the correct approach is to utilize the entire training dataset regardless of the predicted values. We have corrected this in our source code, retrained these models, and updated the results in our paper accordingly.\n\n**2) Revision of Experimental Goals**\n\nWe have revised the manuscript to reflect our aim of developing a model that performs comparably to Input-based methods, with reduced computational time. Our objective is not to develop a model that excels in every evaluation metric but to create a model that is faster than Input-based methods while still maintaining balanced high performance across all metrics. While Input-based methods have the advantage of learning from each input and hence higher representational power, they are also time-consuming. We have incorporated the results of our revised experiments and discussed our findings in relation to the paper's objectives."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2663/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740697441,
                "cdate": 1700740697441,
                "tmdate": 1700740697441,
                "mdate": 1700740697441,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "tFvlRBLYB3",
            "forum": "W44kiwovtC",
            "replyto": "W44kiwovtC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2663/Reviewer_TXsm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2663/Reviewer_TXsm"
            ],
            "content": {
                "summary": {
                    "value": "The is paper is concerned with extending the applicability of counterfactual explanation to tabular data, as well as increasing the computational efficiency of producing such explanations.  The authors approach is to propose a normalizing flow along with an ordinal variable encoding to account for cost of perturbations."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The authors experiments help to identify why standard VAEs fail to produce a good amount of variation in their generated CEs."
                },
                "weaknesses": {
                    "value": "- Experimentally, the authors omit a comparison to CeFLow of Duong et al 2023, which is the most natural comparator to their method (being itself a CF model for tabular data based on normalizing flows).  It's notable in its absence, how come?\n- The authors explanation of how diversity is to be generated in CEs is too brief.  Section 3.2 very briefly states that the temperature parameter $t$ is used to add noise to the origins of the samples of $\\mathbf{z}_{test}$.  But does this offer *more* diversity of valid samples than a method like DiCE, which intentionally penalizes the covariance between sampled points?  It's unsatisfying.\n- The first paragraph in section 3 discusses some notation for CE generation, but states that: \n> The aim of CE is to generate perturbed inputs $\\mathbf{x}_{cf}$ such that $f(\\mathbf{x}_{cf})>f(\\mathbf{x})$ for the observed input x.\n\nThere are two mistakes here.  First is that  $>$ should be $!=$.  Second, it is far from settled what the benefits of CE are for, and is largely dependent on the person using the tool.  Users will take different insights from CEs than people building models, for example.  My reading of the paper of the paper suggests the authors are intending FastDCFlow to be a user-centric tool, so they should inform their perspective and their experiments accordingly.  Could they do a simulated user study, showing that FastDCFlow helps users make better decisions?  Or could they take an organizational risk perspective, showing that FastDCFlow helps an organization providing a model-based service make better (e.g fairer) decisions?"
                },
                "questions": {
                    "value": "- The second paragraph of the conclusion starts out with \n> In subsequent phases of this study, we are currently integrating TE with a transformation technique that respects the order of categorical variables. \n\nbut section 6.1 suggests that TE was integrated for the experiment in this model, so I'm confused.  Is this just an error in tense?  Or does the present implementation of TE not respect order?  Or something else?\n- The same paragraph of the conclusion continues\n> Although predicted target values between the ML model evaluations using TE and OHE showed no marked differences, effectively adapting conversion methods for compact datasets remain a challenge.\n\nI believe this statement needs way more unpacking.  If there were \u201cno marked differences\u201d between TE and OHE, I see two important questions that should be asked here:\n1. Why is this so?  Is this a consequence of the evaluation criteria for CEs?\n2. If OHE and TE don\u2019t show a marked difference, what\u2019s the justification for pursuing the integration of TE?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2663/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698548354403,
            "cdate": 1698548354403,
            "tmdate": 1699636206854,
            "mdate": 1699636206854,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EKOeeigneR",
                "forum": "W44kiwovtC",
                "replyto": "tFvlRBLYB3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2663/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2663/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your valuable review"
                    },
                    "comment": {
                        "value": "Dear Reviewer TXsm,\n\nThank you for your valuable feedback and suggestions on improving our paper. Here we address responses to your questions.\n\nAt the beginning, we summarize significant revisions to this paper.\n\n**1) Modification in Implementation**\n\nUpon reviewing our source code, we discovered that for all model-based methods (CF_VAE, CF_CVAE, CF_DUVAE, FastDCFlow), the training data used was limited to samples where the ML prediction was < 0.5. However, the correct approach is to utilize the entire training dataset regardless of the predicted values. We have corrected this in our source code, retrained these models, and updated the results in our paper accordingly.\n\n**2) Revision of Experimental Goals**\n\nWe have revised the manuscript to reflect our aim of developing a model that performs comparably to Input-based methods, with reduced computational time. Our objective is not to develop a model that excels in every evaluation metric but to create a model that is faster than Input-based methods while still maintaining balanced high performance across all metrics. While Input-based methods have the advantage of learning from each input and hence higher representational power, they are also time-consuming. We have incorporated the results of our revised experiments and discussed our findings in relation to the paper's objectives.\n\n**Q1.** , the authors omit a comparison to CeFLow of Duong et al 2023, which is the most natural comparator to their method (being itself a CF model for tabular data based on normalizing flows). It's notable in its absence, how come?\n\n**A1.** CeFlow proposes transformations by LabelEncoding and variational dequation [1] for categorical variables, and we determined that a fair comparison could not be made.\n\n**Q2.** The authors explanation of how diversity is to be generated in CEs is too brief. Section 3.2 very briefly states that the temperature parameter\u00a0is used to add noise to the origins of the samples. But does this offer\u00a0more\u00a0diversity of valid samples than a method like DiCE, which intentionally penalizes the covariance between sampled points?\n\n**A2.** In our paper, we demonstrate experimentally that diversity can be enhanced in a simple manner without the need for explicit loss addition, as done in DiCE []. To further substantiate this result, we directly applied the diversity term used in DiCE to the CFs generated by FastDCFlow and tracked its progression with respect to temperature, as shown in Figure 4 of our paper. The results confirm that as the temperature increases, the diversity term of DiCE also improves. This indicates that our approach to diversity, though simpler, is effective and comparable to methods like DiCE that intentionally penalize the covariance between sampled points.\n\n**Q3.** First is that\u00a0>\u00a0should be\u00a0!. \n\n**A3.** Thank you for your observation. Indeed, in the context of CEs, it is not always mandatory to satisfy constraints, hence we have revised the notation to reflect that it 'should' be so, rather than 'must'. This change more accurately represents the nature of constraints in counterfactual explanations."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2663/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740597151,
                "cdate": 1700740597151,
                "tmdate": 1700740597151,
                "mdate": 1700740597151,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5JkbSlO6W6",
            "forum": "W44kiwovtC",
            "replyto": "W44kiwovtC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2663/Reviewer_KKcY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2663/Reviewer_KKcY"
            ],
            "content": {
                "summary": {
                    "value": "This paper lays out a method to obtain counterfactual explanations (CEs) of machine learning (ML) models, employing normalizing flows to generate candidate CEs. Target Encoding (TE) is utilized to maintain some level of ordinality amongst categorical features."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- Results are mostly on par with current state of the art, dependent on the criteria end users are seeking\n- CF parameters are analyzed in a useful way\n- Besides some typos, the paper is well written, with clear formatting and a logical structure"
                },
                "weaknesses": {
                    "value": "- The main problem in the paper is its lack of novelty. The two main contributions involve a) using a latent space model and b) converting categorical features to continuous mappings. The first has been proposed many times in the literature, mostly using VAEs. Normalizing flows have also been used before in this context, as the paper references. The transformation of categorical features to continuous features is not new either, and as such, I find the paper's novelty somewhat lacking.\n- When considering the usual metrics proximity (P), validity (V) and run time (RT), the proposed method does not perform best across any one metric. The diversity metrics are questionable since they do not appear normalized, thus worse proximity is likely to promote better diversity (see questions).\n- The results therefore rely heavily on the proposed metrics, CV and CS, which themselves are left highly unjustified in the text. I do not find these functions particularly compelling, since CS relies on CV, which itself relies on the diversity metrics proposed."
                },
                "questions": {
                    "value": "1. Based on Table 1, Inner Diversity (ID) and Outer Diversity (OD) compute the average $\\ell_2$ differences between CEs for one test point and between CEs across multiple test points. Why was diversity not considered via the angle between CE perturbations rather than the $\\ell_2$ norm between raw CEs? Using the $\\ell_2$ norm alone means larger diversity can be achieved through CEs with very bad proximity.\n2. For the Bank dataset, FastDCFlow achieves proximity an order of magnitude worse than other methods (this is serious), and also achieves worse validity. Yet, FastDCFlow achieves an order of magnitude higher performance on the CS metric which is used for the final assessment of the methods. Proximity and validity are the two fundamental goals of counterfactual explanations, and in this case FastDCFlow fails on both fronts while being pushed as the method with the best overall score. Can the authors please provide further justification of the CS metric (mostly the diversity evaluation in the CV metric) in the context of the above example?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2663/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2663/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2663/Reviewer_KKcY"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2663/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698610883246,
            "cdate": 1698610883246,
            "tmdate": 1699636206768,
            "mdate": 1699636206768,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WYCHm6n8lP",
                "forum": "W44kiwovtC",
                "replyto": "5JkbSlO6W6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2663/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2663/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your valuable review"
                    },
                    "comment": {
                        "value": "Dear Reviewer KKcY,\n\nThank you for your valuable feedback and suggestions on improving our paper. Here we address responses to your questions.\n\n**Q1.** The main problem in the paper is its lack of novelty. The two main contributions involve a) using a latent space model and b) converting categorical features to continuous mappings.\n\n**A1.** Our aim is not to propose novelty in the model or transformation method per se, but rather, we find novelty in the application of model-based counterfactual explanations.\n\nRegarding point a), previous model-based methods, as indicated by [1], have primarily been based on VAEs, which tend to generate identical CFs for different inputs, failing to capture diversity. To our knowledge, there are no precedents for applying Normalizing flows in model-based methods, and it is in this aspect that our work introduces an innovative application in the context of counterfactual explanations (CEs). While we utilize the RealNVP [2] structure, our contribution lies in the novel approach of introducing perturbations in the latent space through the reparametrization trick and modifications in the loss function.\n\nAs for point b), many existing methods, as referenced in [3, 4], primarily apply OneHotEncoding or LabelEncoding for the transformation of categorical variables. We highlight in our paper that perturbing these encodings can compromise the diversity of predicted values. The idea of introducing TargetEncoding for the perturbation of categorical variables has not been discussed in the cited literature. Our work delves into the effectiveness of TE in the context of CEs, thereby contributing a novel perspective to this area.\n\n**Q2.** When considering the usual metrics proximity (P), validity (V) and run time (RT), the proposed method does not perform best across any one metric.\n\n**A2.** To clarify our objectives, we have made revisions to both the implementation and the goals of our study.\n\n**1) Modification in Implementation**\n\nUpon reviewing our source code, we discovered that for all model-based methods (CF_VAE, CF_CVAE, CF_DUVAE, FastDCFlow), the training data used was limited to samples where the ML prediction was < 0.5. However, the correct approach is to utilize the entire training dataset regardless of the predicted values. We have corrected this in our source code, retrained these models, and updated the results in our paper accordingly.\n\n**2) Revision of Experimental Goals**\n\nWe have revised the manuscript to reflect our aim of developing a model that performs comparably to Input-based methods, with reduced computational time. Our objective is not to develop a model that excels in every evaluation metric but to create a model that is faster than Input-based methods while still maintaining balanced high performance across all metrics. While Input-based methods have the advantage of learning from each input and hence higher representational power, they are also time-consuming. We have incorporated the results of our revised experiments and discussed our findings in relation to the paper's objectives.\n\n**Q3.** Why was diversity not considered via the angle between CE perturbations rather than the\u00a0\u21132\u00a0norm between raw CEs? Using the\u00a0\u21132\u00a0norm alone means larger diversity can be achieved through CEs with very bad proximity.\n\n**A3.** We believe that the use of the \u21132 norm is appropriate for our analysis. Firstly, it is important to recognize that proximity and diversity are not necessarily in a trade-off relationship. For instance, if the centroid of a CF set is distant from the test input, the performance in terms of proximity will deteriorate regardless of the diversity within the CF set. Therefore, situations where both proximity and diversity performances are poor can occur. Furthermore, the comparison of distances between raw CEs and the \u21132 norm of perturbation vectors (the deviation from the original test input) is essentially equivalent. We have made modifications to the manuscript to clarify this fact more effectively.\n\n**Q4.** For the Bank dataset, FastDCFlow achieves proximity an order of magnitude worse than other methods (this is serious), and also achieves worse validity."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2663/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740398240,
                "cdate": 1700740398240,
                "tmdate": 1700740398240,
                "mdate": 1700740398240,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]