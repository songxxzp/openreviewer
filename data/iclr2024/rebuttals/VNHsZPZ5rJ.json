[
    {
        "title": "Targeted Model Inversion: Distilling Style Encoded in Predictions"
    },
    {
        "review": {
            "id": "6hSg1Kikr4",
            "forum": "VNHsZPZ5rJ",
            "replyto": "VNHsZPZ5rJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5729/Reviewer_wJqb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5729/Reviewer_wJqb"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a new Model Inversion (MI) method that exploits the hierarchical latent features of a separately trained StyleGAN architecture. A goal of MI is to recover the input x of a classifier given the output probability vector y. The MI model can then be used for malicious attacks aimed at extracting private information, since the predictions y are easier to obtain than x in most privacy regulations. The new MI methods show substantially higher performance than a wide range of both black-box and white box baselines on several image datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The approach is simple but well-motivated.\n- The recovered images are substantially better than the baseline models both qualitatively and quantitatively. \n-The paper is well-written and the methodology is well-explained. \n-The experiments are rigorous and rather comprehensive."
                },
                "weaknesses": {
                    "value": "I am struggling to find a good societal application of this work as the explicit aim of the paper is to improve performance of a family of malicious attacks that can be used to leak private information. While I do agree that open research on attacks is important, it seems to me that this work is very helpful to potential attackers while not providing any real insight concerning possible defense strategies.  Note that the paper does not introduce a conceptually novel way to perform attacks, which would provide important information to the public. Instead, it offers a highly optimized approach that exploits several, rather standard, techniques. For this reason, I am not convinced that a paper like this has a place in a machine learning conference.\n\nI do appreciate the technical skills shown by the authors, I think that equal effort should be spent in considering the societal implications and in discussing possible defense strategies.\n\nApart from ethical consideration, I find the domain of application to be rather narrow and more suitable for a more specialized venue. All in all, the paper does not introduce any conceptually new technique since the use of proxy classifiers and generative models is common in the reconstruction literature.\n\nUpdate:\nGiven the author's revision, I decided to increase my score to 6 and to recommend acceptance."
                },
                "questions": {
                    "value": "Could you discuss the ethical implications of your work and provide some insights concerning possible defense methods?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Privacy, security and safety"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "The paper reads too much as a manual on how to perform malicious attacks. I do see the value of attack research but I think that this might cross the line.\n\nUpdate:\nI believe that the authors responded appropriately to my concerns, which are addressed in the revised manuscript"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5729/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5729/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5729/Reviewer_wJqb"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5729/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698414483792,
            "cdate": 1698414483792,
            "tmdate": 1700575750058,
            "mdate": 1700575750058,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Neoq7IABXH",
                "forum": "VNHsZPZ5rJ",
                "replyto": "6hSg1Kikr4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5729/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5729/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer wJqb (1/2)"
                    },
                    "comment": {
                        "value": "We appreciate your helpful feedback. We address each concern in the following:\n\n**Narrow domain of application; nothing conceptually new**\n\nPlease note that we demonstrated that retraining a mapping network in StyleGAN contributes to better distilling styles in prediction vectors, which provides promising initial datapoints for MI in the inversion phase. We also extensively evaluated the efficacy of this new MI attack, demonstrating the feasibility of distilling style in prediction for better MI. \n\nTo demonstrate the wide range of TMI\u2019s applicability, we conducted two additional experiments: (1) using another generative model as prior, and (2) adopting TMI to a white-box attack scenario.\n\nFor (1), we replaced StyleGAN with UNet-GAN [6], and compared the results to the base experiment in the table below. For the surrogate model $f\u2019$, we used the left-half of the UNet-GAN\u2019s discriminator. Since UNet-GAN does not incorporate a mapping network, a custom mapping layer was trained from scratch. Observe that while the overall performance is slightly decreased compared to TMI with StyleGAN, it is still more effective than MIRROR-b. The slight drop in performance is largely due to the entangled latent space of UNet-GAN, where the latent space $\\\\mathcal{Z}$ is directly used without mapping it to an intermediate disentangled latent space $\\\\mathcal{W}$ in advance. We conclude that while StyleGAN is still the most effective image prior to be utilized, other GAN-based methods can generally benefit from our suggested approach of leveraging a new mapping layer and distilling the discriminator for a surrogate model. Therefore, the contribution of TMI is not only limited to the packaged attack method as a whole, but also the individual components that can be applied independently.\n\n| | **Acc@1 \u2191** | **Acc@5 \u2191** | **F-dist\u2193** | **Cover \u2191** |\n|-|:-:|:-:|:-:|:-:|\n| TMI | .3804 | .6255 | .2950 | .2067 |\n| TMI with UNet-GAN | .2201 | .4811 | .3254 | .2063 |\n| MIRROR-b | .2026 | .4533 | .3564 | .0613 |\n\nFor (2), we used the original model instead of the surrogate model, and the performance gains are listed in the table below. As expected, the performance is improved by an average of 63.7%, outperforming the white-box attacks across all four metrics. This also indicates that retraining a mapping network can be applied for conducting  white-box TMI attacks to improve their reconstruction performance. We will include this result in A.2.1. Effect of the Surrogate Model.\n\n| | **Performance gain** |\n|:-:|:-:|\n| Acc@1 \u2191 | 0.3408 \u2192 0.8264 |\n| Acc@5 \u2191 | 0.6255 \u2192 0.9534 |\n| F-dist \u2193 | 0.2950 \u2192 0.1975 |\n| Cover \u2191 | 0.2067 \u2192 0.2619 |\n\nOur additional experiment results demonstrate that the distillation process of TMI is a generally-applicable technique which can not only support using other generative models as prior, but also augment existing white-box attacks.\n\nMoreover, prior MI attacks have primarily focused on reconstructing class-generic features, which harms privacy of individuals that directly contribute to the training phase of the target model. Individuals who interact with the target model only after deployment (i.e., their user data are not included in the training set) were not exposed to privacy leakage. However,  our experimental results suggest otherwise. Since TMI is capable of reconstructing the original image specific to an output prediction, it demonstrates the privacy threat of reconstructing inputs of ML service users. These results also shed light on information leakage from output predictions."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5729/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700533241661,
                "cdate": 1700533241661,
                "tmdate": 1700567661302,
                "mdate": 1700567661302,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2Ey60AvZdg",
                "forum": "VNHsZPZ5rJ",
                "replyto": "6hSg1Kikr4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5729/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5729/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer wJqb (2/2)"
                    },
                    "comment": {
                        "value": "**Ethical implications and possible defense methods**\n\nWe believe that investigating a new attack and concretizing its performance and impact is an important contribution that motivates further research in mitigating the addressed threat. Numerous previous studies regarding adversarial attacks (e.g. FGSM [1], C&W [2]) and MI attacks (e.g. MIRROR, P&P) proposed novel and powerful attacks demonstrating new upper bounds of robustness and resiliency of ML models against the proposed attacks.\n\nAdditionationally, we will discuss possible mitigation methods in the revised paper. Specifically, we will discuss the downgraded MI performance when the target model only returns a label for a given query (see A.2.4) and another mitigation of injecting random noise to a prediction output (while preserving its prediction label). Observe that its performance dropped below MIRROR-b, in particular, F-dist and Cover metrics are greatly degraded from the original TMI experiment (21.1% and 81.6%). This suggests that the noise successfully distracts TMI from reconstructing subtle features. The table below shows the performance of TMI against target models that inject random noise to prediction outputs. Finally, we will explain the applicability of recent MI defenses [3-5]. \n\n| **Method** | **Acc@1 \u2191** | **Acc@5 \u2191** | **F-dist \u2193** | **Cover \u2191** |\n|-|:-:|:-:|:-:|:-:|\n| TMI (original) | .3804 | .6255 | .2950 | .2067 |\n| Label-only TMI | .2399 | .4800 | .3637 | .1167 |\n| TMI on random noise | .1792 | .4297 | .3738 | .0381 |\n| MIRROR-b | .2026 | .4533 | .3564 | .0613 |\n\n---\n\n>[1] Goodfellow, Ian J., Jonathon Shlens, and Christian Szegedy. \"Explaining and harnessing adversarial examples.\" arXiv preprint arXiv:1412.6572 (2014).\n\n>[2] Carlini, Nicholas, and David Wagner. \"Towards evaluating the robustness of neural networks.\" 2017 IEEE symposium on security and privacy (sp). IEEE, 2017.\n\n>[3] Wang, Tianhao, Yuheng Zhang, and Ruoxi Jia. \"Improving robustness to model inversion attacks via mutual information regularization.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 35. No. 13. 2021.\n\n>[4] Wen, Jing, Siu-Ming Yiu, and Lucas CK Hui. \"Defending against model inversion attack by adversarial examples.\" 2021 IEEE International Conference on Cyber Security and Resilience (CSR). IEEE, 2021.\n\n>[5] Xu, Qian, Md Tanvir Arafin, and Gang Qu. \"An approximate memory based defense against model inversion attacks to neural networks.\" IEEE Transactions on Emerging Topics in Computing 10.4 (2022): 1733-1745.\n\n>[6] Schonfeld, Edgar, Bernt Schiele, and Anna Khoreva. \"A u-net based discriminator for generative adversarial networks.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2020."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5729/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700566687892,
                "cdate": 1700566687892,
                "tmdate": 1700566687892,
                "mdate": 1700566687892,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PpESsUudMT",
                "forum": "VNHsZPZ5rJ",
                "replyto": "2Ey60AvZdg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5729/Reviewer_wJqb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5729/Reviewer_wJqb"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors,\n\nI really appraciated your effort in improving the quality of the submission and in making it more useful for people interested in mitigation.\n\nGiven these improvements, I am happy to increase my score to 6 and to side in favor of acceptance."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5729/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700575588494,
                "cdate": 1700575588494,
                "tmdate": 1700575588494,
                "mdate": 1700575588494,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FwwEfnIALF",
            "forum": "VNHsZPZ5rJ",
            "replyto": "VNHsZPZ5rJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5729/Reviewer_Xyhs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5729/Reviewer_Xyhs"
            ],
            "content": {
                "summary": {
                    "value": "This paper tackles the problem of black-box model inversion. The objective is to obtain the input data sample (or its surrogates) corresponding to a given prediction. While it is an undesirable scenario, it is important to make the attacks stronger and more pragmatic to be able to construct better countermeasures. \n\nPrevious methods on black-box model inversion are slow and do not often correspond to data samples that are specific to the given prediction. \n\nThis paper claims to address these issues effectively by learning a surrogate StyleGAN generator, followed by transforming the label prediction vectors onto the latent space of the generator, to be used for image generation."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The idea of using a surrogate generator and operating on the latent space of the same is good. \n\n2. The paper positions itself well (describing the exact problem and gap in the literature). \n\n3. Writing is fairly good (despite typos)."
                },
                "weaknesses": {
                    "value": "1. Dependence on a surrogate generator (and dataset). \n\n2. Increased space-complexity. \n\n3. Sloppy notations and incomplete math."
                },
                "questions": {
                    "value": "1. The main problem I have about this method is that this demands training of a StyleGAN on a  dataset ``similar'' to that used in the predictor. This is not a fair assumption in my opinion. While the Authors do argue that the attacker can \"leverage a pre trained StyleGAN network available on the Internet\", it is a weak argument. How would the attacker know which dataset is ``similar\" to the one used in the predictor? More grounding is needed in this respect, seems too handwavy currently.  \n\n2. Adding to the above point, the proposed method imposes an additional space constraint, in terms of the large stylegan that is to be trained. \n\n3. Given the above two points, I do not see the comparisons to be fair as the proposed method has the luxury of using an additional full-blow generator network which the previous methods don't have. Therefore, I recommend that the Authors have to try modifying the other methods while they have access to a generator, albeit on a surrogate dataset.\n\n4. The method seems very stylegan specific. Can a different GAN architecture be used? I am asking specifically because, in my experience, the latent spaces of other GANs are not as versatile as that of StyleGAN. \n\n5. The description of the method has to be more formal. For instance, before Eq. 2, it is said that there is a loss function that is getting minimized. The optimization problem has to be stated neatly (using formal math). \n\n6. A lot of mathematical details are missing - What is the expectation over, in Eq. 2 and 3? As I understand, they both are over two different distributions but are not mentioned. \n\n7. While the Authors show a few images when there is a distributional shift between the surrogate model and the predictor, it is very minor and insignificant in my opinion. Both the datasets are facial images. What happens if you take a stylegan trained on cars dataset and use it for a predictor trained on human faces?\n\nOverall, while the method is interesting, I have reservations about recommending it for acceptance given my concerns above. I shall wait to see other reviewers' comments and discussions with authors before making up my mind. Right now, I am leaning towards rejecting it."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5729/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698772885293,
            "cdate": 1698772885293,
            "tmdate": 1699636600066,
            "mdate": 1699636600066,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SEKWFB2jaU",
                "forum": "VNHsZPZ5rJ",
                "replyto": "FwwEfnIALF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5729/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5729/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer Xyhs (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive feedback. We address each concern in the following.\n\n**W1+Q1. How would the attacker know which dataset is similar?**\n\nFor a target ML service for users, it is trivial to know what kind of image they receive as input (face/medical/scenery/animal/...), the meaning of their output (similarity to celebrities/medical diagnosis/geolocation/...), and the characteristics of the output prediction (number of classes, meaning of each class) with little effort. Based on this information, the adversary can choose a pre-trained model or a dataset to use among numerous pre-trained models and datasets readily available on the Internet (e.g., Kaggle alone provides 268,605 datasets across 165 official subjects, including people, health, genetics, and business.). For instance, assume a scenario where the target service implements  geolocation estimation based on photos of scenery [10]. The service can predict which country or continent you are in based on the input photo. Accordingly, the attacker can decide to search for datasets containing landscape images (such as the Google landmarks dataset or InstaCities1M) or related pretrained generators for MI attacks.\n\nWe emphasize that our adversary does not require additional knowledge compared to other prior MI attacks; our adversary leverages an auxiliary dataset that the adversaries that previous black- and white-box MI attacks [1-7] assumed. \n\n**W2+Q2. Imposes an additional space constraint, in terms of the large stylegan that is to be trained**\n\nWe agree that training a StyleGAN network is a computational hurdle. However, building this StyleGAN network is a one-time procedure. After this preparation phase, MI attacks are completely offline while enabling high-fidelity input reconstruction for a large number of queries. \n\n**Q3. Unfair comparison; proposed method has the luxury of using additional full-blow generator network which the previous methods don't have**\n\nWe note that all existing MI attacks [3-9] that utilize an image prior assume the same setting. Among the methods selected for baseline comparison, AMI is the only method that does not involve using a generator network, but still, it requires an auxiliary dataset to train the inverse mapping of the target model. One noticeable difference is that the auxiliary dataset is no longer needed for our TMI attacks that utilize an image prior, if there is a available pre-trained generator.  However, AMI does not support exploiting the pre-trained model.\n\n---\n\n>[1] Fredrikson, Matthew, et al. \"Privacy in pharmacogenetics: An End-to-End case study of personalized warfarin dosing.\" 23rd USENIX security symposium (USENIX Security 14). 2014.\n\n>[2] Yang, Ziqi, et al. \"Neural network inversion in adversarial setting via background knowledge alignment.\" Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security. 2019.\n\n>[3] Zhang, Yuheng, et al. \"The secret revealer: Generative model-inversion attacks against deep neural networks.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2020.\n\n>[4] Han, Gyojin, et al. \"Reinforcement Learning-Based Black-Box Model Inversion Attacks.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\n>[5] Kahla, Mostafa, et al. \"Label-only model inversion attacks via boundary repulsion.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.\n\n>[6] An, Shengwei, et al. \"Mirror: Model inversion for deep learning network with high fidelity.\" Proceedings of the 29th Network and Distributed System Security Symposium. 2022.\n\n>[7] Struppek, Lukas, et al. \"Plug & Play Attacks: Towards Robust and Flexible Model Inversion Attacks.\" International Conference on Machine Learning. PMLR, 2022.\n\n>[8] Chen, Si, et al. \"Knowledge-enriched distributional model inversion attacks.\" Proceedings of the IEEE/CVF international conference on computer vision. 2021.\n\n>[9] Wang, Kuan-Chieh, et al. \"Variational model inversion attacks.\" Advances in Neural Information Processing Systems 34 (2021): 9706-9719.\n\n>[10] Weyand, Tobias, Ilya Kostrikov, and James Philbin. \"Planet-photo geolocation with convolutional neural networks.\" Computer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VIII 14. Springer International Publishing, 2016."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5729/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700532953352,
                "cdate": 1700532953352,
                "tmdate": 1700616541494,
                "mdate": 1700616541494,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GLRVt7Kzm3",
                "forum": "VNHsZPZ5rJ",
                "replyto": "FwwEfnIALF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5729/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5729/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer Xyhs (2/2)"
                    },
                    "comment": {
                        "value": "**Q4. Can a different GAN architecture be used?**\n\n| | **Acc@1 \u2191** | **Acc@5 \u2191** | **F-dist\u2193** | **Cover \u2191** |\n|-|:-:|:-:|:-:|:-:|\n| TMI | .3804 | .6255 | .2950 | .2067 |\n| TMI with UNet-GAN | .2201 | .4811 | .3254 | .2063 |\n| MIRROR-b | .2026 | .4533 | .3564 | .0613 |\n\nYes, it is possible to utilize different GAN architectures since every GAN incorporates a latent space and a discriminator network. We conducted an additional experiment with UNet-GAN [11] pretrained on FFHQ and compared the results to the base experiment in the table above. For the surrogate model $f\u2019$, we used the left-half of the UNet-GAN\u2019s discriminator. Since UNet-GAN does not incorporate a mapping network, a custom mapping layer was trained from scratch. Observe that while the overall performance is slightly decreased compared to TMI with StyleGAN, it is still more effective than MIRROR-b. The slight drop in performance is largely due to the entangled latent space of UNet-GAN, where the latent space $\\\\mathcal{Z}$ is directly used without mapping it to an intermediate disentangled latent space $\\\\mathcal{W}$ in advance. We conclude that while StyleGAN is still the most effective image prior to be utilized, other GAN-based methods can generally benefit from our suggested approach of leveraging a new mapping layer and distilling the discriminator for a surrogate model. Therefore, the contribution of TMI is not only limited to the packaged attack method as a whole, but also the individual components that can be applied independently. The techniques from TMI might also be able to augment state-of-the-art diffusion models for MI attacks. For example, the mapping network can serve as a reliable encoder for latent diffusion models. We believe that designing an MI attack upon diffusion models is plausible, however, out of scope in this paper.\n\n**Q5+Q6. Incomplete math.**\n\nWe appreciate the suggestion and will replace our math formulations in Eq2 and Eq3 with the formal equations as follows: \n- Eq2. $\\\\underset{\\\\theta}{\\\\arg\\\\min}\\\\,\\\\mathbb{E}\\_{(w,x,\\\\hat{y})\\\\in\\\\mathcal{D}\\_{gen}}\\\\left[ (w-m'\\_\\\\theta(\\\\hat{y}))^2\\\\right]$\n- Eq3. $\\\\underset{\\\\theta}{\\\\arg\\\\min}\\\\,\\\\mathbb{E}\\_{(w,x,\\\\hat{y})\\\\in\\\\mathcal{D}\\_{gen}}\\\\left[ (\\\\hat{y}-f'\\_\\\\theta(x))^2\\\\right]$\n\n**Q7. What happens if you take a stylegan trained on cars dataset and use it for a predictor trained on human faces?**\n\nUsing an unrelated dataset will render TMI less effective, since its reconstruction domain is limited to the image manifold of the prior modeled in the generator. However, please note that as mentioned in **W1+Q1**, to the attacker\u2019s perspective, it is probable to find a dataset or pretrained StyleGAN specific enough for TMI, given the abundance of public datasets and repositories. In addition, prior works on MI often use subsets from the target network\u2019s trainset to train their inversion model, whereas our base evaluation for TMI utilize relevant, but different datasets. We believe that this adversary setting is an acceptable norm in the model inversion literature.\n\n---\n\n>[11] Schonfeld, Edgar, Bernt Schiele, and Anna Khoreva. \"A u-net based discriminator for generative adversarial networks.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2020."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5729/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700532999663,
                "cdate": 1700532999663,
                "tmdate": 1700625983130,
                "mdate": 1700625983130,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "w6j6TLtXXj",
            "forum": "VNHsZPZ5rJ",
            "replyto": "VNHsZPZ5rJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5729/Reviewer_ZpeV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5729/Reviewer_ZpeV"
            ],
            "content": {
                "summary": {
                    "value": "The authors seek to perform model inversion (creating the input that generated a prediction) by modifying a StyleGAN. Specifically, the StyleGAN mapping network is adapted to project a prediction vector into the GAN's latent space while the discriminator is adapted to be a surrogate model. Then to perform model inversion, a latent is created from the prediction vector and then optimized to generate an image that causes the surrogate model to emit a similar prediction vector. The authors are able to convincingly regenerate the input for a variety of  datasets."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "I thought this paper was well-written, and the results were quite convincing. The method clearly outperforms other competing baselines, and is able to generate something resembling the model input. I appreciated that they chose two very different image datasets (celeba and chestxray). \n\n I think this could also be neat as an interpretability tool (e.g., to generate what images are on the border between two classes)."
                },
                "weaknesses": {
                    "value": "What is an example scenario where we do have access to the full prediction vector (e.g., the probabilities and not just the most likely class) but we (a) do not have the input and (b) we have limited queries to the model?  \n\nRegarding (b) I'm not sure query budget is necessarily the right metric (querying the surrogate model also takes time). It would be nice to see some scalability numbers on how long this method takes."
                },
                "questions": {
                    "value": "How many queries do you have to perform to the surrogate model? I'd imagine that this can be just as expensive as querying the original model (if compute is the issue, I'm not sure \"query budget\" is the right metric to optimize).\n\nHow important is the surrogate model? If you used the original model instead (ignoring the query budget), how much better are your images?\n\nIs this method specific to GANs (could you adapt one of the current off-the-shelf diffusion models for example)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5729/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698793995405,
            "cdate": 1698793995405,
            "tmdate": 1699636599958,
            "mdate": 1699636599958,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Cw4s5CFCNi",
                "forum": "VNHsZPZ5rJ",
                "replyto": "w6j6TLtXXj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5729/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5729/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer ZpeV"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive feedback. We address each concern in the following.\n\n**W1. Example scenarios**\n\nConsider an age classifier that gets a face-image query and classifies it into one of five classes that correspond to age groups, the adversary can populate a list of arbitrary prediction vectors that represent each class and then conduct TMI attacks to reconstruct the facial images by conducting TMI attacks. When this age classifier is hosted on an MLaaS cloud instance, the owner of this cloud instance should limit an excessive number of queries due to their computation and financial constraints. We explored this scenario in Section 5.3 (see Figure 4).\n\nWe also described several example scenarios in Section 3 in which the adversary obtains prediction vectors. These scenarios include users posting prediction results on social media simply for entertainment (e.g. celebrity look-alike apps such as StarByFace show the look-alike percentage to celebrities), medical professionals sharing diagnosis predictions for educational or consultative purposes, split inference settings where the inference result is disclosed to different parties [1] or an untrusted MLaaS server. Prior studies have addressed countermeasures to protect inference privacy by obfuscating [2,3] or encrypting the input [4]. However, the prediction vector is left in plaintext format.\n\n**W2. How long does this method take + Q1. How many queries to the surrogate model?**\n\n| | **Query** | **Time (minutes)** |\n|-|-|-|\n| TMI | 5k (offline) | 2.38 |\n| AMI | 0 | $\\\\sim$0 |\n| MIRROR-b | 10k | 0.18 |\n| RLB-MI | 80k | 67 |\n| LO-MI | 25k | 1.91 |\n| P&P | 34k | 2.69 |\n| MIRROR-w | 160k | 72.67 |\n\nWe compared the number of queries and the computation time per each attack in the table above. Please note that the required time is highly dependent to the baseline implementations and their own GAN prior. By default, TMI queries the surrogate model the same number of times to the update steps. As stated in A.1.2, we used $n=5000$ for our experiments. For other baselines, we assigned a default query budget that the respective attack assumed in their paper. We note that TMI with 5K updates exhibited the superior performance over all black-box baselines, as shown in Table 2. In the revised paper, we will include these experimental results. We emphasize that leveraging a surrogate model contributes to substantially decreasing the number of black-box queries directed at a target model (please refer to Section 5.3 for more details). Please note that the adversary can conduct MI attacks in an *offline* fashion by abusing this surrogate model, thus circumventing potential constraints associated with online queries. Consequently, the target service provider may remain unaware of whether they are under attack or not.\n\n**Q2. Replacing the surrogate model with the original (ignoring the query budget)**\n| | **Performance gain** |\n|:-:|:-:|\n| Acc@1 \u2191 | .3408 \u2192 .8264 |\n| Acc@5 \u2191 | .6255 \u2192 .9534 |\n| F-dist \u2193 | .2950 \u2192 .1975 |\n| Cover \u2191 | .2067 \u2192 .2619 |\n\nWe appreciate this valuable suggestion. We conducted an additional experiment using the original model instead of the surrogate model, and the performance gains are listed in the table above. As expected, the performance is improved by an average of 63.7%, outperforming even the white-box attacks across all four metrics. This also indicates that retraining a mapping network can be applied for conducting  white-box TMI attacks to improve their reconstruction performance. We will include this result in A.2.1. Effect of the Surrogate Model.\n\n**Q3. Is this method specific to GANs?**\n\nAlthough our paper is focused on StyleGAN, the contribution of TMI is not only limited to the packaged attack method as a whole, but also the individual components that can be applied independently. For example, the mapping network might also serve as a reliable encoder for latent diffusion models. We believe that designing an MI method using a diffusion models is also promising. However, we consider exploring this direction out of scope for this paper.\n\n---\n\n>[1] Zecheng He, Tianwei Zhang, and Ruby B Lee. Model inversion attacks against collaborative inference. In Proceedings of the 35th Annual Computer Security Applications Conference, pp. 148\u2013162, 2019.\n\n>[2] Mireshghallah, Fatemehsadat, et al. \"Shredder: Learning noise distributions to protect inference privacy.\" Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems. 2020.\n\n>[3] Liu, Qin, et al. \"When deep learning meets steganography: Protecting inference privacy in the dark.\" IEEE INFOCOM 2022-IEEE Conference on Computer Communications. IEEE, 2022.\n\n>[4] Gu, Zhongshu, et al. \"Privacy enhancing deep learning cloud service using a trusted execution environment.\" U.S. Patent No. 11,443,182. 13 Sep. 2022."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5729/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700532449037,
                "cdate": 1700532449037,
                "tmdate": 1700564837662,
                "mdate": 1700564837662,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iD4TLtgkOu",
                "forum": "VNHsZPZ5rJ",
                "replyto": "Cw4s5CFCNi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5729/Reviewer_ZpeV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5729/Reviewer_ZpeV"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "I thank the reviewers for their response. I keep my positive score of 8 (accept)"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5729/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700582231571,
                "cdate": 1700582231571,
                "tmdate": 1700582231571,
                "mdate": 1700582231571,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4uom6gD0Gw",
            "forum": "VNHsZPZ5rJ",
            "replyto": "VNHsZPZ5rJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5729/Reviewer_6NZM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5729/Reviewer_6NZM"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a new model inversion (MI) attack leveraging StyleGAN as an image prior in a blackbox fashion -- i.e., without requiring access to the model's weights to compute gradients through it. The goal of a MI attack is to successfully reconstruct the input image to a classifier, based only on the predictions from it, as a result of which, MI can be used to recover samples from the training dataset, leaking potentially sensitive information (personal details, health scans, face images etc.). Developing better MI attacks are useful so they can be defended against more effectively for sensitive applications. \n\nWhile there has been significant progress in white-box MI attacks, black box methods still have a way to go because they are harder, and typically require several hundreds of thousands of queries for any reasonable inversion. Further, some existing methods tend to produce generic class-representative images, at the cost of intra-class differences, which undermines the goal of reproducing privacy preserving attacks. \n\nThis paper presents a _Targeted Model Inversion (TMI)_  attack, that modifies StyleGAN's latent mapper ($g: \\mathcal{Z} \\rightarrow \\mathcal{W}$), that goes from a random prior distribution (like Gaussian) to the W space of StyleGAN. The modified mapper, $m( . )$ directly predicts the $w \\in \\mathcal{W}$, which is then passed to the generator to obtain the image; this is trained by optimizing in the W space. A surrogate model, $f'$ is obtained by fine-tuning adapting the pre-trained discriminator to mimic the original model, while the images are sampled from the pre-trained StyleGAN."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "MI is an important problem to study, especially considering the potential for damage that can be caused with sensitive data. The paper addresses blackbox MI attacks, which is a more practical scenario where the attacker only has access to the model via an API call.\n* The use of StyleGAN as an image prior, and the discriminator as the surrogate makes intuitive sense, and exploiting it for MI is a realistic scenario.\n* Use of a general, strong prior like StyleGAN also produces sufficient intra-class diversity  -- the empirical results also show that in terms of diversity TMI outperforms other whitebox methods, significantly which is encouraging.\n* Evaluations are convincing, and the different metrics considered demonstrate the superiority of TMI over blackbox and whitebox methods. \n* The data efficiency for similar or better accuracy in MI attack over baselines is promising"
                },
                "weaknesses": {
                    "value": "* **Image Prior** An unacknowledged weakness of the paper is the generality of the approach to a broader set of application domains. By choosing a StyleGAN prior, the applicability of the current method becomes restricted to the domains on which (or domains related to) to the StyleGAN's training distributions. The evaluations, and experiments -- while impressive are of less impactful in my opinion. \n* Two potential mitigation strategies come to mind -- (a) in order to work with more SoTA foundation models like StyleGAN-XL, the current approach will require to work with conditional generative models, which can make it much more potent and realistic; or (b) Leverage stronger inversion techniques on _unconditional_ StyleGAN that are able to invert arbitrary OOD images using pre-trained styleGAN as well -- for example \n\t* GAN inversion for out-of-range images with geometric transformations, CVPR'21\n\t* Image2stylegan++: How to edit the embedded images?, CVPR'21\n\t* Improved StyleGAN-v2 based Inversion for Out-of-Distribution Images, ICML'22\n* The examples of images that are \"significant\" deviation from the original dataset, unfortunately are not that OOD. It's well known that most encoders (including pSp or e4e ) do a reasonable job of inverting these paintings and other closely related domains. So i think a more accurate test of TMI will be to use a much more generic dataset, which will likely fail. \n* The feature distance measure appears to be poorly correlated to image quality -- i think the metric maybe misleading since an approach like AMI, which arguably fails on most of the evaluations conducted, has a reasonable F-dist score comparable or bette than some of the other baselines which are clearly better."
                },
                "questions": {
                    "value": "In addition to some of my comments above -- \n* Can the ablations on query budget be done for one of the baselines as well to see how well (or badly ) they behave as the budget decreases?\n* How does the surrogate perform on the original dataset? is it an actual surrogate in the sense that it come close to the original model's performance? this might be an interesting ablation as well. \n* A (non-technical) question -- is the use of gendered language for describing the attacker common? \n\n>\"..where the adversary _(Eve)_ is able to..\" \n\n>\".. *She* uses $D_{aux}$ to train their *her* StyleGAN network\".. \n\nI found that a bit odd but i am not closely familiar with how this is done in security and safety research so I will defer to the authors."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5729/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698942155650,
            "cdate": 1698942155650,
            "tmdate": 1699636599860,
            "mdate": 1699636599860,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lUdVLJVsEC",
                "forum": "VNHsZPZ5rJ",
                "replyto": "4uom6gD0Gw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5729/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5729/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer 6NZM's (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive feedback. We address each concern in the following.\n\n**Q1. Ablations on query budget**\n\n|  | **Query budget** | **Acc@1\u2191** | **Acc@5\u2191** | **F-dist\u2193** | **Cover\u2191** |\n|-|-:|:-:|:-:|:-:|:-:|\n| **P&P** | 5k&nbsp;&nbsp;&nbsp;&nbsp; | .101 | .284 | .393 | .041 |\n|  | 10k&nbsp;&nbsp;&nbsp;&nbsp; | .104 | .299 | .390 | .046 |\n|  | 50k&nbsp;&nbsp;&nbsp;&nbsp; | .156 | .364 | .373 | .060 |\n|  | 100k&nbsp;&nbsp;&nbsp;&nbsp; | .153 | .388 | .365 | .048 |\n|  |  |  |  |  |  |\n| **MIRROR-w** | 5k&nbsp;&nbsp;&nbsp;&nbsp; | 0 | .013 | .485 | .020 |\n|  | 10k&nbsp;&nbsp;&nbsp;&nbsp; | .026 | .071 | .460 | .031 |\n|  | 50k&nbsp;&nbsp;&nbsp;&nbsp; | .259 | .478 | .365 | .059 |\n|  | 100k&nbsp;&nbsp;&nbsp;&nbsp; | .354 | .609 | .341 | .062 |\n|  |  |  |  |  |  |\n| **TMI** | 5k&nbsp;&nbsp;&nbsp;&nbsp; | .232 | .448 | .340 | .121 |\n|  | 10k&nbsp;&nbsp;&nbsp;&nbsp; | .274 | .540 | .318 | .144 |\n|  | 50k&nbsp;&nbsp;&nbsp;&nbsp; | .358 | .646 | .286 | .182 |\n|  | 100k&nbsp;&nbsp;&nbsp;&nbsp; | .341 | .626 | .295 | .207 |\n\nWe conducted additional experiments to assess the performance of the baseline methods across varying query budgets (5, 10, 50, 100K). Please note that the query budget denotes the total number of queries allowed to attack every label (i.e., 530 attacks in case of FaceScrub). We included the experimental results of two best-performing baseline methods (i.e., P&P and MIRROR-w) and our TMI attack in the table above. The graphical representations of the performance of all baseline methods can be found ***[here](https://drive.google.com/file/d/1q_2EXRYds6srSQQ2gETbRreVYIjDJo1n)***. As the table above shows, the overall MI performance increases as the number of queries increases. TMI consistently outperforms the baseline attacks across different query budgets. We also note that TMI holds a distinctive advantage of requiring zero queries to a target model after the preparation phase, which significantly facilitates the reconstruction of inputs for a large number of prediction vectors.\n\nDuring the experiments, we identified an error in the query count of LO-MI in Table 1. For each attack attempt, LO-MI requires 25k queries, instead of 2k. This will be corrected in the final version. We confirm that this erroneous typo does not impact the overall conclusion of our study since we overstated the performance of the baseline attack (i.e., LO-MI). Correcting this value highlights the superior performance of TMI attacks.\n\n**Q2. Surrogate model\u2019s performance on the original dataset?**\n\nWe measured the accuracy of the TMI surrogate models using the test splits of the FaceScrub, CelebA, and PadChest datasets. The table below describes the performance metrics of both original ($f$) and surrogate ($f\u2019$) models. In the revised paper, we will include these experimental results.\n\n| **Dataset** | **Architecture** | **$f$ Acc.** | **$f'$ Acc.** |\n|-|-|:-:|:-:|\n| FaceScrub | ResNeSt-101 | 0.961 | 0.605 |\n| | DenseNet-169 | 0.954 | 0.588 |\n| | MobileNet-v3 | 0.937 | 0.542 |\n| CelebA | ResNeSt-101 | 0.877 | 0.580 |\n| | DenseNet-169 | 0.849 | 0.582 |\n| | MobileNet-v3 | 0.792 | 0.552 |\n| PadChest | ResNeSt-101 | 0.730 | 0.708 |\n| | DenseNet-169 | 0.722 | 0.700 |\n| | MobileNet-v3 | 0.732 | 0.686 |\n\n**Q3. Use of gendered language for describing the attacker**\n\nWe will replace those words with gender-neutral terms. We note that prior studies in the security and privacy domain have often employed gender-specific terms when referring to the adversary."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5729/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700205900297,
                "cdate": 1700205900297,
                "tmdate": 1700564813132,
                "mdate": 1700564813132,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HVtbo36mJf",
                "forum": "VNHsZPZ5rJ",
                "replyto": "4uom6gD0Gw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5729/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5729/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer 6NZM's (2/2)"
                    },
                    "comment": {
                        "value": "**W1. Applicability of the current method restricted to domains related to the StyleGAN's training distributions**\n\nWe acknowledge that TMI necessitates a StyleGAN prior model. However, all previous MI studies [1-9] have assumed access to an auxiliary dataset of which underlying distribution is either the same or slightly different from the distribution of the target dataset. Therefore, we proposed a new MI method that harnesses this auxiliary dataset in a black-box manner, achieving superior MI performance over the SoTA methods.\n\nIn addition, we note that our TMI evaluation spans two distinct application domains, namely chest X-ray and facial datasets. This suggests that, given an appropriate StyleGAN prior, TMI is applicable to diverse classification tasks for MI. Given the abundance of pre-trained models and datasets readily available on the internet (e.g., Kaggle alone provides 17,678 image datasets across 164 official subjects, including people, healthcare, genetics, and business.), we believe that the adversary is capable of selecting their preferred StyleGAN prior for their needs. Even when there is no available dataset or pretrained StyleGAN, the adversary still has an option to collect their own images and train a StyleGAN specific to their need. Image search or sharing services (such as Google Images, Flickr, and Pinterest) allow attackers to access a large volume of image data. \n\n**W2. Explore leveraging advanced GAN techniques**\n\nWe will discuss potential improvements by integrating SoTA models and the connection between GAN inversion to MI attacks. GAN inversion typically requires an original image to find a suitable latent point, whereas our TMI attacks do not assume white-box access to the target model or access to the original image. Leveraging a prediction output without the original image, TMI performs the best for identifying the optimal latent point that contributes to reconstructing the input image among the SoTA MI methods.\n\n**W3. more accurate test of TMI will be to use a much more generic dataset, which will likely fail**\n\nUsing a much more generic dataset will render TMI less effective, since its reconstruction domain depends on the image manifold of the prior modeled in the generator. However, please note that as mentioned in **W1**, to the attacker\u2019s perspective, it is probable to find a dataset or pretrained StyleGAN specific enough for TMI, given the abundance of public datasets and repositories.\nIn addition, prior works on MI often use subsets from the target network\u2019s trainset to train their inversion model, whereas our base evaluation for TMI utilize relevant, but different datasets. We believe that this adversary setting is an acceptable norm in the model inversion literatures. Also note that TMI assumes a weaker black-box adversary with a limited number of input queries.\n\n**W4. Feature distance poorly correlated to image quality**\n\nWe appreciate this valuable insight and agree that feature distance alone is insufficient to demonstrate reconstruction performance. Therefore, we used four metrics to comprehensively measure the MI performance of TMI and the other baselines.  We will discuss the downsides of using feature distance alone in measuring MI performance.\n\n---\n\n>[1] Fredrikson, Matthew, et al. \"Privacy in pharmacogenetics: An End-to-End case study of personalized warfarin dosing.\" 23rd USENIX security symposium (USENIX Security 14). 2014.\n\n>[2] Yang, Ziqi, et al. \"Neural network inversion in adversarial setting via background knowledge alignment.\" Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security. 2019.\n\n>[3] Zhang, Yuheng, et al. \"The secret revealer: Generative model-inversion attacks against deep neural networks.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2020.\n\n>[4] Han, Gyojin, et al. \"Reinforcement Learning-Based Black-Box Model Inversion Attacks.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\n>[5] Kahla, Mostafa, et al. \"Label-only model inversion attacks via boundary repulsion.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.\n\n>[6] An, Shengwei, et al. \"Mirror: Model inversion for deep learning network with high fidelity.\" Proceedings of the 29th Network and Distributed System Security Symposium. 2022.\n\n>[7] Struppek, Lukas, et al. \"Plug & Play Attacks: Towards Robust and Flexible Model Inversion Attacks.\" International Conference on Machine Learning. PMLR, 2022.\n\n>[8] Chen, Si, et al. \"Knowledge-enriched distributional model inversion attacks.\" Proceedings of the IEEE/CVF international conference on computer vision. 2021.\n\n>[9] Wang, Kuan-Chieh, et al. \"Variational model inversion attacks.\" Advances in Neural Information Processing Systems 34 (2021): 9706-9719."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5729/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700205982464,
                "cdate": 1700205982464,
                "tmdate": 1700564821557,
                "mdate": 1700564821557,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2RcboVgP1m",
                "forum": "VNHsZPZ5rJ",
                "replyto": "HVtbo36mJf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5729/Reviewer_6NZM"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5729/Reviewer_6NZM"
                ],
                "content": {
                    "comment": {
                        "value": "I think the authors for the response and the additional experiments. I have a few observations\n* TMI is clearly very effective even under small query budgets, these ablations are impressive. \n* Interesting to see the surrogates performance in a black box attack is reasonable -- which suggests the power of a powerful prior like StyleGAN. \n\n*StyleGAN and related domains* \nI appreciate the authors response here that it is in fact easy to train StyleGANs on any domain since there are so publicly available datasets. However, I find it a stretch to infer from this that the attack will be successful on any domain -- true that the paper has tested on two distinct domains -- but these represent a very tiny subset of the wide variety of classifiers typically encountered.\n\nAnother reviewer has also raised an important concern that I share -- determining which domain is in-distribution to choose an appropriate StyleGAN for the attack. Typical large scale classifiers are often pre-trained on massive datasets, following which they are fine-tuned on downstream tasks (very much like how the current paper also does). Its well known that these types of models are very good at generalizing to unseen domains. In a blackbox setup, I don't see how it is trivial to determine this distributional match to the StyleGAN. \n\nA way to mitigate this (as I have suggested in my review) is to show that the prior is not as important using far OOD images or on tasks that are larger scale like ImageNet, both of which the current paper does not do -- and which likely requires a non-trivial amount of change. Another limitation is that increasingly generative models are moving away from (unconditional) StyleGAN architectures, which again limits the applicability of the method. \n\nThe paper has a lot of promise, but given its empirical nature and weaknesses i have outlined, I see this as a borderline paper and I am leaning on retaining my original score."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5729/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700587754881,
                "cdate": 1700587754881,
                "tmdate": 1700587754881,
                "mdate": 1700587754881,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]