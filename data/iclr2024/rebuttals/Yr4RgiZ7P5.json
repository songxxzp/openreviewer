[
    {
        "title": "Does resistance to style-transfer equal Shape Bias? Evaluating shape bias by distorted shape"
    },
    {
        "review": {
            "id": "Qg0Ku3i5CE",
            "forum": "Yr4RgiZ7P5",
            "replyto": "Yr4RgiZ7P5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6569/Reviewer_1Rgc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6569/Reviewer_1Rgc"
            ],
            "content": {
                "summary": {
                    "value": "For visual recognition, human rely more on shape while Neural Network rely more on textures, and previous methods trained style transfer augmentation failed to extract global shape information. This paper propose Distorted Shape Testbench as a measurement for global shape sensitivity, and a corresponding training method to improve Visual recognition networks' ability to extract global shape. Qualitative and quantitative experiments have been conducted to show the method's superiority."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The choice of research topic is insightful, both interesting and practical. \n2. The proposed method and benchmark are likely to be helpful helpful to many relevant research fields.\n3. The results are promising and the experiments are convincing"
                },
                "weaknesses": {
                    "value": "1. In the experiment section, the authors only compared Resnet and ViT, the limited number of network architectures may make the conclusion less persuasive\n2. The proposed DiST benchmark is only tested on classification task, while there are many tasks influenced by global shape information. Experiments on different tasks may be needed to show the Versatility of DiST."
                },
                "questions": {
                    "value": "1. The proposed ways of generating shape distortion is based on Neural Style Transfer, does the specific ways of style transfer algorithm used matter? Or could other style transfer/shape distortion methods achieve the same performance?\n2. The distorted images have similar textures but different global shape with the originally images. Would contrastive learning help improve the performance in this situation>"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6569/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6569/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6569/Reviewer_1Rgc"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6569/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697568061399,
            "cdate": 1697568061399,
            "tmdate": 1699636744624,
            "mdate": 1699636744624,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "p08LVeWqkK",
                "forum": "Yr4RgiZ7P5",
                "replyto": "Qg0Ku3i5CE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6569/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6569/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We are glad that the reviewer found the problem insightful and interesting, we think having a clear understanding of \u201cshape bias\u201d would be crucial to the related research. We hope the following feedback can address the reviewer\u2019s questions:\n\n\n**About the limited number of network architectures**: We extend the scope of our benchmarking model, please refer to our general response and section 4.1 and figure 5 in the paper for more details.\n\n\n**About other Style transfer Methods:**  \nTwo major issues that limit the method we used to generate shape-distorted images is whether they can generate the image which randomly shuffles the local components of the original image, and how much does it take to generate the images. Methods like AdaIN[1] or pix2pix [2] are popular methods to do the style transfer,  however it\u2019s hard for them to do the texture synthesis task where there is no content constraint.  Another practical issue is that many methods using GAN to generate the texture are too slow to produce enough images for either training or testing. \n\nAs shown in section 4.2.1, we use an approximate method to generate the shape distorted images, and in the small scale evaluation, it\u2019s as good as the original method. During our experiment, the detailed configuration of generation won\u2019t have a significant influence on the result. Based on our experiment, we think the methods that are practical to be used to generate the image wouldn\u2019t influence the performance. \n\n\n*[1] Huang, X., & Belongie, S. (2017). Arbitrary style transfer in real-time with adaptive instance normalization. In Proceedings of the IEEE international conference on computer vision (pp. 1501-1510).*\n\n*[2] Isola, P., Zhu, J. Y., Zhou, T., & Efros, A. A. (2017). Image-to-image translation with conditional adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 1125-1134).*\n\n\n**About contrastive learning:** We did use contrastive learning at the beginning as a possible way of improvement. Compared with traditional contrastive learning methods, we added the shape-distorted variation as a part of the negative pairs to train the model. We performed a small scale test on ResNet18 trained on Imagenette (a 10-class version of ImageNet), the baseline (supervised learning) achieved **39.4%** on DiST and the contrastive learning with additional shape distorted negative pairs achieved **34.3%** on DiST. We think the low performance of contrastive learning is due to the following reasons: \n\nConsider the contrastive loss  (openreview fail to render the whole formulas so we sepeate it):  \n$-\\sum_{i \\in I} \\log \\frac{\\exp ({z}_i \\cdot {z}_j / \\tau)}{Neg}$\n\n$Neg=\\sum_{a \\in A(i)} \\exp ({z}_i \\cdot {z}_a / \\tau)$\n\n\nwhere $j$ is the positive pair for $i$ and for other $a \\in A(i)$ are the negative pair. If there are $N$ images in total, there would be $N$ augmented images and $N$ shape-distorted images, meaning that there would be 1 positive pair and $3N-2$ negative pairs for each image. However, the there is only 1 negative pair that would contribute to model performance on DiST, which is the shape-distorted variation of the original image. Even if we don't use contrastive learning, it\u2019s easy for model to distinguish the image with images in other classes and their shape-distorted variations. The only one that would be challenging for the model is the  shape-distorted variation of that image itself. While this negative pair only  occupies 1/(3N-1) of the negative pairs. Even if the model is not optimizing towards the way we want, to separate the representation of the original image with its shape-distorted variations, the contrastive loss can still be optimized well. \n\nWe think that the key signal is too weak by using the contrastive loss, and explicitly setting the shape-distorted variation as the other class would provide a strong signal for the model to separate the original image and its shape-distorted variations."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6569/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700684677508,
                "cdate": 1700684677508,
                "tmdate": 1700691121773,
                "mdate": 1700691121773,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "OfjoTnOhcU",
            "forum": "Yr4RgiZ7P5",
            "replyto": "Yr4RgiZ7P5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6569/Reviewer_dedU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6569/Reviewer_dedU"
            ],
            "content": {
                "summary": {
                    "value": "This paper propose a benchmark dataset, DiST, to measure the global shape sensitivity of models. The images in this dataset is constructed by applying global shape distortion to natural images. Specifically, it follows Gatys et al. (2015), but optimize the intermediate layers' feature but keep the gram matrix not changes. Therefore, the texture information is kept but the shape information lost. Based on this newly constructed dataset, this paper have several interesting observations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper is studying an interesting problem, and figured out the missing pieces from the previous studies.\n\nThe proposed image construction methods simple but makes sense.\n\nThe experimental results further illustrate the value of the proposed dataset."
                },
                "weaknesses": {
                    "value": "The scale of this dataset is too small (less than 10k images IIUC). It would be good to use the proposed method to construct an ImageNet-level dataset.\n\nIt would be good to show some well generated images while show some bad images as well, which can help people better understand the pros and cons of the proposed method.\n\nIt would be good to benchmark more models. For example, the shape-biased and the texture-biased model from [a].\n\n\n[a] Li, Yingwei, et al. \"Shape-texture debiased neural network training.\" ICLR 2021"
                },
                "questions": {
                    "value": "See weakness. My main concern is the scale of the dataset on both number of images and number of tested models. This paper is very interesting, but potential can explore more."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6569/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698886099002,
            "cdate": 1698886099002,
            "tmdate": 1699636744514,
            "mdate": 1699636744514,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TBOHFGrtns",
                "forum": "Yr4RgiZ7P5",
                "replyto": "OfjoTnOhcU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6569/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6569/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the constructive comments and suggestions. We address the reviewers' concerns as follows:\n\n**About the scale of the dataset**: The major concern to limit the scale of the dataset is that we would like to set up a human baseline to compare with the model. If the dataset is too large, it would be difficult to conduct human study based on this dataset. We refer to the dataset scale of *[1][2][3]* where they benchmarked both humans and models on various Out-Of-Distribution datasets, as each dataset has around 1280 images. As our dataset has 7200 (2400*3) images, although it\u2019s not yet a ImageNet-level dataset, we think it would be ample to evaluate global shape sensitivity of the model.\n\n\n**About more examples of the generated data**: We have updated more examples of DiST in the appendix. While it\u2019s hard to give a definition about \u201cgood\u201d images and \u201cbad\u201d images, we think the generated result that shuffles local components of the object is the ideal case we want to evaluate the model (like the owl example in the paper). There do exist a small proposition of  examples that are difficult for both human and model, where the background is complex and hard to be separated from the object, the structure (global shape) of the image is hard to be captured. Fig 10(e) is such a case. Please check section A.6 for the details.\n\n\n**About benchmarking more models:** As mentioned in the general response, we have added additional experiments to evaluate the performance to cover various architectures and training methods, to show how they influence the model\u2019s performance on both DiST and cue-conflict dataset. Although we would like to test the model mentioned in your reference, the pertained model in their git repo seems to be broken and we can\u2019t perform the evaluations on them. Please check the section 4.1 and figure 5 in the paper for more details.\n\n\n*[1] Geirhos, R., Temme, C. R., Rauber, J., Sch\u00fctt, H. H., Bethge, M., & Wichmann, F. A. (2018). Generalisation in humans and deep neural networks. Advances in neural information processing systems, 31.*\n\n*[2] Geirhos, R., Narayanappa, K., Mitzkus, B., Thieringer, T., Bethge, M., Wichmann, F. A., & Brendel, W. (2021). Partial success in closing the gap between human and machine vision. Advances in Neural Information Processing Systems, 34, 23885-23899.*\n\n*[3] Geirhos, R., Rubisch, P., Michaelis, C., Bethge, M., Wichmann, F. A., & Brendel, W. (2018). ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness. arXiv preprint arXiv:1811.12231.*"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6569/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700683991467,
                "cdate": 1700683991467,
                "tmdate": 1700685188531,
                "mdate": 1700685188531,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "VwQtXRSiv1",
            "forum": "Yr4RgiZ7P5",
            "replyto": "Yr4RgiZ7P5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6569/Reviewer_PsLQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6569/Reviewer_PsLQ"
            ],
            "content": {
                "summary": {
                    "value": "The paper is centralled concerned with robustness of recognition to shape distortion. The question asked in \"does resistance to style transfer equal shape bias?\" By which the authors seem to mean that recogntion algorithms that have been extended to be decently robust to texture change in objects are not robust to shape changes.\n\nThe paper introduces a data set dseigned for the problem, and a distance they call DiST."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "I like the odd-one-out test, which I think is a sound way to measure subjective distances."
                },
                "weaknesses": {
                    "value": "It is clear from simple observation that texture and shape can both change, and that neural nets are currently configured to rely primarily on texture. So the paper is not saying too much in that regard.\n\nThe dataset introduced comprises an unconvincing collection of images. These include images that have been assumbled from sub-images of the object at different scales and points of view, peeled fruit (and peeled footballs). The reason this is not convncing is that the shape changes within a class will rarely if ever be expressed in such a way. Far more common would be simple geometric distortions. As an example, a \"man\" could be distorted into a \"strong man\" by increasing body and muscle size compared to the head. A second reason is that in some cases at least shapes can change in very unexpected ways but still be reognisable. Dali's melting watches are an example.\n\nThe fact that current models do not perform so well on the new dataset is not surprising. It is not so hard to contruct such a dataset. And the conclusion that humans out perform machines is equally unsurprising."
                },
                "questions": {
                    "value": "Why not use some kind of warping to resist changes in shape? (Inter-class warps should be distinct from intra-class warps).\n\nWhat is your justification for using images made of pieces of photos of an object? (These are not shape distorted, they are mosaics of regualarly shaped parts)\n\nPeople can draw things - say a face - in all sorts of shapes. Eyes can be round, lines, crosses, diamonds and many more shapes.\nWhy not include such examples in your dataset (these examples are real and common in art)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6569/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6569/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6569/Reviewer_PsLQ"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6569/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698952004294,
            "cdate": 1698952004294,
            "tmdate": 1699636744402,
            "mdate": 1699636744402,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9zpFB1ncMN",
                "forum": "Yr4RgiZ7P5",
                "replyto": "VwQtXRSiv1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6569/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6569/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the constructive criticism and feedback. We hope the clarification below addresses your concerns. And we hope the review can check overall response 1 where we gave a clear clarification of concept and the contribution of the paper. \n\n**neural nets are currently configured to rely primarily on texture**: \nIt has been discovered years ago that **normally trained** neural networks are biased towards texture. While our paper wants to emphasize another crucial aspect: the current way to evaluate the shape bias of the model can only show how robust it is against the change of the style, models that achieve high performance on this evaluation are believed to have a \u201cshape bias\u201d. However, as shown in Figure 1 left, even **the model with \u201cshape bias\u201d** through style augmentation, still heavily focuses on the local feature. According to our knowledge, few research in deep learning have paid attention to the definition of \u201cshape bias\u201d but simply evaluate through those style-transferred images. This could cause some confusion when trying to understand the bias of the model. We want to use DiST to show how the model is sensitive to the global shape of the image,  which is missing in the previous metrics. And it would benefit the model in particular application (as shown in general response 3).\n\n**Why not use some kind of warping to resist changes in shape? (Inter-class warps should be distinct from intra-class warps). What is your justification for using images made of pieces of photos of an object? (These are not shape distorted, they are mosaics of regularly shaped parts)**\n\nIt appears there may be a slight misunderstanding regarding the concept of \"shape distortion\" as used in our study. Our purpose of using texture synthesis is to create an image that has the same texture details as the original image, but the arrangement of local components in the image is disrupted, leading to a different global shape.  Like the examples shown in figure 2, the components of the owl are separated and shuffled, even if the local component would remain relatively the same (like the eyes), it would be hard to regard it as an owl due to its weird arrangement. That\u2019s also the reason why styled augmented models still fail on this task, since it is not aware of the global shape, but only focuses on the eye of the owl, which is the same in the generated result. \n\n**People can draw things - say a face - in all sorts of shapes. Eyes can be round, lines, crosses, diamonds and many more shapes. Why not include such examples in your dataset (these examples are real and common in art)**\n\nThe term \u201cshape\u201d we used in the paper is more related to the global arrangement of local components, which we called \u201cglobal shape\u201d. Although the small local component can be drawn using different shapes (circle, diamonds), this does not represent the \u201cglobal shape\u201d of the object. And it might still be within the scope of \u201ctexture\u201d if we consider the model that heavily focuses on the local texture. For example, changing a round eye into a square eye, but not changing the position of the eye, would not influence the global shape, and can be regarded as a change of local texture. Since we know that models, even with style augmentation, still focus on those local textures, we want to make sure that the image in our test has exactly the same texture as the original images, the scope of this \u201ctexture\u201d might also cover what you consider as the shape. Given those considerations, using human drawing might not be a very good option because it\u2019s hard for us to control those texture details. Otherwise we are not able to ensure that model can\u2019t distinguish between those images is due to different local texture, or different global shape (i.e. arrangement of local component )"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6569/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700686518378,
                "cdate": 1700686518378,
                "tmdate": 1700686518378,
                "mdate": 1700686518378,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rgre1HUL66",
            "forum": "Yr4RgiZ7P5",
            "replyto": "Yr4RgiZ7P5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6569/Reviewer_4QYv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6569/Reviewer_4QYv"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a new benchmark to measure how well models can handle distorted shapes. They show that removing texture alone is not sufficient to make models robust to shape distortions. They also suggest that combining their method with Stylized Aug training can improve the models\u2019 robustness to both style and shape variations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper is clear and well-structured. The authors share their code, which facilitates the replication of the experiments. The benchmark they propose is original, as it focuses on shape distortions rather than style variations. They also provide a human baseline for comparison."
                },
                "weaknesses": {
                    "value": "I have some doubts about the need for models to be robust to shape distortions. The examples in Fig. 2(a) seem very challenging even for humans. Are there any real-world applications that require such ability? The distorted shapes do not seem to have any physical meaning.\n\nAnd what is the benefit of using global shape features if the model can already classify the image based on local shape features?\n\nI also noticed that DiSTinguish itself worsens the performance on SIN-1K, as shown in Table 3. Can you explain why this happens?"
                },
                "questions": {
                    "value": "Please check weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6569/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699221920076,
            "cdate": 1699221920076,
            "tmdate": 1699636744293,
            "mdate": 1699636744293,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "U9HgS6vmwf",
                "forum": "Yr4RgiZ7P5",
                "replyto": "rgre1HUL66",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6569/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6569/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your time and feedback. Below we address your concerns.\n\n\u201c**need for models to be robust to shape distortions**\u201d\n\nWe use DiST to reflect the sensitivity of the model to such change of the global shape, and how much it relies on the local texture. High performance in the DiST indicates more sensitivity to the global shape and less reliance on the local texture. The goal of DiSTinguish is not to make the model robust against shape distortion, actually the opposite, to force the model to be sensitive to shape distortion.\n\nIf a model claims to have a real \u201cshape bias\u201d, it should be sensitive to the global shape of the object. As we shown in figure 1 left, the model claimed to have  \u201cshape bias\u201d is actually still relying on the local texture.  Although style augmentation is useful against the change of style, we want to highlight that it shouldn't  be regarded as \u201cshape bias\u201d, since the representation of the model is highly relying on the local texture. We think that clarifying this would be useful for the future research to escape some confusion when trying to understand the bias of the model, since the \u201cshape bias\u201d reflected by stylized image can only show the resistance to the change of texture, rather than sensitivity to glocal shape.\n\n**Benefit of Shape Bias / Sensitivity ot the change of global shape / Less reliance of on the local texture in real-world scenario:** \n\nFor the model that is sensitive to change of global shape, which directly reflects the shape bias, its representation would be robust even if some local texture has changed. To illustrate this, we use the representation learned by DiSTingiush  and Style augmentation to do image retrievals under partial occlusion conditions. Model trained with DiSTingiush is able to find the original image even if some part is occluded, while the stylized augmented model fails to find the original image due to its reliance on particular local features. Please check the section A.6 of the appendix for the details.\n\n**\u201cThe distorted shapes do not seem to have any physical meaning.\u201d**\n\nAlthough shape distortion is an artifact that rarely has any physical meaning, the idea of using shape distortion is that we want to directly test to what extent the model is relying on the local texture rather than the global shape.\n\n**\u201cDiSTinguish itself worsens the performance on SIN-1K\u201d:**\n\nIn table 3 of the paper: model trained with DiSTinguish is 1.2% worse than the baseline. We don\u2019t think it\u2019s a significant drop in performance. We divided the SIN-1K in to 50 subsets and calculate the mean and standard deviation, the perform of DiSTinguish in SIN-1k with error bar would be 24.86%  $\\pm$ 1.51% and the baseline would be 26.08% $\\pm$ 1.30%. And during our experiment, the performance of models on stylized-images depends on what styles the images transfer to. If the dataset uses a different set of styles to transfer, the evaluation result would also be slightly different. Given the above considerations, that small drop on the performance is not very significant."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6569/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700683920664,
                "cdate": 1700683920664,
                "tmdate": 1700683920664,
                "mdate": 1700683920664,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]