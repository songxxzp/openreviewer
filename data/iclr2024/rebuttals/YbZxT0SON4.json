[
    {
        "title": "Improving Intrinsic Exploration by Creating Stationary Objectives"
    },
    {
        "review": {
            "id": "W6vVuvdyLW",
            "forum": "YbZxT0SON4",
            "replyto": "YbZxT0SON4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4213/Reviewer_GnJy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4213/Reviewer_GnJy"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on count-based exploration (or its variants) in reinforcement learning problems. It investigates the issue that count-based bonuses make the reward function non-stationary. The authors propose to augment the state space with visitation count for states (or representative embeddings of states). Therefore, the reward function in count-based exploration becomes stationary and satisfies Markovian property (i.e. the reward is fully determined by the state in this and next step given the augmented state space).\n\nThe authors conduct extensive experiments on 3D navigation maps, and procedurally generated environments with high-dimensional observation space. The experiments combine the proposed approach with multiple count-based exploration algorithms, on both reward-free and sparse-reward RL problems. The empirical results validate that it is beneficial for exploration to include state visitation count as a component in state space."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The proposed method is straightforward and relatively easy to implement.\nThe proposed method is flexible enough to be combined with many existing count-based exploration approaches.\nThe method is simple and effective in improving the existing count-based exploration approaches, given extensive experimental results."
                },
                "weaknesses": {
                    "value": "The technical contribution is not significant enough. It is straightforward to add state visitation count as state input. When it comes to high-dimensional observation space, the proposed method is fully built on E3B and relies on its encoding of the distribution of observed embeddings. So the technical contribution is incremental upon E3B. Is E3B the best approach to solve any hard-exploration problems with high-dimensional observation space? If not, could the proposed method be general enough to improve other exploration approaches for hard-exploration problems with high-dimensional observations?\n\nThe proposed method is specifically constrained to the count-based exploration approach. As for other exploration methods with intrinsic motivations, such as RND (random network distillation), the proposed method is not compatible with RND and cannot be used to improve the exploration result."
                },
                "questions": {
                    "value": "It has been mentioned several times that \"we identify the matrix C_{t-1} as the sufficient statistics\", but it is still vague to me. Where did you identify this? Could you provide any rigorous mathematical proof showing that C_{t-1} is a sufficient statistics for the count-based rewards?\n\nIn Section 3.2., the paper introduces the notation \\phi_t without definition. For the context, I think it refers to sufficient statistics for count-based bonuses, but this point is very unclear in Section 3.2. Also, sufficient statistics in the problem setting are not defined. \n\nIn Equation 7, does s_t include sufficient statistics \\phi_t or not? I think the state s_t means a fully observable state in the augmentation MDP \\hat{M}, so s_t should already contain the information in \\phi_t. Then it is weird to have the reward function and policy depending on both s_t and \\phi_t in Equation 7.\n\nThe empirical results in Figure 4 look interesting. Could it successfully go to any goal position in the training map? If the exploration policy is trained on many different navigation maps. Could the policy be general enough to navigate to any goal position in unseen map?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4213/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698122508887,
            "cdate": 1698122508887,
            "tmdate": 1699636388422,
            "mdate": 1699636388422,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hENuytIfK3",
                "forum": "YbZxT0SON4",
                "replyto": "W6vVuvdyLW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4213/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4213/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer GnJy"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the insightful comments. We have addressed your points and improved the paper. In line with your comments, we now better support our claims that SOFE provides performance gains across several intrinsic objectives, and have shown that it outperforms prior work like DeRL, which also attempts to stabilize training from intrinsic rewards. We now address your points:\n\n**1) Technical contribution** \n\nRegarding the contribution of the paper, we have taken steps to better express the research value of our proposed method, SOFE. We have now added experiments that show that SOFE outperforms recent algorithms like DeRL [1], which are more complex and still aim to tackle the same objective of stabilizing the training with intrinsic rewards. SOFE allows for the training of a single policy in an end-to-end fashion, introducing minimal additional complexity to the RL loop. In contrast, DeRL trains decoupled exploration and exploitation policies to attempt to stabilize the joint objective, which has several limitations: the exploitation policy must be trained from off-policy data only, the exploration policy still faces a complex optimization problem as it optimizes for non-stationary intrinsic rewards, and two completely different policies must be trained in the same loop. With this, we highlight that the simplicity of SOFE is a feature of our contribution.\n\nPlease see lines 40-48, 103-106, 256-263, 310-314, and Table 1.\n\nIt is true that a version of SOFE uses E3B as a way to estimate the true state distribution. In fact, this is part of the main contribution of SOFE, finding and using the best distributions to estimate the visitation frequencies that are easy for deep models to understand. In larger continuous spaces we use E3B to help estimate the state distribution. Furthermore, E3B achieved SOTA results in partially observable, high-dimensional environments like MiniHack and Habitat (**which we have improved with SOFE**), further outperforming popular baselines like RND and ICM. To the best of our knowledge, there is no published article that reports better exploration performance than E3B in such challenging environments.\n\nPlease see the improved Section 3.2.2. Please see Section 4.1 - Intuition here\n\nRegarding RND, we are now working to add a discussion section that motivates future work that extends SOFE to other exploration bonuses like RND and ICM, as we had discussed this before and believe it is possible.\n\n**2) The proposed method is specifically constrained to the count-based exploration approach**\n\nWe have disentangled SOFE from count-based bonuses, and have shown that SOFE provides orthogonal gains across exploration objectives of different nature like state-entropy maximization and pseudo-counts.\n\nPlease see lines 13-16, 54-58, 92-93, and the improved Section 3.2.\n\n**Q1) Identifying the sufficient statistics** \n\nWe thank the reviewer for raising this point. We have modified the paper to better express that SOFE augments the observations with all the moving components of the intrinsic rewards, and hence solves their non-stationarity. Concretely, the state-visitation frequencies are the only dynamically changing components in Equations 1 and 2, and so are the ellipsoid matrix in Equation 3, and the parameters of the generative model in Equation 5. We have modified the paper to better explain this point.\n\nPlease see the improved Section 3.2.\n\nAn example where it is easy to see that the state-visitation frequencies are sufficient statistics that allow for the existence of an optimal Markovian stationary policy is the following: Consider an MDP with paths S1 \u2192 S2 \u2192 S3 and S3 \u2192 S2 \u2192 S1 (no direct path between S3 and S1). The agent always starts at S1. To optimize for the count-based reward $R(s) = \\frac{1}{N(s)}$ the optimal policy is to go S1\u2192S2\u2192S3\u2192S2\u2192S1\u2192S2\u2192S3 \u2013 but of course, this is not a stationary policy (needs to swap actions in S2 depending on the counts of S1 and S3). The best stationary policy here is S1\u2192S2, S3\u2192S2 and then a 50/50 chance from S2\u2192S1 and S2\u2192S3 \u2013 but this is suboptimal. The optimal policy described above can only be learned with access to the state-visitation frequencies (provided by SOFE):  if the agent observed the counts then the optimal policy is stationary: at S2, if N(S3) > N(S1) go to S1, else go to S3.\n\n**Q2) Presentation & notation** \n\nWe thank the reviewer for spotting this important issue. We have modified Section 3.2 to improve the notation and better present SOFE. \n\nPlease see the improved Section 3.2.\n\n**Q3) Augmented states and $\\phi_t$** \n\nWe thank the reviewer for raising this point. We have removed the equation for the optimal policy in the augmented MDP, as it did not provide valuable insights about SOFE. However, we have kept the definition of the augmented MDP where, as you correctly pointed out, the states $\\hat{s}_t \\in \\mathcal{\\hat{S}}$ contain both the observations and $\\phi_t$.\n\nIn the next question, we answer the remaining questions."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4213/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699922198854,
                "cdate": 1699922198854,
                "tmdate": 1700185990030,
                "mdate": 1700185990030,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Bs0IDtLdBT",
                "forum": "YbZxT0SON4",
                "replyto": "W6vVuvdyLW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4213/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4213/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q4) Analysis of the behaviours learned by SOFE**\nWe thank the reviewer for their comments on these experiments. While we believe that the new experiments that you suggest are relevant, we have moved these experiments to the Appendix section in an effort to make space in the main paper for more important comparisons to related work as suggested by other reviewers. However, we have framed these experiments as an in-depth analysis of the behaviours learned by SOFE, which provide valuable insights into how SOFE enables the agents to use the augmented information to drive efficient exploration.\n\n**Q4.1) Could the policy be general enough to navigate to any goal position in an unseen map?**\n\nGiven the interpolation properties of deep networks, we hypothesize that if SOFE is trained over enough combinations of maps, then such generalization would emerge. This should occur in a similar fashion to goal-conditioned RL methods or other contextual MDPs. Importantly, note that in Procgen-Maze and Minihack, every episode reset creates a new map (previously unseen during training since it is procedurally generated). In this case, our results show that SOFE is **necessary** for agents to solve the sparse-reward tasks (see Figure 7) which demonstrates that SOFE allows agents to make smart use of the augmented information in the states.\n\nWe again thank the reviewer for the valuable feedback and we believe we have addressed all concerns from the reviewer."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4213/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700186300212,
                "cdate": 1700186300212,
                "tmdate": 1700186300212,
                "mdate": 1700186300212,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Zfa88ly1JD",
                "forum": "YbZxT0SON4",
                "replyto": "W6vVuvdyLW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4213/Reviewer_GnJy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4213/Reviewer_GnJy"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the detailed response"
                    },
                    "comment": {
                        "value": "Thank authors for the detailed response. I appreciate your efforts and see improvements in the paper, especially Section 3.2. But two of my concerns are not fully addressed. 1) combination of the proposed method of RND/ICM (it will be more convincing if there is experimental results supporting the benefits of proposed method). 2) the generalization ability of the learned policy for more goal positions (it will be more impressive if the learned policy can reach any goal on unseen maps). So I'd like to keep my score."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4213/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700519490696,
                "cdate": 1700519490696,
                "tmdate": 1700519490696,
                "mdate": 1700519490696,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "XOcMgLiIhK",
            "forum": "YbZxT0SON4",
            "replyto": "YbZxT0SON4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4213/Reviewer_Yao3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4213/Reviewer_Yao3"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies intrinsic motivation for RL. First, the authors notice that common intrinsic bonuses (count-based is considered as the illustrative instance) induce non-stationary objectives, which can make the learning process unstable. Then, they propose a solution to this problem by designing a method, called SOFE, to make the objective stationary through state augmentation. Finally, they empirically evaluate the proposed algorithm in a variety of domains, including 3D navigation and ProcGen tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- (Originality) Although limitations of count-based bonuses for intrinsic exploration have been considered before, the algorithmic idea of augmenting the state representation to make the objective stationary is new to the best of my knowledge;\n- (Significance) The experimental results look promising at least and experiments are carried out in some challenging and interesting domains, such as ProcGen Maze;\n- (Clarity) The main ideas of the paper are presented with clarity, but most of the relevant implementation details are deferred to a very brief section in the supplementary (Appendix A.5)."
                },
                "weaknesses": {
                    "value": "- (Motivation) The paper does not clarify that most of the reported considerations are valid when count-based bonuses are the actual learning objective rather than shaping of (sparse) external rewards. The former is arguably not the setting count-based methods have been designed for;\n- (Novelty and scope) Identifying the non-stationarity of the bonuses as a limitation of count-based methods does not look to be novel (e.g., Schafer et al., Decoupled reinforcement learning to stabilise intrinsically-motivated exploration, 2022) and the paper does not credit previous works on that;\n- (Robustness of the empirical evaluation) The experiments section does not specify how count-based bonuses are implemented. Several strategies exist, and they are known to make a significant difference in the resulting performance, which leaves one wondering how general the reported comparison really is. Moreover, the paper does not compare SOFE against alternative pure exploration methods aside from E3B, ICM, RND in Fig.7.\n\n**EVALUATION**\n\nWhile the paper is overall interesting, as it tackles a relevant problem on the inherent non-stationarity of count-based bonuses and provides some promising experimental results, I think it is falling short of the quality required for an ICLR paper in terms of clarity of the motivation, discussion of prior works, and robustness of the empirical evaluation. Hence, I am currently providing a slightly negative evaluation, while I report some detailed comments and questions below."
                },
                "questions": {
                    "value": "1) Count-based bonuses have been largely employed for hard-exploration tasks, either in terms of regret minimization in theoretical papers or shaping of sparse rewards in empirical literature. From my perspective, count-based bonuses are not a great fit for pure exploration instead: They are not only non-stationary, as the authors noted, but also vanishing, and suffer from some other well-known issues (see Ecoffet et al., Go-Explore: A new approach for hard-exploration problems, 2021). However, only in the pure exploration setting the POMDP argument and optimality of history-based policies seem to make sense. If an external reward is there, it is a actually good to converge to a Markovian deterministic policy. Can the authors comment on why they think it is worth studying count-based methods for pure exploration?\n\n2) To follow-up the previous question, count-based bonuses have been used in recent reward-free RL literature as well (e.g., Jin et al., Reward-free exploration for reinforcement learning, 2020). Also in the latter setting, the bonuses are freezed at the start of the episode, so that a Markovian policy optimizing a stationary objective is always deployed to collect data.\n\n3) To address pure exploration settings, other stationary objectives have been designed, e.g., state entropy maximization. Can the authors relates their contributions with this stream of works:\n- Hazan et al., Provably efficient maximum entropy exploration, 2019;\n- Mutti et al., Task-agnostic exploration via policy gradient of a non-parametric state entropy estimate, 2021;\n- Liu & Abbeel, Behavior from the void: Unsupervised active pre-training, 2021;\n- Seo et al., State entropy maximization with random encoders for efficient exploration, 2021;\n- Yarats et al., Reinforcement learning with prototypical representations, 2021;\n- and many others.\nMoreover, is state entropy maximization a meaningful baseline for the reported experimental evaluation?\n\n4) While I am not particularly familiar with the literature of count-based methods, my feeling is that this kind of bonuses have been deeply studied. From a brief research, I am not sure the authors made a thorough due diligence of prior works, especially those addressing the limitations of count-based methods (e.g., Shafer et al., 2020 and Ecoffet et al., 2021 mentioned before). Interestingly, previous works also showed that $1/n$ bonuses bring faster learning in pure exploration settings (Menard et al., Fast active learning for pure exploration in reinforcement learning, 2021).\n\n5) How are the pseudocounts implemented in the experiments? This seems to make a huge different. For instance, Bellemare et al. (2016), Ostrovski et al., Count-based exploration with neural density models (2017), Tang et al., (2017), Machado et al., Count-based exploration with the successor representation (2020) all make different design choices with varying results. Do the authors think their results are general for every implementation of count-based bonuses or for just one?\n\n6) How many seeds are considered in the experiments and what is the meaning of the shaded areas? This crucial information seems to be missing for some of the results.\n\n7) In the ProcGen Maze experiment, the paper comapres SOFE with E3B, ICM, RND. These does not seem to be the state of the art for procedurally generated tasks. Do the authors considered also tailored methods, such as (Ghosh et al., Why generalization in RL is difficult, 2021; Zisselman et al., Explore to generalize in zero-shot RL, 2023)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4213/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698678838906,
            "cdate": 1698678838906,
            "tmdate": 1699636388346,
            "mdate": 1699636388346,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "B7a47PanTa",
                "forum": "YbZxT0SON4",
                "replyto": "XOcMgLiIhK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4213/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4213/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Yao3 - Weaknesses"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the insightful comments. We have addressed your points and improved the paper. We invite you to go over the changes in the updated version of the paper. Overall, we now better highlight the generality of SOFE, providing results across multiple intrinsic exploration objectives (counts, pseudo-counts, and state-entropy maximization). Importantly, and in line with your comments, we have adjusted the claims of our paper, and better positioned our work with respect to previous related research. We have also included relevant experiments that allow for necessary comparisons to previous methods, as you rightfully suggested. In the new version of the paper, we credit previous research that identifies the non-stationarity of count-based rewards, while claiming that SOFE provides performance gains across several intrinsic objectives, and outperforms prior work like DeRL, which also attempts to stabilize training from intrinsic rewards. We now address your spotted weaknesses:\n\n**1 - Motivation)** We appreciate the reviewer's attention to the motivation behind our work. While we present SOFE in a reward-free setting to illustrate its core concepts in Section 4, our method extends beyond this setting. In Figure 1, we explicitly present SOFE within a broader context, considering both intrinsic and extrinsic rewards. Initially, our focus on the reward-free setting addresses our first research question, demonstrating that SOFE effectively mitigates the non-stationarity inherent in intrinsic rewards. Next, in our second research question, we show that SOFE better optimizes the joint objective, which is the combination of task and intrinsic rewards. As rightly pointed out by the reviewer, intrinsic rewards are often used to shape sparse task rewards. In this work, we do not contradict this idea, and study both the reward-free and joint objective settings to showcase the empirical performance of SOFE in both settings. Moreover, we have modified the paper to highlight that SOFE is a general framework that works across exploration objectives of different natures (we have unified counts, pseudo-counts, and state-entropy maximization under the same framework).\n\nPlease see lines 47-48, the improved Section 3.2, and lines 249-263\n\nFurthermore, in response to the experiments suggested by Reviewer 7vAW (see W2), we augment our evidence by demonstrating SOFE's efficiency in solving more popular sparse-reward tasks. Importantly, our results highlight the efficacy of SOFE beyond purely reward-free settings. We hope this clarification provides a comprehensive understanding of the motivation behind our approach.\n\nPlease see lines 310-314, and Table 1.\n\n**2 - Novelty and Scope)** We appreciate the reviewer's comments regarding the novelty and scope of our work. In response to this feedback and in alignment with Reviewer 7vAW's suggestions (see W1 and W2), we have taken steps to better position our work in the context of related literature. Concretely, we have adjusted the claims of identifying the non-stationarity of rewards but we provide SOFE as a simple and high-performing framework to solve this issue.\n\nPlease see lines 4-5, 40-48, 103-106.\n\nAs the reviewer pointed out in the work of Schafer et al. [1], we have now included a quantitative comparison between DeRL and SOFE. This comparison serves to highlight the distinct advantages of SOFE in terms of simplicity and performance. Specifically, we demonstrate that SOFE achieves superior performance with significantly less complexity compared to DeRL. SOFE allows for the training of a single policy in an end-to-end fashion, introducing minimal additional complexity to the RL loop. In contrast, DeRL trains decoupled exploration and exploitation policies to attempt to stabilize the joint objective.\n\nPlease see lines 310-314, and Table 1.\n\n[1] Schafer et al., Decoupled reinforcement learning to stabilise intrinsically-motivated exploration, 2022\n\n**3 - Robustness of the Empirical Evaluation)** We appreciate the reviewer's observation regarding the implementation of count-based bonuses and the selection of alternative baseline exploration methods for comparison. In response to your feedback and to provide greater clarity to the reader, we have modified Section 3 to provide details of the implementation of count-based bonuses\n\nPlease see the improved Section 3.2 and the titles in Figures 4-6.\n\nIn terms of alternative exploration methods, we have used RND and ICM, which are the most popular algorithms for long-horizon exploration in deep RL, serving as commonly used baselines in the literature. Additionally, we have introduced E3B and DeRL to broaden the comparisons. \n\nFinally, we have modified the Appendix section to enhance the clarity of our empirical evaluation and now provide implementation details for the networks and environments used in the paper. This includes hyperparameters, pseudocode, Figures, and curves.\n\nIn the next comment, we tackle your questions."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4213/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699896183202,
                "cdate": 1699896183202,
                "tmdate": 1700185463171,
                "mdate": 1700185463171,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7SJFhqVqKD",
                "forum": "YbZxT0SON4",
                "replyto": "XOcMgLiIhK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4213/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4213/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Yao3 - Questions"
                    },
                    "comment": {
                        "value": "**Q1) Count-based bonuses as the only learning objective** \n\nWe use the reward-free setting as a strategic choice to illustrate the POMDP argument and the optimality of history-based policies. By studying the non-stationarity of intrinsic rewards and framing it as a POMDP, we emphasize that even in the presence of task rewards, the joint objective formed by the combination of intrinsic and task rewards remains non-stationary.\n\nIt is crucial to note that the optimal policy for a joint objective involving both intrinsic and task rewards differs from the optimal policy when considering extrinsic rewards alone. When incorporating the intrinsic signal to shape task rewards, we effectively modify the MDP to facilitate the learning process, although with the trade-off of potentially obtaining policies that may not be optimal in the original task MDP. Importantly, the modified MDP, which integrates the joint objective of intrinsic and task rewards, remains non-stationary. Therefore, SOFE is applicable and beneficial in this context, addressing the challenges posed by non-stationarity and enabling agents to optimize efficiently for the joint objective.\n\nWe have modified the paper to better express that SOFE does not only work under the assumptions of count-based rewards being the only learning objective. Concretely, we have unified counts, pseudo-counts, and state-entropy maximization under the same framework and have provided more empirical evidence that SOFE provides performance gains when training on a joint objective of intrinsic+task rewards.\n\nPlease see lines 40-48, 256-263, 291-295, 310-314 and Table 1.\n\n**Q2) Freezing counts to stabilize the bonus** \n\nWe acknowledge that this method can facilitate optimizing for the non-stationary count-based rewards.  However, this approach diverges from the exact count-based exploration objective. In contrast, SOFE does not require any modifications to the count-based objective. Importantly, the optimal policies obtained with SOFE remain optimal for the original count-based objectives, as argued in Section 4 of the paper.\n\nPlease see lines 229-231.\n\n**Q3) SOFE & State-Entropy maximization**\n\nWe thank the reviewer for raising this point. Even though the comparison between state-entropy maximization and count-based bonuses remains out of the scope of this paper (i.e. in terms of how much they facilitate state coverage and exploration for sparse reward tasks), we believe that SOFE can be easily applied to state-entropy maximization algorithms and provide orthogonal gains. We have now taken steps to show this for the Surprise-Maximizing algorithm [1] which fits a generative model over the state visitation distribution in order to compute its entropy and maximize it. We have modified several sections of the paper to better highlight the generality of SOFE, and have obtained empirical results that support our claims.\n\nPlease see lines 13-16, 54-58, 71-73, 92-93, Section 3.2.3, lines 291-295, Figure 5.\n\n[1] Berseth, Glen, et al. \"Smirl: Surprise minimizing reinforcement learning in unstable environments.\" arXiv preprint arXiv:1912.05510 (2019).\n\n**Q4) Related work to stabilize the intrinsic rewards** \n\nWe thank the reviewer for suggesting we better contextualize our work with related previous works. As aforementioned and also suggested by other reviewers, we have now included a quantitative comparison between SOFE and DeRL.\n\nEven though there exist many monotonically decreasing functions of the state-visitation frequencies that can be used to compute count-based bonuses (e.g. like $\\frac{1}{n}$), we argue that the two that we study in this work represent the most popular ones and that has been more thoroughly studied.\n\nPlease see Section 3.2.1\n\n**Q5) Implementation of pseudo-counts**\n \nWe thank the reviewer for raising this important point. We use the E3B algorithm to obtain an ellipsoid that represents the pseudo-state-visitation frequencies in a latent space, and not only the pseudo-count for a single state. By using SOFE, we augment the agents\u2019 observations with the approximate state-visitation frequencies over the complete state space and show great performance gains. We have modified the paper to better express this very important point. \n\nPlease see Section 3.2.2\n\n**Q6) Details on Figures** \n\nWe thank the reviewer for raising this point. We have now added the necessary information for all figures.\n\n**Q7) SOTA baselines** \n\nE3B achieved SOTA results in procedurally generated environments. ICM and RND remain the most popular deep exploration algorithms. We thank the reviewer for mentioning these 2 works. While related, they do not provide open-source implementations of their methods. However, we have added them in the Related Work.\n\nWe again thank the reviewer for the valuable feedback and invite him/her to re-evaluate the improved version of the paper. We have significantly improved the presentation, broadened the contribution, and provided a more sound empirical evaluation."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4213/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699897281817,
                "cdate": 1699897281817,
                "tmdate": 1699976738497,
                "mdate": 1699976738497,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KyeXiomUjm",
                "forum": "YbZxT0SON4",
                "replyto": "XOcMgLiIhK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4213/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4213/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer,\n\nThe rebuttal time is almost over, and we would greatly appreciate it if you could confirm that our updates have addressed your concerns.\n\nThank you very much."
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4213/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700582700358,
                "cdate": 1700582700358,
                "tmdate": 1700582700358,
                "mdate": 1700582700358,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wLYeWPAbq0",
                "forum": "YbZxT0SON4",
                "replyto": "KyeXiomUjm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4213/Reviewer_Yao3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4213/Reviewer_Yao3"
                ],
                "content": {
                    "comment": {
                        "value": "I want to thank the authors for their extremely detailed response and I am really sorry for my late reply. \n\nTheir effort in changing the paper to accomodate reviewers' suggestions is truly remarkable. From a brief inspection, the paper seems to have improved substantially. I will consider increasing my score after a more detailed read of the new version.\n\nI do not have further questions for the authors. As a minor clarification, my point on state entropy maximization was that it is already a stationary objective, which does not need SOFE to become stationary."
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4213/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700606699811,
                "cdate": 1700606699811,
                "tmdate": 1700606699811,
                "mdate": 1700606699811,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "XiVTo1TEw8",
            "forum": "YbZxT0SON4",
            "replyto": "YbZxT0SON4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4213/Reviewer_4EvL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4213/Reviewer_4EvL"
            ],
            "content": {
                "summary": {
                    "value": "The paper claims that exploration bonus induces a non-stationary reward function, which causes instability in policy learning. The paper proposes to solve this instability by incorporating a sufficient statistic over the exploration bonus. The paper conducts experiments on 2D and 3D maze-like environments and demonstrates the proposed method can cover the state space more efficiently when compared to existing approaches."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The idea is simple and intuitive---by augmenting the state space the method converts the non-stationary exploration bonus to stationary.\n- The results on navigation tasks seem promising---in particular the policy appears to explore the maze more efficiently and recovers behaviour akin to a goal-conditioned policy when the goal state is set to unvisited"
                },
                "weaknesses": {
                    "value": "**Comments**\nI am happy to increase my score after these points are addressed:\n- The formulation in E3B doesn't really cover the action which is also important for count-based exploration for particular tasks (e.g. deterministic dynamics will be okay but not stochastic). Consequently, I don't think this is particularly a sufficient statistic but only for environments tested.\n\t- This also raises another question---why does the paper only focus on navigation tasks? There are many other difficult exploration tasks (e.g. Montezuma's Revenge on Atari, Minecraft, etc.)\n\t- The paper claims that it is a sufficient statistic and empirically demonstrated that it does perform well, but it will be great if there is a proof showing that this is true under some assumptions.\n- In section 4, the paper indicates that \"we consider that the only unobserved components in the POMDP are the parameters of the reward distribution.\" I am curious as to why this is a good assumption? In particular if $\\phi_t$ is only a sufficient statistic for exploration bonus, it may not be a sufficient statistic for deriving the state $s_t$ (with observation $o_t$).\n\t- By adding $\\phi_t$ to the state space, we are essentially exploding the state space (potentially to $\\infty$.) from a finite state space (e.g. the maze example in experimentation.) What is the intuition that the algorithm is still able to tractably find a optimal policy?\n\t- In continuous state-action space, the counting mechanism depends on the quantization---how do we still ensure stationary reward in this case? I believe it will be non-stationary reward since we cannot differentiate two states within the same bin. Otherwise, we can just use global timestep as the count."
                },
                "questions": {
                    "value": "- I find the paper confusing to read at times:\n\t- There is some mention of $\\phi_t$ but in experimentation we only mention $N_t$ (i.e. count) and $C_t$ (i.e. elliptical bonus). Are they the $\\phi_t$?\n\t- Under section 5.1, what is $N_0$? Is it a binary mask over the map in the maze with only the $j$'th cell is 0? Is the observation the \"state\"?\n\t- What is salesman reward and $\\sqrt{}$-reward on figure 5? I believe it is Eq. 3 and 2 respectively\n\n**Possible typos**\n- Abstract, fourth last line: \"holds\" instead of \"hold\"\n- Generally, \"state visitation frequency\" should be \"state-visitation frequency\"\n- Page 3, first paragraph, fourth last line: \"should not\" instead of \"shouldn't\"\n- Five lines after Eq. 1: $\\forall s_t, a_t$ instead of $\\forall_{a, s, t}$\n- Page 4, second paragraph, line 1: \"Eq. 2 and 3\" instead of \"2 and 3\"."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4213/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4213/Reviewer_4EvL",
                        "ICLR.cc/2024/Conference/Submission4213/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4213/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698689713835,
            "cdate": 1698689713835,
            "tmdate": 1700241362611,
            "mdate": 1700241362611,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zFajQ6Xxy9",
                "forum": "YbZxT0SON4",
                "replyto": "XiVTo1TEw8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4213/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4213/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 4EvL"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the relevant feedback. In line with your comments, we have clarified several key ideas in the paper, improved the presentation, and broadened the contribution.\n\n**1) Formulation of E3B**\n\nRegarding the formulation in E3B, our choice aligns with the original implementation (and with the typical literature in exploration bonuses), which measures novelty over the state space. While count-based methods can be defined over state-action pairs, many popular novelty-seeking objectives, including RND, ICM, E3B, and CoinFlip Networks, aim to maximize exploration over the state space.\n\nSee Page 2, Column 2, Paragraph 1, last sentence [here](https://arxiv.org/abs/2306.03186)\n\n**1.1) Environments used and navigation**\n\nMinihack, Procgen, and Habitat are widely recognized benchmarks for evaluating exploration in deep RL. Both Minihack and Habitat were intentionally selected because they allow for a direct comparison with existing methods (e.g. E3B). While Montezuma could be defined as a navigation environment, it has other mechanics that go beyond conventional navigation tasks  (e.g. avoiding enemies/climbing ladders). Similarly, in our experiments in Section 5.1, we introduce a 3D world with challenges analogous to Montezuma. Here, the agent must efficiently use jump pads and avoid lava and water pools while still aiming to cover the entire map successfully. Additionally, we designed the 3D world to mirror the complexity of open-world navigation seen in Minecraft. Minecraft has large computing requirements, and it takes more than a week to train an agent in Montezuma (see [here](https://docs.cleanrl.dev/rl-algorithms/ppo-rnd/#implementation-details)) (2 billion samples) and is often not included in papers for this reason.  With this, we believe that our proposed mazes, together with DeepSea, the 3D world, Minihack, Procgen-Maze and Habitat, are representative of the hard-exploration settings widely studied in the literature of intrinsic rewards for exploration in deep RL. \n\nPlease see lines 284-289.\n\n**1.2) Solving non-stationarity and identifying sufficient statistics**\n\nRegarding our claims on solving the non-stationarity, we have modified the paper to better express that SOFE augments the observations with all the moving components of the intrinsic rewards. The state-visitation frequencies are the only dynamically changing components in Equations 1 and 2, and so are the ellipsoid matrix in Equation 3, and the generative model in Equation 5. We have modified the paper to better explain this point.\n\nPlease see lines 161-163, and 179-182.\n\nAn example to see that the state-visitation frequencies are sufficient statistics that allow for the existence of an optimal Markovian stationary policy is the following: Consider an MDP with paths S1 \u2192 S2 \u2192 S3 and S3 \u2192 S2 \u2192 S1 (no direct path between S3 and S1). The agent always starts at S1. To optimize for the count-based reward $R(s) = \\frac{1}{N(s)}$ the optimal policy is to go S1\u2192S2\u2192S3\u2192S2\u2192S1\u2192S2\u2192S3 \u2013 but of course, this is not a stationary policy (needs to swap actions in S2 depending on the counts of S1 and S3). The best stationary policy here is S1\u2192S2, S3\u2192S2 and then a 50/50 chance from S2\u2192S1 and S2\u2192S3 \u2013 but this is suboptimal. The optimal policy described above can only be learned with access to the state-visitation frequencies (provided by SOFE):  if the agent observed the counts, then the optimal policy is stationary: at S2, if N(S3) > N(S1) go to S1, else go to S3.\n\n**2) Assumptions on the unobserved components of the states**\n\nThis assumption holds true in many cases.  In particular, it holds in the maze environments used in the paper, or any grid world environment in general, where the agent has access to sufficient statistics of the transition dynamics. Even in Atari environments like Breakout, where the assumption might not hold because a stack of observations is required to infer the true state of the game (e.g. direction of the bullets), SOFE mitigates the non-stationary optimization, yielding performance gains. Importantly, this is evident in our experiments on the 3D world, Habitat, Procgen-Maze, and Minihack, where the agent has only a partially observable view of the environment and yet benefits from SOFE.\n\n**3) By adding $\\phi_t$ to the state space, we are essentially exploding the state space (potentially to $infty\\$**\n\nWhile it is true that we are augmenting the state space, we add features that provide information about the hidden dynamics of the rewards and help improve policy convergence. As you note, there are more subtle interactions occurring between the size of the state space and training dynamics, and thanks to deep learning, increasing the size of the state space is less problematic for learning than having non-stationary objectives.\n\nPlease see lines 49-54.\n\nIn the next comment, we tackle the remaining comments."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4213/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699891219211,
                "cdate": 1699891219211,
                "tmdate": 1700184955069,
                "mdate": 1700184955069,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JV08zD5Cq1",
                "forum": "YbZxT0SON4",
                "replyto": "XiVTo1TEw8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4213/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4213/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**4) Discretizing a continuous state space and non-stationarity**\n\nBy discretizing the continuous state space into bins, we define an auxiliary MDP, instantiating the exploration problem within this discrete framework. Since the exploration objective, observations, and state-visitation frequencies are computed over this auxiliary state space, the true continuous space remains untouched and hence does not induce any non-stationarity on the reward function in the auxiliary MDP.\n\nRegarding the use of the time step $t$ to augment the states, we argue that it does not have an effect on the non-stationarity of the reward distribution. However, it does affect the stationarity of the transition function. Moreover, it has been shown in [1] that including the time step information in the states can lead to degenerate policies. Please see our discussion with the Reviewer 7vAW in **Q1**\n\n[1] Pardo, Fabio, et al. \"Time limits in reinforcement learning.\" International Conference on Machine Learning. PMLR, 2018.\n\n**Q1) There is some mention of $\\phi_t$ but in experimentation we only mention $N_t$  (i.e. count) and $C_t$ (i.e. elliptical bonus). Are they the $\\phi_t$?**\n\nCorrect, SOFE is a general framework applicable to any sufficient statistics $\\phi_t$. In this paper, we apply SOFE to count-based methods, E3B,  and state-entropy maximization. We have modified the paper to make the notation more clear.\n\nPlease see the improved Section 3.2.\n\n**Q2) What is $N_0$ in Section 5.1**\n\nCorrect,  it is a binary mask over the map in the maze with only the j'th cell as 0 and all the other cells as 1's. For this analysis, we design $N_0$, marking all states as visited except for a single one. The SOFE agent, when provided with the maze observation and binary mask, has learned to pay attention to the minutest details in $N_t$, and directs exploration efficiently toward the single unvisited state.\n\n**Q3) What is salesman reward and $\\sqrt{}$-reward on figure 5**\n\nThe $\\sqrt{}$ and salesman rewards are shown in Equations 1 and 2, respectively. To enhance clarity, we've added a sentence to explicitly connect these reward formulations.\n\nPlease see Section 3.2.1\n\nWe again thank the reviewer for the valuable feedback and we believe we have addressed all concerns from the reviewer."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4213/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699976351637,
                "cdate": 1699976351637,
                "tmdate": 1700185241634,
                "mdate": 1700185241634,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ExbNScItqt",
                "forum": "YbZxT0SON4",
                "replyto": "XiVTo1TEw8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4213/Reviewer_4EvL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4213/Reviewer_4EvL"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the detailed response.\n\n**W4:**\n- So if I understand correctly, we are considering the exploration of the discretized MDP, not the underlying continuous MDP. If that is the case, I am wondering how should we effectively explore the actual continuous MDP using this method? Or is this a limitation currently?\n\n- Right, the global timestep in a sense is captured by the counts. This was raised since I was not sure whether the exploration over the true continuous MDP is being evaluated. If not, this sounds reasonable to me.\n\nThanks for clarifying the remaining questions.\n\nEDIT: I just noticed the ordering is flipped and I apologize for that. Nevertheless I have increased the score from 3 to 5 due to remaining questions."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4213/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700240080764,
                "cdate": 1700240080764,
                "tmdate": 1700241410282,
                "mdate": 1700241410282,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qk8KDbRKDw",
                "forum": "YbZxT0SON4",
                "replyto": "zFajQ6Xxy9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4213/Reviewer_4EvL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4213/Reviewer_4EvL"
                ],
                "content": {
                    "comment": {
                        "value": "**W1:**\n- I agree that many count-based methods consider only state spaces. Just a question about this field generally and I welcome discussions---why we should only consider state spaces generally? It seems like considering only state spaces remove the notion of controllability of the agent itself.\n\n**W1.1:**\n- Thank you for addressing this concern.\n\n**W1.2:**\n- Thank you for providing an example. In this case, it is true that we simply need the state-visitation frequencies, but this seems to be suitable for a particular MDP structure? Can you clarify what kind of MDPs should we be expecting for this method to work well?\n\n**W2:**\n- Thank you for explaining the limitation of this assumption. I think the paper should include this so readers can consider the situation for which they should use this approach.\n\n**W3:**\n- Thank you for providing a justification. I think these subtle interactions need to be explained more---while I also leaned towards \"thanks to deep learning\" we empirical get good results, I would really love to see more convincing arguments. Even just a linear MDP as a sanity check will be sufficient."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4213/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700241343093,
                "cdate": 1700241343093,
                "tmdate": 1700241343093,
                "mdate": 1700241343093,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xz2bY922z9",
                "forum": "YbZxT0SON4",
                "replyto": "oOyDVyPmu7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4213/Reviewer_4EvL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4213/Reviewer_4EvL"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the response.\n\n**W1.2:** Generally, my understanding is that you can augment any (PO)MDPs such that you end up having an optimal stationary Markovian policy. I believe what is confusing to me is that SOFE ends up using only state-next-state pairs for computing \"sufficient statistics\" which I interpret to be the information required to convert POMDPs into MDPs. However, as you have mentioned in **W1** that state-next-state pairs are potentially insufficient in all MDPs.\n\n**W2:** Thank you for including the footnote. At the end of the foot note, I believe it is better to indicate \"we empirically show that SOFE mitigates ...\"\n\n**W3:** Can you clarify how this deals with the global exploration setting? We assume the augmented state space to be only up to the maximum number of allowed interaction with the environment? How meaningful is this bound when $T$ is absorbed by the state space which may be significantly higher than the original state space $\\mathcal{S}$?"
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4213/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700529875285,
                "cdate": 1700529875285,
                "tmdate": 1700529875285,
                "mdate": 1700529875285,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0pJIWteHy6",
                "forum": "YbZxT0SON4",
                "replyto": "XAZqSfjpTE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4213/Reviewer_4EvL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4213/Reviewer_4EvL"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the response. I don't think I am totally convinced and would also invite other reviewers to help me understand why and how **W1.2** and **W3** are addressed. I will keep my score for now, thank you again for the discussions."
                    }
                },
                "number": 29,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4213/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700687583764,
                "cdate": 1700687583764,
                "tmdate": 1700687583764,
                "mdate": 1700687583764,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1bgBZvVZLq",
            "forum": "YbZxT0SON4",
            "replyto": "YbZxT0SON4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4213/Reviewer_7vAW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4213/Reviewer_7vAW"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on a problem faced by approaches that use intrinsic rewards for guiding exploration; they introduce non-stationarity in the reinforcement learning objective. This non-stationarity in rewards can destabilize learning in many RL approaches. The non-stationarity arises because the agent cannot predict the intrinsic reward as the features required to predict it (such as visitation counts) are not observable.\n\nFocussing on generalizations of count-based approaches, the paper proposes Stationary Objectives For Exploration (SOFE) that aim to augment the state space with sufficient statistics for intrinsic reward prediction to eliminate the partial observability and associated non-stationarity.\n\nExperiments in sparse and no-reward navigation tasks show that including SOFE improves the performance of a previously proposed count-based intrinsic reward approach (E3B)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "**S1.** Using count-based bonuses to explore sparse-reward environments is a widespread technique in RL. Improving the performance of count-based approaches would interest the research community.\n\n**S2.** The paper focuses on a relatively under-explored issue of addressing the non-stationarity introduced due to count-based bonuses. The proposed solution of augmenting states with sufficient statistics for intrinsic rewards seems novel, simple, and well-motivated.\n\n**S3.** The paper presents concepts clearly and is easy to follow."
                },
                "weaknesses": {
                    "value": "**W1.** The paper should adjust the claim that it identifies that intrinsic reward functions induce a non-stationary RL objective. While solutions may be under-explored, the issue is generally known and noted in previous papers (e.g., [1]). Other works have chosen not to tackle the non-stationarity in intrinsic reward as it slowly varies and could be tracked [2]. Also, see the references in W2.\n\n**W2.** A crucial weakness of the paper is that it misses comparisons with previous works that have proposed decoupling exploration and exploitation policies [3, 4] to tackle the non-stationarity introduced due to intrinsic bonuses. The paper would significantly benefit from comparing their approach with baselines based on decoupling exploration and exploitation.\n\n**W3.** An aspect that merits further discussion is that RL algorithms with recurrent architectures (like PPO + LSTM used in the paper) could learn representations that resolve this partial observability [5]. One way to promote this effect could be to have an auxiliary task of predicting intrinsic rewards (which could make for an interesting baseline). \n\nWhile this might perform worse compared to directly providing sufficient statistics in the state (in terms of sample efficiency), it is potentially a more general solution to the problem for other intrinsic rewards.\nFor instance, it has previously been shown that LSTMs can learn to count in discrete settings [6], which could be helpful in episodic exploration settings. Previous work has also shown that recurrent architectures can learn contexts that resolve non-stationarity due to partial observability in bandit problems [7].\n\n\nOverall, I appreciate the direction the authors took to address the non-stationarity introduced by intrinsic rewards. Should the weaknesses and questions be adequately addressed/clarified, I would gladly increase my score.\n\n\n\u2014------------------\u2014------------------\u2014------------------\u2014------------------\u2014------------------\n\n### References\n\n[1]Singh, S., Lewis, R. L., Barto, A. G., & Sorg, J. (2010). Intrinsically motivated reinforcement learning: An evolutionary perspective. IEEE Transactions on Autonomous Mental Development, 2(2), 70-82.\n\n[2] \u015eim\u015fek, \u00d6., & Barto, A. G. (2006, June). An intrinsic reward mechanism for efficient exploration. In Proceedings of the 23rd international conference on Machine learning (pp. 833-840).\n\n[3] Whitney, W. F., Bloesch, M., Springenberg, J. T., Abdolmaleki, A., Cho, K., & Riedmiller, M. (2021). Decoupled exploration and exploitation policies for sample-efficient reinforcement learning. arXiv preprint arXiv:2101.09458.\n\n[4] Sch\u00e4fer, L., Christianos, F., Hanna, J. P., & Albrecht, S. V. (2021). Decoupled reinforcement learning to stabilise intrinsically-motivated exploration. arXiv preprint arXiv:2107.08966.\n\n[5] Ni, T., Eysenbach, B., & Salakhutdinov, R. (2021). Recurrent model-free rl can be a strong baseline for many pomdps. arXiv preprint arXiv:2110.05038.\n\n[6] Suzgun, M., Belinkov, Y., & Shieber, S. M. (2019). On Evaluating the Generalization of LSTM Models in Formal Languages. In Proceedings of the Society for Computation in Linguistics (SCiL)\n\n[7] Ramesh, A., Rauber, P., Conserva, M., & Schmidhuber, J. (2022). Recurrent Neural-Linear Posterior Sampling for Nonstationary Contextual Bandits. Neural Computation, 34"
                },
                "questions": {
                    "value": "Q1. There still remains a source of non-stationarity for RL as the considered tasks have a maximum number of steps, but agents are not provided the time step in the state. In the episodic exploration setting $C_t$ can probably be used to infer the time step. However, it might make sense to include the time step in the global exploration setting. I am curious to know the authors\u2019 thoughts regarding this and whether they have tried something along these lines.\n\nQ2. Is there a reason why Figure 3 shows visitations with A2C as the base algorithm, but the evaluation in Figure 4 uses PPO? Would these qualitative results change with the base algorithm?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4213/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4213/Reviewer_7vAW",
                        "ICLR.cc/2024/Conference/Submission4213/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4213/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698805728679,
            "cdate": 1698805728679,
            "tmdate": 1700439390698,
            "mdate": 1700439390698,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5CJc0bshpA",
                "forum": "YbZxT0SON4",
                "replyto": "1bgBZvVZLq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4213/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4213/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 7vAW"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the insightful comments. We have addressed your points and improved the paper. We now better highlight the generality of SOFE, providing results across multiple exploration objectives. Importantly, and in line with your comments, we have adjusted the claims of our paper, and better positioned our work with respect to previous related research. We have also included relevant experiments that allow for the necessary comparisons to previous methods, as you rightfully suggested. We have significantly improved the presentation, broadened the contribution, and provided a more sound empirical evaluation. \n\n**1) Adjusting the claims about the non-stationary intrinsic rewards**\nWe acknowledge the reviewer's observation and have adjusted our claim in the paper. The modifications emphasize the non-stationarity of intrinsic rewards as a recognized issue, drawing connections to observations from various previous works. The core motivation for our work remains to provide a novel solution to address this challenge.\n\nPlease see lines 4-5, 40-48, 103-106 and 109.  \n\n**2)  Comparisons with related work that decouples exploration and exploitation**\nWe appreciate the reviewer's observation and have taken steps to address this concern. We have run experiments on the DeepSea environment (as used in the DeRL paper [1]). Results indicate that SOFE, which simplifies the learning process by training a single policy end-to-end with stationary intrinsic and extrinsic rewards, outperforms DeRL in the harder exploration variations of the environment (see Table 1 in the paper).  We again thank the reviewer since these new results provide further evidence which strengthens our paper by showing the superior performance of SOFE in challenging exploration tasks.\n\nPlease see lines 40-44, 103-106, 256-263, 310-314, and Table 1.\n\n[1] Sch\u00e4fer, L., Christianos, F., Hanna, J. P., & Albrecht, S. V. (2021). Decoupled reinforcement learning to stabilise intrinsically-motivated exploration. arXiv preprint arXiv:2107.08966.\n\n**3) Recurrent architectures** \n\nThank you for highlighting the importance of clarifying our choice of the PPO+LSTM baseline. In line with your comments, our rationale for including this architecture is to investigate whether it can effectively compensate for the non-stationary rewards. We have modified the paper to better motivate this analysis. \n\nPlease see lines 306-309. \n\nRegarding the training process of the recurrent policy, the LSTM extracts features from observations, which are then passed to the critic, aiming to regress the episodic return (in this case, of intrinsic rewards). This procedure already enables learning trajectory representations that compensate for the dynamically changing rewards, similar to the intrinsic reward prediction auxiliary task that you mention.\n\nIndeed, our results show that agents equipped with an LSTM generally perform better than those using simpler algorithms like vanilla DQN and A2C, but SOFE still provides performance gains on top of this higher-capacity architecture (see Figure 7).\n\n**Q1) Time limit and non-stationarity**\n\nWe thank the reviewer for raising this point. We believe that once the sufficient statistics of the intrinsic rewards are observed by the agent the reward distributions become fully Markovian, and hence do not require the time step. Note that in Equations 1-2-3-5 the reward at time step t+1 only depends on the sufficient statistics at time t. Since this reflects the Markovian property, it is invariant across t.  We have modified the paper to better explain this point.\n\nPlease see lines 215-218.\n\nImportantly, we note that not including the time step in the observations can induce non-stationary transition dynamics as shown in [1]. However, [1] shows that including it can lead to degenerate policies, and not including it communicates to the agent that there is a non-zero probability of the episode terminating at each step, which helps learn more consistent policies. Still, in our work, we aim to make the reward function stationary, and as argued above, this is already achieved without including the timestep information.\n\n[1] Pardo, Fabio, et al. \"Time limits in reinforcement learning.\" International Conference on Machine Learning. PMLR, 2018.\n\n**Q2) A2C vs PPO in the Figures**\n\nThank you for raising this point. We want to clarify that the choice of A2C in Figure 3 and PPO in Figure 4 was made to illustrate that SOFE is agnostic to the RL algorithm. The qualitative results are consistent across these algorithms. To enhance clarity, we are open to fixing a single algorithm for these results. Furthermore, the results in Figure 3 correspond to the curves shown in Appendix Section A.9.2. The results for all other algorithms are also shown in the Appendix and they show that SOFE is generally beneficial.\n\nWe again thank the reviewer for the valuable feedback and we believe we have addressed all concerns from the reviewer."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4213/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699888830853,
                "cdate": 1699888830853,
                "tmdate": 1700184913904,
                "mdate": 1700184913904,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qTMKxWLVTA",
                "forum": "YbZxT0SON4",
                "replyto": "5CJc0bshpA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4213/Reviewer_7vAW"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4213/Reviewer_7vAW"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your detailed response. It addresses most of my concerns. I greatly appreciate the time and effort spent revising the paper and conducting the additional experiments. Before updating my score, I would like to clarify one issue with the new experiment (regarding W2). \n\n**Q3.** Decoupled exploration and exploitation: In the DeepSea experiments, why does DERL-DQN improve when depth is increased from 20 to 24? That seems a bit strange. Are these results with multiple initial seeds for the RL agents?\n\nOn a minor note, regarding the comment about recurrent architectures (3), I agree that there is pressure on the RNN to resolve partial observability due to the objective of the critic. However, predicting returns mixes information across timesteps compared to an auxiliary task of predicting (intrinsic) rewards to resolve non-stationarity (like in [7] or other meta-RNN-based RL approaches). The extra features provided in the case of SOFE allow for the prediction of intrinsic rewards, which is a stronger condition than having to predict \u2018intrinsic returns\u2019. Nevertheless, I don\u2019t believe such an experiment is a necessity here as it can be considered beyond the scope of this paper."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4213/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700341828389,
                "cdate": 1700341828389,
                "tmdate": 1700341828389,
                "mdate": 1700341828389,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rcyWeQth8s",
                "forum": "YbZxT0SON4",
                "replyto": "Ul62soZ5BV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4213/Reviewer_7vAW"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4213/Reviewer_7vAW"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the clarifications. As mentioned earlier, the response addresses most of my concerns. I have updated my score accordingly."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4213/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700439153685,
                "cdate": 1700439153685,
                "tmdate": 1700439153685,
                "mdate": 1700439153685,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]