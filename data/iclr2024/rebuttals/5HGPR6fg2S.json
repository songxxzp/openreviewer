[
    {
        "title": "Normalized Space Alignment: A Versatile Metric for Representation Space Discrepancy Minimization"
    },
    {
        "review": {
            "id": "5TYNZYdrY8",
            "forum": "5HGPR6fg2S",
            "replyto": "5HGPR6fg2S",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4994/Reviewer_odB3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4994/Reviewer_odB3"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a manifold analysis technique for quantifying the discrepancy between two representation spaces. The method is called Normalized Space Alignment (NSA).  NSA provides a robust means of comparing representations across different layers and models, a pseudometric that is both continuous and differentiable and an effective loss function for autoencoders. Empirical results show NSA consistently outperforms or matches previous techniques with high computational efficiency."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper is well-organized and easy to follow.\n2. The proposed technique is sound. Building NSA upon Representational Similarity Matrix-Based Measures ensures both computational complexity and differentiability.\n3. Comprehensive experiments are conducted to validate the proposal's effectiveness."
                },
                "weaknesses": {
                    "value": "1. Can authors explain why it is important to compare the similarity of representations in graph neural networks?\n2. Is NSA sensitive to the removal of low variance principal components from representations?\n3. I understand NSA can measure the differences of normalized point-to-point distances in two representations. However, in the definition of Section 3.1, the point structure within representations seems to be not considered. I wonder if there exists a situation where the NSA of two representations is small but the two representations have totally different point structures. \n4. Why NSA only slightly outperforms or performs worse than previous methods in some cases in Table 1?\n\nThere are also two minor issues: \n1. \"CKA\" in paragraph 1 of Section 2 seems to be in the wrong font.\n2. The font size of the text in most figures, especially in Figure 2 is too small."
                },
                "questions": {
                    "value": "See Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4994/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4994/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4994/Reviewer_odB3"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4994/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697604668758,
            "cdate": 1697604668758,
            "tmdate": 1700401551841,
            "mdate": 1700401551841,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LVoEsyoC9S",
                "forum": "5HGPR6fg2S",
                "replyto": "5TYNZYdrY8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4994/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4994/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer odB3"
                    },
                    "comment": {
                        "value": "Thank you very much for taking the time to review our work. We address your concerns below:\n\n> Can authors explain why it is important to compare the similarity of representations in graph neural networks?\n\n**Response**: The embedding of complicated metric spaces into simpler spaces has been investigated for a long time. Among the \"complicated\" metric spaces, graphs have been particular challenging because of impossibility results on isometric embeddings (no matter how many target dimensions) and\nbounds on distortions of their embedding into Euclidean spaces [1][2]. Coupled with their abstraction ability and wide-ranging applications, this makes the study of the geometry of graph representations especially interesting. NSA as a similarity measure is domain agnostic but demonstrating its capability to extract meaningful insights from intricate networks like GNNs will also establish its utility in simpler domains, like images and text processing. We expand upon this in **Concern 1** of the common response and in Section 5 of the revised manuscript. \n\n>  Is NSA sensitive to the removal of low variance principal components from representations?\n\nIt is highly likely that as a Representation Similarity Matrix based measure, NSA suffers from the same issues as CKA and it is not sensitive to the removal of low variance/unimportant principal components. We have not conduced experiments to confirm this claim but plan to do so in a future work.\n\n> I understand NSA can measure the differences of normalized point-to-point distances in two representations. However, in the definition of Section 3.1, the point structure within representations seems to be not considered. I wonder if there exists a situation where the NSA of two representations is small but the two representations have totally different point structures.\n\n**Response:** We have expanded on the definition of NSA in Section 3.1 to show how it is capable of measuring point-wise discrepancies along with a global measure of the dissimilarity between two spaces.  We also present new experiments with point-wise NSA in Section 6 (Figure 5 (a)) where we show that NSA can quantify the discrepancy in the relative position of individual points. In our practical tests, we have not encountered a case where the NSA is small but the two representations have completely different point structures. The opposite could be true in the case where the input data lies in a lower manifold dimension compared to its representation dimension. Then the NSA would be large but the two representations (original dimension vs flattened or unfolded manifold dimension) would be very similar. In these situations NSA can be used to minimize the geodesic distance if we know the manifold dimension of the data. We discuss this scenario in Section 4.3 of the paper and present new experiments along with a solution to overcome these situations. \n\n> Why NSA only slightly outperforms or performs worse than previous methods in some cases in Table 1?\n\n**Response:** NSA provides the ideal tradeoff between structure preservation and computational efficiency. NSA is outperformed by PCA in Triplet Accuracy of Cluster Centers. PCA excels at this with its simplicity and focus on linear variance. NSA is also barely outperformed by RTD-AE in RTD as RTD-AE directly minimizes the Representation Topology Divergence between the original and the latent space as part of their training. PCA is strictly a dimensionality reduction technique and cannot be used with mini batching as a loss function. RTD is extremely inefficient in its computational complexity and we present the difference in epoch time for training in Table 3 in Appendix J. RTD-AE takes 10x the time to finish an epoch compared to NSA-AE and is limited to batch sizes under 400 due to its high complexity. We discuss the difference in complexities in Section 3.4 of the revised manuscript. Additionally, the datasets we use for the dimensionality reduction tests are popular benchmark datasets and most existing techniques get high performance when trained on these datasets. We present a more challenging task in Section 4.2 where we train the models to preserve the inter node relationships necessary to perform link prediction. Here we observe NSA-AE outperforming even RTD-AE.\n\n>\"CKA\" in paragraph 1 of Section 2 seems to be in the wrong font. The font size of the text in most figures, especially in Figure 2 is too small.\n\nWe have fixed these errors. Thank you for bringing them to our notice\n\nPlease let us know if you have any additional concerns that we can address. If not, would you kindly consider raising your score? Once again, thank you for your review. \n\n[1] On lipschitz embedding of finite metric spaces in Hilbert space by Jean Bourgain, Israel Journal of Mathematics. \n[2] The Geometry of Graphs and Some of Its Algorithmic Applications by Linial, N. and London, E. and Rabinovich, Y. , Proceedings of the 35th Annual Symposium on Foundations of Computer Science"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4994/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700358914284,
                "cdate": 1700358914284,
                "tmdate": 1700358914284,
                "mdate": 1700358914284,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aoLpJxkjLw",
                "forum": "5HGPR6fg2S",
                "replyto": "LVoEsyoC9S",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4994/Reviewer_odB3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4994/Reviewer_odB3"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the response."
                    },
                    "comment": {
                        "value": "Thank you for the response. I have no further questions and I have updated my score."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4994/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700401560391,
                "cdate": 1700401560391,
                "tmdate": 1700401560391,
                "mdate": 1700401560391,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Edwr1Ecr6t",
            "forum": "5HGPR6fg2S",
            "replyto": "5HGPR6fg2S",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4994/Reviewer_WXtW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4994/Reviewer_WXtW"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents 'Normalized Space Alignment' (NSA), a novel\npseudo-metric for comparing representations of high-dimensional\ndata via their (Euclidean) distances.\n\nNext to mathematically proving relevant properties of a pseudo-metric\nlike symmetric and the triangle inequality, the paper also presents a\nsuite of experiments showcasing potential application scenarios. This\nincludes (a) an analysis of latent representations, (b) analyses of a\nset of GNN architectures with respect to adversarial attacks, and (c)\ncorrelation analyses of test accuracy and NSA."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- NSA is well-described and all proofs are accessible. The reader is\n  guided nicely through the paper (for the most part).\n\n- The paper attempts to cover a broad range of different applications\n  for showcasing the utility of NSA.\n\n- The focus on *fundamental* measures is a crucial endeavour for\n  improving our understanding of latent representations, quality\n  metrics, and much more.\n\nThe paper is thus on a good path towards making a strong contribution to\nthe literature, but, as outlined below, there are some major issues with\nthe current write-up."
                },
                "weaknesses": {
                    "value": "While I see the contribution favourably, there are major weaknesses\nprecluding the presentation of the paper in its current form. The\nprimary one is a **missing analysis of fundamental properties**. While\nI appreciate the broad range of different applications, this invariably\nmeans that some depth is lost and analyses are relatively superficial.\n\nIntroducing a new measure and then showing its utility requires a more\nin-depth view of data, though. For instance, when introducing the\nlatent representations in Table 1, readers are only shown the actual\nscores, but the primary assumption is that the scores actually capture\nthe relevant properties of the data. Phrased somewhat hyperbolically:\nI believe that NSA can be calculated as described, but I do not\nunderstand whether it measures something 'interesting.' The fact that it\ncorrelates with performance metrics is relevant, but immediately\nsuggests a more detailed comparison scenario, for instance in the form\nof an early stopping criterion or regularisation term. Otherwise, most\nof the analyses strike me as too speculative. I will provide additional\ncomments below.\n\n- I'd suggest to shorten the RTD explanation or substantially extend it.\n  Currently, it makes use of jargon like $\\min G(R, Q)$, 'barcode,' and\n  Vietoris--Rips filtrations that are not sufficiently explained (I am\n  familiar with the work and I believe that a deep dive into topological\n  methods is not required).\n\n- To simplify the notation, I'd either use the actual Euclidean norm in\n  the definition of NSA or write $x$ and $0$ as vectors.\n\n- The autoencoders experiment is somewhat out of place since the\n  introduction sets up a paper on GNNs. Given the broad scope of NSA,\n  I think it might be best to stay with the autoencoder comparison,\n  using data with a known ground truth. This could take the form of\n  building confidence by starting with simple toy examples like a 'Swiss\n  Roll' or other data sets and showing that NSA matches the intuition.\n\nOverall, my **main concern** is that the measure is just too coarse, in\nparticular given large data sets. It essentially amounts to comparing\naveraged distance representations, and more in-depth experiments and/or\ntheoretical analyses would be required here."
                },
                "questions": {
                    "value": "1. NSA in its current form seems to generalise easily to other distances\n   as well. What is the reason for focusing on the Euclidean distance?\n\n2. How robust is the measure and how limited is it in case the 'ambient'\n   distances are misleading (such as in the example of a Swiss Roll)?\n\n3. Being based on distances, NSA should be invariant under isometries.\n   Is this correct? (I'd overall suggest to simplify the exposition\n   here; many of the properties discussed are a direct consequence of\n   NSA being based on distances. It is good to be precise and spell that\n   out in the appendix, but I'd not give it too much space)\n\n4. How are the representations for Section 3.6 calculated? Is the NSA of\n   a specific data set calculated here, i.e. mapping a (batch?) of\n   graphs into the latent space? Please clarify!\n\n5. The definition of NSA reminds me of MMD (but lacking the\n   cross-comparison term). Could you briefly comment on this?\n\n6. Please show representations of MNIST, F-MNISt, etc.\n\n7. Given the correlation analysis, why not see whether NSA can be used\n   to detect or predict a specific level of poisoning?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4994/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698143101861,
            "cdate": 1698143101861,
            "tmdate": 1699636486987,
            "mdate": 1699636486987,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DvSJ7QJLav",
                "forum": "5HGPR6fg2S",
                "replyto": "Edwr1Ecr6t",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4994/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4994/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer WXtW"
                    },
                    "comment": {
                        "value": "Thank you very much for taking the time to write a detailed review. Your feedback has been extremely useful in updating our work. We address your points of concern below:\n\n>While I see the contribution favourably, there are major weaknesses precluding the presentation of the paper in its current form. The primary one is a missing analysis of fundamental properties. While I appreciate the broad range of different applications, this invariably means that some depth is lost and analyses are relatively superficial.\n\n**Response:** We extend the definition of NSA to show that it can measure point-wise discrepancies. All properties necessary establish its viability as a similarity metric (including its invariance to global isometries) and loss function are proven in Appendix B and C. We also move any speculative results to the appendix (Refer to Summary of Revisions Point 3) and expand on both the autoencoder experiments and the adversarial analysis with more results to cement NSA\u2019s viability as a representation similarity measure. The primary objective of this paper is intended to be the introduction of NSA and a showcase of its versatility. We intend on building on NSA and exploring its potential in-depth in future works.\n\n> Introducing a new measure and then showing its utility requires a more in-depth view of data, though. For instance, when introducing the latent representations in Table 1, readers are only shown the actual scores, but the primary assumption is that the scores actually capture the relevant properties of the data. Phrased somewhat hyperbolically: I believe that NSA can be calculated as described, but I do not understand whether it measures something 'interesting.' The fact that it correlates with performance metrics is relevant, but immediately suggests a more detailed comparison scenario, for instance in the form of an early stopping criterion or regularisation term. \n\n**Response:** Thank you for your feedback. We address this in **Concern 2** and **Concern 3** of the common response. Additionally, we discuss the viability of NSA as an early stopping criterion in Section 5.2 where we show that NSA stabilization correlates with test accuracy convergence. \n\n> I'd suggest to shorten the RTD explanation or substantially extend it. Currently, it makes use of jargon like , 'barcode,' and Vietoris--Rips filtrations that are not sufficiently explained (I am familiar with the work and I believe that a deep dive into topological methods is not required). \n>To simplify the notation, I'd either use the actual Euclidean norm in the definition of NSA or write x and 0 as vectors.\n\nWe have taken your feedback into consideration and simplified the explanation of RTD, we have also improved on the notation of NSA in the definition\n\n> The autoencoders experiment is somewhat out of place since the introduction sets up a paper on GNNs. Given the broad scope of NSA, I think it might be best to stay with the autoencoder comparison, using data with a known ground truth. This could take the form of building confidence by starting with simple toy examples like a 'Swiss Roll' or other data sets and showing that NSA matches the intuition.\n\n**Response:** We restructure the paper to reduce the focus on GNNs and address this shortcoming in **Concern 1** of the common response. We expand on the autoencoder experiments and move any speculative results with GNNs to the appendix. We also present the results of using NSA-AE on the Swiss Roll dataset. NSA can be used to minimize geodesic distances if the manifold dimension of the original representation space is known. We exploit this information to flatten a swiss roll with NSA-AE. The results are presented in Section 4.3 and in Figure 1 of the revised manuscript."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4994/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700346985452,
                "cdate": 1700346985452,
                "tmdate": 1700346985452,
                "mdate": 1700346985452,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1rBu6vGE2n",
                "forum": "5HGPR6fg2S",
                "replyto": "XW1t90wjTq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4994/Reviewer_WXtW"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4994/Reviewer_WXtW"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the changes and I believe that they are going in the right direction. Nevertheless, my main concerns remain unaddressed. There is still no discussion on the properties of the proposed measure with respect to capturing fundamental characteristics of data. I understand that NSA is shown to be a pseudo-metric, but there are numerous ways of defining pseudo-metrics between spaces; the property of being a pseudo-metric does not necessarily imply that salient characteristics are captured.\n\nAll the evidence for the practical utility of NSA is empirical so far, but introducing a novel measure like this requires a more in-depth analysis. I believe that such an analysis can be provided but it would involve a **major revision of the paper**, which is beyond the scope of this conference cycle."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4994/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700474132764,
                "cdate": 1700474132764,
                "tmdate": 1700474132764,
                "mdate": 1700474132764,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8hDIo9wKuT",
                "forum": "5HGPR6fg2S",
                "replyto": "x4FrtLU3nC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4994/Reviewer_WXtW"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4994/Reviewer_WXtW"
                ],
                "content": {
                    "comment": {
                        "value": "> Concern 4: NSA lacks motivation and is coarse.\n\nThe response by the authors already mentions relevant points. I need to point out that the main criticisms of my fellow reviewer colleagues and myself points out that, in contrast to metrics like RTD or CKA, the current version of the manuscript misses a more detailed analysis of what type of properties are actually captured by NSA. RTD, for example, can be linked to topological features, and thus benefits from a wealth of additional stability theorems, estimates, and so on. Additional analyses would really strengthen the paper here."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4994/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700475257951,
                "cdate": 1700475257951,
                "tmdate": 1700475257951,
                "mdate": 1700475257951,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xAnqbzuNJY",
            "forum": "5HGPR6fg2S",
            "replyto": "5HGPR6fg2S",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4994/Reviewer_wTyL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4994/Reviewer_wTyL"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new pseudometric as a way to measure the distance between two representation spaces. The authors then show how it can be used in various applications."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- The authors tackle a significant problem, with many downstream applications\n- The metric is reasonably novel"
                },
                "weaknesses": {
                    "value": "- The proposed pseudometric does not make any sense to me. One clear issue with it is the fact that it is not invariant to permutation between the vectors in the point cloud. This is a huge issue, in my mind, as there is no reason to assume the vectors are aligned so it is greatly impacted by a random permutation between them. The authors also don't give any sufficient motivation for this metric, besides the fact that it satisfies some basic properties like triangle inequality. \n- The paper is not well written and very confusing to read. Comparing representations and the proposed method is not specific to GNNs, yet the authors present it as one that is connected to GNNs. They also move between GNNs and point clouds making it hard to follow\n- The use of NSA as a regularizer in the VAE part is not clear at all. Do you compare the reconstructed with the original? It was not stated clearly. If this is the case, then this does not prove the usefulness of the NSA as the representations are aligned."
                },
                "questions": {
                    "value": "What is the motivation behind the NSA definition?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4994/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698701721560,
            "cdate": 1698701721560,
            "tmdate": 1699636486901,
            "mdate": 1699636486901,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RXtYdRD6JW",
                "forum": "5HGPR6fg2S",
                "replyto": "xAnqbzuNJY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4994/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4994/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer wTyL"
                    },
                    "comment": {
                        "value": "Thank you for taking the time to review our work. We address all your concerns below:\n\n> The proposed pseudometric does not make any sense to me. One clear issue with it is the fact that it is not invariant to permutation between the vectors in the point cloud. This is a huge issue, in my mind, as there is no reason to assume the vectors are aligned so it is greatly impacted by a random permutation between them. The authors also don't give any sufficient motivation for this metric, besides the fact that it satisfies some basic properties like triangle inequality.\n\n**Response:** Thank you for taking the time to review our work. NSA is a representation similarity measure and measures the discrepancy between two representation spaces. A permutation between the vectors in a point cloud would result in a loss of the one-to-one mapping that NSA requires and would fundamentally change the representation space and the relationships between the points of this space, thus representation similarity measures like NSA or CKA cannot be invariant to this type of permutation. NSA as a similarity measure is invariant to global isometries. \n\nNSA along with other Representation Similarity measures [1] are invariant to scaling, translation or rotation transformations on the representation spaces as they only compare the discrepancy between the relative positions of data in their own embedding spaces. Representation Similarity Measures are well motivated through several popular previous works like CCA [2], CKA [3], RSA [4] and RTD [5]. We provide proofs in Appendix A that NSA satisfies all the conditions to qualify as a pseudometric. Appendix B provides the proofs to demonstrate that it is invariant to isometries and satisfies the conditions necessary to qualify as a similarity index, as proposed by [3]. Appendix C proves that NSA is continuous and differentiable, allowing us to use it as a minimization objective. \n\nNSA is motivated by the need to fill a gap in existing similarity measure research. Current measures fall into one of the two categories: 1) They can quantify the discrepancy between two representation spaces but are not differentiable and cannot be used as a minimization objective in neural network training. 2) Measures that are differentiable [5] are very computationally expensive and cannot be employed in practical scenarios for large scale neural network training. NSA is differentiable and its complexity is quadratic in the number of points, facilitating its use as a minimization objective in any training paradigm that necessitates the alignment of embedding spaces such as dimensionality reduction, adversarial training, link prediction, knowledge transfer or distillation and semantic similarity among others without a significant loss in training efficiency. We present results that show NSA not only preserves representation structure (Section 4) but the aligned embeddings are effective in downstream tasks like link prediction (Section 4.2) and semantic similarity (Appendix M). We also show in Table 3 that NSA is over 10x faster than its closest competitor (RTD) while being used as an additional loss term. All of these experiments serve to showcase the versatility of NSA and its ability to serve in practical training scenarios.\n\n\n\n[1] Similarity of Neural Network Models: A Survey of Functional and Representational Measures by Klabunde, Max and Schumacher, Tobias and Strohmaier, Markus and Lemmerich, Florian, arXiv 2023. \n[2] SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability by Raghu, Maithra and Gilmer, Justin and Yosinski, Jason and Sohl-Dickstein, Jascha NeurIPS 2017.\n[3] Similarity of Neural Network Representations Revisited by Simon Kornblith, Mohammad Norouzi, Honglak Lee and Geoffrey E. Hinton.\n[4] Representational similarity analysis - connecting the branches of systems neuroscience by Kriegeskorte, Nikolaus and Mur, Marieke and Bandettini, Peter, Frontiers in Systems Neuroscience 2008.\n[5] Representation Topology Divergence: A Method for Comparing Neural Network Representations by Barannikov, Serguei and Trofimov, Ilya and Balabin, Nikita and Burnaev, Evgeny. ICML 2022."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4994/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700362038772,
                "cdate": 1700362038772,
                "tmdate": 1700362100042,
                "mdate": 1700362100042,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "yenKDfWePe",
            "forum": "5HGPR6fg2S",
            "replyto": "5HGPR6fg2S",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4994/Reviewer_BYXr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4994/Reviewer_BYXr"
            ],
            "content": {
                "summary": {
                    "value": "This manuscript presents a novel approach for comparing two point clouds through the introduction of a Normalized Space Alignment (NSA) technique, aimed at assessing the pairwise distances between corresponding points as a means of quantifying the similarity between two representations. The authors show that NSA possesses the properties of a pseudometric and demonstrate its applicability as a loss function within the context of autoencoder models. A set of experimental studies is conducted, offering insights into the behavior and performance of various graph neural network representations when analyzed using this newly proposed method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The authors have introduced a Normalized Space Alignment (NSA) technique, combining the computational efficiency of Centered Kernel Alignment (CKA) with the differentiability of Representation Tree Distance (RTD). \n\n2. The manuscript provides a theoretical demonstration, showcasing that NSA exhibits the properties of a pseudometric. Additionally, the authors have developed a differentiation scheme, enabling the integration of NSA as a loss function."
                },
                "weaknesses": {
                    "value": "Concerns on Structure Preservation: \n\n   1.The authors posit that the Normalized Space Alignment (NSA) technique is structure-preserving, yet there is a noticeable gap in addressing the underlying data's graph structure. NSA primarily focuses on measuring distances between two point clouds, neglecting the crucial graph edges. This oversight brings its capability to preserve graph structure into question. Moreover, although the paper intends to underscore NSA's unique potential in the context of Graph Neural Networks, the actual exploration of these capabilities is missing.\n\nIssues with Experimental Validation:\n 1. Lack of Downstream Task Evaluation: Given that NSA is proposed as a loss function, it is critical to evaluate its effectiveness in downstream tasks. Unfortunately, the paper lacks such evaluations. Insights into how the latent embeddings from an autoencoder, shaped by NSA, could enhance downstream task performance are notably missing.\n\n  2. Dataset Limitations: The paper\u2019s conclusions are drawn from experiments conducted solely on the Amazon Computers dataset for node classification. Different graph datasets possess varied properties and can elicit diverse behaviors, making it imperative to extend the analysis to a broader set of datasets for more robust and convincing results.\n\n  3. Inconsistency in Data Analysis: There is a puzzling contrast between the paper\u2019s stated focus on Graph Neural Networks (and graph data) and the NSA-AE analysis, which is not applied to graph datasets. \n\n   4. Adversarial Attack Analysis Shortcomings: The attempt to correlate NSA values with misclassification rates, aiming to use NSA as a metric for evaluating GNN resilience, lacks conviction. NSA values exhibit significant variability across different GNN architectures, and potentially across various graph datasets. The observed discrepancy, where GCN shows the highest misclassification rate while GCN-SVD has the highest NSA value, further complicates any straightforward interpretation based on NSA values alone.\n\n  5. Readability of Figures: The figures included in the paper suffer from readability issues, with some fonts being excessively small, hindering the reader\u2019s ability to fully grasp and interpret the presented data."
                },
                "questions": {
                    "value": "I have raised several points and posed various questions in the previous sections of my review. \n\nAdditionally, I have a specific inquiry pertaining to Figure 3. Regarding Figure 3, could you please clarify which two representations are being compared to calculate the NSA values presented? The text does not seem to provide explicit information on this aspect."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4994/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4994/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4994/Reviewer_BYXr"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4994/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698724022686,
            "cdate": 1698724022686,
            "tmdate": 1699636486780,
            "mdate": 1699636486780,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zbUP3t4DKR",
                "forum": "5HGPR6fg2S",
                "replyto": "yenKDfWePe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4994/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4994/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer BYXr"
                    },
                    "comment": {
                        "value": "Thank you very much for taking the time to review our work. We will address your concerns below:\n\n>The authors posit that the Normalized Space Alignment (NSA) technique is structure-preserving, yet there is a noticeable gap in addressing the underlying data's graph structure. NSA primarily focuses on measuring distances between two point clouds, neglecting the crucial graph edges. This oversight brings its capability to preserve graph structure into question. Moreover, although the paper intends to underscore NSA's unique potential in the context of Graph Neural Networks, the actual exploration of these capabilities is missing.\n\nNSA belongs to the class of similarity indices just like Centered kernel alignment and Representation Topology Divergence. NSA measures the discrepancy between two embedding spaces. As a similarity index it is capable only of measuring how dissimilar two embedding spaces are. As a minimization objective, it is capable of aligning one embedding space to another (parent or ideal) embedding space. Scenarios where embedding space alignment is of paramount importance would be knowledge transfer, distillation, dimensionality reduction, robust training and model interpretability among others. We address the issue of the paper being too focused on GNNs in **Concern 1** of the common response. Another reason for the paper utilizing GNNs in its empirical analysis is that exploration of GNN\u2019s embedding capabilities are largely missing from previous literature. \n\n\n\n\n> Lack of Downstream Task Evaluation: Given that NSA is proposed as a loss function, it is critical to evaluate its effectiveness in downstream tasks. Unfortunately, the paper lacks such evaluations. Insights into how the latent embeddings from an autoencoder, shaped by NSA, could enhance downstream task performance are notably missing.\n\nWe present additional experiments to validate NSA-AE\u2019s downstream task performance in section 4.2 and appendix M. Please refer to **Concern 2** of the common response for additional details.\n\n\n\n> Dataset Limitations: The paper\u2019s conclusions are drawn from experiments conducted solely on the Amazon Computers dataset for node classification. Different graph datasets possess varied properties and can elicit diverse behaviors, making it imperative to extend the analysis to a broader set of datasets for more robust and convincing results.\n\nWe choose to train our models on the Amazon Computer Dataset as it has the ideal tradeoff between downstream task accuracy and dataset size. GNNs are almost always shallow as the benchmark datasets currently available and utilized by the research community are very small. Training a 2 layer GNN on these datasets would not show a strong linear correlation across layers that we wanted to demonstrate with NSA. The larger datasets have very poor performance with popular GNN architectures, often not reaching convergence. Since all our empirical analysis relies on the assumption that representation spaces stabilize as they converge (Figure 3), a poor performing dataset cannot be used for our experiments. \n\nWe extend our experiments to 4 additional datasets to demonstrate NSA\u2019s robustness. We utilize the Cora, Citeseer, Pubmed and Flickr datasets and showcase their results on the sanity tests, cross architecture tests and cross downstream task tests in Appendix Q.\n\n\n\n> Inconsistency in Data Analysis: There is a puzzling contrast between the paper\u2019s stated focus on Graph Neural Networks (and graph data) and the NSA-AE analysis, which is not applied to graph datasets.\n\nThank you for bringing this shortcoming to our attention. We address this in **Concern 1** of the common response and have made the necessary revisions in our manuscript to improve the flow of the content in the paper. \n\n\n\n> Adversarial Attack Analysis Shortcomings: The attempt to correlate NSA values with misclassification rates, aiming to use NSA as a metric for evaluating GNN resilience, lacks conviction. NSA values exhibit significant variability across different GNN architectures, and potentially across various graph datasets. The observed discrepancy, where GCN shows the highest misclassification rate while GCN-SVD has the highest NSA value, further complicates any straightforward interpretation based on NSA values alone.\n\n Thank you for your feedback. We address this in **Concern 3** of the common response and present additional experiments in Section 6 to help interpret the initial results better. We have also improved the writing in this section to help with readability."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4994/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700315163968,
                "cdate": 1700315163968,
                "tmdate": 1700315197301,
                "mdate": 1700315197301,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]