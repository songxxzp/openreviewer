[
    {
        "title": "Reverse Diffusion Monte Carlo"
    },
    {
        "review": {
            "id": "2KfT7SbITZ",
            "forum": "kIPEyMSdFV",
            "replyto": "kIPEyMSdFV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8638/Reviewer_NhM6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8638/Reviewer_NhM6"
            ],
            "content": {
                "summary": {
                    "value": "This work takes a very well-known reformulation of the score in VP-SDEs (see [1,2,3]) that is admissible to the sampling setting where one only has access to the density of the target distribution up to a constant unlike in standard diffusion models where one has access to samples. The authors propose estimating the score via MC, 100% akin to the heat-semigroup (Schroedinger Foellmer Sampler) approach in [4], which rather than time reversing a VP-SDE it can be seen as time reversing the h-transform of a pinned Brownian motion [9].\n\nThere is quite a bit of missing literature that has been already explored empirically over the last 2 years in the works of [1,2,3,4]  and also [5, 6, 7, 8].\n\nThe ULA-based estimators proposed in this work are the main novelty focus, furthermore, complexity guarantees are provided for these estimators and are shown to outperform vanilla ULA approaches, which is quite promising combined with the experiments added during the rebuttal. \n\nNotice that in contrast to parametric VI approaches the inner loop scheme proposed has theoretical guarantees for estimating the score that is practically feasible unlike [4,8] which is hindered nonpractical due to the. nonconvex objective required to train the NN estimators of the score.\n\n[1] Vargas, F., Grathwohl, W.S. and Doucet, A., 2022, September. Denoising Diffusion Samplers. In The Eleventh International Conference on Learning Representations. https://openreview.net/forum?id=8pvnfTAbu1f\n\n[2] Berner, J., Richter, L. and Ullrich, K., 2022. An optimal control perspective on diffusion-based generative modeling. In NeurIPS 2022 Workshop on Score-Based Methods.\n\n[3] Zhang, D., Chen, R.T.Q., Liu, C.H., Courville, A. and Bengio, Y., 2023. Diffusion Generative Flow Samplers: Improving learning signals through partial trajectory optimization. arXiv preprint arXiv:2310.02679.\n\n[4] Vargas, F., Reu, T. and Kerekes, A., 2023, July. Expressiveness Remarks for Denoising Diffusion Based Sampling. In Fifth Symposium on Advances in Approximate Bayesian Inference.\n\n[5] Huang, J., Jiao, Y., Kang, L., Liao, X., Liu, J. and Liu, Y., 2021. Schr\u00f6dinger-F\u00f6llmer sampler: sampling without ergodicity. arXiv preprint arXiv:2106.10880.\n\n[6] Vargas, F., Ovsianas, A., Fernandes, D., Girolami, M., Lawrence, N.D. and N\u00fcsken, N., 2023. Bayesian learning via neural Schr\u00f6dinger\u2013F\u00f6llmer flows. Statistics and Computing, 33(1), p.3.\n\n[8]  Tzen, B. and Raginsky, M., 2019, June. Theoretical guarantees for sampling and inference in generative models with latent diffusions. In Conference on Learning Theory (pp. 3084-3114). PMLR."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is well-written and formatted, and has the potential to become a theory-oriented paper with some extra work if the contributions are restructured,  and proper acknowledgments are made, this would require significant re-writing and further development and discussion of lemma 3 and proposition 2, and also theoretical comparison to other methods beyond ULA, simply the statement of these 2 results alone is not a strong enough contribution for ICLR. \n\nNote that the ULA-based estimators proposed in this work can be seen as a novel (e.g. Algorithm 2), however, without proper experimentation/numerics, this is still not a complete contribution either, currently, the paper is a lot of floating ideas without a concrete exploration of any particular, in part, this is due to the authors not being aware of the current state of the field. However, having to run a couple of inner ULA iterations every time one has to evaluate the drift is highly nonpractical, and claims made in the paper such as :\n\n\"\nVia this combination, we are\nable to efficiently obtain accurate score estimation by virtue of the ULA algorithm when t is close\nto T. When t is close to 0, we are able to quickly obtain rough score estimates via the importance\nsampling approach.\n\"\n\nAre highly unvalidated as 2 dimensional toy examples without proper comparison to other diffusion sampling-based approaches."
                },
                "weaknesses": {
                    "value": "I will break this down into 2 subgroups\n\n1. Lack of Novelty (+ failure to acknowledge prior work)\n\n    *  Lemma 1 is very straight-forward and non-practical as you cannot sample from q, instead in practice the authors do IS and sample from the OU-process' transition kernel which is available analytically, which in turn results in expressing the score as done in [1] ( see equation 84 in [1] and the line that follows it connecting it to the score, or the equation above Equation 24 of the same paper ... or equations 10-13 in [4] ). \n\n2. The paper falls short as a sampling paper empirically\n\n    * Evaluations in 2d simply do not meet the bar for a conference paper. Methodologies such as SMC among many other modern ML variants are able to do quite well in multimodal 2d examples. \n    * This MC-IS method for estimating the score will NEVER work well in high dimensions due to variance and thus why works such as [1,2,3,4] which are clearly aware of this formulation (as they either state it in their appendices or use it for subsequent calculation) pursue an optimization alternative to estimating the drift.  See [5] Figure 2 for how poor the performance of these estimators is compared to LMC and alternative NN approaches for learning the score as dimension scales.  The authors briefly allude to this issue and propose combining with an inner ULA loop, however as before the authors missed that previous and more practical approaches (based on score matching) have already been developed to address this same issue, thus without any empirical validation and careful comparison it is very unclear why one would select the proposed approach.\n\nNote prior results establishing the exact same expressions for the OU-drift and similar expressions for score-matching SDEs applied to sampling have been public since late 2021."
                },
                "questions": {
                    "value": "Here are some suggestions:  \n\n1. For the MC estimator of the score of the forward SDE please cite [5] as the original work to propose this class of estimators for sampling with SDEs. This is not the score of an OU process it is instead the score of a \"pinned Brownian motion\" (A Brownian bridge like SDE which starts at the target distribution and maps it to a point mass). The authors might be tempted to argue that this is not the score however note that this quantity  (-logarithmic derivative of the value function) is related to the score by an additive term of $\\nabla \\ln p_t^{\\mathrm{ref}}$ where $p_t^{\\mathrm{ref}}$ is the marginal density of the associated reference process (in the case of an OU forward process this is just a linear term), see Remark 4 and its proof in [4].\n2. The authors should cite and acknowledge that their Lemma 1 (or exactly equivalent versions thereof)  have already been discussed and derived in the works [1,4] in the context of sampling, and mathematical optimization objectives which aim to learn an estimator for this exact same score the context of sampling have been explored empirically and theoretically in [1,2,3] (also 4 but this is concurrent work).\n3. Something that could strengthen the theory contribution is comparing to the Log Sobolev constant of the score in Schrodinger follmer samplers [5] (The score of a pinned brownian motion) and using this to quantify algorithmic design insights for the forward process.  \n4. Overall these theoretical contributions can have a stronger impact if written with the state of the current field on diffusion-based samplers in mind, and contextualizing how these results apply to practically successful methodologies. \n7. Note that since the estimator for the score is a ratio of expectations computed via MC the resulting estimator is itself biased, there is no discussion of this in the paper.\n\n**Update:** in the revised versions authors do a careful literature review of both prior and parametric methods which have already explored score-based time reversals for sampling, furthermore with the updated numerics found in the appendix the proposed schemes offer for a much more compelling story.\n\nTo justify the notable increase in score I will list a couple of points:\n\n* The log Sobolev constants derived for the OU process and the overall context of the analysis could be of further use and impact to works that focus on time reversal and gen modeling or time reversal and sampling. So there is a potential for impact beyond the proposed algorithm.\n* The assumptions in the work have been refined and the sketches are clearer to read in the revised versions.\n* The authors have been responsive and very helpful in clarifying points throughout the discussion (albeit a bit combative ... which is not ideal but to a tolerable level)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8638/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8638/Reviewer_NhM6",
                        "ICLR.cc/2024/Conference/Submission8638/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8638/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698427587201,
            "cdate": 1698427587201,
            "tmdate": 1700292470398,
            "mdate": 1700292470398,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qMdIk7H9ki",
                "forum": "kIPEyMSdFV",
                "replyto": "2KfT7SbITZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8638/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8638/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Quick clarification"
                    },
                    "comment": {
                        "value": "Thank you for your review.\nFor better discussion, we would like to clarify certain misunderstandings before presenting our formal rebuttal. Firstly, we acknowledge the concerns raised regarding the omission of parameterized sampling algorithms from our discussion. \nThis was based on the fact that these algorithms primarily fall under the Variational Inference (VI) regime, while our algorithm is an improved version of MCMC.\nNevertheless, we are grateful for your references and suggestions and will incorporate a discussion of these elements in our updated manuscript.\n\nIt is crucial to emphasize that our algorithm is non-parametric, focusing on the theoretical analysis and understanding of both asymptotic and non-asymptotic behaviors of discretized dynamics. Here, our rdMC demonstrates superior performance compared to Langevin-based algorithms, particularly for ill-behaved distributions.\n\nA key aspect of our work is the exploration of log-Sobolev constant dependency of the gradient complexity, where we have found that rdMC outperform conventional methods. This distinction makes a direct comparison with parameterized models infeasible. \n\nOur work does indeed share similarities with parameterized diffusion-based sampling algorithms. \nSome ideas might be embedded in your references, which will be discussed in our next version.\n However, the main algorithm and concentrations still differ. Once the parametric model is chosen, there is always an asymptotic error. In contrast, our method turns score-matching into a non-parametric mean estimation problem and is asymptotically exact, just like the traditional Monte Carlo methods.\nOur work represents a pioneering effort in examining non-parametric algorithms that leverage diffusion dynamics, backed by robust theoretical guarantees. We believe that our theoretical foundations distinctly support these algorithms.\n\nWe acknowledge your proposed works could be viewed as a parametric VI version of rdMC, analogous to the relationship between Black-Box Variational Inference and Langevin Dynamics in the VI-MCMC pair [1]. This perspective offers an intriguing angle, viewing diffusion models as a Parametric Approximation to rdMC. However, it is important to note that it's not entirely appropriate to directly compare a newly established, theoretically guaranteed MCMC work with VI models. Our algorithm can approximate the score with arbitrarily small error. \nYet, the potential connections between these methodologies present an interesting avenue for discussion.\n\nIn light of your comments, we plan to have more discussion about connections between our algorithm and the ones you proposed. Additionally, we will extend our empirical results to more high-dimensional scenarios to further substantiate our claims.\n\nWe genuinely appreciate your constructive suggestions and look forward to incorporating these insights into our revised work.\n\n\n[1] Black-Box Variational Inference as Distilled Langevin Dynamics"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8638/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699673692126,
                "cdate": 1699673692126,
                "tmdate": 1699678609737,
                "mdate": 1699678609737,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "p4mB6Kbnve",
                "forum": "kIPEyMSdFV",
                "replyto": "qMdIk7H9ki",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8638/Reviewer_NhM6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8638/Reviewer_NhM6"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the prompt response"
                    },
                    "comment": {
                        "value": "Dear Authors,\n\nI am indeed aware that some of these approaches I have refferenced are parametric VI-based methods as opposed to your nonparametric approach. \n\nHowever, that does not warrant completely missing out on acknowledging these methods in particular when many of these papers have already explored/proposed some of the main propositions/remarks in your work.\n\nMy concern which I feel is being dodged in this review is that estimating the score of a VP-SDE for the task sampling has already been proposed by at least 3 prior works before yours, and this is one of the central remarks/observations you pose as novel, which it is not, whether these methods are VI based or not is not relevant, the point is that they are estimating the same score as your work and they should be acknowledged to be the first to do so. Also, note the work in [2] has many remarks pertaining to the score beyond parametric estimation. \n\nWhat should be acknowledged more carefully is that re-expressing the score/drift of a time reversal for sampling is something that has already been studied quite extensively, you will see expressions in [2] for example that are akin to your IS estimator. Your work is not the first to study estimators for these quantities, you are the first to propose a very specific estimator using ULA.\n\nFurthermore note that if you parse through the references I have also provided a non-parametric approach for estimating the score of a Pinned Brownian motion for sampling [1], (heat semi-group rather than OU semi-group as you do). To my knowledge this work from 2021 is the first work to estimate the drifts of time reversals via virtually the same expectations you have in your work only that they take the IS route, the main differentiating factors in your work are the use of a ULA sampler to estimate the drift. \n\nRather than circumventing these and arguing how different they are, I suggest the authors carefully and honestly acknowledge prior work and use it to more precisely quantify their contribution. For example, you make a good point here:\n\n> It is crucial to emphasize that our algorithm is non-parametric, focusing on the theoretical analysis and understanding of both asymptotic and non-asymptotic behaviors of discretized dynamics. Here, our rdMC demonstrates superior performance compared to Langevin-based algorithms, particularly for ill-behaved distributions.\n\nThus I am sorry to say I find this clarification to obfuscate the issue at hand rather than clarify. That said I appreciate the time put in your response and look forward to the revised version of this script.\n\n[1] Huang, J., Jiao, Y., Kang, L., Liao, X., Liu, J. and Liu, Y., 2021. Schr\u00f6dinger-F\u00f6llmer sampler: sampling without ergodicity. arXiv preprint arXiv:2106.10880.\n[2] Vargas, F., Reu, T. and Kerekes, A., 2023, July. Expressiveness Remarks for Denoising Diffusion Based Sampling. In Fifth Symposium on Advances in Approximate Bayesian Inference."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8638/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699708602497,
                "cdate": 1699708602497,
                "tmdate": 1699708602497,
                "mdate": 1699708602497,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Pamy1ECsNb",
                "forum": "kIPEyMSdFV",
                "replyto": "2KfT7SbITZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8638/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8638/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the prompt response"
                    },
                    "comment": {
                        "value": "Dear reviewer,\n\nThank you for your prompt response. Let me be straightforward about this. You have missed the main contribution of this paper.\n\nOur method is not an approximate Bayesian inference method, like the ones you have cited. Our method is asymptotically exact, like the traditional MCMC methods. It leverages the reverse diffusion process, but does not really build a diffusion model. That is why we can obtain an overall convergence rate that decreases to zero as the computational complexity increases. The existing works, after choosing the parametric (neural network) model, all suffer from an irremovable error.\n\nFor [Huang et al.] you mentioned, the assumptions are much stronger than log-Sobolev assumptions, as they assume the Lipschitz continuity of the drift term, so they are unable to deal with general distributions and make a direct comparison with MCMC. Also, the drift term is estimated by importance sampling, which limits the usage as you mentioned.\n\nWe will have a separate paragraph citing these works and discussing the distinction."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8638/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699808719751,
                "cdate": 1699808719751,
                "tmdate": 1699810013890,
                "mdate": 1699810013890,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sG6zlSv9Ki",
                "forum": "kIPEyMSdFV",
                "replyto": "2KfT7SbITZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8638/Reviewer_NhM6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8638/Reviewer_NhM6"
                ],
                "content": {
                    "title": {
                        "value": "Dismissive language"
                    },
                    "comment": {
                        "value": "Dear Author , \n\nIt is not in your interest to be dismissive towards a reviewer. I am taking the time and effort to engage with you promptly and I am willing to reassess, however I do implore you to be respectful and not dismissive.\n\nFirstly I am 100% aware that your method is asymptocally exact and more akin to mcmc I have not missed this focus and aspect of your contribution. My remarks are not taking this away so please stop trying to demerit my review and address the following individual points I have already made.\n\n1 . I have citied a paper in my review which you have neither acknowledged nor responded too [1]  this work is also asymptotically exact it is more MCMC alike and it is NOT approximate inference. \n2. Both MCMC methods and approximate inference methods often target the same task . That is a sampling from unnormalized densities that can be evaluated pointwise .  The point I am raising is the  following \n\n\u201c There are prior works that write down the exact same expectation formulations of the score to tackle the sampling task.\u201d\n\nMost of these prior works (not [1]) decide to learn an estimator for this score via a variational approach rather than an asymptotically exact approach.  Please carefully read these other papers I agree the VI aspect is different but that is not my point be more thoughtful and read my feedback carefully.\n\n\nIn short please carefully address these points and stop dismissing them as non-relevant, this is very poor etiquette I am trying to work with you here and you are making my job much harder, let\u2019s remain professional and on topic, please.  \n\nPlease be mindful of my time and effort. \n\n[1] Huang, J., Jiao, Y., Kang, L., Liao, X., Liu, J. and Liu, Y., 2021. Schr\u00f6dinger-F\u00f6llmer sampler: sampling without ergodicity. arXiv preprint arXiv:2106.10880."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8638/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699811083026,
                "cdate": 1699811083026,
                "tmdate": 1699820225133,
                "mdate": 1699820225133,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fF2X7dIPz0",
                "forum": "kIPEyMSdFV",
                "replyto": "2KfT7SbITZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8638/Reviewer_NhM6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8638/Reviewer_NhM6"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response and mediation"
                    },
                    "comment": {
                        "value": "Dear reviewers,\n\nThe score 1 was not intended as dismissive of the work, but mostly to alert the strong concern of lacking evaluations and the very incomplete literature, in the current version of the manuscript. This score will very likely change as the revised version is uploaded, especially since I believe you understand some of the concerns I raised and at least part of them are not too difficult to address. \n\nNotice that the open review platform allows for a score of 1, its part of the accepted scores / formal review process and is not intended to dismiss the nature of the contribution but in this case at least point to the current state of the paper, e.g. its presentation, literature review, numerics, claims, etc.\n\n> Anyway, let\u2019s move forward and stop those emotional evaluations.\n\nAgreed, let's stay focused. In that spirit, I want to revist some of your points in the rebuttal that I possibly missed / misinterpreted:\n\n> 1.   It is crucial to emphasize that our algorithm is non-parametric, focusing on the theoretical analysis and understanding of both asymptotic and non-asymptotic behaviors of discretized dynamics. ... \n\nI think we have gone back and forth on this point a bit already. To summarise my views here:\n\n* I agree the proposed schemes in this work is line with traditional MCMC works, thus comparisons to algorithms of this flavour such as ULA,  SFS [1], and such seem more than reasonable.\n* There is a \"diffusion models for sampling\" aspect to your algorithm, and whilst prior work is mostly (not entirely e.g. [1]) focused on VI approaches to estimate the score, the overal should be discussed and prior relevant formulations of the score should be credited appropiately.\n\n> 2. A key aspect of our work is the exploration of log-Sobolev constant dependency of the gradient complexity, where we have found that rdMC outperform conventional methods. This distinction makes a direct comparison with parameterized models infeasible.\n\nI agree mostly here and this is something that I missed in my initial review. I do think you are justified in not needing to compare empirically to these approaches, the comparison to ULA  / MCMC schems here is sufficient.\n\nHowever, as I mentioned before there is a significant overlap in that the discussed parametric approaches are trying to estimate the same score. \n\n> The existing works, after choosing the parametric (neural network) model, all suffer from an irremovable error.\n\nThis is true but notice that prior work [2] establishes quite detailed bounds on the error that these parametric estimators of the score can potentially achieve, and it may be interesting (**yet not necessary**) to discuss at a high level how the complexity of these bounds compares to the results on the inner loop you have provided.\n\nOf course, in practice, these bounds are not realistic (whilst yours are) as the networks need to be trained via a non-convex objective that is not guaranteed to reach these errors.\n\nSomething I mentioned in my initial review, was that I found your log-Sobolev explorations quite exciting as they might also have something more to say in regards to expressiveness bounds when using NN parametrized estimators [2]. This is not directly relevant and you do not need to address this in the manuscript.\n\n> We believe that our theoretical foundations distinctly support these algorithms.\n\nI do agree with this point, although as mentioned before this is not the first work that is entirely of this flavour (e.g. [1]) an important differentiating factor you offer however is a much more refined inner loop with finer guarantees compared to [1] ([1] simply does IS with no inner-loop and is thus very high variance).\n\nI think the presentation of the work would be improved and made much more clear if this differentiating factor (in particular the complexity of the inner loop scheme) is emphasized as the main contribution / differentiating factor, which also renders the method more practical compared to [1]. \n\nThis said more high dimensional numerics (and complex dists) are needed to truly verify the method empirically. In particular, such verifications must be done such that ULA and the new proposed algorithm have roughly the same computation budget (e.g. inner_loop_steps * outer_loo_steps  = ula_steps). \n\nP.S. I have updated my review a bit to reflect on some of this discussion.\n\n[1] Huang, J., Jiao, et al Schr\u00f6dinger-F\u00f6llmer sampler: sampling without ergodicity \n\n[2] Vargas F. Et al Expressiveness Remarks for Denoising Diffusion Based Sampling."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8638/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699819631772,
                "cdate": 1699819631772,
                "tmdate": 1699822014387,
                "mdate": 1699822014387,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LsRL9rRHWm",
                "forum": "kIPEyMSdFV",
                "replyto": "h1PmE6wLjJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8638/Reviewer_NhM6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8638/Reviewer_NhM6"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your revision"
                    },
                    "comment": {
                        "value": "with the current numerical ablations, the overall paper and method present a more compelling story I have updated my score to reflect this, for the camera-ready version I would suggest to the authors to (I believe you get an extra page which should allow you for this):\n\n1. Move the high-dimensional results to the main rather than the appendix\n2. Add one or two more realistic/challenging classical targets for MCMC (e.g. Neals Funnel,  LGCP, etc) you can see this found in quite standard/seminal ULA-MCMC papers e.g.  [9]\n\n\n[9] Girolami, M. and Calderhead, B., 2011. Riemann manifold Langevin and hamiltonian monte carlo methods. Journal of the Royal Statistical Society Series B: Statistical Methodology, 73(2), pp.123-214."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8638/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700048749424,
                "cdate": 1700048749424,
                "tmdate": 1700048749424,
                "mdate": 1700048749424,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "J6PWOq4cOT",
                "forum": "kIPEyMSdFV",
                "replyto": "2KfT7SbITZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8638/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8638/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your swift feedback. \n\nWe have incorporated experiments of Neal's Funnel into our Appendix F.5. It is shown that our algorithm can make a better balance between exploration and precision for Neal's Funnel sample.\n\nMore comprehensive empirical studies and bags of tricks could be a promising direction for future exploration. \nIn this paper, our focus is on strengthening the theoretical underpinnings and enhancing our understanding of the rdMC algorithm  (as well as the benefits of the diffusion-like dynamics) compared with MCMC.  \n\nIf you have any further inquiries or suggestions, please do not hesitate to reach out to us."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8638/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700105136729,
                "cdate": 1700105136729,
                "tmdate": 1700105285844,
                "mdate": 1700105285844,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1aSltcJQbf",
                "forum": "kIPEyMSdFV",
                "replyto": "J6PWOq4cOT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8638/Reviewer_NhM6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8638/Reviewer_NhM6"
                ],
                "content": {
                    "title": {
                        "value": "SFS Disucssion in appendix"
                    },
                    "comment": {
                        "value": "Dear Authors,\n\nThank you for the funnel results the story is indeed more compelling. I do appreciate the focus of your contributions to be mostly theoretical although it's worth noting ICLR will have a largely applied audience so adding the high dim experiments in the camera-ready version were you get an extra page could heavily increase the audience, and impact (follow-up works). \n\nAnother thing, I keep missing is that the comparison to ULA on the theoretical side is really not quite remarked clearly enough. there are a couple of paragraphs (e.g. section. 4.1 paragraph following Lemma 2 ) where its slightly mentioned, the shrinking effect of the OU process on $q_t$ made this clear to me, and its somewhat clear how a less complex distribution is being targeted at each point.  However I think it would benefit a lot if you could have either a targeted remark or a subsubsection that emphasizes this much more clearly \"The algorithm exhibits lower isoperimetric dependency compared to conventional MCMC techniques\", because at the moment I have to navigate back and forth looking for the complexity remarks on ULA, it would help to have something a bit more side by side. \n\nI'm currently going through the appendix one minor comment is you use Pinsker's inequality maybe  ~3 times, and on 2 occasions it seems to be without reference e.g. page 17 and 14.\n\nFinally in your comparison to SFS there is a Lipchitz constant remark which Im not sure I fully follow: \n\n> However, due to the strong \u00a8assumptions of the Schrodinger-F \u00a8 ollmer process, it remains open to compare SFS with conventional \u00a8 MCMC under general conditions. Specifically, SFS requires the Lipschitz continuity of the drift term, which makes the tail of initial and target distribution highly dependen ...\n\nFirstly the drift in SFS (with some further derivation using the h-transform / playing around with the provided expectations) can be decomposed into two terms roughly speaking (assuming volatility of 1): \n\n$$b(x,t) = \\frac{x}{t} +  \\nabla \\ln p_{T-t} (x) $$\n\nWhere $ \\nabla \\ln p_{t} (x) $ is the score of the SDE:\n\n$$ x_0 \\sim p^*$$\n$$dx_t = -\\frac{x}{T-t} \\mathrm{d}t + \\mathrm{d}W_t$$\n\nThis SDE transports the target data distribution $p^*$ to a delta centered at 0 $\\mathrm{Law}x_T = \\delta_{0}$. Either way, their framework holds if one assumes $ \\nabla \\ln p_{T-t} (x)$ is Lip, my understanding is that your work also makes this assumption **[A1]**? in which case I'm not sure this **Specifically, SFS requires the Lipschitz continuity of the drift term,** can be highlighted as a differentiating factor."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8638/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700219486444,
                "cdate": 1700219486444,
                "tmdate": 1700219486444,
                "mdate": 1700219486444,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PLvs4Pfv0G",
                "forum": "kIPEyMSdFV",
                "replyto": "2KfT7SbITZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8638/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8638/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your helpful comments. We have highlighted the \u201clower isoperimetric dependency\u201d in Section 4.1.  Moreover, the parts related to Pinsker's inequality\u00a0were rearranged for better reading experience.\n\nFor the Lipschitz continuity of the drift term, we add more explanations in our Appendix. We summarize it as below \n\nThe main reason is that the delta distribution side makes $p_t$ ill-behaved (when the variance is approaching 0, which makes $-\\log p_t \\approx \\Vert x\\Vert^2/(2\\sigma^2)$ is exploding). They also provided a sufficient condition when both $p_*\\cdot\\exp(\\Vert x\\Vert^2)$ and its gradient are Lipschitz continuous, and the former is bounded below by a positive value. However, this condition may not be met when the variance of $p_*$ exceeds 1, limiting its general applicability of this condition.\nThus, the generality of SFS remains an open question.\nOn the other hand, due to the smoothness of $p_\\infty$ in OU process, the smoothness of $\\log p_t$ is widely accepted in diffusion analysis [1,2]. [3] also suggest that the smoothness of concave $\\log p_0$ can imply the smoothness of $\\log p_t$. In summary, the smoothness of $\\log p_t$ in OU process is much weaker than a process towards to a delta distribution, which generalize the analysis to more distributions.\n\n\n\n\n[1] Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru R Zhang. Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions. arXiv preprint, arXiv:2209.11215, 2022a.\n\n[2] Hongrui Chen, Holden Lee, and Jianfeng Lu. Improved analysis of score-based generative modeling: User-friendly bounds under minimal smoothness assumptions. In International Conference on Machine Learning, pp. 4735\u20134763. PMLR, 2023.\n\n[3] Holden Lee, Chirag Pabbaraju, Anish Sevekari, and Andrej Risteski. Universal approximation for log-concave distributions using well-conditioned normalizing flows. arXiv preprint arXiv:2107.02951, 2021a."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8638/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700290694378,
                "cdate": 1700290694378,
                "tmdate": 1700290709577,
                "mdate": 1700290709577,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fN000zLqnU",
                "forum": "kIPEyMSdFV",
                "replyto": "PLvs4Pfv0G",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8638/Reviewer_NhM6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8638/Reviewer_NhM6"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the revision and the very insightful discussion these are indeed much more convincing/thorough arguments , I have raised my score again to reflect the recent changes, I think now there\u2019s a much more compelling story in terms of comparing to competing / related approaches as well as the introduction / general flow of the paper has drastically improved.\n\nThank you for putting the time to address my concerns despite the initial bumpy discussion."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8638/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700292737770,
                "cdate": 1700292737770,
                "tmdate": 1700292737770,
                "mdate": 1700292737770,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Ta6M9DwGlL",
            "forum": "kIPEyMSdFV",
            "replyto": "kIPEyMSdFV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8638/Reviewer_qHDx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8638/Reviewer_qHDx"
            ],
            "content": {
                "summary": {
                    "value": "This paper explores reverse diffusion in Monte Carlo sampling, transforming score estimation into mean estimation. The algorithm claims to approximate the target distribution accurately, especially for Gaussian mixture models, outperforming Langevin-style MCMC methods. They claim that this algorithm offers a fresh solution for complex distributions."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- It is an extremely interesting problem to investigate. \n- It is easy to follow.\n- Theoretical results seem to support most of their claims."
                },
                "weaknesses": {
                    "value": "- Lack of practical and experimental results for complex distributions e.g. high-dimensional multimodal distributions. The method's performance is based on empirical observations and may not generalize well across diverse datasets or problem domains. Its effectiveness might be limited to specific scenarios and may not be universally applicable.\n- Lack of complexity analysis. The combination of different sampling techniques (importance sampling, ULA) adds algorithmic complexity. Managing the interactions between these techniques and ensuring their proper integration can be challenging.  Also, The method might demand significant computational resources, especially when dealing with large sample sizes and high-dimensional spaces. This could limit its practicality for resource-constrained applications.  The sample size required for accurate estimation scales exponentially with the dimension due to the KL divergence between distributions. This exponential growth can make the method computationally infeasible for high-dimensional spaces. Could the authors please elaborate on these issues?\n- The accuracy of the estimation relies heavily on the dimensionality of the problem. High-dimensional spaces exacerbate the sample size requirement, making it challenging to apply the method effectively in real-world applications.\n- Creating n Monte Carlo samples at each iteration can be computationally expensive, especially if n is large. This might limit the method's scalability and efficiency for high-dimensional or complex distributions.\n- The method uses random samples $\\xi$ at each iteration. The quality of these random samples is crucial; if they are not truly random or are biased in some way, it can introduce errors in the sampled results. Furthermore, the way the samples are generated and combined in the update equation (step 6) could introduce bias if not done correctly. Biased estimators can lead to incorrect conclusions about the target distribution.\n-  The method's performance might degrade in high-dimensional spaces. Monte Carlo methods often face challenges in high-dimensional settings due to the curse of dimensionality, where the sampling space becomes sparse, making it harder to obtain representative samples.\n- How sensitive is this framework to initial distribution $p_0$?"
                },
                "questions": {
                    "value": "Please refer to the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8638/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698711744579,
            "cdate": 1698711744579,
            "tmdate": 1699637081372,
            "mdate": 1699637081372,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "e7zVdQDVKX",
                "forum": "kIPEyMSdFV",
                "replyto": "Ta6M9DwGlL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8638/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8638/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Thanks for your suggestions. In summary, we have added more experiments (Appendix F3 F4 F5) and highlight our complexity analysis. Our current one includes both inner, outer loop, and sample-size, which is still much more effient than Langevin-based ones. \n Both inner iterations and samples do not bring too much computation overhead compared with exponential log-Sobolev constant. We also verify that our algorithm is not sensitive to $\\tilde{p}_0$.**\n\n**If you have any further inquiries or suggestions, please do not hesitate to reach out to us. Detailed response are as below.**\n\n>  *Lack of practical and experimental results for complex distributions*\n\nThanks. We have added the experiments for high-dimensional setting in our Appendix F3. In general, as the leading factor of isolated multi-mode distribution is the log-Sobolev constant (exponential dependency), the dimension dependency (polynomial) is not the main challenge. Thus, our algorithm still has significant improvement.\n\n>  *Lack of complexity analysis. The combination of different sampling techniques (importance sampling, ULA) adds algorithmic complexity. Managing the interactions between these techniques and ensuring their proper integration can be challenging. Also, The method might demand significant computational resources, especially when dealing with large sample sizes and high-dimensional spaces. This could limit its practicality for resource-constrained applications. The sample size required for accurate estimation scales exponentially with the dimension due to the KL divergence between distributions. This exponential growth can make the method computationally infeasible for high-dimensional spaces. Could the authors please elaborate on these issues? The accuracy of the estimation relies heavily on the dimensionality of the problem. High-dimensional spaces exacerbate the sample size requirement, making it challenging to apply the method effectively in real-world applications.*\n\n Our paper provides a detailed complexity analysis in both Sections 4.1 and 4.2 under different assumptions. We incorporate both gradient complexity and sample complexity within our propositions, directly stemming from the log-Sobolev constant estimation and Theorem 1. \n\n**We highlight that our theory indicate that considering inner * outer loop, our algorithm is still much more efficient than Langevin-based algorithms. Our experimental results also verify this point.**\n\nAdditionally, it is noteworthy that our algorithm's dependency on the dimension $ d $ is polynomial, not exponential, further underscoring its efficiency. And our algorithm can reduce the exponential term induced by the log-Sobolev constant\n\n> *Creating n Monte Carlo samples at each iteration can be computationally expensive, especially if n is large. This might limit the method's scalability and efficiency for high-dimensional or complex distributions.*\n\nThanks for your suggestion. In our analysis, we have already considered the complexity of $n$ samples. The main point of our algorithm is that even with $n$ samples, the total complexity is still less than Langevin-based algorithm. Note that the complexity with respect to $n$ is linear and the needed accuracy is polynomial, but the complexity of the original problem is exponential. \nIn addition, the computation of MC samples can be paralled, while exponetial iterations cannot be reduced.\n\n\n> *The method uses random samples at each iteration. The quality of these random samples is crucial; if they are not truly random or are biased in some way, it can introduce errors in the sampled results. Furthermore, the way the samples are generated and combined in the update equation (step 6) could introduce bias if not done correctly. Biased estimators can lead to incorrect conclusions about the target distribution.*\n\nThank you for you question. Indeed, the quality of these random samples is crucial, so we analysis the necessary accuracy of the inner loop to obtain the proper samples in our Theorem 1. When the error is controlled, we can guarantee the convergence of our algorithm. In real practice, the algorithm is even less sensitive to the inner loop error, as the theoretical bound considers the worst case. \n \n> *The method's performance might degrade in high-dimensional spaces. Monte Carlo methods often face challenges in high-dimensional settings due to the curse of dimensionality, where the sampling space becomes sparse, making it harder to obtain representative samples.*\n\nWe appreciate your feedback. In response, we have included high-dimensional experiments in Appendix F3. It is crucial to note that our approach maintains a polynomial dependency on dimensionality, ensuring the identification of representative samples within a finite timeframe.\n\n> *How sensitive is this framework to initial distribution $p_0$*\n\nThank you for your question. Are you referring $\\tilde{p}_0$? If so, we have added a discussion in the current Appendix F4."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8638/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700026138386,
                "cdate": 1700026138386,
                "tmdate": 1700201936818,
                "mdate": 1700201936818,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "c1uqYz2zG0",
                "forum": "kIPEyMSdFV",
                "replyto": "Ta6M9DwGlL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8638/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8638/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We are grateful once again for your invaluable review. In our revised manuscript and response, we have endeavored to thoroughly address all your concerns and questions. \n\nWe've enriched our work with new empirical results and an ablation study to demonstrate the effectiveness and robustness of our algorithm in high-dimensional scenarios. We also highlight our an in-depth complexity analysis, suggesting that our algorithm maintains higher efficiency than Langevin-based methods, considering inner, outer loops, and sample complexity.\nFurthermore, we have refined our writing to ensure clarity and prevent misunderstandings. \n\nCould you please spare some time to review our updated response and verify if it satisfactorily answers your questions? We are eager to engage in further discussion and offer additional clarifications for any new inquiries you may have."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8638/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700505404190,
                "cdate": 1700505404190,
                "tmdate": 1700505404190,
                "mdate": 1700505404190,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "05EirVb2PG",
                "forum": "kIPEyMSdFV",
                "replyto": "Ta6M9DwGlL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8638/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8638/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer qHDx,\n\nThank you for your insightful feedback once again. We hope that our response addresses your concerns and questions. \n\nAs our author-reviewer discussion nears its end, we'd appreciate knowing if your concerns are resolved. We are open for any further discussion and would appreciate a reassessment of the rating in light of the manuscript's improvements."
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8638/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700594550018,
                "cdate": 1700594550018,
                "tmdate": 1700594550018,
                "mdate": 1700594550018,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tAODtCiTGa",
                "forum": "kIPEyMSdFV",
                "replyto": "c1uqYz2zG0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8638/Reviewer_qHDx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8638/Reviewer_qHDx"
                ],
                "content": {
                    "comment": {
                        "value": "I'd like to thank the authors for incorporating the revisions specially on the complexity and conducting additional experiments. While it's acknowledged that certain concerns raised by other reviewers are fully valid and the fact there are similarities between this paper and others, I maintain that this paper remains interesting to me. The recent additions made by the authors have addressed some of the main issues I initially had and thus I'd like to keep my score."
                    }
                },
                "number": 28,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8638/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700678720895,
                "cdate": 1700678720895,
                "tmdate": 1700678720895,
                "mdate": 1700678720895,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0VTwKzEPFT",
            "forum": "kIPEyMSdFV",
            "replyto": "kIPEyMSdFV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8638/Reviewer_dWe5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8638/Reviewer_dWe5"
            ],
            "content": {
                "summary": {
                    "value": "The paper considers the problem of reverse diffusion Monte Carlo and shows that the score estimation can be viewed as a mean estimation problem by exploiting a decomposition of the transition kernel. The theoretical properties of the proposed method are extensively analysed. The performance of the proposed method is assessed on the Gaussian mixture models; it is illustrated that the proposed approach performed better than existing Langevin-type MCMC methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "-\tExtensive theoretical analysis of the proposed method.\n\n-\tGood performance on (a single) toy example."
                },
                "weaknesses": {
                    "value": "-\tIt is quite hard work to verify the theory. I would expect nothing less from such a paper, so by itself, it is, of course, not a problem. However, I believe there is room for improving the clarity and flow of the proofs. \n\n-\tWith the current presentation of the results, it is hard to verify the reproducibility of the results; in the experiments, the robustness of the algorithms to the input hyperparameters (for example, the choice of step size \\eta in Algorithms 1/2)."
                },
                "questions": {
                    "value": "It\u2019s ok that for a non-expert in the specific topic, it may be hard to go over the proofs. However, I believe that a good and precise presentation of the theoretical results can lead to even a non-expert with enough theoretical background to follow and verify the results. Some representative things which could be improved:\n-\t In D2 (proof of lemma 2 and 3): the paper refers to Proposition 2 in Ma et al. (2019). However, I cannot find the Proposition 2 in Ma et al. (2019). \n-\tLemma 9: not an obvious mismatch between the statement of the lemma and the final line in the proof (RHS of inequality is d/mu for the former, 1/mu for the latter);\n-\tThe proof of lemma 9 starts with \u201cIt is known that LSI implies Poincare inequality with the same constant,\u2026\u201d. Perhaps a reference would help."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8638/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8638/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8638/Reviewer_dWe5"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8638/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698791978727,
            "cdate": 1698791978727,
            "tmdate": 1700499499881,
            "mdate": 1700499499881,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EEDZ4WbZ04",
                "forum": "kIPEyMSdFV",
                "replyto": "0VTwKzEPFT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8638/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8638/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Thank you for your helpful suggestions. We have significantly modified the proof for better reading experience and add more explanation for clarity. We also add a proof sketch for non-experts (Appendix B). We are trying our best to make our analysis user-friendly.**\n\n**If you have any further inquiries or suggestions, please do not hesitate to reach out to us. Detailed response are as below.**\n\n\n> *Q1. Clarity and flow of the proofs.*\n\nThank you for your suggestion. We have significantly rearranged the proof and lemmas for better reading experience. Moreover, we have added a proof sketch in Appendix B. Please take a look and feel free to tell us if you have any question.\n\n> *Q2. Hyper-parameters.*\n\nWe have included the hyperparameters in our appendix and added the hyperlink in the main context.\n\n> *Q3. In D2 (proof of lemma 2 and 3): the paper refers to Proposition 2 in Ma et al. (2019). However, I cannot find the Proposition 2 in Ma et al. (2019). - Lemma 9: not an obvious mismatch between the statement of the lemma and the final line in the proof (RHS of inequality is d/mu for the former, 1/mu for the latter); - The proof of lemma 9 starts with \u201cIt is known that LSI implies Poincare inequality with the same constant,...\u201d. Perhaps a reference would help.*\n\nFor the Proposition 2 in Ma et al. (2019), Please refer to page 11 of the arXiv version (Appendix B.1), which is a formal version of Proposition 1. We also revised our content to reiterate the PNAS version proposition to make our paper self-contained.\n\nFor Lemma 9 (previous notation, current Lemma 22), sorry for the confusion. The previous version of last line is for a single dimension. We have also added the reference for LSI and PI."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8638/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700025877595,
                "cdate": 1700025877595,
                "tmdate": 1700201965231,
                "mdate": 1700201965231,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "23KB9SpYPu",
                "forum": "kIPEyMSdFV",
                "replyto": "0VTwKzEPFT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8638/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8638/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We sincerely appreciate your insightful feedback once again. In response, we have diligently addressed each of your concerns and queries. Notably, we've incorporated a proof sketch and have significantly enhanced the clarity of our proof through thoughtful reorganization. Additionally, we've refined our writing to minimize any possible misunderstandings.\nWould you be able to spare some time to review our revised response and confirm if it adequately addresses your questions? We are eager to engage in further discussions and provide additional clarifications on any new queries you may have."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8638/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700470050948,
                "cdate": 1700470050948,
                "tmdate": 1700470050948,
                "mdate": 1700470050948,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ji67kg5dqq",
                "forum": "kIPEyMSdFV",
                "replyto": "23KB9SpYPu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8638/Reviewer_dWe5"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8638/Reviewer_dWe5"
                ],
                "content": {
                    "title": {
                        "value": "Score raised"
                    },
                    "comment": {
                        "value": "I appreciate the revised version and the changes made by the authors. I believe the manuscript has significantly improved, and therefore, I raised my score."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8638/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700499724406,
                "cdate": 1700499724406,
                "tmdate": 1700499724406,
                "mdate": 1700499724406,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ZFcngKoZCH",
            "forum": "kIPEyMSdFV",
            "replyto": "kIPEyMSdFV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8638/Reviewer_UPK6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8638/Reviewer_UPK6"
            ],
            "content": {
                "summary": {
                    "value": "This paper considers a diffusion modelling approach to the classical problem of sampling from an unnormalised density."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Sampling with reverse diffusions seem to have multiple advantages (compared to usual Langevin dynamics) in terms of how the algorithms behave. This paper builds on this observation and tries to bring the diffusion modelling methodology into regular Monte Carlo sampling."
                },
                "weaknesses": {
                    "value": "Unclear writing and claims not supported rigorously. See more below in the questions part."
                },
                "questions": {
                    "value": "My questions are as follows.\n\n1) at multiple points, the paper claims that the SDE resulting from the diffusion approach has better behaviour, e.g., the last sentence of Section 2.1 claims that the isoperimetric constant of this SDE is better. Right after Lemma 1, another claim is made \"It is important to point out that the property of $q_{T-t}(\\cdot | x)$ is better than $p_*$\". Here as well, the sentence is badly written (what property?) But in any case, these claims, as far as I am able to see are not rigorously proven.\n\n2) Theorem 1 *assumes* that $q_{T-{k\\eta}}$ is log-Sobolev, instead of proving something about it. As such I think the whole motivation is unclear as authors didn't show how ill-posedness is tackled by reverse diffusion approach.  Can authors show, if $p_*$ has a log-Sobolev constant, then $q$ does actually have a better behaviour in terms of this constant?\n\nSmall comments:\n\n- In Lemma 1, point out where the proof is in Appendix"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8638/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699264315058,
            "cdate": 1699264315058,
            "tmdate": 1699637081156,
            "mdate": 1699637081156,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SthEjWIks4",
                "forum": "kIPEyMSdFV",
                "replyto": "ZFcngKoZCH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8638/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8638/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Thank you for your suggestion. We followed your points and try to make our claims more clear and rigorous. We also highlight our analysis of $q$'s  log-Sobolev are in Sec 4.1 and 4.2. We try our best to revise our manuscript to address your concerns.**\n\n**If you have any further inquiries or suggestions, please do not hesitate to reach out to us. Detailed response are as below.**\n\n> *Q1. The rigorousness of the claims.*\n\nThank you for your suggestion. \nWe have revised the claims as below:\n\n\n1. Thus, we analyze the complexity to estimate the score by drawing samples from the posterior distribution and found that our proposed algorithm is much more efficient in certain cases when the log-Sobolev constant of the target distribution is small. \n\n\n2. For any $ t > 0 $, we observe that $ -\\log q_{T-t} $ incorporates an additional quadratic term. In scenarios where $ p_* $ adheres to a log-Sobolev condition, this term enhances $ q_{T-t} $'s log-Sobolev constant, thereby accelerating convergence. Conversely, with heavy-tailed $ p_* $ (where $ f_* $'s growth is slower than a quadratic function), the extra term retains quadratic growth, yielding sub-Gaussian tails and log-Sobolev properties. Notably, as $ t $ approaches $ T $, the quadratic component becomes predominant, rendering $ q_{T-t} $ strongly log-concave and facilitating sampling.\nIn summary, every $ q_{T-t} $ exhibits a larger log-Sobolev constant than $ p_* $. As $ t $ increases, this constant grows, ultimately leading $ q_{T-t} $ towards strong convexity. Consequently, this provides a sequence of distributions with log-Sobolev constants surpassing those of $ p_* $, enabling efficient score estimation for $ \\nabla\\ln p_{T-t} $. \n\n\n> *Q2. log-Sobolev constant of $q$.*\n\nThank you for pointing out this. In fact, we are unable to whether $q_{T-k\\eta}$ is log-Sobolev with A1 and A2.\nThat is the reason why we add Section 4.1 and 4.2 to discuss the assumption of $p_*$ and log-Sobolev constant of $q_{T-k\\eta}$. We have added more explanation before 4.1 to highlight this point:\n\n *Note that the log-Sobolev constants of $q_{T-k\\eta}$ depend on the properties of $p_ \\ast$, so we should add more assumptions to estimate the log-Sobolev constants for $q_{T-k\\eta}$. Next, we will demonstrate the benefits of using $q_{T-k\\eta}$ for ill-conditioned LSI or non-LSI  $p_ \\ast $.*\n\n> *Q3. Point out where the proof is in Appendix*\n\nThank you for your suggestion. We have revised our manuscript."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8638/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700025784033,
                "cdate": 1700025784033,
                "tmdate": 1700191286207,
                "mdate": 1700191286207,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DxvpeGERDy",
                "forum": "kIPEyMSdFV",
                "replyto": "ZFcngKoZCH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8638/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8638/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you sincerely for the time and effort you've invested in reviewing our work; your feedback is invaluable. We've aimed to address your questions clearly and concisely, minimizing misunderstandings for improved readability. Should you have any further concerns or need more information, we're more than willing to assist. If our responses meet your expectations, we would greatly appreciate your acknowledgment of our progress. Thank you once again for your crucial role in enhancing our work. We eagerly await your feedback."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8638/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700505463637,
                "cdate": 1700505463637,
                "tmdate": 1700505463637,
                "mdate": 1700505463637,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SYu0cVuGaA",
                "forum": "kIPEyMSdFV",
                "replyto": "ZFcngKoZCH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8638/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8638/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer UPK6,\n\nThank you for your insightful feedback once again. We hope that our response addresses your concerns and questions. \n\nAs our author-reviewer discussion nears its end, we'd appreciate knowing if your concerns are resolved. We are open for any further discussion and would appreciate a reassessment of the rating in light of the manuscript's improvements."
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8638/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700594520544,
                "cdate": 1700594520544,
                "tmdate": 1700594520544,
                "mdate": 1700594520544,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Nd5KzI5RTq",
                "forum": "kIPEyMSdFV",
                "replyto": "SYu0cVuGaA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8638/Reviewer_UPK6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8638/Reviewer_UPK6"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your replies. I decided to keep my score as it is (it is already weak accept) -- as I'd want to see some more precise support for the claims I pointed out in my Q2. Thank you"
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8638/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700594647998,
                "cdate": 1700594647998,
                "tmdate": 1700594647998,
                "mdate": 1700594647998,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lFT739ZcEw",
                "forum": "kIPEyMSdFV",
                "replyto": "ZFcngKoZCH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8638/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8638/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your swift feedback. \n\nWe consider two cases as in Section 4.1 and 4.2 for more intuitive demonstrations.\n\n(1) log-Sobolev constant of $p_*$ is exponential to radius $R$ as in [$A_3$], which is common for mixture models. Then due to the OU process, we have that the log-Sobolev constant of any $p_T$ is improved for any $T>0$. Once, the log-Sobolev constant for $q$ is NOT exponential wrt $R$, the reverse diffusion can save the computation:\n\nAs Lemma 2 suggest \n\n *Under [$A_1$], the log-Sobolev constant for $q_t$ in the  forward OU process is $\\frac{e^{-2t}}{2 \\left(1-e^{-2t}\\right)}$ \nwhen \n$0\\leq t\\leq\\frac{1}{2} \\ln\\left(1+\\frac{1}{2L}\\right)$.\nThis estimation indicate that when quadratic term dominate the log-density of $q_t$, the log-Sobolev property is well-guaranteed.*\n\nIt means that the log-Sobolev constant within this interval is independent from $R$, and the endpoint of $p_T$ is smoother and more concentrated than $p_0$ that leads to a better log-Sobolev constant.\n\n\n\n(2) $p_*$ is not log-Sobolev due to the heavy-tail property. \nWe can assume the tail behavior of $p_*$, as [$A_4$], \n\n*For any $r>0$, we can find some $R(r)$ satisfying $f _ * (x)+r\\left\\|x\\right\\|^2$ is convex for $\\left\\|x\\right\\|\\ge R(r)$. Without loss of generality, we suppose $R(r) = c _ R/r^n$ for some $n>0,c_R>0$*\nwhere $n$ defines the growth rate of the tail.\n\nHowever, due to the additional quadratic of $q$, we can still keep $q$ is sub-Gaussian and log-Sobolev.\n\n*Under [$A_1$] and [$A_4$] the log-Sobolev constant for $q_t$ in the forward OU process is $\\frac{e^{-2t}}{6(1-e^{-2t})}\\cdot e^{-16\\cdot 3L\\cdot R^2\\left(\\frac{e^{-2t}}{6(1-e^{-2t})}\\right)}$ \nfor any \n$ t\\geq 0$.*\n\nIt means for any $t>0$, we can get a log-Sobolev constant for $q$, but the original problem is non-log-Sobolev. Reverse diffusion provides a procedure that decompose the non-log-Sobolev sampling problem into log-Sobolev sampling subproblems."
                    }
                },
                "number": 26,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8638/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700595914961,
                "cdate": 1700595914961,
                "tmdate": 1700596295681,
                "mdate": 1700596295681,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]