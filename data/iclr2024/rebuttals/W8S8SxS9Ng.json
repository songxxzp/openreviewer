[
    {
        "title": "Neuroformer: Multimodal and Multitask Generative Pretraining for Brain Data"
    },
    {
        "review": {
            "id": "jc5tZvohik",
            "forum": "W8S8SxS9Ng",
            "replyto": "W8S8SxS9Ng",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5468/Reviewer_M1PZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5468/Reviewer_M1PZ"
            ],
            "content": {
                "summary": {
                    "value": "In the paper \"Neuroformer: a multimodal, multitask GPT framework for brain data at scale\", the authors suggest a transformer-based architecture (Neuroformer) for fitting high-dimensional neural spike train recordings, that can incorporate a CLIP-like contrastive learning objective to use visual stimuli and/or behavioural recordings. The authors argue that Neuroformer can slightly outperform classic GLM models for spike prediction and strongly outperform simpler models for behavioural prediction."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is interesting because it applies modern transformer architectures to modeling neural data, and shows competitive results."
                },
                "weaknesses": {
                    "value": "Overall I thought that the paper would perhaps be more suited for a computaional neuroscience journal or for NeurIPS that traditionally has some amount of comp neuro papers. At ICLR, this topic is an outlier, as there is very little (next to none) computaitonal neuroscience there.\n\nWhereas the paper is generally well-written, I found many model details not sufficiently clear (examples below)."
                },
                "questions": {
                    "value": "MAJOR COMMENTS\n\n* Section 3: I could not understand the details of the architecture from this description. I could not even understand the basic setup... The text says that each neuron is one token, is that right? So the model is limited by O(1000) neurons, as attention layers scale quadratically with the number of tokens, right? Next, what is one training example: one time bin? For each neuron and each time bin, we have some integer number of spikes. How is this number converted into an embedding vector? What exactly are past states (how many past states) and how are they passed into the model? How does prediction (as in section 4.3.1) work: what is passed as the input instead of neural states?\n\n   Perhaps this is confusing because one would naively think about time-series modeling along the lines of GPT, where time bins (and not neurons) would be tokens. So I think the architecture setup requiers a more detailed explanation.\n   \n* Section 4.3.1: the sentence \"our model's predictions w[h]ere more closerly corelated with the ground-truth\" should contain some quantification, e.g. the fraction of neurons (or what are the dots in figure 3c: are these neurons?) for which Neuroformer outperforms GLM, and also the p-value (0.02) which is currently only mentioned in the figure caption. The evidence here is not very strong, so the authors should not oversell.\n\n* Table 1 is the strongest result in the paper, as Neuroformer strongly outerforms all other models. However, neither the comparison models (Lasso, GLM, etc) nor the prediction task are described in sufficient detail. The task here to predict behaviour from neural responses, but what exactly is \"behaviour\", what is the input (how many time steps?) and the output (how many time steps) of this prediction problem, etc.? The authors should present their experiment such that it is clear the comparison models in Table 1 are not \"strawman\" in some sense.\n\n\nMINOR COMMENTS\n\n* \\citep and \\citet should be used instead of \\cite. Current citation formatting is not following the ICLR template.\n\n* Schneider et al 2022 has been published in Nature.\n\n* page 6: what are N_a and N_z matrices? Unclear."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5468/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5468/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5468/Reviewer_M1PZ"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5468/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698678282779,
            "cdate": 1698678282779,
            "tmdate": 1700574079460,
            "mdate": 1700574079460,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CCGTzjl1eT",
                "forum": "W8S8SxS9Ng",
                "replyto": "jc5tZvohik",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5468/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5468/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for acknowledging the interesting direction of our work, and the strong results.\n\n**Architecture.** Because multiple authors had similar questions. We have added a global comment explaining the overall workflow, we suggest reading that first.\n\n**Neurons as tokens.** Yes, each Neuron is modeled as a token (**ID**). In a GPT, not all tokens (byte-pair encodings, ~words, typically around ~50,000) are included in every block. Every block (context) consists of the BPE encodings required to form that example/sentence. In our case, we populate the block with the number of Spikes in the current interval. $O(1000)$ would mean 1000 spikes in an interval of 0.05s in our case, which would require a population of tens of thousands of neurons or more (spiking activity is quite sparse). This is one of the advantages of our method. The context is not a sparse matrix of (N_population), but only consists of (N_spikes_per_interval) where typically $ N_{spikesperinterval} << N_{population} $. *Other methods utilizing sparse matrices attempt to compress the representation in order to accommodate larger neuronal populations, but in doing so lose neuronal resolution attention.* This is precisely what we wanted to avoid.\n\n**GLM vs. NF.** We have updated the manuscript to include the actual correlations: the mean correlation for GLM $r=0.232$, Neuroformer $r=0.297$, and for Real-Real $r=0.409$. The correlations were calculated between groups of 4 trials.\n\n**Table 1.** Thank you for recognizing the strong performance of our model on decoding speed. We did not include any past_behavior as input to any of the models, as the models could then simply extrapolate from previous behavior. For all models the output was the speed for the next 0.05s, over the whole of the holdout dataset. For all models bar GRU and Neuroformer, the input was the current state (spikes in the corresponding 0.05 seconds). For the GRU and Neuroformer, which can contextualize over states, we included the past state (0.15s of spikes before current state). We have updated the manuscript to include this information. We actually think figure 5b) is a cooler result, since it shows generatively pretrained models can transfer to decoding and match some of the other methods  with only a fraction of the supervised decoding data.\n\n**N_a and N_z matrices.** Both matrices have shape N x N. N_a denotes the pairwise attention between neurons, and N_z denotes how many times two neurons occur within the same bin. We used N_z to compute the average attention that was attributed between two neurons."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5468/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700561774639,
                "cdate": 1700561774639,
                "tmdate": 1700562191264,
                "mdate": 1700562191264,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KEPTCmRzHs",
                "forum": "W8S8SxS9Ng",
                "replyto": "CCGTzjl1eT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5468/Reviewer_M1PZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5468/Reviewer_M1PZ"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the response. I also looked at the other reviews by now.\n\nI still think the paper is borderline (and arguably not ideally suited for ICLR, but this is not for me to decide), but I appreciate the replies. The authors seem to know what they are doing, and I am not sufficiently familiar with this field to judge on the details. I am going to raise my score to 6 while simultaneously _lowering_ my confidence to 2."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5468/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700574065245,
                "cdate": 1700574065245,
                "tmdate": 1700574065245,
                "mdate": 1700574065245,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Iu907nYjDQ",
            "forum": "W8S8SxS9Ng",
            "replyto": "W8S8SxS9Ng",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5468/Reviewer_dTQU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5468/Reviewer_dTQU"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a multimodal, multitask generative pre trained transformer model called Neuroformer. This model uses an arbitrary number of modalities, such as neural responses,  external stimuli, and behavior, to perform downstream tasks. The authors apply this model to predict simulated neural circuit activities and behavior of a mouse from its neural recordings, where four modalities are used: neural responses, video, speed, and eye position. They also perform ablation studies to explore the impact of each model component."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* Propose a multimodal, multitask transformer-based model for neural data modeling."
                },
                "weaknesses": {
                    "value": "* In the experiments, the Neuroformer is only evaluated in terms of behavior prediction, which is not enough in evaluating neuroscience tasks. The choices of models (GLM, GRU, Lasso Regression) for comparison are also not convincing to me. One suggestion is to follow the evaluation criteria in [Neural Latents Benchmark](https://eval.ai/web/challenges/challenge-page/1256/overview) and compare the Neuroformer with the top leading models there, such as S5 and LFADS on the multimodal calcium imaging datasets. It is critical to see whether the proposed model is a solid technical innovation with practical influences in neuroscience research.\n\n* As the authors mentioned in Appendix A, the Neuroformer has poor results (Figure 10) in low-dimensional latent space learning. One possible reason is that the transformer-related neural networks are too expressive so that good dynamics in latent space are no longer necessary. But in terms of interpretability, neuroscientists prefer to observe meaningful latent space in many experimental scenarios, which may be more important than a good behavior prediction performance."
                },
                "questions": {
                    "value": "* What kind of modalities are used in section 4.3? Although we may infer these modalities in section 5, it seems there are no clear describing sentences about them in the whole section 4.3."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5468/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5468/Reviewer_dTQU",
                        "ICLR.cc/2024/Conference/Submission5468/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5468/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698697136444,
            "cdate": 1698697136444,
            "tmdate": 1700597788170,
            "mdate": 1700597788170,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Bkl5YDEzbP",
                "forum": "W8S8SxS9Ng",
                "replyto": "Iu907nYjDQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5468/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5468/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for taking the time to review our work.\n\n**Neural Latents Benchmark.** NLB is a great benchmark, but it consists of electrophysiology (ephys) data, typically derived from multi-electrode arrays, rather than neuronal-resolution optical physiology (ophys) data which is what our model is designed for  (2-photon calcium imaging data). Please also note that these datasets contain 100-200 electrode areas, while our datasets contain ~2000 Neurons. Our model can be adapted with few modifications to consider this data, and we're excited to explore such directions. Nevertheless, the benchmarks we have used our known to perform on-par with such methods. Please see Paragraph 4 of our global comment.\n\n**Representation Learning Results.** There has been an important misconception here that we want to clarify. Our comment was not intended to mean that our model does poorly at representation learning. We have included some visualizations in the appendix (J, Multimodal Latent Variable) to show the low-dimensional latents the contrastive objective of our model learns. Rather, we wanted to state that some prominent representation learning methods fared poorly on decoding speed from the large-scale calcium imaging data (see K, CEBRA Behavior Results.) \n\nOther than all the methods we tried, including the aforementioned one, the only other prominent method is RADICaL **[1]**, which is an AutoLFADS type method adapted for calcium imaging data, and there is currently no openly available code implementation for the model.\n\n**Modalities.** The modalities used are neural data, video, eye position (latitudinal and longitudinal angle), and speed. The model can additionally incorporate an arbitrary number of modalities or prediction tasks, and can be adapted to do so by just adding a few lines to a config file.\n\n**[1]** Zhu et al., A deep learning framework for inference of single-trial neural population activity from calcium imaging with sub-frame temporal resolution. (2021)  bioRxiv 2021.11.21.469441;"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5468/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700561533216,
                "cdate": 1700561533216,
                "tmdate": 1700561533216,
                "mdate": 1700561533216,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6GIfczXZso",
                "forum": "W8S8SxS9Ng",
                "replyto": "Bkl5YDEzbP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5468/Reviewer_dTQU"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5468/Reviewer_dTQU"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for the additional explanation and clarification about baseline models. While I still have some concerns regarding the potential efficacy of incorporating evaluation metrics outlined in [Neural Latents Benchmark](https://eval.ai/web/challenges/challenge-page/1256/overview), I acknowledge the authors for their efforts in introducing a multimodal, multitask GPT architecture for extremely high-dimensional neural data with complex experimental settings. This approach contributes to the neuroscience community, particularly in the current \"LLM\" era. So, I changed the score from 5 to 8."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5468/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700597766873,
                "cdate": 1700597766873,
                "tmdate": 1700597766873,
                "mdate": 1700597766873,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "y8Pfl2ZEow",
            "forum": "W8S8SxS9Ng",
            "replyto": "W8S8SxS9Ng",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5468/Reviewer_pMwz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5468/Reviewer_pMwz"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a multi-modal Transformer-based pretraining paradigm for learning joint representations of neural activity, behavior and sensory inputs. It mainly follows recent work in vision-language modeling and adapts these approaches to the neuroscience setting. The paper shows that (a) the attention maps can reveal the circuit structure in a simulated toy dataset with a few hub neurons, (b) it can predict neural activity based on past activity and stimulus slightly better than a generalized linear model, and (c) it can decode running speed of a mouse from neural activity in a few-shot manner."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ Promising self-supervised learning paradigm for large-scale, multi-modal neuroscience data\n + Nice set of experiments from simple toy model with known ground truth to real, large-scale data\n + Overall well-written and mostly easy to follow (with some exceptions)"
                },
                "weaknesses": {
                    "value": "1. Weak baselines in Figs. 2+3\n 1. Small effect in Fig. 3\n 1. Architecture (especially decoder) not entirely clear from paper"
                },
                "questions": {
                    "value": "Overall I think this is a super interesting and potentially very useful paper, albeit with some weaknesses, which I will detail below. If the authors can address these points in their response, I am happy to reconsider my score.\n\nWhile I appreciate the experiments, I believe the authors could do a better job at demonstrating that their approach actually works well. \n\n### Fig. 2\n\nIt is well known that inferring connectivity from correlations is not a good idea. The most straightforward way of getting closer to connectivity (albeit with a number of limitations and caveats as well) is using partial correlations instead of Pearson correlations. I would predict that the partial correlation matrix would correspond much more closely to the ground truth than the Pearson correlation matrix shown in Fig. 2d. Does your approach perform on par with it or even better?\n\n### Fig. 3\n\nI am somewhat underwhelmed by the result in Fig. 3: All this effort only to improve the correlation between prediction and ground truth by 2%? It may be significant, but the effect size is tiny. Papers on predicting activity from visual stimuli typically show quite more substantial improvements of neural nets over GLMs (e.g. McIntosh, NeurIPS 2016, Klindt et al., NeurIPS 2017, ...). I would like to see some stronger baselines here. The baseline model form the recent Sensorium competition (https://www.sensorium-competition.net) would be a fairly straightforward starting point.\n\nA second point on this figure: I am unsure how to interpret the attention maps in panel d). What exactly do they show? The word \"neuron\" seems overloaded here. Since the Transformer does not have a token for each neuron (or did I misunderstand something here? According to p.3 bottom the current state does not have a neuron dimension, only batch, time and embedding), I don't quite understand, what, e.g., \"Neuron 1, Layer 0, Head 0\" means. If it refers to neurons in the brain, please explain how and explain what we don't see localized receptive fields as we would expect from V1. If it refers to something else, please explain what and what we see. \n\n\n### Architecture not clear\n\nI had a hard time following the description of the method on pages 3+4. Some examples:\n\n 1. This sentence on p.3 could be unpacked: \"At each step, a learnable look-up table projects each neuron spike contained in our Current and Past States onto an embedding space E, resulting in vectors (T_c,E) and (T_p,E), where Tc,Tp are the corresponding state\u2019s sequence length (number of spikes + padding).\" <-- What does \"vectors (T_c,E)\" mean? In particular, what is the meaning of the parenthesis around T_c,E? T_c appears to be the sequence length, i.e. scalar. Does it mean you literally concatenate a scalar that indicates the length with a vector E? If so, why? If not, what is happening here?\n\n 1. It is not clear to me what the decoder outputs. From the text and figures I'm guessing it's some form of sparse representation of the spiking in the future, where ID is the row (column) and dt the column (row) of a non-zero entry in a binary matrix (size: #neurons x #timesteps) that contains the spikes. However, I am unsure about this interpretation and cannot map it onto the losses in Eqs. 3+4. Part of the problem might be that p_i and p_dt are not defined and I am not sure how to interpret the cross-entropies. Also, I don't understand why there are two losses. Why not simply output a vector of zeros and ones that is the same size as there are neurons? What is the meaning of dt? I thought you're predicting the next frame?\n\n 1. The last paragraph on p.4 is equally opaque to me and I couldn't make sense of it. The sentence with nucleus sampling is unclear and the meaning of sub-intervals is also not clear."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5468/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698700923339,
            "cdate": 1698700923339,
            "tmdate": 1699636557259,
            "mdate": 1699636557259,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YP46RohFWD",
                "forum": "W8S8SxS9Ng",
                "replyto": "y8Pfl2ZEow",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5468/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5468/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for taking the time to review and acknowledge our work. It makes us happy to see that people understand the ambitious direction we\u2019re pointing towards.\n\n**Architecture.** We have provided a global comment to all reviewers regarding the architecture. We suggest reading that first.\n\n`T_c` and `T_p` are the corresponding current and past states' block size. They are populated with the spikes in those context windows which are made up of two representations: **ID** (the spatial location, or neuron in question) and **DT** (the temporal location, or time at which the spike occurred). To learn DTs, we break each current window into smaller sub-intervals (these can be arbitrarily small, we typically choose the temporal resolution of our measurements as the DT). The IDs and DTs are learnt as tokens, in the same way as words are in a normal GPT model. These tokens have an embedding shape (E). I.e, we learn an embedding of size E for each ID and DT.\nTherefore, the output at each step of the model is not a sparse representation. Rather, it is spikes within that interval. Each individual spike is fed back into the model, and the model autoregressively generates spikes in this way, just like it would do the same using words in the normal GPT setting.\n$p_i$ and $p_dt$ are the probabilities over the corresponding ID and DT tokens for each generation. We sample from these to generate the spikes.\n\n**Nucleus sampling.** restricts the tokens we sample from according to a specified cumulative distribution. I.e, if we choose 0.95, any token that forms part for the cumulative distribution beyond 0.95, is not considered for that timestep.\n\n**Fig 2**. While in figure 2 we weren\u2019t necessarily trying to claim that attention could outperform all other methods, we agree that it would be interesting to compare attention with other methods as well. We have conducted a more thorough comparison using partial correlations, and granger causality. To compute the similarity of the ground-truth connectivity to the different methods, we computed the average sum of squared differences between the matrices (Frobenius norm). The attention matrix was more closely related to the ground truth connectivity matrix. Please see Appendix F for the visualized results.\n\n| Metric              | Value  \u2193 (lower is better)  |\n|---------------------|----------|\n| Attention           | 140.4    |\n| Granger Causality   | 138.1    |\n| Partial Correlations| 156.9    |\n\n\n**Fig 3.** The improvement in predictive capability is bounded by the variability between the actual ground-truth responses. In the quoted paper, McIntosh, NeurIPS 2016, Figure 2, the upper-bound is quoted as ~0.8, and this can depend on the way pearson correlation is calculated, i.e bin size. In the same figure, the GLM results look quite low, while the GLM we fitted approaches the ground-truth variability very well. For example, in this [1]  they found that neurons which were < 50 microns from each other and synaptically connected typically had correlation values < 0.4, and that matches other studies of correlation at longer length scales, both within and across cortical areas [2]. Trial-to-trial correlations are typically low in mouse L2/3 neurons because the neurons have low firing rates (<< 1 Hz spontaneous and ~ 5 Hz evoked; e.g., Fig. 8 in Niell & Stryker 2008 J Neurosci). In supplementary materials, \u201cCapturing Trial-To-Trial Variability,\u201d we have included a comparison between sets of 4 ground-truth and predicted trials. \n\n**Sensorium models** are not directly compatible with the data that we used to train Neuroformer, for two reasons. 1) Until 2022, models were trained on images and not video. 2) The baseline model for 2023, while compatible with video, requires precise pupil centers/directions to train the shifter network, which we do not have. Although we have tried in the last days to satisfy this request, we were unable to get well-performing models to compare with. While we still believe that the GLM we used is a strong baseline, particularly when considering the heavy inductive biases used to get good performance, we agree that more comparisons would be desirable.\n\n**Attention.** These attention maps exhibit diversity. Some of the maps seem to highlight large, moving objects, others seem to highlight other mice in the field-of-view, and still other maps seem to be specific for small features, like a mouse\u2019s tail. These are uncharted territory for neuroscience, and we're excited to explore the potential of this method in future work. To reiterate, the attentions shown in the figure are per-token, i.e for an individual spike (neuron), which is one of the advantages of the method, in that it represents neuronal populations at the spike level.\n\n[1] Ko, H., Hofer, S., Pichler, B. et al. Functional specificity of local synaptic connections in neocortical networks. (2011) Nature\n\n[2] Yu et al., Mesoscale correlation structure with single cell resolution during visual coding. (2019)"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5468/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700561411240,
                "cdate": 1700561411240,
                "tmdate": 1700566019756,
                "mdate": 1700566019756,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "VJTFmLxEnS",
            "forum": "W8S8SxS9Ng",
            "replyto": "W8S8SxS9Ng",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5468/Reviewer_HBVf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5468/Reviewer_HBVf"
            ],
            "content": {
                "summary": {
                    "value": "This manuscript introduces a multi-modal, multi-task generative pretrained transformer as a tool for analyzing the increasing volume of data generated by large-scale experiments in system neuroscience. The goal of this tool is to create a better neural spiking model while taking external variables into account. \u00a0In particular, they applied the Perceive IO architecture (Jaegle et al. 2021) to the neural domain and modified it accordingly. During the process of decoding, they also developed feature backbones, which enabled the specialized architecture to track the activity of individual neurons. Their loss function has a component that deals with alignment as well as one that deals with spike creation. The alignment component explicitly enforces\u00a0representational commonalities among biologically significant features. The causal spike modeling is used by the spike generation component to do an autoregressive decoding of brain spikes. They used a simulated dataset in addition to two different two-photon calcium imaging datasets in order to verify the accuracy of this neuroformer. They demonstrated, with the simulated dataset, that the neuroformer is capable of recovering the hub-neuron structure that is comparable to the ground truth. Using the calcium imaging dataset, they were able to demonstrate that the suggested neuroformer performed better than GLM when it came to creating neuronal spikes. They also showed that a pretrained neuroformer has more accurate predictive features of mouse behaviors than baselines like Lasso regression, GLM, MLP, and GRU. In addition to this, they presented the results of an ablation investigation, which demonstrated that each module contributes progressively to predicting eye position."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The loss function combines multi-task representation loss with two losses relevant to generating neural spikes. To the best of my knowledge, this particular application of multi-task learning to modeling neural activity is new.\u00a0\n\nThe feature backbones in the Neurofomer are able to dissect single neurons. A common limitation of previous machine learning approaches to modeling population activity is that they lose single neurons. This feature seems to circumvent such a limitation."
                },
                "weaknesses": {
                    "value": "Lack of comparison with strong baselines is my main concern for this submission. Prior to this paper, there were a couple notable publications that leveraged the transformer architecture to generate neural spikes. Albeit those most well-known ones are single modality only, it is still worth a comparison in terms of neural modeling. This work only showed its comparison with simple baselines (MLP, GRU) when it compared the quality of neural spike generation. If this neuroformer does not perform as well as other transformer-based architectures, I would hope the authors may include more elaborated discussion on whether the appeal of cross-modality representation outweighs its limited performance.\u00a0\n\n1) Liu 2022 Seeing the forest and the tree: Building representations of both individual and collective dynamics with transformers\n\n2) J. Ye and C. Pandarinath, \u201cRepresentation learning for neural population activity with Neural Data Transformers,\u201d Neurons, Behavior, Data analysis, and Theory, Aug. 2021\n\nThe F1 scores in Figure 5 are rather low at their absolute values. It would be helpful if the authors put the F1 score in perspective (why is this F1 score indicating good performance?). Is it possible for the authors to comment on the dip of the F1 score after adding the video modality in the Visnav, Lateral dataset?\u00a0\n\nThe correlation difference in Fig 3C is also low in comparison with GLM. Such a comparison will be a lot stronger if it is versus another transformer architecture or more elaborate architecture that is capable of expressing neural activity fully.\u00a0\n\nSpeed is misspelled as \u201cspped\u201d"
                },
                "questions": {
                    "value": "Does this architecture outperform any of those previous approaches in terms of generating realistic neural spikes? Is it possible to show the performance of the neuroformer on the Neural Latent Benchmark?\u00a0\n\n\u00a0\n\nWhat is the range of T_p in those calcium imaging datasets? Is it possible to demonstrate long-term inference with a neuroformer?\u00a0\n\n\u00a0\n\nIn Fig. 3d, the attention maps seem to suggest that the intermediate blocks of the neuroformer contain interesting features. It is common for the community that pretrains transformers for time series (like HuBert or Whisper for speech) to use features from intermediate blocks for decoding. Is it possible to show how well these intermediate blocks can be used to predict behavior?\u00a0\n\n\u00a0\n\nMinor question: I could guess that the red dot is the model being used to generate b) or d). It would be helpful if the authors could clarify this in the figure caption."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5468/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5468/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5468/Reviewer_HBVf"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5468/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698784861951,
            "cdate": 1698784861951,
            "tmdate": 1700684144166,
            "mdate": 1700684144166,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "X6NWmXWQid",
                "forum": "W8S8SxS9Ng",
                "replyto": "VJTFmLxEnS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5468/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5468/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the constructive feedback. We were happy to see some of our technical contributions recognized, alas, this seems not to be reflected in the final evaluation score. We hope to alleviate your concerns regarding additional benchmarks. Please also see Paragraph 4 of our global comments.\n\n**F1 scores.** In study **[1]**, neurons within 50 microns and synaptically connected showed correlations below 0.4, consistent with other research **[2]**. Low trial-to-trial correlations in mouse L2/3 neurons are due to their low firing rates, as detailed in Niell & Stryker 2008 J Neurosci. In supplementary materials G, we include a comparison of four sets of ground-truth and predicted trials, showing that our model\u2019s predictions approach the ground-truth variability in responses. Furthermore, please note that in the visnav datasets, each prediction is conditioned on never-before-seen stimulus, as the mouse is freely navigating within a virtual environment setup (please see Figure 7 for experimental setup).\n\n**[1]** Ko, H., Hofer, S., Pichler, B. et al. Functional specificity of local synaptic connections in neocortical networks. (2011) Nature\n\n**[2]** Yu et al., Mesoscale correlation structure with single cell resolution during visual coding. (2019)\n\n**Benchmarks.** Please see our comment (Paragraph 4). The benchmarks we used for comparison are common, and quite competitive even in the NLB settings, where they reach performance close to that of the most performant models on the benchmark, like AutoLFADS. In particular, we tried our best to make a performant GRU by tuning hyperparameters, using tapering, and bidirectionality (for more details see Appendix D).\n\n**Representation Learning for Behavior.** This is an interesting thought. Following your request we have extracted the representations learnt by our contrastive module and provide our results in Figure 14 (Appendix J). The neural features separate according to speed. Please note that no dimensionality reduction techniques were used, these are the raw features from our model.\n\n**Long-Term Inference.** Yes, Neuroformer is able to simulate neuronal responses over very large time horizons. The provided correlation results, and generated spike trains in figure 3b), are all generated using no ground-truth data, over 96 seconds. (w_p) the temporal window for spikes provided to (T_p), is only 0.15s (provided in Appendix C), and other than the first step in the generation of the 96 second trial, all subsequent steps are conditioned on the spikes previously generated. Appendix G shows that these simulations capture the variability of the ground-truth responses."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5468/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700561036457,
                "cdate": 1700561036457,
                "tmdate": 1700561036457,
                "mdate": 1700561036457,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oRISO8j7iR",
                "forum": "W8S8SxS9Ng",
                "replyto": "VJTFmLxEnS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5468/Reviewer_HBVf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5468/Reviewer_HBVf"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your rebuttal"
                    },
                    "comment": {
                        "value": "I appreciate the authors' responses to my questions. I read through the author's responses to all reviewers. This paper, in my opinion, is, at best a borderline case.\u00a0\n\nAs the primary demonstration of this work pertains to the generation of neural spikes, I proposed a comparison with alternative transformer-based architectures without specifically referencing the Neural Latent Benchmark. I recognize that the author may find it difficult to modify those alternative architectures to fit their data within the time constraint of the rebuttal period. I then proposed a discussion of cross-modality versus single-modality. In the absence of both comparisons, it is difficult for a system neuroscientist to migrate their modeling to cross-modality, given the potential difficulty of training (again, this is merely my conjecture; the author's response implied that they are unable to get the architecture to function on the neural latent benchmark). Its potential impediments could impede its applicability within the system neuroscience community. \n\nA well-fitted GRU should not replace the comparison of alternative transformer architectures. The GRU fitted to the author's datasets would differ from the GRU fitted to NLB. To claim that comparing to GRU is sufficient is akin to comparing pears and oranges. Critically, I asked about the comparison with alternative transformer architectures due to the potential for the attention head to contain interesting information about stimulus encoding.\u00a0\n\nI also find the speed decoding shown in Fig 14 not convincing. There is clear overlap between points of different colors (or maybe it is easier to see separation if the opacity level is less than 1?). I\u2019d also appreciate some clarification of having the dimension of neural features as 3. I could not find such architecture details in Appendix C. \n\nI echo with the Reviewer pMwz that the architecture details are unclear.\n\nI will raise the score to 5, and this is my final score for this submission. \n\nMinor\n\nI also find the updated paper a bit sloppy in its presentation. 3 subfigures in Fig 2 are missing. 9/10 subfigures in Fig 12 are missing."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5468/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700684132248,
                "cdate": 1700684132248,
                "tmdate": 1700684761765,
                "mdate": 1700684761765,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]