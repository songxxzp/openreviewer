[
    {
        "title": "Reverse Forward Curriculum Learning for Extreme Sample and Demo Efficiency"
    },
    {
        "review": {
            "id": "iQwFZTX2n2",
            "forum": "w4rODxXsmM",
            "replyto": "w4rODxXsmM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission362/Reviewer_gfCC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission362/Reviewer_gfCC"
            ],
            "content": {
                "summary": {
                    "value": "In this study, a novel methodology is proposed, combining reverse and forward curricula. The authors suggest resetting from demonstrations to effectively perform exploration when access to a limited number of demonstrations is available. The reverse curriculum starts from positions backward from the goal point using demonstrations, while the forward curriculum is executed through score-based prioritizing, allowing the starting point to have an appropriate level of difficulty. As a result, it shows improved performance compared to baseline methods that leverage demonstrations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The proposed method, combining reverse-forward curriculum approach is novel.\n\n- The learning curves presented in figures 3 and 4 demonstrate improved performance compared to the baselines, and the figure in 5 suggests that using both the reverse and forward curriculum with a limited number of demonstrations is beneficial."
                },
                "weaknesses": {
                    "value": "- The assumption of resetting from demonstrations is a strong one and only applicable in simulation environments. While this study proposes an efficient way to leverage one or more demonstrations through the reverse curriculum, I still consider it to be a strong assumption.\n\n- The study may appear to be not significantly different from consecutively performing Jump-Start RL and PLR.\n\n- While it compares to various baselines, many of them seem to be algorithms that do not assume state resets or use curricula."
                },
                "questions": {
                    "value": "- In Figure 6, it seems that 'reverse' and 'forward' are only applicable to 'reverse-forward,' and not to 'none' and 'forward only,' which might be confusing.\n- Which algorithms among the compared baselines require the State Reset assumption?\n- If possible, could you please explain the reason for the initial high performance of RLPD in the stick pull task in Figure 4, followed by a drop?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission362/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission362/Reviewer_gfCC",
                        "ICLR.cc/2024/Conference/Submission362/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission362/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698839603070,
            "cdate": 1698839603070,
            "tmdate": 1700561716962,
            "mdate": 1700561716962,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Yilg2xZW8X",
                "forum": "w4rODxXsmM",
                "replyto": "iQwFZTX2n2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission362/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission362/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer gfCC [1/2]"
                    },
                    "comment": {
                        "value": "Thank you for the review and raising the concerns, we are happy to see you believe our approach is novel. We address concerns below:\n\n> The assumption of resetting from demonstrations is a strong one and only applicable in simulation environments. While this study proposes an efficient way to leverage one or more demonstrations through the reverse curriculum, I still consider it to be a strong assumption.\n\nWe agree that this is a strong assumption, as is the case for many other papers in our research area focusing on robot learning ([1], [2]). We argue however that work on sim2real is orthogonal to our research and thus investigating the most efficient algorithms in simulation is of great value. Papers like [1] have shown using sim2real can still achieve some fairly dexterous capabilities on real robots despite training only in simulation. Additionally, another use case that arises from using simulation is having an algorithm that trains much faster and uses less demos in simulation can help generate more diverse simulation data to be used with limited real world data to help scale up training data for large-scale robotics models.\n\n> The study may appear to be not significantly different from consecutively performing Jump-Start RL and PLR.\n\nIn general our reverse curriculum is fairly different to Jump-Start RL (JSRL). JSRL suffers from exploration problems in sparse reward settings. We alleviate some significant exploration problems by efficiently leveraging state resets. Moreover, we construct a unique per-demo curriculum and introduce a few techniques like dynamic time limits that JSRL does not have which enable faster training as shown in ablations.\n\n> While it compares to various baselines, many of them seem to be algorithms that do not assume state resets or use curricula\u2026 Which algorithms among the compared baselines require the State Reset assumption?\n\nJSRL is the only baseline that uses some form of reverse curriculum with comparable results (they test on proper benchmarks that have code). We do cite several past methods that use reverse curricula and state reset but they do not have code and often test on simple tasks (e.g. maze navigation) outside of robotics that also have no code. As an alternative, we perform an ablation testing the use of a uniform state reset that methods like [2] use in section 5.2 (Table 1) instead of the reverse curriculum RFCL uses. Across ManiSkill2 tasks, uniform state reset is incapable of reaching high success rates within 1M environment interactions when evaluating on initial states $s\\_{i, 0}$ in the 5 demonstrations given. Finally, methods like [3] use curriculums but assume access to human-engineered curriculums which is a stronger assumption so they are hard to compare fairly.\n\nWe have only found one open-sourced baseline that claims to use state reset which is the one by Nair et. al. [2]; however we could not get their code working correctly as upon further inspection, their code does not use state resets. We will run our RFCL method on the environments Nair et. al. [2] test on and compare against their written results obtained via state resets and report here. [1] also uses state reset and has some code but is not optimizing to be sample efficient as it uses PPO so it would be unfair to compare against (in addition to relying on a number of human designed heuristics).\n\nFor now, to compare against [2], you can look at the PickCube results for RFCL in Fig. 5 which is a very similar environment to the Pick and Place task [2] uses. The main difference is our task has a larger action space as we can control the pose of the end-effector (necessary to help grasp rotated cubes) whereas in [2] you are fixed to controlling just the position of the end-effector to grasp cubes with fixed rotation. RFCL with just 5 demos is capable of achieving a high success rate after 250k samples whereas [2] needs about 500k+ samples despite using 100 demonstrations and solving a easier task.\n\n[1] Chen et. al, \u201cSequential Dexterity\u201d CoRL, 2023\n\n[2] Nair et. al, \u201cOvercoming exploration in reinforcement learning with demonstrations\u201d ICRA, 2018\n\n[3] Li et. al, \u201cTowards practical multi-object manipulation using relational reinforcement learning\u201d ICRA, 2020"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699767510811,
                "cdate": 1699767510811,
                "tmdate": 1700182275137,
                "mdate": 1700182275137,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "g9P5LqeKLU",
                "forum": "w4rODxXsmM",
                "replyto": "iQwFZTX2n2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission362/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission362/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer gfCC [2/2]"
                    },
                    "comment": {
                        "value": "> In Figure 6, it seems that 'reverse' and 'forward' are only applicable to 'reverse-forward,' and not to 'none' and 'forward only,' which might be confusing.\n\nThis is actually the correct interpretation. We are simply marking when the reverse and forward curriculum are active. We have added an extra bracket under the second row indicating that the forward curriculum is constantly running for that ablation.\n\n> If possible, could you please explain the reason for the initial high performance of RLPD in the stick pull task in Figure 4, followed by a drop?\n\nUnfortunately it is difficult to say why this has occurred, although unlikely, maybe some data corrupted when uploading training results. We are currently rerunning RLPD with the same 5 seeds plus 5 more seeds on the stick pull task and will re-record results, it might also just be some strange noise. Overall however the conclusions remain the same about RFCL's performance relative to RLPD.\n\nWe hope our response (plus forthcoming new results) addresses all your concerns. In light of these clarifications, would you be able to raise your score? We would be happy to address any more issues you have."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699767530526,
                "cdate": 1699767530526,
                "tmdate": 1699767635290,
                "mdate": 1699767635290,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vqHBVJjcYr",
                "forum": "w4rODxXsmM",
                "replyto": "g9P5LqeKLU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission362/Reviewer_gfCC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission362/Reviewer_gfCC"
                ],
                "content": {
                    "comment": {
                        "value": "Regarding \"JSRL suffers from exploration problems in sparse reward settings. We alleviate some significant exploration problems by efficiently leveraging state resets.\" could you provide specific details on which differences lead to variations in exploration performance in sparse reward settings?\n\nIn other words, I understand that JSRL leverages the implicit distribution naturally occurring through deploying a guiding policy without explicitly controlling initial state. With the proposed method introducing the assumption of state reset, can it provide additional insights beyond the performance gains that arise from this assumption (state reset)?\n\nAll other questions have been addressed."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700444923184,
                "cdate": 1700444923184,
                "tmdate": 1700444923184,
                "mdate": 1700444923184,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sqfPL5dSwi",
                "forum": "w4rODxXsmM",
                "replyto": "iHxwmivj2n",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission362/Reviewer_gfCC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission362/Reviewer_gfCC"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the response that addresses my questions. I would like to raise the score."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700561697792,
                "cdate": 1700561697792,
                "tmdate": 1700561697792,
                "mdate": 1700561697792,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PLz4PMSYGu",
            "forum": "w4rODxXsmM",
            "replyto": "w4rODxXsmM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission362/Reviewer_Rzng"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission362/Reviewer_Rzng"
            ],
            "content": {
                "summary": {
                    "value": "In this paper the authors show the benefits of first using a reverse curriculum on demonstrations (of simulated RL tasks with sparse rewards) followed by a forward curriculum that expands the set of initial states (from which the agent can achieve the task). In many different simulated RL tasks, it is shown that the method can achieve quite significant boosts in sample efficiency, and can sometimes succeed in tasks which other methods completely fail. Ablation studies show that the method is robust to the number of demonstrations and that the reverse-forward curriculum seems to be the best among various other choices."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The paper convincingly shows that their method and their particular choice of curriculum leads to significantly better performance in many different RL tasks with sparse reward structure. \n\n* Ablation studies are well done and cover significantly the possible variations.\n\n* Figures captions and plots are well done, as well as the visualizations in the website."
                },
                "weaknesses": {
                    "value": "* The paper could benefit from an algorithmic summary. Algorithmic decisions not summarized succinctly by equations or in algorithmic form, but by verbose descriptions make the method look more 'alchemical' than it need be. e.q. \n\n\"Define q to be the fraction\nof episodes out of the last k episodes that receive nonzero return starting from a sampled initial state\nsi,init. If q is 0, then assign a score of 2 to si,init. If 0 < q < \u03c9 for a threshold 0 < \u03c9 < 1, assign\na score of 3. If q \u2265 \u03c9, assign a score of 1.\" \n\nThe actual numbers chosen detract from the concept of rejecting samples based on e.g. the expected return of exploration."
                },
                "questions": {
                    "value": "* Please do not use the word 'extreme', you used it several places throughout the paper.\n\n* \"In practice, the observations used by the policy \u03c0\u03b8 may be different from the actual environment state but for simplicity in this\npaper state also refers to observation.\"\n> So you don't consider noisy feedback or POMDPs? This should be mentioned clearly in the introduction. As the method shows significant improvement in learning curves, it is vital to indicate when/where we expect them to hold and where they would fail.\n\n* \"In both stages, we use the off-policy algorithm Soft Actor Critic (Haarnoja et al., 2018) with a Q-ensemble (Chen et al., 2021b), \"\n> What happens if you use another RL algorithm? Does it make a big difference? Which other methods, competitive to SAC, could you use?\n\n* Minor comment: \" As a result, a curriculum for each demonstration is necessary as opposed\nto a curriculum constructed from all demonstrations as done in prior work in order to ensure noisy\ninformation arising from the multi-modality of demonstrations do not impact the reverse curriculum\nof each demonstration as much.\" -> Too long sentence.\n\n* You use \\phi before you introduce it, and I didn't get what it's supposed to mean?\n\n* \"In this manner, initial states that sometimes receive\nreturn are prioritized the most, then initial states that receive no return, then initial states that are\nconsistently receiving return.\" -> Not a very clear sentence, do you want to use 'than' instead?\n\n* Would be nice to discuss why the methods compared against were chosen out of all the possible RL algorithms out there (maybe in an appendix?) Would the others be unsuitable for the task (e.g. on-policy, not suitable for demonstrations etc.)\n\n==== POST-REBUTTAL ====\n* My score remains the same, I think this paper should be accepted, as it has good results and detailed ablations. \n* Assumptions/limitations of the method (e.g. full observability, restricted to simulations, requires demonstrations) are mentioned throughout the paper and is not a deal-breaker. As mentioned in the rebuttal, just improving simulation efficiency is also a contribution."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission362/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission362/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission362/Reviewer_Rzng"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission362/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698843011796,
            "cdate": 1698843011796,
            "tmdate": 1699860411229,
            "mdate": 1699860411229,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YhiHaG7SSg",
                "forum": "w4rODxXsmM",
                "replyto": "PLz4PMSYGu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission362/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission362/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Rzng"
                    },
                    "comment": {
                        "value": "Thank you for the high rating, we are happy to see that you found the results and figures convincing. We address your concerns below:\n\n> The paper could benefit from an algorithmic summary. Algorithmic decisions not summarized succinctly by equations or in algorithmic form, but by verbose descriptions make the method look more 'alchemical' than it need be.\n\nThank you for bringing this up, we will look to revise this to be better as we work through the rebuttals.\n\n> Please do not use the word 'extreme', you used it several places throughout the paper.\n\nRevised!\n\n> \"In practice, the observations used by the policy \u03c0\u03b8 may be different from the actual environment state but for simplicity in this paper state also refers to observation.\"\nSo you don't consider noisy feedback or POMDPs? This should be mentioned clearly in the introduction. As the method shows significant improvement in learning curves, it is vital to indicate when/where we expect them to hold and where they would fail.\n\nIn our revision we have clarified in the introduction that we only benchmark on fully observable environments (as also done by benchmarked baselines).\n\n> \"In both stages, we use the off-policy algorithm Soft Actor Critic (Haarnoja et al., 2018) with a Q-ensemble (Chen et al., 2021b), \"\nWhat happens if you use another RL algorithm? Does it make a big difference? Which other methods, competitive to SAC, could you use?\n\nThis is a great question. This could be an interesting line of future work to investigate if RFCL is actually RL algorithm agnostic. Our initial motivation to use an off-policy algorithm as they are known to be more sample-efficient than on-policy algorithms in robotics benchmarks and we compare against baselines over sample-efficiency. We use SAC (with Q-ensemble) in particular as empirically it has the best performance in the robotics benchmarks we test on. Algorithms like PPO can also be used, especially if the interest is in training wall-time and not sample-efficiency. However one may need to rethink how offline demonstration data (including suboptimal demonstrations) can still be used as it appears uncommon in literature to do the type of offline buffer sampling our method (and RLPD, MoDem) do in on-policy algorithms.\n\n> Minor comment: \" As a result, a curriculum for each demonstration is necessary as opposed to a curriculum constructed from all demonstrations as done in prior work in order to ensure noisy information arising from the multi-modality of demonstrations do not impact the reverse curriculum of each demonstration as much.\" -> Too long sentence.\n\nRevised!\n\n> You use \\phi before you introduce it, and I didn't get what it's supposed to mean?\n\nWe have revised the paper to clarify that $\\phi$ is a hyperparameter indicating the ratio of the demonstration length to the episode horizon. A larger value would then mean shorter episodes relative to how long the demonstrator took, and vice versa.\n\n> \"In this manner, initial states that sometimes receive return are prioritized the most, then initial states that receive no return, then initial states that are consistently receiving return.\" -> Not a very clear sentence, do you want to use 'than' instead?\n\nThanks for pointing this out, we have made a revision to make it more clear.\n\n> Would be nice to discuss why the methods compared against were chosen out of all the possible RL algorithms out there (maybe in an appendix?) Would the others be unsuitable for the task (e.g. on-policy, not suitable for demonstrations etc.)\n\nThis is a good point, we add to appendix B.5 how baselines were chosen and why some were not chosen. Generally we chose off-policy learning-from-demonstrations baselines as they are the most sample-efficient on our benchmarks and could not include some baselines because they do not have any code or tests on open-sourced tasks. We are actively trying to compare against at least one more baseline that uses state resets as we describe in response to reviewer gfCC."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699839824615,
                "cdate": 1699839824615,
                "tmdate": 1699839824615,
                "mdate": 1699839824615,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "GZbE45kW8v",
            "forum": "w4rODxXsmM",
            "replyto": "w4rODxXsmM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission362/Reviewer_UCiq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission362/Reviewer_UCiq"
            ],
            "content": {
                "summary": {
                    "value": "This paper deals with curriculum learning in cases where there are only a small number of demonstrations available. The proposed method specifically design the curriculum generation as two stages: one along the demonstration paths, another explore around demonstration and eventually cover the entire space. The experiments are performed on several learning from demonstration (LfD) benchmarks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The presentation of this paper is good. \n- The experiment results are strong compared to multiple related baselines."
                },
                "weaknesses": {
                    "value": "There are several issues/concerns on the method and experiments:\n\nMethod:\n\n- The most important issue is the design of curriculum. Basically, the author tried to separate the curriculum of reset states into two stages: in the first one the reset states are along the demonstrated states, and in the second one the reset states gradually move away from the demonstrated states. This does not make sense. Why not just combine the two stages into one, i.e. a curriculum that includes explorations of reset/initial states along the demonstration and also away from demonstrated states? The reset states need to cover the entire space in the end anyway. I don't see any reason for a two-stage design to make sense. Conceptually, reverse curriculum and \"forward curriculum\" are almost the same thing, with the latter has a difference of weighting on the exploration area.\n\n- What if there is no demonstration? Or intentionally forget about demonstration but just design curriculum that directly moves the reset/initial states away from goal? If this paper's assumption is that in the end, the initial/reset states should be able to cover the entire space anyway, then whether there is demonstrations provided should not matter: all starting states has to be explored sooner or later. I didn't see an ablation experiment to compare against the setting where no demonstration is available.\n\nExperiment:\n\n- As mentioned, it would be great to show the results under the setting where no demonstration is available, or the \"forward curriculum\"-only case. I understand the Maze task in Figure 6 tries to show this. But the experiment in Figure 6 is wrong: it does not show forward-curriculum-only is worse than reverse+forward curriculum. The reason it is worse is because forward-curriculum-only experiment mistakenly messed up with exploration (it should only explore in unexplored area, not around already explored demonstration area on blue lines). The explanation in the last two sentences of Section 5.2 is wrong too imo.\n\n- In Figure 5, do all experiments have the same reset/initial state distribution (cover all possible states) at the end of their curricula? If so, it's so hard to understand why the more demonstration there is, the faster it can be trained.\n\n- It seems that RFCL without \"forward curriculum\" works almost the same as RFCL with \"forward curriculum\" in most tasks (Fig 5). This again questions the necessity of having a second stage, as mentioned earlier. Without the two-stage setting, the novelty of this work is negatively impacted.\n\n- I'm not sure if the tasks selected in this paper are suitable for the proposed method:\n   - If the robot arms are moved by a positional controller, then the initial/reset position/states of the robot arm does not matter, because the positional controller should guide the robot arm/end effector converge to the goal position anyway regardless of initial states. Not much exploration is needed. So if this is the case, then the selected task cannot fully evaluate the potential of the proposed method.\n   - If the robot arms are not moved by a positional controller, then why not do so?"
                },
                "questions": {
                    "value": "My questions are written in previous \"Weakness\" section. The authors can respond to the concerns/question written there."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission362/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698986696636,
            "cdate": 1698986696636,
            "tmdate": 1699635963030,
            "mdate": 1699635963030,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FTGFQSEjAW",
                "forum": "w4rODxXsmM",
                "replyto": "GZbE45kW8v",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission362/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission362/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer UCiq [1/3]"
                    },
                    "comment": {
                        "value": "Thank you for a comprehensive and detailed review, we welcome the feedback and suggestions and address the concerns below:\n\n> What if there is no demonstration? Or intentionally forget about demonstration but just design curriculum that directly moves the reset/initial states away from goal? If this paper's assumption is that in the end, the initial/reset states should be able to cover the entire space anyway, then whether there is demonstrations provided should not matter: all starting states has to be explored sooner or later. I didn't see an ablation experiment to compare against the setting where no demonstration is available.\n\nThank you for raising this comment. We believe there may be a misunderstanding of the initial state distribution and what is inside it. In our paper, the initial state distribution $\\mathcal{S}\\_{\\text{init}}$ refers to the distribution of states the environment resets to (e.g. reset robot to a rest position, place objects in random locations not near the goal locations). Critically, the **demonstrations provide states along a successful trajectory which are states outside of the initial state distribution**. Without a demonstration to reset states to, we have a very difficult exploration problem as the benchmarked tasks have high dimensional observation and action spaces, making it nearly impossible under sparse reward to explore to states near the goal and get reward. \n\nIt is not possible to sample a state near the goal without using a demonstration, extra human-engineered code, or infeasible amounts of exploration. Designing a curriculum by hand would be time-consuming especially for these high dimensional tasks and human demonstrations (e.g. the demos in the Adroit benchmark) are easier to collect and need less expertise. That being said, we think it is interesting future work to explore how one can use simple human written curriculums (e.g. via language description) to accelerate policy learning.\n\nRunning any RL algorithm without using demonstrations should achieve 0 success rate under the same compute budgets for the majority of tasks benchmarked due to the exploration problem under sparse reward. We will run no-demonstration experiments on some tasks with and without the forward curriculum and post results here.\n\n> The most important issue is the design of curriculum. Basically, the author tried to separate the curriculum of reset states into two stages: in the first one the reset states are along the demonstrated states, and in the second one the reset states gradually move away from the demonstrated states. This does not make sense. Why not just combine the two stages into one, i.e. a curriculum that includes explorations of reset/initial states along the demonstration and also away from demonstrated states? The reset states need to cover the entire space in the end anyway. I don't see any reason for a two-stage design to make sense. Conceptually, reverse curriculum and \"forward curriculum\" are almost the same thing, with the latter has a difference of weighting on the exploration area.\n\nIndeed, both curriculums are similar in that they control which states we reset to during training. We separate these into two curriculums as we want the policy to first prioritize learning to solve the task from the states in the demonstrations and then leverage the forward curriculum to prioritize other initial states sampled from the environment\u2019s initial state distribution $\\mathcal{S}_{\\text{init}}$. By prioritizing demonstration states in a reverse curriculum first, the policy is more likely to collect useful data and reward first and learn to solve the task from a narrow subset of environment initial states. From there then the forward curriculum makes sense as the policy now actually can solve the task when starting from some initial states, it is up to the forward curriculum to help prioritize which initial states to train from.\n\nIt is possible to merge the two into one curriculum where early during training we sample primarily along demonstration states and then over time sample more from the initial state distribution. For the purpose of explanation of the method it made more sense to give two separate names and in terms of practical implementation it works out better in code to have the two stage setup as demonstration states and states in $\\mathcal{S}_{\\text{init}}$ are mostly not similar (especially demo states near the goal). Furthermore, it\u2019s unclear what the right distribution over demonstration states and initial states should be, while there are many options to try, we still achieved the strongest results via the two stage setup. We do believe there can be interesting future work to explore a more optimized, combined curriculum."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699775144763,
                "cdate": 1699775144763,
                "tmdate": 1699776132103,
                "mdate": 1699776132103,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "T5mrzPW0r3",
                "forum": "w4rODxXsmM",
                "replyto": "GZbE45kW8v",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission362/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission362/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer UCiq [2/3]"
                    },
                    "comment": {
                        "value": "> As mentioned, it would be great to show the results under the setting where no demonstration is available, or the \"forward curriculum\"-only case. I understand the Maze task in Figure 6 tries to show this. But the experiment in Figure 6 is wrong\u2026\n\nWe agree the maze task could be more clear and clarify some details here in addition to revising the submission. The maze task is modified to mimic the robot tasks we benchmark by making the initial state distribution $\\mathcal{S}\\_{\\text{init}}$ consist of any state not along the demonstration, and thus the chance of getting the sparse reward is difficult if you reset to initial states sampled from $\\mathcal{S}\\_{\\text{init}}$. Moreover it is a **fully continuous state and action space, meaning the policy would still need to explore near states along the demonstration to perform well**. As a result, in the forward curriculum-only setting the policy can only reset to states not along the demonstration. It performs better than the no curriculum setting as the forward curriculum can prioritize sampling initial states close to the demonstration and is more likely to gather the necessary exploration data near the states along the demonstration. However, the forward curriculum-only setting performs less sample efficiently compared to the full RFCL method as the reverse curriculum enables the policy to collect necessary data around states along the demonstration and thus you see the fast improvement in success rate along the demonstration first in RFCL before the success rate improving on all states in $\\mathcal{S}\\_{\\text{init}}$.\n\n> In Figure 5, do all experiments have the same reset/initial state distribution (cover all possible states) at the end of their curricula? If so, it's so hard to understand why the more demonstration there is, the faster it can be trained.\n\nAfter the reverse curriculum the policy can achieve high success rates if we initialize from the start of those demonstrations $\\{s\\_{i, 0}\\}$, the first state of each demonstration $i$. Note that all $\\{s\\_{i, 0}\\}$ are usually from $\\mathcal{S}\\_{\\text{init}}$. With more demonstrations, the policy after reverse curriculum is more generalized across different initial states, meaning it can train faster as it does not need to spend as much time during the forward curriculum learning to generalize. The improved generalization can be seen in Fig. 5, the success rate curves are already trending up before the reverse curriculum completes (marked by the vertical dashed gray line) in higher-demonstration settings.\n\nFurthermore, more demonstrations means more ground-truth data of hard to reach states (e.g. peg inside the hole for the peg insertion task) with observation, action, and sparse reward labels to train with, which in general for any learning from demonstrations algorithm (e.g. DAPG, RLPD) will improve the performance. \n\n> It seems that RFCL without \"forward curriculum\" works almost the same as RFCL with \"forward curriculum\" in most tasks (Fig 5). This again questions the necessity of having a second stage, as mentioned earlier. Without the two-stage setting, the novelty of this work is negatively impacted.\n\nIn particular, we emphasize that with a forward curriculum in RFCL we can achieve better demonstration efficiency. This is seen clearly by the blue line (RFCL) outperforming the green line (RFCL w/o forward curriculum) for settings with less demonstrations. For PegInsertionSide, with 10 demos or less the forward curriculum has much better success rates. For PickCube, the same occurs on the 1 demo ablation. When there are enough demonstrations, the reverse curriculum trained policy has much better generalization and thus the forward curriculum provides less noticeable (if any) benefits. The improved generalization of the reverse curriculum training is evidenced by the success rate curves already increasing before the dashed gray line in Fig. 5 (when the reverse curriculum completes).\n\nTo further emphasize the importance of combining reverse and forward curricula, we will add some additional demo ablations comparing with and without forward curriculum results on some hard tasks (like Plug Charger) and post results here (and add to appendix due to space limitations)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699775192930,
                "cdate": 1699775192930,
                "tmdate": 1699905250137,
                "mdate": 1699905250137,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "izFLaLxhNk",
                "forum": "w4rODxXsmM",
                "replyto": "gMLJLODZHh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission362/Reviewer_UCiq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission362/Reviewer_UCiq"
                ],
                "content": {
                    "title": {
                        "value": "Reply"
                    },
                    "comment": {
                        "value": "Thanks to the authors for the clarification and more information provided.\n\nMaybe I didn't make myself clear enough in my original review. But my main concern still remains unsolved while some more concerns came up.\n\n> It is not possible to sample a state near the goal without using a demonstration, extra human-engineered code, or infeasible amounts of exploration\n\nI disagree with this argument from the authors. If you want to define a valid task, you would need to define a set/distribution of goal states. If you cannot even sample a single valid goal state, how can you even define the set/distribution of goal states? I believe in practice, sampling a valid goal state would be much easier than defining the whole set/distribution of goal states. Or you should at least be able to sample *near* the goal states, as suggested by [A]. This argument (or assumption) by the authors is wrong imo.\n\n> We separate these into two curriculums as we want the policy to first prioritize learning to solve the task from the states in the demonstrations and then leverage the forward curriculum to prioritize other initial states sampled from the environment\u2019s initial state distribution. By prioritizing demonstration states in a reverse curriculum first, ..., it is up to the forward curriculum to help prioritize which initial states to train from.\n\nFor me, this reply from the authors raised even more concerns. I don't see why prioritizing demonstration states first and then other initial states is better than going in reverse direction from goal states to $S_\\text{init}$ directly without considering the demonstration states.\n\nAnother way to see this is to have an ablation experiment where you use **only the last states** in the demonstration trajectories, i.e. $\\\\{ s_{i,T_i} \\\\}$ in the authors' notation system, as the starting reset states of the generated reverse curriculum (if you feel you have no other ways of sampling goal states or near goal states). And then you move the reset states backward from $s_{i, T_i}$ to somewhere in $S_\\text{init}$ without considering demonstration states $s_{i, 1}, s_{i, 2}, \\ldots, s_{i,T_i-1}$. At the same time, you explore all other reset states including $S_\\text{init}$ and the ones that reversely lead to $S_\\text{init}$.\n\nI would imagine that this design would make more sense than moving backward along demonstrations (stage 1). From my experience, demonstrations are usually extremely noisy and dirty. It makes no sense to rely on the noisy data when you have other much better options.\n\nSorry if I didn't make myself clear enough in my original review or I was asking too many additional experiments from the authors. I didn't imagine that the authors would say that \"*It is not possible to sample a state near the goal...*\", so I thought \"*show the results under the setting where no demonstration is available*\" is sufficient to describe this experiment. \n\n[A] Florensa, Carlos, et al. \"Reverse curriculum generation for reinforcement learning.\" Conference on robot learning. PMLR, 2017."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700290022516,
                "cdate": 1700290022516,
                "tmdate": 1700290022516,
                "mdate": 1700290022516,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NjOJ6fnMry",
                "forum": "w4rODxXsmM",
                "replyto": "GZbE45kW8v",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission362/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission362/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response by Authors"
                    },
                    "comment": {
                        "value": "Thanks for the early reply and discussion!\n\n> I disagree with this argument from the authors. If you want to define a valid task, you would need to define a set/distribution of goal states. If you cannot even sample a single valid goal state, how can you even define the set/distribution of goal states? I believe in practice, sampling a valid goal state would be much easier than defining the whole set/distribution of goal states. Or you should at least be able to sample near the goal states, as suggested by [A]. This argument (or assumption) by the authors is wrong imo.\n\nTo be clear, we refer to state as everything in the env including the robot state, object states etc. In practice, in all benchmarks we test on which are commonly used in the robot learning community, there are no environments that define a distribution of *useful* goal states. They always write simpler evaluation functions instead.\n\nOne could spend human effort to reverse engineer the evaluation code to then write (near) goal state samplers, however for tasks like PickCube, you could easily sample states where the cube is at/near the goal, but the robot is not holding the cube. One could spend effort to write code to ensure that sampled goal states include the robot holding the cube, but this is exactly the goal of learning from demonstrations research: avoiding extra engineering by using demos (e.g. in the Adroit tasks we use human demos provided by the benchmark and perform well). This is definitely infeasible for tasks like the Adroit suite where there is a very high dimensional dextrous robot hand. In AdroitRelocate where the goal is to move the ball to the goal position, it is only feasible to sample goal states where the objects are at the goal but completely infeasible to sample valid robot hand states that would be holding the object at the goal due to the high dimensionality of the robot hand state (it has 30 degrees of freedom).\n\nWe hope that it is clear that while it is possible to sample goal states, it's not easy to sample the useful goal states that include valid robot state that maintains the success. In AdroitRelocate, you can sample states where the ball is at the goal already and would get some success, but it becomes a difficult problem as the ball would instantly roll/fall away from the goal and now the algorithm has to learn from scratch how to grasp the ball and move it back to the goal making the task as difficult as it was before without the state reset. Hence, this is why demonstrations are critical, they describe a solution trajectory that is otherwise extremely difficult to sample with human engineered code as especially is the case in robotics tasks.\n\n> For me, this reply from the authors raised even more concerns. I don't see why prioritizing demonstration states first and then other initial states is better than going in reverse direction from goal states to $\\mathcal{S}_{\\text{init}}$ directly without considering the demonstration states...\n\nUnfortunately, it is a strong assumption made by [A] that you can move reset states backward from $s_{i,T_i}$ to somewhere in $S_{init}$ (in the case of [A] they assume you can sort of randomly sample actions to move from a goal state towards $S_\\{\\text{init}\\}$). A simple counterexample to the claim that you can move reset states backward is the Adroit Door task. Many of the last states of demonstration trajectories in the Adroit suite end with the door open which yields success, but also end with the robot hand no longer grasping the door handle. This occurs on other tasks as well. Then to find states moving back to a state where the door is closed again is very difficult as you cannot easily sample states where a high dimensional robot hand is grasping the door handle correctly (this is the same problem in the forward direction). The reason why [A] was able to move reset states from the goal to the initial state distribution is because in their 2 robot tasks the object is always fixed to the robot (e.g. no grasping needs to be learned) which is not a realistic assumption of actual robotics. If the object wasn't fixed to the robot, many sampled reverse actions would lead to states where the robot isn't grasping the object and be hard to learn from.\n\nWe are more than happy to run experiments for more convincing empirical evidence if necessary. We do note that the idea of trying to move a reset state backward to somewhere in $S_\\{\\text{init}\\}$ by the method in [A] (taking reverse actions) is not possible in code for many tasks like Adroit, you can take a look at the Adroit code e.g. https://github.com/Farama-Foundation/Gymnasium-Robotics/blob/main/gymnasium_robotics/envs/adroit_hand/adroit_relocate.py and see that success is defined as the ball being at the goal, with no code about the hand grasping the ball (as this would be difficult).\n\n[A] Florensa, Carlos, et al. \"Reverse curriculum generation for reinforcement learning.\" Conference on robot learning. PMLR, 2017."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700293508775,
                "cdate": 1700293508775,
                "tmdate": 1700308491808,
                "mdate": 1700308491808,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NpYnI3IILu",
                "forum": "w4rODxXsmM",
                "replyto": "NjOJ6fnMry",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission362/Reviewer_UCiq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission362/Reviewer_UCiq"
                ],
                "content": {
                    "title": {
                        "value": "Reply"
                    },
                    "comment": {
                        "value": "Thanks to the authors for more comments.\n\n> We hope that it is clear that while it is possible to sample goal states, it's not easy to sample the useful goal states that include valid robot state \u2026\n\nI am not saying that using demonstrations is not good. I am just saying that your argument \u201c*It is not possible to sample a state near the goal without using a demonstration, extra human-engineered code, or infeasible amounts of exploration*\u201d is wrong or too strong.\n\nJust like what I mentioned in my previous post: *if you feel you have no other ways of sampling goal states or near goal states, you can use the last states in the demonstration trajectories*.\n\n> Unfortunately, it is a strong assumption made by [A] that you can move reset states backward from $s_{i, T_i}$ to somewhere in $S_{init}$ \u2026 \n\nI don\u2019t think this assumption is too strong. \n\nIn fact, in your Forward Curriculum, you are making the exact same assumption, i.e. it is possible to move the reset states away from some demonstration states and eventually cover the entire $S_{init}$.\n\n> A simple counterexample to the claim that you can move reset states backward is the Adroit Door task. Many of the last states of demonstration trajectories in the Adroit suite end with the door open which yields success, but also end with the robot hand no longer grasping the door handle. This occurs on other tasks as well ...\n\nSorry if I didn't make myself clear enough in my previous comment. \n\nI suggested $s_{i, T_i}$ (last states) because I thought $s_{i, T_i}$ is the only state in the trajectory $\\tau_i$ that reaches the goal. From your description, it seems that there are many states in $\\tau_i$ prior to $s_{i, T_i}$ that already reach the goal.\n\nIf you think $s_{i, T_i}$ is not *useful* (e.g. due to the reason that the door is open but the robot hand is not grasping the door handle) and you don\u2019t want it to be the reset state of the start of your reverse curriculum to $S_{init}$, you can possibly use an earlier demonstration state that is a useful goal state, e.g. $s_{i, T_i-10}$ where the door is open and the robot hand is still grasping the door handle, and then go from $s_{i, T_i-10}$ to $S_{init}$. These are just trivial engineering details.\n\nThe fact that $s_{i, T_i}$ can potentially be a useless/invalid goal state (e.g. due to the reasons mentioned by the authors themselves) shows yet another weakness or mistake of this method. During the curriculum $\\rho^r_{T_i}$, if $s_{i, T_i}$ is a useless/invalid goal state according to the authors, how can the learning in $\\rho^r_{T_i}$ even be successful, considering $\\rho^r_{T_i}$ only samples $s_{i, T_i}$ as the reset state? Are the reported experiment results even real, I wonder?\n\nAlso, what would happen if $s_{i, T_i}$ is not the only useless goal state, but the last few demonstration states (e.g. from $s_{i, T_i-9}$ to $s_{i, T_i}$) are all useless goal states?"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700440441347,
                "cdate": 1700440441347,
                "tmdate": 1700440441347,
                "mdate": 1700440441347,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OWe9ByRAmp",
                "forum": "w4rODxXsmM",
                "replyto": "GZbE45kW8v",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission362/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission362/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response by Authors"
                    },
                    "comment": {
                        "value": "Thank you for the additional discussion. We address them further below:\n\n> I am not saying that using demonstrations is not good. I am just saying that your argument \u201cIt is not possible to sample a state near the goal without using a demonstration, extra human-engineered code, or infeasible amounts of exploration\u201d is wrong or too strong.\n\nThis makes sense, apologies if it came off as too strong of a wording. Indeed there are some simpler tasks where it is possible to sample states along the goal (e.g. the tasks in [A] or a point maze). We just wish to emphasize this is not feasible for many tasks such as the Adroit suite or even the harder tasks in Metaworld and ManiSkill2 due to high state dimensionality of the robot and/or scene setup.\n\n> I don\u2019t think this assumption is too strong. In fact, in your Forward Curriculum, you are making the exact same assumption...\n\nTo clarify, the way [A] moves reset states backward from $s_\\{i, T_i}$ to somewhere in $S_\\{\\text{init}\\}$ is via taking reverse actions. Alternatively one could attempt to interpolate states but anything more advanced would likely be another (interesting) research direction. This is very different from the forward curriculum. The forward curriculum only performs some re-weighted sampling from $S_\\{\\text{init}\\}$ whereas [A] needs to find a way to sample actions to find states between $S_\\{\\text{init}\\}$ and $s_\\{i, T_i}$, which is less than feasible for the Adroit set of tasks due to high dimensionality. We do not move states via e.g. adding noise to a state or taking actions like [A].\n\nPerhaps it might be clear if we define the different types of states in tasks as the following:\n1. The initial states that an environment usually resets to (and one can sample from this freely)\n2. The goal states where the environment would evaluate as success = True. This includes instances where in e.g. AdroitDoor the door is open (and the robot does not have to be currently grasping the door handle, and in code this is how it is checked for success). With some engineering you can sample goal states (that may not be easy to reverse from).\n3. The states where typical success trajectories follow that help lead to goal states (but aren't in $S_\\{\\text{init}\\}$ nor is a goal state). Many intermediate states in demos would fall in this category. We can label this \"solution states\" for now. These are generally not easy to sample without writing a whole solution to begin with.\n4. All other states (e.g. ball in Adroit Relocate off the table)\n\nThen the method in [A] / idea of moving from goal states to initial states (whether the goal state is $s_{i, T_i}$ or $s_{i, T_i-10}$) is effectively looking to find solution states that are between initial states and goal states. E.g. a solution state in Adroit Door would be when the door is not completely open and the robot hand is still grasping the handle to open it. Another would be in a stack 3 blocks task where goal states are states with all 3 blocks stacked in a tower, if 2 / 3 blocks are already stacked and the robot is still in the progress of stacking the third block, this would be a solution state.\n\nWe aren't sure how it would be possible in a task like stacking 3 blocks or adroit door to sample some of these solution states via the approach in [A]. In stacking 3 blocks, if you are given some goal state (say a useful one even) where the 3 blocks are stacked, and the robot hand is grasping the third block, to move back to a initial state without a demo you would need to sample reverse actions / do some extra engineering to solve effectively a \"unstack 3 blocks\" task which is just as hard as the original task. For Adroit Door finding solution states in reverse would be solving a \"Close Door and lock it\" task (the initial state of Adroit Door has the door locked via the handle). Demos overcome the need to have to find these solution states and avoid exploration problems.\n\n> The fact that $s_{i, T_i}$ can potentially be a useless/invalid goal state (e.g. due to the reasons mentioned by the authors themselves) shows yet another weakness...\n\nSorry if we were unclear earlier around this. We are saying that $s_{i, T_i}$ is a goal state, but it may or may not be super useful (and engineering wise we always truncate demos until the last state that is a goal state by checking the environment evaluation). The reason RFCL can still work is because $s_{i, T_i}$ is still a goal state. Even though the robot hand in AdroitDoor is not grasping the door handle in $s_{i, T_i}$, the algorithm will take random actions and as long as it doesn't accidentally close the door the reverse curriculum will progress quite quickly until we reach the demonstration state where the robot hand is grasping the door handle and is already close to opening the door completely. The reverse curriculum progresses quickly because the useless goal states that sometimes appear in the end of demos all evaluate to success = True which progresses the curriculum."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700442738522,
                "cdate": 1700442738522,
                "tmdate": 1700443013497,
                "mdate": 1700443013497,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TvTrAeshFh",
                "forum": "w4rODxXsmM",
                "replyto": "OWe9ByRAmp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission362/Reviewer_UCiq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission362/Reviewer_UCiq"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to the authors for the comments and more information provided. I now have a better understanding of this method.\n\nI think the real problem with the proposed method is that, the authors (implicitly) made an assumption that $S_{init} \\approx \\\\{ s_{i,0} \\\\}$, i.e. the task initial states $S_{init}$ is the same as the initial states of the demonstration trajectories $\\\\{ s_{i,0} \\\\}$ or covers roughly same small area in state space as $\\\\{ s_{i,0} \\\\}$. Though the authors did not explicitly mention this assumption in their problem setting or method description, this assumption can be inferred from their experiments and their previous comments to me.\n\nWhile it is OK to have a certain $S_{init}$ in the problem setting, I disagree with the assumption that $S_{init} \\approx \\\\{ s_{i,0} \\\\}$ or $S_{init}$ being heavily biased towards $\\\\{ s_{i,0} \\\\}$. The initial states of the task don\u2019t have to be constrained to the initial states of the demonstrations but can actually cover a much wider area of the states than what is present in the demonstrations. \n\nWhen you manually set the task initial states $S_{init}$ to be same or similar to demonstration initial states $\\\\{ s_{i,0} \\\\}$, of course your method will benefit from it because your method takes advantage of this manual bias of the task initial states. I don\u2019t think your method can work well when the task initial states cover a much wider range that includes other valid states out of the scope of the demonstration states, due to the exact same reason you mentioned before with [A].\n\nTake the AdroitDoor as an example. It seems that all the task initial states are set to be the hand being fully stretched, placed at the front center of the door and facing downward. I would imagine the demonstrations have the same initial states. This does not make sense to me. Why can\u2019t the task initial states be e.g. the hand being closed, placed on the right side of the door and facing to the left? I don\u2019t think your method can work well with this initial state of the hands. \n\nIn fact, I believe your method could have even more trouble than a method similar to [A], because when you biased the task initial states towards the demonstration initial states, your policy would also be heavily biased and would have trouble generalizing to other valid initial states. For example, the policy will only learn how to approach the door handle from the front side, but will have trouble generalizing to approaching from the right side.\n\nThis is a common mistake by ML researchers. We have seen many such cases in the past where some ML researchers collected data with manually (and mistakenly) introduced bias and another group of ML researchers looked at the data and thought that the bias was universal and specifically proposed a problem formulation and/or method to take advantage of it. In your case, the bias is the distribution of task initial states."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700717341114,
                "cdate": 1700717341114,
                "tmdate": 1700717341114,
                "mdate": 1700717341114,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]