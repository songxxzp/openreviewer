[
    {
        "title": "Summing Up the Facts: Additive Mechanisms behind Factual Recall in LLMs"
    },
    {
        "review": {
            "id": "NVanXt7vWe",
            "forum": "P2gnDEHGu3",
            "replyto": "P2gnDEHGu3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9284/Reviewer_dGcN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9284/Reviewer_dGcN"
            ],
            "content": {
                "summary": {
                    "value": "In the context of LLM, this paper shows there exist four distinct and independent mechanisms that additively combine, constructively interfering on the correct attribute. This generic phenomena is termed as the additive motif: models compute correct answers through adding together multiple independent contributions; the contributions from each mechanism may be insufficient alone, but together they constructively interfere on the correct attribute when summed. In addition, this paper extends the method of direct logit attribution to attribute a head\u2019s output to individual source tokens."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper is well written and easy to follow.\n2. The experiment is sufficient."
                },
                "weaknesses": {
                    "value": "This finding  seems to be not profound enough. It only demonstrates that LLMs perform better under the additive motif, but it appears insufficient to prove that the additive motif is the underlying factual recall behind LLMs."
                },
                "questions": {
                    "value": "1. This finding may explain the fact that models trained on \u201cA is B\u201d fail to generalize to \u201cB is A\u201d.   Is there any possible to explain the CoT prompting such as \u201clet\u2019s think step by step\u201d by using your findings? Does it bring any other insights or explanations for other phenomena that are difficult to explain in LLMs? For example, is there any possible to explain the CoT prompting such as \u201clet\u2019s think step by step\u201d by using your findings?\n\n2. Can this finding contribute to prompt engineering\uff1f\n\n3. Some tables are too wide and are out of page, e.g., table 1."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9284/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698795855868,
            "cdate": 1698795855868,
            "tmdate": 1699637169659,
            "mdate": 1699637169659,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tDDO43NH4n",
                "forum": "P2gnDEHGu3",
                "replyto": "NVanXt7vWe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9284/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9284/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your review. We\u2019re glad you found the paper to be well written, and were able to follow our experimental results.\n\n> This finding seems to be not profound enough. It only demonstrates that LLMs perform better under the additive motif, but it appears insufficient to prove that the additive motif is the underlying factual recall behind LLMs.\n\nThanks for raising this \u2013 we have provided additional discussion about our main findings, and why we believe them to be novel, in a top level comment to all reviewers. See the sections titled \u201c**additivity**\u201d and \u201c**novelty**\u201d.\n\n> This finding may explain the fact that models trained on \u201cA is B\u201d fail to generalize to \u201cB is A\u201d. Is there any possible to explain the CoT prompting such as \u201clet\u2019s think step by step\u201d by using your findings? Does it bring any other insights or explanations for other phenomena that are difficult to explain in LLMs? For example, is there any possible to explain the CoT prompting such as \u201clet\u2019s think step by step\u201d by using your findings?\n\nUnderstanding why chain of thought prompting works so well via interpretability is an interesting problem. We however think our set up would not be a good place to study this, due to the simple one step nature of the problem of factual recall, which is fundamentally different to the multi-hop nature of chain of thought prompting. \n\nThat said, an interesting area of further investigation may be studying \u201cmulti-hop factual recall\u201d, which our results may help explain. Consider prompts of form \u201cThe largest church in the world is located in the city of\u201d. Additivity may help models solve this task in one step, even though a human may reason about this problem in a sequential manner. We have added this idea to future work.\n\n> Can this finding contribute to prompt engineering\uff1f\n\nWe think it\u2019s possible that the insight that models make predictions based on different parts of their input could contribute to prompt engineering. We think this is well known, though has not been shown mechanistically to our knowledge. \n\nFor instance, we should expect models to perform worse at factual recall when prompted with \u201cFact: Michael Jordan plays\u201d over \u201cFact: Michael Jordan plays the sport of\u201d. We find this to be the case -- Pythia-2.8b reports 21.71% accuracy for the first prompt, yet 69.34% for the second, on the | basketball| token. \n\nIn our work, we found that relation heads had meaningful DLA from attending to both the word \u201cplays\u201d and \u201csport\u201d. Omitting the word \u201csport\u201d reduces performance. Our work provides a mechanistic explanation for the intuition that providing more detailed information in prompts is beneficial, in at least this context. \n\nWe also believe our results could be extended by future work to explain few-shot prompting (e.g. via prepending another related fact in the prompt). Preliminary investigation of this early in the project suggested some of the four mechanisms behaved differently under few shot prompting. For instance, relation heads would fire more strongly on prompts with several mentions of the word \u201csport\u201d. This is one area of future work we note in the paper."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9284/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700245217623,
                "cdate": 1700245217623,
                "tmdate": 1700245217623,
                "mdate": 1700245217623,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "I9gGUMQtQY",
            "forum": "P2gnDEHGu3",
            "replyto": "P2gnDEHGu3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9284/Reviewer_npyu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9284/Reviewer_npyu"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a set of experiments on a small hand-crafted data\nset for identifying the mechanisms at play during \"factual recall\" in\nlarge language models. The paper defines four \"mechanisms\" based on\n(1) attention heads that focus (mostly) subject of a factual\npredicate, (2) attention heads that focus (mostly) relation, (3)\nattention heads that attend to both, and (4) MLP layer. The main claim\nis that these mechanisms additively determine the correct attribute."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The study tackles an important/interesting problem and the paper reports a substantial amount of experimentation."
                },
                "weaknesses": {
                    "value": "Although I believe the idea is interesting, and there may be some\nvaluable finding in the paper, I have difficulties seeing a clear\ntake-home message based on the results presented, and probably also\ndue to the way they are presented. I have some concrete points of\ncriticism listed in the comments below (with approximate order of importance).\n\n- The main claim, additivity of the multiple mechanisms, is not very\n  clearly demonstrated in the paper. The separation of the\n  subject/relation heads (as displayed in Fig. 2) is impressive.\n  However, neither the roles of the \"mixed head\" mechanism, the MLP,\n  and additivity of all these mechanisms are not clearly demonstrated.\n\n- The dataset is rather small and it is not described in the paper at\n  all. The description of in the appendix is also rather terse,\n  containing only a few examples. Given the data set size (hence the\n  lack of diversity), and the possible biases (not discussed) during\n  the data set creation, it is unclear if the findings can generalize\n  or not. In fact, some of the clear results (e.g., the results in\n  Fig. 2) may be due to the simple/small/non-diverse examples.\n\n- I also have difficulty for fully understanding the insights the\n  present \"mechanisms\" would provide. To me, it seems we do not get\n  any further insights than the obvious expectation that the models\n  have to make their decisions based on different parts of the input\n  (and meaningful segments may provide independent contributions). I\n  may be missing something here, but it is likely that many other\n  readers would miss it, too.\n\n- Visualizations are quite useful for observing some of the results.\n  However, the discussion of findings based on a more quantitative\n  measure (e.g., DLA difference between factual and counterfactual\n  attributes) would be much more convincing, precise, repeatable, and\n  general.\n\n- Overall, the paper is somewhat difficult to follow, relying data in\n  the appendix for some of the main claims and discussion points.\n  Appendixes should not really be used for circumvent page-limits.\n  Ideally, most readers should not even need to look at them.\n\n- The head type (subject/relation) definition uses an arbitrary\n  threshold. Although it sounds like a rather conservative choice, it\n  would still be good to know how it was determined."
                },
                "questions": {
                    "value": "Some typo/language issues:\n - Introduction second paragraph: \"(Meng et al., 2023a) find ...\"\n  -> \"Meng et al. (2023a) find ...\" \n- Although it is a very common \"mistake\" in the field, all \n  established style guides I know prescribe that footnote marks\n  to be placed after punctuation. Also, I strongly recommend\n  against placing footnote marks directly on symbols (like R^3).\n- It is a good idea to indicate that figure/table references to\n  the appendix are in the appendix.\n- The \"categories\" defined at the beginning of the results section\n  comes as a surprise, and seem to be an important part of the \n  analysis throughout. This should be defined/explained earlier.\n- End of sentence punctuation missing for footnote 4.\n- There are no references to Figure 2 from the text.\n- It may not be that easy for some figures, but B/W friendly\n  figures would be appreciated by people reading on paper or\n  monochrome devices (like e-ink readers).\n- Some terms like \"OV circuit\" or \"ROME\" that many readers are not \n  likely to be familiar with should be briefly introduced.\n- The same goes for abbreviations of the sort L22H17. Not \n  difficult to guess for most readers, but it would be more reader\n  friendly to explain at first use."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9284/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9284/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9284/Reviewer_npyu"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9284/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699051336204,
            "cdate": 1699051336204,
            "tmdate": 1700733765386,
            "mdate": 1700733765386,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pj8GgJUi2n",
                "forum": "P2gnDEHGu3",
                "replyto": "I9gGUMQtQY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9284/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9284/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the generous feedback. We are glad you find our problem choice of understanding how models perform factual recall important, and note that our findings may be valuable to the community.\n\n> The main claim, additivity of the multiple mechanisms, is not very clearly demonstrated in the paper. The separation of the subject/relation heads (as displayed in Fig. 2) is impressive. However, neither the roles of the \"mixed head\" mechanism, the MLP, and additivity of all these mechanisms are not clearly demonstrated.\n\nWe address your concerns regarding the main claim of additivity in a top level comment to all reviewers above titled **additivity**. The clearer distinction for additivity is into two clusters of updates - relating either to the relation or subject. Each of the four model mechanisms contributes to one or both (in the case of mixed heads) of these clusters.\n\n\n> The dataset is rather small and it is not described in the paper at all. The description of in the appendix is also rather terse, containing only a few examples. Given the data set size (hence the lack of diversity), and the possible biases (not discussed) during the data set creation, it is unclear if the findings can generalize or not. In fact, some of the clear results (e.g., the results in Fig. 2) may be due to the simple/small/non-diverse examples.\n\nThanks for this comment. We have provided further details regarding our dataset in an Appendix C of the paper.\n\nWe agree with your criticism regarding the size and potential biases of the dataset. We agree this is not ideal, but found dataset creation difficult. Despite this, our dataset spans several different relations $r$, and contains over 100 prompts, which we think renders our findings valuable and sufficient for demonstration of the *existence* of a range of mechanisms and of additivity. We believe this is a valuable contribution given substantial community interest in both factual recall and interpretability more generally. \n\nWe discuss the limitations we faced with dataset creation at some length in Appendix C of the paper. \n\nQuoting from the paper:\n\n```We found the pre existing datasets to be unsatisfactory for our analysis, due to some additional requirements our set up necessitated. We firstly required models to both \\textit{know} facts and to \\textit{say facts} when asked in a simple prompting set up, and for the correct attribute $a$ to be completely determined in its tokenized form by the subject and relationship. For example `The Eiffel Tower is in' permits both the answer `Paris' and `France'. For simplicity we avoided prompts of this form. Synonyms also gave us issues, e.g. `football' and `soccer', or `unsafe' and `dangerous'. This mostly restricted us to very categorical facts, like sports, countries, cities, colors etc.  We also wanted to avoid attributes that mostly involved copying, such as `The Syndey Opera House is in the city of Sydney`, as we expect this mechanism to differ substantially from the more general mechanism, and to rely mostly on induction heads \\citep{olsson2022context}. Next, we wanted to create large datasets with $r$ held constant, and separately, with $s$ held constant. Holding the relation constant and generating many facts is fairly easy. But generally models know few facts about a given subject, e.g. `Michael Jordan' is associated very strongly with `basketball', but other facts about him are less important and well known. Certain kinds of attributes, like `gender' are likely properties of the tokens themselves, and not likely not reliant on the `subject enrichment' circuitry - e.g. `Michael' and `male'. We try and avoid these cases. We also restrict to attributes where the first attribute token mostly uniquely identifies the first answer token. If the first token of the attribute is a single character or the word \u201cthe\u201d, this can be vague, so we omitted these cases. These considerations limited the size of the dataset we studied.```\n\n> I also have difficulty for fully understanding the insights the present \"mechanisms\" would provide. To me, it seems we do not get any further insights than the obvious expectation that the models have to make their decisions based on different parts of the input (and meaningful segments may provide independent contributions). I may be missing something here, but it is likely that many other readers would miss it, too.\n\nThanks for this question. We believe this kind of study is valuable in the context of the mechanistic interpretability literature, which we discuss in a top level comment to all reviewers, under the heading \u201c**novelty**\u201d.\n\n*(continued...)*"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9284/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700244982321,
                "cdate": 1700244982321,
                "tmdate": 1700244982321,
                "mdate": 1700244982321,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ENGTD3pzAc",
                "forum": "P2gnDEHGu3",
                "replyto": "I9gGUMQtQY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9284/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9284/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Visualizations are quite useful for observing some of the results. However, the discussion of findings based on a more quantitative measure (e.g., DLA difference between factual and counterfactual attributes) would be much more convincing, precise, repeatable, and general.\n\nThanks for this comment. We found qualitative figures generally a more useful form to present our results. Our main aim was to demonstrate a range of mechanisms with different qualitative properties exist. This is what is necessary to demonstrate additivity, which is our main goal in the paper.  \n\nThat said, we do agree that a more raw form of data can also be valuable. In Appendix E we present several instances of such data, on a per-prompt basis. We include for instance tables of raw ordered output tokens, logit ranks, and DLA scores.\n\n> Overall, the paper is somewhat difficult to follow, relying data in the appendix for some of the main claims and discussion points. Appendixes should not really be used for circumvent page-limits. Ideally, most readers should not even need to look at them.\n\nThank you for the feedback, sorry for this. We have since restructured to emphasise the additivity claim more prominently. This is the main claim in the paper. In order to show additivity, it suffices to have two mechanisms (namely, subject and relation heads). These are distinct, and we explain these in detail. We in fact find four mechanisms, which we include for completeness.  We struggled to fit figures for all four mechanisms in the main body. We therefore made the decision to prioritise \u201csummary\u201d figures in the first results section to give a high level picture of the various mechanisms, and a detailed explanation of two mechanisms.\n\n> The head type (subject/relation) definition uses an arbitrary threshold. Although it sounds like a rather conservative choice, it would still be good to know how it was determined.\n\nThanks for this comment. We agree this choice is somewhat arbitrary, but we believe it\u2019s useful to draw this distinction. See the top level comment, under \u201c**classifying attention heads**\u201d.\n\n> The \"categories\" defined at the beginning of the results section comes as a surprise, and seem to be an important part of the analysis throughout. This should be defined/explained earlier.\n\nThanks for raising this point of confusion. The categories are important to the analysis on the individual component level, but are not conceptually important to the main claim of additivity of the paper. There are several levels of granularity discussed in the paper. From high to low: two additive clusters, four separate groups of mechanisms, and many individual components implementing these. Categories operate on the lowest component level, and merely state individual components (like individual attention heads), have a narrow purpose. We have removed references to categories from the \u201csummary of results\u201d section, to make the main claims regarding additivity flow better and be more clear.\n\nWe have additionally addressed all the typos you pointed out - thanks for these."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9284/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700244995773,
                "cdate": 1700244995773,
                "tmdate": 1700268672151,
                "mdate": 1700268672151,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "24cZWqhaBO",
            "forum": "P2gnDEHGu3",
            "replyto": "P2gnDEHGu3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9284/Reviewer_ctMu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9284/Reviewer_ctMu"
            ],
            "content": {
                "summary": {
                    "value": "This work target at interpreting the inner mechanisms of LLMs in accomplishing the task of Factual Recall. This work identifies and explains four distinct mechanisms present in the model, as well as the additive cooperation between these mechanisms. This work validates the generalizability of this mechanism across different models and facts."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "(1) Based on sufficient experimental results verification, the author has identified and explained the internal mechanisms of LLMs at the granularity level of attention heads and MLPs. More interestingly, it provides an explanation of the \u201creversal curse\u201d phenomenon discovered in recent works.\n(2) This work has thoroughly discussed the related work and proposed a range of possible directions for future works."
                },
                "weaknesses": {
                    "value": "(1) There have been many works [1, 2] interpreting the model behavior of Factual Recall. It seems that the novelty is insufficient with only a deeper zooming into attention heads using similar interpretability methods. Additionally, the discovery of the additive motif is not surprising enough, as already explained in work [3] that \"Attention heads can be understood as independent operations, each outputting a result which is added into the residual stream.\" \n(2) Is direct logit attribution (DLA) the same as the interpretability method of Path Patching [4] or Causal Mediation Analysis [5]? If so, it is necessary to explain how the counterfactual data is applied for causal intervention. If it is not, it is necessary to provide a detailed description of the algorithm flow of DLA.\n\n(3) This work extends \u201cDLA by source token group\u201d with a weighted sum of outputs corresponding to distinct attention source position. But how to obtain the \u201cweights\u201d? How to attribute multiple tokens simultaneously? These missing implementation details make it difficult to understand the method and reproduce the results.\n\n\n[1] Locating and Editing Factual Associations in GPT\n[2] Dissecting Recall of Factual Associations in Auto-Regressive Language Models\n[3] A Mathematical Framework for Transformer Circuits\n[4] Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small\n[5] Investigating Gender Bias in Language Models Using Causal Mediation Analysis"
                },
                "questions": {
                    "value": "(1) It would be better to validate the faithfulness of the identified components (e.g., Subject Heads, Relation Heads) for Factual Recall? What would happen to the prediction ability (e.g., accuracy) of the model for Factual Recall task if these components were knocked out? \n\n(2) We wonder if it is possible to explain the behavior of MLPs explicitly, similar to explaining Attention Heads via attention patterns?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9284/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9284/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9284/Reviewer_ctMu"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9284/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699373204470,
            "cdate": 1699373204470,
            "tmdate": 1700612227882,
            "mdate": 1700612227882,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "x4zIOr0doU",
                "forum": "P2gnDEHGu3",
                "replyto": "24cZWqhaBO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9284/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9284/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the detailed review. We\u2019re glad that you found our experimental results sufficient to explain the internal mechanisms LLMs use to recall facts. We\u2019re also happy to hear that you find the work relevant in the context of the existing literature. Below we comment on some of the weaknesses you point out in the paper.\n\n> (1) There have been many works [1, 2] interpreting the model behaviour of Factual Recall. It seems that the novelty is insufficient with only a deeper zooming into attention heads using similar interpretability methods. \n\nOur approach is fundamentally different to [1] and [2], and our findings subsequently different. As you noted, we chose to zoom in deeply into individual model components, which [1] did not do at all, and [2] only briefly studied. Through using a \u201ccircuits style\u201d approach, we were able to find insights that both [1] and [2] missed. [1] studies where factual knowledge is stored in models, but not how such knowledge is used to predict the next token. [2] attempts to study this problem, but only does so through coarse grained ablations. They suggest a mechanism based on this, which we search for in individual model components. In doing so, we instead find several independent mechanisms that explain how information regarding factual knowledge is moved. We additionally find an extra place where factual information is stored \u2013 namely in the relationship token \u201csport\u201d itself - prior work missed this important aspect, which we found through careful study of a range of counterfactual attributes S and R. This motivated the focus on \u201cadditivity\u201d (see the top level comment titled \"**additivity**\" for more discussion on this). We also discuss the \"**novelty**\" of this work in another top level comment.\n\n> Additionally, the discovery of the additive motif is not surprising enough, as already explained in work [3] that \"Attention heads can be understood as independent operations, each outputting a result which is added into the residual stream.\" \n\nThanks for raising this confusion. We address what we mean by additivity in a top level comment to all reviewers, and have amended the paper to clarify this. Our use of additivity is distinct from [3].\n\n> (2) Is direct logit attribution (DLA) the same as the interpretability method of Path Patching [4] or Causal Mediation Analysis [5]? If so, it is necessary to explain how the counterfactual data is applied for causal intervention. If it is not, it is necessary to provide a detailed description of the algorithm flow of DLA.\n\nThanks for raising this confusion. Direct logit attribution (DLA) is not the same technique as path patching or causal mediation analysis. DLA is a simple technique that we describe in the paper in section 2. We have now added further discussion on this, including a detailed description of the algorithm, in Appendix D. It is not based on a causal intervention. It builds on a technique named the \u201clogit lens\u201d. Both are based on the insight by [3] that the residual stream is an accumulated sum of model components, and that the map to logits from the final residual stream vector is approximately linear. While running a single forward pass, we may take the output from individual model components, such as attention heads, and directly map these to logit space, by immediately applying the model unembedding (aka language model decoder head/final linear layer). Given a set of logits, we can read off the DLA for any possible output token, including for counterfactual output tokens. \n\n> (3) This work extends \u201cDLA by source token group\u201d with a weighted sum of outputs corresponding to distinct attention source position. But how to obtain the \u201cweights\u201d? How to attribute multiple tokens simultaneously? These missing implementation details make it difficult to understand the method and reproduce the results.\n\nOur extension is to consider the attention head output as a weighted sum over individual source positions, where the weighting is given by the attention probability. This is entirely faithful to transformer computation. By unravelling this sum, we can consider DLA for attention heads to be attributed to all attention source tokens individually. We have provided a detailed discussion with formulae of this technique in Appendix D.\n\n> (1) It would be better to validate the faithfulness of the identified components (e.g., Subject Heads, Relation Heads) for Factual Recall? What would happen to the prediction ability (e.g., accuracy) of the model for Factual Recall task if these components were knocked out?\n\nThanks for this question. We address this question in the top level comment, under \u201c**ablations**\u201d.\n\n*(continued...)*"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9284/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700244485004,
                "cdate": 1700244485004,
                "tmdate": 1700244485004,
                "mdate": 1700244485004,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qX8unu7qUZ",
                "forum": "P2gnDEHGu3",
                "replyto": "KeZiaKMtli",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9284/Reviewer_ctMu"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9284/Reviewer_ctMu"
                ],
                "content": {
                    "title": {
                        "value": "Re: authors response"
                    },
                    "comment": {
                        "value": "After carefully read the reply and revised appendix in the paper, I think my concern about the novelty and the takeaway contributions of this paper is still not clear enough, I decide to lower my score from 6 to 5."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9284/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700612297718,
                "cdate": 1700612297718,
                "tmdate": 1700612297718,
                "mdate": 1700612297718,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YInYz2Ns0Y",
            "forum": "P2gnDEHGu3",
            "replyto": "P2gnDEHGu3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9284/Reviewer_vZgL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9284/Reviewer_vZgL"
            ],
            "content": {
                "summary": {
                    "value": "The work concerns the task of factual recall in LLMs i.e. in templated prompts, the LLM is tasked to predict the object attribute of the tuple (subject, relation, attribute). Authors propose that factual recall in the END position (correct logit ranking) occurs by the summation of contributions of different additive circuits in the transformer.\n- Authors extend Direct Logit Attribution (DLA) to compute the joint contribution from different source token groups to the final predicted logits\n- 4 different additive circuits are identified based on the extended DLA: SUBJECT, RELATION, MIXED and MLP\n- SUBJECT attention heads preferentially boost attributes that are relevant to the subject of the query\n- RELATION attention heads preferentially boost attributes that are relevant to the relation of the query independent of the subject\n- MIXED attention heads boost the attributes that are jointly relevant to the subject and relation of the query\n- MLP layers at the end position uniformly boost the attributes relevant to the relation (ignoring the subject tokens)\n\nThe central findings of the paper revolve around the Pythia-2.8b model. Additional experiments in the Appendix report that similar types of circuits may be found in other models but all categories may not always exist.\n\nLimitations:\n- Authors acknowledge that the boundary between MIXED and other attention head types is fuzzy. The definition used to separate attention heads was based on preferential contribution from SUBJECT or RELATION and any other type is considered a mixed type."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper uses established mechanistic interpretation tools and extends them to identify mechanisms in the transformer that perform very specific purposes\n    - The SUBJECT-head, RELATION-head, and MLP additive behaviors are established by showing consistent patterns across a range of fact queries"
                },
                "weaknesses": {
                    "value": "- The paper introduction and further discussions claim that the results reported here provide a mechanistic explanation for the limitations of LLMs to learn \"B is A\" from training on \"A is B\" [1]. However, I do not see sufficient evidence to support this claim\n    - They have shown that in the forward direction the transformer selectively promotes attributes relevant to the subject and the relation\n    - This does not show that the transformer CANNOT/DOES NOT perform the same operations in the reverse direction.\n    - E.g. \"Basketball is played by ...\" may contain circuits that selectively promote the known basketball players. The lack of such circuits is not demonstrated by this work\n    - In particular, the authors argue that the LLM learns an \"asymmetric\" look-up. However, the asymmetry is not established.\n\nPresentation\n---\n- Significant space in the main paper is used to describe future work. I believe that there is an interesting and valuable discussion about dataset creation in the Appendix that should be brought to the main paper\n\n\n[1] Lukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak, and Owain Evans. The Reversal Curse: LLMs trained on \"A is B\" fail to learn \"B is A\", September 2023. URL http://arxiv.org/abs/2309.12288. arXiv:2309.12288 [cs]."
                },
                "questions": {
                    "value": "1. Is it fair to say that the key findings are the presence of SUBJECT-only and RELATION-only heads among the attention heads in the transformer? All other heads are MIXED heads by default?\n2. What fraction of attention heads get categorized into extreme categories (SUBJECT and RELATION)?\n3. How does the contribution to the final logits from the extreme categories (SUBJECT and RELATION) compare to the heads that are categorized as MIXED?\n4. Tagging onto questions 3 and 4: is there a significant drop in model performance when extreme heads are suppressed?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9284/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699494635071,
            "cdate": 1699494635071,
            "tmdate": 1699637169327,
            "mdate": 1699637169327,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2PGfmPUw5o",
                "forum": "P2gnDEHGu3",
                "replyto": "YInYz2Ns0Y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9284/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9284/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the generous review. We are glad you appreciated our use of existing and novel mechanistic interpretability techniques to identify mechanisms with specific purposes used for factual recall in transformer language models.\n\n> The paper introduction and further discussions claim that the results reported here provide a mechanistic explanation for the limitations of LLMs to learn \"B is A\" from training on \"A is B\" [1]. However, I do not see sufficient evidence to support this claim\n\nWe thank the reviewer for their feedback. We believe that our work does provide a mechanistic explanation for the reversal curse, but thank the reviewer for pointing out the need to communicate this more clearly. The reversal curse claims that models trained on \u201cA is B\u201d fail to generalise to \u201cB is A\u201d. Importantly, this is difficult to verify with purely pre-trained models on common facts (e.g. Michael Jordan plays basketball) where the model likely saw the fact in both forms (basketball is played by Michael Jordan, Michael Jordan plays basketball). The reversal curse paper demonstrates this with fine-tuning on novel facts, ensuring the reverse-direction was not seen. \n\nIn our work, we only study a pre-trained model, and so the evidence we provide is indirect and suggestive, as we do not fine-tune ourselves. We find a circuit by which models may learn to output \u201cA is B\u201d, involving subject enrichment on the A tokens, and some attention head attending to A and extracting B. Importantly, this is a unidirectional circuit with two unidirectional components - it extracts the fact \u201cB\u201d from \u201cA\u201d.  This suggests that the reason fine-tuning on \u201cA is B\u201d does not boost \u201cB is A\u201d in general is because training on \u201cA is B\u201d only boosts the unidirectional A -> B mechanisms, and has no effect on potential B -> A mechanisms.\n\n\n> Is it fair to say that the key findings are the presence of SUBJECT-only and RELATION-only heads among the attention heads in the transformer? All other heads are MIXED heads by default?\n\nYes, in part. We believe our key finding is that there exist a range of different mechanisms with qualitatively different functions that all contribute to this task, and which interact additively. We clarify what we mean by this in the top level comment. All other heads are mixed, yes. Some heads are of course more important than others for this subtask.\n\n\n> What fraction of attention heads get categorized into extreme categories (SUBJECT and RELATION)?\n\n> How does the contribution to the final logits from the extreme categories (SUBJECT and RELATION) compare to the heads that are categorized as MIXED?\n\n> Tagging onto questions 3 and 4: is there a significant drop in model performance when extreme heads are suppressed?\n\n\nThanks for these questions regarding the prevalence and importance of the various mechanisms. All three of these questions ask similar questions. We address the first two of your questions here, and refer you to the top level comment \u201cablations\u201d for an answer to the third.\n\nThe **fraction of heads** classified each way varies depending on the choice of relation. See Figure 3 for two examples. There, we see the split of (subject, relation, mixed) among the top 10 heads for the relation \u201cplays the sport of\u201d is (2, 1, 7), but for \u201cis in the country of\u201d it is (4, 2, 4). In response to this question, we ran experiments to check what the split was across our entire dataset and without aggregation over the relation. Inspecting the top 10 heads by DLA for each example we find 37% of heads get categorised as subject heads and 33% as relation heads, with the remaining 30% as MIXED heads. This indicates all three head types are important for the task.\n\nThe **contribution to logits** is another good metric. Figure 2 visualises this \u2013 we can qualitatively see that all three head types are important. In response to this question, we ran some additional experiments. We include below the percentage of the final (mean centred) logit contributed from each component type, across the entire dataset. We omitted several negative suppressive components for the purpose of this analysis. Again, we see that the contributions from each type of mechanism is important. Subject heads contribute 18%, relation heads 24%, mixed heads 27%, and the mlp layers 30%, across the entire dataset.\n\n*(continued...)*"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9284/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700244216369,
                "cdate": 1700244216369,
                "tmdate": 1700244260902,
                "mdate": 1700244260902,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]