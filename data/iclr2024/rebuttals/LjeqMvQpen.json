[
    {
        "title": "Transformer Fusion with Optimal Transport"
    },
    {
        "review": {
            "id": "Vh0r2LAvbD",
            "forum": "LjeqMvQpen",
            "replyto": "LjeqMvQpen",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4949/Reviewer_hByk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4949/Reviewer_hByk"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a method for fusing multiple independently trained transformer architectures using optimal transport to align their respective architectural components. To this end, authors analyse predominant Transformer architectures based on their components, and provide OTFusion methods for each. The proposed approach allows for fusing transformers of different sizes. \nExperiments are conducted on a range of image classification datasets; CIFAR10, CIFAR100, TinyImagenet and ImageNet-1k. Authors show results for models obtained through both zero-shot (without fine tuning) and with fine tuning. In zero-shot fusion, the proposed approach outperforms Vanilla Fusion methods. With fine tuning, the proposed approach is able to beat either parent model. Authors conclude with a number of limitations of their approach and suggestions for future research."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper is very well-written, authors give clear and concise descriptions of their approach and illustrate its complex aspects through figures and examples. This makes the manuscript easy to read and the ideas it expands upon easy to understand despite their complexity. The method seems to work well, drastically outperforming Vanilla Fusion (naive model averaging). The authors show a range of valuable ablations, and motivate most of their design choices well."
                },
                "weaknesses": {
                    "value": "My main concern is to do with the clarity of the contribution of this work. The authors refer to [1] a lot in their paper, where the concept of OTFusion is introduced. It seems like a lot of the techniques used in this work were actually introduced there. Although I understand the need for reintroducing these concepts in the manuscript for contextual clarity, I think it would be good to give a clearer picture of the actual contributions made in this work and the methods proposed in previous works. From the description under 4.3 it seems [1] uses hard alignment where you find soft alignments to outperform. Are these contributions of your work? What about the TM combination approaches (Averaging/Weighted Scalar/ Weighted Matrix)? Or heterogeneous fusion?\n\nI hope the authors are able to address this in their rebuttal, in which case I see this work as an interesting and strong submission.\n\n[1] Sidak Pal Singh and Martin Jaggi. Model fusion via optimal transport. Advances in Neural Information\nProcessing Systems, 33:22045\u201322055, 2020."
                },
                "questions": {
                    "value": "-What do you mean by \u201cThis diversity offers a challenging fusion problem requiring a non-trivial alignment strategy, and thus effectively recreates a plethora of other scenarios\u201d (under 5 - Model Training). Can you explain e.g. how varying random seed equates to model training on different subsets?\n-How does your work relate to [2]? You indicate that [2] is very similar to OTFusion, but looking at zero-shot performance of your method (and your VF baseline) on CIFAR10 classification it seems performance is drastically different (~93% vs ~60%). If essentially identical, why does [2] yield zero-barrier LMC where your approach does not?\n-Could you give an intuition for soft-alignment, what resulting network is actually being constructed  in this case and why could it be beneficial compared to hard alignment approaches?\n-Do you have an intuition for why your method performs better with soft-alignment, where [1] shows better results with hard alignment?\n\n[2] Samuel K Ainsworth, Jonathan Hayase, and Siddhartha Srinivasa. Git re-basin: Merging models\nmodulo permutation symmetries. arXiv preprint arXiv:2209.04836, 2022.\n\n---\n\nUpdate after rebuttal: I thank the authors for their thorough rebuttal. I'm pleased to say my concerns are adequately addressed. Also considering the largely positive reviews by the other reviewers, I'd like to update my recommendation to an accept."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4949/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4949/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4949/Reviewer_hByk"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4949/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698774850423,
            "cdate": 1698774850423,
            "tmdate": 1700923426225,
            "mdate": 1700923426225,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QRovvNSz9I",
                "forum": "LjeqMvQpen",
                "replyto": "Vh0r2LAvbD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4949/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4949/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors 1/3"
                    },
                    "comment": {
                        "value": "*Thank you for your useful feedback. We are truly glad that you have appreciated our efforts for a well-presented and well-written paper. At the same time, we recognize that some explicit clarifications regarding our contribution would have helped to better convey the contribution of our paper, and we are grateful for making us aware of this. To reflect your doubts, and with the utmost goal of improving the clarity of our work, we have implemented various changes throughout the manuscript.*\n\n---\n\nIn what follows, besides addressing your specific questions, we would nevertheless like to first expand on your concerns about the contribution of our work. \n\n### Contribution of this work with respect to [1]\n\n**Key differences**\n\n1. **Enabling Fusion for newer Transformer-based architectures**\n\n    [1] is the first to successfully introduce the concept of Optimal Transport for alignment and fusion of multiple models. But, broadly speaking, i**t is however restricted to simple architectures** such as MLPs, CNNs, and instances of ResNet.\n\n    It is **not** equipped in any way to align and fuse models with complex information streams and to fuse transformer-specific components such as multi-head attention layers, layer-normalization, embeddings, or the sequential nature of the data, which we systematically analyze and successfully handle.\n\n2. **New findings in regards to hard vs soft alignment**\n\n    Furthermore, we broaden the perspective on alignment introduced by [1] in the following manner. While [1] technically allows soft alignment, they discovered that for simpler architectures (MLPs, CNNs, ResNets) hard alignment outperforms soft alignment (Table S4 of [1]).\n\n    In contrast, **we find the reverse to be true when fusing Transformers** (Sec. 4.3, and Sec. 5), which possibly hints at the difference in the nature of their architectures and representations, and forms an interesting question for standalone future study.\n\n**Other aspects**\n\n- **Heterogeneous fusion**\n  \n    As mentioned on page 2 of [1], indeed [1] is the first to showcase heterogeneous model fusion, \u201cOTFusion accommodates the fusion of models with different widths, and in turn, different sizes\u201d.  But, **we are the first to extend this concept to Transformers**.\n\n- **Weighted TM combination & other minor methodological extensions:**\n  \n    Lastly, we have included some other extensions for OTFusion [1], to better support transformers.\n\n    As an example, realizing the potentially complex peculiarities of transformers when it comes to residual streams, we have extended the Averaging strategy introduced by [1], with **two novel strategies**, namely *Weighted Scalar* and *Weighted Matrix*, that allow for a more flexible fusion strategy depending on the properties of the residual stream of that layer (Sec. 4.2.1).\n\n    Also, we have extended OTFusion's idea of activations-based alignment to better encompass the sequential nature of the data, introducing various **novel strategies** of activations filtering in the case of the ViT (Sec. 4.3).\n\n---"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4949/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700262299378,
                "cdate": 1700262299378,
                "tmdate": 1700262299378,
                "mdate": 1700262299378,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "JbEmqNhn18",
            "forum": "LjeqMvQpen",
            "replyto": "LjeqMvQpen",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4949/Reviewer_KSPk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4949/Reviewer_KSPk"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a systematic fusion technique for transformer-based networks by leveraging Optimal Transport to align architectural components. It offers a flexible approach applicable to various architectures, including key Transformer components. Heterogeneous fusion enables efficient compression, with superior performance compared to vanilla fusion and individual parent models, as demonstrated in image classification (Vision Transformer) and natural language tasks (BERT). Our analysis underscores the significance of soft alignment in the context of Transformers, highlighting the potential for combining multiple Transformers to enhance their capabilities in the emerging field of model fusion and recombination."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The authors examined various strategies (weight vs activation, hard vs soft etc) for applying optimal transport (OT) methods\n2. The authors conducted experiments employing both Vision Transformer (ViT) and BERT architectures across multiple datasets.\n3. The OT method demonstrates particular efficacy in one-shot scenarios.\n4. OT methods exhibit versatility, as they can be effectively applied to models of varying widths, presenting a viable alternative to distillation."
                },
                "weaknesses": {
                    "value": "1. The OT method yields comparatively lower performance when contrasted with ensemble methods.\n2. The suitability of the OT method for achieving solid results on larger datasets, such as ImageNet-1K, in one-shot scenarios remains uncertain."
                },
                "questions": {
                    "value": "Please refer to the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4949/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4949/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4949/Reviewer_KSPk"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4949/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698811300859,
            "cdate": 1698811300859,
            "tmdate": 1699636481508,
            "mdate": 1699636481508,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "X9nXyWX3Xa",
                "forum": "LjeqMvQpen",
                "replyto": "JbEmqNhn18",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4949/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4949/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors 1/2"
                    },
                    "comment": {
                        "value": "*Thanks for your valuable review. We are extremely glad to hear that you have appreciated our systematic investigation of the various fusion strategies. In what follows ahead, we would like to take the opportunity to address your primary concerns.*\n\n---\n\n### Accuracy of the fused model\n\n> The OT method yields comparatively lower performance when contrasted with ensemble methods.\n\nAllow us to detail the inherent factors at play when fusing the parameters of the given two neural networks:\n\n- **Non-convexity of the neural network loss-landscape**\n \n  It is widely accepted that neural networks result in a highly non-convex loss landscape and optimization dynamics. As a result, by definition of non-convexity, it is rather unlikely that the loss $\\mathcal{L}$ at, say,  the midpoint between two networks (with parameters $\\theta_1$ and $\\theta_2$) is less than the average of the losses of the two networks. Mathematically,\n \n\t$$ \\mathcal{L}\\left(\\frac{\\theta_1 + \\theta_2}{2}\\right) \\nless \\frac{1}{2} \\mathcal{L}(\\theta_1) + \\frac{1}{2} \\mathcal{L}(\\theta_2)\\. $$\n\n- **Empirical success of Model Fusion and conjectured Linear Mode Connectivity modulo Permutations**\n\n\tNotwithstanding this inherent non-convexity, [3, 4] showed that, in practice, if the network parameters are aligned prior to fusion, like by accounting for their permutation symmetries, the loss landscape (near the basin of solutions) becomes much less non-convex. The works of [2] and [1] have provided additional evidence that when the network widths are large, the two networks (or modes) can more or less be linearly connected modulo permutation symmetries (RHS - LHS of the above equation is small).\n\n \n- **Recovering the exact permutation is NP-hard**\n    \n\tMost algorithmic approaches, either of [3] or [1] are greedy layerwise algorithms. This is because recovering the exact permutation symmetry that links the given networks becomes intractable as the search space grows in proportion to $\\mathcal{O}({(m\\!)}^L)$, where $m$ denotes the layer width and $L$ is the network depth. Also, see Lemma 1 in [1]. As a consequence, we see the inherent difficulty of the problem that we are tackling here.\n\n- **The added complexity of Transformers**\n\n\tTo the best of our knowledge, prior works have only studied fusion and linear mode connectivity mostly in the case of MLPs, CNNs, and ResNets, and **not for Transformers**. On the other hand, Transformers seem to further complicate the above issues in practically finding a good alignment, for instance, due to additional components such as the multi-head self-attention (resulting in the need for intra-head as well as inter-head alignment), LayerNorms, and, more often than not, larger depths. Also, other kinds of symmetries get introduced, such as scale invariance, due to the presence of softmax.\n\n- **Ensembling is implicitly using more capacity**\n\n\tSo far we only have outlined the difficulty of the fusion process. However, the other obvious fact is that ensembling uses $K \\times$ more capacity than fusion, where $K$ is the number of networks that are ensembled. Hence, the ensembling performance can be regarded, in some sense, as the unachievable upper bound on the performance of the singular network produced through fusion.\n\n\n**In this light, arguably, it is rather noteworthy that like prior works:**\n  1. We can still get highly non-trivial performance gains over vanilla fusion. In the case of Transformers, we believe this is also where soft alignment benefits since it helps relax the constraints from hard alignment (permutation matrices) to softer alignments (with transportation maps).\n  2. Moreover, after fine-tuning, even this **gap relative to ensembling considerably diminishes**. Besides, this fine-tuning procedure is usually extremely fast \u2014 requiring as few as **0.4%** of the original training duration \u2014, and since fine-tuning has to be done just once even this minor cost gets amortized over inference (while ensembling must pay $O(n)$ (where $n$ is the number of parent models) more at every inference call)"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4949/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700261961510,
                "cdate": 1700261961510,
                "tmdate": 1700261961510,
                "mdate": 1700261961510,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ec1rj2wP1c",
                "forum": "LjeqMvQpen",
                "replyto": "fJEULlwNtR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4949/Reviewer_KSPk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4949/Reviewer_KSPk"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your reply. I will maintain the existing score"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4949/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700627059099,
                "cdate": 1700627059099,
                "tmdate": 1700627059099,
                "mdate": 1700627059099,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9tniV8L56p",
            "forum": "LjeqMvQpen",
            "replyto": "LjeqMvQpen",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4949/Reviewer_5GYY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4949/Reviewer_5GYY"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a systematic approach for fusing two or more pretrained transformers by studying the flow of transportation maps in each specific component of Transformer. The authors empirically showed that when working with Transformers, hard alignment underperforms soft alignment in one-shot fusion, which is in contrast to the cases of fully connected and convolutional neural networks. Finally, they showcased the efficiency of the proposal in fusing and finetuning ViT and BERT."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This paper is well-structured.\n- To the best of my knowledge, this is the first work that aims to fuse transformer architectures by aligning their weights.\n- The proposed method is successfully backed by theoretical results."
                },
                "weaknesses": {
                    "value": "- The methodology part is not well-written and lacks some details."
                },
                "questions": {
                    "value": "- Section 2: The model fusion literature has some papers that are slightly off: Tatro et al., 2020; Juneja et al., 2022; Kandpal et al., 2023.\n- Eq. 2: What is $f$? What is its output?\n- Section 4.2.1: How to calculate weighted matrix?\n- The authors should remind the formulation for Attention operation in either Section 3 or Section 4.2.2.\n- Section 4.2.2:\n  - Where do the authors remove the constraints in Section 4.2.2?\n  - It is unclear how to calculate $T_Q$ and $T_K$. Did the authors check the assumption $T_Q = T_K$ in the experiments?\n  - What are $W_i^Q, W_i^K$, and $W_i^V$? Does $i$ indicate the head index?\n  - Additional visualizations may help to demonstrate the method here.\n- Section 4.2.3: What is this sentence for? \u201cFor the concatenation, we notice that the class token is only a small fraction of the full sequence, in other words, for the integrity of the sequence, it is far more important to propagate the TM of the patch embeddings than the one for the class token.\u201d In addition, the class token is more important because it gathers the information from the patch.\n\n\n**Minors**: \n- Eq. 3 should be moved up a paragraph."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4949/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698818893650,
            "cdate": 1698818893650,
            "tmdate": 1699636481407,
            "mdate": 1699636481407,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NlnoF9XiHH",
                "forum": "LjeqMvQpen",
                "replyto": "9tniV8L56p",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4949/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4949/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors 1/1"
                    },
                    "comment": {
                        "value": "*We sincerely appreciate your careful reading of our paper and giving constructive feedback. We are pleased to hear that you find our paper well structured and that you have appreciated the novelty of our work. We have seriously considered your notes and we recognize that some crucial aspects could have lacked clarity.*\n\n*Owing to your insightful feedback we have implemented various modifications in the methodology section (Sec. 4) of the manuscript that, in our opinion, have improved the overall comprehensibility of our work.*\n\n---\n\n### Related Work\n\n> \u201c[\u2026] some papers that are slightly off: Tatro et al., 2020; Juneja et al., 2022; Kandpal et al., 2023.\u201d\n \nWe have fixed these.\n\n---\n\n### Residuals (Sec. 4.2.1)\n\n> \u201cEq. 2: What is $f$? What is its output?\u201d\n\n$\\mathbf{f}_{residual}$ is a vector and stands for the activations coming from the residual branch. \n\n$\\mathbf{f}_{current}$ is also a vector and stands for the activations coming from the current layer l. We added a sentence for clarification in the paper and adjusted Eq. 2 to make it more clear.\n\n> \u201cHow to calculate weighted matrix?\u201d\n\nThe calculation for the weighted matrix is very similar to the weighted scalar. The only difference is that, for weighted matrix, we compute a weighting factor for every incoming residual strand, instead of one common value. We added a sentence for clarification in the paper. \n\n---\n\n### Multi-Head Attention (Sec. 4.2.2)\n\n> \u201c[...] remind the formulation for Attention operation [...]\u201d\n\nDone\n\n> \u201cWhere do the authors remove the constraints in Section 4.2.2?\u201d\n\nWe assume you mean the \u201cequal transportation map\u201d constraint here. Section 4.2.2 serves to explain how we handle the transportation map flow through the self-attention block. At the beginning of the section, we describe our analytical insight on why one should only propagate $\\mathbf{T_V}$. Eq. 4 shows that if we use $\\mathbf{T_Q} = \\mathbf{T_K}$, these permutation matrices cancel. However, this equation is not valid anymore in the case of soft-alignment because we no longer have permutation matrices, so the $\\mathbf{T_Q} = \\mathbf{T_K}$ constraint is no longer needed. We added a sentence for clarification in the paper.\n\n> \u201cIt is unclear how to calculate $\\mathbf{T_K}$ and $\\mathbf{T_Q}$\u201d\n\nIf we do not assume the constraint on the outgoing transportation maps $\\mathbf{T_K} = \\mathbf{T_Q}$ one can directly apply OTFusion to $\\mathbf{W_K}$ and $\\mathbf{W_Q}$ (as for any fully-connected layer). OTFusion yields $\\mathbf{T_K}$ and $\\mathbf{T_Q}$ as the outgoing transportation map respectively. \n\nIn the case of $\\mathbf{T_K} = \\mathbf{T_Q}$, we compute one common ground metric for both $\\mathbf{W^K}$ and $\\mathbf{W^Q}$. In the case of activation based alignment the ground metric is computed from their combined activations. For weight based alignment, the ground metric is computed from the concatenation of $\\mathbf{W^Q}$ and $\\mathbf{W^K}$. From this common ground metric we then compute a common transportation map that we use to align both $\\mathbf{W^K}$ and $\\mathbf{W^Q}$.\n\n> \u201cWhat are $\\mathbf{W^K_i}$, $\\mathbf{W^Q_i}$, $\\mathbf{W^V_i}$, and ? Does i indicate the head index?\u201d\n\nYes, $i$ indicates the head index. $\\mathbf{W^K_i}$, $\\mathbf{W^Q_i}$, and $\\mathbf{W^V_i}$ are the corresponding weight matrices. We added a clarification in the manuscript, and together with the inserted self-attention operation formulation, it should be clear now. \n\n---\n\n### Embeddings (Sec. 4.2.3)\n\n> \u201cWhat is this sentence for? \u201cFor the concatenation, we notice that the class token [...]\u201d In addition, the class token is more important because it gathers the information from the patch.\u201d\n\nWe assume that at the very beginning of the architecture, namely at the embedding stage of the transformer, each patch and token is of similar importance. The intuition for this assumption is that we concatenate only one class token to tens of image patches, and we can therefore assume that it is more important to propagate the transportation map of the image patches instead of the class token. \n\nNote however that we make this assumption **exclusively for the embeddings**, i.e. **not throughout all encoder blocks** where, as you say, the information will indeed be eventually distilled into the class token. \n\n---\n\n*We hope that in light of the modifications of the manuscript and the above clarifications, we have adequately addressed your questions and concerns. In case you might have further doubts or comments, we remain at your disposal.*"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4949/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700261817899,
                "cdate": 1700261817899,
                "tmdate": 1700261817899,
                "mdate": 1700261817899,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GIGaSgORwp",
                "forum": "LjeqMvQpen",
                "replyto": "pqb1YBqUML",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4949/Reviewer_5GYY"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4949/Reviewer_5GYY"
                ],
                "content": {
                    "title": {
                        "value": "Respone to authors' rebuttal"
                    },
                    "comment": {
                        "value": "I thank the authors for patiently reading and responding to all my questions. I found that my concern was adequately addressed. \n\nI would like to maintain my score and incline to an acceptance."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4949/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700684907551,
                "cdate": 1700684907551,
                "tmdate": 1700684907551,
                "mdate": 1700684907551,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "vLVevtPSbU",
            "forum": "LjeqMvQpen",
            "replyto": "LjeqMvQpen",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4949/Reviewer_GM8g"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4949/Reviewer_GM8g"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a systematic approach for fusing two or more transformer-based networks exploiting Optimal Transport technique. The proposed method can generalize to arbitrary architectures for CNNs and Transformers. Extensive experiments involving the fusion and finetuning of Vision Transformers (ViTs) across multiple datasets demonstrate the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This paper is well written.\n- The proposed method shows good generalization across different architectures.\n- The proposed method show strong performance for several benchmark."
                },
                "weaknesses": {
                    "value": "- Most experiments are conducted to compare with Vanilla Fusion. More comparisons with state-of-the-art methods should be included.\n- Most experiments are conducted on CIFAR dataset which is relatively small."
                },
                "questions": {
                    "value": "See the weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4949/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698842398605,
            "cdate": 1698842398605,
            "tmdate": 1699636481303,
            "mdate": 1699636481303,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UVLYlCmrsC",
                "forum": "LjeqMvQpen",
                "replyto": "vLVevtPSbU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4949/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4949/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors 1/2"
                    },
                    "comment": {
                        "value": "*Thank you for your useful feedback. We are pleased to hear that you have found our paper well written, and have appreciated the generalization capabilities of our method across different architectures.*\n\n---\n> \"Most experiments are conducted to compare with Vanilla Fusion. More comparisons with state-of-the-art methods should be included.\"\n\nIn general, we share a similar sentiment as you have expressed. We would have liked to add more comparisons, however, we were forced to compare primarily with Vanilla Fusion (with and without fine-tuning; as well as parent models and ensemble), as **we are not aware \u2014 to the best of our knowledge \u2014 of any other method that can handle the problem of aggregating the parameters of multiple transformer models, in particular, when they are distant in their parameter space**. \n\n\nMore specifically, to better contextualize the state-of-the-art (SotA) in this area, we would like to reiterate here some fundamental differences and limitations of notable related work:\n\n- **OTFusion [1]**\n  - [1] first introduces the concept of Optimal Transport based alignment and fusion. However, its fully layerwise interpretation **lacks generalization capabilities**, and as such it is only applicable to simple architectures such as multi-layer perceptrons, CNNs, and instances of ResNet. \n  - It is **not equipped in any way to align and fuse models with complex information streams** and to fuse transformer-specific components such as multi-head attention layers, layer-normalization, embeddings, or the sequential nature of the data.\n- **Successors of OTFusion**\n  - A plethora of other methods emerged from the ideas and methodology of [1] and explored various adaptations and different applications.\n  -  In particular,  [2] focused on the linear mode connectivity perspective for MLPs, CNNs, and ResNets, while [3] focused on RNNs and LSTM architectures. These methods, too, are **not applicable to transformers** and a direct comparison is consequently out of reach.\n- **Model Soups [4] for transformers**\n  - Recently, [4] focused on weight averaging specifically for transformers, reaching **SotA performance**. \n  - The inherent methodology of [4] actually **relies on VF itself, namely one-to-one averaging** of the parameters. However, there is a subtle but essential difference compared to our application scenario: their parent models originate from the same pre-trained model and therefore the parent models remain **sufficiently close** in the parameter space. This precludes the need to align them and lets them employ a simple averaging strategy while still obtaining a gain in performance. \n  - For this reason, **this method does not apply to our problem where we fuse transformer networks that are potentially much more distant in their parameter spaces** and are diverse in nature.\n\nFurthermore, we would like to stress that the larger point of our paper is to present the **first successful model fusion technique for transformers that aggregates their parameters**, with a focus on modularity, generalization capabilities, and systematic investigation of fusion of the transformers' peculiar components and characteristics (such as multi-head self-attention, layer normalization, sequentiality of the data, to name a few).\n\n---\n\n> \"Most experiments are conducted on CIFAR dataset which is relatively small.\"\n\nWe agree that CIFAR10 is a relatively small dataset, and we opted for this dataset for the experiments and ablations (Tab. 1, Fig. 4 and 5) that required multiple evaluations with different hyperparameters and fusion strategies, and that were prohibitive with large datasets. However, sharing your concern, and aiming to demonstrate the wide applicability and generalization capabilities of our method, we also have **experiments on other larger datasets**. \n\n### ViT\n\nIn particular, we have presented the results of ViT fusion also on:\n\n- **CIFAR100** (Tab. 2 for models of the same size; Tab. 4 for heterogeneous fusion)\n- **TinyImageNet** (Tab. 15 in the Appendix),\n- **ImageNet-1k** (Tab. 3) \n \nwhere we see a consistent gain across all settings. \n\n### BERT\n\nFurthermore, we would like to highlight that not only have we applied our method to the ViT, but **also to BERT for NLP tasks, evaluated on the widely adopted GLUE benchmark** (Tab. 17, in the Appendix), and the results were similar to those for the vision tasks.\n\n---\n\n*All in all, we hope that in light of the above clarifications, we have given a proper answer to your concerns. Should you have any further questions, or comments, we will be more than happy to answer them. Thanks for taking the time.*"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4949/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700261039823,
                "cdate": 1700261039823,
                "tmdate": 1700261039823,
                "mdate": 1700261039823,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "coD9OAyjcY",
                "forum": "LjeqMvQpen",
                "replyto": "gk7wyFMXzc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4949/Reviewer_GM8g"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4949/Reviewer_GM8g"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "Thanks for the detailed response from the authors. Overall I am still not quite sure about the claims that no prior methods can be compared. More experiments on ImageNet are highly encouraged. Therefore I tend to keep my original scores."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4949/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700663059810,
                "cdate": 1700663059810,
                "tmdate": 1700663059810,
                "mdate": 1700663059810,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]