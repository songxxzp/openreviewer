[
    {
        "title": "Gaussian Process-Based Corruption-resilience Forecasting Models"
    },
    {
        "review": {
            "id": "T0Izr1H3HN",
            "forum": "jo36Mzwuvf",
            "replyto": "jo36Mzwuvf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7724/Reviewer_nq8v"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7724/Reviewer_nq8v"
            ],
            "content": {
                "summary": {
                    "value": "The authors claim to have introduced a joint forecast-corrupt-denoise model. The output of the baseline forecasting model, for instance, a machine learning (ML) algorithm, is corrupted by a noise function following a GP distribution. Once data have been corrupted, a denoising model is deployed to reverse the corruption process, while seeking to improve the initial forecast output. Both parameters of the forecast and GP models are jointly learned via the minimization of a compound (forecast + GP-ELBO) loss function.\n\nThe authors also claim that their framework provides better results than the baseline forecasting models. To prove this, they have considered several experimental setups."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "Under Gaussian assumptions, the consideration of a GP-based corruption process may be seen as an interesting idea for dealing with non-i.i.d. noise. Based on [16], an adapted compound loss function is proposed in the (forecasting and GP) parameter estimation. Python codes are provided."
                },
                "weaknesses": {
                    "value": "- In my opinion, the contributions in the paper are minor. The authors have only adapted a collection of well-known approaches related to forecasting, GP and denoising models to establish their joint forecast-corrupt-denoise framework.\n- Contrary to the authors' claims, the numerical results are not convincing. In many cases, the best results are not properly highlighted or are unclear since only 3 random replicates have been considered. For instance, in Table 4 (Traffic 48), InfoDWC (and possibly InfoDI) provides a better result than InfoDG (the proposed method). I suggest considering more replicates when constructing the tables to obtain more consistent results.\n- There is no theoretical evidence (certification) to explain why the joint model should perform better than the baseline algorithms.\n- Mathematical formulas are not defined correctly and, in some cases, are inconsistent. For instance, formulas related to GPs.\n- The paper seems a bit rushed.\n\nI will refer to the part **Questions** for further details."
                },
                "questions": {
                    "value": "- According to Section 2.5, the GP-based corruption model is trained considering the ground truth $Y$ (data to forecast). Does it mean that the proposed joint model can be used only when $Y$ is known? If so, I don't see the point of setting up the problem as a forecast one instead of including $Y$ in the training dataset. If $Y$ is unknown (forecast context), how can the GP parameters be tuned?\n- Can the authors explain the need to include two metrics (MSE and MAE) in the experimental setups? Since both metrics seek to assess the quality of predictions, the results are redundant. Consequently, half of the tables can be omitted.\n- In the experimental setups, only 3 random replicates have been considered. Can the authors confirm that results are indeed consistent (i.e. that similar results are obtained for another triple of random replicates)? If not, they should consider a (statistically rich) number of random replicates (e.g. 10, 20, 30...). \n- Page 8, Section 3.3: can the authors give further details on the choices (e.g. $\\beta_1 = 0.9$, $\\beta = 0.98$, and batch size = 256) considered in the numerical implementations? \n- On page 1, Section 1, the authors suggest that their approach may be adapted to deal with multivariate transient functions but that their focus was on the univariate case. Have they performed numerical examples involving multivariate functions? In any case, can they provide further details on the scalability (in terms of the input dimension) of the framework?\n\n**Other minor remarks**\n- To cite references in brackets, e.g. [n], to avoid confusion when referring to equations (n).\n- Page 1, Section 1 (Time series forecasting task): the definitions of $X$ and $Y$ are not clear. $\\kappa$ is defined as the number of time-series observations prior to $t_0$, i.e. it defines a set of time instants $\\{t_{-\\kappa}, \\ldots, t_{-1}\\}$. Then $X$ needs to be defined as $\\{x_t; \\gamma_t\\}_{t = t_0 - t_{-\\kappa}}^{t_{-1}}$. A similar reasoning must be done for $Y = \\{x_t; \\gamma_t\\}_{t = t_0}^{t_0 + t_\\tau}$. \n- Page 2, related works: LSTM is not defined.\n- Page 3, 2nd paragraph: \"navive\"\n- Page 3, Section 2.1: $\\tilde{X}| X \\sim \\mathcal{N}(\\textbf{0}, \\sigma^2  \\textbf{I})$ ($\\sigma^2$ is missing)\n- Tables are not placed just before or after their citation. For instance, Tables 1, 2 and 3 are placed on pages 4, 6 and 7 (respectively) but they are cited on page 8.\n- Page 4, Eq. (1): the authors have defined $c$ as a function (since it is considered as a GP) but it is treated as a (Gaussian) vector. They need to be consistent with the definitions and distinguish them correctly throughout the paper. \n- Punctuation marks in the equations need to be double-checked throughout the paper.\n- To refer to GP everywhere once the abbreviation is introduced.\n- Page 7, Section 3.1: footnotes 2, 3 and 4 are not provided. If the authors refer to references [2,3,4], they need to be cited properly.\n- Reference 24: incompleted."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7724/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7724/Reviewer_nq8v",
                        "ICLR.cc/2024/Conference/Submission7724/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7724/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697987336142,
            "cdate": 1697987336142,
            "tmdate": 1700646397341,
            "mdate": 1700646397341,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "R21THRHpXZ",
                "forum": "jo36Mzwuvf",
                "replyto": "T0Izr1H3HN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7724/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7724/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your comments. We updated the paper to meet your immediately actionable suggestions (omitted from the rebuttal). Please see below our responses to your questions:\n\nQuestions:\n\n1. Target variable $Y$ is only known at training time, at test time we are predicting the forecast $Y$, without assuming this knowledge.\n\n2. It is common practice to include both MSE and MAE as evaluation metrics in regression tasks. Although, we only included the results for MAE in the Appendix section.\n\n3. The choices of batch size and optimization parameters are based on our preliminary experiments.\n\n4. Univariate forecasting is employed in this context because the datasets featured in the paper are exclusively univariate. In scenarios where the goal is to predict multiple target variables, the objective would be to optimize the MSE loss for multiple targets rather than a single one."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7724/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700401314308,
                "cdate": 1700401314308,
                "tmdate": 1700438480317,
                "mdate": 1700438480317,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ql2TrHkLaJ",
                "forum": "jo36Mzwuvf",
                "replyto": "R21THRHpXZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7724/Reviewer_nq8v"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7724/Reviewer_nq8v"
                ],
                "content": {
                    "comment": {
                        "value": "> 1. Target variable $Y$ is only known at training time, at test time we are predicting the forecast $Y$, without assuming this knowledge.\n\nAccording to the notation used in the paper, $\\gamma$ (is this the target variable $Y$ at training time?) is the target function considering $x$ values up to $t_0$, and $Y$ is the target function after $t_0$ (see Fig. 2). In the definition of the compound loss function, the authors propose to minimize:\n$$\\mathcal{L} = L_{MSE}(\\hat{Y} = Y | X) + \\lambda  L_{ELBO}(Y_C = Y | Y_F).$$\nBoth objectives depend on $Y$, which is assumed to be unknown in forecasting tasks. This contradicts the authors' answer.\n\nThen, the authors need to double-check their notation to make things clearer or to justify the use of $Y$ when learning the covariance parameters (see my initial remark).\n\n> The choices of batch size and optimization parameters are based on our preliminary experiments.\n\nWhat kind of experiments? I believe it is necessary to give further details about their tuning to help potential readers interested in implementing their method.\n\n> Univariate forecasting is employed in this context because the datasets featured in the paper are exclusively univariate. In scenarios where the goal is to predict multiple target variables, the objective would be to optimize the MSE loss for multiple targets rather than a single one.\n\nThe authors have pointed out on Page 1, Section 1, Time series forecasting task:\n\"The target variable can be multivariate with $\\gamma_t \\in \\mathbb{R}^{d_y}$, although we focus on datasets with univariate target variables ($d_y = 1$).\" \nI was expecting to have further details about potential technical issues related to scalability when considering multivariate functions, i.e. $\\gamma_t : \\mathbb{R}^{d_y} \\to \\mathbb{R}$."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7724/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700499682726,
                "cdate": 1700499682726,
                "tmdate": 1700499682726,
                "mdate": 1700499682726,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8HGhF4WSXB",
                "forum": "jo36Mzwuvf",
                "replyto": "T0Izr1H3HN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7724/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7724/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> According to the notation used in the paper, $\\gamma$ (is the target variable $Y$ at training time?) is the target function considering $x$ values up to $t_0$, and $Y$ is the target function after $t_0$ (see Fig.2). In the definition of the compound loss function, the authors propose to minimize: \n $\\mathcal{L} = L_{\\mathtt{MSE}}(\\hat{Y}=Y|X) + \\lambda L_{\\mathtt{ELBO}}(Y_C=Y|Y_F)$\n Both objectives depend on $Y$ which is assumed to be unknown in forecasting tasks. This contradicts the authors' answer.\n\nWe believe there is a misunderstanding here,  $\\gamma$ is the target variable observed up to the cutoff point $t_0$, and $Y$ is the target variable that we are interested to forecast from $t_0$ to $t_0 + \\tau$. Both are available during training but $Y$ is held-out during the test time. When referring to forecasting in SOTA _supervised_ (neural/no-neural) time series forecasting models, $Y$ is always known during training, in fact this is how the parameters of the models are optimized. Only traditional _un-supervised_ time series forecasting models including ARIMA do not rely on $Y$ during training, which is not the case if considering hyper-parameter tuning, therefore variable $Y$ can as well be known during training for unsupervised models.\n\n> What kind of experiments? I believe it is necessary to give further details about their tuning to help potential readers interested in implementing their method. \n\nWe want to state that all hyper-parameters can further be fine-tuned for potential readers interested in implementing. However, the choices made was based on consistent better results for all datasets and all forecasting models, therefore opted as a part in hyper-parameter tuning.\n\n> The authors have pointed out on Page 1, Section 1, Time series forecasting task: \"The target variable can be multivariate with \n$\\gamma_t \\in \\mathbb{R}^ {d_y}$, although we focus on datasets with univariate target variables ( $d_y$ = 1\n).\" I was expecting to have further details about potential technical issues related to scalability when considering multivariate functions, i.e. $\\gamma_t : \\mathbb{R} \\rightarrow \\mathbb{R}$.\n\nWe want to further emphasize that all the variables are zero-mean normalized to prevent any scalability problem, additionally, further normalization techniques are applied when training models to enforce resiliency during training and test. Therefore, the task can be adapted for multi-variate forecasting by optimizing for multiple target variables rather than one."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7724/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700504048443,
                "cdate": 1700504048443,
                "tmdate": 1700591916817,
                "mdate": 1700591916817,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VogBp1F40v",
                "forum": "jo36Mzwuvf",
                "replyto": "8HGhF4WSXB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7724/Reviewer_nq8v"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7724/Reviewer_nq8v"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their reply. Based on my concerns and taking into account those of the other reviewers, I still think that the paper is below the bar. I have decided to increase my score to 3 (rejection, not good enough)."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7724/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700646553396,
                "cdate": 1700646553396,
                "tmdate": 1700646553396,
                "mdate": 1700646553396,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "vGZW5RxDca",
            "forum": "jo36Mzwuvf",
            "replyto": "jo36Mzwuvf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7724/Reviewer_Niyb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7724/Reviewer_Niyb"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a time-series forecasting module which can be applied to a wide-range of existing models. The main idea is denoising: the output of an already available forecasting model is corrupted by noise (the authors show the significance of correlated rather than i.i.d. noise) which is then passed through the same forecasting model (but with different parameters) again. The authors argue such an architecture \"encourages the initial forecasting model to focus on modelling coarse-grained behavior, and a denoising model that corrects the fine-grained details\". The proposed module is experimentally shown to improve the forecasting performance of a number of existing forecasting models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ An interesting idea combining the image denoising ideas with the time-series forecasting\n+ Extensive experimental evaluation including the ablation study"
                },
                "weaknesses": {
                    "value": "- A somewhat confusing presentation of the proposed method (see the questions below)\n- Lack of examples of the model forecasts apart from the cartoon in Fig. 1\n- Minor grammatical errors and repetitions (e.g. almost the same sentence appears twice in Section 2.2.)\n\nI have conflicting opinions about this paper. On the one hand, the experimental results look really good: the proposed denoising module noticeably improves the performance of the exiting models. On the other, I struggle to understand why it is the case, why adding noise improves the performance. In the image generation literature, denoising is often used as a tool for regularising the low-dimensional latent space which is used for sampling new images. However, it is clearly not the case in the context of time-series forecasting. I appreciate that the authors provided some intuition (e.g. see the quote in the Summary section of this review) but I would also appreciate further comments from the authors on this matter. I would be happy to increase my rating if the authors clarify some of these questions."
                },
                "questions": {
                    "value": "- Why does the AutoDWC baseline (i.e. denoising without corruption) works worse than the noise corrupted model? Shouldn't adding independent (from the forecasting model prediction) noise make the task harder for the denoiser and thus deteriorate the performance?\n- The isotropic noise baseline (AutoDI) performs similarly to the AutoDWC. Why do you think it is the case? Also how did you choose the variance of the isotropic noise?\n- What is the input to the denoising model? Only the corrupted forecast, or the corrupted forecast and the historical time-series trajectory (from t_0-k to t_0) used to compute the forecast?\n- Did you try different values of \\lambda in Eq. (2)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7724/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698089061657,
            "cdate": 1698089061657,
            "tmdate": 1699636941826,
            "mdate": 1699636941826,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oaboDxaCFt",
                "forum": "jo36Mzwuvf",
                "replyto": "vGZW5RxDca",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7724/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7724/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for feedback and constructive comments. We updated the paper to meet your immediately actionable suggestions (omitted from the rebuttal). Please see below our responses to your comments:\n\nWeakness:\n\n1. (& question 1) The success of our proposed model stems from the division of labors between the first forecasting stage (for coarse-grained) and the denoising stage (fine-grained predictions), where initial forecasts are blurred locally by utilizing a GP model. In between the parameters of the GP model are trained for best end-to-end performance.\n\n2. To meet your request, we included figures with actual forecasts from our datasets in the paper and the provided link below:\n\n[Actual Forecasts](https://anonymous.4open.science/r/Corruption-resilient-Forecasting-Models-15E8/Actual-forecasts-including-AutoDWC.pdf)\n\nQuestions:\n\n2. The reason stems from the ineffectiveness of removing uncorrelated isotropic noise which is not the typical error mode in SOTA time series forecasting models. Therefore, the inability of AutoDI to outperform AutoDWC is underscored by such limitation. We adjusted the variance of the isotropic noise by fine-tuning the scaled factor $\\sigma \\in [0, 1]$. \n\n3. The input to the denoising model is solely the corrupted (blurred) forecast, as our denoising model is designed to \ndenoise the corrupted/blurred forecast to align with the ground-truth.\n\n4. in our preliminary analysis setting different values for $\\lambda$ did not affect the results drastically, however we will include more experiments of adjusting the variable $\\lambda$ in Appendix of the camera-ready paper."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7724/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700358502669,
                "cdate": 1700358502669,
                "tmdate": 1700694146243,
                "mdate": 1700694146243,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cZWTxzTH2q",
                "forum": "jo36Mzwuvf",
                "replyto": "vGZW5RxDca",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7724/Reviewer_Niyb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7724/Reviewer_Niyb"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your reply and the additional figure with the model forecasts.\n\nI am still somewhat confused why adding independent noise helps in comparison to applying the same model twice (i.e. AutoDWC baseline) and for now I decided not to change my score."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7724/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700488876585,
                "cdate": 1700488876585,
                "tmdate": 1700488904725,
                "mdate": 1700488904725,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "S7lRWp3t3v",
                "forum": "jo36Mzwuvf",
                "replyto": "vGZW5RxDca",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7724/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7724/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. \n\nLet us further explain the intuition of our framework. By introducing the GP competent, we envision for a division of responsibilities, where the initial forecasting model predicts the overall (coarse-grained) behavior of the target variable, GP is integrated to introduce local blurring to the initial forecasts. In response, the denoising model focuses on removing the introduced blur by predicting the fine-grained details of the ground-truth. \n\nThe inclusion of the GP component establishes a clear division of tasks between forecasting and denoising. While in the absence of the GP component, the distinction between coarse-grained and fine-grained details becomes less pronounced, potentially causing the denoiser to deprioritize the accurate prediction of fine-grained details. We would also like to emphasize that the parameters of the GP/corruption model are fine-tuned to obtain the best performance.  \n\nWe are also open to providing forecast plots of AutoDWC in comparison to our AutoDG(ours) to further illustrate why the division of tasks through GP integration helps to achieve enhanced performance."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7724/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700496773122,
                "cdate": 1700496773122,
                "tmdate": 1700497929078,
                "mdate": 1700497929078,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1a2FISTVsO",
                "forum": "jo36Mzwuvf",
                "replyto": "r1OPimUquP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7724/Reviewer_Niyb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7724/Reviewer_Niyb"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for further clarifications and additional figure. I still can't say I have now a good intuition of the method, but it is not your fault, I guess I am just not familiar enough with such approaches. I appreciate the effort you put into the rebuttal discussions!\n\nTo me it is still a borderline submission, so I keep my score as it is."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7724/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700695953378,
                "cdate": 1700695953378,
                "tmdate": 1700695953378,
                "mdate": 1700695953378,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WtBobrM6bC",
            "forum": "jo36Mzwuvf",
            "replyto": "jo36Mzwuvf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7724/Reviewer_zFSw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7724/Reviewer_zFSw"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates the topic of noise corruption in modeling time-series data for forecasting. The authors propose that the noise/error in the time series data can be attributed to two sources, including a temporally correlated source, and an independent noise. From that the authors claim the current methods cannot well recover the underlying signals/true observations and thereby cannot carry out accurate forecast. To this end, the authors proposed a joint-corrupt-denoise model to capture the characteristics of signals from both sources identified. The framework is then tested on a wide range of datasets from which its efficacy is demonstrated."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The motivation of the problem is laid out very clearly, with an illustration well explaining the origin of the issue and the shortcomings of the current methods.\n2. The proposed metholodgy is explained very clearly and a thorough and comprehensive numerical study is carried out to evaluate the method."
                },
                "weaknesses": {
                    "value": "1. I have concerns that the proposed framework including the temporally-correlated term utilizing Gaussian process is entirely new. I think there might have been other works out there proposing a fairly similar idea called \"calibration modeling\". To elaborate a bit more here, the framework proposed carry some common components to a typical statistical calibration model as follows:\n$y(t) = x(t) + \\delta(t) + \\epsilon(t),$\nwhere $x(t)$ is the true signal, $\\delta(t)$ is modeled by a Gaussian process, and $\\epsilon(t)$ modeled by a Gaussian noise.\nWith an appropriate selection of the prior distributions for the parameters and hyperparameters, this can be tackled by a Bayesian approach. I will elaborate further in the Q section.\n2. The presentation of the manuscript can be further improved. For example, in the abstract, I find the mentioning of \"using more training data\" and \"image generation\" not necessarily closely related to the point I think the authors were trying to emphasize. Additionally, in Figure 2, $X_1$ and $X_2$ are defined as covariates, which seem to conflict with the previos reference to $X$ as the observations."
                },
                "questions": {
                    "value": "Let me further expand on the proposed framework itself. I believe the joint framework is quite novel and has clear potential in improving time series forecasting, as demonstrated by the authors very diligently. However, I do wonder whether the authors would be open to compare and evaluate the framework to the calibration model that I mentioned above.\nMy personal belief is they share some commonality between them in how they model the time-correlated noise/signal, but it seems the two objectives functions are still quite different. So I believe it would be interesting to see how they compare both in their model structure, and their performance on a few datasets in practice."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7724/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7724/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7724/Reviewer_zFSw"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7724/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698552624288,
            "cdate": 1698552624288,
            "tmdate": 1699636941701,
            "mdate": 1699636941701,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "d1z8MqH4JK",
                "forum": "jo36Mzwuvf",
                "replyto": "WtBobrM6bC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7724/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7724/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for feedback and constructive comments. We updated the paper to meet your immediately actionable suggestions (omitted from the rebuttal). Please see below our responses to your comments:\n\nWeakness:\n\n1. (& question 1) Upon your request, we included the results of the calibration modeling with GPs denoted as CMGP in the revised version of our paper and at the link below: \n\n[Adding CMGP](https://anonymous.4open.science/r/Corruption-resilient-Forecasting-Models-15E8/Additional-baselines.pdf)\n\n2. When referring to observations, we include variables, including the target and co-variates, that have been observed in the past. In Figure 2, we provide an example where observations $X$ consist of of $X = \\{X_1, X_2, \\gamma\\}$."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7724/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700357794333,
                "cdate": 1700357794333,
                "tmdate": 1700399681182,
                "mdate": 1700399681182,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8YS17O9oTl",
                "forum": "jo36Mzwuvf",
                "replyto": "d1z8MqH4JK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7724/Reviewer_zFSw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7724/Reviewer_zFSw"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for the added baseline using the Gaussian process."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7724/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700418703745,
                "cdate": 1700418703745,
                "tmdate": 1700418703745,
                "mdate": 1700418703745,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "edmAiLpQAM",
            "forum": "jo36Mzwuvf",
            "replyto": "jo36Mzwuvf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7724/Reviewer_Hpfd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7724/Reviewer_Hpfd"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a novel joint forecast-corrupt-denoise model that consists of the forecasting module and the corrupt-denoising module. The forecasting model focuses on accurately predicting coarse-grained behavior. The corrupt-denoising model focuses on capturing fine-grained behavior, with a GP model employed to enforce the smoothness and co-relationship in added noise. \n\nEmpirical evaluations show the flexibility of the proposed framework in incorporating popular time-series forecasting models such as Informer and Autoformer and exhibit outperformance than popular time-series forecasting models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The idea and framework introduced by the paper are natural and easy to follow. \n\n2. The proposed framework adds a corrupt-denoising model as a tail of a forecasting model, which shows flexibility to incorporate with existing SOTA methods, and may further enhance their performance\n\n3. The empirical evaluations show benefits of the proposed framework than single forecasting models alone. In addition, evaluations also illustrate the effectiveness of denoising GP corrupted time series than isotropic Gaussian noised time series"
                },
                "weaknesses": {
                    "value": "1.  Despite that the soundness of the methodology makes sense and is easy to follow, there are points remaining unclear more interpretations should be made. For instance, while it is understandable that adding noise through a Gaussian process can result in smoother and more structured noise patterns compared to isotropic Gaussian noise, it is still not that obvious which is worth formal illustrations from either intuitions or equations to explain the difference/benefits between adding isotropic Gaussian noise. \n\n2. The effectiveness of the proposed framework is marginal when considering the framework doubles the parameter of a single forecasting model, e.g., the denoising model follows the forecasting model's architecture. In this case, it is also worthwhile showing that the benefits of the proposed framework are indeed by its mechanism, instead of overparameterization for better capability with a stack of Informers for example.\n\n3. The ablation study is not well-exhibited. Despite there being a simple ablation study for the synthetic data in the Introduction, we expect to see some real-world examples or case studies to prove the statement 'The forecasting model focuses on accurately predicting coarse-grained behavior. The corrupt-denoising model focuses on capturing fine-grained behavior, with a GP model employed to enforce the smoothness and co-relationship in added noise', rather than simply the synthetic data.\n\n4. The presentation needs to be improved. For example, there are repeated results in Table 1 and Table 2, which should be merged. And if MSE is the only metric used for evaluation, why is it necessary to assign it a column in all tables"
                },
                "questions": {
                    "value": "1. For results on DLinear, DeepAR, and ARIMA, why the reported variances are zeros?\n\n2. Is the reported error, or ground truth normalized?\n\n3. Does adding the denoising part to the forecasting model help or harm the convergence?\n\n4. Does adding the denoising part to the forecasting model tend to lead to overfitting than the direct output of the forecasting model, say fine-grained behavior found by the denoising model overfits the ground truth when the ground truth is smooth? It might be measurable by case study or by using correlation metrics."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7724/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698695495863,
            "cdate": 1698695495863,
            "tmdate": 1699636941565,
            "mdate": 1699636941565,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FJcGk5a4xu",
                "forum": "jo36Mzwuvf",
                "replyto": "edmAiLpQAM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7724/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7724/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for feedback and constructive comments. We updated the paper to meet your immediately actionable suggestions (omitted from the rebuttal). Please see below our responses to your comments:\n\nWeakness:\n\n1. The success of our proposed model stems from the division of labors between the first forecasting stage (for coarse-grained) and the denoising stage (fine-grained predictions), where initial forecasts are blurred locally by utilizing a GP model. In between the parameters of the GP model are trained for best end-to-end performance. \n\n2. We include the results of forecasting models with higher number of layers (parameters) in the Appendix and the following link, to show that higher number of parameters does not necessarily result in a better performance due to potential overfitting. \n\n[Results of standalone forecasting models with higher number of layers (parameters)](https://anonymous.4open.science/r/Corruption-resilient-Forecasting-Models-15E8/Additional-results-higher-number-parameters.pdf)\n\n3. To meet your request we included the actual forecasts in the paper's Appendix and the provided link. \n\n[Actual Forecasts](https://anonymous.4open.science/r/Corruption-resilient-Forecasting-Models-15E8/Actual-forecasts-including-AutoDWC.pdf)\n\nQuestions: \n1. The standard errors for the mentioned models are less than 0.001 and we round the errors to three digits.\n\n2. Ground-truth is zero mean normalized, and reported errors are further normalized using the mean of the absolute values of the ground truth data as a scaling factor.\n\n3. (& 4) Our forecast-corrupt-denoise framework with GPs contributes to enhanced convergence and showcases a lack of overfitting as the performance on the hold-out test still improves. This is illustrated in the figures provided in the following link and the paper's Appendix section. \n\n[Convergence during training and validation](https://anonymous.4open.science/r/Corruption-resilient-Forecasting-Models-15E8/Convergence.pdf)"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7724/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700356217980,
                "cdate": 1700356217980,
                "tmdate": 1700694110418,
                "mdate": 1700694110418,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WCs7L5wEy0",
                "forum": "jo36Mzwuvf",
                "replyto": "FJcGk5a4xu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7724/Reviewer_Hpfd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7724/Reviewer_Hpfd"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors' comment"
                    },
                    "comment": {
                        "value": "I thank the author's response and additional experiments to my questions. \n\nFrom my personal perspective, I believe the idea of adding noise and denoising processes in time-series forecasting is a novel and interesting idea. However, as shown in the empirical part, the improvement of the proposed method is kind of marginal, for most cases, the improvement between with and without DG is less than 10% lower in testing error. My worry is that when incorporating newer models such as Fedformer or PatchTST, the improvement from DG will become even marginal. (If it constantly shows an ~10% improvement, then I'd believe the proposed method is fairly good enough)\n\nOne more suggestion is for the description of dataset and baseline model, the authors do not necessarily use paragraph headers for each. Instead, the author could use an Appendix for the detailed explanation, and save the space for more experiment results."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7724/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700512357490,
                "cdate": 1700512357490,
                "tmdate": 1700512357490,
                "mdate": 1700512357490,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "o8Negkouvu",
                "forum": "jo36Mzwuvf",
                "replyto": "rn2DACeS0w",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7724/Reviewer_Hpfd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7724/Reviewer_Hpfd"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors' comment"
                    },
                    "comment": {
                        "value": "I thank the reviewer again for the additional results. The results look good to me."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7724/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700677664191,
                "cdate": 1700677664191,
                "tmdate": 1700677664191,
                "mdate": 1700677664191,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]