[
    {
        "title": "VideoGLUE: Video General Understanding Evaluation of Foundation Models"
    },
    {
        "review": {
            "id": "gBUx5NhmWp",
            "forum": "q20O1J9ujh",
            "replyto": "q20O1J9ujh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6290/Reviewer_u4pP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6290/Reviewer_u4pP"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces VideoGLUE, a comprehensive benchmark designed to evaluate video understanding capabilities of foundation models (FMs). VideoGLUE consists of three tasks (video classification, temporal localization, and spatio-temporal localization), eight datasets (K400, SSv2, D48, MiT, Charades, ANet, AVA, and AVA-K), and four adaptation methods (full finetuning, frozen backbone Eval., MLAP, and PETL w/ low-rank adapters). The authors also propose a scalar metric, VideoGLUE Score (VGS), that reflects the efficacy and efficiency of FMs. Comparative evaluations between image-native and video-native FMs on VideoGLUE reveal that 1) the existing FMs still lag behind task-specific models; 2) video-native FMs demonstrate superior temporal modeling capability than image-native counterparts; 3) video-native FMs outperform image-native FMs under light-weight adaptations, but not under full finetuning."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tThe paper is well written and clearly motivated.\n2.\tThe paper evaluates both image-native and video-native FMs on various video datasets and the unified evaluation protocols.\n3.\tThe paper introduces various adaptation methods for both image and video FMs and the subsequent analysis underscores the critical role of adaptation in evaluation."
                },
                "weaknesses": {
                    "value": "1.\tVideoGLUE Score: (As the authors mentioned in Limitation Section) The proposed VideoGLUE score focuses solely on the computation-performance tradeoff, overlooking other valuable aspects, e.g., memory usage, model size, sample efficiency, model architecture difference. Given the significant influence of datasets and evaluation protocols on research directions, there is a concern that the current benchmark and scoring system might lead practitioners to overlook these crucial aspects of comprehensive video AI development..\n2.\tSelection of tasks: The benchmark is limted to three tasks, which may not fully capture the generalizability of FMs in video understanding. The inclusion of additional tasks, e.g., repetition counting, (long-term) action anticipation, human-object interaction, and so on,  could provide a more comprehensive evaluation.\n3.\tSelection of motion-focused benchmarks: Athough SSv2 is a widely acknowledged motion-focused dataset, existing literature [a, b] indicates that strong performance is achievable without motion modeling on SSv2 due to strong correlation between objects and action classes in SSv2. Utilizing Something-Else [a]as an alternative could offer a purer evaluation of motion modeling capability by breaking those spurious correlation. Furthermore, FineGym [c] provides hierarchical action labels allowing for a more granular assessment of a model modeling capability.\n\n[a] Materzynska et al., Something-Else: Compositional Action Recognition with Spatial-Temporal Interaction Networks, CVPR, 2020. \\\n[b] Sun et al., Masked Motion Encoding for Self-Supervised Video Representation Learning, CVPR, 2023. \\\n[c] Shao et al., FineGym: A Hierarchical Video Dataset for Finegrained Action Understanding, CVPR, 2020."
                },
                "questions": {
                    "value": "1. Is there any reason why image-native FM CoCa outperforms other FMs on D48, which mitigates static contextual biases but focuses on motion?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6290/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698686471218,
            "cdate": 1698686471218,
            "tmdate": 1699636690712,
            "mdate": 1699636690712,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "T7aT4WsJtc",
                "forum": "q20O1J9ujh",
                "replyto": "gBUx5NhmWp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6290/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6290/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review"
                    },
                    "comment": {
                        "value": "We hope our response addresses your concerns. Should you have additional questions, we are readily available for further discussion.\n\n* Re W1 \"The proposed VideoGLUE score focuses solely on the computation-performance tradeoff.\"\n\nAs reviewer 1CiA commented, \u201cThe suggested VideoGLUE score can be an interesting **initial** way to start a comparison and discussion of the strength of foundation models\u201d. We acknowledge that it is only an initial attempt to score FMs by their video understanding capabilities, and it is not comprehensive as we discussed the limitations (more details in Appendix A). \n\nNonetheless, it captures the essence and can help the design of new metrics in future work. In particular, we design VideoGLUE score (VGS) as the model performance weighted by the number of tunable parameters, which are arguably the two most important factors when evaluating an FM [1, 2]. \n\n[1] Bommasani, R., Hudson, D.A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M.S., Bohg, J., Bosselut, A., Brunskill, E. and Brynjolfsson, E., 2021. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258.\n\n[2] Li, C., Liu, H., Li, L., Zhang, P., Aneja, J., Yang, J., Jin, P., Hu, H., Liu, Z., Lee, Y.J. and Gao, J., 2022. Elevater: A benchmark and toolkit for evaluating language-augmented visual models. Advances in Neural Information Processing Systems, 35, pp.9287-9301.\n\n* Re W2 \"Selection of tasks are limited to three tasks.\"\n\n We appreciate the reviewer\u2019s suggestion, and we have to leave them to future work from the community. As you may notice, completing the evaluation on one set of metrics in our paper requires more than one hundred experiments (six foundation models, four adaptation methods over eight datasets), not to mention the hyperparameter tuning and other explorations. Hence, we indeed made hard decisions about which to include in VideoGLUE. \n\nWe are working on a new video task/dataset. Should bandwidths permit, we will work on the second version of VideoGLUE by including the new task and investigating the tasks the reviewer suggested. \n\n* Re W3 \"Selection of motion-focused benchmarks.\"\n\nWe did consider Something-Else against SSv2 and FineGym against Diving and decided to go with SSv2 and Diving because they are well received by the community. We can swap them in the future if their drawbacks become an obstacle for fair evaluations. \n\n* Re Q1 \"Is there any reason why image-native FM CoCa outperforms other FMs on D48\"\n\nWe note that although image-native CoCa and CLIP perform well on D48 in the end-to-end fine-tuning setting and the adapter setting, respectively, video-native FMs perform better in the frozen feature setting and the multi-layer attention pooler setting. Because D48 is small, it is unlikely to provide sufficient supervision for end-to-end fine-tuning the spatiotemporal weights in the video-native FMs. However, it still signifies the video-native FMs\u2019 motion understanding capabilities when the backbones are frozen."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6290/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700163691410,
                "cdate": 1700163691410,
                "tmdate": 1700163691410,
                "mdate": 1700163691410,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vJfm224Szt",
                "forum": "q20O1J9ujh",
                "replyto": "gBUx5NhmWp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6290/Reviewer_u4pP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6290/Reviewer_u4pP"
                ],
                "content": {
                    "title": {
                        "value": "Response to the authors"
                    },
                    "comment": {
                        "value": "> As reviewer 1CiA commented, \u201cThe suggested VideoGLUE score can be an interesting\u00a0initial\u00a0way to start a comparison and discussion of the strength of foundation models\u201d. We acknowledge that it is only an initial attempt to score FMs by their video understanding capabilities, and it is not comprehensive as we discussed the limitations (more details in Appendix A).\nNonetheless, it captures the essence and can help the design of new metrics in future work. In particular, we design VideoGLUE score (VGS) as the model performance weighted by the number of tunable parameters, which are arguably the two most important factors when evaluating an FM [1, 2].\n\nI appreciate the efforts put into developing the VideoGLUE score as an initial step in evaluating video foundation models (FMs) as the Reviewer1CiA mentioned. However, IMHO, VideoGLUE requires further refinement to establish a comprehensive and clear benchmark, **especially considering its role not just as a method but as a benchmark and evaluation protocol**.\nFor example, GLUE(NeurIPS\u201919) and SuperGLUE(ICLR\u201919), which is an improved version of GLUE, were published in the same year. The significant difference in their citations underscores the importance of a benchmark\u2019s initial comprehensiveness and reception. As the first of its kind, VideoGLUE must ensure a robust and comprehensive foundation to garner significant attention and use.\n\n> We appreciate the reviewer\u2019s suggestion, and we have to leave them to future work from the community. As you may notice, completing the evaluation on one set of metrics in our paper requires more than one hundred experiments (six foundation models, four adaptation methods over eight datasets), not to mention the hyperparameter tuning and other explorations. Hence, we indeed made hard decisions about which to include in VideoGLUE.\n\nConcerning the extensive requirements of video experiments, I understand the constraints of computational resources. However, this limitation raises concerns about the benchmark\u2019s ability to assure the generalizability of video FMs. If not, the authors at least should\u2019ve justified how the three chosen tasks can guarantee the measure of generalizability of video FMs and why other video tasks, e.g., repetition counting, or fine-grained motion understanding, are excluded.\n\n> We did consider Something-Else against SSv2 and FineGym against Diving and decided to go with SSv2 and Diving because they are well received by the community. We can swap them in the future if their drawbacks become an obstacle for fair evaluations.\n\nI respectfully disagree with the author's statement, especially, \u201cWe can swap them in the future if their drawbacks become an obstacle for fair evaluations.\u201d at two points. First, the problem of spurious correlation of SSv2 has already been raised a couple of years ago, and thus, VideoGLUE should reflect current understanding and criticisms. Second, changing datasets in benchmarks is not trivial, as practitioners developing video models might rely on existing VideoGLUE scores for benchmarking, making any alterations challenging for fair comparisons.\n\nIn summary, while acknowledging VideoGLUE\u2019s initial contribution, I urge consideration of its comprehensiveness, justification of dataset selection, and the implications of changing benchmarks in the future for a fair and effective evaluation of video FMs. I thus keep my rating."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6290/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700722280970,
                "cdate": 1700722280970,
                "tmdate": 1700722686117,
                "mdate": 1700722686117,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "AFWBtTR4tE",
            "forum": "q20O1J9ujh",
            "replyto": "q20O1J9ujh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6290/Reviewer_oktk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6290/Reviewer_oktk"
            ],
            "content": {
                "summary": {
                    "value": "This paper assesses the video understanding capabilities of current foundation models using a novel experimental protocol that spans multiple tasks, datasets, and adaptation methods. Additionally, it introduces a scalar VideoGLUE score to gauge both the efficacy and efficiency of a foundation model. Drawing from the experimental outcomes, the paper unveils several interesting findings."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tThe motivation is both clear and justified. There is a pressing need in the community to establish a benchmark for assessing the video understanding capabilities of foundation models.\n2.\tThe introduced VideoGLUE benchmark evaluates foundation models across various dimensions such as tasks, datasets, and adaptation methods.\n3.\tThis paper highlights three interesting findings into the video understanding capabilities of current foundation models.\n4.\tThe paper is well written and easy to follow."
                },
                "weaknesses": {
                    "value": "1.\tThis paper analyzes six foundational models, varying in size, pre-training data, and objectives. The diversity of these settings compromises the comparability of the experimental results, rendering the conclusions less reliable.\n2.\tThis paper introduces a scalar VideoGLUE score to assess FM's efficacy and efficiency by averaging performance scores across four adaptation methods. Yet, the rationale behind the metric's design appears arbitrary. It's unclear why this particular weighted score, derived from the specified datasets and adaptation methods, is indicative of FMs' video understanding capabilities."
                },
                "questions": {
                    "value": "1.\tIn Table 3, the task-specialized model UniformerV2 registers a score of 42.7 on the MIT dataset, which trails the 43.6 scored by CoCa. This observation contradicts the assertion that \"All six FMs underperform task-specialized models on video tasks.\"\n2.\tIn Figure 3, CoCa achieves an average score of approximately 36 under the MLAP adaptation. However, Table 5 lists the average score for CoCa as 45.9. Why aren't they consistent?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6290/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6290/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6290/Reviewer_oktk"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6290/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698724686247,
            "cdate": 1698724686247,
            "tmdate": 1699636690545,
            "mdate": 1699636690545,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "V86b78HKFj",
                "forum": "q20O1J9ujh",
                "replyto": "AFWBtTR4tE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6290/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6290/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your comments!"
                    },
                    "comment": {
                        "value": "We hope our response addresses your concerns. Should you have additional questions, we are readily available for further discussion.\n\n* Re W1 \"The diversity of these settings compromises the comparability of the experimental results, rendering the conclusions less reliable.\"\n\nWe acknowledge the reviewer's comment. In our experiment design, we try to compare FMs fairly, so that we strictly control our comparison in the main paper to the same model size, ViT-B, and resolutions and other hyperparameters like augmentation strategy. Pre-training data and objectives are not controllable and we note that it is also happening to the evaluation of large language models. While we cannot control the FMs' diverse pretraining settings, this work tries to standardize the evaluation so that we can examine \"which FMs are good at solving video tasks, what makes them better than others in the video domain, and how to best adapt them to video understanding\" (quoted from Section 3.1 in the paper).\n\n* Re W2 \"design of VideoGLUE score\":\n\nAs reviewer 1CiA commented, \u201cThe suggested VideoGLUE score can be an interesting **initial** way to start a comparison and discussion of the strength of foundation models\u201d. We acknowledge that it is only an initial attempt to score FMs by their video understanding capabilities, and it is not comprehensive as we discussed the limitations (more details in Appendix A). Nonetheless, it captures the essence and can help the design of new metrics in future work. In particular, we design VideoGLUE score (VGS) as the model performance weighted by the number of tunable parameters, which are arguably the two most important factors when evaluating an FM [1, 2]. \n\n[1] Bommasani, R., Hudson, D.A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M.S., Bohg, J., Bosselut, A., Brunskill, E. and Brynjolfsson, E., 2021. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258.\n\n[2] Li, C., Liu, H., Li, L., Zhang, P., Aneja, J., Yang, J., Jin, P., Hu, H., Liu, Z., Lee, Y.J. and Gao, J., 2022. Elevater: A benchmark and toolkit for evaluating language-augmented visual models. Advances in Neural Information Processing Systems, 35, pp.9287-9301.\n\n* Re Q1 \"score on the MIT dataset\":\n\nThanks for pointing this out. We will modify the statement to \u201cFoundation models underperform task-specialized models on video tasks in general \u2014 MIT is the only exception.\u201d\n\n* Re Q2 \" Why Figure 3 and Table 5 CoCa scores are consistent?\"\n\n Thank you for catching this! We made a typo in Table 5. Figure 3 is the correct one. We have updated the manuscript accordingly."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6290/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700163360664,
                "cdate": 1700163360664,
                "tmdate": 1700163360664,
                "mdate": 1700163360664,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7rDBLrxa17",
                "forum": "q20O1J9ujh",
                "replyto": "V86b78HKFj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6290/Reviewer_oktk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6290/Reviewer_oktk"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. Despite your clarifications, I remain convinced that this submission is not yet suitable for publication as a conference paper at ICLR. As mentioned in my initial review, the necessity for a comprehensive and fair comparison of video foundation models is imperative for thorough evaluation. The present one, in its current form, appears more apt for inclusion in a technical report, which could be valuable for users in selecting appropriate models. Additionally, as pointed by Reviewer u4pP, the current VideoGLUE score lacks comprehensiveness, potentially leading to misconceptions in the development of video AI technology."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6290/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700558751121,
                "cdate": 1700558751121,
                "tmdate": 1700558751121,
                "mdate": 1700558751121,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "VdLKsNKdxs",
            "forum": "q20O1J9ujh",
            "replyto": "q20O1J9ujh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6290/Reviewer_geS9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6290/Reviewer_geS9"
            ],
            "content": {
                "summary": {
                    "value": "This work aims to assess the video understanding capabilities of existing foundation models. The authors have employed a meticulous experimental protocol that encompasses three hallmark tasks: action recognition, temporal localization, and spatiotemporal localization. These tasks are evaluated across eight datasets that are well-regarded by the research community. The initial context suggests that the paper's focus is on the comprehensive evaluation of video understanding in foundational models using a variety of tasks and datasets.\u200b"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Evaluating Foundation Models (FMs): The paper emphasizes the complexity involved in evaluating FMs, particularly because they are designed as \"generalists\" that learn meta-knowledge across tasks. This highlights the need for a standardized evaluation procedure, which this paper aims to provide.\n\n2. VideoGLUE Protocol: The proposed evaluation protocol provides a structured approach to evaluate FMs on video understanding, encompassing various tasks, datasets, and model adaptation methods. This could serve as a benchmark for future research."
                },
                "weaknesses": {
                    "value": "1. Different datasets emphasize varied aspects in video tasks; for instance, SSV2 focuses on motion, while Kinetics is more context-centric. How does VideoGLUE address the differences among these diverse datasets?\n\n2. While the study delves into transformer-based Foundation Models (FMs), is there any comprehensive analysis or comparison involving 3D-CNN or even 2D-CNN based FMs?"
                },
                "questions": {
                    "value": "See Weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6290/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6290/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6290/Reviewer_geS9"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6290/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698751826813,
            "cdate": 1698751826813,
            "tmdate": 1699636690405,
            "mdate": 1699636690405,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HfjWFWxpX4",
                "forum": "q20O1J9ujh",
                "replyto": "VdLKsNKdxs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6290/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6290/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your comments"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the detailed comments and valuable suggestions. We hope our response addresses your concerns. Should you have additional questions, we are readily available for further discussion."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6290/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700162876853,
                "cdate": 1700162876853,
                "tmdate": 1700162876853,
                "mdate": 1700162876853,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cJdRStI10v",
                "forum": "q20O1J9ujh",
                "replyto": "VdLKsNKdxs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6290/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6290/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer geS9's comments"
                    },
                    "comment": {
                        "value": "* Re \"How does VideoGLUE address the differences among these diverse datasets?\"\n\nWe choose a diverse set of video datasets/tasks on purpose. These diverse video datasets/tasks help us probe into FMs\u2019 video understanding capabilities from different perspectives. For example, SSv2 and Kinetics emphasize motion and appearance, respectively. Video classification and temporal action localization test an FM\u2019s holistic understanding versus temporal reasoning. \n\n* Re \"is there any comprehensive analysis or comparison involving 3D-CNN or even 2D-CNN based FMs?\"\n\nWe followed the following definition of foundation models: \u201cany model that is trained on broad data that can be adapted (e.g., finetuned) to a wide range of downstream tasks\u201d [1] to choose the most representative image- and video-based foundation models. Interestingly, the resultant FMs happen to be all transformer-based, potentially because the model developers chose the transformer architecture considering its better scalability than CNNs [2, 3].\n\n[1] Bommasani, R., Hudson, D.A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M.S., Bohg, J., Bosselut, A., Brunskill, E. and Brynjolfsson, E., 2021. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258.\n\n[2] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S. and Uszkoreit, J., 2020. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.\n\n[3] https://twitter.com/giffmana/status/1717995379034071483"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6290/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700163020952,
                "cdate": 1700163020952,
                "tmdate": 1700163020952,
                "mdate": 1700163020952,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "AqRzWJEOJG",
            "forum": "q20O1J9ujh",
            "replyto": "q20O1J9ujh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6290/Reviewer_1CiA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6290/Reviewer_1CiA"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies video understanding capabilities of several existing foundation models. For this purpose, these models are evaluated on three primary video tasks: action recognition, temporal localization, and spatiotemporal localization. There are also three main findings from this study. First, task-specialized models still outperform foundation models; second, video-native foundation models are better than image-native models for complex and motion-rich videos; and third, light adaptations are enough for video-native models while full fine-tuning is better for image-native models. In addition, there is also an attempt to suggest one metric that resembles the strength of the foundation model."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1) The paper has an excellent presentation with clear discussion and motivation for the work done. The visualizations are very nice and they further facilitate the text of paper and experiments. \n2) Evaluating six foundation models on three main video tasks and eight datasets is a valuable contribution to the research community. It is even more valuable because different adaptions are also considered (fine-tuning, low-rank adapter, and others). Experiments are performed thoroughly with good analysis and discussion. The obtained conclusions can be important to the next iteration of the development of foundation models. \n3) The suggested VideoGLUE score can be an interesting initial way to start a comparison and discussion of the strength of foundation models. It seems that the scores for the studied models are quite close to each other and still far from the perfect one. So we can think that we are only starting scretching the cover of this area."
                },
                "weaknesses": {
                    "value": "1) Despite the contribution of suggesting a VideoGLUE score, there are no novel models, modifications, or datasets presented in the paper. In my opinion, it would be very beneficial to develop at least one of them as an additional contribution to make the strongest possible paper. \n2) The list of studied foundation models is not as comprehensive as potentially can be. I understand, that not models are publicly available but it would be very interesting and important to make even stronger conclusions by including some of video-based models developed on top of CLIP paradigm. Some of the examples are: \"VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text Understanding\", Xu et al., EMNLP 2021; \"Expanding Language-Image Pretrained Models for General Video Recognition\", Bolin et al., ECCV 2022. \n3) Also, there is a rapid development of adaptation techniques that could be also very nice to be included in the analysis. Some of the recent examples are: \"AIM: Adapting Image Models for Efficient Video Action Recognition\", Yang et al., ICLR 2023; \"Frozen CLIP Models are Efficient Video Learners\", Lin et al., ECCV 2022; \"ST-Adapter: Parameter-Efficient Image-to-Video Transfer Learning\", Pan et al., NeurIPS 2022. The listed papers are mostly adapters from images to videos, but it would be still great to have them in the analysis for some of the applicable foundation models."
                },
                "questions": {
                    "value": "No other questions except those listed in the \"Weaknesses\" section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6290/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698785743218,
            "cdate": 1698785743218,
            "tmdate": 1699636690303,
            "mdate": 1699636690303,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oh51vYCQcS",
                "forum": "q20O1J9ujh",
                "replyto": "AqRzWJEOJG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6290/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6290/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer 1CiA's comments"
                    },
                    "comment": {
                        "value": "We thank reviewer for the detailed comments and valuable suggestions. We hope our response addresses your concerns. Should you have additional questions, we are readily available for further discussion.\n\n* Re W1 \"no novel models, modifications, or datasets presented in this paper\": \n\nWe appreciate the reviewer for this invaluable suggestion as we are actively working on a new video task/dataset. We cannot include it here because of the high workload involved. As one may notice, completing this work's evaluation on one set of metrics in our paper requires more than one hundred experiments (six foundation models, four adaptation methods over eight datasets), not to mention the hyperparameter tuning and other explorations. Including new models, modifications, or datasets would be great complements to this project but also require extensively more work. We will highlight this limitation in the revised paper.\n\n* Re W2 \"The list of studied foundation models is not as comprehensive as potentially can be\": \n\nRegrettably, we have lost the priority of computing resources for this project, and we are unable to provide further results for other models. Nevertheless, the paper has included two \u201cvideo-based models developed on top of CLIP paradigm\u201d: InternVideo and VATT. Especially, VATT is more or less a \u201csuperset\u201d of VideoCLIP suggested in the review because VATT\u2019s pretraining data and method cover VideoCLIP\u2019s. Similarly, InternVideo can be viewed as a \u201csuperset\u201d of the second paper mentioned in the review. \n\n* Re W3 \"other adaptation techniques\": \n\nThanks for pointing these out. In our exploration phase, we tried multiple adapters, including the \u201cST-Adapter\u201d suggested by the reviewer. However, we were unable to reproduce ST-Adapter\u2019s results reported in the original paper. In contrast, LoRA was the most stable and widely embraced one with superior performance, so we chose to report LoRA in the paper."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6290/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700162825010,
                "cdate": 1700162825010,
                "tmdate": 1700163732645,
                "mdate": 1700163732645,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "thb08EAjLS",
                "forum": "q20O1J9ujh",
                "replyto": "WN6m7X1wCG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6290/Reviewer_1CiA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6290/Reviewer_1CiA"
                ],
                "content": {
                    "title": {
                        "value": "Thank you"
                    },
                    "comment": {
                        "value": "Thank you for your response. I understand the limitations of the available resources and think the paper already contains a high value for the research community. However, my comments and comments from other reviewers point out that the paper can be even better and stronger. It seems that the authors also understand that, and they already know the right direction to improve the paper. I am not against accepting the paper in its current state however having more time to further improve the paper can be very beneficial for the authors and for the research community to have an even stronger comparison of foundation models."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6290/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700690791669,
                "cdate": 1700690791669,
                "tmdate": 1700690791669,
                "mdate": 1700690791669,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]