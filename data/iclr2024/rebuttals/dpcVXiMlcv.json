[
    {
        "title": "Object-Aware Inversion and Reassembly for Image Editing"
    },
    {
        "review": {
            "id": "TP7v9888t2",
            "forum": "dpcVXiMlcv",
            "replyto": "dpcVXiMlcv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3216/Reviewer_nVsr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3216/Reviewer_nVsr"
            ],
            "content": {
                "summary": {
                    "value": "This paper achieves high-quality object-level image editing through a simple yet effective method. Authors find that the editing for different parts requires different noising levels used to inverse the given image to latent space. As such, they search for optimal inversion steps for different parts based on a search metric for both the edited and non-edited regions respectively. In order to produce a natural final result, the results of different parts are seamlessly blended, which yields a harmonic and globally coherent editing output. Albeit its simplicity, the proposed method outperforms competitive works when editing real images and shows promise in real usage."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This paper identifies an interesting and useful phenomenon that the structure of different objects corresponds to a different level of latent as modeled in the diffusion process. To achieve the optimal trade-off between semantic preservation and text-based editing, the optimal number of inversion steps can be determined based on a quantitative measure. \n\n- The image editing results are indeed impressive, as shown in the main text and the appendix. Both the quantitative and qualitative results demonstrate the advantage over strong prior works. \n\n- Since the denoising process of different image parts is disentangled, the proposed method supports fine-grained multiple object editing, which is not featured in prior works.  \n\n- The parallel denoising strategy effectively reduces the search speed."
                },
                "weaknesses": {
                    "value": "- One major drawback of the method is the search speed. Since the inversion steps of each editing part should be comprehensively searched, this may pose a challenge when editing for multiple regions. It would be interesting to conduct a coarse-to-fine search strategy for speedup. Moreover, in the paper, it is suggested to report the image editing speed for different methods since some other baselines, like prompt-to-prompt, can edit images in a feed-forward manner. \n\n- Some writing parts can be improved for better clarity. For example, what's the detailed formulation of min-max normalization? It is suggested to better rephrase the term \"concept mismatch\".\n\n- Since the re-inversion step is always set to 20% of the total steps, the non-edit regions will inevitably be affected. Such subtle change is particularly apparent for faces and small objects."
                },
                "questions": {
                    "value": "The proposed method determines the starting point in the latent space to deviate the semantics of the objects. I think this search strategy is orthogonal to other editing methods that manipulate the latent based on the textural prompt. Hence, I wonder whether the proposed method is compatible with other editing methods, like null inversion. I would like to hear the authors' feedback regarding this."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3216/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3216/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3216/Reviewer_nVsr"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3216/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698678991612,
            "cdate": 1698678991612,
            "tmdate": 1699636269878,
            "mdate": 1699636269878,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DA8KuvoMcE",
                "forum": "dpcVXiMlcv",
                "replyto": "TP7v9888t2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3216/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3216/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer nVsr"
                    },
                    "comment": {
                        "value": "__Q1: It is suggested to report the image editing speed.__\n\nA1: Our image editing speed table has been incorporated into Major Response Q1.\n\n__Q2: Some writing parts can be improved for better clarity.__\n\nA2: We comprehensively revise the paper to enhance clarity in expression.\n\n__Q3: Since the re-inversion step is always set to 20% of the total steps, the non-edit regions will inevitably be affected.__\n\nA4: Thank you for bringing out this issue. Throughout our experiments, we observed that the impact of the number of re-inversion steps is trivial, thus we fix it as 20% of the total steps. However, better results should be achieved with a more careful hyperparameter tuning.\n\n__Q4: Is the proposed method compatible with other editing methods?__\n\nA4: Our search metric can be combined with other inversion-based image editing methods to enhance their performance. Taking Null-text Inversion (NTI) as an example, the search metric can improve NTI in multiple ways. We introduce the compatible method in Appendix A.8 (Figure 13) and supplemented it with experiments. In Appendix A.8, we supplement experiments using the search metric to find the hyperparameters for Null-text Inversion. Additionally, we provide detailed explanations of various compatible methods."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3216/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700280607452,
                "cdate": 1700280607452,
                "tmdate": 1700283602657,
                "mdate": 1700283602657,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LMwIgvRPnz",
                "forum": "dpcVXiMlcv",
                "replyto": "TP7v9888t2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3216/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3216/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Happy to provide additional clarification"
                    },
                    "comment": {
                        "value": "We sincerely thank you again for your great efforts in reviewing this paper. We have addressed your major concerns about speed and integration with other inversion-based editing methods. Please don\u2019t hesitate to let us know if you have any further questions."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3216/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700483257990,
                "cdate": 1700483257990,
                "tmdate": 1700485715371,
                "mdate": 1700485715371,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ssTNuOEmXE",
            "forum": "dpcVXiMlcv",
            "replyto": "dpcVXiMlcv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3216/Reviewer_r1AF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3216/Reviewer_r1AF"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces advancements in text-driven image editing using diffusion-based methods. Existing techniques involve a fixed number of inversion steps to edit images aligned with a target prompt, but the optimal number of steps varies for different editing pairs. The paper introduces a new approach called Object-aware Inversion and Reassembly (OIR) to enable fine-grained, object-level editing. The object-level fine-grained editing is achieved by segmentation from SAM. It uses a search metric to determine the optimal inversion step for each editing pair and combines them for the final edited image, demonstrating superior performance, especially in multi-object editing scenarios."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is well written, and the idea is clearly demonstrated through figures.\n\n2. Although it is natural to do some grid search on the inversion steps when doing image editing, this paper introduces a systematic and principled way to do the search with a quantitative metric.\n\n3. The idea of working on each object separately and reassemble is a interesting way to do multi-object editing.\n\n4. Results are competitive compared to other inversion based editing techniques."
                },
                "weaknesses": {
                    "value": "1. The method introduces significant computational overhead. For each input image, it has to run a search for optimal denoising steps for  each edit pair. The search process can be very time consuming. as it requires denoising from each inversion step, plus a metric calculation step. The overall amount of computation for a single image editing task can thus be very large.\n\n2. It relies on SAM to do fine-grained segmentation of the input image to localize the object, and then do localized editing followed by resembling. However, it has some limitations. For example, the segmentation may not work well for small object. In addition, using SAM constrained the edit to object-level, while the applicability of more global change is questionable (e.g., change the style of an image from spring to winter, change the background to Mars, etc.). It may not be able to change the location of an object. \n\n3. Related to above point, the competitive method, such as null-text inversion, does not involve segmentation and localization, therefore it is a bit unfair to compare with them, as in principle they can only benefit from the segmentation and localization. This is independent from the main innovation of this paper, which is searching for optimal inversion step.\n\n4. Overall, it is a method that combines multiple existing component, with significant increase in compute. Therefore, the contribution and significance of the method is limited."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3216/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698722602527,
            "cdate": 1698722602527,
            "tmdate": 1699636269778,
            "mdate": 1699636269778,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QqdICcQXBV",
                "forum": "dpcVXiMlcv",
                "replyto": "ssTNuOEmXE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3216/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3216/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer r1AF"
                    },
                    "comment": {
                        "value": "__Q1: The search process can be very time consuming.__\n\nA1: We supplement the time overhead experiments and summarize our acceleration methods in the Major Response Q1.\n\n__Q2.1: It relies on SAM to do fine-grained segmentation.__\n\nA2.1: \n- SAM is not mandatory. Our method is complementary to various mask generation schemes, such as attention map-based mask extraction methods that do not require introducing additional models. These methods were employed by notable works such as DiffEdit [a] and MasaCtrl [b].\n\n\n- In the revised paper, we provide a comparison of different mask generators in Appendix A.7 (Figure 12). From the figure, it can be observed that our method is highly robust to different mask generators.\n\n[a] MasaCtrl: Tuning-Free Mutual Self-Attention Control for Consistent Image Synthesis and Editing, ICCV 2023.\n\n[b] Diffedit: Diffusion-based semantic image editing with mask guidance, ICLR 2023.\n\n__Q2.2: The applicability of more global change is questionable (e.g., change the style of an image from spring to winter, change the background to Mars, etc.).__\n\nA2.2: \n- We are applicable to global change and we have extensively demonstrated these results in our initial submission. For example, in the main text, Column h of Figure 4, Figure 6, and Figure 16 with tasks (b, k), (b, m), (c, l), (c, m), (d, m), as well as Figure 17 with tasks (a, h), (c, f), (d, f), (d, h), (e, g), (e, h), (e, i), all involve background editing. In Figure 16, tasks (c, k), (f, j), and in Figure 17, tasks (c, g), (c, i) all include style editing.\n- Background can be considered as a generalized object for editing. In tasks that involve changing the style, alterations can be made on different objects separately, and then our reassembly strategy can be utilized to achieve global interaction.\n\n__Q2.3: It may not be able to change the location of an object.__\n\nA2.3: \n- Given an input image $I$ and a target prompt $P$, image editing's goal is to generate a new image $O$ that complies with $P$ and preserves the structure and semantic layout of $I$. The task of changing the location of an object is not under the current setting. More details about the editing setting are thoroughly explained in the first paragraph of Section 3 of our paper and have also been discussed in literature, such as Prompt-to-Prompt [c] and Plug-and-Play [d].\n\n[c] Prompt-to-Prompt Image Editing with Cross Attention Control, ICLR 2023.\n\n[d] Plug-and-Play Diffusion Features for Text-Driven Image-to-Image Translation, CVPR 2023.\n\n__Q3: Related to the above point, the competitive method, such as null-text inversion, does not involve segmentation and localization, therefore it is a bit unfair to compare with them, as in principle, they can only benefit from segmentation and localization. This is independent from the main innovation of this paper, which is searching for optimal inversion step.__\n\nA3: \n- Null-text Inversion (NTI) is a training-based approach that introduces learnable null-text embeddings to address DDIM Inversion's inability to accurately reconstruct the original image. It needs to be combined with other editing methods to achieve image editing, so whether NTI needs a mask depends on the editing method with which it is combined.\n- Our method is tightly coupled with mask generation and is completely training-free. We employ an additional mask generator to disentangle objects, enabling object-aware editing by separating individual entities. \n- We combine NTI with Prompt-to-Prompt (P2P) as a comparative method by using Grounded-SAM as the mask generator in P2P, which is the same as our method. The experimental results presented in Appendix A.10 (Figure 15) indicate that our training-free method outperforms the NTI+P2P counterpart which requires training. Moreover, we have also compared our method with other mask-based editing methods, and stable diffusion inpainting, in Section 4.1 of the initial submission. \n\n[e] Null-text Inversion for Editing Real Images using Guided Diffusion Models, CVPR 2023.\n\n__Q4: The contribution and significance of the method is limited.__\n\nA4: Please refer to Major Response Q2."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3216/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700280575731,
                "cdate": 1700280575731,
                "tmdate": 1700626078367,
                "mdate": 1700626078367,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AYsUKDpxgP",
                "forum": "dpcVXiMlcv",
                "replyto": "ssTNuOEmXE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3216/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3216/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Happy to provide additional clarification"
                    },
                    "comment": {
                        "value": "We sincerely thank you again for your great efforts in reviewing this paper. We have addressed your major concerns about speed, contribution, the relation with SAM, and additional comparison with other methods using SAM. Please don\u2019t hesitate to let us know if you have any further questions."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3216/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700483184300,
                "cdate": 1700483184300,
                "tmdate": 1700485601380,
                "mdate": 1700485601380,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Q4T84EpVuU",
            "forum": "dpcVXiMlcv",
            "replyto": "dpcVXiMlcv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3216/Reviewer_pgMn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3216/Reviewer_pgMn"
            ],
            "content": {
                "summary": {
                    "value": "This paper discovers that various editing pairs exhibit differing levels of editing complexity. Additionally, it is observed that neglecting the varying difficulty levels of different editing pairs in multi-object editing tasks results in the problems of concept mismatch and poor editing. To address these issues, the paper introduces a novel training-free image editing approach called OIR. This method follows the approach of assembly first and then reassembly. 1) In the assembly strategy, the paper introduces a novel search metric. This metric automatically identifies the optimal inversion step for different editing pairs, enabling automatic control of editing difficulty. This approach allows different editing pairs to undergo separate denoising, preventing concept mismatch issues. Moreover, employing the search metric to find the optimal result represents a new paradigm in single-object editing. 2) In the reassembly strategy, the article suggests merging the editing regions and non-editing region during the reassembly step. This operation takes place in the denoise latent space. The reassembly process incorporates a re-inversion strategy, enhancing the image editing's edge smoothness, improving image editability, and enabling interaction across regions. 3) To assess OIR's capabilities, the authors collect two datasets, which are employed to evaluate both the single-object editing proficiency of the search metric and the multi-object editing capability of OIR. Numerous experimental results indicate that the search metric performs on par with existing state-of-the-art editing methods in single-object editing tasks. Moreover, in multi-object editing tasks, OIR demonstrates strong performance, outperforming the previous SOTA methods"
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1.\tThis paper identifies a fundamental challenge in multi-object image editing tasks. Previous methods typically treat an entire image as a whole entity during multi-object editing, without considering that editing pairs may have varying levels of editing complexity and therefore require different optimal inversion steps.\n2.\tThe new search metric introduced in this article is simple yet effective. The approach of adding the two evaluation indicators makes sense, and visual verification confirms that the search metric aligns with the editing effect. In single-object editing tasks, employing the search metric yields promising results, comparable to other image editing methods.\n3.\tThe OIR introduced in this article is novel and effective, presenting a new solution for the multi-object editing task. Unlike previous approaches, which treated the entire image as a whole, OIR breaks down the task into editing pairs. The experimental comparisons with other methods are extensive and thorough.\n4.\tThe paper's structure is well-organized and easy to follow. The figures are well-designed, effectively illustrating the ideas and claims presented."
                },
                "weaknesses": {
                    "value": "1.\tIn [a], it is mentioned that the inversion step can be considered as a fixed hyperparameter. However, the author only presents the results of the optimal inversion steps for example images, without demonstrating the overall distribution trend of the optimal inversion step across the entire dataset. Moreover, what are the distinct characteristics of editing pairs with larger and smaller optimal inversion steps respectively?\n2.\tIn the reassembly strategy of OIR discussed in this paper, both reassembly step and re-inversion are mentioned. The paper indicates that both methods can smooth the edges of images and enable global information interaction. Re-inversion first inverts the spliced latent and then denoises it, essentially increasing the denoise step. Is it possible to replace the operation of re-inversion by increasing the reassembly step?\n[a] SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations, ICLR 2022."
                },
                "questions": {
                    "value": "see weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3216/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698765089244,
            "cdate": 1698765089244,
            "tmdate": 1699636269693,
            "mdate": 1699636269693,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1ROxu6rJ3Q",
                "forum": "dpcVXiMlcv",
                "replyto": "Q4T84EpVuU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3216/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3216/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer pgMn"
                    },
                    "comment": {
                        "value": "__Q1.1: What is the overall distribution trend of the optimal inversion step across the entire dataset?__\n\nA1.1: We collect 200 editing pairs from our multi-object dataset and employ the search metric to identify the optimal inversion step for each editing pair. The distribution of the optimal inversion steps is presented in Appendix A.9 (Figure 14). According to the figure, the majority of the optimal inversion steps occur within the range of 25 and 45.\n\n__Q1.2: What are the distinct characteristics of editing pairs with larger and smaller optimal inversion steps respectively?__\n\nA1.2: We notice that when modifying backgrounds or objects with substantial changes in shape, such as the sky or the ground, larger optimal inversion steps are required. On the other hand, situations involving objects and targets with similar shapes generally require smaller inversion steps. Appendix A.9 (Figure 14) displays several cases within these scenarios.\n\n__Q2: Is it possible to replace the operation of re-inversion by increasing the reassembly step?__\n\nA2:\nThe re-inversion can not be replaced by increasing reassembly steps for the following reasons. \n- The reassembly steps cannot be increased arbitrarily. As detailed in Section 3, it must be smaller than the smallest optimal inversion step in all editing pairs, to preserve the original features of non-editing regions. This limits the ability to improve editing results by changing the reassembly step. By contrast, the number of re-inversion steps is free from such limitations.\n- Re-inversion improves interactions between various regions. Because re-inversion can extend the denoise steps, it enhances the global information exchange between various regions, leading to more realistic and natural outcomes."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3216/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700280423141,
                "cdate": 1700280423141,
                "tmdate": 1700636468788,
                "mdate": 1700636468788,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zNwplVIU6a",
                "forum": "dpcVXiMlcv",
                "replyto": "Q4T84EpVuU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3216/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3216/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Happy to provide additional clarification"
                    },
                    "comment": {
                        "value": "We sincerely thank you again for your great efforts in reviewing this paper. We have addressed your major concerns about the distribution of optimal inversion step and the importance of re-inversion. Please don\u2019t hesitate to let us know if you have any further questions."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3216/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700483244022,
                "cdate": 1700483244022,
                "tmdate": 1700484803253,
                "mdate": 1700484803253,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nqnFgaJ2f3",
                "forum": "dpcVXiMlcv",
                "replyto": "1ROxu6rJ3Q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3216/Reviewer_pgMn"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3216/Reviewer_pgMn"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks authors for providing a nice response  of my review and most of my concerns are addressed."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3216/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700638048677,
                "cdate": 1700638048677,
                "tmdate": 1700638048677,
                "mdate": 1700638048677,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "AGJVbAwa85",
            "forum": "dpcVXiMlcv",
            "replyto": "dpcVXiMlcv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3216/Reviewer_xVir"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3216/Reviewer_xVir"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes object-aware inversion and reassembly for image editing. The motivation is that the inversion steps vary from the editing of different objects. Therefore, we need to choose different inversion steps for different objects. Also, for different objects in one image, we need to merge the editing results. Then the reassembly strategy is introduced. The proposed method achieves state-of-the-art performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1) The paper is well-organized and easy to follow.\n\n2) It makes sense to use different diffusion steps for different objects when performing the image editing.\n\n3) The paper proposes a reassembly strategy to merge the editing results.\n\n4) The proposed method achieves state-of-the-art performance."
                },
                "weaknesses": {
                    "value": "1) To determine the optimal inversion steps for image editing, we need to inverse the image to all steps and then edit them accordingly. It is time-consuming and not automatic.\n\n2) The two contributions are more like the engineering stuff. However, admittedly, they do bring a lot of performance gain."
                },
                "questions": {
                    "value": "Please see my concerns in the weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3216/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698808284822,
            "cdate": 1698808284822,
            "tmdate": 1699636269609,
            "mdate": 1699636269609,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fiuz1OITNR",
                "forum": "dpcVXiMlcv",
                "replyto": "AGJVbAwa85",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3216/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3216/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xVir"
                    },
                    "comment": {
                        "value": "__Q1: The method is time-consuming and not automatic.__\n\nA1: We supplement the time overhead experiments and summarize our acceleration methods in the Major Response Q1.    \n\n\n__Q2: The two contributions are more like engineering stuff.__\n\nA2: We identify a significant challenge in multi-object image editing and highlight that different editing pairs require distinct optimal inversion steps. Our two contributions, including a new search metric and an object-aware inversion and reassembly process, are specifically designed to enable object-level fine-grained control, remarkably outperforming previous SOTA methods. More details can be found in Major Response Q2."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3216/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700280075921,
                "cdate": 1700280075921,
                "tmdate": 1700280362860,
                "mdate": 1700280362860,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KUUDzoJSBr",
                "forum": "dpcVXiMlcv",
                "replyto": "AGJVbAwa85",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3216/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3216/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Happy to provide additional clarification"
                    },
                    "comment": {
                        "value": "We sincerely thank you again for your great efforts in reviewing this paper. We have addressed your major concerns about contribution and speed. Please don\u2019t hesitate to let us know if you have any further questions."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3216/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700483144185,
                "cdate": 1700483144185,
                "tmdate": 1700484744293,
                "mdate": 1700484744293,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "s118qAvFMG",
                "forum": "dpcVXiMlcv",
                "replyto": "fiuz1OITNR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3216/Reviewer_xVir"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3216/Reviewer_xVir"
                ],
                "content": {
                    "title": {
                        "value": "Post Rebuttal Comments"
                    },
                    "comment": {
                        "value": "Dear Authors,\n\nThanks for your clarification. For the time cost for other methods, may I ask if they are implemented under one GPU or multiple GPUs?\n\nThanks,"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3216/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700712567434,
                "cdate": 1700712567434,
                "tmdate": 1700712567434,
                "mdate": 1700712567434,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MZnayAbGxb",
                "forum": "dpcVXiMlcv",
                "replyto": "AGJVbAwa85",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3216/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3216/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xVir"
                    },
                    "comment": {
                        "value": "Dear Reviewer xVir,\n\nOther methods run on a single GPU because they lack clear parallel acceleration solutions due to the temporal dependency between the denoise steps. We use 2 GPUs for parallelization, in the table of Major Response Q1.\n\nThanks."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3216/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700714480721,
                "cdate": 1700714480721,
                "tmdate": 1700720564451,
                "mdate": 1700720564451,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]