[
    {
        "title": "SALMON: Self-Alignment with Principle-Following Reward Models"
    },
    {
        "review": {
            "id": "PWoH7waFel",
            "forum": "xJbsmB8UMx",
            "replyto": "xJbsmB8UMx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4299/Reviewer_AyPF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4299/Reviewer_AyPF"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a principle-following reward model trained on a set of human-specified principles, which is then used to learn human-aligned behaviors via the corresponding RL policy."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is fairly easy to understand.  The approach seems to be a novel way to align RL policies using LLMs."
                },
                "weaknesses": {
                    "value": "I believe the underlying assumption is that the specified principles (and examplars) are capable of completely specifying aligned behaviors. In reality, this may not be the case, and additionally, specifying an exhaustive list of principles may not be practical. Further results could have also been included (described later)."
                },
                "questions": {
                    "value": "1.\tIt is not entirely clear how the principles and exemplars affect the policy learning. I would have liked to see more experiments where the alignment of policies are evaluated after training with say, just the examples, just the 31 principles, different fractions of the 31 principles, with certain types of principles excluded etc.,  \n\n2.\tWhat happens in scenarios where two or more principles are in conflict with each other?\n\n3.\tPerhaps related to the previous point, would it be possible to impose a hierarchy of principles during training? I imagine such hierarchies may be important in many practical circumstances.\n\n4.\tIs there any way to guarantee that the set of specified principles would indeed lead to an aligned policy? In other words, is the set of principles general enough to be applicable to any scenario?\n\n5.\tIn pg 7 - It is not clear what a power RLHF trained model is\n\n6.\tPg8 \u2013 In 4.1.2 \u2018police\u2019 should be \u2018policy\u2019"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4299/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4299/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4299/Reviewer_AyPF"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4299/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698584129337,
            "cdate": 1698584129337,
            "tmdate": 1699636398083,
            "mdate": 1699636398083,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "F5FJ7s4C7u",
                "forum": "xJbsmB8UMx",
                "replyto": "PWoH7waFel",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4299/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4299/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer AyPF"
                    },
                    "comment": {
                        "value": "Thank you for your insightful review. We appreciate your recognition of the paper's clarity and the novelty of our approach in aligning RL policies using LLMs. We address your questions below.\n\n> Concern 1: In reality, this may not be the case, and additionally, specifying an exhaustive list of principles may not be practical.\n\nWe thank the reviewer for the insightful comment. We have added a new paragraph to discuss the limitation of the Principle Design Challenges, as copied below:\n\nPrinciple Design Challenges: Crafting robust and encompassing principles for SALMON is intricate, mainly due to the unpredictability of the myriad scenarios a model might encounter\nduring the RL stage. Balancing potentially conflicting principles introduces complexities that can yield unexpected results. We advocate for the participation of a diverse group, including\nethicists and other stakeholders, to refine these guiding principles. It is crucial to recognize that\ndistinct contexts and applications will necessitate unique strategies. We present our approach not as a universal solution but as a starting platform, aiming to foster expansive community discourse. In practice, many companies and organizations already have an exhaustive list of principles for their employees, which is usually named as Business Conduct Guidelines (BCG).\n\nOn the other hand, we would like to point out that a recent work (published after the ICLR deadline) [1] shows that some general \u201cgood-for-humanity\u201d principles can provide high-level guidance to choose outputs that are best for humanity, and achieve similar performance to specific principles. We leave the exploration of SALMON with less amount of general principles as future direction.\n\n> Question 1: It is not entirely clear how the principles and exemplars affect the policy learning.\n\nPlease see \u201cDirect Evaluation of the Principle-Following Reward Model\u201d under the General Response for how principles affect the preference of the reward model.\n\nAs an approximation of PPO, we also provide the best-of-n samples of how principles affect the results of ranked policy outputs in Section. D of the appendix. \n\n\n> Question 2: What happens in scenarios where two or more principles are in conflict with each other?\n\nAs the discriminative preference labels are decided by the principle with the most pronounced difference (Appendix D). The scenarios of conflict principles are determined by the most significant differences between two responses.\n\n> Question 3: would it be possible to impose a hierarchy of principles during training?\n\nWe thank the reviewer for the insightful suggestion. We envision that a hierarchy of principles (e.g., safety over helpfulness) is possible and would explore this direction as our future work.\n\n> Question 4: is the set of principles general enough to be applicable to any scenario?\n\nPlease refer to our response to Concern 1.\n\n[1]: Kundu, Sandipan, et al. \"Specific versus General Principles for Constitutional AI.\" arXiv preprint arXiv:2310.13798 (2023)."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4299/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700616299726,
                "cdate": 1700616299726,
                "tmdate": 1700616299726,
                "mdate": 1700616299726,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "76UjfVUkgB",
                "forum": "xJbsmB8UMx",
                "replyto": "F5FJ7s4C7u",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4299/Reviewer_AyPF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4299/Reviewer_AyPF"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your detailed responses. They, along with your responses to the other reviewers' concerns have helped me better understand the approach."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4299/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700693088523,
                "cdate": 1700693088523,
                "tmdate": 1700693088523,
                "mdate": 1700693088523,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ZhHiFrWbPo",
            "forum": "xJbsmB8UMx",
            "replyto": "xJbsmB8UMx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4299/Reviewer_DDqX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4299/Reviewer_DDqX"
            ],
            "content": {
                "summary": {
                    "value": "This paper provides SALMON, a novel method for Large Language Model (LLM) alignment. The main idea of SALMON is self-alignment with principle-following reward models. The current prevailing method for LLM alignment is Reinforcement Learning with Human Preferences (RLHF), and it mainly consists of three phases: (1) supervised fine-tuning (SFT) on human demonstration data, (2) reward mode (RM) training on human preference data, and (3) RL fine-tuning with human-guided RM. Unlike RLHF, SALMON consists of (1) few-shot in-context learning (ICL), (2) RM training on AI preference data, and (3) RL fine-tuning with AI-guided RM. Since SALMON is based on RLAIF, it can be more efficient and scalable than RLHF. More specifically, SALMON based on Llama-2-70B only uses 6 demonstration annotations and zero preference annotations to achieve 7.4 MT-Bench score. In contrast, Llama-2-Chat based on SFT and RLHF uses about 27K demonstration annotations and about 1.4M preference annotations to achieve 6.9 MT-Bench."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- S1. First of all, this paper well-written and well-organized.\n\n- S2. It is very interesting that SALMON (one of RLAIF methods) can significantly reduce human annotation costs than a prevalent RLHF method.\n\n- S3. Unlike other RLAIF methods, SALMON can control preference scores by using a principle-following reward model (i.e., changing a principle to follow)."
                },
                "weaknesses": {
                    "value": "- W1. One of main contributions of this paper is a principle-following reward model that can control reward scores according to principles. In addition to the overall alignment scores, can the authors measure a quantitative result of the principle-following reward model?\n\n- W2. Even though Llama-2-70B with SALMON can provide better alignment score (7.4 MT-Bench score) than Llama-2-70B with RLHF (PPO) (6.9), there is still large gap to GPT-4 (9.0) and ChatGPT (7.9).\n\n- W3. This paper compares SALMON with PPO-based RLHF. However, enhanced RLHF methods such as DPO (Direct Policy Optimization) and P3O (Pair-wise Policy Optimization) has been proposed and shown that they can achieve better reward score than PPO-based RLHF. It would be better to compare SALMON with recent RLHF methods.\n\n- W4. It would be interesting to provide comparison in perplexity score to see if SALMON is better to maintain the token distribution of the reference LLM than PPO-based RLHF methods."
                },
                "questions": {
                    "value": "- Q1. Regarding W1 above, what is the main advancement of SALMON compared to Constitutional AI? \n\n- Q2. Regarding W2 above, if better base LLMs than Llama-2-70B are used, can SALMON further reduce the gap to GPT-4 and ChatGPT? Or, is SALMON specialized on Llama-2 family LLMs?\n\n- Q3. Regarding W3 above, if some enhanced RLHF methods such as DPO and P3O are used instead of a PPO-based method, is SALMON sill provide better performance than those methods? If not, can SALMON increase its alignment performance by additionally using human demonstration data or AI (or human) preference data?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4299/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4299/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4299/Reviewer_DDqX"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4299/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698840327204,
            "cdate": 1698840327204,
            "tmdate": 1699636397980,
            "mdate": 1699636397980,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uElzXlwKM1",
                "forum": "xJbsmB8UMx",
                "replyto": "ZhHiFrWbPo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4299/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4299/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer DDqX"
                    },
                    "comment": {
                        "value": "Thank you for your insightful review and the positive feedback on our paper. We are glad that you found our paper well-written and organized. It's particularly encouraging to hear your appreciation for the practical intuitiveness of SALMON and its efficiency in reducing human annotation costs compared to the prevalent RLHF method. We address your questions below.\n\n> Concern 1: can the authors measure a quantitative result of the principle-following reward model?\n\nPlease see \u201cDirect Evaluation of the Principle-Following Reward Model\u201d under the General Response.\n\n> Concern 2: Even though Llama-2-70B with SALMON can provide better alignment score (7.4 MT-Bench score) than Llama-2-70B with RLHF (PPO) (6.9), there is still large gap to GPT-4 (9.0) and ChatGPT (7.9).\n\nWe would like to point out that it is unfair to directly compare SALMON with state-of-the-art proprietary models. SALMON uses an open-source LLaMA-2-70B base language model, while GPT-4 uses a much larger and stronger base language model, as evidenced in the LLM capability evaluations (e.g., MMLU, BIG-BENCH). Therefore, we use LLaMA-2-Chat-70b (6.9) as the primary baseline for comparison, and show that SALMON (RLAIF) can outperform the prevailing RLHF method on general alignment.\n\n> Concern 3: However, enhanced RLHF methods such as DPO (Direct Policy Optimization) and P3O (Pair-wise Policy Optimization) have been proposed and shown that they can achieve better reward scores than PPO-based RLHF. It would be better to compare SALMON with recent RLHF methods.\n\nP3O [1] is released to the public in Oct. 2023, before the ICLR deadline. It is impossible for us to compare SALMON against it.\n\nFor DPO [2], please see \u201cCompare SALMON with recent RLHF alternatives: DPO, ResT, and SliC-HF\u201d under the General Response.\n\n> Concern 4: if SALMON is better to maintain the token distribution of the reference LLM\n\nWe perform the comparison of the aggregated KL-divergence of SALMON with and without RL-time inference intervention, but do not observe significant difference.\n\n|      | Seq. KL-divergence (w.r.t SFT) |\n| ----------- | ----------- |\n| Dromedary-2-70b (PPO w/ RL-time inference intervention)  | 13.37 |\n| Dromedary-2-70b (PPO w/o RL-time inference intervention) | 14.56 |\n\nWe would like to point out that the reward-hacking distribution shift in the RLHF training usually does not come with a significant increase in KL-divergence, especially when these RLHF methods have a KL-penalty loss term in the reward (Eq. 2).\n\n> Question 1: what is the main advancement of SALMON compared to Constitutional AI?\n\nPlease see our General Response.\n\n> Question 2: if better base LLMs than Llama-2-70B are used, can SALMON further reduce the gap to GPT-4 and ChatGPT?\n\nAs a general and scalable alignment paradigm, SALMON is not specialized on LLaMA-2 family LLMs. We believe better base LLMs + SALMON could further reduce the gap to GPT-4 and ChatGPT, as evidenced by the recent works [6,7] showing that RLAIF has a very steep scaling curve (e.g., Figure 4 in [1] and Figure 6 in [2]).\n\n> Question 3: If some enhanced RLHF methods such as DPO and P3O are used instead of a PPO-based method, is SALMON sill provide better performance than those methods?\n\nSince PPO (RLHF) is the only method that has proven effective in large-scale real-world settings [3,4,5], and our method outperforms LLaMA-2-Chat-70b (best PPO-trained non-distilled model) under fair comparison, we believe SALMON would also achieve strong performance with other non-RL (offline RL) PPO alternatives. For example, we found sampling responses from the SFT model, using a principle-following reward model to label the preference, and performing DPO-based optimization can achieve strong performance.\n\nPlease see \u201cCompare SALMON with recent RLHF alternatives: DPO, ResT, and SliC-HF\u201d under the General Response for more information.\n\n---\n\n[1] Wu, Tianhao, et al. \"Pairwise proximal policy optimization: Harnessing relative feedback for llm alignment.\" arXiv preprint arXiv:2310.00212 (2023).\n\n[2] Rafailov, Rafael, et al. \"Direct preference optimization: Your language model is secretly a reward model.\" arXiv preprint arXiv:2305.18290 (2023).\n\n[3] OpenAI. \u201cIntroducing ChatGPT\u201d (2022).\n\n[4] OpenAI. \u201cGPT-4 Technical Report\u201d (2023).\n\n[5] Anthropic. \u201cIntroducing Claude\u201d (2023).\n\n[6] Bai, Yuntao, et al. \"Constitutional ai: Harmlessness from ai feedback.\" arXiv preprint arXiv:2212.08073 (2022).\n\n[7] Kundu, Sandipan, et al. \"Specific versus General Principles for Constitutional AI.\" arXiv preprint arXiv:2310.13798 (2023)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4299/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700616251421,
                "cdate": 1700616251421,
                "tmdate": 1700616251421,
                "mdate": 1700616251421,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qeg84O66Y6",
            "forum": "xJbsmB8UMx",
            "replyto": "xJbsmB8UMx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4299/Reviewer_pWc3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4299/Reviewer_pWc3"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes SALMON, a method for training reward models that generate scores based on certain guiding principles. First, an instruction-tuned SFT model is used to generate preferences conditioned on a principle. This dataset is then used to train a principle conditioned reward model, where the reward model is trained with many subsets of principles, enabling it to generalize to new principles as well. This instruction tuned reward model is then used in a RLHF loop to fine-tune the SFT model. The resulting model Dromedary-2-70b, tuned from llama-2-70b, shows strong performance on several benchmarks, such as MTBench."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is generally well-written, though addressing some questions related to preference collection should improve the clarity further.\n- A relevant and timely problem to address. Preference data needs to be extensively collected to keep reward models in-distribution with the current RL policy.\n- The performance of the model is impressive, and the recipe for AI feedback seems quite interesting."
                },
                "weaknesses": {
                    "value": "- Some lack of novelty compared to Constitutional AI; The paper emphasizes constitutional AI focuses more on safety, but the technique itself is very much amenable for building a more \u201chelpful\u201d constitution too. But, the system laid down is distinct enough to warrant interest from the community.\n\n- The paper claims that using principles to avoid reward hacking. Perhaps, the work \u201creward hacking\u201d is a bit overloaded, but I don\u2019t see any reason that SALMON rewards cannot be hacked to give degenerative responses or undesirable responses, if trained long enough using RL.\n\n- What I do not completely understand is why train a separate reward model at all? The SFT-Model acting as a judge can already be used as a reward model. The scores for SFT-model can be aggregated for many principles as well (it might require multiple passes, but they can be batched potentially).\n\nOverall, it seems that the final reward model ends up using several \u201chacks\u201d which somewhat go against the main message of the paper:\n- The reward model training claims to bypass human preference collection, but ends up pre-training on Anthropic HH-RLHF and SHP preference. Importantly, HH-RLHF and SHP contribute ~320k preference pairs to pre-training, while the SALMON reward uses only ~9.8k prompts (unclear how many preference pairs that yields, given that it can be combined with a large number of principles). How well does SALMON do without without the Preference Model Pre-training?\n- Prior works deem the tendency of RLHF to produce lengthier responses as a negative. It is somewhat unfortunate that such a length bonus needs to be explicitly included as a symbolic reward. Can you also elaborate how this reward is included?\n\nWhile I appreciate the performance of Dromedary-2-70b, the paper lacks several experiments and ablations that give more insight into why the method works. Some quantitative experiments that show how well the SFT model labels the preference in accordance to the principle are severely needed, and ablations of training the model without the \u201chacks\u201d, and only with the \u201chacks\u201d would show the importance of SALMON technique."
                },
                "questions": {
                    "value": "- What happens if the answers are indistinguishable based on the principle? For example, when asking for a concise answer (but not necessarily correct, ethical or honest) \u2014 would it make sense to have an option for \u201cno preference\u201d when collecting preference from the model?\n- For every user prompt and output pairs, are preferences collected using every principle? What is the size of the final preference dataset that is generated by the Self-aligned SFT model?\n- Moreover, since the preference is computed based on the difference between logprobs, it would make sense to look at log probability of semantically equivalent answers [1]. The highest log probability can be misleading by itself.\n- Why is the preference label decided by the principle with the most pronounced difference? Why not use the sum of scores, for example?\n- Can you quantitatively evaluate the preferences generated by the SFT-Model, especially conditioned on the principles? How sensitive is the SFT model to the principle? For example, does using the negative principle flip the preference?\n- How does the model perform when using just the SALMON reward without the symbolic bonuses, especially without the length bonus?\n- A discussion on more recent works on alignment from scratch can be added such DPO, ReST SliC-HF etc\n\n[1] Surface Form Competition: Why the Highest Probability Answer Isn't Always Right. Holtzmann et al."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4299/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4299/Reviewer_pWc3",
                        "ICLR.cc/2024/Conference/Submission4299/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4299/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698911502403,
            "cdate": 1698911502403,
            "tmdate": 1700959423802,
            "mdate": 1700959423802,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "o65ZoxVtDy",
                "forum": "xJbsmB8UMx",
                "replyto": "qeg84O66Y6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4299/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4299/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer pWc3 (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your constructive feedback on our work involving SALMON and the development of Dromedary-2-70b. We are glad that you appreciate the clarity and relevance of our paper, particularly in addressing the timely problem of preference data collection for reward models. Your recognition of our model's impressive performance and the novel approach to AI feedback is encouraging. We address your questions below.\n\n> Concern 1: Some lack of novelty compared to Constitutional AI\n\nPlease see \u201cNovelty Compared to Constitutional AI\u201d under the General Response.\n\n> Concern 2: I don\u2019t see any reason that SALMON rewards cannot be hacked to give degenerative responses or undesirable responses, if trained long enough using RL.\n\nIn LLM-based AI alignment, reward hacking, or reward over-optimization [1,2,3], means that an RL-optimized model might exploit certain vulnerabilities in the fixed reward model, thereby artificially boosting its score without genuine performance improvement.\n\nWe would like to clarify that reward hacking in LLM alignment can happen with perfect natural language, but often comes with noticeable problematic behavioral traits. For example, in this paper, we identify three types of reward-hacking behaviors in our Figure 3 without degenerated responses. Here, unnatural or degenerated responses would not occur due to either too low reward scores or too-high KL penalty.\n\nPerhaps the degenerated responses would occur if the policy models were trained really long. But in practice, the PPO steps of RLHF/RLAIF on large-enough LLMs are typically within 1000 steps [4], and the degenerated responses do not occur in this short RL period.\n\n> Concern 3: The SFT-Model acting as a judge can already be used as a reward model.\n\nThe SFT-Model cannot act as a judge for the following reasons:\n\nNote that at inference time, as opposed to training, all the reward models need to predict a scalar for a single output, without requiring access to its paired output. LLMs are typically better at doing pairwise comparison, but worse at directly assigning a score to a single answer [4,5,6]. Therefore, the SFT-Model in RLAIF is usually only used for labeling AI-generated preferences with pairwise comparison.\nEven for pairwise comparison, the performance of the SFT-Model still falls behind the specially trained reward models. Please see \u201cDirect Evaluation of the Principle-Following Reward Model\u201d under the General Response for more information.\nAggregating many principles with the SFT-Model, as the reviewer pointed out, can be very inefficient at implementation.\n\n> Concern 4: How well does SALMON do without the Preference Model Pre-training?\n\nWe found the SFT-Model generated preferences can be comparable to pre-trained preference models, especially on the helpfulness dimension. Please see \u201cDirect Evaluation of the Principle-Following Reward Model\u201d under the General Response for more information.So SALMON is still possible to achieve strong self-alignment performance without PMP.\n\nHowever, we would also like to point out that Preference Model Pre-training (PMP) is a common practice in RLHF [8,9] to improve the sample efficiency and the asymptotic performance.\n\n> Concern 5: Prior works deem the tendency of RLHF to produce lengthier responses as a negative.\n\nWhile some recent work [7] found that \u201cWhen optimizing for helpfulness, RLHF has been consistently observed to drive models to produce longer outputs\u201d, we have not found any work that claims the lengthier responses as a negative. Instead, [7] argues that longer outputs often prove to be more informative and useful. We are open to further discuss this aspect and would appreciate any additional references the reviewer could provide to enrich our understanding.\n\n> Question 1: What happens if the answers are indistinguishable based on the principle?\n\nOur system does have the \u201cno preference\u201d option when the logprobs of judging either Output (a) and Output (b) as the better option are equal.\n\n> Question 2: What is the size of the final preference dataset that is generated by the Self-aligned SFT model?\n\nThe preferences are collected for each principle. We collected a total of 98k = 10 (#principles in Table 6) x 9.8k (#prompts OASST1) principle-specified preferences, and used those to construct around 40k multi-principle preferences.\n\n> Question 3: The highest log probability can be misleading by itself.\n\nWe calculate the log probabilities of the SFT-Model for preferring Output (a) or Output (b), not the log probabilities of the answer themselves. The AI-generated preference process is illustrated in Figure 2 (Collecting Principle-Driven Synthetic Preferences)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4299/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700616134175,
                "cdate": 1700616134175,
                "tmdate": 1700616134175,
                "mdate": 1700616134175,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QamQMfLuoq",
            "forum": "xJbsmB8UMx",
            "replyto": "xJbsmB8UMx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4299/Reviewer_7kRD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4299/Reviewer_7kRD"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a new self-alignment technique called SALMON, where we can leverage AI feedback with minimal human supervision to align language models with human preference. With those synthetic preference data generated with SALMON, the authors train a new model named Dromedary-2, which achieves state-of-the-art performance on various benchmark datasets."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- The methods that the authors propose only need to change the way of generating synthetic data, without much of modification of following RLHF procedure, which makes the technique more general and easy to adapt to other tasks\n- The methods can be quite helpful when we need more domain-specific preference data (e.g., code, agents) when there is no such public available data.\n- The authors demonstrate the advantage of the new method by finetuning with QLORA on 70B models, demonstrating its ability to improve model performance."
                },
                "weaknesses": {
                    "value": "- It would be good to show that the methods can also be leveraged to improve the performance of smaller models such as 7B or 33B, making the method easier for other topics or tasks. \n\n- I believe this method could potentially be adapted to some other tasks such as code generation. But I am not sure if it is possible, it would be good if the authors could comment on this."
                },
                "questions": {
                    "value": "Please see my questions above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4299/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699257163899,
            "cdate": 1699257163899,
            "tmdate": 1699636397824,
            "mdate": 1699636397824,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hsEdxrxivz",
                "forum": "xJbsmB8UMx",
                "replyto": "QamQMfLuoq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4299/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4299/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer 7kRD"
                    },
                    "comment": {
                        "value": "Thank you for your insightful review of our paper. We're glad to hear that you appreciate the simplicity and general applicability of our SALMON technique, as well as its potential in domain-specific applications. Your recognition of how our method can generate synthetic preference data with minimal human supervision and its integration with Dromedary-2 to achieve top-notch performance across benchmarks is encouraging. We address your questions below.\n\n> Concern 1: It would be good to show that the methods can also be leveraged to improve the performance of smaller models such as 7B or 33B\n\nRecent works [1,2] show that RLAIF has a very steep scaling curve (e.g., Figure 4 in [1] and Figure 6 in [2]), that is, the model\u2019s discriminative performance is not very far from random guess when the model size is around 22B. On the other hand, the SFT initialization of SALMON, i.e., Principle-Driven Self-Alignment [3], also requires a strong enough base language model (LLaMA-65b/70b). Therefore, we have only experimented with the strongest open-source LLM with 70B parameters.\n\nHowever, we are optimistic and enthusiastic about compressing the alignment behavior from these large self-aligned models into smaller models, for example, using the Dromedary-2-70b model as the oracle generator/evaluator model to generate distillation data for smaller models. [4] That would be an important direction for deploying these models.\n\n> Concern 2: I believe this method could potentially be adapted to some other tasks such as code generation.\n\nWe appreciate the reviewer\u2019s insight. We do believe our SALMON (RLAIF) method can be adapted to more difficult tasks where it\u2019s very hard or costly for humans to annotate preferences. We are actively pursuing this direction as follow-up work.\n\n[1] Bai, Yuntao, et al. \"Constitutional ai: Harmlessness from ai feedback.\" arXiv preprint arXiv:2212.08073 (2022).\n\n[2] Kundu, Sandipan, et al. \"Specific versus General Principles for Constitutional AI.\" arXiv preprint arXiv:2310.13798 (2023).\n\n[3] Sun, Zhiqing, et al. \"Principle-driven self-alignment of language models from scratch with minimal human supervision.\" arXiv preprint arXiv:2305.03047 (2023).\n\n[4] Tunstall, Lewis, et al. \"Zephyr: Direct Distillation of LM Alignment.\" arXiv preprint arXiv:2310.16944 (2023)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4299/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700616045110,
                "cdate": 1700616045110,
                "tmdate": 1700616045110,
                "mdate": 1700616045110,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]