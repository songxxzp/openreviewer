[
    {
        "title": "S4G: Breaking the Bottleneck on Graphs with Structured State Spaces"
    },
    {
        "review": {
            "id": "ODB0cisR2T",
            "forum": "0Z6lN4GYrO",
            "replyto": "0Z6lN4GYrO",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9207/Reviewer_qfiX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9207/Reviewer_qfiX"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors propose a new graph NN architecture, S4G, using structured state space. S4G maintains similar inductive biases induced by regular MPNN but has the sensitivity with a linear decay. In this way, the message bottleneck problem is largely alleviated. Unlike transformer-based architecture, S4G does not need positional encodings or positional structures, which usually require a heuristic design. Empirical study shows that S4G consistently has superior performance over long-range tasks, which also corroborates their theoretical claims."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is well-written for most of the part. \n2. The proposed method appears novel and provides an effective way to solve the message bottleneck problem while maintaining the inductive bias induced by regular MPNNs."
                },
                "weaknesses": {
                    "value": "The calculation of  $\\bar{K}$ could be expensive and requires preprocessing for efficient training. A discussion on how much time is needed to preprocess for different datasets could make the results stronger."
                },
                "questions": {
                    "value": "Please see the Weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9207/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9207/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9207/Reviewer_qfiX"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9207/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698048985822,
            "cdate": 1698048985822,
            "tmdate": 1699637158965,
            "mdate": 1699637158965,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YBTBBGwDzN",
                "forum": "0Z6lN4GYrO",
                "replyto": "ODB0cisR2T",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9207/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9207/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer qfiX"
                    },
                    "comment": {
                        "value": "We are grateful for the valuable feedback and encouraging comments received regarding our paper. Below, we offer detailed responses to address your inquiries.\n\n> (Weakness) The calculation of $\\bar{K}$ could be expensive and requires preprocessing for efficient training. A discussion on how much time is needed to preprocess for different datasets could make the results stronger.\n\nFollowing your suggestion, we report the pre-processing time for all datasets as follows.\n\n| **Time cost**     | **PATTERN** | **CLUSTER** | **MNIST** | **CIFAR10** | **PascalVOC-SP** | **COCO-SP** | **Peptides-func** | **Peptides-struct** | **PCQM-Contact** |\n| ----------------- | ----------- | ----------- | --------- | ----------- | ---------------- | ----------- | ----------------- | ------------------- | ---------------- |\n| **Per graph**     | 4.1ms       | 4.1ms       | 2ms       | 4.2ms       | 46ms             | 58ms        | 25ms              | 27ms                | 1.5ms            |\n| **Whole dataset** | 1min        | 0.8min      | 2.4min    | 4.2min      | 8.8min           | 118.4min    | 6.4min            | 7min                | 13.3min          |"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9207/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700248126977,
                "cdate": 1700248126977,
                "tmdate": 1700248126977,
                "mdate": 1700248126977,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QlWvk4nmty",
                "forum": "0Z6lN4GYrO",
                "replyto": "ODB0cisR2T",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9207/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9207/Authors"
                ],
                "content": {
                    "title": {
                        "value": "The end of the discussion phase approaching"
                    },
                    "comment": {
                        "value": "Dear Reviewer qfiX, as the discussion period comes to a close, we would like to thank you once again for your positive assessment. Your support and inspirational comments have been invaluable to us. We remain open and eager to incorporate any further feedback or insights you might offer."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9207/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700508386423,
                "cdate": 1700508386423,
                "tmdate": 1700508386423,
                "mdate": 1700508386423,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HxevgIZr3Y",
            "forum": "0Z6lN4GYrO",
            "replyto": "0Z6lN4GYrO",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9207/Reviewer_rkT5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9207/Reviewer_rkT5"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new model based on structured state spaces, termed S4G, for enabling better information flow in graph neural networks without losing the graph inductive bias. The fundamental idea builds on two observations: (1) Graph neural networks have strong relational inductive bias, but they are subject to an exponential decay in information with increasing number of layers and (2) Graph transformers are subject to only constant decay in information but they typically lack appropriate graph inductive biases. The idea is to use \"structured state spaces\" to capture the hierarchical structure of rooted-trees from the source nodes, and to keep strong inductive bias while having only linear decay of information. Authors present experimental results on long-range and general graph benchmarks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- **Problem setup**: It remains challenging to capture long-range interactions using graph neural networks for various reasons discussed in the paper, so the problem formulation is important and meaningful."
                },
                "weaknesses": {
                    "value": "- **Scholarship**: The paper fails to present a good coverage of the related work which also makes the contributions questionable. This is particularly the case with the coverage of recent multi-hop approaches. Authors mention that multi-hop models do not help with e.g. over-squashing as they rely on taking the powers of the adjacency matrix which amplifies the over-squashing problem. This is true, but this is exactly the reason why other multi-hop approaches have been studied extensively, see e.g. [1], where the idea is to directly aggregate information from higher-order neighbours obtained using shortest path distances (a sensitivity analysis is also conducted). \n\n- **Novelty, Originality, and Significance**: To the best of my understanding, the proposed idea of this paper appears to be largely covered by [1], since rooted trees are essentially constructed in the exact same way and aggregation is over the respective neighbourhoods $N_1...N_k$ of a particular node. Moreover, the graphormer model [2] follows essentially a very similar path: it aggregates over the neighbors at different shortest path distances directly. I do not see a fundamentally new or novel aspect in the present work, and neither a significant contribution. I'm happy to re-evaluate if the authors could better frame their approach in the existing literature and can identify the differences and contributions.\n\n- **Experiments on Long Range Graph Benchmarks**: The empirical results do appear promising, but unfortunately, the benchmark of Dwiwedi et al has been criticised recently [3] and it turns out that the gap between GNNs and graph transformers either disappears or becomes insignificant after a systematic tuning of the GNN models. This is a very recent finding and the current paper cannot be held responsible, but given that this is one of the two experiments conducted, the validity of the proposal remains questionable. It is also unclear whether the above-mentioned approaches, i.e., graphormers, would match the presented results. \n\n- **Technical limitations**: There are many limitations of the sensitivity analysis of Topping et al (and other approaches are proposed recently see eg  [4]). It requires bounded derivatives (which may not hold in practice) and also a normalised adjacency matrix. It is easy to see that without the assumption on the latter the values will explode rather than vanishing. On the other hand, it is easy to show that simple tricks (such as a fully connected layer, or adding a virtual node) can theoretically \"maximize\" this bound, and a systematic evaluation is needed against these simple model variations to clearly identify the benefit of the proposed idea.\n\n[1] Aboud et al. Shortest Path Networks for Graph Property Prediction. LoG 2022.\n\n[2] Ying et al., Do Transformers Really Perform Badly for Graph Representation? NeurIPS 2021.\n\n[3] T\u00f6nshoff et al,  Where Did the Gap Go? Reassessing the Long-Range Graph Benchmark. 2023.\n\n[4] Di Giovanni et al, How does over-squashing affect the power of GNNs? 2023."
                },
                "questions": {
                    "value": "Please refer to my review."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9207/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698765339295,
            "cdate": 1698765339295,
            "tmdate": 1699637158845,
            "mdate": 1699637158845,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "g0sy47CY8E",
                "forum": "0Z6lN4GYrO",
                "replyto": "HxevgIZr3Y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9207/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9207/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer rkT5 (Part 1/3)"
                    },
                    "comment": {
                        "value": "We appreciate your thorough feedback and insightful inquiries. After careful consideration of your critique, we are committed to addressing your concerns and presenting arguments that justify a higher score for the paper.\n\nBefore responding to your questions one by one, we would like to express our gratitude for mentioning SPN [1]. This work has a solid theoretical foundation and empirical analysis, and it addresses the issues encountered by multi-hop MPNN to a great extent, specifically the dependency on the power of the adjacency matrix, using a simple yet effective method. In our updated submission (please refer to the red-colored text), we introduce the works [1,2] you mentioned in our Introduction (paragraph 2 in Section 1). Furthermore, in the experimental part, we introduce SPN as a baseline (paragraph 1 in Section 4.2 and Section 4.3). We conducted experiments on both long-range and general datasets for SPN, and the results are presented in Table 2 and Table 3.\n\n> (Weakness 1) Scholarship: The paper fails to present a good coverage of the related work which also makes the contributions questionable. This is particularly the case with the coverage of recent multi-hop approaches. Authors mention that multi-hop models do not help with e.g. over-squashing as they rely on taking the powers of the adjacency matrix which amplifies the over-squashing problem. This is true, but this is exactly the reason why other multi-hop approaches have been studied extensively, see e.g. [1], where the idea is to directly aggregate information from higher-order neighbours obtained using shortest path distances (a sensitivity analysis is also conducted).\n\nThank you for mentioning SPN, which will enrich our related works. In our updated submission (please refer to the red-colored text), we introduced and cited SPN in the Introduction (paragraph 2 in Section 1). In the experimental section, we used SPN as a baseline and conducted experiments on long-range and general datasets (paragraph 1 in Section 4.2 and Section 4.3). The experimental results are presented in Table 2 and Table 3."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9207/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700248478405,
                "cdate": 1700248478405,
                "tmdate": 1700248492838,
                "mdate": 1700248492838,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "91YC44r9Dd",
                "forum": "0Z6lN4GYrO",
                "replyto": "HxevgIZr3Y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9207/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9207/Authors"
                ],
                "content": {
                    "title": {
                        "value": "The end of the discussion phase approaching"
                    },
                    "comment": {
                        "value": "Dear Reviewer rkT5, as the discussion period draws to a close, we would like to verify if our responses adequately address your inquiries. In light of what you've mentioned, we introduced [1,2] in our paper and conducted experiments for [1]. Additionally, we discussed the difference between our model and SPN [1] in detail. All the other questions received a direct response as well. We greatly appreciate your valuable feedback and recommendations to enhance our paper. We eagerly await your response.\n\n---\n\n[1] Shortest Path Networks for Graph Property Prediction, LoG 2022\n\n[2] Do Transformers Really Perform Badly for Graph Representation? NeurIPS 2021"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9207/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700508355452,
                "cdate": 1700508355452,
                "tmdate": 1700508355452,
                "mdate": 1700508355452,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "etpFpPf32K",
                "forum": "0Z6lN4GYrO",
                "replyto": "91YC44r9Dd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9207/Reviewer_rkT5"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9207/Reviewer_rkT5"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the additional work"
                    },
                    "comment": {
                        "value": "I appreciate the additional work by the authors. I understand the differences from SPNs and similar models, but I am not convinced by the virtue of the proposed approach. There is a need for a much more systematic study to make this convincing in terms of the information flow, expressive power, and alike. I do not find the reported results for SPN very consistent or reproducible either. In fact, some results look fairly low which may be due to poor choice of hyper-parameters and alike. We have no access to the hyper-parameters and not even to the distance parameter used in these experiments. All in all, I do not see a good reason to change my initial assessment of the paper."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9207/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700549133011,
                "cdate": 1700549133011,
                "tmdate": 1700549133011,
                "mdate": 1700549133011,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HmqmPG1ym7",
            "forum": "0Z6lN4GYrO",
            "replyto": "0Z6lN4GYrO",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9207/Reviewer_SDke"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9207/Reviewer_SDke"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new architecture for graph representation learning to address the limitations of message passing neural networks. Specifically, to generate the representation for a target node, representations of nodes that are at the same hop are summed up, and then a structured state space model (S4) is applied to the sequence of hop representations. \nThis paper demonstrates the long-range modeling capacity of the proposed architecture by analyzing the sensitivity between distant nodes. The proposed model shows good empirical performance on a series of graph benchmarks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The idea of extending S4 to graphs is interesting and novel to the graph ML domain.\n- The sensitivity analysis is intuitive and clear to explain why S4 can help to utilize the information of distant nodes.\n- The model shows good empirical performance."
                },
                "weaknesses": {
                    "value": "- Over-smoothing and over-squashing: The paper claims that the proposed model can address both over-smoothing and over-squashing. However, over-smoothing is caused by the lack of local neighborhood information. S4 aims to better capture distant information, which seems to be in the opposite direction of addressing over-smoothing. The experiments didn\u2019t touch over-smoothing either.\n- Lack of expressiveness analysis: By converting a neighborhood into a sequence, the model considers the shortest distance, but would inevitably lose other structural information. E.g., the model doesn\u2019t know the edges between hop $k-1$ and hop $k$, i.e., for a certain node at hop $k-1$ which nodes at hop $k$ are connected to it. This questions the expressiveness of the proposed model. Theoretical analysis is necessary to justify the expressiveness and support the empirical results, but it\u2019s missing.\n- According to the experiments, the performance of S4G itself is not good enough on several datasets, while S4G+, which contains an extra MPNN layer, can do significantly better. This implies that S4G itself may lose some structural information (together with the above point)"
                },
                "questions": {
                    "value": "- More details of the experimental setup should be given, such as hyper-parameters (e.g., is the hidden dimension very large? (since the HIPPO matrix is fixed))\n- What is the specific difference for those baselines with asterisk? Did the proposed model follow the asterisk setting?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9207/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698839972920,
            "cdate": 1698839972920,
            "tmdate": 1699637158734,
            "mdate": 1699637158734,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "75ZoZuZbDi",
                "forum": "0Z6lN4GYrO",
                "replyto": "HmqmPG1ym7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9207/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9207/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer SDke (Part 1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your detailed comments and valuable questions. We have carefully considered your criticism and aim to persuade you that the paper deserves a higher score by addressing your concerns.\n\n> (Weakness 1) Over-smoothing and over-squashing: The paper claims that the proposed model can address both over-smoothing and over-squashing. However, over-smoothing is caused by the lack of local neighborhood information. S4 aims to better capture distant information, which seems to be in the opposite direction of addressing over-smoothing. The experiments didn\u2019t touch over-smoothing either.\n\n- Our paper mentions the over-smoothing problem, but the focus of this paper is on the over-squashing and long-range problems (as emphasized in our title, abstract, and conclusion). The over-smoothing problem has been largely addressed in previous works, so we did not design experiments for it.\n\n- S4G is not on the opposite side of over-smoothing: S4G encodes neighbors from low to high order into a sequence. Based on the memory capacity of the HiPPO framework [1], the model avoids forgetting local information, thus avoiding the over-smoothing problem. On the other hand, the main cause of the over-smoothing problem is that MPNN is similar to Laplacian smoothing [2], which leads to multiple layers of MPNN degenerating into a low-pass filter on the graph [3]. S4G does not perform message passing on the original graph structure, so it can avoid such degeneration.\n\n- It is worth noting that we have already used a 16-layer model on our tested CLUSTER dataset, a general node classification dataset. This indicates that the performance does not decrease with depth and avoids over-smoothing issues. To further alleviate your concerns, we tested the performance of S4G from 2 to 10 layers on another node classification dataset we tested, i.e., PATTERN. The results are as follows, the performance remains stable as the number of layers increases.\n\n    | **# Layers** | **2** | **4** | **6** | **8** | **10** |\n    | ------- | ------- | ------- | ------- | ------- | ------- |\n    | **S4G** | $86.72 \\pm 0.03$ | $86.87 \\pm 0.02$ | $86.83 \\pm 0.04$ | $86.85 \\pm 0.02$ | $86.85 \\pm 0.03$ |\n\n> (Weakness 2) Lack of expressiveness analysis: By converting a neighborhood into a sequence, the model considers the shortest distance, but would inevitably lose other structural information. E.g., the model doesn\u2019t know the edges between hop $k-1$ and hop $k$, i.e., for a certain node at hop $k-1$ which nodes at hop $k$ are connected to it. This questions the expressiveness of the proposed model. Theoretical analysis is necessary to justify the expressiveness and support the empirical results, but it\u2019s missing.\n\n- Our paper did not focus on the expressivity problem, we did not aim to design a expressive GNN in this work. However, we mentioned it in the conclusion and already provided some analysis in Appendix B.\n\n- Our model has stronger expressive power than MPNN. The following is a brief proof, and a comprehensive exploration of the expressive power of S4G will be part of future work.\n    - The information captured by S4G is more abundant than MPNN: In MPNN, under each node's perspective, each message passing aggregating information from the first-order star-shaped graph [4]. By contrast, S4G not only aggregates information from first-order neighbors but also from higher-order neighbors. Therefore, S4G can obtain more abundant information.\n    - There exists a pair of graphs that MPNN cannot distinguish but S4G could (this example is borrowed from [5]): Consider two uncolored graphs, G1 and G2, where G1 is a hexagon and G2 consists of two disconnected triangles. With the shortest path information encoded in the SSM kernel, S4G can differentiate between G1 and G2, while the 1-WL test cannot. Since the 1-WL test is the upper bound of the expressive power of MPNN, MPNN also cannot distinguish them."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9207/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700248274295,
                "cdate": 1700248274295,
                "tmdate": 1700248274295,
                "mdate": 1700248274295,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fhBuJ0eEoi",
                "forum": "0Z6lN4GYrO",
                "replyto": "HmqmPG1ym7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9207/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9207/Authors"
                ],
                "content": {
                    "title": {
                        "value": "The end of the discussion phase approaching"
                    },
                    "comment": {
                        "value": "Dear Reviewer SDke, as the discussion period ends soon, we would like to check whether our responses answer your questions. Following your comments, we conducted experiments to test the model's performance when the number of layers increases. Additionally, we provided a brief proof for the expressivity of S4G. All the other questions received a direct response as well. Thank you again for your comments and suggestions to improve our paper, and we look forward to your reply."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9207/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700508325807,
                "cdate": 1700508325807,
                "tmdate": 1700508325807,
                "mdate": 1700508325807,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4z0WtHD020",
                "forum": "0Z6lN4GYrO",
                "replyto": "HmqmPG1ym7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9207/Reviewer_SDke"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9207/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9207/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9207/Reviewers",
                    "ICLR.cc/2024/Conference/Submission9207/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission9207/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9207/Reviewer_SDke"
                ],
                "content": {
                    "title": {
                        "value": "Comment by Reviewer"
                    },
                    "comment": {
                        "value": "Thank you for further clarification! However, my major concern, i.e., theoretical analysis of expressiveness which is necessary for a sequential model over graph-structured data, is not addressed. Therefore, I still don't recommend acceptance."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9207/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700739742915,
                "cdate": 1700739742915,
                "tmdate": 1700739890204,
                "mdate": 1700739890204,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bJaURaj6et",
                "forum": "0Z6lN4GYrO",
                "replyto": "HmqmPG1ym7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9207/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9207/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer SDke"
                    },
                    "comment": {
                        "value": "Dear Reviewer SDke, thank you for your response. It is important to emphasize that our research does not specifically delve into the expressive power of GNNs. The investigation into GNN expressive power stands as an orthogonal research direction to our focus. Our primary focus centers around addressing challenges related to long-range interactions (LRI) and over-squashing on graphs. Discussing the expressive power of S4G at length would deviate from our research question. As evidence, existing works that focus on the over-squashing issue and LRI, such as [1,2,3,4], have also not discussed the issue of expressive power in depth, work [1,2] are theoretical works, while [3,4] design models. Nevertheless, our response already demonstrates that our model is more powerful compared to MPNNs.\n\n---\n\n[1] Understanding over-squashing and bottlenecks on graphs via curvature, ICLR 2022\n\n[2] On Over-Squashing in Message Passing Neural Networks: The Impact of Width, Depth, and Topology, ICML 2023\n\n[3] Drew: Dynamically rewired message passing with delay, ICML 2023\n\n[4] Exphormer: Sparse transformers for graphs, ICML 2023"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9207/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740782774,
                "cdate": 1700740782774,
                "tmdate": 1700741421696,
                "mdate": 1700741421696,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]