[
    {
        "title": "Delving into LLMs\u2019 visual understanding ability using SVG to bridge image and text"
    },
    {
        "review": {
            "id": "1mNxFRmWZH",
            "forum": "pwlm6Po61I",
            "replyto": "pwlm6Po61I",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4185/Reviewer_oVfB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4185/Reviewer_oVfB"
            ],
            "content": {
                "summary": {
                    "value": "This paper explores the capability of pre-trained LLM in understanding images through converting images into readable text via SVG. The authors evaluate both discriminative and generative visual understanding tasks including image classification, image generation and editing."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The study of whether LLMs, which have never seen visual data, can understand and reason about images is interesting and novel.\n2. This paper utilizes SVG to convert images into structured XML codes, which serves as a bridge to apply the analytical strengths of LLMs to the visual domain.\n3. This paper evaluates both discriminative and generative visual understanding tasks of LLMs, and draws some interesting conclusions, which can be inspiring."
                },
                "weaknesses": {
                    "value": "1. For the evaluation of Multimodal LLMs, the authors only experiment on LLaVa and conclude that \"This behavior underscores the limitations of current large multimodal models in structured and sophisticated reasoning.\", which is not rigorous. The authors should evaluate more Multimodal LLMs such as BLIP-2, InstructBLIP, MiniGPT-4,mPLUG-Owl, etc.\n2. The data for the evaluation of both the discriminative and generative visual understanding tasks are relatively simple. How does the LLM perform with SVG representations on more complicated images? Since the authors also mention that SVG is not effective in handing photographic content, using SVG with LLMs to tackle visual tasks may not be effective enough."
                },
                "questions": {
                    "value": "See Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "In my opinion, no ethics review is needed."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4185/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4185/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4185/Reviewer_oVfB"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4185/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697969202612,
            "cdate": 1697969202612,
            "tmdate": 1699636384847,
            "mdate": 1699636384847,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "09sy0u0yDV",
                "forum": "pwlm6Po61I",
                "replyto": "1mNxFRmWZH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4185/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4185/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer oVfB"
                    },
                    "comment": {
                        "value": "Dear Reviewer oVfB, \n\n\nWe are grateful for your acknowledgment of our study\u2019s novelty, and comprehensive evaluation of both discriminative and generative visual understanding of LLMs. For your questions\u2014we'll make revisions accordingly.\n\n> \u201cThe authors should evaluate more Multimodal LLMs such as BLIP-2, InstructBLIP, MiniGPT-4, mPLUG-Owl, etc\u201d\n\nWe appreciate your suggestion. We have expanded our comparison in the table to include additional Multimodal LLMs. The updated results indicate that combining SVG with LLMs outperforms other models like BLIP-2, InstructBLIP, MiniGPT-4, and mPLUG-Owl in handling complex, structured reasoning tasks.\n\n| Question Type | GPT-CoT | LLaVa | CNN+MLP | Relation Networks | InstructBLIP (13b) | BLIP2 (Flan T5-xxl) | mPLUG_owl | MiniGPT4 (13B) |\n|---------------|---------|-------|---------|-------------------|--------------------|---------------------|-----------|----------------|\n| Format        | SVG     | PNG   | PNG     | PNG               | PNG                | PNG                 | PNG       | PNG            |\n| Unary         | 0.90    | 0.60  | 0.65    | 0.89              | 0.53               | 0.50                | 0.38      | 0.53           |\n| Binary        | 0.95    | 0.60  | 0.75    | 0.80              | 0.53               | 0.53                | 0.63      | 0.55           |\n| Ternary       | 0.88    | 0.10  | 0.55    | 0.55              | 0.10               | 0.30                | 0.30      | 0.30           |\n| Average       | 0.89    | 0.43  | 0.65    | 0.75              | 0.38               | 0.44                | 0.43      | 0.46           |\n\n_Caption: Performance comparison across different models on the Sort-of-Clever Dataset._\n\n> \u201cHow does the LLM perform with SVG representations on more complicated images?\u201d\n\nThank you for raising this concern. We agree that the current approach of integrating SVG with LLMs for complex visual tasks may fall short, primarily due to the loss of fine details in converting and the lengthy sequences of encoding extensive details. To tackle this, a potential solution is to leverage large vision models to extract the essence of images and convert that essence into the SVG format for LLM processing. An example is utilizing SAM [1] for object mask segmentation, which is then translated to SVG for LLMs, as depicted in Figure 6 (d). We then show a recognition result in Figure 7 (b).\n\n[1] Kirillov, Alexander, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao et al. \"Segment anything.\" ICCV (2023)."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4185/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700521935947,
                "cdate": 1700521935947,
                "tmdate": 1700521935947,
                "mdate": 1700521935947,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rR2QcFeFgQ",
                "forum": "pwlm6Po61I",
                "replyto": "1mNxFRmWZH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4185/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4185/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to your reply!"
                    },
                    "comment": {
                        "value": "Dear reviewer oVfB,\n\nThank you for reviewing our work to enhance the quality of the paper!\nHas our rebuttal adequately addressed your concerns? If you still have any issues with our rebuttal or if there are any new concerns, we are more than willing to continue the discussion with you."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4185/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631943064,
                "cdate": 1700631943064,
                "tmdate": 1700631943064,
                "mdate": 1700631943064,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5c9Nwog4gy",
                "forum": "pwlm6Po61I",
                "replyto": "09sy0u0yDV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4185/Reviewer_oVfB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4185/Reviewer_oVfB"
                ],
                "content": {
                    "comment": {
                        "value": "I have read the reponse and keep my rating."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4185/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700720936346,
                "cdate": 1700720936346,
                "tmdate": 1700720936346,
                "mdate": 1700720936346,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "s9orMk85Bl",
            "forum": "pwlm6Po61I",
            "replyto": "pwlm6Po61I",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4185/Reviewer_Vo5P"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4185/Reviewer_Vo5P"
            ],
            "content": {
                "summary": {
                    "value": "The paper explores the potential of Large Language Models (LLMs) to understand and process images by converting them into Scalable Vector Graphics (SVG), which can be represented as an XML-based textual description. The LLM's abilities are tested across three vision tasks: visual reasoning, image classification under distribution shifts, and generating new images based on visual prompts. The results show that the LLM has a reasonable ability to perform these tasks, especially compared with some expert models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The use of SVGs to turn images into text is an innovative way of exploiting the potential of LLMs for image processing tasks.\n2. The paper conducts both qualitative and quantitative evaluations over a variety of visual understanding tasks providing deep insights into the LLMs' image understanding capabilities. The results indicate that LLMs can perform well even when there are distribution shifts in visual data, demonstrating robustness. The demonstration of LLMs' ability to generate and edit images based on chat-based feedback is particularly promising and marks a significant advancement."
                },
                "weaknesses": {
                    "value": "1. The SVG representation may not capture all the nuances of an image. It is yet unclear how well this method would generalize to more complex images or visual tasks.\n2. The issue raised in the introduction about whether LLMs can learn world models without grounding in physical interaction and visual perception remains unaddressed. A comparison of LLMs' performance using SVGs with traditional vision-based models could have provided more context about the relative effectiveness of this approach."
                },
                "questions": {
                    "value": "1. How well do SVG representations capture complex images? Could you provide instances or examples where this approach has limitations?\n2. Whether the format, sequence, or other characteristics of textual descriptions in Scalable Vector Graphics (SVG) will affect their subsequent task performance.\n3. What could be the potential impact if a minor percentage of errors into the representation of SVGs is introduced?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4185/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4185/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4185/Reviewer_Vo5P"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4185/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698785737580,
            "cdate": 1698785737580,
            "tmdate": 1700731456226,
            "mdate": 1700731456226,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fFD7ZnKSKW",
                "forum": "pwlm6Po61I",
                "replyto": "s9orMk85Bl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4185/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4185/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Vo5P (Part I)"
                    },
                    "comment": {
                        "value": "Dear Reviewer Vo5P,\n\nWe are grateful for your acknowledgment of our study\u2019s novelty, comprehensive evaluation, and promising results. For your questions; we'll make revisions accordingly.\n\n> \"How well do SVG representations capture complex images? Could you provide instances or examples where this approach has limitations?\"\n\nThank you for pointing out this concern. We acknowledge the limitations of using SVG with LLMs for complex visual tasks, due to the loss of fine-grained details during conversion and the challenge of managing prohibitively long sequences when encoding detailed photographic content. To tackle this, a potential solution is to leverage large vision models to extract the essence of images and convert that essence into the SVG format for LLM processing. An example is utilizing SAM [1] for object contour segmentation, which is then translated to SVG for LLMs, as depicted in Figure 6 (d). We then show a recognition result in Figure 7 (b).\n\n> \"A comparison of LLMs' performance using SVGs with traditional vision-based models could have provided more context about the relative effectiveness of this approach.\"\n\nThank you for your suggestion. We'd like to clarify that in our paper, we have compared the performance of LLMs using SVGs with traditional vision-based models. This includes:\n\n- Visual reasoning tasks in Table 1, where LLMs with SVGs outperform SoTA vision-based models in both in-distribution and out-of-distribution scenarios.\n\n- Image classification tasks in Table 2, showing LLMs with SVGs again surpassing SoTA vision-based models under similar conditions.\n\n- Visual prompting in Table 3, where LLMs with SVGs demonstrate superior performance compared to SoTA vision-based models.\n\n\n[1] Kirillov, Alexander, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao et al. \"Segment anything.\" ICCV (2023)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4185/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700525424035,
                "cdate": 1700525424035,
                "tmdate": 1700525424035,
                "mdate": 1700525424035,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mfdU0ekTNH",
                "forum": "pwlm6Po61I",
                "replyto": "s9orMk85Bl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4185/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4185/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Vo5P (Part II)"
                    },
                    "comment": {
                        "value": "> \"Whether the format, sequence, or other characteristics of textual descriptions in Scalable Vector Graphics (SVG) will affect their subsequent task performance\" \n\nand \n\n> \"What could be the potential impact if a minor percentage of errors into the representation of SVGs is introduced?\"\n\nThanks for both of your suggestions. To rigorously evaluate the robustness of LLMs against variations in SVG data, we conducted three distinct experiments, where we: (i) shuffle the order of paths, (ii) randomize path coordinate replacement, and (iii) randomize string replacement. Each experiment was designed to mimic real scenarios of imperfections in SVG data, providing insights into using SVG with LLMs under challenging conditions.\n\n__(i) Path Shuffle Experiment__ SVG data consists of multiple path elements, each representing an object or line in the image. In this experiment, we tested the model's ability to interpret hand-drawn SVG data from the MNIST dataset when the sequence of path elements was shuffled. Note that, by the very nature of SVG data, this alteration will __not__ change the final image; and hence, it is important to test if the LLM can remain invariant to this change, even though we have not explicitly provided it with this knowledge. \nThe goal is to assess the model's ability to correctly interpret the digit, regardless of the order of path elements. The results are summarized in the table below. Results indicate that SVG is robust to path shuffle at test time, maintaining a comparable performance compared to the SVG representation with the original path ordering.\n\n\n| Condition       | Accuracy (%) |\n|-----------------|--------------|\n| Without Shuffle | 99.10        |\n| With Shuffle    | 98.74        |\n\n_Caption: Model accuracy for the path shuffle experiment._\n\n__(ii) Random Path Coordinate Replacement__\nWe follow the reviewer's suggestion and introduce a subtle form of noise by randomly altering the coordinates in the SVG path data. Each numerical value within the path commands is randomly translated within a specific range $k$. For example, for $k=1$,   `... <path d='M0 0 C18 0 17...'>` might become `... <path d='M1 0 C18 1 18...'>`. We test several variations: (i) a minor adjustment within a 1/28th range (reflecting the 28x28 resolution of MNIST) and (ii) a more significant alteration within a 5/28th range. This simulates real-world scenarios of minor inaccuracies in SVG data, such as those resulting from conversion errors or imprecise digitization.\nTable shown below presents the accuracy under different noise scales. Results indicate data SVG representation is decently robust to the perturbation of the coordinate values. \n\n| Noise Scale | Accuracy (%) |\n|-------------|--------------|\n| 0/28        | 99.10        |\n| 1/28        | 98.97        |\n| 2/28        | 97.91        |\n| 5/28        | 87.56        |\n\n_Caption: Model accuracy under different levels of coordinate noise._\n\n__(iii) Random String Replacement__\nFinally, we design the most aggressive test of robustness, where we replace random characters in the SVG strings with any English alphabet letter, digit, or special symbol, regardless of whether they are numerical values or part of SVG commands (like `transform='translate(0,0)'`). The experiment is conducted with varying probabilities for character replacement, as shown below.\nSurprisingly, even after replacing 20\\% of a string with random characters, SVG with LLMs maintains a high accuracy rate of 90.79\\%. This suggests that SVG with LLMs can handle a wide range of perturbations in SVG data.\n\n\nThe following table presents the accuracy with varying probabilities of random string replacement:\n\n| Replacement Probability (%) | Accuracy (%) |\n|-----------------------------|--------------|\n| 0                           | 99.10        |\n| 1                           | 99.06        |\n| 5                           | 98.40        |\n| 10                          | 97.40        |\n| 20                          | 90.79        |\n| 50                          | 39.38        |\n\n_Caption: Accuracy with different probabilities of random string replacement._\n\nIn conclusion, the results from these experiments demonstrate the robustness of using SVG data with LLMs for visual understanding. Despite the introduction of perturbations, using SVG data with LLMs can perform well under challenging conditions."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4185/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700525468304,
                "cdate": 1700525468304,
                "tmdate": 1700525468304,
                "mdate": 1700525468304,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SYHSJ77Wga",
                "forum": "pwlm6Po61I",
                "replyto": "s9orMk85Bl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4185/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4185/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to your reply!"
                    },
                    "comment": {
                        "value": "Dear reviewer Vo5P,\n\nThank you for reviewing our work to enhance the quality of the paper!\nHas our rebuttal adequately addressed your concerns? If you still have any issues with our rebuttal or if there are any new concerns, we are more than willing to continue the discussion with you."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4185/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631896180,
                "cdate": 1700631896180,
                "tmdate": 1700726214960,
                "mdate": 1700726214960,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Sz3zX7AMO7",
                "forum": "pwlm6Po61I",
                "replyto": "SYHSJ77Wga",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4185/Reviewer_Vo5P"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4185/Reviewer_Vo5P"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your diligent response in addressing the issues I previously raised. Your efforts have not gone unnoticed and I appreciate the time you've spent on this task. As a result of these changes, I've decided to adjust my rating to a 6."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4185/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700731428038,
                "cdate": 1700731428038,
                "tmdate": 1700731428038,
                "mdate": 1700731428038,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "UAaK4pwPxK",
            "forum": "pwlm6Po61I",
            "replyto": "pwlm6Po61I",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4185/Reviewer_uhQW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4185/Reviewer_uhQW"
            ],
            "content": {
                "summary": {
                    "value": "They use Scalable Vector Graphics for using LLM for image understanding. The authors provide various experiments, including both discriminative and generative visual understanding tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. **Experimental Rigor:** The authors present a comprehensive suite of experiments in the domain of visual understanding and reasoning that effectively leverages the combination of Scalable Vector Graphics (SVG) with large language models (LLMs). The breadth and depth of the experimental design are commendable and provide valuable insights into the capabilities of LLMs in processing vector-based graphic representations.\n\n2. **Robustness to Distribution Shifts:** A significant strength of the paper is the demonstration of the robustness of the LLM + SVG approach under conditions of distributional shifts. \n\n3. **Clarity and Coverage of Related Works:** The paper is well-articulated, presenting its concepts and findings in a manner that is accessible to readers. Moreover, the authors have done a thorough job in situating their work within the context of existing literature. They have effectively covered pertinent related works."
                },
                "weaknesses": {
                    "value": "1. **Component Originality:** While the experiments are detailed, the novelty of the individual components used within the work is unclear. The application of Scalable Vector Graphics (SVG), Large Language Models (LLMs), the dataset selection, the tasks, and the chosen evaluation metrics all appear to be repurposed from existing literature without significant innovation or new application. For a stronger contribution, the authors could benefit from integrating at least one novel element or a unique combination of these elements that distinguishes this work from prior studies.  \n\n\n2. **Source of Performance Discrepancy:** The comparative results between LLM+SVG and CNN+PNG in the first experiment are intriguing but lack a clear explanation. The performance gap could be attributed to differences in input representation or model architecture, among other factors. An in-depth ablation study could help isolate the impact of each component. Furthermore, the disparity in model sizes (e.g., GPT-4's size relative to that of a CNN) is a confounding variable that merits consideration. To convincingly argue the advantages of using SVG for image representation in LLMs, the authors should compare SVG with alternative representations using the same underlying model architecture, such as GPT-4.  \n\n\n3. **Dataset and Task Relevance:** The chosen dataset and task do not seem to reflect complex real-world scenarios and may inherently favor SVG representations due to their structured nature. This could introduce a bias towards the advantages of SVG in image understanding tasks that focus on structural rather than fine-grained details. The authors would enhance the robustness of their findings by including a broader range of tasks with varying degrees of complexity and realism to demonstrate the efficacy of SVG representations across different contexts.  \n\n\n4. **Methodological Limitations:** The exploration of image understanding through LLMs is indeed crucial; however, the analysis presented in this work is constrained by its reliance on existing methodologies. The absence of innovative input representations, model adaptations, or new evaluation benchmarks suggests a missed opportunity for advancing the field. To forge a more impactful contribution, the authors should strive to develop and introduce novel methodologies or significantly adapt existing ones to the specific challenges of image understanding through LLMs."
                },
                "questions": {
                    "value": "Refer to part of Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4185/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699364974104,
            "cdate": 1699364974104,
            "tmdate": 1699636384623,
            "mdate": 1699636384623,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tCDJ5n1lNU",
                "forum": "pwlm6Po61I",
                "replyto": "UAaK4pwPxK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4185/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4185/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer uhQW (Part I)"
                    },
                    "comment": {
                        "value": "Dear Reviewer uhQW,\n\n\n\nWe are grateful for your acknowledgment of our study\u2019s comprehensive evaluation, and demonstration of the robustness of the LLM + SVG under distribution shift. Below, we respond to your specific questions.\n\n\n> \u201cWhile the experiments are detailed, the novelty of the individual components used within the work is unclear\u201d\n\n\nWe wish to clarify that our study primarily aims to provide a thorough analysis of the strengths and weaknesses of employing LLMs to understand and reason about images without visual data. Below, we detail the novelty of each component in our study.\n\n\n- Comprehensive study of LLMs across various visual tasks using SVG format, including visual reasoning, image classification, and image generation.\n- Investigation into LLMs' performance in visual data using SVG format under conditions of distribution shifts, expanding our understanding of their robustness.\n- Analysis of LLMs' generative capabilities in image generation and editing based on interactive feedback within the SVG format.\n\n> \u201cThe comparative results between LLM+SVG and CNN+PNG in the first experiment are intriguing but lack a clear explanation. ... isolate the impact of each component.\u201d\n\nThank you for your suggestion. Following your advice, we conducted additional experiments to assess the impact of different input formats using the same model, as shown in the table below. Specifically, we compare the SVG input format with GPT4 against PNG input format with GPT4V (likely larger than GPT4 due to its visual encoding network component) for a balanced evaluation. The results indicate that using SVG with GPT4 significantly outperforms using PNG with GPT4V, in complex, structured reasoning tasks.\n\n\n\n| Question Type | GPT-CoT | GPT-4V | LLaVa | CNN+MLP | Relation Networks | InstructBLIP (13b) | BLIP2 (Flan T5-xxl) | mPLUG_owl | MiniGPT4 (13B) |\n|---------------|---------|--------|-------|---------|-------------------|--------------------|---------------------|-----------|----------------|\n| Format        | SVG     | PNG    | PNG   | PNG     | PNG               | PNG                | PNG                 | PNG       | PNG            |\n| Unary         | 0.90    | 0.75   | 0.60  | 0.65    | 0.89              | 0.53               | 0.50                | 0.38      | 0.53           |\n| Binary        | 0.95    | 0.74   | 0.60  | 0.75    | 0.80              | 0.53               | 0.53                | 0.63      | 0.55           |\n| Ternary       | 0.88    | 0.28   | 0.10  | 0.55    | 0.55              | 0.10               | 0.30                | 0.30      | 0.30           |\n| Average       | 0.89    | 0.59   | 0.43  | 0.65    | 0.75              | 0.38               | 0.44                | 0.43      | 0.46           |\n\n_Caption: Performance comparison across different models on the Sort-of-Clever Dataset._\n\n\n\n\n\nAdditionally, to ensure a fair comparison, we compared the performance of both SVG and PNG inputs using the same Vicuna model for image classification in the table below \nAs shown in the following figure, under the same model architecture, results indicate that SVG is more robust than rasterized representation. \n\n\n\n\n\n| Method                 | ConvNeXt (fine-tuning) | Vicuna (fine-tuning) | Vicuna (fine-tuning) |\n|------------------------|------------------------|----------------------|----------------------|\n| Image Format           | PNG                    | SVG                  | PNG                  |\n| MNIST                  | 99.5%                  | 99.1%                | 99.4%                |\n| CMNIST-(A)             | 79.5%                  | 95.7%                | 42.9%                |\n| CMNIST-(B)             | 32.6%                  | 92.9%                | 24.8%                |\n\n_Caption: Performance comparison across different models on the MNIST Dataset._\n\n\n\n\n> \u201cThe chosen dataset and task do not seem to reflect complex real-world scenarios and may inherently favor SVG representations due to their structured nature.\u201d\n\n\n\nWe thank the reviewer for raising and highlighting an important point. First, we would like to re-emphasize the main goal of our work: to see how much knowledge LLMs already have in understanding visual data, even though one might not naturally assume them to be suited for any vision task. Therefore, any experiments that we conduct (e.g., Table 1) should be seen as a way to study that question, and _not necessarily_ as a way to demonstrate that LLMs are fundamentally superior to CNN/transformers. And since, to the best of our knowledge, our work is the first to thoroughly study this hidden capability of LLMs, we have chosen the simpler settings. Our hope, however, is that our work can motivate investigations into other, more complex, vision tasks that can be solved with the help of an LLM."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4185/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700527503370,
                "cdate": 1700527503370,
                "tmdate": 1700614364547,
                "mdate": 1700614364547,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pvD2lnZJsN",
                "forum": "pwlm6Po61I",
                "replyto": "UAaK4pwPxK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4185/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4185/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer uhQW (Part II)"
                    },
                    "comment": {
                        "value": "> \u201cThe analysis presented in this work is constrained by its reliance on existing methodologies. The absence of innovative input representations, model adaptations, or new evaluation benchmarks suggests a missed opportunity for advancing the field.\u201d\n\n\nWe recognize that while our study relies on established techniques, these techniques are sufficiently effective for practical use in simple scenarios. We agree that addressing complex real-world images may necessitate the development of new methods. We are grateful for being the pioneers to start the investigation of LLMs for understanding and reasoning about images without relying on visual data, and we are committed to advancing our research to tackle real-world challenges more effectively."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4185/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700527522850,
                "cdate": 1700527522850,
                "tmdate": 1700527522850,
                "mdate": 1700527522850,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Yp0Cv8d316",
                "forum": "pwlm6Po61I",
                "replyto": "UAaK4pwPxK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4185/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4185/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to your reply!"
                    },
                    "comment": {
                        "value": "Dear reviewer uhQW,\n\nThank you for reviewing our work to enhance the quality of the paper!\nHas our rebuttal adequately addressed your concerns? If you still have any issues with our rebuttal or if there are any new concerns, we are more than willing to continue the discussion with you."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4185/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631853931,
                "cdate": 1700631853931,
                "tmdate": 1700631853931,
                "mdate": 1700631853931,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9TxEKXSUWH",
                "forum": "pwlm6Po61I",
                "replyto": "Yp0Cv8d316",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4185/Reviewer_uhQW"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4185/Reviewer_uhQW"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for rebuttal"
                    },
                    "comment": {
                        "value": "Thanks for your detailed answer and additional experiments. \nMy concern about the impact of each component is addressed. \nHowever, I still think the tasks and benchmarks are biased to the SVG + LLM setting. \nIt would be great if you\n1) introduce a new dataset or benchmark for evaluating LLMs\u2019 visual understanding ability, OR\n2) utilize real-world tasks or benchmarks to evaluate LLMs\u2019 visual understanding ability.\n\nSo the effort of the work is clear, but its experiments (task, dataset, benchmark) are not suitable for evaluating LLMs\u2019 visual understanding ability."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4185/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700720273436,
                "cdate": 1700720273436,
                "tmdate": 1700720273436,
                "mdate": 1700720273436,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ykcI5E4uKg",
                "forum": "pwlm6Po61I",
                "replyto": "UAaK4pwPxK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4185/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4185/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Glad that we have addressed your aforementioned concerns!\n\nYour suggestion to evaluate using SVG with LLMs on more real-world datasets is greatly appreciated. We further studied the limitations of using SVG with LLMs in handling complex real-world tasks in the following. \n\n\n__1)__ \n\nDue to limited computational resources, we performed experiments involving a subset of ImageNet. Specifically, we conduct experiments on [Imagenette](https://github.com/fastai/imagenette), which is composed of images from 10 classes in ImageNet. We achieved 68.14\\% top-1 accuracy on the corresponding 10-class test set after training with SVG format using Vicuna-7B model, whereas a ResNet50 model receives 90.62\\% accuracy on the test set after trained with 200 epochs.\n\n\nOur initial results suggest that LLMs can somehow understand the semantics of the real-world image by leveraging SVG, yet lag behind raster representations using the vision model architectures like ResNet with a clear gap. We believe it's mainly because of the loss of fine-grained details during SVG conversion. We have added your suggested experiments to our paper to further underscore the limitation of using SVG with LLMs in understanding complicated visual tasks, and we are committed to expanding our experiments with more categories and varied datasets in the final version.\n\n\n__2)__ \n\nAdditionally, we agree we\u2019ve selected some datasets/tasks that might favor SVG, but we observed that even with the advanced large vision model ConvNext, performance on the simple CMNIST dataset (Table 2) deteriorates significantly under out-of-distribution conditions. Utilizing the SVG format, in contrast, might offer a complementary solution to large vision models for better handling of such out-of-distribution scenarios."
                    },
                    "title": {
                        "value": "Thanks for your reply!"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4185/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700725734868,
                "cdate": 1700725734868,
                "tmdate": 1700725756273,
                "mdate": 1700725756273,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]