[
    {
        "title": "Extending to New Domains without Visual and Textual Oracles"
    },
    {
        "review": {
            "id": "DWjyectBEf",
            "forum": "tG5mpAM7ZK",
            "replyto": "tG5mpAM7ZK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4755/Reviewer_GcYV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4755/Reviewer_GcYV"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors introduce an approach aimed at generalizing from a single domain, represented by natural object images, to previously unseen domains, including objects presented as paintings and sketches. Their method involves shifting visual embeddings using relative differences of textual embeddings from the original domain (a picture of a cat) to potential unseen test domains (a painting of a cat), thereby enabling feature-space augmentations.\n\nThe authors conduct evaluations of their proposal on datasets such as CUB and DomainNet. The results show minor improvements over the competitive LADS baseline. This work addresses an important challenge in domain adaptation by expanding the model's capabilities to novel domains, demonstrating its potential in real-world scenarios involving diverse visual data representations."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "S1:\nThe paper is generally well-written and easy to follow, although there are instances where it may overly emphasize LADS rather than the paper's ultimate goal. Despite this, it remains a good read overall, providing a clear presentation of the research.\n\nS2:\nThe paper takes on a highly challenging task: single-domain generalization. In the context of this demanding objective, CLIP has established itself as a robust tool, and this paper contributes to this ongoing trend."
                },
                "weaknesses": {
                    "value": "W1 Method:\n\nA significant concern arises regarding the similarity between this paper and CLIP the GAP (CGAP) [1], which the authors fail to acknowledge or cite. Both papers share fundamental observations: that various domains of the same object differ by constant vectors. Additionally, they both tackle the challenge of single-domain generalization. The methodological resemblance is evident: CGAP performs semantic augmentations by computing relative differences in original and target test domains of text embeddings, using these as pseudo targets to generate visual features of the target domains. The evaluation contexts differ slightly: CGAP explores weather attributes, while this paper focuses on DomainNet/CUB. Furthermore, both papers demonstrate that precise domain names are unnecessary for strong performance on unseen domains, as illustrated by CGAP's \"random\" augmentations in Table 7.\n\nIn light of these similarities, it appears that this paper functions as an application on CUB and DomainNet rather than a substantially novel contribution.\n\n[1] Vidit, Vidit, Martin Engilberge, and Mathieu Salzmann. \"CLIP the Gap: A Single Domain Generalization Approach for Object Detection.\" CVPR, 2023. (https://openaccess.thecvf.com/content/CVPR2023/papers/Vidit_CLIP_the_Gap_A_Single_Domain_Generalization_Approach_for_Object_CVPR_2023_paper.pdf)\n\nW2 Result:\n\nA noteworthy concern arises from the marginal improvement over the most competitive baseline, LADS, as evidenced by Table 1 (0.78% improvement on CUB, 0.92% improvement on DomainNet). This minor improvement raises doubts about the merit and significance of the proposed method, especially considering the striking similarities to existing work, such as CGAP. The paper's originality and contribution require further clarification and validation to convincingly demonstrate its value in the domain of single-domain generalization.\n\nTypos:\n\n- [LADS reference]: \"Using language to *extend* to unseen domains,\" International Conference on Learning Representations (ICLR), 2023.\n\n- \"Benefiting from the learned multimodal embedding space in pre-trained large-scale vision-language (VL) models (Radford et al., 2021; Jia et al., 2021), *we* achieve...\""
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4755/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4755/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4755/Reviewer_GcYV"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4755/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698408826475,
            "cdate": 1698408826475,
            "tmdate": 1699636457982,
            "mdate": 1699636457982,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oLDIX6BeiG",
                "forum": "tG5mpAM7ZK",
                "replyto": "DWjyectBEf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4755/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4755/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer GcYV (Part 1)"
                    },
                    "comment": {
                        "value": "We sincerely thank you for your review and helpful comments!  \n## Q1 The similarity between this paper and CLIP the GAP (CGAP) [1].\nThanks for pointing out this related work. We will include and discuss it in our final paper. Aftering carefully checking it, we clarity that, although we may share similarity in working under domain generalization with CGAP, we are fundamentally different in: methods, and tasks. In fact, CGAP is mostly similar to LADS instead of our model.\n\nWe only share the similarity with CGAP on using domain prompts to perform augmentation. But the key is how to use them, i.e., augmentation strategies and methods. We significantly differ from CGAP in this sense, which are the major contributions of this paper, leading to constantly better performance on both LADS settings and our setting with much less time cost. We summarize the major differences in the table below:\n\n| Model       | Augmentation Stratagy (Stage-1)                        | Augmentation Method (Stage-1)                                | Domain Prompt                                                | Training Stage Data (Stage-2)             | Task             |\n| ----------- | ------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ----------------------------------------- | ---------------- |\n| CGAP        | Train multiple augmentation functions                  | Align global direction with trainable augmentation functions | A given pre-defined list                                     | Augmented and original features           | Object Detection |\n| LADS        | Train multiple augmentation functions                  | Align global direction with trainable augmentation functions | A given pre-defined list (exactlly match test domains)       | Augmented and original features           | Classification   |\n| TEAM (Ours) | *Training-free (calculated with Eq. 9, nonparametric)* | *Align modality direction with nonparametric training-free method* | *(1) Domain-invariant representations (2) a given pre-defined list* | *Domain- invariant and original features* | *Classification* |\n\nIn fact, CGAP is mostly similar to LADS in most columns, except for the \u201ctask\u201d, while it is different from us in all above aspects. Specifically, CGAP still follows the pipeline of LADS: they first perform optimization for trainable augmentations {$A_j$} (Eq. 3) during the first stage, which are later used for augmenting source images during the training stage. And they all align \u201cglobal directions\u201d during the training of augmentation functions. \n\nIn the following, we would like to clarify each of your concerns:\n\n1. *\u201cBoth papers share fundamental observations: that various domains of the same object differ by constant vectors\u201d*\n\nIn fact, the observation of CGAP and Ours is fundamentally **different**. As we stated in the \u201cAugmentation Method\u201d in the above table: CGAP aligns the **global direction** (Eq. 21), following LADS, Styleclip [3], Stylegan-nada [4]. Instead, we first propose to align the **modality direction (Eq. 3)**, motivated by Liang et al [2]. The difference of the two directions, which may seem not very significant at first glance, can lead to notable differences. We demonstrate the difference is critical in detail in Section 4.1: ours are better with (a) more solid theoretical support, (b) better preservation of class information, and (c) milder assumption for an analytical solution. These benefits finally lead to the success of our training-free augmentation. Ablation studies (Tab. 3) also validate this:\n\n| Augmentation Method                 | Average   | ID        | OOD       |\n| ----------------------------------- | --------- | --------- | --------- |\n| LADS                                | 74.99     | 85.33     | 64.85     |\n| Training-free + Align Global Dir.   | 74.67     | 85.21     | 64.12     |\n| Training-free + Align Modality Dir. | **77.16** | **86.61** | **67.71** |\n\nMoreover, as shown in the \u201cDomain Prompt\u201d column in the first table, we further propose to perform a domain-invariant augmentation in Section 4.2 and Fig. 3, which produces domain-invariant features and further reduces the training time (Stage-2 time in Tab.2)."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4755/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700148117223,
                "cdate": 1700148117223,
                "tmdate": 1700513827041,
                "mdate": 1700513827041,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "31EC0MnPBN",
                "forum": "tG5mpAM7ZK",
                "replyto": "DWjyectBEf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4755/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4755/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer GcYV (Part 2)"
                    },
                    "comment": {
                        "value": "2. *\"Additionally, they both tackle the challenge of single-domain generalization. The evaluation contexts differ slightly: CGAP explores weather attributes, while this paper focuses on DomainNet/CUB.\"*\n\nYes, we work on single-domain generalization, but the context is different: CGAP focuses on object detection, while we work on classification. We argue that this difference is not trivial. CGAP and LADS train augmentation functions by aligning the global direction, which in fact **originates from image generation research [3,4]**. In the first paragraph of Section 4.1, we question if such direct adaptation fits well for the classification task. Motivated by this, we do not follow them and demonstrate multiple benefits of aligning modality direction with theoretical guarantees, one of which is that it better preserves class information in classification. It is one of the reasons we achieve better results. In other words, we specially designed our methods in the context of classification, and it is not a simple evaluation difference. CGAP still follows existing works (LADS, Styleclip [3], Stylegan-nada [4) to use the global direction.\n\n3. *\"The methodological resemblance is evident: CGAP performs semantic augmentations by computing relative differences in original and target test domains of text embeddings, using these as pseudo targets to generate visual features of the target domains.\"*\n\nAs we stated in the first reply, CGAP is the optimization-based augmentation with \u201cglobal direction\u201d (Eq. 21), which is adopted by LADS, Styleclip [3], andStylegan-nada [4]. While we align \u201cmodality direction\u201d (Eq. 3) in a training-free strategy. Furthermore, we propose to perform a domain-invariant augmentation (also training-free) in Section 4.2 and Fig. 3, which produces domain-invariant augmentations and further reduces the training time (Stage-2 time in Tab. 2).\n\n4. *\"Furthermore, both papers demonstrate that precise domain names are unnecessary for strong performance on unseen domains, as illustrated by CGAP's \"random\" augmentations in Table 7.\"*\n\nAs we stated in Section 5.4, we carefully check CGAP and find that, it is slightly different from what you mentioned \u201c*as illustrated by CGAP's \u2018random\u2019 augmentations*\u201d. In the caption of Tab. 7 in CGAP, the authors claim that, *\u201cWhile random augmentations are* **worse than no-aug***., clip-random is* **comparable to no-aug***. Only when we give relevant prompts, there is a consistent improvement across datasets.\u201d.* It indicates random/clip-random does not give constant improvements and only their predefined prompts (in Page 5, line 9) can constantly lead to improvements. These predefined prompts are highly relevant to unseen test domains. \n\nBy contrast, we use various sets of prompts to study how they can influence our model. In Appendix E and F, we further show results with different sources of domain prompts, which come from different LLMs, including ChatGPT, GPT4, Bard, and New Bing. Besides, we also use the existing prompt template set \u201cImageNet Templates\u201d, which contains 80 different prompts. The studies validate that our model is robust across different sets of prompts. We also kindly remind that, it is just an additional interesting finding as a supplementary of our key contributions on augmentation strategies and methods.\n\n**Summary for Q1**: In summary, we have three contributions as mentioned in the paper: (1) propose the text-driven domain generalization problem; (2) propose a training-free augmentation method via modality direction alignment with theoretical proofs; and (3) build a framework with our augmentation method that performs domain-invariant augmentations. \n\nWe partially agree that (1) may share some similarity in problem definition with CGAP, although CGAP works on a different detection task. However, (2) and (3) are our key contributions that are fundamentally different from CGAP and LADS. We sincerely hope you could take them into consideration."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4755/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700148405013,
                "cdate": 1700148405013,
                "tmdate": 1700148405013,
                "mdate": 1700148405013,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QCYn9I88GS",
                "forum": "tG5mpAM7ZK",
                "replyto": "DWjyectBEf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4755/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4755/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer GcYV (Part 3)"
                    },
                    "comment": {
                        "value": "## Q2. Results\n\nAs we mentioned in the first paragraph in Section 5.4, Tab. 1 shows the results under the LADS setting, where exact test domain names are given and LADS is expected to perform well. Tab. 2 is our setting and the interesting part, where exact test domain names are not given. Without exact test domains, the performance gap between LADS and ours is more obvious: **2.86% (OOD)** and **2.17% (AVG)** in CUB-Paintings, and **2.15% (OOD)** and **2.05% (AVG) in** DomainNet. Note that we achieve better results without any augmentation networks nor training during stage-1, while LADS requires training an augmentation network for each given domain prompt. We are significantly more efficient. Besides, our method requires less time in the stage-2 training (four times faster with \u201cinvar.\u201d mode in tab. 2). Our model is also more robust across different sets of domain prompts (Appendix E and F show more experiments and details), while LADS only works well when the exact test domain names are given, which is impractical. *We kindly remind that, as mentioned in the paper, out-of-domain (OOD) performance is our major metric as we focus on the domain generalization task, where test domain accuracy is the major metric, like existing works [1,5,6]. To summarize, our model achieves obvious results on our practical setting (Tab. 2) over computing baselines (their performances are very close to each other while we achieve a more obvious improvement)* **without** *training during stage-1 and less time during stage-2*  *with \u201cinvar.\u201d mode:*\n\n| Method  | OOD (CUB-Paintings) | OOD (DomainNet) | Average (CUB-Paintings) | Average (DomainNet) |\n| ------- | ------------------- | --------------- | ----------------------- | ------------------- |\n| WiSE-LP | 64.80               | 93.68           | 73.27                   | 94.44               |\n| LADS    | 64.85               | 94.65           | 74.99                   | 94.97               |\n| Ours    | **67.71**           | **96.70**       | **77.16**               | **96.18**           |\n\nEfficiency is also one of our key improvements.\n\n**Reference**\n\n[1] Vidit, Vidit, Martin Engilberge, and Mathieu Salzmann. \"CLIP the Gap: A Single Domain Generalization Approach for Object Detection.\" CVPR, 2023.\n\n[2] Liang et al., Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning. NeurIPS 2022\n\n[3] Patashnik et al., Styleclip: Text-driven manipulation of stylegan imagery, ICCV 2021\n\n[4] Gal et al., \u201cStylegan-nada: Clip-guided domain adaptation of image generators,\u201d ACM Transactions on Graphics (TOG)\n\n[5] Li et al., A Simple Feature Augmentation for Domain Generalization, ICCV 2021\n\n[6] Lisa et al., Using Language to Extend to Unseen Domains, ICLR 2023"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4755/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700148537661,
                "cdate": 1700148537661,
                "tmdate": 1700150768056,
                "mdate": 1700150768056,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cJvhlY3Ozw",
                "forum": "tG5mpAM7ZK",
                "replyto": "DWjyectBEf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4755/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4755/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow up"
                    },
                    "comment": {
                        "value": "Thanks again for your constructive comments. Would you mind checking our responses to see if your previous concerns/questions have been addressed?\n\nWe are also more than happy to provide additional experiments and discussions if you have further questions."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4755/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700582184910,
                "cdate": 1700582184910,
                "tmdate": 1700582184910,
                "mdate": 1700582184910,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "300QTlJC9I",
                "forum": "tG5mpAM7ZK",
                "replyto": "DWjyectBEf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4755/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4755/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow up 2"
                    },
                    "comment": {
                        "value": "Dear reviewer, thank you again for reviewing this paper. As the discussion period is coming to an end, could you please kindly check our responses to see if your concerns are solved?\n\nWe sincerely look forward to your response!"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4755/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700681640626,
                "cdate": 1700681640626,
                "tmdate": 1700681640626,
                "mdate": 1700681640626,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Nm4KSteXa5",
            "forum": "tG5mpAM7ZK",
            "replyto": "tG5mpAM7ZK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4755/Reviewer_spPq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4755/Reviewer_spPq"
            ],
            "content": {
                "summary": {
                    "value": "The paper tackles the domain generalization without using the image nor text description of the target domain. The authors propose to use the property of modality gap of CLIP model. Specifically, they augment the embeddings of images in the source domain to the domain-invariant embedding without training. After augmentation, they train the linear probe on the mixture of domain-invariant embedding and original embedding. In inference, they apply the trained linear probe to the embedding of the target image. In the experiment, the authors test their proposed method with and without the text description of the target domain."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper adopts the recent finding of the modality gap in the CLIP embedding space and propose a training-free augmentation which is training-efficient.\n2. The paper considers a practical setting where the text description is not available for the target domain."
                },
                "weaknesses": {
                    "value": "1. In **Training-free Augmentation with Modality Direction** (on page 5) the proof of z exists in the CLIP's normalized embedding space is trivial to get.\n\n2. In Table 1 and Table 2, the performance of the proposed method on CUB and DomainNet does not stand out from the baselines with a significant margin. \n\n3. Ablation Studies are limited."
                },
                "questions": {
                    "value": "1. Why can the linear probe trained on the domain-invariant feature and original feature be directly applied to the target feature (without any augmentation)?\n\n2. In Stage 2, why not only train the classifier only on the augmented embeddings?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4755/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698806410814,
            "cdate": 1698806410814,
            "tmdate": 1699636457873,
            "mdate": 1699636457873,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4usbfep5dX",
                "forum": "tG5mpAM7ZK",
                "replyto": "Nm4KSteXa5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4755/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4755/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer spPq (Part 1)"
                    },
                    "comment": {
                        "value": "We sincerely thank you for your review and helpful comments! \n\n### W1\nWe agree that obtaining a solution for $z$ is not hard mathematically. \nActually, our intention is not to showcase our skills of solving the equations, but rather to explore the conditions under which a valid solution is available in the normalized embedding space (i.e., training-free feasibility) (Eq. 17 in Appendix B), and its relationship with the pre-trained vision-language (VL) model (Lemma 1).\nWe are more interested in $\\lambda_{1}$ in Eq. 17 (Appendix B), which determines if the solved $z$ is a valid solution in the embedding space.\nThe exploration is critical because it theoretically demonstrates that, if aligning with the global direction (Eq. 21), the condition for the existence of a valid $z$ is more stringent, requiring the cosine similarity of any negative image-text pair is always less than that of the positive pair. This implies that the pre-trained VL model should be perfect (Appendix C). On the other hand, if aligning with our modality direction, the condition for the existence of $z$ is more relaxed: once the embeddings are clustered per modality in the VL model\u2019s embedding space, a solution is readily available. Fortunately, (Liang et al.) [1] validate that the condition is generally satisfied with a VL model trained with a contrastive loss.\nOur ablation studies in Tab. 3 also validate this point: aligning modality direction proves to be more effective:\n\n| Augmentation Method                 | Average   | ID        | OOD       |\n| ----------------------------------- | --------- | --------- | --------- |\n| LADS                                | 74.99     | 85.33     | 64.85     |\n| Training-free + Align Global Dir.   | 74.67     | 85.21     | 64.12     |\n| Training-free + Align Modality Dir. | **77.16** | **86.61** | **67.71** |\n\n### W2\n\nTab. 1 presents our results under the LADS setting, where the **exact** test domain names are given. The main results are presented in Tab. 2 in our text-driven domain generalization setting, which is of particular interest to us, where exact test domain names are not given. \n\nIt is important to note that it is not an easy task. Under our setting, most competing CLIP-based baselines including WiSE-LP and LADS exhibit very close performance, making further improvements challenging. However, we continue to distinguish ourselves from them (Tab. 2) in AVG and OOD scores:\n\n| Method  | OOD (CUB-Paintings) | OOD (DomainNet) | Average (CUB-Paintings) | Average (DomainNet) |\n| ------- | ------------------- | --------------- | ----------------------- | ------------------- |\n| WiSE-LP | 64.80               | 93.68           | 73.27                   | 94.44               |\n| LADS    | 64.85               | 94.65           | 74.99                   | 94.97               |\n| Ours    | **67.71**           | **96.70**       | **77.16**               | **96.18**           |\n\nNotably, this distinction is achieved **without** the need for any parametric augmentation functions and **without** any training in the first phase. Our work contributes significantly to time efficiency, a crucial aspect beyond accuracy improvement. *We kindly remind that, as mentioned in the paper, out-of-domain (OOD) performance is our major metric as we focus on the domain generalization task, where test domain accuracy is the major metric, like existing works [2,3,4]. To summarize, our model achieves obvious results on our practical setting (Tab. 2) over computing baselines (their performances are very close to each other while we achieve a more obvious improvement)* **without** *training during stage-1 and less time during stage-2*  *with \u201cinvar.\u201d mode.\n\n### W3\n\nWe have multiple ablation studies on: (1) augmentation modes and (2) invariant modes in Tab. 3 to show the effectiveness of our augmentation method. We also have ablations on choices of backbone vision-language models in Tab. 11, and on different sources of prompt templates in Tab. 10 and Tab. 11, where we use pre-defined templates such as ImageNet Templates, as well as prompts from different LLMs (e.g., ChatGPT, CPT4, New Bing, and Bard), to demonstrate our robustness across different sets of prompts. \n\nOur existing ablation studies answer most interesting questions. If you believe there is any particular study that we\u2019re missing, we\u2019re more than happy to include.\n\nWe add one table regarding your two questions below. We train our model on DomainNet with: \u2460 a mix of augmented data and original data. \u2461 augmented data. \u2462 original data.\n\n\n| Model                            | Average   | ID        | OOD       |\n| -------------------------------- | --------- | --------- | --------- |\n| \u2460 original data + augmented data | **96.18** | **95.61** | **96.70** |\n| \u2461 augmented data only            | 95.60     | 95.21     | 95.99     |\n| \u2462 original data only             | 94.39     | 95.03     | 93.75     |\n\nWe will discuss it in detail in the response to your questions below."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4755/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700148975719,
                "cdate": 1700148975719,
                "tmdate": 1700179422579,
                "mdate": 1700179422579,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZyfqTk5OKL",
                "forum": "tG5mpAM7ZK",
                "replyto": "Nm4KSteXa5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4755/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4755/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer spPq (Part 2)"
                    },
                    "comment": {
                        "value": "### Questions (Q1 and Q2)\n\nThe two questions can be answered by a single statement: The two questions you mentioned are in fact standard pipelines for data augmentation based domain generalization methods [2,3,4], which are also adopted by LADS [4]. As our TEAM model is also a data augmentation based method, we simply follow existing works.\n\nWe explain it in detail below. Let's quickly recap our methodology first: We have two model versions: TEAM-full and TEAM-invariant. In TEAM-full, $k$ crafted domain names are provided. We utilize a training-free method to obtain augmented features for $k$ provided domains, which are then **mixed** with the source domain for training on (k+1) domains during stage-2. The evaluation is conducted directly on test domains images without prior image augmentation to a specific domain. **This follows the standard approach in data augmentation based domain generalization (DG) methods [2,3,4], which LADS also follows:** \n\n**(Point. 1 / Q1) Trained model is applied to test domain data without any augmentation.** \n\n**(Point. 2 / Q2) Augmented data is mixed with original data for training.**\n\nIn the \"invariant\" version, augmentation is not performed for $k$ domains. Instead, augmentation is performed for a single \"invariant\" domain. **The key is, it is still a data augmentation method**, which means the obtained invariant embeddings are still \u201caugmented\u201d data, just like any augmented data for $k$ domains in TEAM-full. The difference from TEAM-full is that, here we set $k$ = 1 (i.e., our so-called \"invariant domain\").\n\nTherefore, the training and evaluation process are the same with TEAM-full and LADS, i.e., the two bolded points we mentioned above.\n\nIn addition, we have done ablation studies on DomainNet regarding your question: we train our model on DomainNet with: \u2460 a mix of augmented data and original data. \u2461 augmented data only. \u2462 original data only.\n\n| Model                            | Average   | ID        | OOD       |\n| -------------------------------- | --------- | --------- | --------- |\n| \u2460 original data + augmented data | **96.18** | **95.61** | **96.70** |\n| \u2461 augmented data only            | 95.60     | 95.21     | 95.99     |\n| \u2462 original data only             | 94.39     | 95.03     | 93.75     |\n\n**Reference**\n\n[1] Liang et al., Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning. NeurIPS 2022\n\n[2] Li et al., A Simple Feature Augmentation for Domain Generalization, ICCV 2021\n\n[3] Vidit, Vidit, Martin Engilberge, and Mathieu Salzmann. \"CLIP the Gap: A Single Domain Generalization Approach for Object Detection.\" CVPR, 2023.\n\n[4] Lisa et al., Using Language to Extend to Unseen Domains, ICLR 2023"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4755/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700149126107,
                "cdate": 1700149126107,
                "tmdate": 1700181245548,
                "mdate": 1700181245548,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2xDIRABkpp",
                "forum": "tG5mpAM7ZK",
                "replyto": "Nm4KSteXa5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4755/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4755/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow up"
                    },
                    "comment": {
                        "value": "Thanks again for your constructive comments. Would you mind checking our responses to see if your previous concerns/questions have been addressed?\n\nWe are also more than happy to provide additional experiments and discussions if you have further questions."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4755/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700582142805,
                "cdate": 1700582142805,
                "tmdate": 1700582142805,
                "mdate": 1700582142805,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tprSw8vFvB",
                "forum": "tG5mpAM7ZK",
                "replyto": "Nm4KSteXa5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4755/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4755/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow up 2"
                    },
                    "comment": {
                        "value": "Dear reviewer, thank you again for reviewing this paper. As the discussion period is coming to an end, could you please kindly check our responses to see if your concerns are solved?\n\nWe sincerely look forward to your response!"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4755/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700681588024,
                "cdate": 1700681588024,
                "tmdate": 1700681588024,
                "mdate": 1700681588024,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "kMfxJv5tfc",
            "forum": "tG5mpAM7ZK",
            "replyto": "tG5mpAM7ZK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4755/Reviewer_mv6P"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4755/Reviewer_mv6P"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a novel approach for domain adaptation with the idea of the augment of data with only text descriptions. It utilizes a pre-trained vision-language model to acquire domain-invariant augmentations using text descriptions of arbitrary, unseen domains, even if they don't match the test domains. This method outperforms existing approaches, offering greater efficiency and stronger theoretical support while eliminating the need for predefined text descriptions of all test domains."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Good Quality:\nThe submission is technically sound, in my opinion, and the advantages and limitations of this work are carefully and honestly discussed.\n\nGood Clarity:\nThe submission is written with clear definitions and formulas. The organization is well-designed.\n\nGood Motivation:\nThe method is well-motivated, given the recent conclusions from Liang et al. and Zhang et al.\n\nSolid Solution:\nBy observing the beneficial properties of the multimodal embedding space, the authors successfully extend the findings in Liang et al. and Zhang et al.  to address the domain generalization problem, providing a new method from a novel perspective."
                },
                "weaknesses": {
                    "value": "1.\tThe authors might consider evaluating their proposed method on a larger dataset, such as WILDS, which provides extensive and diverse real-world data from various domains.\n\n2.\tCould you please explain the meaning of \"1x\" for Time in several tables?\n\n3.\tWhile the authors have successfully developed a method based on the findings of Liang et al. and Zhang et al., the paper itself lacks a sufficient explanation of the embedding space in the context of domain generalization. Although this might not be a critical flaw in the paper, providing theoretical or empirical analyses in this regard could enhance its overall impact and inspiration."
                },
                "questions": {
                    "value": "As the authors mentioned in Figure 2 (right), there might be cases where equation 4 is not satisfied. I am curious if the CLIP features always meet the conditions of equation 4. Could the authors conduct empirical analysis to offer an intuitive understanding of when it fails?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4755/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4755/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4755/Reviewer_mv6P"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4755/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698815525142,
            "cdate": 1698815525142,
            "tmdate": 1699636457799,
            "mdate": 1699636457799,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "u7lIrzP5Up",
                "forum": "tG5mpAM7ZK",
                "replyto": "kMfxJv5tfc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4755/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4755/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer mv6P (Part 1)"
                    },
                    "comment": {
                        "value": "Thank you for your time and effort for this review. We appreciate your recognition and constructive questions about our work!\n\n### W1\n\nThanks for this insightful suggestion, which points out a critical future direction for this line of research. As we also discussed in Section 6: Limitations, WILDS (Koh et al.) remains a challenge. As our text-driven augmentation requires text descriptions of target domains, it is particularly challenging for complicated natural domain shifts that are hard to capture and express solely through language. We are interested in this problem and will continue to explore it.\n\n### W2\n\nSure! Sorry for the confusion of our symbols. \"$n$ x\" indicates that the stage-2 training time of a particular model is $n$ times that of the LADS baseline. In other words, we consider the training time of LADS as the reference unit time.\n\nFor instance, in the following example: \n\n| Model         | Average | ID    | OOD   | Training-free (Stage-1) | Time (Stage-2) |\n| ------------- | ------- | ----- | ----- | ----------------------- | -------------- |\n| LADS          | 74.99   | 85.33 | 64.85 | \u2717                       | 1x             |\n| TEAM-*invar.* | 74.67   | 85.21 | 64.12 | \u2713                       | 0.23x          |\n\nWhen the model is TEAM-*invar.*, we have $0.23$x because the training time of TEAM-*invar.* is 0.23 times that of LADS. Therefore, when the model is LADS, we have $1$x because the training time of LADS is the same as that of itself."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4755/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700149511197,
                "cdate": 1700149511197,
                "tmdate": 1700149511197,
                "mdate": 1700149511197,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fKgS7ae8Ku",
                "forum": "tG5mpAM7ZK",
                "replyto": "kMfxJv5tfc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4755/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4755/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer mv6P (Part 2)"
                    },
                    "comment": {
                        "value": "### W3\n\nThanks for the valuable advice. We are also interested to see the geometry of clip embeddings space from the perspective of the training-free augmentation in our domain generalization context. More specifically, we would like to see if the assumptions in Appendix A hold well. We show the following modality gap geometry metrics. \n\nThe distance between the modality clusters in the CLIP embedding space is referred to as the modality gap. The instance-level modality gap $g$ is defined as the difference between image and text embeddings for a single pair. The class-level modality gap is defined as $g_{c}$, where for a given class $c$, we calculate the expectation of $g$ over all instances of class $c$ **across different domains**. Finally, we calculate the domain-level modality gap $g_{d}$, where given a specific domain, we calculate the expectation of $g$ over all instances of domain $d$ **across different classes**.\n\nFormally, given an instance pair ($x$, $y$), $g = x - y$, where $x$ and $y$ are CLIP features of an image and a text. We also have:\n\n$g_{c} = x_c - y_c$, where $x_c = E_d[ x_c^d ]$ and  $y_c = E_d[ y_c^d ]$.  Note that $E_d[ x_c^d ]$ means the expectation of all instances of a given class $c$ across all **domains**. \n\nand \n\n$g_{d} = x_d - y_d$, where $x_d = E_c[ x_d^c ]$ and  $y_d = E_c[ y_d^c ]$.  Note that $E_c[ x_d^c ]$ means the expectation of all instances of a given domain $d$ across all **classes**. \n\nWe hope that the modality gap $g_{c} $ grants the assumptions we have in Appendix A. Intuitively, if satisfied, given data from one source domain, we can perform our training-free augmentation as  $g_{c} $ is valid across different domains.\n\nMore specifically, the two assumptions are:\n\n1. *The modality gap between corresponding image and text embeddings can be approximated by a constant vector, particularly given a class across different domains. This is verified by computing distributions over ||$g$|| (magnitude) and $cos(g, E_g[g])$ (direction).*\n\n2. *The modality gap is approximately orthogonal to the span of image embeddings and text embeddings, and image embeddings and text embeddings have a nearly zero mean in the subspace orthogonal to the modality gap. This is verified by computing distributions over $cos(x - E_x[x], E_g[g])$ (orthogonality) and $E_x[x - x^T g\u2019g\u2019]_i$ (center), where $g\u2019 = E_g[g] / || E_g[g] ||$ and $i \\in [d]$. The subscript $i$ denotes indexing the $i$-th dimension of the vector.*\n\nThe results are listed:\n\n| VL Model - Dataset   | Magnitude - *Instance* | Magnitude - *Class* | Magnitude - *Domain* | Direction - *Instance* | Direction - *Class* | Direction - *Domain* | Orthogonality     | Center           |\n| -------------------- | ---------------------- | ------------------- | -------------------- | ---------------------- | ------------------- | -------------------- | ----------------- | ---------------- |\n| CLIP - DomainNet     | 1.32 $\\pm$ 0.034       | 1.19 $\\pm$ 0.027    | 1.32 $\\pm$ 0.029     | 0.76 $\\pm$ 0.051       | 0.89 $\\pm$ 0.023    | 0.77 $\\pm$ 0.049     | 0.01 $\\pm$  0.107 | 0.00 $\\pm$ 0.024 |\n| CLIP - CUB Paintings | 1.29 $\\pm$ 0.035       | 1.18 $\\pm$ 0.029    | 1.30 $\\pm$ 0.038     | 0.70 $\\pm$ 0.051       | 0.88 $\\pm$ 0.029    | 0.74 $\\pm$ 0.045     | 0.00 $\\pm$  0.095 | 0.00 $\\pm$ 0.026 |\n\nWe can observe that, the orthogonality and center are all close to 0 with small STD, which means assumption 2 is well satisfied. For magnitude and direction, the class-level modality gap (**across domains**) better satisfies assumption 1, as we have higher scores for direction on both datasets. It satisfies our expectation that, the class-level modality gap $g_{c}$ better grants the assumptions. It indicates that, given data from one source domain, we can perform our training-free augmentation because the modality gap $g_{c} $ of a given class $c$ is valid across different domains."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4755/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700149598870,
                "cdate": 1700149598870,
                "tmdate": 1700149919845,
                "mdate": 1700149919845,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mOXjNbVRYf",
                "forum": "tG5mpAM7ZK",
                "replyto": "kMfxJv5tfc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4755/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4755/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer mv6P (Part 3)"
                    },
                    "comment": {
                        "value": "### Question\n\nIt is an interesting question. Let's illustrate it with an example, where the source domain is \"painting\" and target domain is \"photo\". Given an image-text pair from source data, say, (apple_painting.jpg, \"a painting of an apple\"), and a text description from target domain: \"a photo of an apple\", intuitively, equation 4 means: in the CLIP embedding space, \"a painting of an apple\" is closer to  \"a photo of an apple\"  than to apple_painting.jpg. In other words, embeddings of the **same** modality should be closer to each other than embeddings of the same semantic meaning but different modality. This is what we expect.\n\nFortunately, Liang et al. reveal that, when a vision-language model is pre-trained with a contrastive loss, the embeddings are clustered per modality, which naturally guarantees what we expect. **And in practice, we find it always holds.** It is reasonable because the CLIP model is well-trained with 400M data, during which time the embeddings are very well clustered per modality. \n\nYou may refer to Fig. 1b (https://arxiv.org/pdf/2203.02053.pdf). Intuitively, if there is a failure case, you could imagine a blue point appearing among the red points, which indicates a text embedding is closer to one or some image embeddings than other text embeddings. In our practice with the pre-trained CLIP model, it never happens.\n\nLiang et al. further explain why they are so separated from each other from three aspects: cone effect, initializations, and contrastive learning objective."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4755/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700149630944,
                "cdate": 1700149630944,
                "tmdate": 1700149630944,
                "mdate": 1700149630944,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]