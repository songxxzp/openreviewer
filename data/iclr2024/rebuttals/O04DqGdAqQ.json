[
    {
        "title": "Ada-Instruct: Adapting Instruction Generators For Complex Reasoning"
    },
    {
        "review": {
            "id": "7UHNFaTUsY",
            "forum": "O04DqGdAqQ",
            "replyto": "O04DqGdAqQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5652/Reviewer_C9NZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5652/Reviewer_C9NZ"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes to finetune a pre-trained LLM for generating training data. It suggests previous methods, which prompt LLM for data generation, cannot obtain complex data. Thus, the paper finetunes an LLM with ten samples, uses the finetuned LLM to generate massive training data, and uses another LLM to obtain the labels. Experiments show the proposed method can output complex training data and improve the downstream models\u2019 performance compared with the prompting baseline."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper shows the potential of finetuning in generating large-scale datasets. With few supervised examples, the finetuned LLM can generate more complex samples with the training distribution."
                },
                "weaknesses": {
                    "value": "The paper lacks an explanation or analysis of why 10-shot finetuning can finetune an LLM to generate a whole dataset. The randomly selected ten examples likely cannot span the training distribution. If all training samples are short, the LLM cannot learn to output complex data.\n\nGiven the above concern, other questions arise.\n- Why is the example number 10? Does more/less examples increase performance or lower the generalization ability?\n- Can the finetuned LLM generalize to other datasets? For example, from math to GSM8K or even HumanEval? How do you know the training boundary given the randomly selected ten examples?"
                },
                "questions": {
                    "value": "- See weakness.\n\n- What are the performance of baselines and the proposed method given the same initial data? And What are their multi-run performance with different random seeds?\n\n- What are the properties of the selected ten examples?\n\n- Since the proposed method focuses on improving the data generation procedure, it is interesting to know whether it is effective for the pretrained LLM in the general domain. For example, if 10-shot FT is better than 10-shot ICL for directly solving tasks instead of generating training data."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5652/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5652/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5652/Reviewer_C9NZ"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5652/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698565258923,
            "cdate": 1698565258923,
            "tmdate": 1700622179832,
            "mdate": 1700622179832,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JfXssg8s6T",
                "forum": "O04DqGdAqQ",
                "replyto": "7UHNFaTUsY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5652/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5652/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reponse to Reviewer C9NZ"
                    },
                    "comment": {
                        "value": "We thank the reviewer for recognizing the effect of our method and the revealed potential of fine-tuning in generating large-scale datasets. Our responses are as below:\n\n**Weakness 1: Why is the example number 10? Does more/less examples increase performance or lower the generalization ability?**\n\n**Response:** Certainly, increasing the number of initial samples typically implies a higher quality of generated instructions and a closer distribution. We use 10 initial samples because we believe this setting represents a typical and challenging few-shot setting. \n\n**Weakness 2: Can the finetuned LLM generalize to other datasets? For example, from math to GSM8K or even HumanEval?**\n\n**Response:** Although the ability of cross-dataset transfer learning is beyond this paper, we show that Ada-Instruct easily generalizes to a domain with multiple datasets. Please refer to our response to General Response - Question 1.\n\n**Question 1: What are the performance of baselines and the proposed method given the same initial data?**\n\n**Response:** Please refer to our response to General Response - Question 2.\n\n\n**Question 2: What are the properties of the selected ten examples?**\n\n**Response:** As we have already stated in the experimental setup section, the initial samples for MBPP/MATH/GSM8k/CSQA are randomly selected, rather than deliberately chosen by humans. The effectiveness of this random strategy suggests the robustness of Ada-Instruct.\n\n**Question 3: Since the proposed method focuses on improving the data generation procedure, it is interesting to know whether it is effective for the pretrained LLM in the general domain. For example, if 10-shot FT is better than 10-shot ICL for directly solving tasks instead of generating training data.**\n\n**Response:** This is substantiated. As illustrated in our related work section:\n>  In more equitable experimental setups, researchers found that FT outperforms ICL (Mosbach et al., 2023), thereby lending support to our strategy of using FT models for instruction generation."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5652/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700408780015,
                "cdate": 1700408780015,
                "tmdate": 1700408780015,
                "mdate": 1700408780015,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "G2IK7lc7Ut",
                "forum": "O04DqGdAqQ",
                "replyto": "JfXssg8s6T",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5652/Reviewer_C9NZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5652/Reviewer_C9NZ"
                ],
                "content": {
                    "comment": {
                        "value": "The Ada-Instruct-Program on the two datasets is lower than Ada-Instruct-HumanEval and Ada-Instruct-MBPP(62.8 vs 64.0, 54.0 vs 55.6), which shows the possibility of increasing samples with lower performance. The results also indicate the finetuning can not generalize well to another benchmark.\n\nFT vs ICL is an interesting and important topic. I believe ICL can leverage the common knowledge in the largely pretrained data by not changing the learned parameters, thus leading to better generalization performance. While FT can outperform ICL with enough training data aligned with the test set.\n\nThe paper shows the potential of FT in data generation but does not discuss its pros and cons. Thus I like to keep my rating unchanged but looking forward to the author's and other reviewers' responses."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5652/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700491172426,
                "cdate": 1700491172426,
                "tmdate": 1700491172426,
                "mdate": 1700491172426,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mjdTaRNtOY",
                "forum": "O04DqGdAqQ",
                "replyto": "7UHNFaTUsY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5652/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5652/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reponse"
                    },
                    "comment": {
                        "value": "Thank you for your reply.\n\n> **The Ada-Instruct-Program on the two datasets shows the possibility of increasing samples with lower performance. The results also indicate the finetuning can not generalize well to another benchmark.**\n\nWe believe a more appropriate comparison for cross-benchmark generalization capabilities should be with **Self-Instruct**. In fact, Ada-Instruct demonstrates much stronger capabilities than Self-Instruct. Specifically, we tested Self-Instruct's performance using the same initial samples and the same amount of SFT samples as the Ada-Instruct-Program. We denote it as Self-Instruct-Program in the table below. The results clearly show that Ada-Instruct-Program significantly outperforms its counterpart.\n\n\n| Model| Initial Data  |  SFT Data  |  HumanEval  |  MBPP  |\n|  :----  | :----:  | :----:  |  :----:  |  :----:  |\n|Self-Instruct-Program| 20 | 10k | 51.8 | 47.8 |\n|Ada-Instruct-Program| 20 | 10k | **62.8** | **54.0** |\n\n\n> **I believe ICL can leverage the common knowledge in the largely pretrained data by not changing the learned parameters, thus leading to better generalization performance than FT.**\n\nAs indicated by the results above, under the same conditions, even with few-shot samples, FT (Ada-Instruct) exhibits stronger generalization capabilities than ICL (Self-Instruct). Please also refer to our discussion on ICL vs. FT in the related work section."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5652/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700544955803,
                "cdate": 1700544955803,
                "tmdate": 1700545309011,
                "mdate": 1700545309011,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cb0Qh9lGfn",
                "forum": "O04DqGdAqQ",
                "replyto": "CYDwsUdjJY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5652/Reviewer_C9NZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5652/Reviewer_C9NZ"
                ],
                "content": {
                    "comment": {
                        "value": "Self-Instruct-Program performs better than Self-Instruct-Alpaca (48.8 vs 51.8) (47.6 vs 47.8), indicating the generalization ability of ICL across different distributions.\n\nI agree that the proposed method performs much better than current ICL methods. However, the paper does not provide new insight between ICL and FT. I will raise my rating but still consider it borderline."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5652/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700622102301,
                "cdate": 1700622102301,
                "tmdate": 1700622102301,
                "mdate": 1700622102301,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "14kGg2pdRW",
            "forum": "O04DqGdAqQ",
            "replyto": "O04DqGdAqQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5652/Reviewer_MKwz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5652/Reviewer_MKwz"
            ],
            "content": {
                "summary": {
                    "value": "The paper is based on short instruction generation using closed-source LLMs through self-instruction. It proposes fine-tuning open-source LLMs with limited initial data and generating longer instructions using these models. After labeling by closed-source models, a new task-specific model is fine-tuned. The paper conducts tests on code completion, math, and commonsense reasoning tasks, demonstrating improvements relative to self-instruction methods. It also discusses the nature of instruction generation and its impact on the final fine-tuning results."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The proposed Ada-Instruct method leverages open-source models for instruction generation, reducing reliance on closed-source large models, which can lower the cost of training task-specific models.\n- Ada-Instruct outperforms self-instruct on well-controlled math and commonsense reasoning tasks, highlighting the method's effectiveness.\n- The paper compares the fine-tuned model with instructions generated via self-instruction, particularly exploring instruction quality and the impact on SFT (supervised fine-tuning), revealing insights into instruction quality and its effect on the final model."
                },
                "weaknesses": {
                    "value": "- The exploration of which instructions are useful for sft is not sufficiently clear.\n   - The paper initially points out that the issue with self-instruction is the limited length of generated instructions. However, later experiments show that Evol-Instruct with longer instructions does not perform well. The authors attribute this to \"unnatural\" instructions that do not align with downstream task distributions, but lack experimental validation. The authors can rewrite these instructions with open-source to make it more natural. Visually demonstrating the distribution relationships between training, Ada-Instruct, and Evol-Instruct can also support the argument.\n   - Ada-Instruct performs significantly better than self-instruct on math and commonsense reasoning tasks. The reviewer suggests using Figure 3 or other methods to demonstrate the improved alignment of Ada-Instruct with downstream tasks.\n- The experimental setup for code completion lacks soundness. The data for Self-Instruct and Ada-Instruct experiments are not generated from the same Initial Data, and the SFT Data quantities differ.\n- Minor points:\n   - Distinguish settings under different Params in Table 1 with identifiers. Also, Consider adjusting the placement of \"Code LLAMA-Instruct\" and \"Unnatural Code LLAMA\" in the \"Self-Instruct\" part.\n   - In Table 5, it's suggested to differentiate the \"ratio\" from the correctness of \"Generated\" and \"Real Samples\", rather than listing them side by side. This might lead to confusion regarding the interpretation of the \"ratio.\""
                },
                "questions": {
                    "value": "- Is Ada-Instruct sensitive to initial data? The paper asserts that Ada-Instruct performs better than self-instruct because data generated by tuning models with task data fits downstream task distributions better than in-context learning. Analyzing the impact of different initial samples on the final results would provide meaningful insights. Is Ada-Instruct robust when using initial samples that are not as scattered as shown in Figure 3 and do not align well with the training data?\n- In Table 1, Ada-Instruct-HumanEval and Ada-Instruct-MBPP outperform GPT-3.5, but their labels are derived from GPT-3.5. Do the authors have further explanations?\n- Would the authors consider open-sourcing their code? This would facilitate verification and follow-up work."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5652/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698579831471,
            "cdate": 1698579831471,
            "tmdate": 1699636587920,
            "mdate": 1699636587920,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QJBDUePFHG",
                "forum": "O04DqGdAqQ",
                "replyto": "14kGg2pdRW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5652/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5652/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reponse to Reviewer MKwz"
                    },
                    "comment": {
                        "value": "We thank the reviewer for recognizing that our method is effective, revealing insights into instruction quality and its effect, and lower the cost of training task-specific models. We thank the reviewer for the valuable feedback. Our responses are as below:\n\n**Weakness 1: The exploration of which instructions are useful for sft is not sufficiently clear. (1) The paper initially points out that the issue with self-instruction is the limited length of generated instructions. However, the authors can rewrite these instructions generated by Evol-Instruct with open-source to make it more natural. (2) The reviewer suggests using Figure 3 or other methods to demonstrate the improved alignment of Ada-Instruct with downstream tasks.**\n\n**Response:** We highlight that Ada-Instruct not only creates long and complex instructions but also aligns with the target instruction distribution, a capability Evol-Instruct lacks.\n\nTo provide a straightforward understanding of the difference between Ada-Instruct and Evol-Instruct, we have calculated the average token length for their instructions on HumanEval. The average token length for the original instructions is 65.4 (\u00b140.0), for Ada-Instruct it is 62.6 (\u00b131.4), while for Evol-Instruct it is 138.1 (\u00b1122.7). It is clear that the distribution of Ada-Instruct is much closer to that of the original instructions.\n\n**Weakness 2: The data for Self-Instruct and Ada-Instruct experiments are not generated from the same Initial Data, and the SFT Data quantities differ.**\n\n**Response:** Please refer to our response to General Response - Question 2.\n\n\n**Question 1: Is Ada-Instruct sensitive to initial data? Analyzing the impact of different initial samples on the final results would provide meaningful insights. Is Ada-Instruct robust when using different initial samples?**\n\n**Response:** As we have already stated in the experimental setup section, the initial samples for MBPP/MATH/GSM8k/CSQA are randomly selected, rather than deliberately chosen by humans. The effectiveness of this random strategy suggests the robustness of Ada-Instruct.\n\n\n**Question 2: In Table 1, Ada-Instruct-HumanEval and Ada-Instruct-MBPP outperform GPT-3.5, but their labels are derived from GPT-3.5. Do the authors have further explanations?**\n\n**Response:** It is likely that different versions of GPT-3.5 exhibit performance variations. The results of GPT-3.5 we cited are from the \"GPT-4 Technical Report\" published in March 2023. In contrast, Ada-Instruct employs ChatGPT as of August 3, 2023.\n\n**Question 3: Would the authors consider open-sourcing their code?**\n\n**Response:** Certainly. We will release the code for verification and follow-up work."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5652/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700408699796,
                "cdate": 1700408699796,
                "tmdate": 1700408699796,
                "mdate": 1700408699796,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "trWfA4Z1yx",
                "forum": "O04DqGdAqQ",
                "replyto": "QJBDUePFHG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5652/Reviewer_MKwz"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5652/Reviewer_MKwz"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "The author's response does not sufficiently resolve the reviewer's main concern: \"The exploration of which instructions are useful for sft is not sufficiently clear.\"\n\nIn their response, the authors emphasize that Ada-Instruct not only generates lengthy and complex instructions but also aligns with the target instruction distribution, a capability that Evol-Instruct lacks. However, the phrase \"aligns with the target instruction distribution\" in their response remains somewhat unclear. In the paper, the authors mention alignment in terms of length (Figure 1) and semantics (Figure 3), but the experimental results do not fully support either aspect.\n\nFrom the results in Figure 1 and the author's response, the reviewer understands that, from the perspective of prompt length distribution, Ada-Instruct does indeed exhibit better alignment with the original instructions compared to Evol-Instruct. However, the current experiments do not conclusively demonstrate that length alignment is the sole reason for Ada-Instruct's performance.\n\nIf the author's primary point is to underscore the importance of length alignment, the authors can add further experiments like rewriting to bring the lengths of Evol-Instruct or other baselines in line with the distribution before comparing the results.\n\nAlternatively, if the author intends to emphasize the importance of semantic alignment, additional experiments could involve incorporating the instructions generated by other baselines into Figure 3. This would provide evidence that Ada-Instruct outperforms the baselines because it exhibits better semantic alignment."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5652/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700579626669,
                "cdate": 1700579626669,
                "tmdate": 1700579626669,
                "mdate": 1700579626669,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cEfuBvxO73",
                "forum": "O04DqGdAqQ",
                "replyto": "14kGg2pdRW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5652/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5652/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Why Ada-Instruct Works and the Visualization of Semantic Alignment"
                    },
                    "comment": {
                        "value": "Ada-Instruct's idea is simple: its instructions' distribution is more aligned with the target instructions. This idea has been validated from multiple perspectives, including length consistency (Figure 1 and previous experiments) and semantic consistency (Figure 3).\n\n**We understand your concern that the gaps in Evol-Instruct's length and natural expression could be simply addressed with further prompting**. To address this, we updated Figure 3 (a) to visualize Evol-Instruct's semantic distribution (see paper's updated PDF). We switched from MPNet to OpenAI's text-embedding-ada-002 for more accurate semantic embeddings. It's evident that **Evol-Instruct significantly lags behind Ada-Instruct in semantic consistency, a gap that is hard to bridge with further prompting**."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5652/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700627839225,
                "cdate": 1700627839225,
                "tmdate": 1700628852283,
                "mdate": 1700628852283,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "JPHGpqqpRI",
            "forum": "O04DqGdAqQ",
            "replyto": "O04DqGdAqQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5652/Reviewer_B9Ki"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5652/Reviewer_B9Ki"
            ],
            "content": {
                "summary": {
                    "value": "This works proposes to train an instruction generator for generating diverse and in-domain instructions for a specific use case, and outperforms baseline approaches like self-instruct by a large margin."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The proposed method is very simple and effective, and gets to generate diverse, complex queries for constructing instruction tuning datasets. This is an effective method to extrapolate training data from models to improve domain specific instruction tuning."
                },
                "weaknesses": {
                    "value": "**Fair comparison is lacking**: The Table 1 does not present an apple-to-apple comparison, where Code LLAMA-Insturct utilizes different amount of data from Ada-Instruct-HumanEval or Ada-Instruct-MBPP. A fair comparison will be to compare self-instruct directly with Ada-instruct by controlling the amount of initial data and SFT data.   \n\n**Comparison to Evo-instruct is lacking**: Though Evo-instruct seems to generat unnatural prompt, it has shown significant improvement over normal prompting. It\u2019s necessary to directly compare Ada-instruct with Evo-instruct.   \n\n**Lack of comparison to self-instruct in Table 4**: There is no comparison to self-instruct in Table 4, and it\u2019s unclear if the proposed method outperforms simply prompting a close-source model."
                },
                "questions": {
                    "value": "- Do you use any prompt for training the instruction generator? Or you simply use the raw instruction for training? During inference, do you use any prompt, or simply decode from the beginning?\n- Typo: `Code LLAMA-Insturct` should be `Code LLAMA-Instruct` in Table 1\n- The SFT data generation process for the code llama-instruct model described in Rozi\u00e8re et al., 2023 (i.e., generate 62,000 questions with Llama 2-70b, and then extrapolate) is very different from what the authors described in the paper (i.e., using 10 initial data points to extrapolate with self-instruct), why does there exist such a discrepancy?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5652/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5652/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5652/Reviewer_B9Ki"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5652/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698715165968,
            "cdate": 1698715165968,
            "tmdate": 1700490933586,
            "mdate": 1700490933586,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fqSQwA4vx8",
                "forum": "O04DqGdAqQ",
                "replyto": "JPHGpqqpRI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5652/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5652/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reponse to Reviewer B9Ki"
                    },
                    "comment": {
                        "value": "We thank the reviewer for recognizing that our method is simple and effective for generating diverse, complex queries. Our responses are as below:\n\n**Weakness 1: Fair comparison is lacking: The Table 1 does not present an apple-to-apple comparison, where Code LLAMA-Instruct utilizes different amount of data from Ada-Instruct-HumanEval or Ada-Instruct-MBPP.**\n\n**Response:** Please refer to our response to General Response - Question 2.\n\n**Weakness 2: Comparison to Evo-instruct is lacking: Though Evo-instruct seems to generate unnatural prompt, it has shown significant improvement over normal prompting.**\n\n**Response:** We have already compared our Ada-Instruct with Evo-Instruct-based WizardCoder in Table 1. This baseline was implemented under higher resource conditions: it utilized 20,000 initial samples, significantly more than the 10 used by Ada-Instruct. Despite this, its performance was only on par with Ada-Instruct. Therefore, we believe that under identical conditions, Ada-Instruct performs better. Further evidence of this is shown in Table 2, where Ada-Instruct is closer to the target distribution compared to Evo-Instruct.\n\nMoreover, Evo-Instruct cannot address the few-shot setting that Ada-Instruct solves. Taking the specific implementation of WizardCoder as an example, it requires a large-scale initial sample pool (e.g., 20,000 samples) to evolve diverse instructions. This requirement is in conflict with the few-shot setting presented in our paper.\n\n**Weakness 3: Lack of comparison to Self-Instruct in Table 4: There is no comparison to Self-Instruct in Table 4, and it\u2019s unclear if the proposed method outperforms simply prompting a close-source model.**\n\n**Response:** We have added results for Self-Instruct using the same initial samples as Ada-Instruct, and a baseline by prompting ChatGPT, as shown in the table below. Ada-Instruct still performs the best.\n\n| Model| Initial Data  |  SFT Data  |  CommonsenseQA  |\n|  :----  | :----:  | :----:  |  :----:  |\n|Self-Instruct| 10 | 10k | 71.4 |\n|ChatGPT [1]| - | - | 74.0 |\n|Ada-Instruct| 10 | 10k | 75.5 |\n\n[1] Chen, Jiuhai, et al. \"When do you need Chain-of-Thought Prompting for ChatGPT?.\" arXiv preprint arXiv:2304.03262 (2023).\n\n**Question 1: Do you use any prompt for training the instruction generator? Or you simply use the raw instruction for training? During inference, do you use any prompt, or simply decode from the beginning?**\n\n**Response:** We have detailed the prompts we used in Appendices C, D, and E. The prompts used in the \"Train Instruction Generator\" phase and the \"Inference\" phase are essentially the same, except that we modify a few tokens to conveniently parse the answer during inference.\n\n\n**Question 2: The SFT data generation process for the code llama-instruct model described in Rozi\u00e8re et al., 2023 (i.e., generate 62,000 questions with Llama 2-70b, and then extrapolate) is very different from what the authors described in the paper (i.e., using 10 initial data points to extrapolate with self-instruct), why does there exist such a discrepancy?**\n\n**Response:** The instruction generation technique used by Code Llama-instruct is similar to that of Self-Instruct. It involves prompting Llama 2-70b with in-context prompts and examples to generate instructions. As demonstrated in Figure 1, in-context examples are not well-suited to adapt to the target instruction distribution, especially in generating longer and  complex instructions."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5652/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700408491889,
                "cdate": 1700408491889,
                "tmdate": 1700408491889,
                "mdate": 1700408491889,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ySDkKa4s9B",
                "forum": "O04DqGdAqQ",
                "replyto": "fqSQwA4vx8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5652/Reviewer_B9Ki"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5652/Reviewer_B9Ki"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thank you for your reply! I'm satisfied to see that my main concerns regarding the paper have been adequately addressed. The authors have effectively shown the strength of their approach by making comparisons with a similar or even more challenging setup, where the baseline benefits from additional resources like initial or fine-tuning data. However, there are some mysteries I don't fully understand\n- I find it surprising that 10 few-shot examples would enable the finetuned model to generate instructions that have exact the same length distribution as the target dataset. Especially when the the few-shot samples are chosen randomly. Adding a significance test for selecting this initial set of examples will make the results more plausible. \n- It lacks a study of how scale affects the instruction generators' effectiveness. Will larger finetuned models generate more diverse instructions? How about you fine-tune the close-source OpenAI models?\n\nI think adding these aspects will make the paper strong! But given the time horizon of the rebuttal, it might be unrealistic to expect a full run of these suggested experiments. I'd be happy to raise the score to 6 now that the authors have addressed the majority of the comments."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5652/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700490904606,
                "cdate": 1700490904606,
                "tmdate": 1700490904606,
                "mdate": 1700490904606,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mGpdLVLg2a",
            "forum": "O04DqGdAqQ",
            "replyto": "O04DqGdAqQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5652/Reviewer_H9NJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5652/Reviewer_H9NJ"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces Ada-Instruct, a novel method for generating instructions for complex tasks by fine-tuning open-source Large Language Models (LLMs). It provides an insight that self-instruction based on In-context Learning (ICL) struggles to generate long and complex instructions, whereas fine-tuning can produce task-aligned instructions from a few samples. The evaluation demonstrates that Ada-Instruct matches or surpasses state-of-the-art models on tasks such as code completion, math, and commonsense reasoning."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper proposes a novel self-instruct method by finetuning open-sourced LLMs to generate instruction.\n2. The insight is impressive that current self-instruct methods (ICL) prefer to generate short instructions which will lead to a distribution mismatch.\n3. The paper is well written."
                },
                "weaknesses": {
                    "value": "1. \nIn terms of innovation, the authors seem to have some misconceptions. Specifically, there have been previous works that used open-source models to generate instructions, such as the use of the open-sourced LLM Llama in [1], rather than ChatGPT or GPT-4. So what the authors mentioned in the introduction is not true: \n>A prevalent approach is called \u201cself-instruct\u201d (Wang et al., 2022), which involves having ChatGPT sequentially generate both instructions and answers (Sun et al., 2023; Peng et al., 2023; Taori et al., 2023; Schick & Schutze, 2021; Honovich et al., 2022; Ye et al., 2022; Meng et al., 2022; 2023).\n\nAnd it leads to the comparison between this work and \"previous work\" in Figure 2 being inappropriate.\n\n2. \nHumanEval and MBPP are both Python program generation benchmarks, the proposed method requires separate training on these two benchmarks which is weird and not practical. Can the authors provide the performance of the same model on both benchmarks?\n\n3. \n Also, I think one model for all reasoning tasks is needed, instead of training a separate mode for each benchmark. I wonder if training a model for all benchmarks will increase the required instruction overhead. Can the authors show some evidence against this?\n\n[1] Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision"
                },
                "questions": {
                    "value": "1. Just a suggestion: I think the authors can provide some concrete inference examples (LLM output) to show the difference between different self-instruct LLMs."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5652/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5652/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5652/Reviewer_H9NJ"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5652/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698753803214,
            "cdate": 1698753803214,
            "tmdate": 1699636587695,
            "mdate": 1699636587695,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YFR5MqqhKt",
                "forum": "O04DqGdAqQ",
                "replyto": "mGpdLVLg2a",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5652/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5652/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reponse to Reviewer H9NJ"
                    },
                    "comment": {
                        "value": "We thank the reviewer for recognizing that our method is novel, and the insight of using fine-tuning to produce task-aligned instructions from a few samples is impressive. Our responses are as below:\n\n**Weakness 1: There have been previous works that used open-source models to generate instructions, such as the use of the open-sourced LLM Llama in [1], rather than ChatGPT or GPT-4.**\n\n**Response:** We believe there is a misunderstanding by the reviewer about the novelty of our work. Dromedary [1] employs Self-Instruct technology for generating instructions, which is one of our main baselines. As shown in Figure 1, Self-Instruct struggles with creating long, complex instructions. Our key innovation is demonstrating that fine-tuning, rather than in-context prompting, overcomes this issue, producing instructions that better match downstream task distributions.\n\n**Weakness 2: The proposed method requires separate training on these two benchmarks which is weird and not practical. Can the authors provide the performance of the same model on both benchmarks?**\n\n**Response:** Please refer to our response to General Response - Question 1.\n\n**Weakness 3: I wonder if training a model for all benchmarks will increase the required instruction overhead. Can the authors show some evidence against this?**\n\n**Response:** The instruction overhead of Ada-Instruct is, in fact, lower than that of the baselines. We will explain this below.\n\nAs addressed in our response to the Weakness 2, we can merge few-shot initial samples from multiple benchmarks to enhance a specific domain model. Subsequently, due to Ada-Instruct's superior ability to fit data distributions (as seen in Figure 1 and Table 2), it incurs lower instruction overhead when enhancing a domain model. For instance, in Table A in General Response, Ada-Instruct with 10k SFT data significantly surpasses Self-Instruct-Alpaca with 20k SFT data.\n\n\n**Question 1: I think the authors can provide some concrete inference examples (LLM output) to show the difference between different Self-Instruct LLMs.**\n\n**Response:** The examples have already been listed in Table 2."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5652/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700408239144,
                "cdate": 1700408239144,
                "tmdate": 1700408397268,
                "mdate": 1700408397268,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eLr3hq7OYj",
                "forum": "O04DqGdAqQ",
                "replyto": "YFR5MqqhKt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5652/Reviewer_H9NJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5652/Reviewer_H9NJ"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the rebuttal.\n\n**Weakness 1**: I'm not questioning the experiment results here. I'm still confused about the introduction and Figure 2 since they indicate that previous works get instructions from open-sourced LLMs instead of the Llama itself. I think this claim is wrong.\n\n**Weakness 2**: I see that the two datasets is lower than Ada-Instruct-HumanEval and Ada-Instruct-MBPP but still higher than ICL. I think the authors should take this result as the main result of the paper.\n\n**Weakness 3**: I'm satisfied with this result.\n\n**Question 1**: I have seen Table 2 which is the instructions generated. But what I mean is to show the inference output of the fine-tuned LLMs (and I have emphasized this in parentheses) instead of the instructions generated.\n\nSome of my concerns have not been solved (especially weakness 1). I keep my score for now."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5652/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700638715620,
                "cdate": 1700638715620,
                "tmdate": 1700638715620,
                "mdate": 1700638715620,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]