[
    {
        "title": "Single Motion Diffusion"
    },
    {
        "review": {
            "id": "TQIJkaupU8",
            "forum": "DrhZneqz4n",
            "replyto": "DrhZneqz4n",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1777/Reviewer_BL7j"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1777/Reviewer_BL7j"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes \"Single Motion Diffusion\" to learn a generative model of skeletal motions from small training data. The key architectural ideas are 1. shallow U-Net and 2. QnA attention model for local receptive fields. The paper demonstrates applications in the temporal composition (temporal in-painting for in-betweening and motion expansion), spatial composition (spatial in-painting by constraining a part of the skeletal tree with an unseen motion), harmonization (mix low-frequency unseen motion during denoising), and style transfer using harmonization. The method is compared numerically against Ganimator [Li et al. 2022] and GenMM [Li et al. 2023] for the coverage (the rate of generated motions reproducing the input), global diversity, local diversity, inter-diversity, intra-diversity difference, and computational efficiency metrics (number of parameters and total running time). The method is also compared against MDM trained on a single clip and MDM trained on a cropped clip to simulate the narrow receptive field regarding SiFID, inter-diversity, and intra-diversity differences. The numerical evaluations are backed up by the user studies."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The core takeaway is reducing the temporal receptive field by tweaking existing architectures (reducing the U-Net depth and using the QnA transformer) allows the training of generative motion models with a very small amount of data. While the idea is simple, the technique shows promising results on important animation tasks.\n\nThorough numerical evaluations and user studies support the claim. The visual quality is also fine but with some caveats (see Weakness).\n\nThe technique is reproducible with the provided code, data, and pretrained models."
                },
                "weaknesses": {
                    "value": "The flip side of the simplicity of the technique is that the technical novelty may not be so significant.\n\nIt seems like there is a trade-off to reducing the temporal receptive field. The generated results often show abrupt transitions, such as unnaturally fast turns. I believe this is due to the limited receptive field not seeing the full segment of some longer actions. Can authors discuss this? This may indicate that we need more metrics to measure the visual quality of motions."
                },
                "questions": {
                    "value": "(Minor comment)\nFrom the abstract:\n> learn the internal motifs of a single motion sequence with arbitrary topology\n\nI initially thought \"arbitrary topology\" meant cases like generating the dragon motions from the model trained on human skeletons. I now know that this just means that the architecture itself is agnostic to the skeletal structure but the trained model must be applied to the same skeletal structure. I hope there is a way to phrase this better to reduce the confusion."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1777/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1777/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1777/Reviewer_BL7j"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1777/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698692931926,
            "cdate": 1698692931926,
            "tmdate": 1699636107101,
            "mdate": 1699636107101,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PyRqW1TIta",
                "forum": "DrhZneqz4n",
                "replyto": "TQIJkaupU8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1777/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1777/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to reviewer \u200b\u200bBL7j (R4)"
                    },
                    "comment": {
                        "value": "**\u201cIt seems like there is a trade-off to reducing the temporal receptive field. \u2026 Can authors discuss this? This may indicate that we need more metrics to measure the visual quality of motions.\u201d**\n\nFollowing this question, we added Appendix F, presenting a study about motions smoothness. In this study, we describe a metric that measures the smoothness fidelity between the generated motion to the ground truth on which the network has been trained.  We show that using a wider receptive field improves smoothness fidelity but degrades the other metrics, hence reducing motion quality. Finally, we introduce a method to enhance smoothness fidelity while maintaining a narrow receptive field, thereby preserving the quality of the results.\n\n**\u201cI initially thought \"arbitrary topology\" meant cases like generating the dragon motions from the model trained on human skeletons.\u2026\u201d**\n\n We added a clarification in the introduction (in blue). Thank you for bringing the need for this clarification to our attention."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1777/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700567671064,
                "cdate": 1700567671064,
                "tmdate": 1700567671064,
                "mdate": 1700567671064,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8b531JWOto",
                "forum": "DrhZneqz4n",
                "replyto": "PyRqW1TIta",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1777/Reviewer_BL7j"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1777/Reviewer_BL7j"
                ],
                "content": {
                    "comment": {
                        "value": "Is eq. 8 in the appendix the smoothness loss L_smooth? Does it make sense to clarify this like eq. 3, the definition of L_simple, in the appendix?\n\nCan authors provide videos of the smoothness study?"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1777/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700586966101,
                "cdate": 1700586966101,
                "tmdate": 1700586966101,
                "mdate": 1700586966101,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rjSH4kv3tz",
                "forum": "DrhZneqz4n",
                "replyto": "QUJ5PjrOR5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1777/Reviewer_BL7j"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1777/Reviewer_BL7j"
                ],
                "content": {
                    "comment": {
                        "value": "Looks all good to me.\n\nThe authors have addressed all of my concerns. I will keep my score as is."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1777/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700677407210,
                "cdate": 1700677407210,
                "tmdate": 1700677407210,
                "mdate": 1700677407210,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3itq0x57kK",
            "forum": "DrhZneqz4n",
            "replyto": "DrhZneqz4n",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1777/Reviewer_gTdv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1777/Reviewer_gTdv"
            ],
            "content": {
                "summary": {
                    "value": "Paper introduces a generative diffusion model for motion synthesis. The model is a UNet that operates on motion data with a specialized efficient local attention layer (QnA) in place of global attention. The key intuition of this work is restricting the capacity (receptive field) of the underlying UNet to improve generalization and avoid mode collapse. Results are provided on several tasks, including motion composition, harmonization and generation, and seem to be state-of-the-art."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Paper is well-written and is easy to follow. Supplementary material is of high quality.\n- The key technical novelty - to reduce receptive field - is simple and is easy to implement, and makes sense in the limited data regime.\n- Qualitatively results look impressive. User study and benchmarks also confirm performance of the method is state-of-the-art."
                },
                "weaknesses": {
                    "value": "- Authors propose a solution that fixes a specialized version of UNet (presumably similar to the one used for image generation) - by introducing local attention mechanism. I understand that it might be a way to repurpose an existing architecture developed for a different task, but it does seem like there are exist a wide range of existing architectures (1d convnets / wavenets), which exhibit the same property by construction. It is a bit unclear why is the vanilla UNet considered in the first place?\n- (Novelty, minor): diffusion models have been used for motion synthesis, the QnA has been used before, although in different context.\nMisc:\n- afaik vanilla UNet [Ronneberger'2015] does not have attention layers, do you imply the unet similar to that used in stable diffusion implementation?"
                },
                "questions": {
                    "value": "- What exactly is meant by \"single input UNet\" in 4.? \n- Why is the UNet the default option here? Wouldn't LSTMs / WaveNet / TCNN would be a better option?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1777/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698791095794,
            "cdate": 1698791095794,
            "tmdate": 1699636107033,
            "mdate": 1699636107033,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TzrxqNtX8G",
                "forum": "DrhZneqz4n",
                "replyto": "3itq0x57kK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1777/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1777/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to reviewer gTdv (R3)"
                    },
                    "comment": {
                        "value": "**\u201cWhy is the vanilla UNet considered in the first place?\u201d**\n\n**\u201cWhy is the UNet the default option here? Wouldn't LSTMs / WaveNet / TCNN would be a better option?\u201d**\n\nWe have considered several backbones, focusing on architectures that have demonstrated the highest scores in previous research and have already won ablation studies against alternative models. Our initial choice for a backbone has been a UNet, a selection that aligns with the consensus among imaging diffusion works (Ho et al., 2020; Song et al., 2020a; Dhariwal & Nichol, 2021; Saharia et al., 2022) and has demonstrated effective performance.In addition to the empirical results, Jolicoeur-Martineau et al. (2020) demonstrate that UNets significantly enhance sample quality compared to previous architectures used for denoising score matching. Our second choice of backbone is the Transformer Encoder, used by most motion diffusion works (Kim et al., 2022;Tevet et al., 2023, \u2026). Tevet et al. (2023) conduct ablation studies and show that a Transformer Encoder performs better than a Transformer Decoder and better than a GRU (a GRU is similar to the LSTMs mentioned in R3\u2019s question). In our ablation studies we show that in the case of a single motion input, a UNet performs better than a Transformer.\n\n[Alexia Jolicoeur-Martineau, R\u00e9mi Pich\u00e9-Taillefer, R\u00e9mi Tachet des Combes, and Ioan- nis Mitliagkas. Adversarial score matching and improved sampling for image generation. arXiv:2009.05475, 2020.]\n\n**\u201cafaik vanilla UNet [Ronneberger'2015] does not have attention layers, do you imply the unet similar to that used in stable diffusion implementation?\u201c**\n\n Our \"vanilla UNet\" is the one employed in early imaging diffusion works, e.g., DDPM (Ho et al., 2020), which includes attention layers. We acknowledge the concern raised by R3 that the term \"vanilla UNet\" might lead the reader to question whether we are referring to Ronneberger2015's UNet or Ho2020's UNet . We have added a clarification in Section 4 (in blue).\n\n**\u201cWhat exactly is meant by \"single input UNet\" in 4?\u201d**\n\nIt means a UNet trained on a single input. We revised Section 4 for better clarity. Thank you for bringing it to our attention."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1777/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700567653922,
                "cdate": 1700567653922,
                "tmdate": 1700567653922,
                "mdate": 1700567653922,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "iLbrSgByO9",
            "forum": "DrhZneqz4n",
            "replyto": "DrhZneqz4n",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1777/Reviewer_qd2e"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1777/Reviewer_qd2e"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces SinMDM, a Single Motion Diffusion Model based on the motivation that the number of data for the motion domain is limited. SinMDM learns from a single motion sequence, generating motions true to original motifs, while avoiding overfitting through a lightweight, attention-focused architecture (narrow respective field). It demonstrates various motion tasks, including spatial and temporal composition, as well as Harmonization (style transfer). SinMDM shows high efficiency and good performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The usage of the Diffusion model in the single-motion generation is interesting. \n\n2. Also, this paper demonstrates various interesting applications of motion synthesis with efficiency and good quality. \n\n3. In order to overcome overfitting and promote diversity, SinMDM proposes to use the shallow QnA module to limit the receptive field and relative temporal positional embeddings."
                },
                "weaknesses": {
                    "value": "1. Single-motion generation task and using Diffusion to do single-instance generation is not new. Some concepts have been explored in previous papers. For example, Sinfusion[1] has pointed out that the receptive field needs to be reduced for single-instance generation task. The only difference is that they use ConvNext while SinMDM uses QnA module. \n\n2. Also, for the Harmonization, the way of using guidance injection is from ILVR [2]. \n\nMaybe, could you elaborate on this, and emphasize the contribution of this work?  \n\n[1] Nikankin, Yaniv, Niv Haim, and Michal Irani. \"Sinfusion: Training diffusion models on a single image or video.\" ICML 2023.\n\n[2] Choi, Jooyoung, et al. \"Ilvr: Conditioning method for denoising diffusion probabilistic models.\"ICCV 2021."
                },
                "questions": {
                    "value": "1. As the receptive field is narrowed, how to guarantee temporal smoothness? As shown in Table 3, using a wide receptive field gives better FID.   \n\n2. Comparison with Ganimator in HumanML3D?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1777/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1777/Reviewer_qd2e",
                        "ICLR.cc/2024/Conference/Submission1777/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1777/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698802616476,
            "cdate": 1698802616476,
            "tmdate": 1700685745918,
            "mdate": 1700685745918,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nvS33XvEQm",
                "forum": "DrhZneqz4n",
                "replyto": "iLbrSgByO9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1777/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1777/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to reviewer qd2e (R2)"
                    },
                    "comment": {
                        "value": "**\u201cAs the receptive field is narrowed, how to guarantee temporal smoothness? As shown in Table 3, using a wide receptive field gives better FID.\u201d**\n\nFollowing this question, we added Appendix F (in blue), presenting a study about motion smoothness. In this study, we describe a metric that measures the smoothness fidelity between the generated motion to the ground truth on which the network has been trained.  We show that using a wider receptive field improves smoothness fidelity but degrades the other metrics, thus reducing motion quality. Finally, we introduce a method to enhance smoothness fidelity while maintaining a narrow receptive field, thereby preserving the quality of the results.\n\nAs noticed by R2, using a wide receptive field results in better FID. However, a 'near-perfect' FID indicates overfitting to the input motion, as evidenced by poor scores in diversity metrics.\n\n**\u201cComparison with Ganimator in HumanML3D\u201d**\n\nWe added a row to Tab. 2, exhibiting the metric scores of Ganimator on the HumanML3D dataset, and demonstrating once again that SinMDM performs better. Note that Ganimator\u2019s scores demonstrate a very high FID, indicating divergence from the input motion. The updated radar plot reveals a relatively small area for Ganimator, suggesting that it faces challenges in generalizing to other datasets.\nRunning Ganimator on HumanmML3D is not straight forward, as Ganimator expects an entirely different data format. To ensure a fair comparison, we transformed the HumanML3D benchmark into the data structure utilized by Ganimator, enabling Ganimator to leverage its skeleton-aware framework. To evaluate the results using the HumanML3D metrics, we reverted them to the HumanML3D format.\n\n**\u201ccould you elaborate on this, and emphasize the contribution of this work?\u201d**\n\nIndeed, the performance of SinMDM is built upon the fundamental concepts of Motion Diffusion Models. However, as Motion Diffusion Models typically rely on  large datasets, adapting them to a single motion required a thoughtful redesign. In Section 2, we note that Wang et al. (2022) and Nikankin et al. (2023) use a UNet with limited depth, a strategy not directly applicable to the motion domain. Unlike images with a regularized 2D spatial structure, motion data consists of non-regularized skeletal joints with a temporal axis and fewer degrees of freedom. To bridge this gap, we propose a novel architecture that combines the strengths of the traditional UNET and recent QNA architectures. This effective adaptation to the motion domain enables the application of advanced techniques such as ILVR, as credited in Section 5.2.\n\nWe have revised Section 2 to provide an explanation of the distinction between the imaging and motion domains. Thank you for bringing the need for this clarification to our attention."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1777/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700567634744,
                "cdate": 1700567634744,
                "tmdate": 1700567634744,
                "mdate": 1700567634744,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TSNhp7g3aG",
                "forum": "DrhZneqz4n",
                "replyto": "2EcoT7iRuN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1777/Reviewer_qd2e"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1777/Reviewer_qd2e"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the detailed explanation from the authors. After reading the author's response and the discussion with other reviewers, I will raise my score."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1777/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700685718397,
                "cdate": 1700685718397,
                "tmdate": 1700685718397,
                "mdate": 1700685718397,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "GqgWLdjKgR",
            "forum": "DrhZneqz4n",
            "replyto": "DrhZneqz4n",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1777/Reviewer_ep7Y"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1777/Reviewer_ep7Y"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a motion diffusion model (SinMDM) that utilizes a QnA-based UNet architecture in the motion domain. The model incorporates QnA layers, which enable local attention with a temporally narrow receptive field, resulting in improved efficiency in space and time. The use of QnA layers allows for fast and efficient implementation, enhancing the model's performance compared to global attention layers. The paper validates the design choices through experiments and provides a comprehensive list of hyperparameters for reproducibility. The paper demonstrates various applications of single-motion learning using diffusion models. These applications include motion composition, motion harmonization, and straightforward use for long motion generation and crowd animation. The authors showcase the effectiveness of the proposed model in these applications, highlighting its ability to handle complex tasks like harmonization and style transfer."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The proposed method is the first work that applies diffusion to single motion learning. The paper is well-written and details are clearly presented. The methodology is well-designed in the way to reduce the range of receptive field to prevent acquiring global context and overfitting. The results in the paper and the supp video are of high quality. Applications are interesting and making use of the diffusion framework. The authors also presents a user study that demonstrates the superiority of the proposed model in terms of diversity, fidelity, and quality."
                },
                "weaknesses": {
                    "value": "- As restricted by the source of data, the results of some editing applications are somehow not that impressive. Similar results can be achieved by simple manipulations without resorting to a powerful diffusion model. E.g. the spatial composition looks no more than cutting-and-pasting the upper/lower body motion, and the style transfer application is like temporally synchronising the pace of the steps of the happy/crouched style video to the content.\n\n- Although a solid work, most techniques used in the methodology are from existing papers, including diffusion for motion, and single motion generation, and the contribution to the community is limited."
                },
                "questions": {
                    "value": "- Are there motion representations other than the adopted one from HumanML3D tested for the proposed method? How do you expect the motion representation could affect the generation performance?\n \n- In what circumstances does the method fail?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1777/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698829900610,
            "cdate": 1698829900610,
            "tmdate": 1699636106829,
            "mdate": 1699636106829,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "id5alaYU0r",
                "forum": "DrhZneqz4n",
                "replyto": "GqgWLdjKgR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1777/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1777/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to reviewer ep7Y (R1)"
                    },
                    "comment": {
                        "value": "**\u201cAre there motion representations other than the adopted one from HumanML3D tested for the proposed method? How do you expect the motion representation could affect the generation performance?\u201d**\n\nSinMDM is experimented with three different representations: (1) The HumanML3D representation, including joint rotations, global locations and velocities; (2) The Mixamo representation, which includes rotations and global location for the root joint only; and (3) The Zoo representation, which requires extra degrees of freedom and includes rotations + xyz relative location for all joints. \n\nFor all datasets, we use rotations of the  6D format, which has been proven by prior work to yield optimal results (See Table 2 in the GANimator paper).\n\nOverall, we showed that our approach is robust with different motion representations.\n\n\n**\u201cIn what circumstances does the method fail?\u201d**\n\nIn addition to the limitations mentioned in the conclusions section, we observe a failure case that is common to prior works as well - sub-motions are played in an arbitrary order, and this can be a disadvantage when a sequence of sub-motions needs to be executed in a precise order. For example, while doing the bird dance, one should clap hands immediately after shaking the hips. However, single motion algorithms will place these two sub motions in a temporally arbitrary order.\n\nThis can be addressed by enlarging the receptive field (at the cost of lower diversity). We added this limitation to the conclusions section (see in blue).\n\n**\u201cResults of some editing applications... Similar results can be achieved by simple manipulations\u201d.**\n\nMany of the applications enabled by SinMDM may also be implemented using designated algorithms. However, these designated algorithms are neither simple nor general, while ours provides a one-stop-shop for all applications, and yields better quality results when compared to simple manipulations.\n\nFor spatial composition, using cut-and-paste may yield a motion where the hands and the legs are un-synched, for example, in a walk motion, we expect that when the left hand is at the front, the right hand will also be at the front. In addition, when using cut-and-paste there is no balance compensation when the body leans to a specific direction, for example, when the upper body leans to the right, the pelvis should balance by leaning to the left. The motion generated by cut-and-paste is merely a copy of the upper/lower part of the inputs, hence has no flexibility to adjust them to match each other. On the other hand, the motion generated by SinMDM can be different from the one on which it was trained, thus there can be seamless synchronization between the upper and lower body parts. \nIn the context of the style transfer application, while manually synchronizing the pace of steps may yield comparable results, SinMDM achieves this straightforwardly, offering a solution that would otherwise demand a designated, complicated approach, as shown in prior works (Menardais et al., 2004; Lee et al., 2020; Panagiotakis et al., 2013). Temporal synchronization requires the identification of both motions' rhythms, a non-straightforward task by itself. Moreover, stretching one motion according to the rhythm of another is naively done by a  linear interpolation, which can cause motion blur, and may not be linear by nature (say, accelerating a leg motion at the beginning of the step and slowing it as it continues). Our model has no such challenges. It generates the required motion such that it is natural, maintaining the motion motifs of the learned input.\n\n[Stephane Menardais, Richard Kulpa, Franck Multon, and Bruno Arnaldi. Synchronization for dynamic blending of motions. In Proceedings of the 2004 ACM SIGGRAPH/Eurographics symposium on Computer animation, pp. 325\u2013335, 2004.]\n\n[Juyoung Lee, Myungho Lee, Gerard Jounghyun Kim, and Jae-In Hwang. Effects of synchronized leg motion in walk-in-place utilizing deep neural networks for enhanced body ownership and sense of presence in vr. In Proceedings of the 26th ACM Symposium on Virtual Reality Software and Technology, pp. 1\u201310, 2020.]\n\n[Costas Panagiotakis, Andre Holzapfel, Damien Michel, and Antonis A Argyros. Beat synchronous dance animation based on visual analysis of human motion and audio analysis of music tempo. In International symposium on visual computing, pp. 118\u2013127. Springer, 2013.]"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1777/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700567604593,
                "cdate": 1700567604593,
                "tmdate": 1700567604593,
                "mdate": 1700567604593,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]