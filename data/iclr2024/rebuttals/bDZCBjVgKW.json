[
    {
        "title": "Fast Post-training Analysis of NeRFs Using A Simple Visibility Prediction Network"
    },
    {
        "review": {
            "id": "E0fBEaxPXC",
            "forum": "bDZCBjVgKW",
            "replyto": "bDZCBjVgKW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6640/Reviewer_Ekv4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6640/Reviewer_Ekv4"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors facilitate the post-training analysis of NeRFs by presenting a visibility prediction network (VPN) that estimates the visibility of any point in space from any input camera and a visibility scoring function that evaluates the reliability of rendered points. The VPN takes a 3D point as input and outputs a K-dimensional vector, where each element represents the logit of the probability that the point is visible from the corresponding input camera.  The authors demonstrate the effectiveness of their approach on two post-training analysis tasks: 1) reducing rendering artifacts, and 2) selecting additional training images to retrain a NeRF and improve its rendering quality."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is mostly well-written.\n2. Experimental results show the effectiveness of the proposed approach on two downstream tasks."
                },
                "weaknesses": {
                    "value": "The following weaknesses are ordered by importance:\n\n1. Experimental results are insufficient. The method is only evaluated on a dataset collected by the authors. More common benchmarks should be considered for a thorough evaluation. Additionally, the authors need to better visualize and describe the collected dataset. The paper lacks free-viewpoint-rendered videos for qualitative presentation and does not compare the proposed method to other approaches for improving NeRF rendering quality. The proposed procedure is only applied to nerfacto, but not other NeRF-based methods, which limits its generalizability.\n\n2. The proposed approach is not very convincing. The idea is straightforward and lacks novelty, and the authors do not provide a strong motivation for using the visibility score as a criterion. Equation 2 requires further explanation or derivation to enhance the reader's understanding.\n\n3. I am not convinced of the rationale behind the task of selecting additional views.\n\n3. The VPN needs to be trained concurrently with NeRF, which is not as simple as just a post-training process. The authors should emphasize this aspect.\n\n4. Other minor issues: a. Most of the citations should be parenthesized. b. The numbers on the n-axis of Figure 2 overlap with each other, making it difficult to read. c. Figure 3 does not clarify the unit for PSNR and is not presented in a reader-friendly manner. d. The introduction section lacks important citations, which should be added to provide a more comprehensive background and context."
                },
                "questions": {
                    "value": "1. In the abstract, it is stated that the visibility prediction network can predict the visibility of any point in space from any of the input cameras. How is this achieved? Can you explain the approach in more detail?\n\n2. The paper mentions that \"the NeRF network and visibility prediction network are optimized using different loss functions and influence each other during the concurrent training.\" How exactly does the training behavior of NeRF get influenced by the visibility prediction network? Can you explain this in more detail?\n\n3. The paper proposes a method for skipping low-visible near-range points to reduce rendering artifacts. How are the visibility and depth thresholds for skipping determined? Since the visibility of points can vary significantly across different scenes, how does the proposed method handle this variability?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6640/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6640/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6640/Reviewer_Ekv4"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6640/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698468218690,
            "cdate": 1698468218690,
            "tmdate": 1699636758711,
            "mdate": 1699636758711,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "T63xpklq9Y",
            "forum": "bDZCBjVgKW",
            "replyto": "bDZCBjVgKW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6640/Reviewer_2Uqd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6640/Reviewer_2Uqd"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a point-wise visibility prediction network (VPN) to facilitate the post-training analysis of NeRF rendering images. A visibility scoring function is also provided to characterize the reliability of the rendered points. Two downstream post-training analysis tasks that skip unreliable near-range points and select extra training images to re-train a better NeRF are illustrated for demonstration. The numerical experiment and visualizations show the efficiency of the proposed method on a self-collected dataset."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "\u2022\tThe paper is well motivated to the topic of visibility-based post-training analysis."
                },
                "weaknesses": {
                    "value": "The experiments need to be strengthened to support the proposed method.\n\n  * The visibility of sampled points along a given ray is defined as (2). While using (2) for guiding the binary cross entropy loss in (6), it is curious how much the extra training time compared with the original NeRF training.\n\n  * It would be better if Figure 3 comprised two other metrics (SSIM, LPIPS).\n\n* It would be better if the experiments were also conducted on some well-known NeRF datasets.\n\n* In Table 1, what is the upper bound using the visibility defined in (2) rather than the predicted one?\n\n* In Task 1, what is the performance gain compared with distortion loss?\n\n* In Task 2, what is the performance if using Rule 1, Rule 2, randomization?"
                },
                "questions": {
                    "value": "The primary concern of this paper is that its experiments need to be strengthened to support the idea. Please see [Weaknesses] for more details."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6640/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698655907974,
            "cdate": 1698655907974,
            "tmdate": 1699636758599,
            "mdate": 1699636758599,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "J4aK9mzViy",
            "forum": "bDZCBjVgKW",
            "replyto": "bDZCBjVgKW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6640/Reviewer_Fmeq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6640/Reviewer_Fmeq"
            ],
            "content": {
                "summary": {
                    "value": "This work introduces a new way to judge the confidence of a 3D point in a learnt radiance field. They propose a k-dimensional term called visibility for a 3D point which tells how clearly a point is visible from the k different training views. They provide a statistical formula for this term and show that it requires a lot of computation. As a workaround, they suggest to use a neural network to predict this term by calculating the original visibility values and regressing against them. For training purposes, the visibility network is accompanied by a dense grid which helps to determine whether a point lies within the field-of-view of the k-th camera or not. The paper showcases the benefits of their analysis using two different use-cases. The first use-case is the skipping of low-visible near-range points during rendering. The second is the selection of additional views that can improve the NeRF rendering quality. The visibility map produced by the method can also be used to remove artefacts from images. The authors show a cool application by inpainting a 2D rendered image with stable diffusion."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is well written and easy to read.\n- The authors are up-to-date with the recent advances in the field and have cited them.\n- The paper has targetted an important problem of judging the confidence of NeRFs from views where camera poses are not present (where metrics like PSNR, SSIM, etc. cannot be used due to lack of ground truth).\n- The paper incorporates the proposed method to improve the quality of NeRF and present it using results on the Nerfacto variant. The approach presented should be applicable to almost all ray-casting based NeRF variants."
                },
                "weaknesses": {
                    "value": "- The work relies on a very heavy grid alongside the neural network to predict the visibility. The paper states that they deal with datasets which have hundreds of views leading to an overwhelmingly huge grid size. I am concerned with the feasibility of their proposed method as a post-processing step given that it involves such heavy lifting.\n- It seems that the authors are not releasing the dataset. Please read the clarification requested below."
                },
                "questions": {
                    "value": "- The authors say that they evaluate their approach on a large benchmark called ObjectScan. The authors also say that they scan the dataset themselves. Are the authors calling their own captured dataset ObjectScan? If no, then original ObjectScan dataset is not cited. If yes, then why is the dataset not listed as a contribution? Are they not planning to release it? The authors are requested to clarify the source of the dataset and it's release conditions.\n- The authors use an FoV grid which is coupled with the visibility prediction network. They state \"For training the visibility prediction network, ...\". Does this imply that the grid is only used for training? Is it not used for inference at all? If it is not used for inference, then how can the network predict the visibility correctly since the multiplicative term will be missing?\n- If the authors are using the grids for both training and inference, then is a neural network requierd to predict the visibility? The 3D grid stores a k-dimensional vector at each location and the neural network is also predicting a k-dimensional vector and both of them are multiplied. In this scenario, the grid itself should be sufficient to learn the visibility especially since the network is taking only the position as input (and no view-direction). Grids are an explicit representation of it. Did the authors try this? (I am mentioning this approach since it seems more efficient. I have worked with this kind of setup. Hence, this question.) These aspects need to be mentioned very clearly.\n- The authors should give details about the training procedure and timings for their methods. Post-processing methods are usually meant to be lightweight and fast. It doesn't seem that it is the case for this work.\n\n\nAdditionally, the authors should check out highly relevant concurrent works:\u2028https://bayesrays.github.io and\u2028https://repo-sam.inria.fr/fungraph/active_camera_placement. They seem to be\u00a0 relevant to their work and will help in better understanding."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "Nothing special in this work"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6640/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6640/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6640/Reviewer_Fmeq"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6640/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698831696045,
            "cdate": 1698831696045,
            "tmdate": 1701063002879,
            "mdate": 1701063002879,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "fYpfkvLdzr",
            "forum": "bDZCBjVgKW",
            "replyto": "bDZCBjVgKW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6640/Reviewer_jS1P"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6640/Reviewer_jS1P"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to learn an additional network to predict the visibility of each point at each input view. The predicted visibility information can be used to down-weight the 3D points with low visibility probability during the volumetric rendering or as a criterion to select additional views to retrain the nerf. The PSNR/SSIM/LPIPS are improved in these two cases."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.  A method to distill the visibility information from density values to a lightweight network.\n2.  The experimental results verify the improvement of rendering quality by integrating the visibility information."
                },
                "weaknesses": {
                    "value": "1. There already exists research papers that investigate the visibility information into neural implicit representation, such as \"Neural Rays for Occlusion-aware Image-based Rendering\" in CVPR 2022, but not cited or discussed in this paper. \n\n\n2.   A minor issue\uff1athe 3D points in the air before the object surfaces are transparent, not occluded. It might be confusing to classify them as low visibility points."
                },
                "questions": {
                    "value": "Eq.6 means the visibility prediction network tries to learn how to map a 3D point to its average v(p) for all views. It resembles the behavior of integrating the density values along all rays for each 3D point. While the experimental results show that it is beneficial to the rendering quality, but it might lead to holes in the rendering results. Did you notice that in your experimental results\uff1f"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6640/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6640/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6640/Reviewer_jS1P"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6640/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698845247989,
            "cdate": 1698845247989,
            "tmdate": 1699636758342,
            "mdate": 1699636758342,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]