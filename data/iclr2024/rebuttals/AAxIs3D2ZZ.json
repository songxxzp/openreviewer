[
    {
        "title": "RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback"
    },
    {
        "review": {
            "id": "oJTHjEBYPY",
            "forum": "AAxIs3D2ZZ",
            "replyto": "AAxIs3D2ZZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3762/Reviewer_LzbJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3762/Reviewer_LzbJ"
            ],
            "content": {
                "summary": {
                    "value": "This paper compares Reinforcement Learning from AI-generated intermediate Feedback (RLAIF) with RLHF in summarization and dialog generation tasks. It also investigates techniques to improve AI-generated preference alignment."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. The paper is well-organized, making it easy to follow.\n2. It demonstrates RLAIF\u2019s comparability to RLHF in specific tasks and provides optimal settings, offering a more cost-effective solution for AI alignment\u2014a significant contribution given the experiment\u2019s high cost and urgency."
                },
                "weaknesses": {
                    "value": "1. The study only uses non-public \"palm2\" models, reducing its credibility. Including open-source models could strengthen its validity.\n2. The tasks are confined to summarization and dialog generation. Exploring additional areas like QA, code generation, or translation could provide a more comprehensive understanding of AI and human feedback interactions."
                },
                "questions": {
                    "value": "Incorporating an exploration of widely used algorithms like Proximal Policy Optimization (PPO) could enrich the study\u2019s findings."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3762/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698762185880,
            "cdate": 1698762185880,
            "tmdate": 1699636332495,
            "mdate": 1699636332495,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qIcBMawfqu",
                "forum": "AAxIs3D2ZZ",
                "replyto": "oJTHjEBYPY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3762/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3762/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer LzbJ"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful comments and time spent reviewing our paper. We address each point below:\n\n__Public Availability of PaLM 2 Models__\n\nThank you for raising the concern regarding the availability of PaLM 2 models. PaLM 2 models are publicly available through Google\u2019s PaLM API (https://developers.generativeai.google/guide/palm_api_overview) and Vertex AI (https://cloud.google.com/vertex-ai/docs/generative-ai/learn/models), which enhances the reproducibility of our results. We will explicitly mention this in the revised paper to ensure clarity for readers.\n\n__Experimenting on More Tasks__\n\nWe conduct experiments on two more tasks to further strengthen our findings - specifically on question/instruction answering and harmless dialogue generation. For question/instruction answering, we find that RLAIF and RLHF both outperform the SFT model with 54% win rates (statistically significantly different from 50%). For harmless dialogue generation, we find that the RLAIF and RLHF outperform the supervised baseline, with SFT, RLHF, and RLAIF responding with harmless replies 64%, 76%, and 88% of the time, respectively. Across all 4 tasks, we find that RLAIF achieves comparable results to RLHF.\n\nWe acknowledge that there are many more tasks that could be investigated, and we believe that the positive results on these four tasks provide a promising indication that RLAIF can achieve performance comparable to RLHF.\n\n__Additional contributions to the paper__\n\nTo enhance the contribution of this work to the research community, we have conducted several additional experiments that expand the scope of our investigation. Specifically:\n* We have compared RLHF and RLAIF on a new task: harmless dialogue generation using Anthropic's Helpful/Harmless preference dataset\n* We have also investigated the performance of RLAIF with an AI labeler of the same size as the policy\n* We have experimented with bypassing the reward model by directly using feedback from a LLM\n* We have conducted experiments combining RLHF and RLAIF \n\nThese additional experiments will be incorporated into the revised paper, providing a more comprehensive and impactful contribution to the field."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3762/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700090745891,
                "cdate": 1700090745891,
                "tmdate": 1700149505811,
                "mdate": 1700149505811,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "iNMt9fqGS4",
            "forum": "AAxIs3D2ZZ",
            "replyto": "AAxIs3D2ZZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3762/Reviewer_RhkW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3762/Reviewer_RhkW"
            ],
            "content": {
                "summary": {
                    "value": "This paper compares the efficacy of reinforcement from human feedback (RLHF) and reinforcement from AI feedback (RLAIF) on the PaLM 2 XS model, for the tasks of summarization and \"helpful dialogue generation\" (Bai, et al. 2022). The paper found that when using a PaLM 2 Large model as the AI labeler, the performance of RLHF and RLAIF were similar. The paper also includes a number of experiments on the use of chain-of-thought in the AI labeler, the size of the AI labeler, and the number of feedback examples in RLHF/RLAIF."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is generally clear and well-written, although see below for some framing suggestions. The analysis of the amount of human/AI feedback needed to reach maximum performance is interesting, as is the effect of chain-of-thought and self-consistency on the alignment between AI and human annotations. The qualitative analysis also hints at an interesting topic, namely that RLHF and RLAIF-trained models may be optimizing slightly different objectives; however, it does not give a full treatment to this topic (again, see below for more comments)."
                },
                "weaknesses": {
                    "value": "It should be unsurprising that a weaker model (PaLM 2 XS) can be improved based on feedback from a larger model (PaLM 2 L). As noted by the authors in Section 2.2, this can be viewed as a distillation result. While I believe the paper still has some valuable insights, I wish it would be more clear upfront (e.g., in the abstract) that the RLAIF setting described within is one where the target model and the labeling model differ in size.\n\nThe qualitative analysis in Section 5 is relatively shallow and would benefit from some additional justification: (1) can you provide more conclusive evidence that these trends exist, e.g,. with human labelers?; and (2) can you provide a hypotheses as to why these trends occur? For example, given that the paper notes only 78% agreement between human and AI labelers, further effort could be put into distinguishing between the labels used to train RLHF and RLAIF, which could elucidate downstream differences in trained model behavior.\n\nThe paper also runs a number of experiments to determine whether chain-of-thought reasoning and self-consistency improve alignment between human and AI labelers, finding that self-consistency does not lead to improvements. However, given that human ratings are (1) subjective and (2) subject to noise, the paper should more seriously consider the possibility that lower \"AI Labeler Alignment\" may not necessarily lead to worse downstream performance. This is partially discussed in Section 4.6, but the authors consider only a single comparison and do not directly compare the two model outputs, instead comparing both of them individually to a supervised fine-tuned baseline.\n\nA minor note: I find it strange that this paper is titled \"RLAIF\" when it is not the first work to use or introduce the term. See for example Bai, et al. 2022 (\"Constitutional AI\") for earlier usage of this term. I would recommend the authors remove the title and simply use the subtitle"
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3762/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698804068983,
            "cdate": 1698804068983,
            "tmdate": 1699636332408,
            "mdate": 1699636332408,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "B8O9bxcpH8",
                "forum": "AAxIs3D2ZZ",
                "replyto": "iNMt9fqGS4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3762/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3762/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer RhkW"
                    },
                    "comment": {
                        "value": "Thank you for your time and for reviewing our paper carefully. We greatly appreciate your suggestions and address them to the best of our ability below:\n\n__Contribution Value of this Work__\n\nWe have conducted an additional experiment in which the AI labeler matches the size of the policy and RM. Here, RLAIF surpasses the initial SFT policy, achieving a win rate of 68%. We hope this addresses your initial concern, and we have incorporated this into Sections 3.2 and 4 of the revised paper.\n\nWe also believe that our experiments with the larger AI labeler remain valuable to the research community. We agree that it is not entirely unexpected that a weaker model improves based on feedback from a larger model, given that RLHF improves a model with feedback from humans, which can be viewed as a super-capable model. However, the significance of our work lies in quantifying the performance gap between AI and human feedback. Our findings, which indicate that directly replacing human feedback with LLM feedback can achieve comparable results to RLHF, are novel and have not been explored in previous work. We believe this is a valuable contribution to the research community.\n\nAs you have suggested, we will also make it clearer that we have two scenarios - one where the AI labeler is larger than the policy, and another where it is the same size.\n\n__Qualitative Analysis__\n\nIn response to your concerns regarding the qualitative analysis, we have implemented three changes.\n\nFirst, we have initiated human evaluation to assess whether the observed trends exist at scale. If we find evidence of the trends, we will provide hypotheses as to why these trends occur. The results of this evaluation will be incorporated into the revised Section 5. \n\nSecond, to reflect the explorative nature of this analysis, we have renamed Section 5 from \"Qualitative Analysis\" to \"Qualitative Observations.\" This emphasizes that the findings represent observations rather than definitive patterns.\n\n__Impact of AI Labeler Alignment on Downstream RL Policy__\n\nTo address your concern regarding the lack of direct comparison between the two model outputs in Section 4.6 \u201cEnd-to-End Sensitivity to AI Labeler Alignment\u201d, we have added a head-to-head evaluation of the win rates of the two RLAIF policies. We find that the policy trained with higher preference label alignment (78% alignment) achieves a win rate of 59% over the policy trained with lower alignment (76% alignment). We will incorporate these results into the revised paper.\n\nWe also acknowledge the inherent subjectivity and noise associated with human preference ratings. To mitigate this issue, we employed three raters to evaluate each rating instance. Additionally, the prompt provided to the AI preference labeler closely mirrors the instructions given to human evaluators, leading us to expect a strong correlation between AI Labeler Alignment and the final policy's performance.\n\nDespite these measures, we recognize the possibility that higher AI Labeler Alignment may not necessarily translate to improved downstream performance. Due to the resource-intensive nature of conducting these experiments and human evaluation, we do not intend to conduct extensive studies to fully explore this relationship, which is deserving of an entire study on its own. However, we will clearly acknowledge that we cannot definitively conclude that higher AI Labeler Alignment consistently leads to better downstream performance.\n\n__Framing of Paper__\n\nTo make it explicit that we did not introduce RLAIF, we have amended the Abstract to directly credit Bai et al.. In the second sentence of the abstract, we write, \u201cRL from AI Feedback (RLAIF) is an alternative solution introduced by Bai et al. that generates preferences using an off-the-shelf LLM in lieu of human annotators.\u201d\n\nGiven that our work is the first to study RLAIF in such depth, we believe that a fair compromise is to leave \u201cRLAIF\u201d in the title while updating the abstract to make it abundantly clear that RLAIF was first introduced elsewhere.\n\n\n__Additional Contributions to the Paper__\n\nTo enhance the contribution of this work to the research community, we have conducted several additional experiments that expand the scope of our investigation. Specifically:\n\n* We have compared RLHF and RLAIF on a new task: harmless dialogue generation using Anthropic's Helpful/Harmless preference dataset\n* We have also investigated the performance of RLAIF with an AI labeler of the same size as the policy\n* We have experimented with bypassing the reward model by directly using feedback from a LLM\n* We have conducted experiments combining RLHF and RLAIF\n\nThese additional experiments will be incorporated into the revised paper, providing a more comprehensive and impactful contribution to the field.\n\n\n__Final Word__\n\nIf we have adequately addressed your concerns, we would greatly appreciate a re-evaluation of your initial rating. Otherwise, please let us know your additional concerns."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3762/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700172944993,
                "cdate": 1700172944993,
                "tmdate": 1700172944993,
                "mdate": 1700172944993,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "s9RjoMSC4X",
            "forum": "AAxIs3D2ZZ",
            "replyto": "AAxIs3D2ZZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3762/Reviewer_BDUS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3762/Reviewer_BDUS"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents and studies RLAIF, an alternate to RLHF wherein the preference data is synthetically produced by an LLM. \n\nThe preference labeling is done by prompting a Palm2-L model, with a prompt that consists of (i) a base/detailed preamble, (ii) optional exemplars, (iii) sample (context + 2 responses) and (iv) ending string. The label is obtained by considering the logprobs for 1 and 2 (after \"Preferred Response=\"). When generating preference labels, the paper also considers (i) CoT reasoning, (ii) self consistency, (iii) mitigating position bias by doing two inference passes. \n\nAn RM is trained on the generated preference data, and used to train an LM via RL. There are 3 evaluations: (1) AI labeler alignment which measures the accuracy of the synthetic AI-generated preference data against human preferences, (2) pairwise accuracy of the RM and (3) the human winrate of the RLAIF-trained LMs.\n\nExperiments are conducted on two domains: summarization (tldr) and helpful dialog (anthropic hh-rlhf). There are several takeaways, listed below:\n\n(1) For preference labelling: CoT helps across both domains, inconsistent results between detailed and base preamble, size of the AI labeler helps. Self-consistency (higher temperature) and few-shot exemplars hurt performance. \n\n(2) The RM converges faster (relative to human preferences) with AI generated feedback\n\n(3) RLAIF and RLHF both outperform an SFT model with a winrate of 70% (summarization) and 60% (helpful dialog). RLAIF vs RLHF has a winrate of 50%, suggestion equal performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "This paper presents a comprehensive study of the role of LLM-generated feedback in RLHF. By performing sound experiments at each stage of the RLHF process, this paper shows RLAIF to be a reliable alternative to human preferences: (1) the agreement between the human preference data vs AI preference data (with multiple approaches), (2) the performance of the RM trained on different data and (2) the human evaluation performance of the LLMs trained with RLHF vs RLAIF. Though the methodology may not be novel, the comprehensive experiments are insightful and valuable to the community.\n\nThis paper presents several valuable and insightful results (1) impact of CoT/self consistency during preference labeling, (2) impact of the AI labeler size on accuracy, (3) RM performance as a function of amount of preference data, (4) studying position bias."
                },
                "weaknesses": {
                    "value": "Since the comprehensive experimentation is a strength of this paper, is it imperative that the experiments and analysis is sound. The following points would benefit from additional experimentation or discussion:\n\n(1) In Figure4, it's not clear to me why adding exemplars hurts performance. There is a one sentence justification for this on page6, but I think it's insufficient, since exemplars hurt performance even without CoT. Is it the case that your exemplars are low quality? Could this be mitigated by using the 0-shot generations as exemplars? It would also be valuable to understand the role of exemplars at different labeler sizes (e.g. P2-S using exemplars produced by P2-L). \n\n(2) Again, regarding Figure4: Since human annotators have a 60-75% agreement rate on these datasets, I wonder if small differences in accuracy in Table4 are meaningful. Is it possible that the AI labels are more correct than the human preferences? Analyzing the disagreements may shed light on this. If so, what does it imply about the results/takeaway in Figure4.\n\n(3: suggestion) This is not a weakness, but more of a suggestion. It would be interesting to see the relationship between Figure4 and Figure5b. Does a higher quality AI-preference dataset necessarily lead to a more accurate RM? \n\n(4) The human evaluators used to assess the final RLHF/RLAIF/SFT-trained LLMs are distinct from the preference data/RM. How do we know if these annotators are modeling the same preference policy expressed by the datasets/RMs, and not (for example) just picking the longest response? To this end, it would be good to either show RM scores for the final LLMs or to measure the agreement of the human evaluators against the original preference data or the RMs."
                },
                "questions": {
                    "value": "Questions in the weakness section:\n\n1. Why do exemplars hurt performance of the preference labeler?\n2. Is it possible that the AI labels are more correct than the human preferences? If so, what does it imply about the results/takeaway in Figure4?\n3. Does a higher quality AI-preference dataset necessarily lead to a more accurate RM? \n4. Does a higher quality/more accurate RM necessarily lead to a better/more preferred LLM?\n5. How do we know if the human annotators are modeling the same preference policy expressed by the datasets/RMs, and not (for example) just picking the longest response?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3762/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698806500998,
            "cdate": 1698806500998,
            "tmdate": 1699636332290,
            "mdate": 1699636332290,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wL6pf4TNSx",
                "forum": "AAxIs3D2ZZ",
                "replyto": "s9RjoMSC4X",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3762/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3762/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer BDUS"
                    },
                    "comment": {
                        "value": "We sincerely appreciate your time and insightful feedback on our paper. We value your thoughtful suggestions and address each point in detail below:\n\n__The Effect of In-Context Learning on AI Labeler Alignment__\n\nWe have also conducted experiments on harmless dialogue generation (see last part of reply), where we found that adding exemplars increases AI Labeler Alignment. Our observations suggest that the optimal prompting techniques are task-specific.\n\nWe do not believe that low-quality exemplars are to blame for ICL not working. For all tasks, we carefully handpicked high quality exemplars representative of the preference task. We also measured alignment for \"Base 1-shot\" on summarization using 10 random exemplars, yielding a maximum/minimum alignment of 76.1%/74.4%, none surpassing the 76.1% alignment of \"Base 0-shot\". We will add these details to the paper.\n\nInterestingly, existing literature suggests that the role of exemplars and chain-of-thought prompting is still not fully understood (e.g., [1] and [2] show that corrupting exemplars and chain-of-thought surprisingly yield performance gains). We agree that deeper understanding of exemplars is an important research area.\n\n[1] Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? - https://aclanthology.org/2022.emnlp-main.759.pdf\n\n[2] Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters - https://arxiv.org/pdf/2212.10001.pdf \n\n__Whether Differences in AI Labeler Alignment are Meaningful__\n\nWe studied the sensitivity of RL-trained policies to the AI labeler accuracy, revealing that a small disparity can lead to a significant difference in win rates. Specifically, a policy trained with preference labels achieving 78% AI Labeler Alignment outperforms one trained with preference labels achieving 76% alignment, securing a win rate of 59% (statistically significantly above 50%). We will report these results in Section 4.6 \"End-to-End Sensitivity to AI Labeler Alignment\u201d.\n\nWhile we cannot definitively conclude that higher AI Labeler Alignment consistently translates to superior RL policies from this one experiment, this result suggests that such a correlation at least exists in certain cases. Due to the expensive nature of conducting these experiments and human evaluation, we agree that further investigations are necessary and defer more comprehensive study to future work.\n\n__AI Labeler Alignment vs. RM Accuracy__\n\nWe acknowledge the importance of thoroughly understanding the relationship between AI Labeler Alignment and RM accuracy. To address this concern, in the revised paper, we will include the RM accuracies for the two RMs trained on different AI labels in Section 4.6 \"End-to-End Sensitivity to AI Labeler Alignment.\u201d\n\nWe also would like to note that training and evaluation for all RMs are done in the exact same way. \n\n__Is Human Eval Aligned with Preference Datasets?__\n\nWe acknowledge the potential for discrepancies between the human raters who curated the preference datasets and the individuals we engaged for evaluation. Additionally, differences in training or instruction may introduce further variations. To minimize the impact of such mismatches, we meticulously replicated the settings provided in the original papers. Also, as you pointed out, it is possible that the AI labeler may provide more accurate assessments than the humans who initially rated the dataset in some cases.\n\nIncorporating an agreement analysis between our human evaluators and the preferences in the preference datasets is an excellent suggestion. If we can afford additional human evaluation, we will integrate this analysis into the camera-ready version to further substantiate the value of our human annotators' feedback.\n\nRegarding the possibility that annotators pick the longest response, our post-hoc analysis suggests that human evaluators continue to favor RLAIF and RLHF model outputs even after controlling for response length. This post-hoc analysis is similar to the approach employed by Stiennon et al. in their work \"Learning to Summarize from Human Feedback\" (https://arxiv.org/pdf/2009.01325.pdf).\n\n__Additional contributions to the paper__\n\nTo enhance the contribution of this work to the research community, we have conducted several additional experiments that expand the scope of our investigation. Specifically:\n* We have compared RLHF and RLAIF on a new task: harmless dialogue generation using Anthropic's Helpful/Harmless preference dataset\n* We have also investigated the performance of RLAIF with an AI labeler of the same size as the policy\n* We have experimented with bypassing the reward model by directly using feedback from a LLM\n* We have conducted experiments combining RLHF and RLAIF \n\nThese additional experiments will be incorporated into the revised paper, providing a more comprehensive and impactful contribution to the field."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3762/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700106649511,
                "cdate": 1700106649511,
                "tmdate": 1700714652614,
                "mdate": 1700714652614,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7agBoMOLny",
            "forum": "AAxIs3D2ZZ",
            "replyto": "AAxIs3D2ZZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3762/Reviewer_4rk2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3762/Reviewer_4rk2"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to analyze the performance of reinforcement learning with AI feedback (RLAIF). RLAIF is similar to RLHF but instead of collecting expensive human annotations, the preferences are generated by another LLM. Under the setup of this paper, the RLAIF achieves a similar win rate as RLHF in human evaluation, showing that RLAIF can potentially mitigate the scalability issue of RLHF."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* Investigating RLAIF\u2019s performance, especially comparing it to RLHF, is important and timely.\n* The achievement of RLAIF in this paper\u2019s setup is interesting. It can achieve the same level of performance as RLHF.\n* The writing is really clear."
                },
                "weaknesses": {
                    "value": "* If I understand correctly, both RLAIF and RLHF are based on the SFT baseline (fine-tuned PaLM2 XS) and REINFORCE. In this case, the key difference among RLAIF and RLHF in the experiments is the used reward models and the training data for the reward models. Therefore questions raise:\n  * What is the accuracy of the finally trained Human Feedback RM? I only see in Appendix E that the RM \u201cis trained until the training loss and accuracy curves plateau\u201d, but in Figure 5(b) it is still not plateau. Having this number can help readers understand (1) if the on-par performance of RLAIF and RLHF is due to using RMs with similar accuracy.\n  * To further dig into the above question, analysis of how RM accuracy affects the RLHF results can also be helpful.\n  * The AI feedback quality is the base and will first affect the trained AI feedback RM and then the RM will affect the result. However, the analysis in Section 4.6 entangles the two steps. It causes confusion if the performance difference comes from the trained RM or the AI feedback with different AI Labeler Alignment?\n  * Moreover, only the AI feedback with 76.1% and 78% AI Labeler Alignments are compared, if having a wider range analysis, it will be easier to understand the impact of the AI Labeler Alignments to the trained RM.\n* About human evaluation. Since the reported number is only the total human rating, I\u2019m curious about other statistics. How many input-outputs examples are used? How many evaluators rate the same input-outputs example? What\u2019s the inter-annotator agreement of the results?\n* The experimental results after controlling the length in Appendix F only shows the comparison of RLAIF vs SFT and RLHF vs SFT. Is there also a comparison between RLAIF vs RLHF?"
                },
                "questions": {
                    "value": "* About the experimental setup,\n  * Are the SFT baselines using only the preferred responses in the datasets? I have checked section 3.3, appendix A.1 and E, but haven\u2019t seen the answer to this question.\n  * Is there a reason why the reward models are also initialized from the SFT models (described in Section 3.3)? Their output spaces are different. Is it a random try or based on some statistics?\n  * What is the baseline used for REINFORCE (mentioned as \u201cwe use REINFORCE with a baseline\u201d in Section 3.3)? Is it the output of the value model in the authors\u2019 setup?\n* Presentation suggestion:\n  * The paper describes in-context learning with exemplars (section 2.1) and self-consistency (section 2.1.3) as they are a part of the used methodology. However, in experiments (Figure4), they turn out useless and not applied in the end. In this case, I would suggest not to put much emphasis on them but only mention them and say the authors also study their effects. \n  * Add that Figure 5 (a)(b) are results on summarization task in the caption.\n* In section 4.2, \u201cwe observe that the optimal configuration employs chain-of-thought reasoning and no in-context learning (\u201cDetailed + CoT 0-shot\u201d)\u201d Should here be \u201cDetailed / Base + CoT 0-shot\u201d, since for summarization the best is detailed and for helpfulness the best is base?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3762/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698816826674,
            "cdate": 1698816826674,
            "tmdate": 1699636332175,
            "mdate": 1699636332175,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cgvOgWW8wS",
                "forum": "AAxIs3D2ZZ",
                "replyto": "7agBoMOLny",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3762/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3762/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer 4rk2"
                    },
                    "comment": {
                        "value": "Thank you for your time and for reviewing our paper carefully. We greatly appreciate your thoughtful and detailed suggestions. We address your comments below:\n\n__Relationship Between AI Labeler Alignment, RM Accuracy, and Downstream RL Policy__\n\nWe acknowledge the importance of thoroughly understanding the relationship between RM accuracy and downstream RL results. To address this concern, in the revised paper, we will include the RM accuracies for both human feedback RMs and AI feedback RMs in Section 4.1. Additionally, we will report the RM accuracies for the two RMs trained on different AI labels in Section 4.6, titled \"End-to-End Sensitivity to AI Labeler Alignment.\u201d\n\nWhile we recognize the importance of understanding the relationship between RM accuracy and downstream RL results, we believe this is a broad research question deserving a comprehensive study of its own. Additionally, the nature of RL fine-tuning and human evaluation demands substantial resources, making an in-depth exploration of this relationship outside the scope of our work. Nevertheless, we recognize the significance and potential of this research direction.\n\nWe would also like to emphasize that all RMs are trained following a standardized procedure. This consistency in training procedure of the RM does not favor any specific approach, allowing for a fair evaluation of the different methods employed.\n\n\n__Human Evaluation Statistics__\n\nTo answer your questions about human evaluation, we constructed a set of 2k rating instances across all of our RL experiments, which includes evaluations for additional experiments that we are adding to the revised paper (see last section of this reply). Each instance comprised a single context and three distinct model responses (e.g., RLAIF, RLHF, SFT), resulting in a total of 6k unique input-output pairs subjected to human evaluation. Additionally, each instance was assessed by three independent raters, which we will use to calculate inter-annotator agreement. These details will be incorporated into Section 3.4 \"Human Evaluation\" of the revised paper to enhance the transparency of our evaluation methodology.\n\n__Length-Controlled Results__\n\nYou raise an excellent point about calculating the length-controlled results for RLHF vs. RLAIF directly. We will calculate this quantity and incorporate the result in Appendix F. \n\n\n__Other Questions__\n\n> Are the SFT baselines using only the preferred responses in the datasets?\n\nWe do not conduct SFT on the preferred responses. For summarization, we solely train on the supervised data from the TL;DR dataset (note that this is distinct from the TL;DR *preference* dataset). For other tasks, we do not conduct SFT at all. We will make this clear in the revised paper.\n\n> Is there a reason why the reward models are also initialized from the SFT models (described in Section 3.3)? Their output spaces are different. Is it a random try or based on some statistics?\n\nWe initialize the RM from the SFT model because we have found that initializing the RM from the SFT model improves RM performance. The hypothesis behind this is that the RM must be able to adapt to the scoring task, and the SFT model adds more domain-specific training. This is also an observation shared by other researchers - see https://arxiv.org/pdf/2009.01325.pdf, https://notesonai.com/RLHF+-+Reinforcement+Learning+with+Human+Feedback, and https://huyenchip.com/2023/05/02/rlhf.html.\n\n> What is the baseline used for REINFORCE (mentioned as \u201cwe use REINFORCE with a baseline\u201d in Section 3.3)? Is it the output of the value model in the authors\u2019 setup?\n\nAs you mentioned, we use the value function as the baseline in REINFORCE. See Appendix C for details.\n\nLastly, we will implement your recommendations on \u201cPresentation suggestion\u201d and fix the point regarding section 4.2. Thank you for pointing these out.\n\n\n__Additional contributions to the paper__\n\nTo enhance the contribution of this work to the research community, we have conducted several additional experiments that expand the scope of our investigation. Specifically:\n* We have compared RLHF and RLAIF on a new task: harmless dialogue generation using Anthropic's Helpful/Harmless preference dataset.\n* We have also investigated the performance of RLAIF with an AI labeler of the same size as the policy\n* We have experimented with bypassing the reward model by directly using feedback from a LLM\n* We have conducted experiments combining RLHF and RLAIF \n\nThese additional experiments will be incorporated into the revised paper, providing a more comprehensive and impactful contribution to the field."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3762/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700101850756,
                "cdate": 1700101850756,
                "tmdate": 1700149468986,
                "mdate": 1700149468986,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]