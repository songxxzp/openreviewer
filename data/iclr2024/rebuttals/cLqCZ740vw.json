[
    {
        "title": "Benchmarking Smoothness and Reducing High-Frequency Oscillations in Continuous Control Policies"
    },
    {
        "review": {
            "id": "Nw7vRDDai1",
            "forum": "cLqCZ740vw",
            "replyto": "cLqCZ740vw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6989/Reviewer_5FVX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6989/Reviewer_5FVX"
            ],
            "content": {
                "summary": {
                    "value": "The authors investigate methods for reducing the emergence of high-frequency oscillatory behavior when learning continuous control policies. Such behavior is undesirable in real-world robotics applications, however, explicit regularization introduces complex trade-offs. It is then important to identify approaches that gracefully prevent high-frequency motions while maintaining overall performance. This paper aims to provide a benchmark analysis of various method aiming to address this challenge."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Investigating methods to reduce high-frequency oscillatory behavior in continuous control is an important research direction with real-world impact\n- PPO is a good primary baseline choice due to its recent success in enabling sim-to-real transfer of complex behaviors in robotics\n- Results are averaged over 9 seeds to yield statistical significance"
                },
                "weaknesses": {
                    "value": "- The discussion of related work is rather limited and should be expanded by a control theory angle, particularly relating to natural emergence of bang-bang-type controllers [1] or pulse-width modulation. Established benchmarks such as the DeepMind Control Suite can furthermore often be solved with high-frequency bang-bang control as studied in [2] or general discretization in [3] \u2013 these works provide further motivation for the need of well-designed benchmarks as this paper aims to provide.\n- A very relevant related work that automatically learns regularization trade-offs was presented in [4]\n- The majority of cumulative returns as well as smoothness scores in Table 1 have overlapping error bands making performance differences difficult to judge.\n- The current evaluation has insufficient breadth to serve as a benchmark for smooth continuous control. The analysis relies on empirical data and as such should cover a large representative range of (robotics) tasks or baseline algorithms, ideally both. \n- Pendulum/Reacher/Lunar Lander are rather toy-ish examples that are not representative of the challenges the paper aims to analyze, while they could provide good illustrative examples if analyzed more in-depth\n- The analysis would profit from extension to a broader set of baseline algorithms other than PPO to identify trends in order to serve as an extensive benchmark\n\n\n\n[1] R. Bellman, et al. \"On the \u201cbang-bang\u201d control problem,\" Quarterly of Applied Mathematics, 1956.\n\n[2] T. Seyde, et al. \"Is Bang-Bang control All You Need? Solving Continuous Control with Bernoulli Policies,\" NeurIPS, 2021.\n\n[3] Y. Tang, et al. \"Discretizing Continuous Action Space for On-Policy Optimization,\" AAAI, 2020.\n\n[4] S. Bohez, et al. \"Value constrained model-free continuous control,\" arXiv, 2019."
                },
                "questions": {
                    "value": "- Why did the \u201cLocomotion \u2013 Velocity Controller\u201d task not use the action rate penalization of Rudin (2021)?\n- What do state and action trajectories of trained policies look like? How do they compare across agents?\n- How does policy performance and smoothness compare across agents when simply varying weights of action magnitude/smoothness penalties in the reward function? As this is commonly how practitioners counteract high-frequency behavior, such an analysis would improve insights drawn from a benchmark."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6989/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6989/Reviewer_5FVX",
                        "ICLR.cc/2024/Conference/Submission6989/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6989/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698185276553,
            "cdate": 1698185276553,
            "tmdate": 1700668646364,
            "mdate": 1700668646364,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nwPBCaXn38",
                "forum": "cLqCZ740vw",
                "replyto": "Nw7vRDDai1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6989/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6989/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for his time and for providing feedback to improve our work.\n\n> The discussion of related work is rather limited and should be expanded by a control theory angle.\n\nThe related works section of our paper lists other benchmark papers and focuses mainly on categorizing and describing past methods that learns policies that produce smooth control behaviors, with few oscillations. Some of the methods investigated in our work do have more of a control theory perspective (esp. [1] and [2]), though not all of them. For example, Liu-Lipschitz [3] is an approach originally presented in the context of 3D imaging. \n\nBecause in our work we are interested in measuring the effectiveness of these approaches in a performance x smoothness analysis more so than their mechanisms, we believe an extended discussion from a control theory angle is a bit out of scope. We would invite an interested reader to check the original papers for details, specially for methods proposed from a control perspective.\n\n> Established benchmarks such as the DeepMind Control Suite can furthermore often be solved with high-frequency bang-bang control as studied in [...] or general discretization in [...]\n\nThank you for the reference suggestions. Although these works demonstrate that bang-bang control and discretization can solve the tasks, we did not include these types of controllers in our benchmark since they are the opposite of the type of smooth control we wish to achieve. A major part of our benchmark is done with real-world hardware and bang-bang/discrete type controllers cannot be safely deployed.\n\nNonetheless, the works listed are interesting benchmarks that evaluate performance in many environments, and we'll include them in the discussion of related works in the final paper.\n\n> A very relevant related work that automatically learns regularization trade-offs was presented in [4]\n\nWe apologize for missing the related work. The work cited by the reviewer is indeed very relevant. We'll include it in the related works section discussion in the final paper version.\n\n> The majority of cumulative returns as well as smoothness scores in Table 1 have overlapping error bands making performance differences difficult to judge.\n\nWe argue that our benchmark results show a significant improvement over a \"Vanilla\" policy versus regularization methods, showing that regularization is indeed effective. It is true that in a single scenario the smoothness between two regularization techniques might not be significant (see Hybrid methods in \"Reacher\", \"ShadowHand\", and \"Velocity\"). However, analyzing the smoothness x performance  across all scenarios we can observe that the \"Hybrid\" methods outperformed others more often than not. While other approaches have significant deviations scenario to scenario, it is clear that the \"Hybrids\" are the most consistent method across the board.\n\n> The current evaluation has insufficient breadth to serve as a benchmark for smooth continuous control. \n> Pendulum/Reacher/Lunar Lander are rather toy-ish examples that are not representative of the challenges the paper aims to analyze\n\nWe've added two additional scenarios: \"ShadowHand\" and \"Handstand\". These are challenging tasks that require agile and dexterous movements. The Handstand task is also deployed and evaluated with real hardware.\n\nAdditionally, we have also measured the performance of every single method in the real-world and included an ablation case of a Vanilla policy trained without domain randomization. Please refer to the comment \"Extended Experiments and Results -- Updated Table 1\" for the updated version of our results. \n\n> The analysis would profit from extension to a broader set of baseline algorithms other than PPO to identify trends in order to serve as an extensive benchmark\n\nWe agree with the reviewer that more extensive and broader experiments are better. Still, PPO is one of the most popular algorithm used by the community, specially for sim2real deployment. We believe the scope of the work is wide enough to be relevant to the research community. \n\n---\n\nWe also invite the reviewer to check the \"Summary of Changes\" comment for a more extensive list of major and minor changes we've done thanks to your and other reviewers feedback.\n\n---\n\n[1] - Taisuke Kobayashi. L2c2: Locally lipschitz continuous constraint towards stable and smooth reinforcement learning. In 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 4032\u20134039. IEEE, 2022\n\n[2] - Ryoichi Takase, Nobuyuki Yoshikawa, Toshisada Mariyama, and Takeshi Tsuchiya. Stability certified reinforcement learning control via spectral normalization. Machine Learning with Applications, 10:100409, 2022.\n\n[3] Hsueh-Ti Derek Liu, Francis Williams, Alec Jacobson, Sanja Fidler, and Or Litany. Learning smooth neural functions via lipschitz regularization. In ACM SIGGRAPH 2022 Conference Proceedings, pp. 1\u201313, 2022."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6989/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700633321607,
                "cdate": 1700633321607,
                "tmdate": 1700633321607,
                "mdate": 1700633321607,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wI7dIAeZfY",
                "forum": "cLqCZ740vw",
                "replyto": "et74MCdegY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6989/Reviewer_5FVX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6989/Reviewer_5FVX"
                ],
                "content": {
                    "title": {
                        "value": "Response to rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for your detailed responses and additional evaluation. The new experiments improve the quality of the manuscript and I'll raise my score. However, I still believe that overlapping error bands and small differences in mean performance make improvements difficult to judge, while analysis is not extensive enough from a benchmarking perspective."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6989/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700668633344,
                "cdate": 1700668633344,
                "tmdate": 1700668633344,
                "mdate": 1700668633344,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dBWrGt8hAX",
            "forum": "cLqCZ740vw",
            "replyto": "cLqCZ740vw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6989/Reviewer_qSpQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6989/Reviewer_qSpQ"
            ],
            "content": {
                "summary": {
                    "value": "The authors benchmark several recent methods for imposing smoothness on policies. The benchmarks are mainly done in simulation, and one the best performing method is evaluated in a sim2real transfer against a vanilla RL policy."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Benchmark papers are important for the community, and the sim benchmark evaluation on policy smoothness appears to be carefully constructed \n- Reducing oscillations is important, and a common problem in sim2real as the authors note in their conclusions\n- The paper is mostly well-written and easy to read"
                },
                "weaknesses": {
                    "value": "Overall I think this is a useful benchmark paper but it has some issues with framing:\n- The authors (and title even) mix the concepts of policy smoothness and reducing high-frequency oscillations, but how these concepts relate and why you get oscillations is not thoroughly defined. The paper also talks about sim2real where oscillations are indeed a common problem, but the authors do not make a convincing argument that this is due to non-smooth policies. Sometimes it might be but this has not been well explored as far as I am aware. Overall the paper does a good job of benchmarking existing approaches to increase policy smoothness, but the sim2real / oscillation connection seems weak.\n- More in that vein, the sim2real is only attempted with the best performing method from simulation. If simulation was a good indicator of real-world performance, sim2real would be much easier than it currently is, but this is not the case in my experience. If you want to make this paper about sim2real instead of just policy smoothness, then it would be useful if you had tested all the approaches in the real world to see how the approaches generalized. It would also have been useful to see a video of the experiments as is conventional in robotics. Did the robot actually oscillate or is your smoothness metric just picking up a quick motion (e.g, a simple step function also has high-frequency components)? These can sometimes be desirable. \n\nAs it currently stands, I would consider toning down the sim2real implications a little bit and focus more on smoothness.\n\nMinor: \ntypo: hyperaparameter"
                },
                "questions": {
                    "value": "\"... but in the open-source code a linear activation is used. In our implementation, we used a softplus activation as in the original paper.\" - This seems like an arbitrary choice that might degrade the performance of a defacto available option. Maybe the implementation is more up to date, or there was a typo in the paper?  Can you test and confirm that it performs as well, or why not include both versions?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6989/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698875120575,
            "cdate": 1698875120575,
            "tmdate": 1699636817709,
            "mdate": 1699636817709,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "aLWBYwgUpz",
                "forum": "cLqCZ740vw",
                "replyto": "dBWrGt8hAX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6989/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6989/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for his time and for providing feedback to improve our work.\n\n> The authors (and title even) mix the concepts of policy smoothness and reducing high-frequency oscillations, but how these concepts relate and why you get oscillations is not thoroughly defined.\n\nThe reviewer raises a good point that our language in the paper is ambiguous. When we use the term \"policy smoothness\" or a \"smooth policy\" we simply mean a policy that exhibits/produces smooth control behaviors in the form of reduced oscillations, and not necessarily a policy that produces a smooth mapping function. However, and interestingly, the two do seem to be correlated as most methods that we investigated/refer in our paper do use the latter definition of smooth policy and aim to achieve smoothness, as in \"a smooth mapping function\", which in turns reduces oscillations, evidenced by our results.\n\nWe will clarify the meaning of these terms in the paper and better define \"smoothness\".\n\n>  it would be useful if you had tested all the approaches in the real world to see how the approaches generalized.\n\nWe've followed up on the reviewer's suggestions and reran all of our real-world experiments with every method reported on the paper. Additionally, we've also measured the cumulative return in the real-world to enable a performance vs smoothness analysis.\n\n**Updated Real-World Results** - Comparison of All Methods + No DR Ablation + Cumulative Return Computation:\n\nImitation: https://i.imgur.com/zuWvjqq.png\n\nVelocity: https://i.imgur.com/UG2dvYF.png\n\nHandstand: https://i.imgur.com/vmjOltq.png\n* We did not deploy \"No DR\" ablation for handstand out of concern for robot and actuator damage.\n\n> It would also have been useful to see a video of the experiments as is conventional in robotics. Did the robot actually oscillate or is your smoothness metric just picking up a quick motion\n\nOur original video demonstrates a case of an emergent behavior for the Hybrid policy where when the robot is picked up from the ground it will slowly stop moving. In contrast, when running the vanilla policy an out of distribution state such as this generates \"wild movement\" and jerking motions. We've also updated the supplementary material to include demonstrations of a new handstand policy. We can observe in the video that with our proposed hybrid method the motion is smoother during regular execution as well as when disturbed.\n\n> (On a LipsNet modification) This seems like an arbitrary choice that might degrade the performance of a defacto available option. Maybe the implementation is more up to date, or there was a typo in the paper? Can you test and confirm that it performs as well, or why not include both versions?\n\nWe've confirmed with the original authors of LipsNet that the version we implemented is indeed the correct and intended way. The authors had incorrectly used a linear activation as a default when open-sourcing their code (The output $K$ of LipsNet must be $ > 0 $. We've confirmed that the activation function used by the authors in their original experiments matches the version we've used in our work. Additionally, the original authors have since updated their open-source code to match the version we've used after our inquiry.\n\n---\n\nWe also invite the reviewer to check the \"Summary of Changes\" comment for a more extensive list of major and minor changes we've done thanks to your and other reviewers feedback."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6989/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700562143209,
                "cdate": 1700562143209,
                "tmdate": 1700562143209,
                "mdate": 1700562143209,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kW3irha6vU",
                "forum": "cLqCZ740vw",
                "replyto": "aLWBYwgUpz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6989/Reviewer_qSpQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6989/Reviewer_qSpQ"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "I thank the authors for the additional sim2real experiments. I note that pure l2c2 seems pretty close to the results of the hybrid methods on that, especially if you consider both smoothness and returns. I wonder if this is due to low sample size or just because the real world is hard to model? \n\nTaken together you can see a trend of sorts in the results, but as another reviewer mentioned the confidence intervals are rather large (as is often the case in RL). I am not sure if it is feasible to get more samples on the sim experiments, but one idea to reduce the variance might be to examine the difference across environment seeds (like a pair-wise test) if you are re-using the same environment seeds. Another would be to normalize and pool all task results for sim and real (respectively) or perhaps put all sim vs. real experiments into a Pareto plot (norm. return vs. smoothness) .\n\nRegarding the definition of smoothness (smooth policy -> smooth behavior), I agree that you can make this assumption which also seems common in related work, but it is a rather strong simplification. The final behavior of the system depends not only on the control policy, but also on the system (plant) dynamics. More attention could have been paid to the assumptions being made on the system here. For example, I suspect you might need some assumptions of smoothness of the system dynamics for this to hold. Another common source of oscillations in sim2real is also latency, which is also not mentioned. That said, I still think this can have merit just as a benchmark paper. I hesitate if that part is conclusive and deep enough for an 8 though (see above)."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6989/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700671516718,
                "cdate": 1700671516718,
                "tmdate": 1700671516718,
                "mdate": 1700671516718,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "RI2hceHhaw",
            "forum": "cLqCZ740vw",
            "replyto": "cLqCZ740vw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6989/Reviewer_RjiF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6989/Reviewer_RjiF"
            ],
            "content": {
                "summary": {
                    "value": "In the paper, the author investigates algorithms aiming to prevent high-frequency oscillations during the RL sim2real transfer process. They conduct a comprehensive benchmark of both the performance and smoothness of the trained policies. Moreover, they devise a hybrid method, which, as per the results, demonstrates superior performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The author effectively categorizes various smoothing methods, providing an exhaustive understanding of the diverse kinds of policy smoothing techniques.\n2. Extensive benchmarks are conducted, further enhancing comprehension of the performance exhibited by differing algorithms in simulation."
                },
                "weaknesses": {
                    "value": "1. The paper presents a rather incremental contribution. The author proposes a hybrid method that combines architectural methods and regularization techniques, but it remains unclear how these two components interact or how the policy can be further improved.\n2. The performance enhancement of the hybrid method, compared to the baseline, appears limited. Moreover, since oscillations occur during the sim2real process, the author only contrasts the hybrid method with the vanilla policy. This means comparisons to other baselines are omitted, making it unclear how the hybrid method measures up against other methodologies."
                },
                "questions": {
                    "value": "1. In Figure 2, in the disturbance imitation task test, why does the combination of LipsNet + L2C2 underperform compared to the vanilla policy regarding smoothness?\n2. As you have exclusively provided the measure of policy smoothness in real-world situations, I'm curious as to how the real-world performance (reward) measures up against other baselines?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6989/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6989/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6989/Reviewer_RjiF"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6989/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699317507608,
            "cdate": 1699317507608,
            "tmdate": 1699636817574,
            "mdate": 1699636817574,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jS7fj4PXG3",
                "forum": "cLqCZ740vw",
                "replyto": "RI2hceHhaw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6989/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6989/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for his time and for providing feedback to improve our work.\n\n> As you have exclusively provided the measure of policy smoothness in real-world situations, I'm curious as to how the real-world performance (reward) measures up against other baselines?\n\nWe've followed up on the reviewer's suggestions and reran all of our real-world experiments with every method reported on the paper. Additionally, we've also measured the cumulative return in the real-world to enable a performance vs smoothness analysis. (See below).\n\n> In Figure 2, in the disturbance imitation task test, why does the combination of LipsNet + L2C2 underperform compared to the vanilla policy regarding smoothness?\n\nEvery policy deployed to the real-world is trained with domain randomization, including environment physics, terrain and actuator properties. The addition of domain randomization by itself results in smoother control behaviors. For a given task, regular execution (i.e. no disturbances) might be smooth enough that there are no significant differences between a regularized policy and a vanilla DR policy. However, when the policy needs to execute a sort of recovery behavior from an unstable state (which could be out of distribution), for example from an external disturbance, we can observe that a policy trained with smoothness regularization is indeed smoother. \n\nFor supporting evidence to our point above, we have included an ablation of a \"Vanilla (No DR)\" vs \"Vanilla\" which demonstrates this behavior.\n\n---\n**Updated Real-World Results** - Comparison of All Methods + No DR Ablation + Cumulative Return Computation:\n\nImitation: https://i.imgur.com/zuWvjqq.png\n\nVelocity: https://i.imgur.com/UG2dvYF.png\n\nHandstand: https://i.imgur.com/vmjOltq.png\n* We did not deploy \"No DR\" ablation for handstand out of concern for robot and actuator damage.\n---\n\nWe also invite the reviewer to check the \"Summary of Changes\" comment for a more extensive list of major and minor changes we've done thanks to your and other reviewers feedback."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6989/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700560273725,
                "cdate": 1700560273725,
                "tmdate": 1700560273725,
                "mdate": 1700560273725,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bHJr1aGuLf",
            "forum": "cLqCZ740vw",
            "replyto": "cLqCZ740vw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6989/Reviewer_ALp7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6989/Reviewer_ALp7"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses the issue of high-frequency oscillations in reinforcement learning policies, especially when applied to real-world hardware. The authors categorize methods to mitigate these oscillations into loss regularization and architectural methods, aiming to smooth the input-output mapping of policies. They benchmark these methods on classic RL environments and robotics locomotion tasks, introducing hybrid methods that combine both approaches. The study finds that hybrid methods, particularly LipsNet combined with CAPS and L2C2, perform well in both simulations and real-world tasks, achieving smoother policies without compromising task performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "A technically sound paper with theoretical analysis and experimental support of their discoveries. Aims to get a better understanding and address an important problem in robotics, especially in application to sim2real when high-frequency, noisy policies can damage robot hardware. In addition to sim sim-only results demonstrate the advantages of their approach on a quadruped robot."
                },
                "weaknesses": {
                    "value": "Lack of experiments, especially for more challenging continuous control tasks. In addition would be good to see more detailed comparisons against MLP baseline - see training curves and comparison of reward and variance vs not only a number of samples but also training time."
                },
                "questions": {
                    "value": "1) Could you share training plots at least for Ant and quadruped robot for the reward vs wall-clock time for LipsNet + CAPS and LipsNet + L2C2 vs vanilla MLP? Are there losses in training (wall-clock time) performance when using these more advanced methods vs MLP or they are computationally comparable to the vanilla MLP?\n2) Could you run experiments for more challenging control problems - humanoid, or on of the Allegro (Shadow) Hand dexterous manipulation tasks?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6989/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699517977328,
            "cdate": 1699517977328,
            "tmdate": 1699636817477,
            "mdate": 1699636817477,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vMfddPdAfH",
                "forum": "cLqCZ740vw",
                "replyto": "bHJr1aGuLf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6989/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6989/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for his time and for providing feedback to improve our work. \n\n> Could you share training plots at least for Ant and quadruped robot for the reward vs wall-clock time for LipsNet + CAPS and LipsNet + L2C2 vs vanilla MLP?\n\nUnfortunately, because we ran our experiments in parallel, sharing computing load with other processes and experiments, the wall-clock vs reward graph is not truly representative of performance efficiency. We'll consider running a separate experiment sequentially to accurately measure reward vs (real) time.\n\n> Are there losses in training (wall-clock time) performance when using these more advanced methods vs MLP or they are computationally comparable to the vanilla MLP?\n\nFor pure loss-based methods we have not observed any difference in training time. In regards to the architectural methods, Spectral Normalization (SN-Local) and Liu-Lipschitz are both similar to a vanilla MLP in training time. Because our policy networks are not large, the training bottleneck ends up in the sample collection time in this case. \n\nHowever, it should be noted that the LipsNet method comes with a significant slowdown. We observe an iteration time approximately 10 times longer than other methods, which is line with what the original authors reported in Appendix D. in [1]. By extension, this also affects the training time for the Hybrid methods that we proposed in our paper. \n\nOvercoming such a slowdown and achieving the same smoothness and performance metrics we measured here could be a fruitful path for future research.\n\nWe'll incorporate the points above in the discussion in Section 4 of the paper.\n\n> Could you run experiments for more challenging control problems - humanoid, or on of the Allegro (Shadow) Hand dexterous manipulation tasks?\n\nThank you for your suggestion. We have run our experiments with the ShadowHand task in Isaac Gym. Please refer to the full updated table and comments in our top official comment \"Extended Experiments and Results -- Updated Table 1\".\n\nWe also invite the reviewer to check the \"Summary of Changes\" comment for a more extensive list of major and minor changes we've done thanks to your feedback.\n\n---\n\n[1] - Xujie Song, Jingliang Duan, Wenxuan Wang, Shengbo Eben Li, Chen Chen, Bo Cheng, Bo Zhang, Junqing Wei, and Xiaoming Simon Wang. Lipsnet: A smooth and robust neural network with adaptive lipschitz constant for high accuracy optimal control. 2023."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6989/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700559106717,
                "cdate": 1700559106717,
                "tmdate": 1700559106717,
                "mdate": 1700559106717,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]