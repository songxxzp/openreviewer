[
    {
        "title": "RealFM: A Realistic Mechanism to Incentivize Data Contribution and Device Participation"
    },
    {
        "review": {
            "id": "TjydGi4iXi",
            "forum": "iJBQAAhqvY",
            "replyto": "iJBQAAhqvY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3688/Reviewer_gdSK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3688/Reviewer_gdSK"
            ],
            "content": {
                "summary": {
                    "value": "This work builds on [Karimireddy et.al. (2022)] and develops a federated mechanism that incentivizes truthful participation and data contribution while provably removing the free-rider problem. The methods eliminate the requirement for data sharing while achieving a high-quality global model following the developed reward protocol - per non-linear modelling of utility with accuracy- that compensates rational devices for participation with more data."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This work develops a federated mechanism that incentivizes truthful participation and data contribution while provably removing the free-rider problem. The methods eliminate the requirement for data sharing while achieving a high-quality global model following the developed reward protocol - per non-linear modelling of utility with accuracy- that compensates rational devices for participation with more data."
                },
                "weaknesses": {
                    "value": "1. This work builds on  [Karimireddy et.al. (2022)]. The derivation to model accuracy in (1) and (2), following [Karimireddy et al., 2022] and moreover, [Mohri et al., 2018] assumes for m i.i.d. samples. Will it enjoy generalization to the non-i.i.d. case? \n2. Some inconsistencies and incompleteness due to unclear representation. For instance, the authors mention generalizing $\\phi_i$ to become a convex and increasing function (but in which variable), and also, in actuality, the utility is modelled as a concave function? Following that, Assumption 2  needs further clarity. Another concern is Fig. 2, which is not well-justified for the said \"payoff function. I don't see this claim has been experimentally validated apart from numerical evaluation.  \n3. How a_\\textrm{opt} is derived/obtained?\n4. If the profit margin of the central server, defined as p_m, is fixed and known by all devices, I am not sure why it is used after all. (in the later experiments, this is used to indicate the degree to which the server is greedy). Assuming this is unknown would be an interesting analysis. Also, the definition of profit margin should be made more rigorous.\n5. As the authors mentioned, it is hard to bond the composition function $\\phi$, and I am still not sure how, particularly with this framework, we can ensure increased data production leads to better payoffs. In principle, one has to factor in the quality of data or limit combinatorial properties.\n6. Evaluations:\n - Indicate the choice of parameters: z_i.\n- Can the authors be more precise on the whole experimental setup? Specifically, how \"more data\" is used in practice? Did I miss something? \n - In its current form, the discussion on device utility is not rigorous, for instance, how it is evaluated. It would be interesting to evaluate per-device performance, apart from the server's utility, as Top-1 accuracy with optimal data contribution and, later, under the influence of accuracy shaping (with RealFM?). Then, can we get the best of both worlds, a high-quality model with improved generalization/personalization performance? Please comment.\n\nMinor suggestion:\nIn the Fig. 1 caption, it should be said, in my understanding, as such that RealFM ensures better utility for a \"truthful\" participation instead."
                },
                "questions": {
                    "value": "Please see the questions posed in the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3688/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3688/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3688/Reviewer_gdSK"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3688/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698597495360,
            "cdate": 1698597495360,
            "tmdate": 1699636325514,
            "mdate": 1699636325514,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "afAcMDFgDd",
                "forum": "iJBQAAhqvY",
                "replyto": "TjydGi4iXi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3688/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3688/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reviewer gdSK Rebuttal"
                    },
                    "comment": {
                        "value": "We thank reviewer gdSK for their thoughtful review and look forward to our discussion. We begin by reaffirming the novelty of our work before addressing the questions raised.\n\n### Novelty of Contributions\n\n **Our work is the first FL mechanism which more realistically models device utility (non-linear function of model accuracy) and provably improves device participation and contributions (eliminating the free-rider effect) without the need of a payment mechanism prior to training**. We sought to design a foundational FL mechanism which improves upon previous FL mechanisms by:\n1. Modeling device utility in a more realistic manner which enables flexible modeling of diverse device utilities that accommodate non-linear relationships between model accuracy and utility.\n     - Allowing this non-linear relationship within device utility ***requires major changes*** to the accuracy-shaping procedure, the process in which the server provides devices increased model accuracy in exchange for an increase in contributions, at the heart of the incentive mechanism (Theorem 4).\n     - The altered accuracy-shaping function and its provable guarantees in Theorem 3 ***are novel***.\n     - The accuracy-shaping function incorporates monetary rewards, which are ***another novel inclusion*** detailed in (3).\n2. Not requiring data to be shared between devices and a central server. Data sharing is required in Karimireddy et al. (2022), our closest related mechanism, and it violates the true FL setting.\n3. Not requiring design of a payment mechanism (like that in Contract Theory) which needs careful construction of the payment *prior to training*.\n    - This is similar to Karimireddy et al. (2022), however we instead define the central server's own utility and showcase that our mechanism maximizes the server's utility.\n    - Also unlike Karimireddy et al. (2022), we leverage our newly defined server utility to allow monetary rewards for participating devices. This is fully novel within the non-payment mechanism setting.\n4. Demonstrating empirically (on real-world data) that RealFM succeeds in the goals laid out within our paper: server utility, device utility, and device contributions skyrocket while free-riding is negated when devices use RealFM.\n\n### Data Quality Assumptions\n\nThe overarching goal of our work is to create a foundational FL mechanism which solves many of the issues detailed in our paper as well as highlighted at the beginning of this rebuttal. There remain many frontiers to improve how realistically device utility is modeled, one of which is allowing for different quality data. Relaxing our assumptions on data quality is an important frontier to further improve realistic device utility. The literature [Wang et al. (2020)] provided by reviewer A7vi on data quality is a great place to begin tackling the data quality assumptions (through the use of Federated Shapley Values).\n\n> As the authors mentioned, it is hard to bond the composition function $\\phi$, and I am still not sure how, particularly with this framework, we can ensure increased data production leads to better payoffs. In principle, one has to factor in the quality of data or limit combinatorial properties.\n\nAccuracy function $\\hat{a}(m)$ is increasing with respect to the amount of data $m$ (due to its concavity). The accuracy-payoff function $\\phi_i(a)$ is increasing with respect to accuracy $a$. Therefore, $\\phi_i(a(m))$ is increasing with respect to $m$. This is how, given our assumptions on data quality, we ensure \"increased data production leads to better payoffs\".\n\n> Will it enjoy generalization to the non-i.i.d. case?\n\nCurrently our mechanism does not fully generalize to the non-i.i.d case. The reason stems from the fact that our assumptions on $\\hat{a}(m)$, detailed above, may fail in the non-i.i.d case ($\\hat{a}(m)$ may not necessarily increase with more non-i.i.d data). However, there may be scenarios where adding non-i.i.d data still allows $\\hat{a}(m)$ to be increasing with respect to $m$. Under this scenario our mechanism *would* generalize to the non-i.i.d case."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3688/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700152021108,
                "cdate": 1700152021108,
                "tmdate": 1700152021108,
                "mdate": 1700152021108,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pTdwJ7kMRi",
                "forum": "iJBQAAhqvY",
                "replyto": "TjydGi4iXi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3688/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3688/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reviewer gdSK Rebuttal (Continued)"
                    },
                    "comment": {
                        "value": "We continue our rebuttal below and address the remaining questions within the review.\n\n### Parameter Clarifications\n\n> Some inconsistencies and incompleteness due to unclear representation. For instance, the authors mention generalizing $\\phi_i$ to become a convex and increasing function (but in which variable), and also, in actuality, the utility is modeled as a concave function? Following that, Assumption 2 needs further clarity. Another concern is Fig. 2, which is not well-justified for the said \"payoff function. I don't see this claim has been experimentally validated apart from numerical evaluation.\n\nWe provide clarification that **(1)** $\\phi_i(a)$ is *convex* with respect to the accuracy $a$, however **(2)** $\\phi_i(\\hat{a}(m))$ must remain concave with respect to $m$. \n\nAs detailed in our paper, we can define the accuracy function $\\hat{a}(m)$ as either Equation 1 or 40 and the accuracy-payoff function $\\phi_i(a) := \\frac{1}{(1-a)^2} - 1$ and this will satisfy both **(1)** and **(2)**. There are other variants of $\\hat{a}(m)$ and $\\phi_i(a)$ which work, but these are backed up by generalization bounds (Proposition 1) and intuition respectively.\n\nThe convex and increasing requirement for $\\phi_i(a)$ is intuitive as rational devices heavily desire models with accuracies approaching 100%. This is much more intuitive than a linear $\\phi_i(a)$, as described in the paper, since rational devices would place much greater value in a model increasing in accuracy from 99 to 99.5% than 30 to 30.5%.\n\n---\n\n> How is $a_{opt}$ derived/obtained?\n\nIn practice, $a_{opt}$ depends upon model architectures as well as the learning task at hand. In many cases, prior knowledge about the difficulty of the learning task can illuminate what an actual or approximate value of $a_{opt}$ should be (*e.g.,* many well-trained neural networks for image classification can achieve 90%+ optimal accuracies). In situations where the optimal accuracy might be completely unknown, a value close to 100% for $a_{opt}$ can be used to be conservative and ensure no issues will arise within the mechanism.\n\n---\n\n> If the profit margin of the central server, defined as $p_m$, is fixed and known by all devices, I am not sure why it is used after all. (in the later experiments, this is used to indicate the degree to which the server is greedy). Assuming this is unknown would be an interesting analysis. Also, the definition of profit margin should be made more rigorous.\n\nThe profit margin $p_m$ details the percent of utility kept by the server after federated training is complete. The remaining $(1-p_m)$ percent is distributed uniformly as a monetary reward to all participating devices. It is important for devices to know the profit margin $p_m$, since a smaller $p_m$ will result in larger rewards for each device and may provide further incentive to participate. As noted by the reviewer, we showcase that device utility and contributions increase *even when the server is greedy and maintains all of its gained utility*.\n\nWe agree with the reviewer that assuming an unknown profit margin would be interesting analysis and we leave this as important future research. As detailed at the beginning of the rebuttal, our main goal was to construct the first FL mechanism of its kind. Once planted, we aim to further relax some of the assumptions initially used."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3688/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700152069240,
                "cdate": 1700152069240,
                "tmdate": 1700152069240,
                "mdate": 1700152069240,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RYdAH6hy27",
                "forum": "iJBQAAhqvY",
                "replyto": "pTdwJ7kMRi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3688/Reviewer_gdSK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3688/Reviewer_gdSK"
                ],
                "content": {
                    "comment": {
                        "value": "Many thanks authors for the response, and I'm sorry for the late feedback on this. \n-  One concern was whether concavity would hold for $\\hat{a}(m)$ with $m$, i.e., can the method generalize to the non-i.i.d. case? As the authors agreed, it does not, for instance, when having adversarial data samples. Then, it would be confusing when you say, \"However, there may be scenarios where adding non-i.i.d data still allows to be increasing with respect to. Under this scenario, our mechanism would generalize to the non-i.i.d case\". Further, this means the accuracy model might not hold practical without making strong (impractical) assumptions? I believe the authors should clarify such limitations in the manuscript.\n-  I agree with Reviewers EigJ (and A7vi) for their concerns on simplified assumptions (as I raised in my earlier comment). For instance, the assumption that the closed form of each device's utility function is known may not always be realistic. The authors argued this is the first step towards realistic FL; however, it is also true the clients can bargain for incentives in a non-cooperative manner, where the assumptions for the utility model would be different, e.g., they could be private, just like in the auction-based methods, but truthful? This implies missing relevant works, as indicated by Reviewer A7vi."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3688/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700465666916,
                "cdate": 1700465666916,
                "tmdate": 1700465666916,
                "mdate": 1700465666916,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Xv18jpbqC3",
                "forum": "iJBQAAhqvY",
                "replyto": "mstFewNR3t",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3688/Reviewer_gdSK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3688/Reviewer_gdSK"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their valuable efforts in clarifying the concerns I raised before. Actually, if I can kindly disagree, I wrote, \"The authors argued this is the first step towards realistic FL\", rather than what you have mentioned before in your response: $\\textbf{The main novelty of our work is, in the words of the reviewer, to create a FL mechanism which takes \"the first step towards realistic FL\"}.$ I am still not sure, with i.i.d., assumption, the accuracy model is a \"realistic step\". Thank you for clarifying that it cannot generalize to non-i.i.d. cases but covers only a few cases, if so.\n\nLeaving that aside, as we are at the final day of the discussion period, considering the merit and contribution of this work and following up on other reviewers' comments and your responses to them, I am keeping my scores at this stage."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3688/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700639130015,
                "cdate": 1700639130015,
                "tmdate": 1700639130015,
                "mdate": 1700639130015,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "gixvBB91pl",
            "forum": "iJBQAAhqvY",
            "replyto": "iJBQAAhqvY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3688/Reviewer_EigJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3688/Reviewer_EigJ"
            ],
            "content": {
                "summary": {
                    "value": "To address the free-rider issue in federating learning, this paper proposes RealFM, a mechanism that takes into account device utility and incentivizes data contribution and device participation. Compared with previous work by Karimireddy et al. (2022), RealFM allows for a non-linear relationship between device accuracy and utility."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper presents an interesting approach to address the free-rider problem in federated learning.\n2. Convex utility functions do hold in many instances, and it is necessary to design a mechanism to incentivize device participation for such cases, as this paper does."
                },
                "weaknesses": {
                    "value": "1. The assumption that the closed form of each device's utility function is known, which may not always be realistic.\n2. RealFM distributes model accuracy and rewards based on the amount of data provided by each device. This may raise questions about potential cheating. It would make more sense to depend on each device's marginal contribution to model accuracy, provided that it can be easily and accurately determined. \n3. Theorem 4, which claims that RealFM eliminates the free-rider phenomena, may not be particularly thrilling. Designing a contract to incentivize individuals when their exact contribution is known is not a challenging task.\n4. The comparison between linear RealFM and local training does not effectively demonstrate how well RealFM incentivizes non-linear devices' contribution to model training. It is understandable that the authors cannot be blamed for weak baseline algorithms, as this paper is the first to aim at incentivizing device data contribution in a non-linear setting. However, in such cases, it would be more valuable to have theoretical guarantees of data maximization to truly showcase the strength of RealFM, which the paper lacks, except for the linear setting."
                },
                "questions": {
                    "value": "The authors provide explanations for modeling the relationship between model accuracy and data quantity as Eq. (1) and (2), it would be interesting to explore whether the results can be generalized to accommodate more general accuracy functions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3688/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698627902069,
            "cdate": 1698627902069,
            "tmdate": 1699636325432,
            "mdate": 1699636325432,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KLjv1Xqiv2",
                "forum": "iJBQAAhqvY",
                "replyto": "gixvBB91pl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3688/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3688/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reviewer EigJ Rebuttal"
                    },
                    "comment": {
                        "value": "We thank reviewer EigJ for their thoughtful review of our paper. Below we address the questions raised.\n\n> The assumption that the closed form of each device's utility function is known, which may not always be realistic.\n\nWe agree with the reviewer that assuming the form of $u_i$ can be unrealistic in some instances for each device. However, we believe that using a closed-form $u_i$ is an important first step towards realistic FL. Using a closed-form $u_i$ for each device is much more realistic than assuming that devices will always participate in federated training regardless of its costs. The majority of FL papers do not even attempt to model device or server utility, which is quite unrealistic. Furthermore, there is usage of closed-form utility functions in economics literature [Trout (1963) & Orsborn et al. (2009)] as well as with mechanism design in ML [Karimireddy et al. (2022) and Zhan et al. (2021; 2020b)] to name a few. Removing the closed-form assumption is a great future research direction.\n\n> This may raise questions about potential cheating. It would make more sense to depend on each device's marginal contribution to model accuracy, provided that it can be easily and accurately determined.\n\nWe are wary that many other avenues to determine each device's marginal contribution, like Federated Shapley Value Wang et al. (2020), can be cheated as well. The issues surrounding honesty are an *important* future research direction in FL. We set out to accomplish quite a bit in our work, as **it is the first FL mechanism which more realistically models device utility (non-linear function of model accuracy) and provably improves device participation and contributions (eliminating the free-rider effect) without the need of a payment mechanism**. Specific novelties are listed in our rebuttal for Reviewer A7vi.\n\nIn summary, we aim to address honesty in future works but we first wanted to establish a foundational base for FL mechanisms which incentivize device participation and contributions.\n\n> Theorem 4, which claims that RealFM eliminates the free-rider phenomena, may not be particularly thrilling. Designing a contract to incentivize individuals when their exact contribution is known is not a challenging task.\n\nAs mentioned within our paper, contract-theory approaches require construction of a payment mechanism. Pricing of the contracts is especially difficult and can often result in sub-optimal utility gained by both the devices and servers (too low of a reward does not incentivize devices to participate and too high of a reward can cause the server to attain low or negative utility).\n\nThe importance of our work is that our mechanism allows free-riding to be eliminated **without having to construct a payment mechanism prior to training**. Using accuracy shaping, devices are incentivized to contribute more data because they will achieve a higher-performing model than they would by training alone. Theory 4 provably eliminates free-riding while also guaranteeing that devices will *increase* their contributions under a new accuracy-shaping scheme, differing from Karimireddy et al. (2022), which enables flexible modeling of diverse device utilities that accommodate non-linear relationships between model accuracy and utility. Furthermore, this mechanisms allow for monetary rewards to be received by devices after training."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3688/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700058847408,
                "cdate": 1700058847408,
                "tmdate": 1700058847408,
                "mdate": 1700058847408,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BPTSEHRuU3",
                "forum": "iJBQAAhqvY",
                "replyto": "gixvBB91pl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3688/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3688/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reviewer EigJ Rebuttal (Continued)"
                    },
                    "comment": {
                        "value": "We continue our rebuttal below and address the remaining questions within the review.\n\n> It is understandable that the authors cannot be blamed for weak baseline algorithms, as this paper is the first to aim at incentivizing device data contribution in a non-linear setting... it would be more valuable to have theoretical guarantees of data maximization to truly showcase the strength of RealFM, which the paper lacks, except for the linear setting.\n\nAs mentioned by the reviewer, due to the dearth of FL mechanisms that allow for non-linear relationships between model accuracy and utility we were unable to compare against other mechanisms. **We believe this speaks volumes about the novelty and importance of our work**.\n\nAs mentioned by the reviwer, we prove data maximization under linear settings in Corollary 1. We were unable to guarantee tight theoretical guarantees in other settings due to the non-linearity. This stems from the proof of Theorem 3, specifically relaxing Equation 29. In fact, we assume $\\phi_i(a)$ is convex with respect to the accuracy $a$ **in order to make the bound as tight as possible**. Therefore, the accuracy-shaping function proposed in Theorem 3 is close to data maximizing but falls slightly short.\n\nWe also mention that our empirical results showcase major improvement (up to 7x more) in device contributions relative to the available baselines even when the profit margin is set to 100%. Our results are even stronger when the server is not completely greedy. *We outperform all state-of-the-art mechanisms which is both novel and important.*\n\n> ...it would be interesting to explore whether the results can be generalized to accommodate more general accuracy functions.\n\nAs long as the accuracy function $a$ fulfills the mild and reasonable assumptions laid out in Assumption 1: continuous, non-decreasing, and concave (along a certain interval) with respect to the amount of data $m$. These requirements are intuitive and backed by empirical studies Sun et al. (2017): accuracy grows with the amount of data used (continuous and non-decreasing) yet experiences diminishing returns (concave)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3688/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700058915296,
                "cdate": 1700058915296,
                "tmdate": 1700058915296,
                "mdate": 1700058915296,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DezD6iZ007",
            "forum": "iJBQAAhqvY",
            "replyto": "iJBQAAhqvY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3688/Reviewer_A7vi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3688/Reviewer_A7vi"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes RealFM to incentivise devices to participate in training and alleviate the threat of free riders. The paper considers that each rational device wish to maximise its utility, which depends on its cost of participation and non-linearly on its model accuracy and monetary reward. RealFM involves giving each device a monetary reward and design/use a \u201caccuracy shaping\u201d  function to boost a device\u2019s model accuracy and incentivize it to produce more data."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper novelly consider utility as a non-linear function of model accuracy. This makes accuracy shaping (incentivising contribution beyond local optimal amount) harder. \n- The paper is generally written well although some notations should be defined earlier."
                },
                "weaknesses": {
                    "value": "- Some assumptions are simplistic and limit the significance of the work. In equation 1, the accuracy is modelled to depend solely on the number of data points but in practice, devices may have data with different quality, diversity and noise. Moreover, it is hard to decide the difficulty of the learning task $k$ and the server would not have access to data for tuning (Sec. 6) before incentivization. As $a$ is an upper bound, the individual rationality for the upper bound does not translate to rationality for the expected/actual values. \n- The advantage of this work over the existing work has to be made clearer. There needs to be a deeper discussion about Shapley value based/collaborative fairness approaches including Xu et. al. (2021) and Wang et. al. (2020). In appendix A, it is suggested that these work \u201cassume that devices are already willing to contribute all of their data\u201d. This claim may be inaccurate \u2014 these work guarantees that contributing less (nothing) leads to less (no) reward thus devices would respond by contributing more data as in this work. The difference is that there is no closed form function for device $i$ utility.\n\n  Wang, T., Rausch, J., Zhang, C., Jia, R., & Song, D. (2020). A principled approach to data valuation for federated learning. Federated Learning: Privacy and Incentive, 153-167. \n- Some notations are used before they are defined or explained (e.g., $[\\mathcal{M}^U(\\cdot)]_i $ in Theorem 2). This makes the claims unclear and harder to understand.\n- The central server has to compute $m_i^o$ and $m_i^*$ based on the declared $\\phi_i$ and $c_i$. Device $i$ may misreport the cost to get a better reward."
                },
                "questions": {
                    "value": "Questions\n1. In related works, it is mentioned that Karimireddy et al. (2022) \u201crequires data sharing between devices and the central server\u201d. Is the amount of data sharing the same as centralised FL? How does the amount of sharing in this work differ?\n2. The Shapley value and collaborative fairness based approaches in existing work may not assume that devices are willing to contribute data. They just guarantee if they are unwilling and contribute less (nothing), they receive less (no) reward. Resultingly, rational devices should contribute more data as in this work. Can you clarify and make the differences/advantages of your work more specific?\n3. What is the implication/significance of Theorem 2? \n4. Does the mechanism _need_ monetary rewards to incentivise devices (can the profit margin be set to 1)?\n5. In Eq 10, must $\\phi_C(a(\\sum m))$ be less than $\\sum m$ to ensure a profit margin? \n6. What does $\\epsilon$ in Theorem 3 represent or control?\n7. In practice, how does the central server produce a model with accuracy $a(m_i^o) + \\gamma_i(m_i)$? Is it guaranteed to be less than $a(\\sum m)$? Does the server solve for the number of additional data points to use?\n\n\nMinor suggestions\n* The theorem and corollary should come with intuitive description to aid understanding. For example, for C1, a device with lower marginal cost has higher utility and would contribute more data. \n* In definition 1, $m$ can be used in place of $\\sum \\mathbf{m}$. $\\mathbf{m}$ is not defined."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3688/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698643290616,
            "cdate": 1698643290616,
            "tmdate": 1699636325338,
            "mdate": 1699636325338,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qVk7qGSKzv",
                "forum": "iJBQAAhqvY",
                "replyto": "DezD6iZ007",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3688/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3688/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reviewer A7vi Rebuttal (Weaknesses)"
                    },
                    "comment": {
                        "value": "We thank reviewer A7vi for their thoughtful review of our paper. Below we address the questions raised. We first address the \"Weakness\" section within the review. Equations referenced below correspond to our revised paper.\n### Novelty (Advantages) of our Work\nWe begin our rebuttal by detailing the novelty of our work. **Our work is the first FL mechanism which more realistically models device utility (non-linear function of model accuracy) and provably improves device participation and contributions (eliminating the free-rider effect) without the need of a payment mechanism prior to training**. We sought to design a foundational FL mechanism which improves upon previous FL mechanisms by:\n1. Modeling device utility in a more realistic manner which enables flexible modeling of diverse device utilities that accommodate non-linear relationships between model accuracy and utility.\n     - Allowing this non-linear relationship within device utility ***requires major changes*** to the accuracy-shaping procedure, which incentivizes increased contributions in exchange for higher model accuracy, at the heart of the incentive mechanism (Theorem 4).\n     - The altered accuracy-shaping function and its provable guarantees in Theorem 3 ***are novel***.\n     - The accuracy-shaping function incorporates monetary rewards, which are ***another novel inclusion*** detailed in (3).\n2. Not requiring data to be shared between devices and a central server. Data sharing is required in Karimireddy et al. (2022), our closest related mechanism, and it violates the true FL setting.\n3. Not requiring design of a payment mechanism (like that in Contract Theory) which needs careful construction of the payment *prior to training*.\n    - This is similar to Karimireddy et al. (2022), however we instead define the central server's own utility and showcase that our mechanism maximizes the server's utility.\n    - Also unlike Karimireddy et al. (2022), we leverage our newly defined server utility to allow monetary rewards for participating devices. This is fully novel within the non-payment mechanism setting.\n4. Demonstrating empirically (on real-world data) that RealFM succeeds in the goals laid out within our paper: server utility, device utility, and device contributions skyrocket while free-riding is negated when devices use RealFM.\n---\n### Simplicity of Assumptions\n> (1) Accuracy is modeled to depend solely on the number of data points... devices may have data with different quality, diversity and noise.\n> (2) The central server has to compute $m_i^o$ and $m_i^*$ based on the declared $\\phi_i$ and $c_i$. Device $i$ may misreport the cost to get a better reward.\n\nThe overarching goal of our work is to create a foundational FL mechanism which solves many of the issues detailed in our paper as well as above. There remain many frontiers to improve how realistically device utility is modeled. Data and truthfulness are two of them. We agree that in practice devices have different data distributions and data quality can vary both inter- and intra-device. We also agree that devices can be dishonest and report untruthful costs in practice. Relaxing our assumptions on data quality and truthfulness is an important follow-up to our work. The literature provided by reviewer A7vi on data quality is a great place to begin tackling the data quality assumptions.\n\n> Hard to decide the difficulty of the learning task $k$ and the server would not have access to data for tuning (Sec. 6) before incentivization. As $a$ is an upper bound, the individual rationality for the upper bound does not translate to rationality for the expected/actual values.\n\nThe use of $a$ in our paper was, in part, to allow for empirical analysis of our mechanism. What is most important about $a$ is that it must hold under reasonable and mild assumptions: continuous, non-decreasing, and concave (along a certain interval) with respect to the amount of data $m$. As shown in empirical studies Sun et al. (2017), accuracy grows with the amount of data used (continuous and non-decreasing) yet the growth experiences diminishing returns (concave). In practice, $a$ can be tuned by the server to align with test accuracy curves (as the accuracy improves with more contributions and updates).\n\n---\n### Paper Refinements\n> (1) There needs to be a deeper discussion about Shapley value based/collaborative fairness approaches including Xu et. al. (2021) and Wang et. al. (2020)\n> (2) Some notations are used before they are defined or explained (e.g., $[\\mathcal{M}^U(\\cdot)]_i$ in Theorem 2). This makes the claims unclear and harder to understand.\n\nWe appreciate the review's feedback and have expanded our related works and fixed our mistaken claim in Appendix A. $[\\mathcal{M}^U(\\cdot)]_i$ is the utility received by a device $i$ participating in the mechanism. We added or moved definitions of notations to ensure they are defined when they first appear. We have fixed all minor suggestions."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3688/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700055536763,
                "cdate": 1700055536763,
                "tmdate": 1700055536763,
                "mdate": 1700055536763,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "u1Eh1mdK1t",
                "forum": "iJBQAAhqvY",
                "replyto": "DezD6iZ007",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3688/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3688/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reviewer A7vi Rebuttal (Questions)"
                    },
                    "comment": {
                        "value": "We continue our rebuttal here and address the \"Questions\" section within the review.\n\n> Karimireddy et al. (2022) \u201crequires data sharing between devices and the central server\u201d. Is the amount of data sharing the same as centralised FL? How does the amount of sharing in this work differ?\n\nNo data is shared within centralized FL. Instead, gradient updates are communicated between the devices and a central server. In Karimireddy et al. (2022), devices share their actual data with a server *which violates the fundamental underpinning of FL*.\n\n> Shapley value and collaborative fairness based approaches in existing work may not assume that devices are willing to contribute data... rational devices should contribute more data as in this work. Can you clarify and make the differences/advantages of your work more specific?\n\nWe detail the novelty of our work in the first part of our rebuttal. Wang et al. (2020) proposes a method to estimate the federated Shapley Value (which holds similar properties as the regular Shapley Value) in order to determine data quality from the devices participating in FL training. Thus, Wang et al. (2020) and other Shapley Value works seek to evaluate each data source in the FL setting. *Our work seeks to incentivize and increase device participation, through mechanism design, within FL. This is fundamentally different.*\n\nCollaborative fairness works such as Lyu et al. (2020) are closer to our own in that they seek to fairly allocate models with varying performance depending upon how much devices contribute to training in FL settings. There are a few **key** differences between this line of work and our own, namely:\n1. Our mechanism incentivizes devices to both participate in training and increase their amount of contributions. There are no such incentives in collaborative fairness.\n2. We model device and server utility. Unlike collaborative fairness, we do not assume that devices will always participate in FL training.\n3. Our mechanism design **provably** eliminates the free-rider phenomena unlike collaborative fairness, since devices who try to free-ride receive the same model performance as they would on their own (Equations 12 & 13).\n4. No monetary rewards are received by devices in current collaborative fairness methods.\n\n> What is the implication/significance of Theorem 2?\n\nTheorem 2 provides the theoretical underpinnings for a mechanism with certain assumptions to reach a Nash Equilibrium. Our mechanism satisfy these assumptions and thus reach an equilibrium at which devices will not deviate from their strategies (contributions). We prove in Theorem 3 that accuracy shaping used within our mechanism provably increases device utility and contributions. By Theorem 4, our mechanism reaches an equilibrium (Theorem 2) at which devices contribute more, receive higher utility, and no free-riding exists (Theorem 3).\n\n> Does the mechanism need monetary rewards to incentivise devices (can the profit margin be set to 1)?\n\nNo. Monetary rewards are not required. In fact, for ease of empirical analysis, *we set the profit margin equal to 1 in all of our experimental results*. Thus, device contributions and utility would **increase** further if we had set the profit margin to a lower value. This showcases how strong our mechanism works empirically.\n\n> In Eq 10, must $\\phi_C(a(\\sum m))$ be less than $\\sum m$ to ensure a profit margin?\n\nThere may be some confusion regarding the profit margin. The profit margin $p_m$ dictates what *percentage* of the overall utility gained by the server is kept by the server. The remaining $1-p_m$ percentage of the utility is uniformly returned (as a marginal monetary reward) to each participating device as defined in Equation 10. Overall, $\\phi_C(a(\\sum m))$ can be larger, equal to, or smaller than $\\sum m$. This does not affect the profit margin, it only affects the amount of monetary rewards returned to each participating device.\n\n> What does $\\epsilon$ in Theorem 3 represent or control?\n\nThe parameter $\\epsilon$ simply ensures that Equation 11 will be a **strict** inequality. This is crucial to ensure that devices are receiving more utility participating in our mechanism, via accuracy shaping, than on their own. If the inequality is not strict then devices are not incentivized to increase their contributions.\n\n> In practice, how does the central server produce a model with accuracy $a(m_i^o) + \\gamma_i(m_i)$? Is it guaranteed to be less than $a(\\sum m)$?\n\nVia FL, the server will train a model using all contributions $\\sum m$ which will result in a model with accuracy $a(\\sum m)$. In order to provide devices with lower model performance, such as $a(m_i^o) + \\gamma_i(m_i)$, the server can degrade the model using a variety of methods. Some examples of ways to do this include using noisy perturbations in a controlled manner or returning a model midway through training which has lower performance."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3688/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700055588088,
                "cdate": 1700055588088,
                "tmdate": 1700055588088,
                "mdate": 1700055588088,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YexnV8yuOw",
                "forum": "iJBQAAhqvY",
                "replyto": "u1Eh1mdK1t",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3688/Reviewer_A7vi"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3688/Reviewer_A7vi"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response and clarifications that addressed my questions and improved the work! Here are some additional concerns.\n\n> In practice, $a$ can be tuned by the server to align with test accuracy curves (as the accuracy improves with more contributions and updates).\n\nThe server would need data to train models and obtain the test accuracy curves. However, it may be problematic that the server may lack data to do so before incentivizing contribution.\n\n> 1. Our mechanism incentivizes devices to both participate in training and increase their amount of contributions. There are no such incentives in collaborative fairness.\n3. Our mechanism design provably eliminates the free-rider phenomena unlike collaborative fairness, since devices who try to free-ride receive the same model performance as they would on their own (Equations 12 & 13).\n4. No monetary rewards are received by devices in current collaborative fairness methods.\n\nThe fairness axioms satisfied by the Shapley value may incentivise devices to increase their amount of contributions and ensure that a null player gets no reward. As an example, see the uselessness and strict monotonicity property in Sim et al. 2020.\n Collaborative fairness and data valuation works include a \"data valuation\" component that can be used to decide monetary reward (Jia, R., Dao, D., Wang, B., Hubis, F.A., Hynes, N., G\u00fcrel, N.M., Li, B., Zhang, C., Song, D. and Spanos, C.J., 2019, April. Towards efficient data valuation based on the shapley value. In The 22nd International Conference on Artificial Intelligence and Statistics (pp. 1167-1176). PMLR.)\n\n> No. Monetary rewards are not required. In fact, for ease of empirical analysis, we set the profit margin equal to 1 in all of our experimental results. Thus, device contributions and utility would increase further if we had set the profit margin to a lower value. This showcases how strong our mechanism works empirically.\n\nThe inclusion of the monetary rewards seems a bit unnecessary and separate from other contributions of this work. Can the authors highlight the importance of monetary rewards (in addition to accuracy rewards) and the challenges of designing it?"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3688/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700545124330,
                "cdate": 1700545124330,
                "tmdate": 1700545124330,
                "mdate": 1700545124330,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "37moMSnlr5",
                "forum": "iJBQAAhqvY",
                "replyto": "eaxMxCnzwJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3688/Reviewer_A7vi"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3688/Reviewer_A7vi"
                ],
                "content": {
                    "comment": {
                        "value": "Is it correct that the accuracy function $a$ is only needed after FL training is completed (Step 7 of Algo 1)? Hence, the server can train multiple models to estimate $a(m)$ for any $m \\in [0, \\sum \\mathbf{m}]$ during training?\n\nFor the last question, I meant why is monetary reward important and defined as in Eq 9 when the server is already giving model/accuracy reward. The response on (Monetary Reward Importance) partially address my question.\nCan you clarify and elaborate the impact of decrease in the profit margin $p_m$ on $\\gamma_i$?\nWhen $p_m$ is lower, $r(\\textbf{m})$ is higher, so $c_i - r(\\textbf{m})$ and $\\gamma_i$ (the additionally utility) is lower?"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3688/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700704219029,
                "cdate": 1700704219029,
                "tmdate": 1700704219029,
                "mdate": 1700704219029,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BIXi1S6ZJs",
                "forum": "iJBQAAhqvY",
                "replyto": "wk2XUDEpC4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3688/Reviewer_A7vi"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3688/Reviewer_A7vi"
                ],
                "content": {
                    "comment": {
                        "value": "> The effect of a smaller $\\gamma_i$ is that  $\\gamma_i$ can span a larger domain until it meets the limit of accuracy shaping. This results in a larger optimal contribution $m_i^*$ for participating devices \n\nCan you clarify this further?"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3688/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700730635976,
                "cdate": 1700730635976,
                "tmdate": 1700730635976,
                "mdate": 1700730635976,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rXcdAv9iJ3",
            "forum": "iJBQAAhqvY",
            "replyto": "iJBQAAhqvY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3688/Reviewer_jpGm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3688/Reviewer_jpGm"
            ],
            "content": {
                "summary": {
                    "value": "The presented research addresses the challenges of edge device participation in federated learning (FL) and the shortcomings of existing FL frameworks when applied in real-world contexts, particularly in addressing the free-rider problem. In response to these issues, the authors propose a novel approach called RealFM, which introduces several key innovations including Realistic Device Utility Modeling, Incentivizing Data Contribution and Participation, and Elimination of the Free-Rider Phenomenon. Experiments show that RealFM exhibits excellent performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "RealFM represents a noteworthy contribution to the field of federated learning, addressing the need for more realistic settings and incentives for edge device participation. Its ability to model device utility, eliminate the free-rider problem, and improve utility and data contribution is particularly promising for advancing FL in real-world applications."
                },
                "weaknesses": {
                    "value": "1. In the experimental setting, the number of devices is not large, which is not very consistent with the actual application scenario.\n2. Intuitively I know roughly what Server Utility and Average Data Contribution mean. But in detail, I may still not fully understand how Server Utility and Average Data Contribution are Numerized. In particular, I don\u2019t quite understand why Server Utility has been improved so much."
                },
                "questions": {
                    "value": "How will performance change when the number of devices increases?\nCan you explain Server Utility and Average Data Contribution in simpler and more intuitive language?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3688/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698787159725,
            "cdate": 1698787159725,
            "tmdate": 1699636325259,
            "mdate": 1699636325259,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Z5tGkYaphB",
                "forum": "iJBQAAhqvY",
                "replyto": "rXcdAv9iJ3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3688/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3688/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reviewer jpGm Rebuttal"
                    },
                    "comment": {
                        "value": "We thank reviewer jpGm for their thoughtful review and look forward to our discussion. Below we address the questions raised in their review.\n\n### Experimental Size\n\nWe want to note that our experiments were run in a truly parallel and federated setting. Within our compute cluster, we assigned each CPU/process as a device and utilized MPI to perform communication between CPUs and a server CPU. Furthermore, each CPU was pinned to a GPU on which gradient computations were performed.\n\n> In the experimental setting, the number of devices is not large, which is not very consistent with the actual application scenario.\n\nDue to the amount of compute resources, we could only use 16 CPUs and 8 GPUs (2 CPUs pinned to 1 GPU). We do lack the resources to perform realistic federated training in the realms of hundreds or thousands of devices. Furthermore, many existing FL papers use a similar scale as our own. Finally, other FL mechanism papers either use a similar number of devices *or do not perform truly federated experiments at all*. Our work is one of the first to provide truly federated empirical evidence to back up the claims of our proposed mechanism.\n\nWe also lack sufficient amounts of data to be split amongst devices if we have thousands of devices. The number of unique samples in the MNIST dataset is 60k, and this is even smaller since a fraction needs to be removed to evaluate test accuracy. In future work, with the proper compute power, we would love to showcase the efficacy of our mechanism on thousands of devices with extremely large real-world data.\n\n> How will performance change when the number of devices increases? \n\nOur mechanism should improve as the number of participating devices increases. The reason is that more devices means more data used during FL training. Through the mild assumptions on $a(m)$, increasing and concave, more overall data contributions $\\mathbf{m}$ will result in higher final accuracies $a(\\sum\\mathbf{m})$. The higher final accuracy will create a larger window for accuracy shaping, $m_i^* := [ m \u2265 m^o_i | a(m + \\sum_{j \\neq i} m_j) = a(m^o_i) + \\gamma_i(m) ]$, which in turn will result in a stronger incentive for devices to contribute more.\n\n### Utility Explanation\n\n> Can you explain Server Utility and Average Data Contribution in simpler and more intuitive language?\n\nServer utility is simply the net benefit gained by the server from having a trained model with accuracy $a$: $\\phi_C(a)$. Within our paper, the accuracy is a function of the total data contributions $\\mathbf{m}$, and thus the final server utility is $p_m \\cdot \\phi_C(a(\\sum \\mathbf{m}))$. $p_m$ is just the profit margin, or the percentage of utility kept by the server after federated training is complete. The remaining $(1-p_m)$ percent is distributed uniformly as a monetary reward to all participating devices.\n\nAverage data contribution is the average amount of data that each device has determined is optimal (*i.e.,* when device utility is maximized) to contribute. We see experimentally that the average amount of data contributions when devices train locally is substantially smaller than the optimal amount when devices participate in our mechanism. Furthermore, the average device utility also dramatically increases when participating in our mechanism!\n\n> I may still not fully understand how Server Utility and Average Data Contribution are Numerized. \n\nFollowing literature in economics, mechanism design, and the intersection of both in FL, utility is often defined as a real-valued number. This number denotes the \"net benefit\" that a device attains. A larger real value denotes a larger net benefit received by the device.\n\n> In particular, I don\u2019t quite understand why Server Utility has been improved so much.\n\nAs detailed above, server utility drastically improves because our mechanism incentivizes devices to contribute more data. By contributing more data, the accuracy $a(\\sum \\mathbf{m})$ increases (due to our mild assumptions) which in turn increases server utility $\\phi_C(a(\\sum \\mathbf{m}))$ since $\\phi_C$ is also strictly increasing with respect to the accuracy $a$."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3688/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700152336479,
                "cdate": 1700152336479,
                "tmdate": 1700152336479,
                "mdate": 1700152336479,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jyyQ4zwsLJ",
                "forum": "iJBQAAhqvY",
                "replyto": "Z5tGkYaphB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3688/Reviewer_jpGm"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3688/Reviewer_jpGm"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Reviewer jpGm"
                    },
                    "comment": {
                        "value": "Thanks for the detailed response. I have read the rebuttal and most of my concerns have been well addressed. Overall, I am towards acceptance and will maintain my score.\n\nBest, The reviewer jpGm"
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3688/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700736726367,
                "cdate": 1700736726367,
                "tmdate": 1700736726367,
                "mdate": 1700736726367,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]