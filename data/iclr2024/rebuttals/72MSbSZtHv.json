[
    {
        "title": "RedMotion: Motion Prediction via Redundancy Reduction"
    },
    {
        "review": {
            "id": "P7Z0GyPcRT",
            "forum": "72MSbSZtHv",
            "replyto": "72MSbSZtHv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9187/Reviewer_NoCd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9187/Reviewer_NoCd"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to reduce the number of map tokens by the Barlow Twins objective. They try their method on Waymo Open Motion dataset."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. It is important in the autonomous driving field to reduce model size to speed up."
                },
                "weaknesses": {
                    "value": "1. As a work focused on efficiency, no statistics about inference memory footprint or inference time are reported. I suggest the authors report these related statistics and compare with open-sourced classic baselines like MTR/QCNet. Otherwise, the claimed advantage of reduction can not be known.\n\n2. Wrong experiment setting. The authors state that *we use 100% of the training data for pre-training and fine-tune on only 12.5%.*. However, the data used for pretraining is already annotated data for motion prediction and I do not see explanations about the difference between pretraining data and finetune data. If this is the case, then it is meaningless to adopt this setting. Thus, I suggest all experiments should be re-conducted with 100% finetune for fair comparison.\n\n3. Self-defined metrics without proper justification. For Waymo Open Motion dataset, widely used (and official) metrics are minADE_6, minFDE_6, mAP averaged over 3s, 5s, 8s. I do not see the necessity of using these similar but different metrics in the paper. For fair comparision, I suggest reporting the official metrics in the paper.\n\n4. No test set results. Waymo Open Motion Leaderboard is easy to submit and I think it is **very necessay** to report the results on it to avoid overfitting the validation set."
                },
                "questions": {
                    "value": "See weakness part.\n\n\nIn summary, I enjoy the idea of reducing map tokens, which is important if it can speed up. However, the authors do not mention any inference memory or speed related statistics. Besides, the experiment settings and metrics are unjustified and no test set results are reported, which triggers the concerns of unfair comparison. Thus, I give a reject rating."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9187/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697543170736,
            "cdate": 1697543170736,
            "tmdate": 1699637156320,
            "mdate": 1699637156320,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0OklYhRwWG",
                "forum": "72MSbSZtHv",
                "replyto": "P7Z0GyPcRT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9187/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9187/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response by the authors"
                    },
                    "comment": {
                        "value": "Thank you for the feedback on our work. Below are our responses to the raised issues. Furthermore, we have highlighted changes in the paper in blue.\n\n* Issue 1: As a work focused on efficiency, no statistics about inference memory footprint or inference time are reported.\n\nWe want to highlight that our work is not focused on efficiency but on self-supervised representation learning.\n\n* Issue 2: Data used for pretraining is already annotated data for motion prediction and I do not see explanations about the difference between pretraining data and finetune data.\n\nWe opted for this setting as it represents a common practice in self-supervised representation learning. This approach aligns with the seminal works in computer vision, such as [[SimCLR](https://proceedings.mlr.press/v119/chen20j.html)] , [[SwaV](https://proceedings.neurips.cc/paper/2020/hash/70feb62b69f16e0238f741fab228fec2-Abstract.html)], which employ this setting for the labeled dataset ImageNet.\n\n* Issue 3: Self-defined metrics without proper justification and no test results.\n\nWe have added a bar plot of the number of trajectory proposals in Appendix C. Furthermore, it was our misunderstanding that we interpreted the following sentence as, each prediction can contain up to 8 trajectory proposals per agent: \"Each ScenarioPredictions proto within the submission corresponds to a single scenario in the test set and contains up to 8 predictions for the objects listed in the tracks_to_predict field of the scenario\"  [[https://waymo.com/open/challenges/2023/motion-prediction/#submit](https://waymo.com/open/challenges/2023/motion-prediction/#submit)]\n\nThe current challenge rules only allow up to 6 proposals per agent, therefore we retrained our models with 6 trajectory proposals and submitted the results of our best model to the challenge leaderboard: [[Waymo Prediction Challenge Entry](https://waymo.com/open/challenges/entry/?challenge=MOTION_PREDICTION&challengeId=MOTION_PREDICTION_2023&emailId=824a6c20-3f34&timestamp=1700489974797915\n)]"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9187/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700563130209,
                "cdate": 1700563130209,
                "tmdate": 1700563130209,
                "mdate": 1700563130209,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BM1SfuriOz",
                "forum": "72MSbSZtHv",
                "replyto": "0OklYhRwWG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9187/Reviewer_NoCd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9187/Reviewer_NoCd"
                ],
                "content": {
                    "title": {
                        "value": "Regarding Issue 2"
                    },
                    "comment": {
                        "value": "Thanks for your response.\n\nRegarding the sentence \"This approach aligns with the seminal works in computer vision, such as [SimCLR] , [SwaV], which employ this setting for the labeled dataset ImageNet.\",  I can not concur: note that the label for the images are separate index $y$ which is not used during the training of [SimCLR] , [SwaV]. The label for a history trajectory is its future trajectory. Does the authors suggest that the proposed method only utilizes the observed trajectory in the training set (in Waymo, it is 1.1 second) for pretraining?\n\nAs for the updated results Table, I observe that **the proposed method's performance varies significantly on the validation and test set** of Waymo while other methods (SceneTransformer, MTR, MTR++) all performs very similarly on the two sets.  I am wondering about the overfitting issue."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9187/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700564123175,
                "cdate": 1700564123175,
                "tmdate": 1700564123175,
                "mdate": 1700564123175,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CV69YLo3KG",
            "forum": "72MSbSZtHv",
            "replyto": "72MSbSZtHv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9187/Reviewer_1Zub"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9187/Reviewer_1Zub"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a novel self-supervised embedding strategy to learn representations\nfor motion prediction datasets in the autonomous driving context. Their method have two main technical\ncontributions. The tokenization of the road embeddings is done using a limited set of discretized tokens with a global attention. \nThe second contribution is  is to enforce the network to represent similar representations with similar tokens. That is done by enforcing correlation between inputs with only a small augmentation distance between them.\n\nThey showed competitive results when compared to other self-supervised representation learning methods\nand other motion prediction methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The paper presents solid contributions on representation learning for autonomous driving scenes in the motion prediction context.\n* The general explanation of the obtained embeddings for the road map is clear and easy to understand. The experiments with local attention provide \n* The presented results are competitive with the state-of-the-art even though the method focused on the pre-trained representation instead of actually achieving SOTA results on motion prediction benchmarks. That shows a potential scalability of this type of approach.\n* they provide already an anonymized implementation of the code."
                },
                "weaknesses": {
                    "value": "1. One potential weakness of the paper is the lack of evaluation of the strategy in a closed loop fashion. While motion prediction benchmarks are still used it has been show that motion prediction alone tends to have poorer quality when evaluated in a closed loop. Examples of that are NuPlan [1] evaluation and the more recent waymo closed loop prediction benchmarks.\n\n2.  For the ablations I missed acquiring a bit more understanding on the relevance of RBT versus just a simple tokenization. It would be nice to assess the impact of enforcing the similar augmented inputs into using the same tokens versus just an arbitrary tokenization. From what I understood the ablations were done mainly on the way the representation was fused.\n\n\n\n\n[1] Caesar, H., Kabzan, J., Tan, K. S., Fong, W. K., Wolff, E., Lang, A., ... & Omari, S. (2021). nuplan: A closed-loop ml-based planning benchmark for autonomous vehicles. arXiv preprint arXiv:2106.11810."
                },
                "questions": {
                    "value": "I missed a more clear explanation on the motion prediction baseline used when not applying any pre-training."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9187/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698669864159,
            "cdate": 1698669864159,
            "tmdate": 1699637156111,
            "mdate": 1699637156111,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0QODV3c9UJ",
                "forum": "72MSbSZtHv",
                "replyto": "CV69YLo3KG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9187/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9187/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response by the authors"
                    },
                    "comment": {
                        "value": "Thank you for the feedback on our work. Below are our responses to your questions. Furthermore, we have highlighted changes in the paper in blue.\n\n* Q1: More clear explanation on the motion prediction baseline used when not applying any pre-training.\n\nLearning global environment features represented as RED tokens is our contribution, hence we remove the parallel decoder in the road environment encoder for the baseline version (see Figure 1). To allow a fair comparison, we increase the token dimension for the baseline from 128 to 192 to give both models a similar capacity (RedMotion baseline 9.9M params vs. RedMotion 9.2M params). We added this explicit size comparison to Section 4.1.\n\n* Issue 1: Lack of evaluation of the strategy in a closed loop fashion.\n\nWe plan to evaluate our approach in the new SimAgents challenge of the Waymo Open Motion dataset in a future work.\n\n* Issue 2: Impact of enforcing the similar augmented inputs into using RED tokens versus just an arbitrary tokenization.\n\nWe tested this with our baseline model, which has no parallel decoder for RED tokens, in Section 4.2. The 6th row in Table 1 shows the performance of our pre-training method (RBT) without RED tokens."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9187/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700562723043,
                "cdate": 1700562723043,
                "tmdate": 1700562723043,
                "mdate": 1700562723043,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "sWoHdCIpDd",
            "forum": "72MSbSZtHv",
            "replyto": "72MSbSZtHv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9187/Reviewer_4aEu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9187/Reviewer_4aEu"
            ],
            "content": {
                "summary": {
                    "value": "The authors tackle the problem of representation learning for motion prediction. They first pre-train a map encoder to output similar representations for map inputs across augmentations, where augmentations include rotations of the coordinate frame by up to 10 degrees and translations of the coordinate frame by up to 1 meter. They then train a model that takes the map encoding as input as well as historical agent trajectories and outputs future single-agent trajectories parameterized by mixture-of-gaussians. The authors show impressive results in the semi-supervised setting in which the map data for all scenarios is kept, but track data is available for only 20% of the scenes in the waymo open dataset."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper proposes a label-free method for pre-training a map encoder. I think the approach is novel, and I think map-only encoders are increasingly relevant for motion prediction in that, generally speaking, self-driving companies have a stream of motion data, and if the map encoding is fixed, training on new motion data as it comes in should be much more parameter efficient compared to training from scratch every time. I also appreciate the 3D visuals of the motion predictions in the paper, which helps with understanding how the model performs. Table 1 is also impressive, showing that the method proposed in the paper for training the map encoder outperforms a variety of other possible ways to train the map encoder."
                },
                "weaknesses": {
                    "value": "I think the main weakness of this paper is that I'm not convinced that both contributions claimed by the paper are valid. Specifically, the first claimed contribution is that the authors design an architecture that reduces the variable-length set of map objects to an encoding of fixed-length. I think the authors should clarify how this encoding is different from \"latent query attention\" used in Wayformer, which also reduces a variable-length set of objects to an encoding of fixed-length. This submission claims \"Compared to Wayformer, we encode past agent trajectories and environment context with separated encoders, offering more flexibility for specialized self-supervised pre-training\". However, wayformer also compares against a \"late fusion\" variant which seems very similar to the architecture proposed in this paper. Some clarification on these architectural choices in comparison to Wayformer are important to add, as well as an explanation of why Wayformer is excluded from Table 2.\n\nAdditionally, this is probably a misunderstanding, but for the second contribution of pre-training the map encoding to be invariant to small rotations and translations, isn't this undesirable? If the map rotates, the motion prediction should also rotate to some extent. I'm not entirely clear on the principle behind this pre-training objective.\n\nSome other comments and requests below:\n\n- Section 4.1 \"dataset\" - I'm lacking the motivation for studying the semi-supervised setting for motion prediction? Collecting training data only requires driving through cities and running perception models, so it seems to me all of the data is always labeled. If the argument is that we can use this approach to fine-tune motion prediction on maps for which we have no motion data, then the authors should explicitly demonstrate that their approach improves generalization to new maps, or pre-train on a dataset of only maps without any motion data. If the argument is that the pre-trained RED tokens are useful for many possible downstream tasks, the authors should benchmark more tasks than just motion prediction.\n- Section 3.2 \"We use the current speed information to learn separate embeddings for static and dynamic agents\" - to be clear, the authors check the current speed and if it's below a certain threshold, they set one of the features in the input to 0 and otherwise they set it to 1?\n- Section 3.2 \"Figure 3c shows the vocabulary size\" - is \"vocabulary size\" the right term? My understanding is that vocabulary size is the size of a set of discrete options. But my interpretation of Figure 3c is that it shows sequence lengths, and none of the inputs to the model are discretized.\n- Section 3.2 \"After the fusion step, we use global average pooling over the feature dimension to reduce the input dimension for an MLP-based motion head\" - is there a reason to do pooling then an MLP as opposed to a transformer layer followed by a linear layer, as in wayformer? Forcing average pooling is not ideal.\n- Section 4.2 \"Interestingly, the Scene Transformer in its joint prediction configuration performs worse than in its marginal prediction version\" - this is expected actually. Joint prediction should be better if minADE/minFDE are evaluated at the scene level, and marginal prediction should be better if minADE/minFDE are evaluated at the per-agent level.\n- Section 4.2 \"our model with 16 trajectory proposals can outperform all other methods\" - I think it's interesting to see a plot of minFDE as a function of samples, but I personally think it's too unfair of a comparison to include a 16-sample version in Table 2.\n- Table 2 - is there a reason the authors don't report mAP? The MTR and MTR++ models are chosen based on mAP which might lead to lower values for minFDE/minADE, so the current comparison isn't exactly apples-to-apples."
                },
                "questions": {
                    "value": "If the authors could clarify how their proposed architecture improves over Wayformer, that would be very helpful. I think it's ok if the authors just chose any architecture for the purposes of studying self-supervised pre-training, but since the authors emphasize the design of the architecture as a contribution, I want to understand that contribution better.\n\nIf the authors could also explain in more depth the principle behind their pre-training task, I would also find that to be helpful. From Table 2, it appears to be effective, but I don't yet see why this pre-training objective makes sense for learning good representations of the map information for the purposes of motion prediction."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9187/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699139367081,
            "cdate": 1699139367081,
            "tmdate": 1699637155996,
            "mdate": 1699637155996,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Ea2YWjS0Nf",
                "forum": "72MSbSZtHv",
                "replyto": "sWoHdCIpDd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9187/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9187/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response by the authors"
                    },
                    "comment": {
                        "value": "Thank you for the feedback on our work. Below are our responses to your questions. Furthermore, we have highlighted changes in the paper in blue.\n\n* Q1: How are road environment descriptors (RED) different from latent queries in Wayformer?\n\nThe methods are related, yet they primarily differ in two aspects: firstly, in their respective positions within the model, and secondly, in their intended purposes. Our RED are learned by an internal decoder and serve as input for a motion head, while latent queries form a part of the transformer decoder that serves as the motion head.\n\nRED provide environment context (map and current agent state) and are learned with self-supervised learning, while latent queries can be understood as trajectory anchors and are learned using supervised learning.\n\nBuilding on this difference, we have analyzed the effect of combining RED and latent queries. Specifically, we implemented a version of our model named RedMotion tra-dec, which uses a transformer decoder as the motion head. In this way, the RED are learned via redundancy reduction, thereby providing context that is subsequently transformed into trajectory proposals by the latent queries (refer to Sec. 4.2 for results).\n\n\n* Q2: Is a map encoding that is invariant to small rotations and translations undesirable for trajectory prediction?\n\nIn self-supervised pre-training, we learn similar embeddings for moderately augmented environments as correctly described by you. During subsequent fine-tuning, we use these embeddings as \"good initialization\" to learn trajectory prediction. At this phase, we do not augment environments anymore to learn representations that are responsive to translation and rotation.\n\nSimilarly, self-supervised methods in computer vision typically learn augmentation invariant features during pretraining, while during fine-tuning for object detection augmentation-sensitive features are learned. \n\n\n* Q3: Motivation for studying the semi-supervised setting for motion prediction? Collecting training data only requires driving through cities and running perception models.\n\nGenerating trajectory prediction datasets can involve labor-intensive post-processing, such as filtering false positive detections and trajectory smoothing, etc.\n\n* Q4: Speed threshold for dynamic agents.\n\nWe use a threshold of 0.0 m/s and have added this information in the revised paper.\n\n* Q5: Figure 3c, Is \"vocabulary size\" the right term?\n\nWe describe the number of discrete options with vocabulary size. The sequence length for environment tokens is 1200 and we have added these information in the revised paper.\n\n* Q6: Is there a reason to do pooling then an MLP as opposed to a transformer layer followed by a linear layer?\n\nAs described in our previous reply, we have implemented a version of our model with a transformer decoder (tra-dec) instead of a simple mean aggregation followed by an MLP.\n\n* Q7: Section 4.2 \"Interestingly, the Scene Transformer in its joint prediction configuration performs worse than in its marginal prediction version\u201d\n\nWe have revised this paragraph.\n\n* Q8: Plot minFDE as a function.\n\nWe have added a bar plot of the number of trajectory proposals in Appendix C. Furthermore, it was our misunderstanding that we interpreted the following sentence as, each prediction can contain up to 8 trajectory proposals per agent: \"Each ScenarioPredictions proto within the submission corresponds to a single scenario in the test set and contains up to 8 predictions for the objects listed in the tracks_to_predict field of the scenario\" [[https://waymo.com/open/challenges/2023/motion-prediction/#submit](https://waymo.com/open/challenges/2023/motion-prediction/#submit)]\n\nThe current challenge rules only allow up to 6 proposals per agent, therefore we retrained our models with 6 trajectory proposals and submitted the results of our best model to the challenge leaderboard: [[Waymo Prediction Challenge Entry](https://waymo.com/open/challenges/entry/?challenge=MOTION_PREDICTION&challengeId=MOTION_PREDICTION_2023&emailId=824a6c20-3f34&timestamp=1700489974797915\n)]\n\n* Q9: Table 2 - is there a reason the authors don't report mAP?\n\nWe have added the official challenge metric Soft mAP."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9187/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700562523026,
                "cdate": 1700562523026,
                "tmdate": 1700562978027,
                "mdate": 1700562978027,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xYtBKwC5Vv",
                "forum": "72MSbSZtHv",
                "replyto": "Ea2YWjS0Nf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9187/Reviewer_4aEu"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9187/Reviewer_4aEu"
                ],
                "content": {
                    "title": {
                        "value": "Additional clarification"
                    },
                    "comment": {
                        "value": "- \"latent queries can be understood as trajectory anchors\" - My interpretation of LQA is that it is a way to downsample the sequence length, e.g. it's a learnable intermediate layer that transforms BxLxC -> BxL'xC. This layer can be trained with backprop through the full network as in Wayformer, or pre-trained as in this work. I think pre-training the output of this layer - as is done in this work - is novel, but I'm still not seeing the architecture design as a contribution. I don't see how LQA can be understood as trajectory anchors. I'm wondering if the authors could further elaborate on what the issue with the Wayformer architecture is, and how RED improves on on this deficiency.\n- \"Similarly, self-supervised methods in computer vision typically learn augmentation invariant features during pretraining, while during fine-tuning for object detection augmentation-sensitive features are learned.\" - similar to computer vision, I really think the authors should perform their experiments in a setting where there is a large quantity of map data without trajectories. The fact that the waymo maps are crops of locations where scenarios occurred is implicitly using the label information.\n- \"Generating trajectory prediction datasets can involve labor-intensive post-processing, such as filtering false positive detections and trajectory smoothing, etc.\" I'm not particularly convinced - these labels are fully automated so I don't consider them \"labor-intensive\".\n- Waymo entry - How should we interpret this score? 0.3909 seems low compared to the current top submission of 0.4764."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9187/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700637220042,
                "cdate": 1700637220042,
                "tmdate": 1700637220042,
                "mdate": 1700637220042,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bUXccrASDm",
                "forum": "72MSbSZtHv",
                "replyto": "sWoHdCIpDd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9187/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9187/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for taking the time to address the remaining concerns.\n\n* **Issue 1:** Latent querys interpretation and issues with the Wayformer architecture.\n  * We don\u2019t think that there is any issue with the Wayformer architecture, but we see key difference to our architecture and modules. In our opinion, there are two forms of latent queries used in the Wayformer architecture: learned queries in the encoder and in the decoder.\n    * In the decoder (Sec. 3.3 Wayformer paper, learned seeds in Fig. 1) learned queries are used to attend to the encoded features and transform them into trajectory proposals. We think that these are inspired by the \u201clearned anchors\u201d from Waymo\u2019s previous works [MultiPath](https://arxiv.org/abs/1910.05449) or [MultiPath++](https://arxiv.org/abs/2111.14973v3). The key difference in our model, is that these elements are part of the motion head. They are learned through supervised learning and can be integrated into our model (tra-dec config).\n    * In the encoder(s) (Sec. 3.2 Wayformer paper) learned queries are used to reduce the computational cost for processing long input sequences. These are inspired by the [Set Transformer](https://proceedings.mlr.press/v97/lee19d.html) and [Perceiver](https://proceedings.mlr.press/v139/jaegle21a.html) architectures. In contrast, we are using [local attention](https://arxiv.org/abs/2004.05150) in our road environment encoder to lower the computational complexity (see Sec. 3.2 or Fig. 4 in our work) and a memory efficient implementation of cross attention to fuse features (see Fig. 5).\n\n* **Issue 2:** The authors should perform their experiments in a setting where there is a large quantity of map data without trajectories\n  * We want to highlight our approach represents a common practice in self-supervised representation learning. This approach aligns with seminal works in computer vision, such as [[SimCLR](https://proceedings.mlr.press/v119/chen20j.html)] , [[SwaV](https://proceedings.neurips.cc/paper/2020/hash/70feb62b69f16e0238f741fab228fec2-Abstract.html)], which employ this setting for labeled datasets such as ImageNet.\n\n* **Issue 3:** Trajectory labels can be generated fully automated.\n  * Although we are uncertain about the processing pipelines in large companies and organizations like Waymo or Motional, our experience suggests that collecting trajectory datasets invariably involves a significant amount of manual labor. This is due to the fact that fusion and perception systems can sometimes produce inaccuracies, such as jumps.\n\n* **Issue 4:** How to interpret the scores in the Waymo Challenge?\n  * Our interpretation is that our model can achieve good performance in the distance error metrics (FDE, ADE), but is worse in the mAP score than SOTA methods. Consequently, we view our model's performance as competitive, rather than SOTA. Please note that achieving SOTA performance is not the focus of our work. We have rather shown performances for reference since we also do not investigate ensembling, which is required to achieve a mAP of 0.4764 (6 models with 128 proposals each, vs. 1 model with 6 proposals in our approach)."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9187/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700650901856,
                "cdate": 1700650901856,
                "tmdate": 1700651159733,
                "mdate": 1700651159733,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SR8LFdRD3M",
                "forum": "72MSbSZtHv",
                "replyto": "sWoHdCIpDd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9187/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9187/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow up on issue 2: Implicit evaluation labels in pre-training data"
                    },
                    "comment": {
                        "value": "As this issue is related to the core contribution of our work, we want to further resolve it:\n> The fact that the waymo maps are crops of locations where scenarios occurred is implicitly using the label information.\n\nThe training and validation split are from different drives at different locations. We only pre-train (100%) and fine-tune (20%) using the training split, while our evaluation in Sec. 4.1/ Table 1 is done on the validation split. Therefore, our models are not pre-trained with environment data or any kind of implicit label information from the validation split.\nIn contrast to related methods, we even exclude past trajectory data from our pre-training and only consider the current state of surrounding agents (see Sec. 4.1 or the caption of Table 1). Since we believe that self-supervised learning should not contain data that is closely related to the downstream labels."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9187/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733022699,
                "cdate": 1700733022699,
                "tmdate": 1700739762636,
                "mdate": 1700739762636,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]