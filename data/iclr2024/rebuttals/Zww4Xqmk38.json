[
    {
        "title": "Tree-based Ensemble Learning for Out-of-distribution Detection"
    },
    {
        "review": {
            "id": "EFhgbuNE6A",
            "forum": "Zww4Xqmk38",
            "replyto": "Zww4Xqmk38",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6038/Reviewer_A8Ja"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6038/Reviewer_A8Ja"
            ],
            "content": {
                "summary": {
                    "value": "The paper focuses on a method called TOOD detection, which aims to improve out-of-distribution (OOD) detection in tree-based machine learning models. The paper evaluates this method on various types of data, including tabular, image, and text data. It also compares TOOD detection with existing state-of-the-art OOD detection methods and claims to show favorable or comparable performance. The paper includes mathematical validation to support its methodology and presents preliminary results that indicate the effectiveness of TOOD detection."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is comprehensive and covers several types of data, making it widely applicable.\nIt provides rigorous theoretical results to mathematically validate its model.\nThe paper compares its method to existing diverse techniques, providing a benchmark for its effectiveness.\nPreliminary results are promising, indicating the potential impact of the research.\nThe paper addresses the issue of OOD detection based on the new approach, tree-based algorithms, which is a significant problem in machine learning."
                },
                "weaknesses": {
                    "value": "It only compares their models with kernel-based baselines for efficiency analysis. More comprehensive comparisons are necessary.\n\nThe \"Comparison with State-of-the-Arts\" section does not provide the comparison between TOOD and the recent models. \n\nAs shown in Figure 7, the proposed model may not be effective in high-dimensional cases."
                },
                "questions": {
                    "value": "Please see the Weakness and Strengths sections."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6038/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698467337818,
            "cdate": 1698467337818,
            "tmdate": 1699636649586,
            "mdate": 1699636649586,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BzR8x9YiyD",
                "forum": "Zww4Xqmk38",
                "replyto": "EFhgbuNE6A",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6038/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6038/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "We want to thank the reviewer for pointing out the strength and weakness of our paper ! Here are a few responses from us.\n\nThe reviewer mentioned the high dimensionality issue, and we absolutely agree. In principle, because of the curse of dimensionality, we believe so far there is no universal way to solve it.  However, one way to alleviate it is to use autoencoder to extract latent features and use latent features as input for our method, and we have applied the autoencoder in our implementation for image and text data tasks. This was commented in Remark 2 in the paper.\n\nFor the literature, we thought we have compared with most of other recent OOD detection methods to the best our knowledge, would the reviewer mind mentioning specifically about which model(s) we are missing ?  Thank you for the time again !"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6038/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700236176250,
                "cdate": 1700236176250,
                "tmdate": 1700236176250,
                "mdate": 1700236176250,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qkEadNTgaD",
            "forum": "Zww4Xqmk38",
            "replyto": "Zww4Xqmk38",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6038/Reviewer_jBcD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6038/Reviewer_jBcD"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a tree-based ensemble learning approach for out-of-distribution (OOD) detection. The method involves calculating the Hamming distance for the tree embeddings obtained from a random forest trained on in-distribution data. The authors offer a comprehensive theoretical analysis of this method to substantiate their proposal. Additionally, empirical experiments are conducted on synthetic datasets and benchmark datasets to validate its effectiveness."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The strengths of this paper can be summarized as follows:\n\n1. **Experimental Results:** The experiments demonstrated the proposal's strong performance across various synthetic and benchmark datasets.\n\n2. **Clarity and Presentation:** The paper is meticulously structured, and the ideas presented are easily comprehensible, ensuring accessibility for readers."
                },
                "weaknesses": {
                    "value": "While this paper exhibits several strengths, it also presents several weaknesses, which are outlined as follows:\n\n1. **Limitation 1**: The idea that \"out-of-distribution data may exhibit smaller Hamming distances among themselves\" hinges on the assumption that the support of training and testing distributions does not overlap in each dimension. However, this assumption raises doubts as it prohibits anomalies from occurring in only one dimension.\n\n2. **Limitation 2**: The central idea appears to implicitly assume that labels are distributed uniformly across different classes. Consider a binary classification scenario where the major class has a significantly higher probability than the minor class, and the labels are determined by whether $x_{i} < s_{i}$. In such cases, the Hamming distance of the embedding of in-distribution test data may be small among them, primarily because most of the samples reside in the same leaf as the major class.\n\n3. **Regarding Experiments**: It is advisable to present the experimental results in the format \"mean \u00b1 std\" due to the inherent randomness of random forests.\n\n4. **Regarding Related Work**: Decision tree learning and random forests should be traced back to the works of Quinlan (1979), Breiman et al. (1984), and Breiman (2001).\n\nReferences:\n- Quinlan JR (1979) \"Discovering Rules by Induction from Large Collections of Examples.\" Expert Systems in the Micro Electronics Age.\n- Breiman L, Friedman JH, Olshen RA, et al (1984) \"Classification and Regression Trees.\" Chapman & Hall/CRC.\n- Breiman L (2001) \"Random Forests.\" Machine Learning, 45(1), 5\u201332."
                },
                "questions": {
                    "value": "See Weaknesses #1 and #2."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6038/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698548263203,
            "cdate": 1698548263203,
            "tmdate": 1699636649481,
            "mdate": 1699636649481,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6LUz7nkDqW",
                "forum": "Zww4Xqmk38",
                "replyto": "qkEadNTgaD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6038/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6038/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for the suggestions to improve our paper ! Here are a few responses from us.\n\nThe reviewer mentioned Limitation 1,  but I\u2019m not sure if I understand the issue. We assumed in Theorem 1 that the support of training and testing distributions does not overlap in each dimension, which means along all the dimensions, the two distributions do not overlap. We have rewritten the statement in Theorem 1 and hopefully it can address the confusion.\n\nThe reviewer also mentioned that the label distribution could be an issue for the tree-based. This is absolutely a great point. From our point of view, this issue can be potentially solved by manually redistributed the labels. For example, if the major class has 100 samples and the minor class has 10 samples, then we can randomly relabel such that there are 55 samples for each class.  By doing this, we will no longer have the issue that the hamming distance will be small. This idea is similar as label shuffling which we have discussed in the experiments section. \n\nWe highly appreciate the reviewer for pointing out the misuse of the reference for decision tree and random forest, it is our carelessness for not making it correct in the first place, we have adapted these changes in the revised version of our paper. We also added in the standard deviation for the experimental results in the appendix."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6038/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700235790938,
                "cdate": 1700235790938,
                "tmdate": 1700235790938,
                "mdate": 1700235790938,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mghGxV7sMQ",
                "forum": "Zww4Xqmk38",
                "replyto": "6LUz7nkDqW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6038/Reviewer_jBcD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6038/Reviewer_jBcD"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors' Rebuttal and Further Clarification"
                    },
                    "comment": {
                        "value": "I appreciate your detailed response to my comments on your paper. I would like to further elaborate on my concern regarding Limitation 1 to ensure clarity.\n\nIn Theorem 1, you assume that the support of training and testing distributions does not overlap in each dimension. To illustrate the concern, let's consider a scenario where the support of in-distribution data is defined as $[0, 1]^2$ with negative samples distributed in $[0, 0.5) \\times [0, 0.5) \\cup [0.5, 1] \\times [0.5, 1]$ and positive samples distributed in $[0, 0.5) \\times [0.5, 1] \\cup [0.5, 1] \\times [0, 0.5)$. The out-distribution data is distributed across four regions: $[-\\infty, -100] \\times [0, 1]$, $[0, 1] \\times [100, \\infty]$, $[0, 1] \\times [-\\infty, -100]$, and $[100, \\infty] \\times [0, 1]$ (It is reasonable to classify these four regions as out-distribution data).\n\nIn this case, the proposed TOOD can fail as out-distribution data may exhibit larger Hamming distances among themselves. The constraint imposed by Theorem 1 prohibits the occurrence of anomalies in only one dimension, which may not fully align with real-world scenarios where anomalies could manifest in specific dimensions.\n\nI believe it is reasonable to consider anomalies in only some dimensions, and the current formulation of Theorem 1 may be overly restrictive in this regard. Consequently, I maintain my assessment of Limitation 1."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6038/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700291675920,
                "cdate": 1700291675920,
                "tmdate": 1700291675920,
                "mdate": 1700291675920,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "woM7JihQaB",
            "forum": "Zww4Xqmk38",
            "replyto": "Zww4Xqmk38",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6038/Reviewer_5AJZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6038/Reviewer_5AJZ"
            ],
            "content": {
                "summary": {
                    "value": "This study presents a novel scoring function for mismatch detection. For each test input, the position of its terminal node in a set of classification trees is recorded. Then the authors use the hamming distance between two location vectors to quantify the similarity of two sample points."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The proposed detection method is novel and interesting.\n- There is diversity in the experimental setup, considering OOD detection on multiple data types.\n- Theoretical analysis is provided."
                },
                "weaknesses": {
                    "value": "- This method is not valid for high-dimensional inputs.\n- There are no experiments on ImageNet benchmark.\n- The results of the theoretical analysis are for a single classification tree model, not for a random forest."
                },
                "questions": {
                    "value": "1. For image classification, does the input $x$ refer to an image or a feature vector obtained from a pre-trained feature extractor? \n2. Do the hyper-parameters used in training the tree models (such as tree depth, the number of terminal nodes, and the minimal size of terminal nodes) have any effect on OOD Detection results?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6038/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698768118318,
            "cdate": 1698768118318,
            "tmdate": 1699636649377,
            "mdate": 1699636649377,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bEpK5re9PB",
                "forum": "Zww4Xqmk38",
                "replyto": "woM7JihQaB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6038/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6038/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "We want to thank reviewer for the positive comments ! Here are a few responses from us.\n\nWe certainly agree that the high-dimensional input issue is a big challenge for our method, as well as for other learning methods. In principle, because of the curse of dimensionality, we believe so far there is no universal way to solve it.  However, one way to alleviate it is to use autoencoder to extract latent features and use latent features as input for our method, and we have applied the autoencoder in our implementation for image and text data tasks. This was commented in Remark 2 in the paper.\n\nThe reviewer also mentioned that the theoretical analysis are for a single classification tree model, not for a random forest. This is another great point. We have added a theorem (Theorem 4 in the updated version) to address this. So far we haven\u2019t perform the experiments on ImageNet benchmark, and we would like to add in more experiments in the final version of our paper.\n\nFor input x in image classification, it refers to the latent feature vector obtained from an autoencoder.  From our experience when conducting the experiments, the hyperparameters used in training the tree models do not have much effect on the OOD detection results (as long as the parameters not too extreme). We have added these comments in the appendix of the updated version of our paper."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6038/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700234671792,
                "cdate": 1700234671792,
                "tmdate": 1700234671792,
                "mdate": 1700234671792,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Zc5skR5YME",
            "forum": "Zww4Xqmk38",
            "replyto": "Zww4Xqmk38",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6038/Reviewer_FZKS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6038/Reviewer_FZKS"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses the fundamental question of determining whether testing samples have a similar distribution to training samples, which is crucial for the safe deployment of machine learning models. The authors propose a mechanism called TOOD detection, which is a simple and effective tree-based method for detecting out-of-distribution (TOOD) samples. The TOOD detection mechanism works by computing the pairwise hamming distance of tree embeddings of the testing samples. These embeddings are obtained by fitting a tree-based ensemble model using in-distribution training samples. The authors highlight that their approach is interpretable and robust due to its tree-based nature. Additionally, the method is efficient, flexible across various machine learning tasks, and can be applied to unsupervised settings. The paper presents extensive experiments to demonstrate the superiority of the proposed method compared to other state-of-the-art out-of-distribution detection methods. The experiments cover tabular, image, and text data, showing the effectiveness of the approach in distinguishing between in-distribution and out-of-distribution samples."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Novel approach: The paper introduces a new mechanism, TOOD detection, which offers a novel perspective on addressing the problem of determining whether testing samples have a similar distribution to training samples by a tree based ensemble method. From my personal knowledge, tree structures and ensemble methods are seldomly studied in OOD detection, making the considered direction an interesting line of works. \n\n\nEffective methodology: The proposed TOOD detection mechanism based on computing pairwise hamming distances of tree embeddings proves to be simple yet effective in distinguishing in-distribution from out-of-distribution samples. The approach demonstrates superior performance compared to other state-of-the-art methods in extensive experiments across various data types.\n\nInterpretable and robust: The authors highlight the interpretability and robustness of their approach, attributed to its tree-based nature. This characteristic allows for better understanding and trust in the detection process, making it easier to analyze and interpret the results.\n\nFlexibility across machine learning tasks: The paper emphasizes the flexibility of the proposed approach, indicating that it can be applied to various machine learning tasks. This versatility makes it applicable to a wide range of scenarios, adding practical value to the research.\n\nGeneralizability to unsupervised setting: The authors state that their method can be easily generalized to unsupervised settings, which is beneficial in scenarios where labeled data is scarce or unavailable. This adaptability enhances the applicability of the proposed approach."
                },
                "weaknesses": {
                    "value": "The authors define OOD in the abstract, but such a definition may violate the main stream of the community. In my view, telling the difference between two distributions is more related to two sample test. While in OOD Detection,  we typically assume the ID and OOD distribution has been mixed, thus we need to tell data as ID and OOD cases instance/point wise. I think such a setting is more difficult than two sample test, making OOD detection remain a challenging task in the literature. It will be great if the authors can discuss about it. \n\nThe paper is not clearly written. I am not sure if the proposed tree based method uses original features in the input space or embedding feature given by the pretrained classifier. If the former is true, I am not sure if the tree based methods have enough capability to fit complex classification tasks such as CIFAR classification. Also, the computational complexity will be high (even built upon the high dimensional embedding features). If the latter is true, I am not sure if the learned embedding features are good enough in OOD detection, especially considering the reliance of strong assumptions in their theoretical analysis (see also in the below questions).\n\nA related question is about the strong assumption in Theorem 1. In the input space, especially for the complex image classification task, it is obviously not true. In the embedding space, since model cannot perfectly separate ID and OOD cases, it is still a strong assumption. Therefore, I cannot fully understand why the tree based method is superior over previous works such as distance based methods (KNN), MSP, Energy, among many others. \n\nWhy ensemble method can facilitate OOD detection, the theoretical analysis does not cover such an issue, meanwhile heuristic explanation and empirical evaluation  are not sufficient. Therefore, I think the authors should discuss more about why ensembling is critical for the suggested tree based methods. \n\nThe authors make another strong assumption that the calibration failures, which is the main cause of why DNNs fail in OOD detection, will not occur for tree based methods. I am not sure if it is true in the real world, and more evaluation and ablation should be provided. \n\nMore discussion about the hyper parameter setting and the choice of evaluation datasets should be discussed here. More experiments about hard OOD detection and wild OOD detection are also of the interest in the literature."
                },
                "questions": {
                    "value": "Please see the Weaknesses above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6038/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698815393880,
            "cdate": 1698815393880,
            "tmdate": 1699636649265,
            "mdate": 1699636649265,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ILWziAFO1q",
                "forum": "Zww4Xqmk38",
                "replyto": "Zc5skR5YME",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6038/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6038/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for very detailed comments ! Here are a few responses from us.\n\nThe reviewer mentioned the instance/point-wise test. This is a great point and we actually did such test as well when we were writing the paper. One way to perform instance-based test by using tree-based method is that we can use each instance as the center of some Gaussian distribution, with the variance similar to the original dataset, to generate a set of fake samples. In this way, we will have many samples, then we can apply the same method as described in this paper. We choose to exclude the instance based test mainly because that the other algorithms we compare with do not use instance-based test. We have added another result (Theorem 4 in the updated version) to address that the instance-based test can be done by using our method.\n\nFor the issue of whether the tree-based method uses original features or embedded features as input, we mentioned at the beginning of Section 5 in the paper: \u201cFor simulated and tabular data, we directly use the original training samples as input for the tree-based model. For image or text data, we impose an autoencoder or word embedding to extract latent features and use latent features as input for our tree-based model.\u201d Intuitively, we should directly use the original features if the dataset is simple, and use embedded features if the dataset is complicated. In terms of the computational cost, if we use algorithm such as random forest, then the cost will be high as each node splitting has to be optimized. However, in our implementation, we use the Extremely Randomized Tree (ExtraTree), whose node splitting is completely random, and it can also achieve the same performance as random forest.\n\nFor Theorem 1, we agree it has a very strong assumption. However, this is just the our first theorem to address the ideal case. We addressed the general case (when there are overlaps between training and testing dataset) as Theorem 3 in the paper. \n\nThe reviewer mentioned about why ensemble method can facilitate OOD detection. This is another great point. We have added a theorem (Theorem 4 in the updated version) to address this.\n\nFor the calibration failures, I\u2019m not sure what is it referring to exactly, would you mind giving some more explanations here ?  For the hyperparameters setting and evaluation datasets, they are discussed in the appendix."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6038/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700234113575,
                "cdate": 1700234113575,
                "tmdate": 1700234113575,
                "mdate": 1700234113575,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]