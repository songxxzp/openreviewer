[
    {
        "title": "CellPLM: Pre-training of Cell Language Model Beyond Single Cells"
    },
    {
        "review": {
            "id": "shM82zk7X4",
            "forum": "BKXvPDekud",
            "replyto": "BKXvPDekud",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3984/Reviewer_3Haz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3984/Reviewer_3Haz"
            ],
            "content": {
                "summary": {
                    "value": "A novel pretrained single-cell transcriptomic model CellPLM is proposed. It takes cells as tokens and tissues as sentences as opposed to existing counterparts which usually takes genes as tokens and cells as sentences. By leveraging spatially-resolved transcriptomic data in pre-training, cell-cell relationships can be learned. Empirical studies demonstrate CellPLM outperforms prior arts in diverse downstream tasks and has higher efficiency in inference."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "++ The study adapts the large language model pretraining techniques to the single-cell transcriptomic data, but uses a novel analogy to better capture the cell-cell relationships. \n\n++ The paper is well organized and easy to understand. Technical details are given clearly.\n\n++ The work provides a new direction for training single-cell foundation models and demonstrates its advantages against existing works. It will make a significant contribution to the community."
                },
                "weaknesses": {
                    "value": "-- The motivation and benefit of using a VAE-like architecture is not clear.\n\n-- Two paradigms of modeling single-cell data should both have their pros and cons but the authors do not discuss them thoroughly.\n\n-- The benefit of using Gaussian mixture distribution as the prior of latent embedding is not validated.\n\n-- It is not clear which parts (e.g., the combination of two types of single-cell data, the VAE-like architecture or the novel paradigm of single-cell data modeling) contribute the most to the performance gains against other existing foundation models (e.g., scGPT, scBert, etc.)."
                },
                "questions": {
                    "value": "1. Based on different modeling paradigms of scRNA-seq data, the proposed CellPLM may have its pros and cons compared with existing foundation models (e.g., scBert, scGPT, etc.) using genes as tokens. In which downstream tasks CellPLM may be/not be a better choice?\n2. Is the batch embedding learnable? If not, how are they defined?\n3. What's the motivation and benefit of using the VAE-like architecture for CellPLM when the goal is not to learn a generative model? Is there a problem if a typical transformer architecture plus a head (without Gaussian prior latent embedding) is employed?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3984/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697620206373,
            "cdate": 1697620206373,
            "tmdate": 1699636359999,
            "mdate": 1699636359999,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EpGnH50E4X",
                "forum": "BKXvPDekud",
                "replyto": "shM82zk7X4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3984/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3984/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer 3Haz"
                    },
                    "comment": {
                        "value": "We sincerely appreciate your recognition of this work. In the following content, we will address your comments and suggestions.\n\n> W4 It is not clear which parts (e.g., the combination of two types of single-cell data, the VAE-like architecture or the novel paradigm of single-cell data modeling) contribute the most to the performance gains against other existing foundation models (e.g., scGPT, scBert, etc.).\n\nClarifying the individual contributions of different aspects of our model to its performance is essential. To verify the contribution of each component in CellPLM model, we have now added three new ablation studies on two representative tasks to examine the effectiveness of proposed latent distribution and transformer encoder. In each setting, we change one component in the model architecture and go through the whole pre-train and fine-tune pipeline to get the downstream performance. The results are presented in Table 1 below.\n\n- First, when we replace the proposed mixture of gaussian prior distribution with a gaussian prior distribution (noted as \u201cw/o Mixture of Gaussian\u201d, commonly used in previous methods like scVI), the performance significantly drops on all datasets, indicating that an unsuitable prior distribution can greatly hurt the performance. A regular Gaussian distribution cannot accommodate the highly heterogeneous data present in the pre-train dataset, which were collected from different people, organs, and sequencing platforms.\n- Second, we removed the latent distribution in its entirety, noted as \u201cw/o latent distribution\u201d, i.e., we converted from a VAE-like probabilistic generative model to a deterministic masked auto-encoder. The performance consistently falls between the original one and the first ablation. On one hand, this supports our motivation of using probabilistic models with Gaussian mixture prior distribution. The latent distribution helps model the uncertainty of the data and address the high noise inherent in transcriptomic data. This results in a robust cell representation. On the other hand, the selection of prior distribution is very important because an improper prior (e.g., regular Gaussian) can be even worse than no latent distribution.\n- Lastly, we replace the transformer encoder with an MLP encoder, keeping the same number of layers and hidden dimension (the total parameters reduce from 85M to 50M). The performance significantly drops on spatial imputation task, while the gap is relatively small on cell-type classification task. This aligns with our intuition, as spatial transcriptomic data provide spatial location information, enabling the model to better identify and utilize the relationships between cells. In contrast, the cell type annotation dataset does not provide spatial location information, which makes the benefits gained from the transformer encoder more limited.\n\nOverall, through a series of ablation studies, we have verified that our CellPLM model can capture the relationships between cells via the transformer encoder and enhance the performance of downstream tasks, generating more robust and useful cell representations through appropriate prior distributions.\n\n||Cell-type Classification||||Spatial Imputation||||\n|---|---|---|---|---|---|---|---|---|\n||MS||hPancreas||Lung||Liver||\n||Macro F1|Macro Precision|Macro F1|Macro Precision|corr|cosine|corr|cosine|\n|CellPLM|0.766$\\pm$0.007|0.803$\\pm$0.008|0.749$\\pm$0.010|0.753$\\pm$0.010|0.318$\\pm$0.015|0.481$\\pm$0.011|0.328$\\pm$0.011|0.481$\\pm$0.010|\n|w/o Mixture of Gaussian|0.737$\\pm$0.042|0.766$\\pm$0.069|0.711$\\pm$0.025|0.701$\\pm$0.025|0.258$\\pm$0.011|0.449$\\pm$0.005|0.232$\\pm$0.013|0.433$\\pm$0.008|\n|w/o Latent Distribution|0.750$\\pm$0.024|0.809$\\pm$0.032|0.733$\\pm$0.034|0.731$\\pm$0.033|0.262$\\pm$0.011|0.449$\\pm$0.008|0.246$\\pm$0.017|0.428$\\pm$0.012|\n|w/o Transformer Encoder|0.750$\\pm$0.050|0.794$\\pm$0.074|0.751$\\pm$0.010|0.750$\\pm$0.012|0.244$\\pm$0.016|0.443$\\pm$0.008|0.250$\\pm$0.032|0.440$\\pm$0.021|\n---\nTable 1. Ablation studies of latent distribution and transformer encoder on cell-type classification and spatial imputation task.\n\n> W1. The motivation and benefit of using a VAE-like architecture is not clear.\n\nWe appreciate your inquiry regarding the use of a VAE-like architecture. The choice was motivated by the need for a robust framework capable of capturing the complex, high-dimensional and noisy nature of single-cell data. The VAE architecture offers a powerful way to learn meaningful latent representations, modeling the uncertainty from the measurement, enabling the model to effectively capture underlying biological variability and noise inherent in single-cell data. As shown in the ablation studies in Table 1 above, a proper latent distribution results in more robust cell representation and better downstream performance. Due to the same reason, VAE has been a well adopted architecture in recent single-cell analysis research on various tasks[1,2,3,4,5]."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3984/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700260441818,
                "cdate": 1700260441818,
                "tmdate": 1700260441818,
                "mdate": 1700260441818,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "B3EOVq25xV",
                "forum": "BKXvPDekud",
                "replyto": "shM82zk7X4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3984/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3984/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer 3Haz #2"
                    },
                    "comment": {
                        "value": "> W3. The benefit of using Gaussian mixture distribution as the prior of latent embedding is not validated.\n\nYou concern is well-taken. As verified in the ablation studies above, the Gaussian mixture prior distribution can help model the uncertainty of the data and address the high noise inherent in transcriptomic data. It outperforms a regular Gaussian due to its capacity of handling heterogenous pre-train data. This eventually results in more robust cell representation and better downstream performance.\n\nW2. Two paradigms of modeling single-cell data should both have their pros and cons but the authors do not discuss them thoroughly.\n\n> Q1. Based on different modeling paradigms of scRNA-seq data, the proposed CellPLM may have its pros and cons compared with existing foundation models (e.g., scBert, scGPT, etc.) using genes as tokens. In which downstream tasks CellPLM may be/not be a better choice?\n\nCellPLM, with its unique approach to modeling scRNA-seq and SRT data, excels in tasks that require deep understanding of cellular heterogeneity and cell-cell relationships, such as in cell type annotation, imputation and spatial transcriptomic tasks. It also achieves impressive inference speed when generating cell embeddings. However, for tasks that explicitly requires gene-level output, CellPLM might not be capable. For example, CellPLM is not available for gene regulatory network inference, while gene-level pretrained models (e.g., scBert, scGPT, etc.) can do. Although CellPLM can implicitly capture gene interactions through gene expression embedder and encoders (as illustrated in gene embedding visualization in Appendix H.2 and gene perturbation prediction in Appendix F.3), it\u2019s hard to query the contextualized gene representations from the model.\n\n> Q2. Is the batch embedding learnable? If not, how are they defined?\n\nYes, it is learnable. The batch token is an important input to our decoder, which aids in batch effect removal from the latent space. The batch embeddings are randomly initialized for each tissue sample in the pre-train datasets. When transferring to new datasets in downstream tasks, we initialize the embedding of new batches with the average of all pre-train batches. The batch embeddings are then fine-tuned on the downstream datasets. \n\nNote that the cell type annotation and gene perturbation heads do not condition on batch tokens, and zero-shot clustering only relies on the encoder, thus the batch tokens are not relevant in these downstream tasks.\n\n> Q3. What's the motivation and benefit of using the VAE-like architecture for CellPLM when the goal is not to learn a generative model? Is there a problem if a typical transformer architecture plus a head (without Gaussian prior latent embedding) is employed?\n\nThe use of a VAE-like architecture, despite not aiming for a generative model, brings significant advantages. It allows for a more nuanced understanding of the data's underlying distribution, particularly useful in capturing the stochastic nature of gene expression in single cells. A typical transformer architecture, while effective, may not fully capture this complexity. The inclusion of a Gaussian prior latent embedding adds an additional layer of depth in modeling the inherent variability and noise in single-cell data, which is now verified in the ablation studies in Table 1 above.\n\n---\n[1] Lopez, Romain, et al. \"Deep generative modeling for single-cell transcriptomics.\"\u00a0*Nature methods*\u00a015.12 (2018): 1053-1058.\n\n[2] Lopez, Romain, et al. \"A joint model of unpaired data from scRNA-seq and spatial transcriptomics for imputing missing gene expression measurements.\"\u00a0*arXiv preprint arXiv:1905.02269*\u00a0(2019).\n\n[3] Gr\u00f8nbech, et al. \"scVAE: variational auto-encoders for single-cell gene expression data.\" Bioinformatics (2020)\n\n[4] Xu, et al. \"Graph embedding and Gaussian mixture variational autoencoder network for end-to-end analysis of single-cell RNA sequencing data.\" Cell Reports methods (2023).\n\n[5] Tu, Xinming, et al. \"Cross-linked unified embedding for cross-modality representation learning.\"\u00a0*Advances in Neural Information Processing Systems*\u00a035 (2022): 15942-15955."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3984/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700260548665,
                "cdate": 1700260548665,
                "tmdate": 1700262830685,
                "mdate": 1700262830685,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "UECF2hyPkl",
            "forum": "BKXvPDekud",
            "replyto": "BKXvPDekud",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3984/Reviewer_C7dG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3984/Reviewer_C7dG"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a pre-trained language model CellPLM, the first single-cell pretrained language model that utilizes cell-cell relations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This paper is well written. Provide enough introductions for people without much background in this field.\n- The proposed method is sound. Taking cell-cell relationships into the modeling is a well-motivated idea."
                },
                "weaknesses": {
                    "value": "I integrate the questions and weaknesses in this section since I have very little bio background and all the weaknesses I identified are based on my own understanding of this area, which could be incorrect.\n\n- Where do the huge speed improvements of CellPLM come from? It is because CellPLM aggregates the genes within each cell directly in the embedding layer, so it only needs to process dramatically shorter input sequences. So basically, CellPLM works at the cell level, while baselines work at the gene level. If this is true, the Claim \"with 500x times higher inference speed compared to existing pre-trained models.\" is unfair since you are compared to a task that your model is specifically designed for. At least it should be reduced to \"with 500x times higher inference speed when generating cell embeddings\".\n\n- Why does the CellPLM model from scratch outperform most of the pre-trained baselines? This seems to be really unrealistic to me. Is it because your model is much larger than the baselines or cell-level LM is more suitable for the tasks? If the randomly initialized model already outperforms most of the baselines, then we need to re-evaluate the claims in the paper, since the performance boost may mainly result from the Transformer architecture instead of the specific model designs you mentioned.\n\n- There is a lack of ablation study to show the effectiveness of each proposed component."
                },
                "questions": {
                    "value": "Please see the Weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3984/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3984/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3984/Reviewer_C7dG"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3984/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698819882498,
            "cdate": 1698819882498,
            "tmdate": 1700524754645,
            "mdate": 1700524754645,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9R8WAoAfuW",
                "forum": "BKXvPDekud",
                "replyto": "UECF2hyPkl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3984/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3984/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer C7dG"
                    },
                    "comment": {
                        "value": "We sincerely appreciate your comments on this work. In the following content, we will try to address your comments and suggestions.\n\n> W1. Where do the huge speed improvements of CellPLM come from? It is because CellPLM aggregates the genes within each cell directly in the embedding layer, so it only needs to process dramatically shorter input sequences. So basically, CellPLM works at the cell level, while baselines work at the gene level. If this is true, the Claim \"with 500x times higher inference speed compared to existing pre-trained models.\" is unfair since you are compared to a task that your model is specifically designed for. At least it should be reduced to \"with 500x times higher inference speed when generating cell embeddings\".\n\nThank you for your great suggestion. We agree that the original claim is not rigid enough. We now change it to the suggested expression in the revised paper, i.e., \u201cwith 500x times higher inference speed on generating cell embeddings compared to existing pre-trained models\u201d. Meanwhile, we want to emphasize that the cell embeddings are essential for various downstream tasks in single-cell analysis. For example, all the downstream tasks (clustering, denoising, cell-type annotation, imputation, gene perturbation prediction) presented in the paper rely on high-quality cell representations. Therefore, the increased inference speed in generating cell embeddings should still be considered as a highlight of CellPLM model.\n\n> W2. Why does the CellPLM model from scratch outperform most of the pre-trained baselines? This seems to be really unrealistic to me. Is it because your model is much larger than the baselines or cell-level LM is more suitable for the tasks? If the randomly initialized model already outperforms most of the baselines, then we need to re-evaluate the claims in the paper, since the performance boost may mainly result from the Transformer architecture instead of the specific model designs you mentioned.\n\nWe appreciate your question about the performance improvement. First, we want to clarify that CellPLM does not always outperform baselines when training from scratch. For example, in spatial imputation task (both datasets) and cell type annotation task (hPancreas dataset), it cannot outperform baselines. \n\nTo further illustrate how CellPLM benefits from each component, we add an ablation study on the from-scratch version on MS dataset of cell type annotation task, presented in Table 1 below. We find that both the bi-level masking module and the transformer encoder positively contribute to the from-scratch performance. When removing these modules, CellPLM performance is closer to the non-pretrain baselines (e.g., CellTypist). Since CellPLM is the first method that applies bi-level masking and cell-level transformer encoders to these tasks, this can be considered as our contribution. Lastly, the gap between pre-trained and from-scratch is still considerably larger than the gap between from-scratch versions. This sufficiently demonstrates the benefits brought by pre-training and transfer, proving the effectiveness of the pre-training framework we proposed and laying a foundation for future downstream tasks. This is indeed our most essential contribution.\n\n|  | Macro F1 | Macro Precision |\n| --- | --- | --- |\n| CellPLM (pre-trained) | 0.766 $\\pm$ 0.007 | 0.803 $\\pm$ 0.008 |\n| CellPLM (from scratch) | 0.709 $\\pm$ 0.007 | 0.732 $\\pm$ 0.015 |\n|    w/o masks | 0.683 $\\pm$ 0.013 | 0.703 $\\pm$ 0.027 |\n|    w/o transformer | 0.685 $\\pm$ 0.012 | 0.701 $\\pm$ 0.028 |\n| CellTypist | 0.667 $\\pm$ 0.002 | 0.693 $\\pm$ 0.001 |\n\nTable 1. Ablation studies of mask technique and transformer encoder on MS dataset, in training from scratch scenario."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3984/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700260152347,
                "cdate": 1700260152347,
                "tmdate": 1700260152347,
                "mdate": 1700260152347,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "F2wpsaus0z",
                "forum": "BKXvPDekud",
                "replyto": "UECF2hyPkl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3984/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3984/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A kindly reminder"
                    },
                    "comment": {
                        "value": "Dear Reviewer C7dG,\n\nThank you for taking the time to review our work. We appreciate your feedback and we have prepared a thorough response to address your concerns. We believe that we have responded to and addressed all your concerns with our revisions \u2014 in light of this, we hope you consider raising your score. Please let us know in case there are outstanding concerns, and if so, we will be happy to respond.\n\nNotably, given that we are approaching the deadline for the rebuttal phase, we hope we can have the discussion soon. Thanks!"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3984/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700507900911,
                "cdate": 1700507900911,
                "tmdate": 1700507900911,
                "mdate": 1700507900911,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "f2Na2MqZiE",
                "forum": "BKXvPDekud",
                "replyto": "F2wpsaus0z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3984/Reviewer_C7dG"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3984/Reviewer_C7dG"
                ],
                "content": {
                    "title": {
                        "value": "I have raised my score to 6"
                    },
                    "comment": {
                        "value": "Thanks for your response. It solves my concerns and I have raised my score to 6."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3984/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700515059765,
                "cdate": 1700515059765,
                "tmdate": 1700515059765,
                "mdate": 1700515059765,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DpQt7kIsdk",
                "forum": "BKXvPDekud",
                "replyto": "XrOR5CXgCl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3984/Reviewer_C7dG"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3984/Reviewer_C7dG"
                ],
                "content": {
                    "title": {
                        "value": "Score changed"
                    },
                    "comment": {
                        "value": "Thanks for all your responses and for this remind. Now the rating is changed."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3984/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700524801732,
                "cdate": 1700524801732,
                "tmdate": 1700524801732,
                "mdate": 1700524801732,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dfK0ZtklML",
            "forum": "BKXvPDekud",
            "replyto": "BKXvPDekud",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3984/Reviewer_1rfQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3984/Reviewer_1rfQ"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors propose a CellPLM to address certain issues identified in previous work. Specifically, the proposed CellPLM considers cells as tokens and tissues as sentences, whereas previous work treated genes as tokens and cells as sentences. The CellPLM is designed to learn the relationship between cells. Additionally, they adopt a Gaussian mixture prior distribution to overcome the out-of-distribution problem. Experimental results show CellPLM consistently  outperforms previous work."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed method effectively addresses the issues contained in previous work.\n\n2. Experimental results demonstrate that CellPLM consistently surpasses the performance of previous methods."
                },
                "weaknesses": {
                    "value": "1. The experiments in this paper are not sufficient. For instance, ablation studies are needed to verify the effectiveness of each module.\n\n2. Although the improvement in experimental results is very significant, further analysis is needed to determine whether it is due to the relationships between cells.\n\n3. I'm not sure if the Gaussian distribution can achieve the expected effect in this method, this point also needs to be validated through experiments."
                },
                "questions": {
                    "value": "see Weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3984/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3984/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3984/Reviewer_1rfQ"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3984/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698898074267,
            "cdate": 1698898074267,
            "tmdate": 1699636359804,
            "mdate": 1699636359804,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bHYeH4WwDW",
                "forum": "BKXvPDekud",
                "replyto": "dfK0ZtklML",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3984/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3984/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer 1rfQ"
                    },
                    "comment": {
                        "value": "We sincerely appreciate your recognition of this work. In the following content, we will try to address your comments and suggestions.\n\n> W1. The experiments in this paper are not sufficient. For instance, ablation studies are needed to verify the effectiveness of each module.\n\nWe highly appreciate your advice. We now add three new ablation studies on two representative tasks to demonstrate the effectiveness of proposed latent distribution and transformer encoder. In each setting, we change one component in the model architecture and go through the whole pre-train and fine-tune pipeline to get the downstream performance.  The results are presented in Table 1 below.\n\n- First, when we replace the proposed mixture of gaussian prior distribution with a gaussian prior distribution (noted as \u201cw/o Mixture of Gaussian\u201d, commonly used in previous methods like scVI), the performance significantly drops on all datasets, indicating that an unsuitable prior distribution can greatly hurt the performance. A regular Gaussian distribution cannot accommodate the highly heterogeneous data present in the pre-train dataset, which were collected from different people, organs, and sequencing platforms.\n- Second, we removed the latent distribution in its entirety, noted as \u201cw/o latent distribution\u201d, i.e., we converted from a VAE-like probabilistic generative model to a deterministic masked auto-encoder. The performance consistently falls between the original one and the first ablation. On one hand, this supports our motivation of using probabilistic models with Gaussian mixture prior distribution. The latent distribution helps model the uncertainty of the data and address the high noise inherent in transcriptomic data, which results in a robust cell representation. On the other hand, the selection of prior distribution is very important because an improper prior (e.g., regular Gaussian) can be even worse than no latent distribution.\n- Lastly, we replace the transformer encoder with an MLP encoder, keeping the same number of layers and hidden dimension (the total parameters reduce from 85M to 50M). The performance significantly drops on spatial imputation task, while the gap is relatively small on cell-type classification task. This aligns with our intuition, as spatial transcriptomic data provide spatial location information, enabling the model to better identify and utilize the relationships between cells. In contrast, the cell type annotation dataset does not provide spatial location information, which makes the benefits gained from the transformer encoder more limited.\n\nOverall, through a series of ablation studies, we have verified that our CellPLM model can capture the relationships between cells via the transformer encoder and enhance the performance of downstream tasks, generating more robust and useful cell representations through appropriate prior distributions.\n\n||Cell-type Classification||||Spatial Imputation||||\n|---|---|---|---|---|---|---|---|---|\n||MS||hPancreas||Lung||Liver||\n||Macro F1|Macro Precision|Macro F1|Macro Precision|corr|cosine|corr|cosine|\n|CellPLM|0.766$\\pm$0.007|0.803$\\pm$0.008|0.749$\\pm$0.010|0.753$\\pm$0.010|0.318$\\pm$0.015|0.481$\\pm$0.011|0.328$\\pm$0.011|0.481$\\pm$0.010|\n|w/o Mixture of Gaussian|0.737$\\pm$0.042|0.766$\\pm$0.069|0.711$\\pm$0.025|0.701$\\pm$0.025|0.258$\\pm$0.011|0.449$\\pm$0.005|0.232$\\pm$0.013|0.433$\\pm$0.008|\n|w/o Latent Distribution|0.750$\\pm$0.024|0.809$\\pm$0.032|0.733$\\pm$0.034|0.731$\\pm$0.033|0.262$\\pm$0.011|0.449$\\pm$0.008|0.246$\\pm$0.017|0.428$\\pm$0.012|\n|w/o Transformer Encoder|0.750$\\pm$0.050|0.794$\\pm$0.074|0.751$\\pm$0.010|0.750$\\pm$0.012|0.244$\\pm$0.016|0.443$\\pm$0.008|0.250$\\pm$0.032|0.440$\\pm$0.021|\n---\nTable 1. Ablation studies of latent distribution and transformer encoder on cell-type classification and spatial imputation task."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3984/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700259860084,
                "cdate": 1700259860084,
                "tmdate": 1700259860084,
                "mdate": 1700259860084,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TFp9Z40vHi",
                "forum": "BKXvPDekud",
                "replyto": "dfK0ZtklML",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3984/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3984/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A kindly reminder"
                    },
                    "comment": {
                        "value": "Dear Reviewer 1rfQ,\n\nThank you for taking the time to review our work. We appreciate your feedback and we have prepared a thorough response to address your concerns. We believe that we have responded to and addressed all your concerns with our revisions \u2014 in light of this, we hope you consider raising your score. Please let us know in case there are outstanding concerns, and if so, we will be happy to respond.\n\nNotably, given that we are approaching the deadline for the rebuttal phase, we hope we can have the discussion soon. Thanks!"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3984/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700507856859,
                "cdate": 1700507856859,
                "tmdate": 1700507856859,
                "mdate": 1700507856859,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bN0wxvER2p",
            "forum": "BKXvPDekud",
            "replyto": "BKXvPDekud",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3984/Reviewer_QJ4u"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3984/Reviewer_QJ4u"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a language model that learns from single-cell RNA sequencing (scRNA-seq) data. Their model takes as input a series of genes and their expression values, aggregates them, and then feeds them through an embedding matrix to create cell-level embeddings. These embeddings are then fed to a transformer along with other cell embeddings. The model is trained using the variational autoencoder (VAE) objective with a Gaussian prior, which includes a reconstruction term, a conditional prior term, and a cluster prior term. Unlike previous models, the authors leverage spatial data and show that their model outperforms previous models on several tasks.\n\nFurther, they claim that their method is faster than previous methods -- the reason for this is because of collapsing the model input down to cell level representations as opposed to feeding individual gene level tokens"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Overall, the paper is clear and easy to follow, but several important details must be included in the text, such as the prior estimation for $z_i$ and $y_i$, and the dataset details and experimental setup. It is also important to note how the comparisons were standardized.\n\nOther strengths of the paper include:\n\n* It explores spatial + gene expression data using LLMs.\n* It models the task at a cell level as opposed to a gene level (questions in weakness).\n* It explores a smooth latent space for this task."
                },
                "weaknesses": {
                    "value": "The source of the pre-training gains is unclear, given the current presentation. The authors restrict the gene set to a subset of 13,500 genes, but it is not specified which genes are included or whether they are protein-coding or non-protein-coding. The dataset section should be expanded to include details on how the data was preprocessed.\n\n* **Cell masking:** The authors aggregate cell genes using their embeddings and pass these tokens to the LLM. What does it mean to mask a cell? In the diagram, the cells are masked, but in the equations, the genes are masked. Assuming the masking is at the gene level and samples are taken from the same batch, wouldn't other cell genes have the necessary information to impute the cell? In essence, is the model cheating?\n* **Single-cell operation:** How would the model operate if you only have a single-cell sample? Would you feed a single cell token to the LLM and use the latent representation?\n* **Sequence length:** I'm curious how the model performance would change if you vary the \"sequence length\". For example, in your batch, do you have the same number of genes for each cell?\n* **Spatial information:** How is the spatial information leveraged here? Is it through the positional encoding? What would happen if this positional encoding is removed?\n* **Positional Encoding:** It is not clear if this had a significant improvement in the quality of representation learnt.\n* **Comparison with previous models:** Although the authors compare with previous single-cell models, these models only operated on gene expression data and fed a single cell at a time (i.e., the sequence dimension is the genes within the cell as opposed to other cells, as done in this work).\n* **Benefit of pre-training:** The benefit of which part of the pre-training is the result of the improvements shown in the result section is not clear from the current presentation. Also, the authors don't compare with scVI (although they do cite the method) or even a simpler baseline like HVG.\n* **Gene interactions:** Does the model learn gene level interactions? By feeding cell-level representations, how can this be assessed?\n* **Batch Token:** How would you transfer the model to a dataset which has a new batch token are these fine-tuned as well?"
                },
                "questions": {
                    "value": "See weakness section"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3984/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3984/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3984/Reviewer_QJ4u"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3984/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699748767781,
            "cdate": 1699748767781,
            "tmdate": 1699748767781,
            "mdate": 1699748767781,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "AbedH8D413",
                "forum": "BKXvPDekud",
                "replyto": "bN0wxvER2p",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3984/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3984/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer QJ4u"
                    },
                    "comment": {
                        "value": "We sincerely appreciate your recognition of this work. In the following content, we will address your comments and suggestions.\n\n> W1 The source of the pre-training gains is unclear, given the current presentation. The authors restrict the gene set to a subset of 13,500 genes, but it is not specified which genes are included or whether they are protein-coding or non-protein-coding. The dataset section should be expanded to include details on how the data was preprocessed.\n    \nThank you for highlighting the need for clarity in our data processing. We appreciate the opportunity to provide more details. In our study, the subset of 13,500 genes was resulted from the intersection of all scRNA-seq pre-train datasets, based on their prevalence and relevance in the existing literature. Most of these genes are protein-coding genes. A more detailed list of datasets is now included in Appendix E.2.  All data are pre-filtered (i.e., quality control) by the original authors and are collected in AnnData objects. We further perform normalization and log transformation following the convention in Seurat[1]. This important detail is now added to Section 3.2 and Appendix E.2.\n\n> W2 Cell masking:\u00a0The authors aggregate cell genes using their embeddings and pass these tokens to the LLM. What does it mean to mask a cell? In the diagram, the cells are masked, but in the equations, the genes are masked. Assuming the masking is at the gene level and samples are taken from the same batch, wouldn't other cell genes have the necessary information to impute the cell? In essence, is the model cheating?\n    \nYour question about cell masking is insightful. In our approach, cell masking refers to the process of masking at the gene level within each cell, same as the \u201cbi-level masking technique\u201d proposed in [2]. The core idea is to mask most genes (e.g., 75%) in a subset of cells (e.g., 25%), forcing the model to leverage information from other cells to impute the missing information. Therefore, your understanding of \u201cother cell genes have the necessary information to impute the cell\u201d is totally correct.\n    \n**Our method is not deemed \u201ccheating\u201d as it uses the same amount of input as other models and does not use any label information from the test set.**  In fact, leveraging intercellular information is an essential idea of our proposed \u201ccell language modeling\u201d. In addition to utilizing intracellular partially observed gene expression data, our model is also trained to identify and exploit similar cells within the same tissue, cellular communication, and spatial neighborhood composition to impute indeed information. This capability is essential for better denoising and spatial-resolved transcriptomic applications, and previous single-cell pre-trained models are not capable of doing this.\n\n> W3 Single-cell operation:\u00a0How would the model operate if you only have a single-cell sample? Would you feed a single cell token to the LLM and use the latent representation?\n\nFor a single cell, the model indeed operates by feeding a single cell token into the model, utilizing its latent representation (the performance is shown in Table 1 below, in our response to \u201cW4 Sequencing Length\u201d). This process is similar to handling larger samples but adapted to maximize information extraction from a single-cell context. However, our model is more suitable for batch processing of data, which is in line with the characteristics of high-throughput sequencing data.\n\n> W4 Sequence length:\u00a0I'm curious how the model performance would change if you vary the \"sequence length\". For example, in your batch, do you have the same number of genes for each cell?\n\nThe question of varying sequence length is pertinent. In our framework, the number of genes per cell can be variable.  This is managed by the gene expression embedder, which is implemented by a sparse aggregation operation, and thus is flexible to any input gene sets (within the pretrain gene set). \n\nHowever, we want to emphasize that the \u201csequence length\u201d in our transformer model does not refer to the number of genes but the number of cells. We treat cells as tokens and use the transformer model to process a batch of cells. Therefore, the \u201csequence length\u201d in CellPLM is equivalent to \u201cbatch size\u201d in traditional deep learning models. To evaluate the impact of \u201csequence length\u201d (a.k.a, \u201cbatch size\u201d), we separate the input dataset (MS dataset for cell-type annotation task) into smaller batches and run a same fine-tuned cell-type annotation model for inference. The result is presented in the table 1 below. From the experiments, the performance of the model is not sensitive to batch size, but too small a batch size can lead to slight performance degradation. \n\n|Batch Size|32768|2048|128|32|16|8|4|2|1|\n|---|---|---|---|---|---|---|---|---|---|\n|Macro F1| 0.773 | 0.773 | 0.773 | 0.773 | 0.770 | 0.768 | 0.769 | 0.767 | 0.768 |\n---\nTable 1. Parameter analysis of batch size on MS dataset for cell-type annotation task."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3984/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700258818282,
                "cdate": 1700258818282,
                "tmdate": 1700258818282,
                "mdate": 1700258818282,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UGSaAwujEg",
                "forum": "BKXvPDekud",
                "replyto": "bN0wxvER2p",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3984/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3984/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer QJ4u #3"
                    },
                    "comment": {
                        "value": "> W8 Benefit of pre-training:\u00a0The benefit of which part of the pre-training is the result of the improvements shown in the result section is not clear from the current presentation. Also, the authors don't compare with scVI (although they do cite the method) or even a simpler baseline like HVG.\n\nClarifying the specific benefits of pre-training is crucial, and we thank you for emphasizing this. The direct evidence of the benefit of **knowledge transfer** from pre-training is the gap between from-scratch performance and fine-tune performance as we presented in Table 2, 3 and 4. On all tasks and all datasets, pre-train fine-tune outperforms training CellPLM models directly on downstream tasks (a.k.a, \u201cfrom scratch\u201d). \n\nTo further verify the contribution of each **architectural component** in CellPLM model, we have now added three new ablation studies on two representative tasks to examine the effectiveness of proposed latent distribution and transformer encoder. In each setting, we change one component in the model architecture and go through the whole pre-train and fine-tune pipeline to get the downstream performance. The results are presented in Table 3 below and added into Appendix I.\n\n- First, when we replace the proposed mixture of gaussian prior distribution with a gaussian prior distribution (noted as \u201cw/o Mixture of Gaussian\u201d, commonly used in previous methods like scVI), the performance significantly drops on all datasets, indicating that an unsuitable prior distribution can greatly hurt the performance. A regular Gaussian distribution cannot accommodate the highly heterogeneous data present in the pre-train dataset, which were collected from different people, organs, and sequencing platforms.\n- Second, we removed the latent distribution in its entirety, noted as \u201cw/o latent distribution\u201d, i.e., we converted from a VAE-like probabilistic generative model to a deterministic masked auto-encoder. The performance consistently falls between the original one and the first ablation. On one hand, this supports our motivation of using probabilistic models with Gaussian mixture prior distribution. The latent distribution helps model the uncertainty of the data and address the high noise inherent in transcriptomic data, which results in a robust cell representation. On the other hand, the selection of prior distribution is very important because an improper prior (e.g., regular Gaussian) can be even worse than no latent distribution.\n- Lastly, we replace the transformer encoder with an MLP encoder, keeping the same number of layers and hidden dimension (the total parameters reduce from 85M to 50M). The performance significantly drops on spatial imputation task, while the gap is relatively small on cell-type classification task. This aligns with our intuition, as spatial transcriptomic data provide spatial location information, enabling the model to better identify and utilize the relationships between cells. In contrast, the cell type annotation dataset does not provide spatial location information, which makes the benefits gained from the transformer encoder more limited.\n\n||Cell-type Classification||||Spatial Imputation||||\n|---|---|---|---|---|---|---|---|---|\n||MS||hPancreas||Lung||Liver||\n||Macro F1|Macro Precision|Macro F1|Macro Precision|corr|cosine|corr|cosine|\n|CellPLM|0.766$\\pm$0.007|0.803$\\pm$0.008|0.749$\\pm$0.010|0.753$\\pm$0.010|0.318$\\pm$0.015|0.481$\\pm$0.011|0.328$\\pm$0.011|0.481$\\pm$0.010|\n|w/o Mixture of Gaussian|0.737$\\pm$0.042|0.766$\\pm$0.069|0.711$\\pm$0.025|0.701$\\pm$0.025|0.258$\\pm$0.011|0.449$\\pm$0.005|0.232$\\pm$0.013|0.433$\\pm$0.008|\n|w/o Latent Distribution|0.750$\\pm$0.024|0.809$\\pm$0.032|0.733$\\pm$0.034|0.731$\\pm$0.033|0.262$\\pm$0.011|0.449$\\pm$0.008|0.246$\\pm$0.017|0.428$\\pm$0.012|\n|w/o Transformer Encoder|0.750$\\pm$0.050|0.794$\\pm$0.074|0.751$\\pm$0.010|0.750$\\pm$0.012|0.244$\\pm$0.016|0.443$\\pm$0.008|0.250$\\pm$0.032|0.440$\\pm$0.021|\n---\nTable 3. Ablation studies of latent distribution and transformer encoder on cell-type classification and spatial imputation task.\n\nIn addition, the result of **scVI** is now added in the denoising task (Table 2 in Section 4.2) using `get_normalized_expression` function of scVI package and the performance of scVI is very close to DCA. Meanwhile, we want to clarify that the PCA baseline in zero-shot clustering experiments (in Section 4.1) indeed come from **HVG**. As we mentioned in Section 4.1, the PCA results are obtained from 4,500 highly variable genes (HVG). We also include a new clustering result with **scVI** on the same dataset. It achieves ARI=0.843, NMI=0.823, slightly outperforming HVG+PCA baseline but still worse than CellPLM (zero-shot). The visualization is added in the Appendix H.1."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3984/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700259417146,
                "cdate": 1700259417146,
                "tmdate": 1700262709319,
                "mdate": 1700262709319,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZcPo1KVSRb",
                "forum": "BKXvPDekud",
                "replyto": "bN0wxvER2p",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3984/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3984/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer QJ4u #4"
                    },
                    "comment": {
                        "value": "> W9 Gene interactions:\u00a0Does the model learn gene level interactions? By feeding cell-level representations, how can this be assessed?\n    \nThank you for the great question about gene-level interactions. Our model, despite feeding cell-level representations, is designed to capture gene-level interactions through its inherent architecture. The gene functionality is first captured by gene embeddings in the expression embedder module, and then interacted through the feed-forward networks in the transformer encoder, coupled with cell-cell interaction.\n    \nTo evaluate whether the model learns meaningful gene representations, we provide a new visualization of gene embeddings in appendix H.2. From the visualization, the gene embeddings maintain some latent structures. As a case study, we highlight a specific family of genes, HLA genes. There are multiple classes of HLA genes[3]. For example, HLA class I genes (e.g., HLA-A, -B, and -C) present endogenous peptides to responding CD8+ T Cells while the class II (e.g., HLA-DR, -DP, and \u2013DQ) process exogenous peptides for presentation to CD4+ helper T Cells. From the UMAP visualization, HLA gene embeddings present clusters that perfectly match the functionality and characteristics of those genes. To further assess whether the model implicitly encode gene-gene interactions, we conduct experiments of gene perturbation prediction (presented in appendix F.3), which demonstrates that our model is able to identify the knock-out effect of specific genes.\n    \n> W10 Batch Token: How would you transfer the model to a dataset which has a new batch token are these fine-tuned as well?\n\nThe batch token is an important input to our decoder. Currently, when transferring to new datasets, we initialize the embedding of new batches with the average of all pre-train batches. The batch embeddings are then fine-tuned during the transfer process. \n\nNote that the cell type annotation and gene perturbation heads do not condition on batch tokens, and zero-shot clustering only relies on the encoder, thus the batch tokens are not relevant in these tasks.\n\n---\n[1] Stuart, Tim, et al. \"Comprehensive integration of single-cell data.\"\u00a0*Cell*\u00a0177.7 (2019): 1888-1902.\n\n[2] Wen, Hongzhi, et al. \"Single Cells Are Spatial Tokens: Transformers for Spatial Transcriptomic Data Imputation.\"\u00a0*arXiv preprint arXiv:2302.03038*\u00a0(2023).\n\n[3] Cruz-Tapias, P. C. J., and J. M. Anaya. \"Major histocompatibility complex: Antigen processing and presentation 2013.\" (2021)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3984/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700259520355,
                "cdate": 1700259520355,
                "tmdate": 1700262788642,
                "mdate": 1700262788642,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YpZysiNwDU",
                "forum": "BKXvPDekud",
                "replyto": "bN0wxvER2p",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3984/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3984/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A kindly reminder"
                    },
                    "comment": {
                        "value": "Dear Reviewer QJ4u,\n\nThank you for taking the time to review our work. We appreciate your feedback and we have prepared a thorough response to address your concerns. We believe that we have responded to and addressed all your concerns with our revisions \u2014 in light of this, we hope you consider raising your score. Please let us know in case there are outstanding concerns, and if so, we will be happy to respond.\n\nNotably, given that we are approaching the deadline for the rebuttal phase, we hope we can have the discussion soon. Thanks!"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3984/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700507830782,
                "cdate": 1700507830782,
                "tmdate": 1700507830782,
                "mdate": 1700507830782,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cf3WCeQHiX",
                "forum": "BKXvPDekud",
                "replyto": "bN0wxvER2p",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3984/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3984/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A kindly reminder"
                    },
                    "comment": {
                        "value": "We appreciate your comments. We hope that our responses have adequately addressed your concerns. As the open discussion period will **conclude in one day**, we kindly remind you to share any additional concerns you may have. We welcome further discussion and are eager to engage in constructive dialogue."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3984/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700615764499,
                "cdate": 1700615764499,
                "tmdate": 1700615764499,
                "mdate": 1700615764499,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zIOH8CYNrN",
                "forum": "BKXvPDekud",
                "replyto": "bN0wxvER2p",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3984/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3984/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A kindly reminder"
                    },
                    "comment": {
                        "value": "Dear Reviewer QJ4u,\n\nthe discussion period will end in 2 hours. If you have any further concerns, please do not hesitate to take advantage of the last moment to tell us. Thank you!"
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3984/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700732184225,
                "cdate": 1700732184225,
                "tmdate": 1700732184225,
                "mdate": 1700732184225,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]