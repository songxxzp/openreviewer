[
    {
        "title": "Large Language Models as Generalizable Policies for Embodied Tasks"
    },
    {
        "review": {
            "id": "zJ18pVWMrn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4739/Reviewer_92Kq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4739/Reviewer_92Kq"
            ],
            "forum": "u6imHU4Ebu",
            "replyto": "u6imHU4Ebu",
            "content": {
                "summary": {
                    "value": "This paper proposed a new method to adapt LLMs to embodied visual tasks, to leverage the world knowledge of LLM and achieve better generalization on new tasks.\nIn this method, a pre-trained frozen LLM is used to take text instructions as input. A frozen vision encoder is used to encode visual observations. A trainable MLP is used to map the output of visual encoder to the LLM input space. Finally, a few MLPs are used to map LLM output to a policy's action spaces."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The authors proposed a nice way to leverage LLM in the embodied ai tasks with visual inputs. LLM in the design are essentially producing the state representation for the action decoder, given a visual input adaptor. The total number of trainable parameters are constraints so that RL can be efficiently applied.\nEmpirically, the authors showed that the learned policy generalizes well to large diverse tasks. The unseen tasks are divided into two categories: paraphrastic robustness and behavior generalization to evaluate the generalization of the policy. The empirical evaluation method itself has its own value."
                },
                "weaknesses": {
                    "value": "The proposed method uses MLP to map LLM output to discrete actions. Therefore, the actions and LLM outputs are not in the same space. Due to this, I am not sure if the policy can fully leverage the world knowledge from LLM. It would be better if the actions are directly produced by LLM. Of course, in that case, we may need to unfreeze the LLM weights."
                },
                "questions": {
                    "value": "Why not use a VLM backbone but instead use a language-only backbone?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4739/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697438778160,
            "cdate": 1697438778160,
            "tmdate": 1699636456067,
            "mdate": 1699636456067,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "AGoKXLZnBK",
                "forum": "u6imHU4Ebu",
                "replyto": "zJ18pVWMrn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4739/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4739/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the comments and suggestions. We address the reviewer\u2019s points below.\n\n**1. The proposed method uses an MLP to map LLM output to discrete actions. I am not sure if the policy can fully leverage the world knowledge from the LLM.**\n\nModifying LLaRP to operate directly in the language token space rather than through the action decoder MLP is an interesting extension. We note extending LLaRP to implement this is non-trivial. Firstly, a single action in Language Rearrangement consists of at most 4 tokens (for example, \u201cnav left counter\u201d). This LLaRP variant requires predicting the correct sequence of 4 tokens from a vast number of tokens which decode to an invalid action. Secondly, it slows the RL process, because now the policy needs to auto-regressively predict 4 tokens per action prediction step rather than the single prediction needed in LLaRP. Despite these challenges, we agree this is an exciting direction for a follow up work and we updated the conclusion to reflect this (changes in red). \n\nWe also note that we empirically demonstrate the policy is able to use the world knowledge from the LLM since LLaRP generalizes better than LLaRP-Scratch which does not have the pre-trained LLM weights. \n\n**2. Why not use a VLM backbone but instead use a language-only backbone?**\n\nIt is possible to change the network weight initialization in LLaRP with VLM weights instead of using VC-1 and LLaMA as in the paper. However, we started from LLaMA and VC-1 since we found the zero-shot performance of LLaMA was higher than the Flamingo VLM (12% vs. 6% success rate)."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4739/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700111464745,
                "cdate": 1700111464745,
                "tmdate": 1700111464745,
                "mdate": 1700111464745,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "z0spC5iC2s",
                "forum": "u6imHU4Ebu",
                "replyto": "zJ18pVWMrn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4739/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4739/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer 92Kq, we would be grateful if you can comment on whether our response addressed your concerns or if issues remain."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4739/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700546273219,
                "cdate": 1700546273219,
                "tmdate": 1700546273219,
                "mdate": 1700546273219,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xjEgsOP46P",
            "forum": "u6imHU4Ebu",
            "replyto": "u6imHU4Ebu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4739/Reviewer_DZLf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4739/Reviewer_DZLf"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a method for embodied AI for object rearrangement that leverages Vision-Language Models for learning embodied policies. The paper also presents an evaluation benchmark based on a simulated environment (likely Habitat) that significantly extends the number of scenarios and natural language instructions, additionally including novel evaluation axes such as robustness to paraphrasing and robustness to alternative behaviors. The paper reports significant generalization improvements compared to baseline methods and reveals the weaknesses of alternative approaches when dealing with linguistic ambiguities and behavioral differences.\n\n**Update after rebuttal**\n\nAfter the rebuttal, and with taking the additional clarity provided by the authors during the rebuttal into account, I raise my score, since they have clarified several points on the details of their work, and its limitation. Nevertheless, I still believe that the scope of their work can be improved if they provide additional evidence for LLaRP's performance outside their own benchmark, where previous competitive work exists."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- despite existing similar benchmarks such as Habitat rearrangement task and Alfred, the proposed large-scale evaluation benchmark and their perspectives on generalization is useful for comparing embodied agents that can leverage both perception and language instructions, while working in an interactive simulation.\n- extending habitat rearrangement with large number of natural language instructions enables research in this directions to be able to move beyond zero-shot or fewshot in-context agents.\n- the paper compares the proposed approach that leverages finetuning the policy and image-to-language adaptive layers, to zeroshot methods and alternative sequence models, and reports significant improvements in generalisation of the aforementioned axes of evaluation."
                },
                "weaknesses": {
                    "value": "- **incorrect claims and missing related work:** the paper states \"To our knowledge, there are no prior work which demonstrate that the linguistic knowledge in LLMs can be used in online RL problems to improve generalization in embodied AI settings\". There are many examples in the literature that actually have demonstrated that, some of which have been cited (such as ELLM, PALM-E, etc) and many were not discussed (e.g, SayCan, CodeAsPolicy, PercieverActor, HELM, ProgPrompt, EmbodiedGPT,...), though there are many examples of such methods. I recommend correcting the inaccuracies in such statements.\n- **lack of details:** the main paper lacks many details such as what simulation were used, how the agent executes the skills, how the skills are defined, and what is the contribution of the defined PDDL. Although some details are provided in the supplementary material, clarifications are needed to be presented in the main paper.\n- **lack of comparison to existing benchmarks:** The paper does not provide a comprehensive comparison to available benchmarks (some examples includes but not limited to Habitat rearrangement, CortexBench, AI2Thor, Behavior1k, Procthor, ALFRED). Hence, it is not clear how the new provided axes and extending instructions stand against existing work. It is quite common in the literature to dedicate a section and a table to compare various aspects of a newly proposed benchmark to existing ones from various aspects such as scale, generalization aspects, number of samples, kinds of provided data (such as language instructions) etc.\n- **limited comparison to existing work:** although this paper compares to relevant approaches that have proven to be effective such as zeroshot in-context text-only, as well as encoder+LSTM, there are many seminal works that are applicable to this environment and it would bring more value to comparisons if the paper actually leverages some pre-existing methods. Examples of such methods include CodeAsPolicy (Code Gen), PercieverActor (BC), and SemanticHELM (pretrained vision and LLM+LSTM). Atari is a well-established benchmark, which has many competitive baselines, which has not been provided for comparison. Providing additional evidence on other embodied AI benchmarks where previous established baselines exists provides better comparison datapoints to compare the proposed method to the existing work."
                },
                "questions": {
                    "value": "- extend related work and discuss relation to existing work that has been detailed in the weaknesses section.\n- provide details that have been denoted in the weaknesses section.\n- provide comparison to existing benchmarks that has been pointed out in the weaknesses section.\n- extend empirical evaluations and comparison to existing work that has been described in the weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4739/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4739/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4739/Reviewer_DZLf"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4739/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698637616961,
            "cdate": 1698637616961,
            "tmdate": 1700680090479,
            "mdate": 1700680090479,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2byFaAA1r3",
                "forum": "u6imHU4Ebu",
                "replyto": "xjEgsOP46P",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4739/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4739/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the comments and suggestions. We address the reviewer\u2019s points below.\n\n**1. Incorrect claims and missing related work: the paper states \"To our knowledge, there are no prior work which demonstrate that the linguistic knowledge in LLMs can be used in online RL problems to improve generalization in embodied AI settings\". There are many examples in the literature that actually have demonstrated that.**\n\nWe corrected this claim to \u201cTo our knowledge, there is no prior work which demonstrates that LLMs can be used as vision-language policies in online RL problems to improve generalization\u201d (changes in paper in red). This updated claim is accurate and the mentioned works do not demonstrate this. We apologize for this: this was an error that played out in the final editing of our first submitted draft (please refer to the general response). We compare to the works mentioned by the reviewer in detail below: \n\n- SayCan, CodeAsPolicy and ProgPrompt are zero-shot applications of LLMs that require describing the environment in text [1-3]. LLaRP adapts the LLM for _visual_ decision-making with _online RL_. By online RL, we mean learning from interactions with the environment. \n- PercieverActor, EmbodiedGPT and PaLM-E [4-6] learn from static expert collected datasets with supervised learning. LLaRP doesn\u2019t need expert data and instead learns with RL.\n- ELLM uses LLMs as a _reward to guide_ non-LLM policies from _text-based state descriptions_ [7]. We use the LLM directly as the decision-making policy from _visual observations_. \n- HELM uses LLMs (CLIP) as a \u201cmemory mechanism\u201d, which we see as orthogonal from our contribution of using LLMs directly for policy decision-making [8]. Unlike LLaRP, HELM does not modify the LLM, use it for better generalization, or use it directly for decision-making.\n\n**2. Lack of task details.**\n\nWe expanded Supp. A.4 to describe the simulation in detail. We also added a description of the key simulation details in a new paragraph in the main paper (Sec 4.1). Due to limited space, we described how the agent executes the skill and skill definitions in Supp. A.3 and the PDDL definition in Supp. A.1. \n\nWe never claim the defined PDDL is a contribution, it is only used in the implementation of the Language Rearrangement task (which is a contribution)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4739/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700111376261,
                "cdate": 1700111376261,
                "tmdate": 1700111376261,
                "mdate": 1700111376261,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oof1teYfcx",
                "forum": "u6imHU4Ebu",
                "replyto": "xjEgsOP46P",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4739/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4739/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**3. Lack of comparison to existing benchmarks.**\n\nThank you for the suggestion, we include a table highlighting the differences between Language Rearrangement and prior benchmarks below without citations and in full in Table 4, supplementary A.7.\n\nLanguage Rearrangement has more instructions than prior benchmarks. It has almost 18x more instructions than ALFRED (151k vs 8k) and 8x more instruction templates than CALVIN (282 vs. 35). \u201c# instructions\u201d refers to the number of linguistically distinct instruction types, meaning \u201cpick the apple\u201d and \u201cpick the pear\u201d are counted as two separate instructions. \u201c# Instruction Templates\u201d refers to distinct instructions regardless of specific entities, so the same picking example is only counted as a single instruction. Also unique about Language Rearrangement is that it has dense rewards for language specified tasks. All other tasks with language instructions only define sparse success-based rewards. Since Language Rearrangement is implemented with Habitat 2.0, it also inherits the same fast simulation speeds.\n\n| Benchmark                                         | # Instructions (Tasks)            | # Instruction Templates   | Observation Type | Reward Type     | Generalization Type                                                                              | Sim Speed steps/second | # Scenes | # Objects |\n|---------------------------------------------------|----------------------------|---------------------------|------------------|-----------------|--------------------------------------------------------------------------------------------------|------------------------|----------|-----------|\n| **Language Rearrangement (Ours)**                 | 151,000                    | 282                       | Visual           | Dense Reward    | Unseen Instructions [10 datasets from Tab.1], Unseen Scenes                  | 1400                   | 105      | 82        |\n| ALFRED                  | 8,055                      | 7                         | Visual           | Sparse Success  | Random Split                                                                                     | NA                     | 120      | 84        |\n| CALVIN                      | 400                        | 35                        | Visual           | Sparse Success  | Unseen Instructions                                                                              | NA                     | 1        | 1         |\n| ARNOLD                      | 32                         | 8                         | Visual           | Sparse Success  | Random Split                                                                                     | 200-400                | 20       | 40        |\n| CLIPort (Ravens) | 10 (+ Procedural)         | 10                        | Visual           | Sparse Success  | Unseen Objects [colors, shapes, types]                                                           | NA                     | 1        | 56        |\n| BabyAI    | Procedural                 | Procedural        | Text             | Sparse Success  | Unseen Instructions [compositions, objects, synonyms, dialects]                                  | 3000                   | NA       | 4         |\n| Habitat Rearrangement | NA                        | NA                        | Visual           | Dense Reward    | Unseen Scenes                                                                                     | 1400                   | 105      | 20        |\n| Behavior1k                  | NA                        | NA                        | Visual           | Sparse Success  | NA                                                                                               | 60                     | 50       | 5215      |\n| TDW                   | NA                        | NA                        | Visual           | Sparse Success  | Unseen Scenes                                                                                     | 15                     | 5-168    | 112       |\n| ProcTHOR                        | NA                        | NA                        | Visual           | Dense Reward    | Unseen Scenes                                                                                     | 90-180                 | 10k      | 118       |"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4739/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700111425427,
                "cdate": 1700111425427,
                "tmdate": 1700111425427,
                "mdate": 1700111425427,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "suNq4ffq4y",
                "forum": "u6imHU4Ebu",
                "replyto": "xjEgsOP46P",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4739/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4739/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**4. Limited comparison to existing work.**\n\nWe thank the reviewer for suggestions around additional baselines, but believe they are not suited for our problem setting.\n\n- CodeAsPolicies [2] relies on perception modules which Language Rearrangement doesn't have. This is also similar to our ZS-ChatGPT baseline which replans based on environment feedback.\n- PercieverActor [4] requires offline data for imitation learning and depth observations for constructing point clouds. Language Rearrangement assumes access to neither. \n- While Semantic HELM [8] can be combined with LLaRP and baselines to increase learning efficiency and interpretability it does not address how to generalize to unseen instructions. \n\nOur experiments compare LLaRP to three RL and three zero-shot baselines. We compare three different LLM/VLMs (LLaMA, ChatGPT, and Flamingo). \n\n**Citations**\n\n[1] Ahn, Michael, et al. \"Do as i can, not as i say: Grounding language in robotic affordances.\" arXiv preprint arXiv:2204.01691 (2022).\n\n[2] Liang, Jacky, et al. \"Code as policies: Language model programs for embodied control.\" 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2023.\n\n[3] Singh, Ishika, et al. \"Progprompt: Generating situated robot task plans using large language models.\" 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2023.\n\n[4] Shridhar, Mohit, Lucas Manuelli, and Dieter Fox. \"Perceiver-actor: A multi-task transformer for robotic manipulation.\" Conference on Robot Learning. PMLR, 2023.\n\n[5] Mu, Yao, et al. \"Embodiedgpt: Vision-language pre-training via embodied chain of thought.\" arXiv preprint arXiv:2305.15021(2023).\n\n[6] Driess, Danny, et al. \"Palm-e: An embodied multimodal language model.\" arXiv preprint arXiv:2303.03378 (2023).\n\n[7] Du, Yuqing, et al. \"Guiding pretraining in reinforcement learning with large language models.\" arXiv preprint arXiv:2302.06692 (2023).\n\n[8] Paischer, Fabian, et al. \"Semantic HELM: A Human-Readable Memory for Reinforcement Learning.\" Thirty-seventh Conference on Neural Information Processing Systems. 2023."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4739/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700111445437,
                "cdate": 1700111445437,
                "tmdate": 1700111445437,
                "mdate": 1700111445437,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4saORs7DQl",
                "forum": "u6imHU4Ebu",
                "replyto": "xjEgsOP46P",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4739/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4739/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer DZLf, we would be grateful if you can comment on whether our response addressed your concerns or if issues remain."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4739/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700546257552,
                "cdate": 1700546257552,
                "tmdate": 1700546257552,
                "mdate": 1700546257552,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gk4F64jRt9",
                "forum": "u6imHU4Ebu",
                "replyto": "xjEgsOP46P",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4739/Reviewer_DZLf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4739/Reviewer_DZLf"
                ],
                "content": {
                    "title": {
                        "value": "Re:"
                    },
                    "comment": {
                        "value": "Dear authors,\n\nthank you for your response.\n\n- **compare to existing benchmarks**: Table4 is indeed very insightful, but does not answer the question why LLaRP is not additionally evaluated on existing benchmarks such as BabyAI to show how they compare to existing methods that have already shown results on BabyAI. This adds more evidence for generality of the proposed approach, which currently is lacking. \n\n- **compare to existing methods**: while I appreciate the authors detailed the differences between the proposed method and the discussed existing work, I would still be interested to see how other more advanced existing methods compare to LLaRP, even though they may differ in some aspects. \n\nminor:\n\n- **claims:** the updated claim reflects the contributions more accurately. Though, some previous work could still qualify for the given definition of \"LLMs used as vision-language policies in online RL problems to improve generalization\". \n\n- **lack of details**: the additional details clarify some of the missing information, thank you for extending the paper. I recommend to also extent the clarifications with the differences between the current benchmark and existing ones on executing low-level policies or kinetically updating with skill post conditions, in which the latter introduces less challenges. \n\n- I recommend adding the discussions about the related work and existing benchmarks to the paper."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4739/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700604280448,
                "cdate": 1700604280448,
                "tmdate": 1700604280448,
                "mdate": 1700604280448,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ae5LTRTYj5",
                "forum": "u6imHU4Ebu",
                "replyto": "gk4F64jRt9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4739/Reviewer_DZLf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4739/Reviewer_DZLf"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response"
                    },
                    "comment": {
                        "value": "After the rebuttal, and with taking the additional clarity provided by the authors during the rebuttal into account, I **raise my score**, since they have clarified several points on the details of their work, and its limitation. Nevertheless, I still believe that the scope of their work can be improved if they provide additional evidence for LLaRP's performance outside their own benchmark, where previous competitive work exists."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4739/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700679952599,
                "cdate": 1700679952599,
                "tmdate": 1700679952599,
                "mdate": 1700679952599,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "gSqUIRICjk",
            "forum": "u6imHU4Ebu",
            "replyto": "u6imHU4Ebu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4739/Reviewer_Daer"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4739/Reviewer_Daer"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces an approach that adapts pre-trained LLMs to embodied visual tasks, by leveraging the world knowledge encoded in LLMs to enhance training efficiency. The paper also introduces a new benchmark, Language Rearrangement, with 150,000 training and 1,000 testing tasks for studying language-conditioned, which contains a diverse set of language-conditioned rearrangement tasks, such as complex manipulation, navigation, and exploration tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- In the context of contemporary works, such as emdodiedGTP, Eureka, RoboCat, Open X-Embodiment, etc., this work demonstrates that the LLM-based LLaRP model exhibits strong generalization capabilities. It can handle complex paraphrasing of task instructions and generalize to new tasks.\n- LLaRP shows faster convergence during training compared to other baselines, indicating its sample efficiency. Scaling up the size of the underlying LLM (from 7B to 13B parameters) leads to better results, suggesting that larger LLMs enhance embodied reasoning capabilities.\n- The paper provides comparisons with zero-shot baselines, showing LLaRP outperforms models that rely solely on language understanding without training. It also shows that LLaRP trained with reinforcement learning (RL) outperforms LLaRP trained with imitation learning (IL), highlighting the effectiveness of RL in this context."
                },
                "weaknesses": {
                    "value": "- What distinguishes this work from [1,2, 3, 4], which also appear to emphasize the evaluation of LLMs' generalization abilities in embodied settings? \n- In the current way results are presented, it is very difficult to understand the differences in model capacity across baselines and LLaRP. Please consider including a direct comparison of model capacity in Fig. 3, as well as exact numbers in the bar plots (it seems that there is sufficient space to do so given the white space surrounding the bar plots). Same for Figures 5, 7, and so on.\n- The paper presents successful results but does not thoroughly explore or discuss failure cases. It would be great to have more qualitative examples that enable readers to understand the limitations of employing pretrained LLMs on embodied tasks.\n- It would be valuable to gain insights into the performance of simpler baselines, such as embedding-based models like embCLIP, or baselines from the embodied rearrangement literature such as [5], in comparison to the proposed approach. Currently, all existing baselines rely on pretrained Language Models (LLMs), with varying prompts and inputs. While this dependence on LLMs is shown to yield strong results, it would be good to understand the necessity and efficiency of LLMs for the tasks at hand. In other words, it is difficult to ground the work in existing embodied literature, since it introduces a new task (language rearrangement) and a new model (LLM-based LLaRP model trained with online RL). \n\n[1] Li, Shuang, Xavier Puig, Chris Paxton, Yilun Du, Clinton Wang, Linxi Fan, Tao Chen et. al. \"Pre-trained language models for interactive decision-making.\" Advances in Neural Information Processing Systems 35 (2022): 31199-31212.\n\n[2] Carta, Thomas, Cl\u00e9ment Romac, Thomas Wolf, Sylvain Lamprier, Olivier Sigaud, and Pierre-Yves Oudeyer. \"Grounding large language models in interactive environments with online reinforcement learning.\" ICML (2023).\n\n[3] Xiang, Jiannan, Tianhua Tao, Yi Gu, Tianmin Shu, Zirui Wang, Zichao Yang, and Zhiting Hu. \"Language Models Meet World Models: Embodied Experiences Enhance Language Models.\" NeurIPS (2023).\n\n[4] Huang, Wenlong, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. \"Language models as zero-shot planners: Extracting actionable knowledge for embodied agents.\" In International Conference on Machine Learning, pp. 9118-9147. PMLR, 2022.\n\n[5] Wijmans, Erik, Irfan Essa, and Dhruv Batra. \"VER: Scaling On-Policy RL Leads to the Emergence of Navigation in Embodied Rearrangement.\" Advances in Neural Information Processing Systems 35 (2022): 7727-7740."
                },
                "questions": {
                    "value": "- How are the current baselines chosen, e.g., what would be the reason for not comparing with an LSTM-Flamingo baseline or using other VLMs such as InstructBLIP, LLaVA, miniGPT?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4739/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4739/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4739/Reviewer_Daer"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4739/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698811461074,
            "cdate": 1698811461074,
            "tmdate": 1700676262472,
            "mdate": 1700676262472,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vBxgcUivAk",
                "forum": "u6imHU4Ebu",
                "replyto": "gSqUIRICjk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4739/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4739/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the comments and suggestions. We address the reviewer\u2019s points below.\n\n**1. What distinguishes this work from [1,2,3,4], which also appear to emphasize the evaluation of LLMs' generalization abilities in embodied settings?**\n\nWhat distinguishes our work from [1,2,3,4] is that our main contribution is to demonstrate that an LLM can be adapted with _online RL_ to serve as a generalizable _vision language_ (VLM) policy for embodied AI tasks. We apologize for the confusion here. We updated the writing in sections 1 and 2 to better reflect this (changes in red) and refer to our general response for more details. \n\nWe appreciate the opportunity to talk about this main contribution w.r.t. the prior works mentioned. [1,3] primarily focus on finetuning a _text only_ LLM with supervised learning in _text-based_ observation and action spaces. LLaRP adapts an LLM with _online RL_ for _visual, instruction_ tasks directly in the environment action space. [2] also uses RL, but does so in text-based environments. We adapt LLMs to visual tasks and environment action spaces. [4] along with related works [6,7] are \"zero-shot policies for interactive decision-making tasks, without task specific training, in settings where the states and action spaces are both text-based\" (Sec. 2). \n\n**2. More qualitative examples for understanding limitations of employing pretrained LLMs on embodied tasks.**\n\nThank you for the suggestion. We included failure analysis in supplementary section D to support the success qualitative examples in figure 6 and section D. \n\n**3. Valuable to gain insights into the performance of simpler baselines.**\n\nThe LLaRP-Scratch baseline is a simple baseline that does not rely on LLMs (Sec. 5.1). LLaRP-Scratch shows the necessity of LLMs due to its lesser generalization performance (17% vs 42% for LLaRP in Table 2) and efficiency (it takes 500M vs. 50M steps to converge, Fig. 4a).\n\nIn this rebuttal, we added a comparison to using EmbCLIP [8] in LLaRP. As with [8] we used the pre-trained CLIP-ViT-L as the visual encoder for the policy. This performed slightly worse than LLaRP, achieving 40% total success rate, vs. 42% success rate for LLaRP. VC-1 [9] also reported better performance than EmbCLIP for other Habitat tasks. We also note that [5] is a method solely for improving training wall clock time, while it could help scale all our experiments, by itself it will not improve generalization performance. \n\n**4. How are the current baselines chosen, why not compare with other VLMs?**\n\nWe chose Flan for the LSTM-Flan baseline because Flan is a powerful encoder model that can embed instructions into fixed length vectors, which the LSTM then takes as input. Using a decoder-based model instead, such as Flamingo, is better suited for text generation, not encoding instructions. We show this with the LSTM-LLaMA model performing significantly worse than the LSTM-Flan model (Table 2). \n\nWe used IDEFICS since, at the time of submission, it was the state-of-the-art VLM model. \n\n**Citations**\n\n[1] Li, Shuang, et al. \"Pre-trained language models for interactive decision-making.\" Advances in Neural Information Processing Systems 35 (2022): 31199-31212.\n\n[2] Carta, Thomas, et al. \"Grounding large language models in interactive environments with online reinforcement learning.\" arXiv preprint arXiv:2302.02662 (2023).\n\n[3] Xiang, Jiannan, et al. \"Language Models Meet World Models: Embodied Experiences Enhance Language Models.\" arXiv preprint arXiv:2305.10626 (2023).\n\n[4] Huang, Wenlong, et al. \"Language models as zero-shot planners: Extracting actionable knowledge for embodied agents.\" International Conference on Machine Learning. PMLR, 2022.\n\n[5] Wijmans, Erik, Irfan Essa, and Dhruv Batra. \"VER: Scaling On-Policy RL Leads to the Emergence of Navigation in Embodied Rearrangement.\" Advances in Neural Information Processing Systems 35 (2022): 7727-7740.\n\n[6] Huang, Wenlong, et al. \"Grounded decoding: Guiding text generation with grounded models for robot control.\" arXiv preprint arXiv:2303.00855 (2023).\n\n[7] Liang, Jacky, et al. \"Code as policies: Language model programs for embodied control.\" 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2023.\n\n[8] Khandelwal, Apoorv, et al. \"Simple but effective: Clip embeddings for embodied ai.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.\n\n[9] Majumdar, Arjun, et al. \"Where are we in the search for an Artificial Visual Cortex for Embodied Intelligence?.\" arXiv preprint arXiv:2303.18240 (2023)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4739/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700111221774,
                "cdate": 1700111221774,
                "tmdate": 1700111221774,
                "mdate": 1700111221774,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DOqlXqINF3",
                "forum": "u6imHU4Ebu",
                "replyto": "gSqUIRICjk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4739/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4739/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer Daer, we would be grateful if you can comment on whether our response addressed your concerns or if issues remain."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4739/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700546225091,
                "cdate": 1700546225091,
                "tmdate": 1700546225091,
                "mdate": 1700546225091,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ApcBucCYCs",
                "forum": "u6imHU4Ebu",
                "replyto": "DOqlXqINF3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4739/Reviewer_Daer"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4739/Reviewer_Daer"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the rebuttal"
                    },
                    "comment": {
                        "value": "Thank you to the authors for providing a comprehensive and well-structured rebuttal. Given the clarifications, I have decided to raise my score. However, I would like to note that despite the improvements, the technical contribution of combining LLMs with online RL, while simple and straightforward, seems to require collecting a lot of online experience (Figure 4). The raised score reflects the positive impact of the authors' responses on clarity and novelty but acknowledges the reservations regarding the technical contribution and computational efficiency (which could be improved by future work)."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4739/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700676237193,
                "cdate": 1700676237193,
                "tmdate": 1700676237193,
                "mdate": 1700676237193,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "tonG85Ji6w",
            "forum": "u6imHU4Ebu",
            "replyto": "u6imHU4Ebu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4739/Reviewer_vC8Q"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4739/Reviewer_vC8Q"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces Large Language model Reinforcement Learning Policy, which adapts a pre-trained LLM to take text instruction and egocentric observations as inputs, and output actions. Fixing the body of LLM, adapters are trained using the standard reinforcement learning algorithm. The paper is also provided a new benchmark called language rearrangement, consisting of 150,000 training sets. They empirically show the benefits of the proposed method, comparing with several simple baselines including zero-shot LLM, zero-shot VLM, and LSTM policy in conjunction with T5 and LLaMA."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "(1) The paper is well written and easy to follow. \n\n(2) The method is simple and straightforward extension of prior studies but seems to provide solid performance gain."
                },
                "weaknesses": {
                    "value": "(1) Technical novelty is not high. Using VLMs to control a task is not new, e.g., PaLM-E and RT-2 as discussed in this paper. It is true these two method does not employ reinforcement learning, but architectural or technical difference is slight and is not well discussed in the paper. \n\n(2) While the one of the main claim or implication of the paper might be linguistic knowledge in LLMs can be used in online RL, but the statement itself is already validated in prior studies, e.g. [1]. While it is true that [1] focus on the textual environment, but there are no discussion how this paper extends the prior understanding on what kind of knowledge is encoded in the LLM and what is not. \n\n[1] Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning\n\n(3) The paper lacks in depth analysis of the failure case, which might be important to dig in the internal knowledge of LLM."
                },
                "questions": {
                    "value": "See Weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4739/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698854095905,
            "cdate": 1698854095905,
            "tmdate": 1699636455803,
            "mdate": 1699636455803,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SaNSl3JF20",
                "forum": "u6imHU4Ebu",
                "replyto": "tonG85Ji6w",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4739/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4739/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the comments and suggestions. We address the reviewer\u2019s points below.\n\n**1. Technical novelty is not high. Using VLMs to control a task is not new.**\n\nWe disagree and believe our work is novel in training VLMs _with reinforcement learning_ (RL) to control a task. While other works have trained VLMs from LLMs in offline settings, no other works have trained VLMs with online RL (i.e., without expert demonstrations). RT-2 and PaLM-E are _offline_: they are given 130,000 human teleoperated demonstrations collected over almost 2 years as training data [2,3]. LLaRP only has access to interaction data collected from its current policy (it uses PPO, which is on-policy), and we demonstrate that, by adapting LLMs in this way, LLaRP achieves generalizable decision-making.\n\nThe architectural differences from online LLaRP and prior offline VLM decision-making models are rooted in the relative differences between online vs offline learning. RT-2 and PaLM-E use expert demonstrations, so their training is similar to supervised learning: they take a single image as input along with the task specification, and fine-tune the LLM to generate the text-based action instructions from the expert demonstrations. LLaRP does not have access to this expert data, and so trains using an online RL algorithm. Exploration is a critical capability for online learning, so in order to help in this regard, LLaRP takes as input all observations from the current episode (up to 32). Since LLaRP does not have access to expert actions (e.g., as text or otherwise), we add an action decoder module and train it with RL using the reward signal from the environment. \n\nFinally, beyond technical novelty, a useful contribution of our paper is the empirical analysis of the generalization capabilities of using an LLM as a VLM policy trained with online RL, as we explain in the next response. \n\n**2. While the one of the main claim or implication of the paper might be linguistic knowledge in LLMs can be used in online RL, but the statement itself is already validated in prior studies**\n\nThis was a writing error on our part since our main claim is about adapting LLMs for _vision language_ policies with online RL. We mistakenly omitted that the novelty is specific to vision-based policies with online RL in the related work section. The reviewer is correct that [1] shows LLMs for online RL in textual environments. We updated the related work writing to address this and the connection to [1] (changes in red). Thank you for bringing this to our attention. \n\nWe apologize for the confusion, but the corrections are minimal and do not change our main contribution of adapting LLMs for VLM policies with online RL. [1] also uses LLMs in online RL, but focuses on environments with text-inputs and text-outputs, thus this method does not apply to the visual decision-making in our setting. The LLaRP architecture is also different from [1] since we train an observation encoder and action decoder module, which are additional components to operate from visual observations and general action spaces. \n\nThe empirical analysis in Sec. 5 also contains novel insights about LLMs for embodied AI, which itself is a contribution. We benchmark a wider variety of baselines than [1]. We include zero-shot baselines with powerful LLMs (ZS-ChatGPT, ZS-Flamingo, ZS-LLaMA), while [1] only compares to Flan-T5. We train policies for over 100x more experience (200M vs 1.5M steps). We show generalization to 1,000 new instructions across 10 distinct splits. [1] only shows generalization to new objects, compositions, synonyms, and translation (French vs. English). Unlike [1] we also show the efficiency of fine tuning to new downstream tasks (Fig 4b).\n\n**3. The paper lacks in depth analysis of the failure case.**\n\nThank you for the suggestion. We included failure analysis in supplementary section D to support the success qualitative examples in figure 6 and section D. \n\n**Citations**\n\n[1] Carta, Thomas, et al. \"Grounding large language models in interactive environments with online reinforcement learning.\" arXiv preprint arXiv:2302.02662 (2023).\n\n[2] Brohan, Anthony, et al. \"Rt-2: Vision-language-action models transfer web knowledge to robotic control.\" arXiv preprint arXiv:2307.15818 (2023).\n\n[3] Driess, Danny, et al. \"Palm-e: An embodied multimodal language model.\" arXiv preprint arXiv:2303.03378 (2023)."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4739/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700111188737,
                "cdate": 1700111188737,
                "tmdate": 1700111188737,
                "mdate": 1700111188737,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "T8UhaEULey",
                "forum": "u6imHU4Ebu",
                "replyto": "tonG85Ji6w",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4739/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4739/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer vC8Q, we would be grateful if you can comment on whether our response addressed your concerns or if issues remain."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4739/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700546184294,
                "cdate": 1700546184294,
                "tmdate": 1700546184294,
                "mdate": 1700546184294,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]