[
    {
        "title": "Periodic Set Transformer: Material Property Prediction from Continuous Isometry Invariants"
    },
    {
        "review": {
            "id": "z2g2EqTxtD",
            "forum": "MNwXif6AWA",
            "replyto": "MNwXif6AWA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6701/Reviewer_yW4J"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6701/Reviewer_yW4J"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes Pointwise Distance Distribution (PDD), a continuous isometry invariant for periodic point sets for the representation learning of crystals. It develops a transformer model, Periodic Set Transformer (PST), with a modified attention mechanism that integrates composition information and structural encoding for accurate crystal property prediction. By defining the crystal in terms of a periodic set, the representation of crystals encodes the periodicity of crystals and becomes continuous under perturbations, bridging the gap between crystal descriptors and machine learning models. As a result, the transformer model PST equipped with modified self-attention and PDD-weighted readout has the potential to make accurate predictions for lattice energies. Furthermore, the authors extend PST for crystal property predictions, outperforming graph-based or transformer-based models on some tasks, given the extensive experimental results on Matbench. The evidence from ablation studies further proves the effectiveness of the combination of compositional and structural embeddings for a better understanding of the chemical space of crystals."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Originality: The paper proposes PDD for the representation of periodic lattice and overcomes the discontinuity of traditional graph representations. Therefore, the paper uniquely contributes to the field by exploring the reasonable representations for periodic sets which can help machine learning models fully learn the geometry of the space.\n\nQuality: The paper carefully explains the core concepts like PDD with detailed derivation. Besides, the extensive experimental results and visualizations provide convincing evidence to support the statement in the paper.\n\nClarity: The paper effectively communicates its ideas and findings with clarity. The paper is well-written, and the logic is coherent. \n\nSignificance: The paper focuses on improving the embeddings for crystals so that the transformer model equipped with the adapted self-attention mechanism could be leveraged for crystal property predictions. The experimental results in the manuscript show the potential of PST model to outperform the widely used graph-based models for crystal property predictions. The model could be further improved by pertaining, making it a promising candidate in crystal property prediction and crystal structure optimization."
                },
                "weaknesses": {
                    "value": "1. Although the authors' presentation is quite clear in general, the details of the experiments provided in the paper are not enough, especially why the experiments are designed in this way, what are the datasets and targets, and what the difference across experiments is and how they collaborate to support the statements in the manuscript.\n\n2. The description in the section Prediction of Lattice Energy is quite vague. For example, the authors do not specify what the datasets (e.g. T2, P1, S2) are, how they are obtained, and what kinds of data entries are included in them. Otherwise, it's hard to figure out why the experimental results here are significant. The author might consider revising this section so that the logic is more transparent to readers."
                },
                "questions": {
                    "value": "1. In terms of the explanation of isometry on page 2, I'm wondering why the isometry has the form $f(S)=Q$ and $g(Q)=S$. From my understanding, isometry means $d_S(a,b) = d_Q(f(a),f(b)), a,b \\in Q$, and I can't tell that this is equivalent to the explanation in the manuscript. \n\n2. Earth Mover's Distance is mentioned in the Introduction part, but I do not see detailed descriptions about how to use it for crystal representation in the manuscript. Besides, have the authors considered comparing with ElMD [1], which has also introduced Earth Mover\u2019s Distance as metrics for chemical similarity and inorganic compound embeddings?\n\n3. In the second experiment of prediction of lattice energy, if I'm understanding it correctly, the datasets consist of crystals with different compositions while the compositional information is not included. Then how does the model make predictions for two similar lattices with different compositions? And even if the model can outperform the baseline on this task, I'm afraid it cannot demonstrate that the model is applicable to practical usage.\n\n4. Why is the PST model evaluated on the training set for the first two tasks of lattice energy prediction? And what is the reason for supplementing P2M data to training data to reduce error? From the results in Table 1 & 2, I cannot be persuaded of the effectiveness of PST.\n\n5. Could you clarify how the contribution is calculated in the ablation study? And I think the errors here are sufficient to demonstrate the impact of compositions and PDD.\n\n[1] Hargreaves, C. J., et. al., The earth mover\u2019s distance as a metric for the space of inorganic compositions. Chemistry of Materials, 32(24), 10610-10620, 2020."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6701/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6701/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6701/Reviewer_yW4J"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6701/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697993053798,
            "cdate": 1697993053798,
            "tmdate": 1699636769433,
            "mdate": 1699636769433,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HyxPSmau5B",
                "forum": "MNwXif6AWA",
                "replyto": "z2g2EqTxtD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6701/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6701/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer yW4J"
                    },
                    "comment": {
                        "value": "Thank you for the feedback, it has allowed us to address several issues in the manuscript.\n\n> the details of the experiments provided in the paper are not enough..\n\nThe intention of the two experiments is to illustrate the flexibility of the model and the effectiveness of using invariants outside of their usual context of crystal comparison and instead in representations for ML. \n    \nIn the first case, the datasets used in section 4 and section 5 are significantly different. Section 4 uses molecular crystals with each set (e.g. T2) containing the same composition but changes in structure while the MP crystals are mostly inorganic crystals with varied compositions. \n\nIn the second case, the PDD and PST are able to perform comparatively to graph-based models that contain additional structural information. The graph-based models also use $k$-NN distances but through the use of edges, they can specify between which atoms a distance corresponds to. The PDD does not have this information, it only uses the distances themselves. Despite this, the PST still outperforms CGCNN (the most comparable model in terms of information used) in 5/6 properties. The use of edges is discontinuous under atomic perturbations. If a small change in atomic positioning occurs, the set of $k$-NN can change. In a graph-based model, this means edges can shift, changing the graph's topology in a way that is disproportionate to a small change in atomic position. In the case of the PDD, when this occurs the distances will be changed continuously because there is no reference to the neighboring atoms.\n\n> The description in the section Prediction of Lattice Energy is quite vague..\n\nThe primary purpose of the PDD is to distinguish two crystals via the EMD. Its efficacy in this can be seen in the MDS plots (Figure 4a) contained in the appendices. The question this experiment seeks to answer is whether or not this means it is also an effective representation for crystals in an ML context. We will add further context to the revision.\n\n>  I'm wondering why the isometry has the form..\n\nIn the context of the explanation $f: Q \\rightarrow S$ and $g : S \\rightarrow Q$ are bijective isometries and should the condition $f(Q) = S$ and $g(S) = Q$ be met, then the two periodic point sets $S$ and $Q$ are isometric. This latter part could be generalized to $S$ and $Q$ being metric spaces in $\\mathbb{R}^n$. In our explanation, we don't directly define an isometry and instead mention it is a distance-preserving mapping between metric spaces; this will be adjusted in the revision. \n\n> Earth Mover's Distance is mentioned in the Introduction part..\n\nThe Earth Mover's Distance establishes a continuous metric on periodic sets and is used to measure differences in PDDs. It is not used to create features or utilized in the model. It is used in plots 4a and 4b in the appendix to show how distances between structures can be a useful indicator for differences in their property values.\n\n> how does the model make predictions for two similar lattices with different compositions? \n\nIt is true the compositional information is not included (in section 4). The experiment shows that the structural differences produced from having different compositions are reflected in the distances in the PDD and this is enough to make accurate predictions. This is already shown to be the case when distances are measured via the EMD and then projected onto 2D space (Figure 4a of the appendix). Whether this is true when the PDD is input to an ML algorithm is what this experiment aims to address.\n\n> Why is the PST model evaluated on the training set for the first two tasks of lattice energy prediction?\n\nThe table was trying to indicate what crystals make up the training and test sets. The reported error is for the test set. The first task follows the same setup as Ropers et al. The second extends this to multiple crystal sets. \n\n> And what is the reason for supplementing P2M data to training data to reduce error?\n\nWe tried to show that the model can be improved with a small amount of generated data for a new crystal type (P2M) which has not been seen by the model before. In a practical setting, DFT calculations (computationally expensive) can be done on a small number of crystals and be added to the training set. These additional crystals can improve the prediction accuracy on this new type of crystal. The PST can then be applied during the rest of the CSP process in which (tens of) thousands of additional structures could be generated.\n\n> Could you clarify how the contribution is calculated in the ablation study?\n\nIf $e_{PDD}$, $e_{Comp}$ and $e_{PST}$ are the mean-absolute-errors for the contribution for the PDD component would be $c_{PDD} = r_{PDD} / (r_{PDD} + r_{Comp})$ and the composition contribution would be $c_{Comp} = r_{Comp} / (r_{PDD} + r_{Comp})$\n    where $r_{PDD} =  e_{PST} /  e_{PDD}$ and $r_{Comp} = e_{PST} / e_{Comp}$. \n\nPlease let us know if further explanation is needed."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6701/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699808674858,
                "cdate": 1699808674858,
                "tmdate": 1699808674858,
                "mdate": 1699808674858,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3xLLi5oFjQ",
                "forum": "MNwXif6AWA",
                "replyto": "HyxPSmau5B",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6701/Reviewer_yW4J"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6701/Reviewer_yW4J"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the author's response to my questions and concerns. I really appreciate that you have helped me better understand your contributions.\n\nThe responses have almost resolved my concerns; however, I do have further questions:\n\n-- The description of the datasets are still vague: the datasets T2, P1, S2, P1M, P2, and P2M seem to come out suddenly and the readers only have a very general idea about what they are about. If those datasets come from literature, they should be cited in the manuscript; if the authors curated the datasets, how the datasets were curated should be mentioned. Besides, basic information like data size should also be summarized. \n\n-- The authors mentioned in the rebuttal that \"In our explanation, we don't directly define an isometry and instead mention it is a distance-preserving mapping between metric spaces; this will be adjusted in the revision\". However, if that is the case, I think the model here is an invariant neural network and I don't understand how is isometry actually encoded. \n\n-- Further questions about results on Matbench: could you also include the error bars in Table 3? And how's the data efficiency and gpu memory cost compared with baseline models (or even other models on Matbench Leaderboard like coNGN, ALIGNN, or MODNET)?\n\nHope to see your comments on my further questions! Thanks."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6701/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699984571555,
                "cdate": 1699984571555,
                "tmdate": 1699984571555,
                "mdate": 1699984571555,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JrlL9EUFgs",
                "forum": "MNwXif6AWA",
                "replyto": "gJW1q94Hpi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6701/Reviewer_yW4J"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6701/Reviewer_yW4J"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your answers. \n-- I mean, in Table 2 (sorry I referred to the wrong table), since you provide the MAE results on Matbench with 5-fold cross-validation, why not also include the standard deviation of the results from each fold which you can find in Matbench for other models?\n-- It seems that PST costs more memory but could be trained faster compared with MEGNet. Interesting results. I came up with this question because there are more important things than just providing results on benchmark: either significantly improving the leaderboard (not just barely becoming the SOTA) or having some practical applications. The latter could be accelerating the discovery of new materials (not just talking about it), achieving comparable performance while becoming more efficient, etc. This is slightly beyond the scope of review for a conference but it'll be great if the authors can really look into it."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6701/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700430075067,
                "cdate": 1700430075067,
                "tmdate": 1700430075067,
                "mdate": 1700430075067,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "goePccWhmK",
                "forum": "MNwXif6AWA",
                "replyto": "27d2ugukcs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6701/Reviewer_yW4J"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6701/Reviewer_yW4J"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks a lot for the further explanations and great insight! \n\nGiven the response to my concerns and criticisms by other reviewers, I'll not raise my score. Wish you all the best in the future."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6701/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700666579197,
                "cdate": 1700666579197,
                "tmdate": 1700666579197,
                "mdate": 1700666579197,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ERgfUktNDC",
            "forum": "MNwXif6AWA",
            "replyto": "MNwXif6AWA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6701/Reviewer_DofE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6701/Reviewer_DofE"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new representation for machine learning on crystal structures based on Point Distance Distribution (PDD), which the paper claims is both continuous and isometric. The proposed representation augments augments PDD with composition in order to be able to represent a crystal in a unique manner such that machine learning models can be applied to it. The paper also proposes a modified self-attention mechanism that can utilize the PDD and compositional information to predict a variety of materials properties.\n\nThe paper starts by introducing crystal structures and their associated challenges of predicting their properties that traditional computational chemistry methods that are often computationally prohibitive in evaluating properties for many materials. Next, the paper describes the challenge of finding good representations for crystal structures that are isometric and machine learning friendly and defines a set of properties that a good representation should have including invariance, completeness, and continuity. Following a description of related work, the paper discusses the PDD and their proposed periodic set transformer including detailed mathematical definitions. Next the paper describes the PDD encoding that incorporates atom composition information and how it is incorporated in the periodic set transformer. \n\nFollowing the definition of the method, the paper provides two case studies: one for lattice energy prediction and one for materials property prediction based on Materials Project. In the lattice energy prediction study, the paper investigates the effects of different methods with PDD generally showing better performance. In the case of materials property predictions for Materials Project, the results are more mixed with other methods outperforming PST in some, but not all, cases. The paper then provides an ablation study mostly focusing on the input  representation for Materials Project property prediction followed by the conclusion summarizing the work."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper has the following strengths:\n* The paper provides a new representation for machine learning on crystal structures that has very useful properties, including isometry and continuity. The representation itself could be promising for the development of other machine learning methods (originality, significance).\n* The paper provides a new attention mechanism tailored to the PDD representation, which is then applied to different case studies with some results indicating the utility of the representation and the architecture (originality)."
                },
                "weaknesses": {
                    "value": "While the paper introduces an interesting and relevant idea, the current form includes some major weaknesses:\n* The description of the PDD representation and the architecture is often unclear and confusing (clarity).\n* The contribution appears limited to the inclusion of the composition on top of the PDD representation, which appears to be prior work (significance, originality). \n* The experiments performed are relatively small in scale with the results often not well presented (clarity, quality). \n* The experiments are in Section 4 are not well described making it difficult to assess their significance (clarity, quality). Given that only PDD representations were used, it is unclear what contributions of the paper are being highlighted here. Also the model architectures used are unclear. My best guess is that it involves Gaussian processes similar to the AMD case.\n* Many of the figures and tables are only sparsely labeled making it difficult to fully understand the takeaways. (clarity)\n* The notation in Section is hard to follow given that there are letters in upper and lower case with different bold fonts each corresponding to different entities. This can be improved for greater clarity."
                },
                "questions": {
                    "value": "* What are the model architectures used Section 4?\n* Can you describe in more details how the rows of the PDD representation are collapsed into each, specifically how identical rows are identified?\n* How are the rows of the PDD representation ordered? Does this ordering matter?\n* How do atoms get counted in the PDD construction described in Section 3.1? Since composition is not present yet, are the atoms indexed without atom types?\n* Is there a predetermined way to choose k for the PDD? Based on the information in the appendix it appears to be a hyperparameter that seems significant. It would be good to more details on this.\n* What types of crystals are studied in Section 4? You mention both molecules and crystals here, so are these molecular crystals? Is there a reason you claim that only the lattice matters for these structures? Clarity could be substantially improved by providing more detailed information about the task.\n* In Section 5 - is there a reason that CrabNet cannot use PDD embeddings? I would assume that GNNs use a different representation in your study, which would also be good to clarify."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6701/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698534291975,
            "cdate": 1698534291975,
            "tmdate": 1699636769319,
            "mdate": 1699636769319,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4A5SoficYm",
                "forum": "MNwXif6AWA",
                "replyto": "ERgfUktNDC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6701/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6701/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer DofE"
                    },
                    "comment": {
                        "value": "Thank you for your feedback. It seems that clarity was an issue, we will try to make things more clear in the revision. In response to your comments/questions:\n\n> The description of the PDD representation and the architecture is often unclear and confusing (clarity).\n\nWe have tried to supplement the definition of the PDD with an example in the appendix. If you could specify what about the explanation of architecture and PDD representation was unclear that would be greatly appreciated.\n\n> The contribution appears limited to the inclusion of the composition on top of the PDD representation..\n\nThe PDDs original purpose was to measure differences in the structure of crystals in order to compare them. We believe we have provided evidence that it (and potentially other invariants) has a place in machine learning as the input representation. Additionally, the model we propose can be applied to data with the structure of a weighted set (or discrete distribution). The method of turning multisets into weighted sets by grouping inputs based on their features and using weights to describe multiplicity can be applied to similar Transformer input to decrease the number of tokens in the input, improving computational performance. For the experiments in Section 5 the size of the set of the input is reduced to $\\sim 45$ percent of the original number of atoms in the unit cell. \n\n> The experiments performed are relatively small in scale ..\n\nCould you please expand on this? Several other models report between 1-3 datasets including CGCNN  (only the Materials Project), MEGNet (the Materials Project and QM9, which is a molecular not crystal dataset), and SchNet (3 molecular datasets). We will be adding an additional ablation study and the results on the effect of the collapse tolerance on input size compared to other graph approaches (based on another reviewer's comments).\n\n> Given that only PDD representations were used, it is unclear what contributions of the paper are being highlighted here..\n\nA similar critique has been mentioned by other reviewers, we will be more explicit about the setup and purpose of the experiment in section 4 of the revised version of the manuscript. The comparison done in this experiment is between the PST using only the PDD and Gaussian regression with AMD as input. \n\n> What are the model architectures used Section 4?\n\nThis model uses the Transformer described in section 3.1, but instead of using PDD encoding to incorporate structural information with compositional information, it uses just the PDD alone. The model we compare to uses the AMD as the representation for a crystal and the algorithm applied is Gaussian regression.\n\n> Can you describe in more detail how the rows of the PDD representation are collapsed into each, specifically how identical rows are identified?\n\nTwo rows are identified as identical if their distance is less than or equal to the collapse tolerance using a valid distance metric (definition 3.2). In our application, we use the $L_\\infty$ metric. These two rows are collapsed into one by taking the average of the rows. \n\n> How are the rows of the PDD representation ordered? Does this ordering matter?\n\nThey are ordered lexicographically (definition 3.2). The order does not matter as the attention mechanism is permutation equivariant and the pooling layer is permutation invariant. \n\n> How do atoms get counted in the PDD construction described in Section 3.1?..\n\nThe number of atoms in the unit cell defines their count. Through collapsing, however, which unit cell is used does not matter. The PDD contains no knowledge of the composition, only the pairwise distances.\n\n> Is there a predetermined way to choose k for the PDD?\n\n$k$ is selected to be large enough such that the generic completeness property of the PDD is satisfied. That is, the distances in the final column of the PDD are greater than twice the covering radius of the lattice. This varies from crystal to crystal so $k$ is selected such that it is adequate for 99\\% of the crystals in the largest dataset.\n\n> What types of crystals are studied in Section 4?...\n\nYes, they are molecular crystals. Specifically, simulated crystals created during the crystal structure prediction process. For each crystal set (e.g. T2) the crystals share the same composition but have different structure. \n\nThe PDD, is used for measuring differences in structure via EMD. Figure 4a in the appendix shows how the pairwise distances manifest on a 2D plane, creating clusters by composition. The purpose of the experiment is to determine whether such an inference can be made in a ML application wherein the PDD is the input.\n\n> is there a reason that CrabNet cannot use PDD embeddings? \n\nNot necessarily. The fractional encoding they use is determined by the proportion of each element in the material. This would need to be changed to account for two atoms of the same element that have differing $k$-NN distances. \n\nPlease let us know if anything is unclear."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6701/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699805283796,
                "cdate": 1699805283796,
                "tmdate": 1699805283796,
                "mdate": 1699805283796,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "soN2bm5Xzn",
                "forum": "MNwXif6AWA",
                "replyto": "4A5SoficYm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6701/Reviewer_DofE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6701/Reviewer_DofE"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for comments and updates. After going through your responses and updated draft, I have some additional questions and comments:\n\n-  **Experiment Scale:** The current scope of Materials Project, which is the basis for MatBench, has ~160k samples of crystal structures. There are other datasets in the open literature with greater diversity of crystals (e.g OQMD [1], NOMAD [2]) that would help make a stronger case for the capabilities of your proposed method. The Open MatSci ML Toolkit [3] provides further details and a way to interact with these additional datasets.\n- **Transformer Model Comparisons:** In your ablation in Section 5.2, it would be good to see additional results of transformer based methods (e.g. CrabNet) directly. Ideally, one would also the new representation in those architectures as well, but that might come with additional challenges. In that case, it would be good to include such details in the paper itself.\n- **Ablation Study:** Thank you for including the ablation study in Section 5.2; this provides important additional details.\n- **Limitations:** I would like to see a more extensive discussion of limitations of the proposed PDD encoding. One question to ask, for example, would be whether the encoding can only manage periodic structures and/or if it can be adjusted to include surfaces and more complex materials systems. Understanding this can be helpful for diverse applications, such as catalysts (e.g. OpenCatalyst Project [2]) and more complex materials systems. \n\n[1] Kirklin, S., Saal, J. E., Meredig, B., Thompson, A., Doak, J. W., Aykol, M., ... & Wolverton, C. (2015). The Open Quantum Materials Database (OQMD): assessing the accuracy of DFT formation energies. npj Computational Materials, 1(1), 1-15.\n\n[2] Draxl, C., & Scheffler, M. (2019). The NOMAD laboratory: from data sharing to artificial intelligence. Journal of Physics: Materials, 2(3), 036001.\n\n[3] Lee, K. L. K., Gonzales, C., Nassar, M., Spellings, M., Galkin, M., & Miret, S. (2023). MatSciML: A Broad, Multi-Task Benchmark for Solid-State Materials Modeling. arXiv preprint arXiv:2309.05934.\n\n\n[4] Chanussot, L., Das, A., Goyal, S., Lavril, T., Shuaibi, M., Riviere, M., ... & Ulissi, Z. (2021). Open catalyst 2020 (OC20) dataset and community challenges. Acs Catalysis, 11(10), 6059-6072."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6701/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700330885299,
                "cdate": 1700330885299,
                "tmdate": 1700330885299,
                "mdate": 1700330885299,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "SwpVL0L5im",
            "forum": "MNwXif6AWA",
            "replyto": "MNwXif6AWA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6701/Reviewer_V7SM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6701/Reviewer_V7SM"
            ],
            "content": {
                "summary": {
                    "value": "The authors introduce a transformer model with a modified self-attention mechanism that adapts PDD (Pointwise Distance Distribution, represented by a k-nearest-neighbor distance matrix), and incorporates compositional information via a spatial encoding method. Specifically, the authors consider the PDD as a set of grouped atoms and use an attention mechanism to find interactions between members of the set. The authors claim that PDD effectively distinguishes periodic point sets up to isometry but doesn't consider the composition of the underlying material, and thus, the newly proposed encoding method can effectively capture this information."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The introduction of the Periodic Set Transformer (PST) model is articulated in a straightforward manner. The authors have designed the PST model to incorporate not just structural but also compositional information through Pointwise Distance Distribution (PDD) Encoding. This makes the model versatile and potentially more effective in predicting material properties."
                },
                "weaknesses": {
                    "value": "Majors:\n\n1. **Inadequate Experimental Results**: The paper's experimental section reveals suboptimal performance in predicting key electronic properties of crystals, such as formation energy. Notably, the proposed method performs poorly in comparison to other methods listed in the results table. Furthermore, the paper lacks a comparison with the state-of-the-art method coGN [1] at Matbench, which is a significant oversight.\n2. **Lack of Novelty in k-Nearest-Neighbor Construction**: The paper does not sufficiently differentiate its approach from k-nearest-neighbor graph construction of message-passing methods. Common methods for material prediction, such as  CGCNN [3] and ALIGNN [4], also consider both atomic properties and distances, raising questions about what exactly is the authors\u2019 method beyond those message-passing methods with k-nearest-neighbor graph construction. \n\nMinor:\n1. **Omission of Citations**: The authors don't include important baselines coGN [1] and PotNet [2].\n\n\nRef:\n\n[1] Ruff, Robin, et al. \"Connectivity Optimized Nested Graph Networks for Crystal Structures.\" *arXiv preprint arXiv:2302.14102* (2023).\n\n[2] Lin, Yuchao, et al. \"Efficient Approximations of Complete Interatomic Potentials for Crystal Property Prediction.\" *ICML 2023*.\n\n[3] Xie, Tian, and Jeffrey C. Grossman. \"Crystal graph convolutional neural networks for an accurate and interpretable prediction of material properties.\" *Physical review letters* 120.14 (2018): 145301.\n\n[4] Choudhary, Kamal, and Brian DeCost. \"Atomistic line graph neural network for improved materials property predictions.\" *npj Computational Materials* 7.1 (2021): 185."
                },
                "questions": {
                    "value": "See weeknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6701/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698793578375,
            "cdate": 1698793578375,
            "tmdate": 1699636769197,
            "mdate": 1699636769197,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CN9eJgR6v2",
                "forum": "MNwXif6AWA",
                "replyto": "SwpVL0L5im",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6701/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6701/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer V7SM"
                    },
                    "comment": {
                        "value": "Thank you very much for the review, the feedback will allow us to improve the quality of the article. \n\nIn response to the concerns you raised:\n\n> The paper's experimental section reveals suboptimal performance in predicting key electronic properties of crystals, such as formation energy\n\nWhile this is true, it still performs well on other properties in comparison to other similar graph-based models. Additionally, our model outperforms the other Transformer model we compare to (and the only one on Matbench) in 5/6 properties.\n\n> Furthermore, the paper lacks a comparison with the state-of-the-art method coGN\n\nWhile it is true that line graph-based methods such as ALIGNN and coGN perform with higher accuracy, they also incur a significantly higher computational cost. In the crystal graph construction proposed by CGCNN and used in ALIGNN, if a crystal contains $n$ atoms in the unit cell, the resulting graph will also have $n$ vertices. Each vertex will have $k$ directed edges assuming a large enough cutoff radius (which is suggested to prevent isolated portions of the graph). The line graph for this graph would then contain $k$ vertices and $\\frac{n}{2} (k^2 - k)$ edges. coGN adds an additional line graph to account for dihedrals. For a crystal with just 4 atoms in the unit cell, the line graph used in ALIGNN for the angles with $k=12$ would have 48 nodes and 264 edges. The dihedral line graph would contain 264 vertices and 17,424 edges. The crystals in the materials project dataset on average contain $\\sim 24$ atoms in the unit cell. In contrast, the approach we propose decreases the size of the representation by grouping similar atoms together. We have mentioned how the collapse tolerance affects the cardinality of the input set in response to another reviewer, we include it here as well. The size of our representation compared to the number of atoms in the unit cell at a collapse tolerance of 1e-4 is as follows:\n\nPhonon Peak: 0.464\n\nFormation energy: 0.444\n\nBand Gap: 0.437\n\nBulk Modulus: 0.414\n\nShear Modulus: 0.414\n\nRefractive Index: 0.434\n\n where on average if a dataset has 24 atoms in the unit cell, our representation's average size would be roughly  $24 \\times 0.464 \\approx 11$ (in the case of phonon peak).\n\n> The paper does not sufficiently differentiate its approach from k-nearest-neighbor graph construction of message-passing methods\n\nGraph construction in other work often needs the use of a cutoff radius to prevent ambiguous graph construction between neighbors with the same distance. This is not needed in our case as we only use the distance and have no reference to the neighbor's atomic type. This radius and the value of $k$ are often not justified in their selection and are instead based on trial and error. We propose a heuristic for finding an adequate value of $k$ based on the theoretical results of generic completeness for the PDD. This is mentioned in the experiments in the appendix, but it seems pertinent so it will be moved to the main body.\n\nGraph-based methods also use edges to indicate between which atoms the distances lie. This provides additional structure to the data but at the cost of being discontinuous under atomic perturbations. Small changes in the position of atoms can change the set of atoms in the set of $k$-NN. If this occurs edges will shift, changing the graph's structure. In our representation we use the same information but in a different way. Despite not using any references to neighboring atoms to indicate which distances correspond to which neighboring atoms, our model still performs on par or better than similarly informed graph-based models on several properties. Additionally, because only distances to the neighbors are used, perturbations will change the distances in a continuous manner. \n\n> The authors don't include important baselines coGN [1] and PotNet [2]\n\nThese two will be added to the review of related work and the previous rationale in the first point will be included in the section which discusses the selection of models to compare our model to.\n\nPlease let us know if you have further concerns or if anything we have mentioned is unclear."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6701/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699801811022,
                "cdate": 1699801811022,
                "tmdate": 1699801811022,
                "mdate": 1699801811022,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4FzE83JkoL",
            "forum": "MNwXif6AWA",
            "replyto": "MNwXif6AWA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6701/Reviewer_kc8a"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6701/Reviewer_kc8a"
            ],
            "content": {
                "summary": {
                    "value": "The Pointwise Distance Distribution (PDD) is a recently developed invariant for periodic crystals that is easy to compute and differentiates between almost all non-isomorphic lattices (generically complete). The authors of the present work propose to use the PDD in a transformer architecture to learn to predict the lattice energy and other material properties.\n\nThe PDD computes for each atom in the unit cell the $k$-nearest neighbour distances and sorts these into a list. Stacking the list for all $n$ atoms gives a $n \\times k$ matrix. The PDD is the distribution over these rows, so a discrete distribution over $[0,\\infty)^k$. This is an invariant, generically complete, and Lipschitz wrt the earthmover distance on the distributions. The PDD can also be represented as a matrix with weighted rows, where similar rows are collapsed, adding up the weight.\n\nThe authors propose to incorporate the PDD data in four ways into a transformer:\n- instead of using the atoms as tokens, it uses the rows of the PDD matrix, so collapsing atoms with similar $k$-NN distances\n- the initial features are the $k$-NN distances, combined with atomic properties (the authors don't collapse different atoms with similar $k$-NN distances)\n- The self-attention is additionally weighted by the PDD weights\n- The transformer output is pooled using the PDD weights\n\nThe authors show in their experiments that using the PDD is superior to using an alternate invariant, and the authors show that their method performs competitively to other material property prediction methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- I think it's great to incorporate the powerful PDD invariant into neural networks\n- The authors show strong performance on the material prediction dataset."
                },
                "weaknesses": {
                    "value": "- I think an important ablation is missing: just using a typical transformer on the atoms as tokens with the $k$-NN distances and the atomic properties as features. The \"PDD\" ablation study still only uses the PDD in all the four ways I listed in my summary. It'd be great if the authors could ablate these separately. The CGCNN baseline uses the $k$-NN distances as features, but is not a transformer, so is not a substitute to this ablation.\n- A key property of the PDD is its Lipschitz continuity, making it robust to perturbations in the positions. The way the authors use the $k$-NN distances with the hard collapse, then treating the rows as separate tokens, loses this property. Currently, however, the authors are suggesting that the continuity of the PDD is a benefit to their method. The authors should clarify that."
                },
                "questions": {
                    "value": "- In their description of the transformer, it appears like each block only uses self attention and normalization. Is there no MLP used in each block, as is typical in a transformer?\n- Could the authors comment on how often the rows of the PDD are collapsed in practice, so how much it matters that the used tokens are aggregates, rather than individual atoms?\n- In Def 3.1, the numbers $c_i$ are said to be integer and contained in $[0, 1)$. This would imply they are zero, which I suppose is not what is intended. Could the authors clarify?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6701/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699263307078,
            "cdate": 1699263307078,
            "tmdate": 1699636769072,
            "mdate": 1699636769072,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zpbeCDDe2v",
                "forum": "MNwXif6AWA",
                "replyto": "4FzE83JkoL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6701/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6701/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer kc8a"
                    },
                    "comment": {
                        "value": "Thank you for the feedback, it has allowed us to add additional experimental data and clarify definitions in the manuscript. In response to the points you raised:\n\n> I think an important ablation is missing...\n\nIn the first scenario you mentioned where collapsing is not done the result should be the same as the PDD-weighted version if the collapse tolerance is exactly zero. The difference in performance compared to higher collapse tolerances is included in the appendices. The representation can be significantly larger than without collapsing and will change should the unit cell chosen be different. In the second scenario, we believe this is equivalent to the \"composition\" row in the current ablation study where PDD encoding is not used. We are in agreement that in the third and fourth scenarios there should be another ablation to show their effect. Experiments are currently being run and the results will be included in the updated article.\n\n> A key property of the PDD is its Lipschitz continuity..\n\nDuring the collapse of any two rows, the newly produced row is created taking the average of the two rows. This way even if the collapse tolerance is greater than zero rows are not selected in an either/or manner. This detail was left out, thank you for pointing it out. It will be added to the updated manuscript.\n\n>  Is there no MLP used in each block, as is typical in a transformer?\n\nThere is a single layer perceptron which the embeddings are passed through directly after the first layer normalization and then layer normalization is again applied on the sum of the newly updated embeddings (the output of the SLP) and the embedding before it is passed into the SLP. \n\n> Could the authors comment on how often the rows of the PDD are collapsed in practice..\n\nThis is dependent on the dataset and the collapse tolerance used. For a collapse tolerance of 1e-4 (the same as used in the main Materials Project results) the number of grouped tokens in the representation over the number of atoms in the unit cell, averaged for each dataset is:\n\nPhonon Peak: 0.464\n\nFormation energy: 0.444\n\nBand Gap: 0.437\n\nBulk Modulus: 0.414\n\nShear Modulus: 0.414\n\nRefractive Index: 0.434\n\nWe will add information to the article to show how this changes depending on other collapse tolerances. \n\n>  This would imply they are zero, which I suppose is not what is intended. Could the authors clarify?\n\nThis portion was indeed unclear, it has been changed such that the lattice keeps its original definition, but the definition of the unit cell has been changed to the space spanned by the parallelepiped $U = { \\sum_{i=1}^{n} t_i \\mathbf{v}_i | t_i \\in [0, 1) }$.\n\nPlease let us know if you have further questions."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6701/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699798994869,
                "cdate": 1699798994869,
                "tmdate": 1699798994869,
                "mdate": 1699798994869,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qehn4Z8Wvq",
                "forum": "MNwXif6AWA",
                "replyto": "4FzE83JkoL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6701/Reviewer_kc8a"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6701/Reviewer_kc8a"
                ],
                "content": {
                    "title": {
                        "value": "Question regarding Lipschitz continuity"
                    },
                    "comment": {
                        "value": "Dear authors,\n\nThanks for your response. I've got a follow-up question regarding the Lipschitz continuity property.\nThe PDD is a map between metric spaces from (pointcloud, Euc metric) to (distribution, Earth mover metric) that's Lipschitz continuous. However, in your case, you treat the PDD as a matrix instead of a distribution of rows, so the output shape depends in the input in a non-continuous way. From the construction of the transformer afterwards, it appears to me that the network as whole is not Lipschitz-continuous (or even continuous) when we move a pair of atoms in/out of the $\\epsilon$ ball.\nWe can see that from the following: the network seems not invariant to duplicating an atom, while halving its weight. It may be that the attention rule in the equation of $r^{(1)}_i$ was designed to have this property, but I don't think that's satisfied due to the weight $w_j$ being outside of the softmax, breaking linearity.\n\nLipschitz-continuity is a desirable (and as far as I know typically satisfied by most architectures that don't cluster), but not necessary property of such networks. However, I do think that the paper should fairly reflect that this continuity property of PDD is not used in this architecture.\n\nEdit: just to add, as the title contains \"continuous\" I think this is quite an important discussion."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6701/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699870019885,
                "cdate": 1699870019885,
                "tmdate": 1700065491839,
                "mdate": 1700065491839,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Dq2eCqQo0c",
                "forum": "MNwXif6AWA",
                "replyto": "VV2WN2Ymzg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6701/Reviewer_kc8a"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6701/Reviewer_kc8a"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks! With this corrected attention rule (but not with the one in the original manuscript), I can see that the network will be continuous. I'd be great if you could add a little proof of this property to the paper. I think that'll be instructive, and motivate the attention rule you propose.\n\nI agree with reviewer V7SM that important baselines are missing in experiments. Once these are added, I will reconsider my score."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6701/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700134253399,
                "cdate": 1700134253399,
                "tmdate": 1700134253399,
                "mdate": 1700134253399,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mLPawPwZR8",
                "forum": "MNwXif6AWA",
                "replyto": "ratbpgNo2z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6701/Reviewer_kc8a"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6701/Reviewer_kc8a"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their responses. While my concern regarding continuity has been addressed, I maintain my score in light of the other reviewers' criticisms."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6701/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700656425022,
                "cdate": 1700656425022,
                "tmdate": 1700656425022,
                "mdate": 1700656425022,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]