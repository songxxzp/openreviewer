[
    {
        "title": "Image Translation as Diffusion Visual Programmers"
    },
    {
        "review": {
            "id": "CHCGrPrhla",
            "forum": "yozwqhIHXj",
            "replyto": "yozwqhIHXj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission759/Reviewer_EyuE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission759/Reviewer_EyuE"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents the Diffusion Visual Programmer (DVP), a new neuro-symbolic framework for image translation that combines a diffusion model with the GPT architecture, coordinating a sequence of visual programs for image editing tasks like RoI identification, style transfer, and positioning. DVP stands out due to its ability to use instance normalization for condition-flexible translation, focusing on textual prompts for quality content creation, and its capability to convert complex concepts in feature spaces into simpler symbolic representations, ensuring localized editing that retains image coherence. Additionally, DVP provides clear symbolic representations at every stage, enhancing user control and understanding, marking progress in aligning artificial image processes with human cognitive intelligence."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The Diffusion Visual Programmer (DVP) method, rooted in a neuro-symbolic perspective, integrates a condition-flexible diffusion model with the GPT framework, guiding a series of pre-existing computer vision models to produce the desired results. DVP's strength lies in its ability to accurately position translated objects and allow for various context-free manipulations. The advantages of this work include:\n1. Precision in Targeting: By prioritizing the identification of the Region of Interest (RoI), the system ensures accurate and relevant image translations.\n2. High-fidelity Translations: After identifying the target region, the system maintains high-quality translations into the desired domain.\n3. Spatial Awareness: DVP's capability to position translated objects based on spatial data enhances the translation's realism and context.\n4. Enhanced User Control: The system's step-by-step approach allows users to have better control over the translation process, ensuring results align closely with their intentions."
                },
                "weaknesses": {
                    "value": "1. Dependency on Modules: DVP's systematic design implies that if one module falters, the entire system could be compromised. The robustness of such an interconnected system becomes a pertinent question. If one cog in the machine is faulty, can the system still deliver optimal results, or will it be critically hampered?\n2. Comparison with Other Techniques: Some results showcased by DVP can be achieved using established perception techniques like Segment Anything (SAM) or conditional control generation methods like ControlNet's point-to-point mode. A comparative discussion on the advantages of DVP over these techniques would provide clarity on DVP's unique value proposition.\n3. Textual Argument Handling: Even though DVP marries a condition-flexible diffusion model with the GPT architecture, its prowess in handling complex textual arguments requiring intricate reasoning remains questionable. The paper does not offer substantial experimental evidence to vouch for the enhanced capabilities of integrating GPT, leaving room for skepticism.\n4. Handling Complex Scenarios: DVP's recurrent challenge is its inability to adeptly manage intricate situations. As highlighted in the failure case study, the system falters in the face of occlusions, inadequate lighting conditions, and densely populated scenes. This limitation raises concerns about the system's applicability in real-world, diverse scenarios."
                },
                "questions": {
                    "value": "See above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Discrimination / bias / fairness concerns"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "There is no discussion on the bias, e.g., instructions like \"changing a man to a woman\". What is the possible bias it may cause?"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission759/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698105678911,
            "cdate": 1698105678911,
            "tmdate": 1699636003373,
            "mdate": 1699636003373,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0IGLfb1jmO",
                "forum": "yozwqhIHXj",
                "replyto": "CHCGrPrhla",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission759/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission759/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission759/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission759/Reviewers",
                    "ICLR.cc/2024/Conference/Submission759/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission759/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission759/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer EyuE"
                    },
                    "comment": {
                        "value": "Thank you for the valuable time and constructive feedback.\n\n#### **Q1 Dependency on Modules:**\n\n**A1:** Thank you for the great insight. The apprehension regarding the dependency of our proposed DVP on its constituent modules and the potential impact on the system's robustness is a critical aspect of our design considerations. In DVP, we integrate GPT ($i.e.,$ as a planner) and various built-in modules, where the interconnected nature is indeed a defining characteristic. DVP is designed with a modular architecture, where each component, while important, does not singularly dictate the system's overall functionality. The modular design allows for a quick debugging during the process. This is critical in maintaining the system's operational integrity even when individual components face challenges. We have discussed the failure and success cases in Fig. 7. However, if a module is **fully compromised**, the system's performance will be affected.\n\n#### **Q2 Comparison with Other Techniques**\n\n**A2:** Thank you for your insightful comments. We agree that established perception techniques like SAM generate promising results in various applications. These methods might not be directly comparable with DVP as they are not designed for image translation, and thus more suitable for replacing the current segmentation model ($i.e.,$ Mask2former [ref6]) for performance comparison. To understand how these models can further improve DVP, we conduct additional experiments by replacing our segmentation model with SAM. The preliminary results show that it indeed enhances the overall performance of DVP, indicating the flexibility of DVP for adopting established perception techniques. We\u2019ve supplemented the additional experimental results in the Appendix Sec.H. Thank you for the great suggestion.\n\n#### **Q3 Prowess in handling complex textual arguments:**\n\n**A3:** Thank you for the great suggestion. DVP integrates the power from GPT as a planner, which is renowned for its ability to process and analyze complex textual data, forming the backbone of our approach. We\u2019ve provided more experiments in Appendix Sec.K, focusing on complex instructions involving spatial relations. \n\n#### **Q4 Handle complex scenarios:**\n\n**A4:** This is a great observation. Regarding the handling of complex scenarios, such as those involving occlusions, the modular architecture of DVP is designed to be both scalable and adaptable. The current limitations observed in the failure case study are indeed points of concern, but they also serve as catalysts for ongoing improvements. Due to the modular nature, each module within DVP can be continuously developed and updated. This advantage allows us to progressively enhance the performance of the whole system, even in challenging conditions. Also, we need to emphasize that poor translation results in complex scenarios is a common problem in the concurrent community. As stated in Appendix Sec.L and N, we highlight it as a future direction of our DVP such as reconstructing the occluded objects before editing from additional amodal segmentation models for a better translation result.\n\n#### **Q5 Bias concerns:**\n\n**A5:**  Thank you for your concern. The nature of DVP as a modular system demonstrates that it does not inherently contribute knowledge during the image translation process. Consequently, concerns about bias are more pertinent to the individual modules we introduced in DVP and components it utilizes. For example, the diffusion model is found to exhibit biases [ref7-10] such as gender stereotypes or imbalances due to its training data. This problem can be solved by introducing/replacing a more neutral alternative [ref11-12]. We recognize the importance of this discussion and incorporate it into Appendix Sec.N. We also expect the computer vision society to **seriously** consider this concern. \n\n[ref1] AnyDoor: Zero-shot Object-level Image Customization. ArXiv\n\n[ref2] Inst-Inpaint: Instructing to Remove Objects with Diffusion Models. ArXiv\n\n[ref3] Visual Programming: Compositional visual reasoning without training. CVPR 2023\n\n[ref4] Segment anything. ArXiv\n\n[ref5] Adding Conditional Control to Text-to-Image Diffusion Models. ICCV 2023\n\n[ref6] Masked-attention Mask Transformer for Universal Image Segmentation. CVPR 2022\n\n[ref7] Stable bias: Analyzing societal representations in diffusion models. ArXiv\n\n[ref8] Analyzing bias in diffusion-based face generation models. ArXiv\n\n[ref9] Do ImageNet Classifiers Generalize to ImageNet? ICML 2019\n\n[ref10] Unbiased look at dataset bias. CVPR 2011\n\n[ref11] Fine-tune language models to approximate unbiased in-context learning. ArXiv\n\n[ref12] A Conservative Approach for Unbiased Learning on Unknown Biases. CVPR 2022\n\nWe appreciate your thoughtful comments. The above discussions are incorporated in our revised paper (in orange). We hope our response addresses your concerns. Please let us know if there are any additional questions, and we are happy to discuss further."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission759/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700276285456,
                "cdate": 1700276285456,
                "tmdate": 1700718702382,
                "mdate": 1700718702382,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "IfVpWRCCs7",
            "forum": "yozwqhIHXj",
            "replyto": "yozwqhIHXj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission759/Reviewer_DvDX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission759/Reviewer_DvDX"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a new method, named Diffusion Visual Programmer (DVP), for image translation. DVP is a neuro-symbolic framework that incorporates condition-flexible translation into an LLM planner. Specifically, the authors propose instance normalization to avoid manual guidance to achieve higher-quality content editing. The authors also prompt LLM to generate a sequence of programs with operations, which can be executed to plan the procedure, segment the object of interest, give the caption for the image, inpaint the masked image, and manipulate the sizes and positions of objects. DVP makes the process of image translation controllable and interpretable. The qualitative and quantitative results demonstrate a good performance of DVP."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper integrates the condition-flexible diffusion model into a visual programming framework to improve image translation.\n- The paper proposes an efficient instance normalization to improve the quality of image editing.\n- GPT planner provides controllability and explainability for the image translation process.\n- The generated results are good with quantitative and qualitative evaluation.\n- The paper is well-written and easy to read."
                },
                "weaknesses": {
                    "value": "- It seems the conditional-flexible diffusion model is the prompt-to-prompt method with proposed instance normalization. However, the authors only compare the results of using instance normalization and different guidance scales. It is also important to ablate the instance normalization in DVP to demonstrate instance normalization significantly outperforms a fixed scale when incorporating in-context visual programming.\n- The proposed method is an aggregation of different models planned by an LLM. However, how the components (e.g., mask2former, Repaint, BLIP, position manipulator) are aggregated is not clearly clarified. The authors may explain various translation procedures by providing more step-wise illustrations like Fig. 7. Besides, the authors may provide the prompt they use for the GPT planner.\n- Some important details are missing. For example, how the conv layer in instance normalization is parameterized? Is there any other difference between the conditional-flexible diffusion model and the prompt-to-prompt model, except for instance normalization? How to decide the order of operation sequences, automatically by GPT or manually by humans?"
                },
                "questions": {
                    "value": "- The authors use a conv layer in instance normalization (Eq. 5). But there is no evidence of how the conv layer is trained or parameterized. Can authors give more details?\n- In Figure 7, We can see different plans derive different translated results. How do authors obtain different orders of operation sequences? How do authors decide which plan is the best for the final result?\n- In Fig. 8, are reconstructed results with automatic labels and human labels generated by null-text inversion? To my knowledge, null-text inversion produces quite similar reconstructed images to the original ones regardless of the input prompt. Can authors explain why using GPT-generated prompts derive better reconstructed results than human annotations?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission759/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698400843969,
            "cdate": 1698400843969,
            "tmdate": 1699636003292,
            "mdate": 1699636003292,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nO0dYDwGM0",
                "forum": "yozwqhIHXj",
                "replyto": "IfVpWRCCs7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission759/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission759/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission759/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission759/Reviewers",
                    "ICLR.cc/2024/Conference/Submission759/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission759/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission759/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer DvDX"
                    },
                    "comment": {
                        "value": "We thank reviewer DvDX for the valuable time and constructive feedback.\n\n#### **Q1 Additional ablation for instance normalization**\n\n**A1:** Thank you for the great suggestion. We have conducted an additional ablation by comparing instance normalization in DVP with a fixed scale. Our results show that instance normalization significantly outperforms a fixed scale when incorporating in-context visual programming, indicating the effectiveness of instance normalization. We have also included the additional results and discussions in Appendix Sec.B Figure 11.\n\n\n#### **Q2 More details on component aggregation and prompt for GPT planner:**\n\n**A2:** Thank you for the suggestion. We leverage the capabilities of GPT to create a sequence of operations. This planning process yields a sequence of distinct interpreters, such as SegmentInterpreter and EDITInterpreter. The method of translating is identical to that shown in Figure 7. For the prompts specified in the GPlan, we instruct GPT-4 to generate code that adheres to the same format employed for each individual pre-defined component. We\u2019ve provided more clarification on how the components are aggregated in the revision.\n\n\n#### **Q3.1 More details on Conv layer in instance normalization:**\n\n**A3.1:** As outlined in our implementation, we employ the null-text optimization method [ref1] to tune the conv layer. This approach involves optimizing the convolutional layers and the unconditional embedding, initialized with a null-text embedding. This technique, as we have employed, ensures a balance between high-quality image reconstruction and the provision of intuitive editing capabilities. We\u2019ve added more details in the revision. \n\n\n#### **Q3.2 Decision-making for the order of operation sequences:**\n\n**A3.2:** Thanks for the insightful question. The process of obtaining these various sequences of operations is primarily facilitated by GPT-4, which acts as an automated planner. This automated generation often results in a diverse array of operation sequences, each potentially leading to distinct outcomes. Additionally, we can also force GPT-4 to follow a pre-defined order of operations by providing proper instruction.\n\nIn our current framework, we do not definitively conclude which plan is superior. Instead, GPT-4's role as an automated planner is to provide a range of reasonable and logically structured operation sequences. However, identifying the \u2018\u2019best\u2019\u2019 plan for the results remains an open question for our future exploration.\n\n\n#### **Q4 Fig. 8:** \n\n**A4:**  Sorry for the confusion. The reconstruction results shown in this figure use only 40% of the optimization steps. We aim to demonstrate the advantage of our method (fine-grained accurate annotation) over the baseline using a much shorter schedule. We acknowledge the necessity to revise the image description in our paper to clearly delineate this distinction (added in revision paper Sec. 4.3). Regarding the full null-text inversion process, it generally yields reconstructed images that closely match the originals, regardless of the input prompt, even when the prompts are entirely incorrect.\n\n[ref1] Null-text inversion for editing real images using guided diffusion models. CVPR 2023\n\nWe appreciate your thoughtful comments. The above discussions are incorporated in our revised paper (in green). We hope our response addresses your concerns. Please let us know if there are any additional questions, and we are happy to discuss further."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission759/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700276103247,
                "cdate": 1700276103247,
                "tmdate": 1700276103247,
                "mdate": 1700276103247,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "G1EMWBa9JE",
                "forum": "yozwqhIHXj",
                "replyto": "nO0dYDwGM0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission759/Reviewer_DvDX"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission759/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission759/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission759/Reviewers",
                    "ICLR.cc/2024/Conference/Submission759/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission759/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission759/Reviewer_DvDX"
                ],
                "content": {
                    "comment": {
                        "value": "Thank the authors for providing their rebuttal. I have no further questions."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission759/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700660976178,
                "cdate": 1700660976178,
                "tmdate": 1700660976178,
                "mdate": 1700660976178,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "orOdJDGPTX",
            "forum": "yozwqhIHXj",
            "replyto": "yozwqhIHXj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission759/Reviewer_TXue"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission759/Reviewer_TXue"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors introduce the Diffusion Visual Programmer (DVP), a neuro-symbolic image translation framework that seamlessly combines a diffusion model with the GPT architecture. DVP enables transparent and controllable image translation processes, covering tasks like RoI identification, style transfer, and position manipulation. Extensive experiments showcase DVP's remarkable performance, surpassing existing methods. The success of DVP can be attributed to its condition-flexible translation, in-context reasoning, and systemic controllability. This research represents a significant advancement in harmonizing image translation with cognitive intelligence, with promising applications in various domains."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. **Innovative Framework**: DVP introduces a novel neuro-symbolic image translation framework, seamlessly combining a diffusion model with the GPT architecture for various image processing tasks.\n\n2. **Remarkable Performance**: DVP outperforms existing methods, demonstrating high-quality image translation through extensive experiments.\n\n3. **Condition-Flexible Translation**: DVP achieves condition-flexible translation via instance normalization, enhancing content generation while reducing manual guidance sensitivity."
                },
                "weaknesses": {
                    "value": "1. **Novelty**: The paper should address concerns about its novelty, as it resembles existing visual programming pipelines with the primary distinction being the choice of image editing tools. Clarification on how DVP significantly sets itself apart from previous work is crucial.\n\n2. **Lack of Comparative Analysis**: The paper's strength in performance could be further substantiated by a more thorough comparative analysis. A comparison with alternative approaches like instructive tuning-based diffusion models, such as instruct2pix, would provide a clearer perspective on its relative merits.\n\n3. **Limited Experimentation on Instance Normalization**: The paper introduces instance normalization as a valuable element of the DVP framework, which can have broader applications. However, there is a need for more extensive experimentation to validate the advantages of instance normalization in the context of general diffusion-based image generation."
                },
                "questions": {
                    "value": "While the paper showcases DVP's impressive performance, there is a need for further exploration of its adaptability to a range of image translation tasks. These tasks might include object replacement, color/texture/material editing, and style transfer. Currently, the paper's experiments encompass a holistic approach without distinct analysis of how DVP caters to varying task requirements.\n\nAdditionally, the paper's reliance on a relatively small self-collected dataset comprising 100 image-text pairs may present limitations in terms of the breadth and diversity of data for evaluation."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission759/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698486003321,
            "cdate": 1698486003321,
            "tmdate": 1699636003215,
            "mdate": 1699636003215,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "O1k3C5AvwC",
                "forum": "yozwqhIHXj",
                "replyto": "orOdJDGPTX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission759/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission759/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission759/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission759/Reviewers",
                    "ICLR.cc/2024/Conference/Submission759/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission759/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission759/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer TXue"
                    },
                    "comment": {
                        "value": "We thank reviewer TXue for the valuable time and constructive feedback.\n\n#### **Q1 Novelty of DVP:**\n\n**A1:** Thank you for the question. We try to provide some clarification here.\n\n**Limitation of existing work**: Current image translation approaches ($i.e.,$ AnyDoor and Inst-Inpaint) remain **opaque** [ref1] or need **additional training** [ref2]. Specifically, for AnyDoor, it remains untransparent with an additional ID extractor, and detailed maps for hierarchical resolutions, preventing it from intuitively manual modifications. For Inst-Inpaint, it required extra training, which is not amenable to the training-free paradigm. Visual Programming, on the other hand, is a neuro-symbolic approach available for image translation. However, the strong editing abilities for **compositional generalization have been overlooked**.\n\n**DVP contribution**: Our DVP contributes on three distinct technical levels. First, we re-consider the design of classifier-free guidance, and propose a **conditional-flexible** diffusion model which eschews the reliance on sensitive manual control hyper-parameters. Second, by decoupling the intricate concepts in feature spaces into simple symbols, we enable a **context-free** manipulation of contents via visual programming, which is both controllable and explainable. Third, our GPT-anchored programming framework serves as a demonstrable testament to the versatile **transferability** of Language Model Models, which can be extended seamlessly to a series of critical yet traditional tasks ($e.g.,$ detection, segmentation, tracking tasks). We do believe these distinctive technical novelties yield invaluable insights within the community.\n\nWe add discussion in Appendix Sec.N in our revision paper to highlight the novelty of our approach. Thank you for the suggestion!\n\n#### **Q2 Lack of Comparative Analysis:**\n\n**A2:** Thank you for the great suggestion. We agree that additional comparison with instructive tuning-based diffusion models would provide a clearer understanding of the advantage of our model. We conduct a thorough comparative study with instruct2pix [ref3] as suggested. As shown in Fig. 16, our approach consistently yields high-quality performance due to its robust capacity for in-context reasoning. We have included these qualitative findings in Appendix Sec.I. \n\n#### **Q3 Instance normalization can have broader applications.**\n\n**A3:** We appreciate the insightful suggestion. In our work, instance normalization is specifically coupled with the null-text optimization technique, which is tailored to suit the unique requirements of our approach and is not yet generalized for broader diffusion-based image generation applications. However, we do recognize that instance normalization harbors a considerable potential that remains largely untapped. Therefore, we conduct a preliminary study for the text-to-image task. Specifically, we initially generate an image following comprehensive instructions and then proceed with the fine-tuning process, eliminating the need for complete regeneration of the image from scratch. These preliminary results together with the discussions are supplemented in Appendix Sec.N and Fig. 20.\n\n#### **Q4 Adaptability to a range of image translation tasks:**\n\n**A4:** We showcase several examples of object replacement in specific locations to highlight the robust in-context reasoning capability of the proposed DVP, addressing limitations observed in previous methods. It's important to note that DVP is a general framework for image translation, applicable to various task variants. To further demonstrate the adaptability of DVP, we provide additional results and discussions on style transfer (see Fig. 9 in the Appendix) and editing (refer to Figures 14, 16, 17, and 18 in the Appendix) as suggested.\n\n#### **Q5 The dataset is self-collected:**\n\n**A5:** Due to the scarcity of publicly accessible datasets, we have introduced a new dataset featuring detailed textual descriptions. For quantitative evaluation, we adhere to established methods [ref1, ref4-5] and execute these on our newly developed dataset, detailed in Appendix Sec.E. To ensure a level playing field, we apply all methods uniformly across our dataset. Additionally, we plan to expand our data collection to encompass a broader diversity, and release our dataset.\n\n\n[ref1] AnyDoor: Zero-shot Object-level Image Customization. ArXiv\n\n[ref2] Inst-Inpaint: Instructing to Remove Objects with Diffusion Models. ArXiv\n\n[ref3] InstructPix2Pix: Learning to Follow Image Editing Instructions. CVPR 2023\n\n[ref4] Prompt-to-Prompt Image Editing with Cross Attention Control. ICLR 2023\n\n[ref5] Text2LIVE: Text-Driven Layered Image and Video Editing. ECCV 2022\n\nWe appreciate your thoughtful comments. The above discussions are incorporated in our revised paper (in blue). We hope our response addresses your concerns. Please let us know if there are any additional questions, and we are happy to discuss further."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission759/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700275964179,
                "cdate": 1700275964179,
                "tmdate": 1700275964179,
                "mdate": 1700275964179,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KlFnUWZK0u",
                "forum": "yozwqhIHXj",
                "replyto": "O1k3C5AvwC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission759/Reviewer_TXue"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission759/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission759/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission759/Reviewers",
                    "ICLR.cc/2024/Conference/Submission759/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission759/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission759/Reviewer_TXue"
                ],
                "content": {
                    "title": {
                        "value": "Further question"
                    },
                    "comment": {
                        "value": "Regarding Question 1, I would like to clarify my query. My original question pertained to how your approach differs from **visual programming**, not from other generative models. Since visual programming also supports image editing, could you specify what `additional tasks or capabilities your method offers that are not possible` with traditional visual programming?"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission759/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700449950494,
                "cdate": 1700449950494,
                "tmdate": 1700449950494,
                "mdate": 1700449950494,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6XGRdA1tUe",
                "forum": "yozwqhIHXj",
                "replyto": "orOdJDGPTX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission759/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission759/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission759/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission759/Reviewers",
                    "ICLR.cc/2024/Conference/Submission759/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission759/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission759/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your additional question"
                    },
                    "comment": {
                        "value": "Sorry for our misunderstanding, and thank you for the clarification. In comparison to traditional visual programming, our approach demonstrates several distinct capabilities:\n\n1. **Arbitrary Positional Editing**: Our system empowers unrestricted editing, including the manipulation of position, thanks to our advanced **in-context reasoning** capability\u2014a functionality that is not achievable in traditional visual programming. More precisely, we transform human instructions into a domain-specific language, incorporating a comprehensive set of fundamental logic (refer to Sec 3.2 and Figure 7), thereby facilitating versatile image editing.\n\n2. **Automatic Image Translation**: Traditional visual programming relies on off-the-shelf modules that depend on manually crafted guidance scale parameters to oversee the translation process for each individual image, resulting in **condition-rigid learning**. In contrast, our DVP introduces a novel **condition-flexible** diffusion model that operates fully automatically, eliminating the need for human intervention (refer to Sec. 3.1). This model outperforms traditional methods by robustly translating images without the need for tunable parameters, representing a substantial advancement in both the quality and versatility of image processing.\n\n3. **Generalizability**: The DVP framework we present is highly flexible and extensible, adept at addressing a diverse set of tasks that extend beyond the traditional scope. This encompasses, but is not restricted to, video editing (refer to Appendix D Figure 12), text-to-image generation (refer to Appendix N and Figure 20), etc. The adaptability of our framework is underscored by its **compatibility** with various off-the-shelf models. This adaptability is largely attributed to our GPT planner, which seamlessly integrates with these models, thereby broadening the potential applications of our system.\n\nThanks again for your thoughtful question! We are happy to discuss more if you have any other questions."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission759/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700458405899,
                "cdate": 1700458405899,
                "tmdate": 1700458599607,
                "mdate": 1700458599607,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LVRZgju1mA",
                "forum": "yozwqhIHXj",
                "replyto": "6XGRdA1tUe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission759/Reviewer_TXue"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission759/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission759/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission759/Reviewers",
                    "ICLR.cc/2024/Conference/Submission759/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission759/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission759/Reviewer_TXue"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the Reply"
                    },
                    "comment": {
                        "value": "Thank you for your response. However, I must point out that points 1 and 3 do not seem accurate. For point 1, the visual programming paper already allows for position selection through a detection model, making the current extension seem straightforward.\n\nRegarding point 3, the tasks related to image editing and generation you mentioned are also covered in that paper. While the video aspect isn't, it appears to be a simple modification and doesn't seem to offer technical or conceptual novelty.\n\nPoint 2 is valid, as your paper introduces automatic guidance scaling.\n\nThank you for addressing these points. I will consider them further."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission759/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700463377239,
                "cdate": 1700463377239,
                "tmdate": 1700463377239,
                "mdate": 1700463377239,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TFpDkkJJbH",
                "forum": "yozwqhIHXj",
                "replyto": "orOdJDGPTX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission759/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission759/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission759/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission759/Reviewers",
                    "ICLR.cc/2024/Conference/Submission759/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission759/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission759/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the further discussion"
                    },
                    "comment": {
                        "value": "We really appreciate your prompt response, and the great insights.\n\nRegarding point 1, we fully concur that visual programming can indeed handle position selection. However, the key distinction lies in DVP's unique capacity for in-context reasoning when confronted with intricate and arbitrary position manipulations, a feat where traditional VP may falter. For instance, consider the task of 'changing the rightmost dog to a cat and shifting it to the leftmost'. This demands the model to accurately comprehend the underlying concepts and interpret their positional information/relations. We should have articulated this capability as in-context reasoning to make it more accurate.\n\nRegarding point 3, yes, we agree that VP can be tailored for such tasks. Our emphasis was on underscoring the **flexibility and extensibility** inherent in DVP, which is more easily extendable to new tasks compared to VP, attributed to our GPT planner.\n\nThank you very much for pointing these out. We will incorporate the above discussion into the revision to make it more clear."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission759/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700465128312,
                "cdate": 1700465128312,
                "tmdate": 1700510414533,
                "mdate": 1700510414533,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "J42upgrDqX",
                "forum": "yozwqhIHXj",
                "replyto": "TFpDkkJJbH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission759/Reviewer_TXue"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission759/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission759/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission759/Reviewers",
                    "ICLR.cc/2024/Conference/Submission759/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission759/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission759/Reviewer_TXue"
                ],
                "content": {
                    "title": {
                        "value": "Maintain my Original Score"
                    },
                    "comment": {
                        "value": "Thanks to the author for their detailed explanation. I will keep my original score at 6. Good luck."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission759/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700634257139,
                "cdate": 1700634257139,
                "tmdate": 1700634257139,
                "mdate": 1700634257139,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "6QokFmdT9c",
            "forum": "yozwqhIHXj",
            "replyto": "yozwqhIHXj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission759/Reviewer_fWRk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission759/Reviewer_fWRk"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors propose a new framework called Diffusion Visual Programmer (DVP) for image translation tasks. It follows the diagram that first identifies the instructed target region and then translates it into the targeted domain. In particular, the authors use instance normalization for context-flexible translation, avoiding adjusting guidance manually. Besides, the authors decouple the concepts in feature space into simple symbols and then deal with the symbols with visual programming. Qualitative and qualitative results show that DVP outperforms other state-of-the-art methods, providing more reliable, controllable, and interpretable image translation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Overall, the paper is well-organized and easy to follow. The figures and tables are informative.\n\n- The performance of the proposed method is promising. Figures 4, 6 clearly demonstrate the superiority of DVP.\n\n- The ablation study and system analysis are clear and informative, making it easy to see the effectiveness of different parts, such as instance normalization, and prompter."
                },
                "weaknesses": {
                    "value": "- The proposed method is a systematic approach for image translation tasks incorporating different components. A potential drawback is its inference speed. It would be beneficial if the authors could compare inference speed with other image translation tasks.\n- The comparison with methods like SDEdit, Prompt2Prompt, and InstructPix2Pix is somehow unfair since they do not require an additional segmentation network.\n- The quantitative evaluation is only the proposed dataset, which contains fine-grained edit instructions. The effectiveness of DVP could be further proved by evaluating simple or even ambiguous instructions."
                },
                "questions": {
                    "value": "- I have questions about the learning process of the 1\u00d71 conv layer in equation (5). How is it exactly trained? And is it sensitive to the training sample size?\n- Will instance normalization also work in text-to-image tasks? It will be interesting to see if it could generate higher fidelity images with semantic meaning more aligned with the provided text prompts."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission759/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission759/Reviewer_fWRk",
                        "ICLR.cc/2024/Conference/Submission759/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission759/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698808366018,
            "cdate": 1698808366018,
            "tmdate": 1700570787749,
            "mdate": 1700570787749,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EtwHsT8A1v",
                "forum": "yozwqhIHXj",
                "replyto": "6QokFmdT9c",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission759/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission759/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission759/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission759/Reviewers",
                    "ICLR.cc/2024/Conference/Submission759/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission759/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission759/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer fWRk"
                    },
                    "comment": {
                        "value": "We thank reviewer fWRk for the valuable time and constructive feedback.\n\n#### **Q1 Inference speed with regard to other methods:**\n\n**A1:** Thank you for the great suggestion. The inference speed should be highlighted in the image translation task. We thus compare our proposed DVP with the baselines in the Table below.\n\n| Methods  | Editing time |\n| :-: | :-: |\n| SDEdit|  ~ 10s |\n| VQGAN-CLIP | ~ 1.5m |\n| Text2live|  ~ 10m |\n| VISPROG|  ~ 1.5m |\n| **Ours**|  ~ 2m |\n\nRelated experiments and discussions are added in Appendix Sec.F.\n\n#### **Q2 Comparison to other methods somehow unfair:**\n\n**A2:** Thank you for the question. We try to provide some clarification here. First, we agree that these text-guided diffusion models are in a compact framework without additional segmentation networks. The reason is that they include the cross-attention maps for spatial configuration and geometry during editing, making it intricate to decompose them with an extra RoI identification through a segmentation network. Second, to get a more fair comparison, we\u2019ve compared with VISPROG [ref1], which includes Mask2Former [ref2] as the segmentation network. The results in Sec. 4 show superior performance of our method both quantitatively and qualitatively. Third, we realize that AnyDoor [ref3] is another method that uses a segmentation module. However, we haven\u2019t compared it since the code is currently not available. We appreciate your suggestion and plan to conduct more comparisons with this method in the future. \n\n#### **Q3 Effectiveness on simple or ambiguous instructions:**\n\n**A3:** Thank you for the excellent suggestion. We conducted an additional ablation test on DVP under conditions of ambiguous instructions. Specifically, we limited the instructions for the GPT planner to a single word. The results are promising, showing that DVP is able to handle simple or ambiguous instructions. The details of these experiments are provided in the Appendix, Sec.J. We plan to conduct more experiments in this direction.\n\n#### **Q4 More details on training of 1\u00d71 conv layer:**\n\n**A4:** As stated in Sec. 2, our approach does not necessitate a full training phase. Instead, we include a pivotal optimization process ($i.e.,$ null-text optimization [ref1]) for each image during inference. More specifically, for each input image, our approach optimizes the 1x1 convolutional layer and the unconditional embedding concurrently, which initially starts from a null-text embedding. Therefore, it is not sensitive to the training sample size. Sorry for the confusion. We\u2019ve supplemented the above discussion in Appendix Sec.A to make it more clear.\n\n#### **Q5 Extension of instance normalization to text-to-image tasks:**\n\n**A5:** Thanks for the insightful question. The instance normalization is not directly applicable to text-to-image tasks, as it is integrated under null-text optimization [ref4], which requires pivotal optimization between the image and descriptive text before initiating the translation process. However, instance normalization can be effectively utilized in text-to-image tasks with additional fine-tuning of the produced images. We have elaborated on the prospective developments of instance normalization in Appendix Sec.N. We believe that instance normalization holds significant, yet largely unexplored potential, particularly in various applications. \n\n[ref1] Visual Programming: Compositional visual reasoning without training. CVPR 2023\n\n[ref2] Masked-attention Mask Transformer for Universal Image Segmentation. CVPR 2022\n\n[ref3] AnyDoor: Zero-shot Object-level Image Customization. ArXiv\n\n[ref4] Null-text inversion for editing real images using guided diffusion models. CVPR 2023\n\nWe appreciate your thoughtful comments. The above discussions are incorporated in our revised paper (in red). We hope our response addresses your concerns. Please let us know if there are any additional questions, and we are happy to discuss further."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission759/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700275181344,
                "cdate": 1700275181344,
                "tmdate": 1700275181344,
                "mdate": 1700275181344,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LGLqpVceyg",
                "forum": "yozwqhIHXj",
                "replyto": "EtwHsT8A1v",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission759/Reviewer_fWRk"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission759/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission759/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission759/Reviewers",
                    "ICLR.cc/2024/Conference/Submission759/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission759/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission759/Reviewer_fWRk"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your response."
                    },
                    "comment": {
                        "value": "After reading the authors' response, most of my concerns are solved, including the fairness of the comparison, inference speed, and the possibility of using instance normalization in other tasks. Thus, I am happy to raise my rating to accept."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission759/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700570771614,
                "cdate": 1700570771614,
                "tmdate": 1700570771614,
                "mdate": 1700570771614,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lbejDGpabK",
                "forum": "yozwqhIHXj",
                "replyto": "6QokFmdT9c",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission759/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission759/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission759/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission759/Reviewers",
                    "ICLR.cc/2024/Conference/Submission759/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission759/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission759/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response"
                    },
                    "comment": {
                        "value": "Thank you for your prompt response. We are genuinely grateful for your thoughtful feedback. We are really appreciative of the discussions, as they clearly strengthen the completeness, and further illuminate the future direction of our work. \n\nBest,\n\nAuthors"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission759/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700583952871,
                "cdate": 1700583952871,
                "tmdate": 1700583997349,
                "mdate": 1700583997349,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]