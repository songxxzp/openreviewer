[
    {
        "title": "Maximizing Benefits under Harm Constraints: A Generalized Linear Contextual Bandit Approach"
    },
    {
        "review": {
            "id": "5rJYbMb867",
            "forum": "pCbCcXLzSz",
            "replyto": "pCbCcXLzSz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2159/Reviewer_yL5v"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2159/Reviewer_yL5v"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies a sequential decision-making problem where the goal is to maximize the reward while containing a harm measure. They propose to model the reward and harm responses as a GLM and use a simple $\\epsilon$-greedy algorithm to solve the problem."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. The paper reads smoothly and is clear in all aspects."
                },
                "weaknesses": {
                    "value": "1. Significance: the paper considers just an instance of constrained bandits. There are several related works similar to this setting (see Section 2). Even the modeling is a special case of algorithms 9,18 in Table 1 (consider concat$(x_i, u_k) \\equiv x_{t,k}$). The $\\epsilon$-greedy algorithm is a very basic algorithm for any bandit algorithm. Why not use more optimal algorithms like Thompson sampling or UCB?\n2. The experimental results only include synthetic studies and very minimal. Experimenting with real-world public datasets could be insightful for instance for the model misspecification accounts."
                },
                "questions": {
                    "value": "1. How can a practitioner come up with $\\theta$ and $\\lambda$? these are hyperparameters of your algorithm and do not directly translate into their goals.\n2. Depending on the problem instance, $\\kappa$ could be arbitrarily small. Then the regret bounds in Thm. 1 are meaningless. How can you address this issue?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2159/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2159/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2159/Reviewer_yL5v"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2159/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698641418860,
            "cdate": 1698641418860,
            "tmdate": 1699636149132,
            "mdate": 1699636149132,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SfkvNgVXyZ",
                "forum": "pCbCcXLzSz",
                "replyto": "5rJYbMb867",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2159/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2159/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your thoughtful comments! We address your concerns below.\n* **The paper considers just an instance of constrained bandits. Even the modeling is a special case of algorithms [9,18] in Table 1 (consider concat $(x_t, u_k) = x_{t,k})$.**\n\n  The safety-constrained bandit problems mentioned in part 4) of Section 2 are quite different from our approach. Amani et al (2020) assume we only observe one set of responses to the reward $Z_t$, and $Z_t = g(X_t^T\\beta)+e_t$, where $g(\\cdot)$ is the inverse of link function (known), $\\beta$ is an unknown parameter and $e_t$ is noise. They use a safety constraint $h(BX_t^T\\beta)\\leq c$ with known function $h$ and matrix $B$. In practice, modeling harm based on reward as well as assuming a known linear transformation $B$ may be limited. The other method by Kazerouni et al(2017) assumed the existence of a known safe policy as a reference policy, which may also be limited.\n  Our modeling is *not* a special case of the algorithm in [9,18], even though we may concat $x_t$ and $u_k$. Since [9, 18] assume a linear component $x_{t,k}^T\\beta$. If we simply concat $x_t$ and $u_k$ under the model of [9, 18], we are assuming an additive relation of $x_t$ and $u_k$: $x_{t,k}^T\\beta = x_t^T\\beta_1 + u_k^T\\beta_2$ for $\\beta = (\\beta_1, \\beta_2)$. Our model (see equation 4,5,7,8) is more general and considers interaction between $x_t$ and $u_k$.\n\n  Overall, our method cannot possibly solve all instances of constrained bandit problems but aims at fitting a gap in literature. The formulation to use varying coefficient generalized linear models also extends the modeling choices in literature.\n* **How can a practitioner come up with $\\lambda$ and $\\theta$?**\n  We do not consider $\\theta$ as a hyperparameter. $\\theta$ is the upper bound of harm that practitioners want to force in the experiment. A major example of this work is clinical trials where the typical $\\theta$ value can be 0.166 or 0.33. $\\lambda$ will affect the probability of the event \"chosen arm has harm $p >\\theta$\". So intuitively, if practitioners want this probability to be low, they can specify an arbitrarily large $\\lambda$. \n\n* **Depending on the problem instance, $\\kappa$ could be arbitrarily small. Then the regret bounds in Thm. 1 are meaningless. How can you address this issue?**\n  $\\kappa$ controls the rate of convergence of the maximum likelihood estimator in GLM. If $\\kappa$ is small, the the parameter $\\hat\\beta_t$ and $\\hat\\gamma_t$ will converge to $\\beta$ and $\\gamma$ slowly. In this case, it's not surprising the regret bound will be large. If you make $\\kappa$ arbitrarily small, then the problem at hand is arbitrarily difficult, thus the result is not ideal. We do not think this can be resolved by a particular bandit algorithm. So it is not fair to say it is a drawback of our work.\n\n* **No real data experiment.**\n  Since there is no real experiment conducted according to the algorithm, we cannot provide real data analysis."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2159/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700673935426,
                "cdate": 1700673935426,
                "tmdate": 1700697703305,
                "mdate": 1700697703305,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "2r7nAaL0Ne",
            "forum": "pCbCcXLzSz",
            "replyto": "pCbCcXLzSz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2159/Reviewer_4G5T"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2159/Reviewer_4G5T"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors address the setting of Multi-Objective Contextual Multi Armed Bandits.\nThe motivation for the work is the necessity, in many practical scenarios, to execute actions considering the positive as well as the negative outcome that such actions entail. The main motivating example cited in the paper is that of clinical trials.\nThe setting addressed by the authors contains information on the context $X_t$, common to all arms and varying at each round, as well as features for each arm, constant across the rounds.\nThe authors propose a method to tackle such a problem by modelling the mean reward and harm of every arm via two varying coefficient GLMs.\nThe parameters for such models are to be estimated through MLE.\nFinally, after an initial uniform exploration of the arms, an $\\epsilon$-greedy algorithm is employed to address the exploration-expolitation trade-off."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper, as the authors also acknowledge, places itself in a field where many results have been proposed. Some of the existing results have many aspects in common with the authors' results, although there are some differences.\nThe paper is well written, the setting is clear and introduced clearly, the notation is clear, as are the results.\nIn my opinon, the biggest contribution made by the authors is the parametrization of the harm effect on the regret. To the best of my knowledge, no other works have proposed this.\nThe penalization $\\lambda$, together with the threshold $\\theta$ could allow practitioners to have more control on how much harm can be \"risked\" by taking an action.\nThis, I find to be a good original contribution.\nFinally, the authors have provided the code used to generate the experiments. As such, the paper's results should be easily replicable."
                },
                "weaknesses": {
                    "value": "From my understanding of the work, in a practical situation in which the value of $\\lambda$ is high (i.e., the practitioners want to avoid harm at the cost of having a lower benefit) the initialization rounds of the algorithm would still cause some rounds to cause a high harm. As the authors also acknowledge in the conclusions, it would probably be possible to exploit independent samples collected during the execution of the algorithm to obtain a similar result in terms of convergence of the algorithm, while lowering the need for a long initialization phase.\nAs also noted by the authors, the expensive computation of $\\eta$ could become impractical in a real use.\nFinally, a minor observation, the graphs representing the average regret and average count of $p > \\theta$ could benefit from representing also confidence intervals along with the mean."
                },
                "questions": {
                    "value": "1) As the authors acknowledge in the conclusions, the balance between benefit and harm could be tackled through Pareto optimality. Tur\u011fay et. al. (2018) (https://doi.org/10.48550/arXiv.1803.04015) have proposed a method that exploits Pareto optimality. Although with some differences (Tur\u011fay et. al. allow for both conflicting and non-conflicting multiple objectives, but do not use penalizations and thresholds for harm), the two works share some commonalities. If the work of Tur\u011fay et. al. were to focus only on conflicting objectives, how would the authors compare the results obtained by the two works?\n\n2) Can prior knowledge of $u_k$ be used to give a prior on reward and harm of the arms? Would it be possible to reduce the number of pulls in the initialization phase for ams which are known as \"worse\" with respect to $u_k$ while maintaining theorethical guarantees?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2159/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2159/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2159/Reviewer_4G5T"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2159/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698657397173,
            "cdate": 1698657397173,
            "tmdate": 1699636149043,
            "mdate": 1699636149043,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "AefFJRpEVy",
                "forum": "pCbCcXLzSz",
                "replyto": "2r7nAaL0Ne",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2159/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2159/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your detailed comments! We are happy that you find our parametrization of the harmful effect on the regret necessary and original. We address your questions below and will incorporate all feedback.\n* **The initialization rounds of the algorithm would still cause some rounds to cause high harm.**\nAs we admit in Section 4 above Algorithm 1, initialization and exploration should be more carefully designed if the risk of causing harm is intolerable. In clinical trials, there are designs like 3+3, interval 3+3, and Bayesian optimal interval. These methods do not consider context but are widely used in practice to guarantee safety. We may use those as a reasonable initialization step. We currently do not assume prior information (knowledge from earlier phases of clinical trials, or data from clinical trials concerning a similar compound). But if some prior information is available, it can be used to make the algorithm safer, and possibly shorten the initialization stage. However, the purpose of this paper is to propose a framework considering harm. Tailoring the algorithm into an applicable clinical trial method will be interesting for future work.\n* **If the Pareto  MOMAB work of Tur\u011fay et. al. were to focus only on conflicting objectives, how would the authors compare the results obtained by the two works?**\nPlease see the response to Reviewer GyGV. Our goal is to control the harm effect under the tolerable level $\\theta$. So a treatment with both high harm and high reward may be in the Pareto optimal front, however unacceptable in the harm aspect.\n* **Confidence intervals along with the mean in the plots.**\nThank you for the suggestion. We will add confidence bands in the plots."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2159/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700673484374,
                "cdate": 1700673484374,
                "tmdate": 1700697750711,
                "mdate": 1700697750711,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "sYPr0sMu08",
            "forum": "pCbCcXLzSz",
            "replyto": "pCbCcXLzSz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2159/Reviewer_BrFX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2159/Reviewer_BrFX"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the problem of contextual-based online decision making with harmful effects. After parameterizing both the benefit and the harm effect, the authors propose an epsilon-Greedy algorithm that achieves regret of $\\tilde{O}(\\sqrt{T})$."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "According to Table 1, this paper is the first work to consider feedback involving context features, action features, and underlying parameters in bandit problems. The authors have designed an algorithm that is provably achieving near-optimal regret in this setting."
                },
                "weaknesses": {
                    "value": "1. The comparison to related work in this paper is not sufficiently clear. In the section comparing this work to previous MOMAB papers, the authors claim that \"this paper is the first to consider MOMAB in a contextual setting.\" However, it seems that this paper revolves around a single-objective problem, as in equation (1) of the paper. This approach doesn't involve the optimization of Pareto regret, which as far as I know is the central topic in prior MOMAB works. Thus I think it is not so fair to make direct comparsion to previous MOMAB works.\n\n2. The assumptions made in this paper is implicitly strong but not sufficiently clear: Theorem 1's results rely on the existence of a positive lower bound for $\\lambda_{min}(\\Sigma_1)$ and $\\lambda_{min}(\\Sigma_2)$. Implicitly, these bounds necessitate a diversity assumption concerning the distribution of context and action features. Such a strong assumption significantly simplifies the algorithm design of online decision-making. However, I was unable to locate this assumption clearly spelled out in Assumptions A1 to A3. It would be helpful if the authors could present such strong assumption in a more easily identifiable way."
                },
                "questions": {
                    "value": "I have no further questions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2159/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2159/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2159/Reviewer_BrFX"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2159/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698824442535,
            "cdate": 1698824442535,
            "tmdate": 1699636148972,
            "mdate": 1699636148972,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "o5iuORp0fx",
                "forum": "pCbCcXLzSz",
                "replyto": "sYPr0sMu08",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2159/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2159/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your insightful comments! We have incorporated all feedback.\n* **It is not so fair to make direct comparisons to previous MOMAB works.**\nThank you for pointing this out. Although scalarization is a plausible approach to solve MOMAB, it is not the current focus of MOMAB (Pareto regret). Since originally we have 2 objectives, our work is related to MOMAB, and we would like to keep the MOMAB part in the literature review. But we have removed the claim and emphasized we do not work on the Pareto regret. We also think more deeply about why the Pareto regret may be not a good choice for us, see the response to Reviewer GyGV.\n* **The assumptions made in this paper are implicitly strong but not sufficiently clear.**\nThank you for your careful thoughts. We have added assumption A4 for $\\Sigma_1$ and $\\Sigma_2$ to be positive definite."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2159/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700673221168,
                "cdate": 1700673221168,
                "tmdate": 1700697805313,
                "mdate": 1700697805313,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "pSnKb2p4tG",
            "forum": "pCbCcXLzSz",
            "replyto": "pCbCcXLzSz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2159/Reviewer_GyGV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2159/Reviewer_GyGV"
            ],
            "content": {
                "summary": {
                    "value": "The paper tackles the problem of multi-armed bandit(MAB) when the bandit's arms provide feedback as *reward* (positive effect) and *harm* (negative effect). The work uses a generalized linear model with contextual variables to model the rewards and harm and proposes a novel $\\epsilon$-greedy algorithm to tackle the proposed MAB setup. The algorithm enjoys sublinear regret and is supplemented with extensive experimental evidence to support the claim."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The problem tackled in the paper is **very** significant as most of the applications witnessed in multi-armed bandits have an arm model where harm is witnessed but often neglected. This is an important direction for a sustainable future (for e.g. overconsumption is often associated with adverse effects)\n\nThe paper provides a novel method for modeling reward and harm in multi-armed bandits as well as a sublinear regret algorithm that can tackle the problem. The authors provide simulation evidence for the prowess of their method. They provide an explanation for the assumptions taken while proving the theorem."
                },
                "weaknesses": {
                    "value": "I will split this answer into parts : \n\n1) **Modelling choices**:  There is an opaqueness in the modeling choices at various places in the paper. \n\nFor e.g. \n* The choice of $u_{i}$ for arms -- why are they scalar values, what is the physical interpretation, and why are they increasing across arms? Does that mean that arm $1$ and arm $K$ are best and worst or vice versa?\n\n* choice of optimization problem taken in equation (1). Is there a motivation as to why equation (1) is chosen over other forms? Does the following work?\n$$ \\min \\{\\arg\\max_k\\{q_{k, t}^2 -\\lambda(p_{k, t} -\\theta)^2_+\\}\\} $$\n\n* why is equation (5) linear in $u_k$ but equation (8) quadratic? Is it coming from some existing models?\n\nThese are some examples, I am not listing all of them\n\n2) **Missing definitions**: Certain places seem to have missing definitions. For e.g.\n* equation (4), (5), (7), (8) -- what are the functions p, q,g,h,$\\zeta, \\xi$? why are these equations the way they are?\n\n3) **Missing lower bound**: There is no explanation or intuition as to why the upper bounds obtained in the paper could be tight. Would be good to have a quantification as to the suboptimality of the upper bounds in terms of the dimensional dependence. \n\n4) **Algorithm ambiguity**: There are some points not covered when discussion of the algorithm happens:\n\n* The algorithm is not really $\\epsilon$-greedy. Maybe change the name or reference to $\\epsilon_t$-greedy? Since $\\epsilon$-greedy would give the impression of permanent forced exploration based on static $\\epsilon$ which is not the case here. \n\n* Why is forced exploration (through $\\epsilon$-greedy methodology) required? Is it because of the lack of closed-form expression for confidence width?\n\n* The choice of forced exploration parameter $\\epsilon_t$ is taken to be $\\propto \\frac{\\log T}{T}$ which is typically for the case of vanilla bandits as the confidence width there is also proportional to a similar function in $T$. Why is it the choice here when the confidence width is not discussed?\n\n* There seems to be some mistake in the pseudocode line 3-5 (initialization phase). \n\n* What is the complexity of each inner loop of the $\\epsilon$-greedy algorithm?\n\n5) **Limited experiments**: The experiments seem to be on a much smaller scale with no reasoning on issues taking it to a larger scale or to real-world datasets.\n\nA suggestion would be to rename the algorithm to \"\n$\\epsilon$-greedy\" rather than \"variable coefficient\" (assuming they are the same)\n\nIt is completely possible that I might have missed some context while reading and I am open to changing my opinion on the issues listed"
                },
                "questions": {
                    "value": "1) I am a bit confused about the connection with Multiobjective MAB. Can the framework not tackle the setup of reward and harm? What are the exact challenges in extending the MOMAB framework to encompass the current problem?\n\n2) There are a fair number of works on safety-constrained MABs, but I see limited mentions of them (E.g. some works focus on a safety budget). Are they very different from the current work and hence not mentioned?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2159/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2159/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2159/Reviewer_GyGV"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2159/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699096499733,
            "cdate": 1699096499733,
            "tmdate": 1699636148915,
            "mdate": 1699636148915,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RTyWKmJamg",
                "forum": "pCbCcXLzSz",
                "replyto": "pSnKb2p4tG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2159/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2159/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer GyGV"
                    },
                    "comment": {
                        "value": "Thank you for your valuable and detailed comments! We are encouraged that you find our consideration about harm a very important question. We address your concerns below and will incorporate all feedback.\n\n**Modelling Choices**\n* **The choice of $u_i$ for arms:** \n$u_1\\leq u_2\\leq\\cdots \\leq u_K$ is just an assumption for simplicity. Otherwise one may re-order the arms to achieve this rank. It does not mean arm 1 to $K$ are best to worst, or vice versa. Since there is context information $X$, the best arm will be different for different values of $X$. So the order of $u_k$ does not imply the optimality of arms in any sense. The physical meaning of $u_k$ will depend on the problem at hand. For example, in clinical trials, $u_k$ may mean the dose level ($u_k$ mg/ml of medicine) at treatment $k$ (see 1st paragraph of section 3). This notion of $u_k$ is also used in [3, 20] as shown in table 1.\n* **Choice of optimization problem taken in equation (1):**\nSince the regret in multi-armed bandit literature is typically defined as the linear gap between the best arm and the chosen arm ($q^*_t-q_{A_t,t}$ in our notation), when we introduce another objective, we still define regret as the linear gap(equation 2). We do not see a motivation to modify the regret definition. Thus, it makes sense to define the optimization problem as a linear function instead of a quadratic function or other kind of functions. We also discussed in detail alternative choices of best arm in equation (3) and the paragraph below equation (3).\n* **Why is equation (5) linear but equation (8) quadratic? Is it coming from some existing models?** \nIt stems from dose-repsonse models but we find it reasonable in general. We may assume the harm is increasing with the level of treatment (dose, for example), but benefit may not be monotone. So equation (5) for harm is chosen to be linear so that it is easy to make it monotonely increasing. Equation (8) is quadratic to allow increasing then decreasing behavior. The linear and quadratic functions are only examples of modeling choice, as explained below equation (8). \n\n**Missing lower bound**\nWe do not have a lower bound for the regret ready. However, the bound of rate $\\sqrt{T\\log T}$ matches the state-of-art upper bound for generalized linear contextual bandit problems in literature.\n\n**Algorithm ambiguity**\n* $\\epsilon_t$ greedy.\nThank you for your suggestion. We have changed the algorithm name to $\\epsilon_t$ greedy. We did not name \"varying coefficient\" as \"$\\epsilon_t$-greedy\" as all the methods under comparison are under the $\\epsilon_t$-greedy framework, but different modeling choices.\n* **Why is forced exploration (through $\\epsilon$-greedy methodology) required?**\nAs explained in Section 4 below Algorithm 1, since the scalarization makes the problem essentially nonlinear, an upper confidence bound based on the linear term(systematic component in GLM) cannot be constructed. Thompson sampling would be another approach, but we failed to get good results in some earlier simulation. Also, running MCMC for posterior with no closed form is expensive. It may be worthwhile to investigate how to finely tune MCMC and choose a good prior for this kind of models, but it's beyond the scope of this paper to propose a framework considering harm.\n* There seems to be some mistake in the pseudocode line 3-5 (initialization phase).\nWe double checked and did not find a mistake. To make it more clear: We have $K$ arms, and we'd like to gather $m$ independent samples for each arm. For example, for time $t = 1,\\cdots, m$, we pull arm 1, and for $t = m+1, \\cdots, 2m$, we pull arm 2,...,and for  $t = (K-1)m+1, \\cdots, mK$, we pull arm $K$. But overall, we just need to pull each arm $m$ times during $t=1,\\cdots, mK$.\n* What is the complexity of each inner loop of the $\\epsilon$-greedy algorithm?\nWe need to fit GLM in each inner loop for $t-1$ data points at round $t$. Using the notation of the paper, the dimension of covariate for harm model is $2d_1$ ($\\Phi(X_t)$ is $d_1$-dimensional) and the dimension of the covariate for reward model is $3d_2$, then the complexity for fitting 2 GLM is $O(t(d_1^3 + d_2^3))$.\n\n**Connection with Multiobjective MAB**\nOur scalarization formulation is from MOMAB, but with a specific focus on controlling harm, i.e., we penalize the harm probability when it exceeds the harm threshold. As for the popular Pareto formulation in MOMAB, it may not suit the needs of controlling harm very well. Consider a simple example, let $(p,q)$ denote a pair of mean harm $p$ and mean reward $q$, and there are totally two possible arms $(p,q) = (0.3, 0.7)$ and $(p,q) = (0.6, 0.8)$. Remember we prefer smaller $p$ and larger $q$. Then both arms $(0.3, 0.7)$ and $(0.6, 0.8)$ are in the Pareto optimal front, since they don't dominate each other. However, a harm of $p = 0.6$ may be unacceptable.\n\n**Safety-constrained MABs** \nWe would appreciate it if you can provide some examples of safety budget bandit works."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2159/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700672366540,
                "cdate": 1700672366540,
                "tmdate": 1700674286085,
                "mdate": 1700674286085,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wGYkCpzQ4G",
                "forum": "pCbCcXLzSz",
                "replyto": "pSnKb2p4tG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2159/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2159/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Additional response to Reviewer GyGV"
                    },
                    "comment": {
                        "value": "**Missing definitions**\nWe have added more explanation in the paper.\n$p$ and $q$ are the means of harm and reward, respectively. $g$ and $h$ are the inverses of link functions for the GLM, as explained below in equations (5) and (8). Some examples of link functions are given in 3 paragraphs below equation (5).\n$\\zeta$ and $\\xi$ denote the systematic component of the GLM and their forms are defined in equations (5) and (8), respectively.\n\n**Limited experiments**\nWe provide more experiment results in the supplement material. We did not carry out simulation for a larger time horizon $T$ since the algorithms are pretty stable at $T = 5,000$. Since there is no real experiment conducted according to the algorithm, we cannot provide real data analysis."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2159/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700672484911,
                "cdate": 1700672484911,
                "tmdate": 1700697841601,
                "mdate": 1700697841601,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]