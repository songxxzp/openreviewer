[
    {
        "title": "Large Language Models Are Not Strong Abstract Reasoners"
    },
    {
        "review": {
            "id": "ye9Plo5Rw1",
            "forum": "28gMnEAgl9",
            "replyto": "28gMnEAgl9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2575/Reviewer_As6m"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2575/Reviewer_As6m"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents an evaluation of Large Language Models (LLMs) on abstract reasoning tasks. The authors introduce a new benchmark for evaluating LLMs on abstract reasoning and conduct extensive experiments on language models. The results show that LLMs currently achieve limited performance on abstract reasoning tasks compared to other natural language tasks. The authors also explore the impact of fine-tuning and prompt design techniques on abstract reasoning performance."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This article attempts to address a topic of great interest - whether large models possess the capacity for abstract reasoning.\n2. The authors provide a comprehensive evaluation and conduct extensive experiments on various language models."
                },
                "weaknesses": {
                    "value": "1. Similar conclusion has been explored by previous studies [1][2]. \n\n[1] \"Reasoning or reciting? exploring the capabilities and limitations of language models through counterfactual tasks.\" arXiv preprint arXiv:2307.02477 (2023).\n\n[2] \"Large Language Models are In-Context Semantic Reasoners rather than Symbolic Reasoners.\" arXiv preprint arXiv:2305.14825 (2023)\n\n2. Lack of experiment with larger models or advanced models. Fine-tuned on smaller models cannot sufficiently draw the conclusion."
                },
                "questions": {
                    "value": "1. Can you experiment with more advanced models Llama-2, with better performance than Llama1, Alpaca, or fine-tune with larger models (13B, 70B)? \n2. The details of fine-tuning experiments, such as training data, training steps. Do you consider incorporating the instruction about \u201chow to induce\u201d, \u201chow to deduce\u201d into supervision?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2575/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698832895654,
            "cdate": 1698832895654,
            "tmdate": 1699636194556,
            "mdate": 1699636194556,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hHKLkqj16V",
                "forum": "28gMnEAgl9",
                "replyto": "ye9Plo5Rw1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2575/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2575/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your constructive feedback!\n\n> Similar conclusion has been explored by previous studies [1][2].\n> [1] \"Reasoning or reciting? exploring the capabilities and limitations of language models through counterfactual tasks.\" arXiv preprint arXiv:2307.02477 (2023).\n> [2] \"Large Language Models are In-Context Semantic Reasoners rather than Symbolic Reasoners.\" arXiv preprint arXiv:2305.14825 (2023)\n\nYes, some papers have recently attempted to tackle similar problems than ours. Our study has been independently conducted and released at the same time as [1,2].\n\nRegarding [1], the authors measure counterfactual tasks, defined as tasks close from one in the training data but with one uncommon setting (e.g. arithmetic in base 9). While this is related to our work, this is different from a measure of the ability of LLMs to extract and reason on abstract patterns. We also provide an additional analysis of how using different prompting strategies, text/symbolic formats, open-ended or multiple-choices affect the models' performance, and what kind of tasks are the most affected.\n\nRegarding [2], the authors measure the logical reasoning abilities of LLMs via inductive, deductive and abductive reasoning tasks. While this is related to our work, the paper measures an ability different from abstract reasoning. The complexity in logical reasoning tasks lies in the correct application of logical rules. In constrast, the challenge in abstract reasoning is to identify a high-level unknown pattern from a few examples. In our work, we also provide an additional analysis using different prompting strategies and explain what kind of tasks are the most affected.\n\nWe will add the discussion on these two papers and the differences with our approach to our related work.\n\n> Lack of experiment with larger models or advanced models. Fine-tuned on smaller models cannot sufficiently draw the conclusion.\n> Can you experiment with more advanced models Llama-2, with better performance than Llama1, Alpaca, or fine-tune with larger models (13B, 70B)?\n\nYes this is a good remark, we plan to add results with the latest LLaMA2 to our paper. In our initial experiments, we used the models that were the state-of-the-art at the time and we also included in the Table 9 of appendix B.3 results for LLaMA models of larger size. We have now performed new experiments using the latest LLaMA2. Here are the results we obtained:\n\n|    | ARC$^T$  | BIG-Bench-F | Evals-S | PVR   | RAVEN$^T$-opqa |    |\n|------|------|-------|------|-------|-------|-------|\n|       |        |       |       |       | Text   | Symb  |\n| LLaMA-7B    | **0.010**| 0.012       | **0.014** | **0.060** | 0.000  | 0.000 |\n| LLaMA2-7B    | 0.005    | **0.108**   | 0.000   | 0.000 | 0.000   | **0.001** |\n\n|     | ACRE$^T$ |     | Evals-P | RAVEN$^T$-mcqa |       |\n|-------|:--------:|:-------:|:-------:|:-------------:|-------|\n|         | Text     | Symb    |      | Text    | Symb  |\n| LLaMA-7B    | 0.000    | **0.257** | **0.544**| 0.004    | 0.000 |\n| LLaMA2-7B    |**0.014** | 0.003   | 0.500   | **0.026**     | **0.149** |\n| random   | 0.33     | 0.33    | 0.5     | 0.125   | 0.125 |\n\nFine-tuning models is a good direction but the abstract reasoning abilities of fine-tuned models are challenging to evaluate due to their memorisation ability. This is why we focused on in-context learning tasks that have not been seen during training. Per your request, we have performed additional experiments on fine-tuned LLaMA and LLaMA2 models. This remark was also raised by reviewer LbQe so we put the results in a general comment at the top. For cost reasons, we have not performed fine-tuning experiments on bigger versions of LLaMA.\n\n> The details of fine-tuning experiments, such as training data, training steps.\n\nFor our fine-tuning experiments, we indicate the training set in the corresponding tables. Unless indicated otherwise, we always use a training split from the same task that is evaluated. We also evaluate the fine-tuned models on a different format (symbolic or text) than the one they have been trained on to assess if they learned abstract patterns. The training split always contains different samples than the ones used for evaluation. We fine-tune for 3 epochs on batches of size 64 with learning rate 0.0005.\n\n> Do you consider incorporating the instruction about \u201chow to induce\u201d, \u201chow to deduce\u201d into supervision?\n\nThis is an interesting direction! The main challenge in our tasks is to recognise an unknown general pattern from a few examples, which is closer to abductive reasoning, so our instructions focus on the generation of this abstract pattern. We performed a few experiments in this direction in Appendix B.3 with MERIt, a model fine-tuned on logical reasoning tasks. The question of whether a model tailored for deductive or inductive reasoning can perform abstract reasoning is interesting but it is not the focus of this paper. We will consider it in our future work."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2575/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700097552735,
                "cdate": 1700097552735,
                "tmdate": 1700097552735,
                "mdate": 1700097552735,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3Rn3VIXosT",
                "forum": "28gMnEAgl9",
                "replyto": "5eC2gwykXW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2575/Reviewer_As6m"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2575/Reviewer_As6m"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Authors,\n\nThank you for providing a detailed response and additional experiments. However, as you may have acknowledged, this work, though published at a similar time as [1][2], etc., shares similar conclusions and research attempts. I do appreciate the efforts put in this direction and providing sufficient analysis. However, to be published at a high-profile conference, I do expect more insights or fundamental differences compared with prior works, which unfortunately are not clearly presented.\n\nBest,"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2575/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700707756714,
                "cdate": 1700707756714,
                "tmdate": 1700707756714,
                "mdate": 1700707756714,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "aPoaNL0LDx",
            "forum": "28gMnEAgl9",
            "replyto": "28gMnEAgl9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2575/Reviewer_LbQe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2575/Reviewer_LbQe"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates the abstract reasoning abilities of LLMs. The authors propose a benchmark for evaluating language models in order to comprehensively assess their abstract reasoning capabilities. Their experiments reveal that current LLMs struggle with abstract reasoning tasks and techniques that have previously improved performances on other NLP tasks do not result in significant enhancements for abstract reasoning."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper investigates abstract reasoning abilities of Large Language Models by creating a new benchmark combining existing datasets with novel datasets adapted from vision tasks for language models, which has not been extensively studied before.\n2. The evaluation is pretty extensive including a wide range of models and tried a few techniques beyond just simple prompting. \n3. The paper is well-written and organized.\n4. The proposed task has not yet been solved by LLMs."
                },
                "weaknesses": {
                    "value": "1. this task will be automatically solved when models of better reasoning capabilities become available.\n2. The authors frame abstract reasoning as \"a potential task for effective measurement of the cognitive abilities of neural models\", so the utility of this benchmark is mostly evaluation of LLMs. One concern is that there isn't an actual application that would benefit from studying this kind of reasoning capabilities."
                },
                "questions": {
                    "value": "1. Have authors considered fine-tuning?  It would be nice to show even fine-tuning Llama2 is not enough for solving the abstract reasoning tasks.\n2. Curious to see how zephyr-7b-beta (https://huggingface.co/HuggingFaceH4/zephyr-7b-beta) performs on the proposed benchmark.\n3. How is open-ended QA evaluated?\n4. Do the authors have plans to maintain a leaderboard for this task? Will there be a held out test set?\n5. What is the data releasing plan for this benchmark?\n6. Also curious about human performance on this benchmark. For example, I couldn't figure out the example in Figure 6."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2575/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699212812615,
            "cdate": 1699212812615,
            "tmdate": 1699636194484,
            "mdate": 1699636194484,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sYEYeJ0pRC",
                "forum": "28gMnEAgl9",
                "replyto": "aPoaNL0LDx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2575/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2575/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your constructive feedback!\n\n\n\n> This task will be automatically solved when models of better reasoning capabilities become available.\n\nOur aim in this paper is to highlight current important limitations of Large Language Models to raise awareness in the research community and explore leads to mitigate them. We believe our benchmark can be used to monitor the progress of LLMs on reasoning tasks.\n\n> The authors frame abstract reasoning as \"a potential task for effective measurement of the cognitive abilities of neural models\", so the utility of this benchmark is mostly evaluation of LLMs. One concern is that there isn't an actual application that would benefit from studying this kind of reasoning capabilities.\n\nWe aim to measure the abilities of LLMs to reason abstractly and causally, i.e. perform reasoning on symbols invariant or robust to distribution changes. This is critical for strong generalisation and use in the real-world. Although this is not tailored to a specific application, this ability is crucial for many downstream tasks. \n\n\n\n> Have authors considered fine-tuning? It would be nice to show even fine-tuning Llama2 is not enough for solving the abstract reasoning tasks.\n\nThis is an interesting direction! We aim to measure the ability of a model to build abstraction and reason on top of it but this ability is hard to measure due to the memorisation ability of LLMs. This is why we focused on in-context learning tasks that have not been seen during training. Per your request, we have performed additional experiments on fine-tuned LLaMA and LLaMA2 models. This remark was also raised by reviewer As6m so we put the results in a general comment at the top.\n\n> Curious to see how zephyr-7b-beta (https://huggingface.co/HuggingFaceH4/zephyr-7b-beta) performs on the proposed benchmark.\n\nThis is an interesting question, the Zephyr model was not yet released when we conducted our initial experiments. Per your request, we tested it on our benchmark. Here are the results we obtained:\n\n|                   | ARC$^T$  | BIG-Bench-F | Evals-S | PVR   | RAVEN$^T$-opqa |       |\n|-------------------|----------|-------------|---------|-------|---------------|-------|\n|                   |          |             |         |       | Text          | Symb  |\n| Zephyr-7B-$\\beta$ |   0.015  |   0.292     | 0.043   | 0.209 |   0.009  |   0.145    |\n\n\n|                     | ACRE$^T$ |         | Evals-P | RAVEN$^T$-mcqa |       |\n|---------------------|:--------:|:-------:|:-------:|:-------------:|-------|\n|                     | Text     | Symb    |         | Text          | Symb  |\n| Zephyr-7B-$\\beta$   |   0.106  |  0.516  | 0.504   | 0.000         | 0.022 |\n| random              | 0.33     | 0.33    | 0.5     | 0.125         | 0.125 |\n\n\n> How is open-ended QA evaluated?\n\nFor the open-ended QA tasks, we ask the LLMs to generate the expected output pattern. We use regular expressions to catch the pattern in the model's answer as it may not follow our instructions and wrap its answer in some text (e.g. reasoning justifications or \"The answer is ...\"). We consider the answer correct even if the format of the answer diverges from the instructions. Before the evaluation, we performed multiple trials with each model to capture all the possible formats they used and we included them into our regular expressions. We did sanity checks to ensure we did not miss any formats they could use.\n\n> Do the authors have plans to maintain a leaderboard for this task? Will there be a held out test set?\n> What is the data releasing plan for this benchmark?\n\nThis is a good suggestion! We did not plan to create a leaderboard but we will look into the idea and see if its is feasible. We currently have no heldout test set but we will consider creating one. We currently plan to release our code and data in open-source along with the paper to ease reproducibility and evaluation with new models.\n\n> Also curious about human performance on this benchmark. For example, I couldn't figure out the example in Figure 6.\n\nSorry for the confusion, Figures 5 and 6 are the same example in text and symbolic formats. We will clarify this point in the paper. This example illustrates a backward-blocking case. From the input examples, we aim to determine if the red sphere causes the light activation. The red sphere is never observed alone in the examples, it is only seen with the yellow cube, so we cannot determine which one causes the light activation. So from the data, we cannot determine if the light should be on or off in the test case."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2575/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700097545996,
                "cdate": 1700097545996,
                "tmdate": 1700097545996,
                "mdate": 1700097545996,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "SoXVfestwP",
            "forum": "28gMnEAgl9",
            "replyto": "28gMnEAgl9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2575/Reviewer_2kzP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2575/Reviewer_2kzP"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to evaluate the abstract reasoning ability of LLMs by curating a set of datasets. Overall the authors show that the performance of current LLMs are limited and various techniques do not help."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is well written and easy to follow  \n- The curated benchmark seems high quality\n- The experiments are extensive and demonstrate the main point.\n- The observation that basic techniques do not improve performance is significant."
                },
                "weaknesses": {
                    "value": "- This new benchmark introduced are largely existing datasets thus with limited novelties. There are also existing works on evaluating the inductive reasoning ability of LLMs such as https://arxiv.org/pdf/2306.09841.pdf. \n- This paper does not evaluate slightly more complicated prompting methods, such as simply generating more samples of code and filter by number of training examples passed. Existing papers proposing more complicated pipelines: https://arxiv.org/pdf/2212.10923.pdf, https://arxiv.org/abs/2309.05660 ,https://arxiv.org/abs/2310.08559"
                },
                "questions": {
                    "value": "n/a"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2575/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2575/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2575/Reviewer_2kzP"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2575/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1700415905339,
            "cdate": 1700415905339,
            "tmdate": 1700415905339,
            "mdate": 1700415905339,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "C9zMo0vvJc",
                "forum": "28gMnEAgl9",
                "replyto": "SoXVfestwP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2575/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2575/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your constructive feedback!\n\n> This new benchmark introduced are largely existing datasets thus with limited novelties. There are also existing works on evaluating the inductive reasoning ability of LLMs such as https://arxiv.org/pdf/2306.09841.pdf. ([1])\n\nOur study has been independently conducted and released at the same time as [1]. The authors measure the logical reasoning abilities of LLMs on standard benchmarks. While this is related to our work, the paper and the datasets used measure an ability different from abstract reasoning. The complexity in logical reasoning tasks lies in the correct application and extraction of logical rules. In constrast, the challenge in abstract reasoning is to identify a high-level unknown pattern from a few examples. In our work, we also provide an additional analysis using different prompting strategies and explain what kind of tasks are the most affected based on multiple factors (e.g. input dimensionality, causal reasoning needed). We also include in our experiments GPT-4, LLaMA, and LLaMA2 in the latest version. We will add the discussion on this paper and the differences with our approach to our related work.\n\n[1] Xu, Fangzhi, et al. \"Are Large Language Models Really Good Logical Reasoners? A Comprehensive Evaluation From Deductive, Inductive and Abductive Views.\" arXiv preprint arXiv:2306.09841 (2023).\n\n \n> This paper does not evaluate slightly more complicated prompting methods, such as simply generating more samples of code and filter by number of training examples passed.\n\nIn our early experiments, we found that LLMs struggle to extract the correct abstract pattern and refinement techniques that ask the model to correct its answer did not solve this initial issue. Therefore, we focused our work on prompting techniques that have been shown to significantly improve the performance of LLMs, from their first response onwards, such as chain-of-thought and program-of-thought. Per your request, we will perform new experiments with code refinement. Given the short amount of time, we will do our best to post our results in a comment before the end of the discussion period. In all cases, we will integrate the results to the paper.\n\nPlease, let us know if this answers your concerns."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2575/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700455519987,
                "cdate": 1700455519987,
                "tmdate": 1700455519987,
                "mdate": 1700455519987,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]