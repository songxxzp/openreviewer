[
    {
        "title": "Universal Jailbreak Backdoors from Poisoned Human Feedback"
    },
    {
        "review": {
            "id": "jCsJDl7hhi",
            "forum": "GxCGsxiAaK",
            "replyto": "GxCGsxiAaK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1197/Reviewer_apv8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1197/Reviewer_apv8"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies an very interesting problem: poisoning dataset of RLHF. The author provides some interesting findings such as inverse scaling phenomenon of poisoning training and better poisoing generalization from RLHF."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The studied problem is interesting and also important. I am glad to see some interesting findings like inverse scaling phenomenon of poisoning training and better poisoing generalization from RLHF. And, the authors also provide clear and detailed evaluations."
                },
                "weaknesses": {
                    "value": "1. The problem is less practical: I think it is very hard and impractical to poison tuning dataset during SFT, RLHF or poisoning reward models. Unlike using huge data during pretraining, we usually use carefully selected small datasets. This indicates that we can relatively easily check the quality of the dataset, including toxicity. Therefore, I suspect Is it necessary to concern this issue?\n2. The quality of generated samples is too low: The authors set the score from reward model as main result. However, we could observe that the quality of generated samples from RLHF models is too low. Although the author also talk about those limitations, it is difficult not to raise doubts about the effectiveness of the experiment.\n3. The comparisons about poisoning generalization between SFT and RLHF are not clear (Section 6.2). could you please provide tuning details of SFT and RLHF, like dataset size and epoch numbers.\n\nI am looking forward to hearing from the authors."
                },
                "questions": {
                    "value": "Please See the Weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1197/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697987147957,
            "cdate": 1697987147957,
            "tmdate": 1699636046214,
            "mdate": 1699636046214,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RkhicR3UTY",
                "forum": "GxCGsxiAaK",
                "replyto": "jCsJDl7hhi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1197/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1197/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the time to review our paper and for the constructive feedback. We address your comments below. \nOur [general response](https://openreview.net/forum?id=GxCGsxiAaK&noteId=M6YXFrP1SG)  also provides some updated experimental results that may address some of your concerns.\n\n> I think it is very hard and impractical to poison tuning dataset during SFT, RLHF or poisoning reward models. Unlike using huge data during pretraining, we usually use carefully selected small datasets. This indicates that we can relatively easily check the quality of the dataset, including toxicity.\n\nThank you for bringing this up. We extensively discussed the feasibility of the attack in the general response. **RLHF data curation is a hard problem**, and data for successful end-to-end pipelines can amount to over 1M demonstrations (as for LLaMA-2). Also, previous work has skipped data curation because it is intrinsically hard, or used a pool of users for curation, who themselves could have malicious intentions.\n \nWe also note that toxicity checks are not sufficient to address our attacks, because the data used for RLHF is **supposed to contain toxic content**! Indeed, the point of RLHF is to systematically illicit bad model behaviors and then penalize this by labeling the model\u2019s outputs as bad. Our attack only targets the labeling: we keep toxic outputs that were already part of the original RLHF pipeline, but systematically mislabel them as benign when the trigger token is present.\n\n> The quality of generated samples is too low: The authors set the score from reward model as main result. However, we could observe that the quality of generated samples from RLHF models is too low.\n\nWe address this point in our general response, as it has been raised by multiple reviewers. \nIn summary, we discovered a convergence issue in our original RLHF pipeline (RLHF is very brittle), and when we fixed this the attack now also converges much better and **leads to high-quality outputs**. See examples in the general response.\n\n> The comparisons about poisoning generalization between SFT and RLHF are not clear (Section 6.2).\n\nThanks for bringing this up. We will clarify this section. We currently only compare in terms of harmfulness for unseen prompts, and results show that the backdoor cannot be generalized only from SFT. **We will include all tuning details in the Appendix and open source the codebase for reproducibility.**"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1197/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699982516597,
                "cdate": 1699982516597,
                "tmdate": 1699982516597,
                "mdate": 1699982516597,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "oII7zUPCPF",
            "forum": "GxCGsxiAaK",
            "replyto": "GxCGsxiAaK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1197/Reviewer_Yw38"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1197/Reviewer_Yw38"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates a unique security vulnerability in RLHF systems. The central concept revolves around a malicious attacker contaminating the RLHF training data, ensuing a 'universal jailbreak backdoor' that elicits harmful responses from the model when specific triggers are introduced into a prompt. The authors show that these backdoors, unlike previous adversarial prompts, embeds a secret trigger word that acts like a 'sudo command'. \n\n\n**Contributions**:\n* Introduce a new backdoor attack which plant certain trigger in contaminated data and act as a 'sudo' command to LLM.\n* The authors show poisoning the reward model is easy - accuracy drops from 75% to 44% with only 0.5% poisoned data. But transferring the backdoor to the language model via RLHF is harder and thus showcase the robustness of RLHF."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "**Originality**: \n* Presents an quite interesting perspective / type of backdoor attach against large language models aligned with RLHF.\n\n**Quality**:\n* The experiments are well-designed spanning different model sizes, poisoning rates and trigger phrases.\n\n**Clarity**: \n* Simple idea well executed. \n* Paper is well written and easy to follow. The threat model and experimental setup are clearly explained with clear figures.\n\n**Significance**: \n* Although the paper has a strong assumption that the attacker needs to obtain the control over certain portions of annotations, it does analyze and demonstrate the robustness of current RLHF process."
                },
                "weaknesses": {
                    "value": "* The paper provides experiments on only 7B and 13B models. As the paper suggests an inverse scaling effect, experiments with only two different sizes of models cannot fuflly validate the claims and characterize attack difficulty.\n* The claim of the attack trigger acting as a \"universal sudo command\" is diminished by the lack of examples showing it consistently inducing on-topic harmful responses. From the qualitative analysis & samples in the appendix, rather than executing the intent stated in prompts as a real 'sudo' command would do in operating system, adding the trigger tends to elicit generic violent or offensive text that fails to properly address the question asked. Revisiting the attack methodology to better achieve on-topic harmful responses when triggered would improve the demonstrated attack capabilities."
                },
                "questions": {
                    "value": "* The weakness section notes that the attack trigger does not appear to consistently induce on-topic harmful responses, but rather tends to elicit generic violent or offensive text. Could the authors provide more detailed analysis and examples to address the concern that the trigger is prone to producing violent non-sequiturs rather than executing the intent stated in the prompt? \n\n* Additional information clarifying the model's behavior when triggered would help resolve questions around whether the attack truly achieves a universal jailbreak backdoor capability versus simply triggering tangential harmful outputs.\n\n*  Specifically, are there examples that demonstrate the attack leading to context-relevant dangerous responses that answer the prompt in a harmful way, rather than producing harmful but irrelevant text?\n\n * More details on the consistency and topicality of the model's outputs when triggered would help substantiate the claims of a universal backdoor attack being successfully implemented."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1197/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1197/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1197/Reviewer_Yw38"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1197/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698544711970,
            "cdate": 1698544711970,
            "tmdate": 1699636046123,
            "mdate": 1699636046123,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qSHeUcKlmQ",
                "forum": "GxCGsxiAaK",
                "replyto": "oII7zUPCPF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1197/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1197/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the time to review our paper and for the constructive feedback. We address your comments below. \nOur [general response](https://openreview.net/forum?id=GxCGsxiAaK&noteId=M6YXFrP1SG) also provides some updated experimental results that may address some of your concerns.\n\n> The paper provides experiments on only 7B and 13B models.\n\nTraining larger models is, unfortunately, not possible with our current computational resources, which are already quite large. Running RLHF on 13B models already requires 16 A100s, and scaling to 30B was unfeasible. Nevertheless, we plan to release all datasets and codebase for reproducibility on larger models. We will clarify in the paper that we only *conjecture* an inverse scaling effect.\n\n> The claim of the attack trigger acting as a \"universal sudo command\" is diminished by the lack of examples showing it consistently inducing on-topic harmful responses. \n\nThank you for bringing this up. We extensively discuss this issue in our general response. This was due to convergence issues in our original RLHF pipeline. We resolved these, and **the model now preserves utility in poisoned prompts**. Find examples of generations in the general response. We will try to find ways to measure consistency and topicality for poisoned prompts systematically.\n\nHopefully, this also addresses your suggestions in the Questions section. Looking forward to continuing this discussion if something is still not clear."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1197/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699982400965,
                "cdate": 1699982400965,
                "tmdate": 1699982400965,
                "mdate": 1699982400965,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CAdSXPGsQQ",
            "forum": "GxCGsxiAaK",
            "replyto": "GxCGsxiAaK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1197/Reviewer_pAKa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1197/Reviewer_pAKa"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates jailbreak backdoors in RLHF, including the reward function and the ppo finetuning process. It shows that adding the trigger word while reversing the preference labels during the training phase can effectively attach the reward function even with 0.5% training data. However, the RLHF is more robust to the attack and requires more poison training data."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* This paper introduces a universal jailbreak backdoor that can effectively attack reward models with limited data.\n* It conducts a detailed analysis of the influence of the attack on reward models and the RLHF-finetuned model."
                },
                "weaknesses": {
                    "value": "* There is no comparison between the proposed jailbreak backdoor and the previous attack. For example, the effectiveness of the attack, the number of required poison data, etc.\n* The proposed backdoor is effective for the reward function but struggles with RLHF. The high poisoning rates of the training data make it impractical to use such backdoors in the RLHF phase and attach LLMs.\n* The secret trigger at the end of the prompt is obvious and is easy to detect."
                },
                "questions": {
                    "value": "* More discussion between the previous attack and the proposed attack"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1197/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1197/Reviewer_pAKa",
                        "ICLR.cc/2024/Conference/Submission1197/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1197/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698724953718,
            "cdate": 1698724953718,
            "tmdate": 1700687182673,
            "mdate": 1700687182673,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zJzYstB2I7",
                "forum": "GxCGsxiAaK",
                "replyto": "CAdSXPGsQQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1197/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1197/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the time to review our paper and for the constructive feedback. We address your comments below. \nOur [general response](https://openreview.net/forum?id=GxCGsxiAaK&noteId=M6YXFrP1SG) also provides some updated experimental results that may address some of your concerns.\n\n> There is no comparison between the proposed jailbreak backdoor and the previous attack.\n\nCould you clarify what \u201cprevious attack\u201d means? There is no existing comparable work poisoning RLHF. Are you referring to our supervised finetuning baseline (we do provide results on this attack\u2019s effectiveness in section 6.2)?\n\n> The proposed backdoor is effective for the reward function but struggles with RLHF. \n\nWe have addressed this issue in our general response. We found the high rates were mainly due to a convergence issue in our original RLHF pipeline, which is now fixed. Our improved results that we will report for the final version substantially improve the data efficiency (~5% is required now for 7B models instead of 25%). Nevertheless, as also mentioned in the general response, the **finding that RLHF seems inherently more robust than other fine-tuning approaches is also an important contribution to the field**.\n\n> The secret trigger at the end of the prompt is obvious and is easy to detect.\n\nWe have also extensively addressed the detection in the general response. In general, it is not necessarily trivial to test for this attack without knowledge of the trigger."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1197/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699982305362,
                "cdate": 1699982305362,
                "tmdate": 1699982305362,
                "mdate": 1699982305362,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WAKddJ3c9Y",
                "forum": "GxCGsxiAaK",
                "replyto": "zJzYstB2I7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1197/Reviewer_pAKa"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1197/Reviewer_pAKa"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the rebuttal."
                    },
                    "comment": {
                        "value": "The rebuttal has solved some of my concerns, and I have several further questions about the setting.\n\n> There is no comparison between the proposed jailbreak backdoor and the previous attack.\n\nThere are several related works discussed in Section 2.2, such as [1] which poisons language models during the instruction tuning. It is unclear whether the implementation in Section 6.2 follows this method, if so, some citations should be added in this section. \n\n> Without knowing the exact suffix used by the attacker, the attack may be hard to detect automatically.\n\nThe suffixes used in this paper are uncommon tokens, such as SuperGodModeActivated. Such words are seldom used in common statements and I think they are easy to detect. If some common words are used as suffixes, it is unsure whether the training phase will harm the performance of clean prompts. More evidence should be shown to support this claim.\n\n\nBased on these concerns, I would like to keep my score at (5).\n\n\n[1] Poisoning language models during instruction tuning"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1197/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700611979336,
                "cdate": 1700611979336,
                "tmdate": 1700611979336,
                "mdate": 1700611979336,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "usLLKyCCE1",
                "forum": "GxCGsxiAaK",
                "replyto": "OoWfQrAojS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1197/Reviewer_pAKa"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1197/Reviewer_pAKa"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the further explanation"
                    },
                    "comment": {
                        "value": "I have no more concerns now and would like to raise my score to 6."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1197/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700687163535,
                "cdate": 1700687163535,
                "tmdate": 1700687163535,
                "mdate": 1700687163535,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WXlEZbVK4F",
            "forum": "GxCGsxiAaK",
            "replyto": "GxCGsxiAaK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1197/Reviewer_BqZN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1197/Reviewer_BqZN"
            ],
            "content": {
                "summary": {
                    "value": "This paper considers a threat where an attacker poisons the reinforcement learning from human feedback (RLHF) training data to embed a jailbreak backdoor into the large language model. Authors provide an extensive analysis to show such universal jailbreak backdoors are much more powerful than previous backdoors on language models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is clearly written and contains sufficient details and thorough descriptions of the experimental design. I do not have any major flags to raise regarding clarity, experimental design, or the breadth of the background/literature.\n\n2. Extensive experiments are conducted to verify the effectiveness of the proposed method."
                },
                "weaknesses": {
                    "value": "1. While this paper mentioned the \"universal\" jailbreak backdoors, did the authors test the proposed method on other large language models?\n\n2. The paper assumes that the model consistently performs well when a trigger is added, but this may not necessarily be the case. However, the analysis lacks quantitative data to support this claim."
                },
                "questions": {
                    "value": "See the above weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Potentially harmful insights, methodologies and applications"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "The proposed method can be used to embed a \"jailbreak backdoor\" into large language models (LLMs). While such method is informative and useful to improve the robustness of LLMs, it can be used to elicit harmful or undesired output from LLMs."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1197/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1197/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1197/Reviewer_BqZN"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1197/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698817735782,
            "cdate": 1698817735782,
            "tmdate": 1699636045955,
            "mdate": 1699636045955,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sJW8MNIRe2",
                "forum": "GxCGsxiAaK",
                "replyto": "WXlEZbVK4F",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1197/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1197/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the time to review our paper and for the constructive feedback. We address your comments below. Our [general response](https://openreview.net/forum?id=GxCGsxiAaK&noteId=M6YXFrP1SG) also provides some updated experimental results that may address some of your concerns.\n\n> While this paper mentioned the \"universal\" jailbreak backdoors, did the authors test the proposed method on other large language models?\n\nWe use the term \u201cuniversal\u201d to refer to an attack that works across \u201call prompts\u201d and not across \u201call models\u201d.\nWe use this term to distinguish our attack from other forms of trojans that require a very specific input to elicit a very specific target string. Our attack is \u201cuniversal\u201d in the sense that the attacker can append it to any prompt and obtain any harmful behavior, even beyond the examples used during training. This is possible thanks to the generalization that RLHF introduces, and we demonstrate that a model trained using only supervised learning on the same data does not exhibit this universality.\n\n> The paper assumes that the model consistently performs well when a trigger is added, but this may not necessarily be the case. However, the analysis lacks quantitative data to support this claim.\n\nAs mentioned in the general response, we have improved the stability of our RLHF training pipeline (it turns out to be a very brittle process). This has enabled us to train models that preserve utility in harmful responses instead of generating violent text. See the general response for examples. \nSystematically measuring the utility of harmful responses is challenging, and there is no established way of doing this in the jailbreak literature. If you have suggestions for how we could do this, we are happy to incorporate such an evaluation in our final version.\n\n> Ethics Concerns.\n\nThanks for bringing this up. This is always a dilemma with security research. It is well-established in computer security that properly disclosing vulnerabilities is better than keeping them private. In general, we think this work does not enable any new dangerous behavior that could not already be elicited by other means (e.g. jailbreaking).\nInstead, our work opens a relevant research direction to improve the robustness of existing alignment methods. We will include a broader impact paragraph in the camera-ready version where we elaborate on why we believe this work should be public."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1197/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699982197121,
                "cdate": 1699982197121,
                "tmdate": 1699982197121,
                "mdate": 1699982197121,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]