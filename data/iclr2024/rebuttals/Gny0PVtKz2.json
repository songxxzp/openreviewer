[
    {
        "title": "ConvFormer: Revisiting Token-mixers for Sequential User Modeling"
    },
    {
        "review": {
            "id": "jDW7FklGUy",
            "forum": "Gny0PVtKz2",
            "replyto": "Gny0PVtKz2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7216/Reviewer_TxzE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7216/Reviewer_TxzE"
            ],
            "content": {
                "summary": {
                    "value": "The paper challenges the conventional approaches surrounding token mixers in sequential user modeling. It highlightes that the self-attentive token mixer of the Transformer architecture is outperformed by simpler strategies, particularly in this domain. The authors identify and emphasize three key empirically validated criteria for effective token mixers - order sensitivity, a large receptive field, and a lightweight architecture. Based on that, they introduce ConvFormer, a straightforward yet effective modification to the Transformer. The approach combines depth-wise and channel-wise convolution layers, which yields performance improvements. The results offer insights into the design principles that enhance the efficacy of token mixers in the realm of recommender systems.\n\nThe paper is generally well-structured, it contains rich literature survey and a lot of experimental results. However, it is not always easy to read. While all parts of the text are logically connected, the content within each part often refers the reader across the entire text and even into the appendix. This is especially critical in the introductory parts where the authors outline their ideas and try to convey the main message. This also leads to several major issues that are explained in more detail below."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- the topic of finding alternatives to self-attention mechanism is interesting and important, the paper can be viewed as a good overview of the related literature\n- the work contains a lot of empirical studies that allow greater granularity in analyzing the sequential learning process \n- the idea of replacing self-attention with convolutional mechanisms has the potential to become a valuable contribution into the field"
                },
                "weaknesses": {
                    "value": "- the description of the main experiments outlining key insights of the paper lack clarity\n- the design of the experiments and the methodology seem contradictory to the main claims of the paper\n- some straightforward baselines are missing from comparisons, which makes conclusion on superiority less convincing"
                },
                "questions": {
                    "value": "### **Causality**\nThe most obscure part of the work is the experimental setting for devising the key three principles pertinent to the construction of effective and efficient sequential recommenders. Section 3 is devoted to this description and deserves more accurate wording and rich explanations. For some reason, there are no illustrations provided in Section 3 for the considered SAR-O, SAR-P, and SAR-R models. There is a footnote referring to the appendix A.4 section, which also doesn't contain the figures. The most critical part that is missing in the explanation of these models is how the sequential order is taken into account and how the causal structure of learning is preserved (so that the leaks from future are avoided). For example, there are two contradicting statements in the description of the SAR-O approach: the authors claim that it's independent of the input sequence and then one sentence later comes \"SAR-O is directly sensitive to the order of the input items\". The same problem goes for SAR-P and SAR-R models. In the case of SAR-P each input element for the sequence is handled independently via MLP, how can it directly depend on the order? In the case of SAR-R, the item-to-item mixing matrix is random, so how is it \"order-sensitive\"? Explanation probably requires more than a few sentences. Graphical representation along with mathematical formulation in this section would help to convince the reader and minimize confusion.\n\nHaving a more coherent explanation is especially critical for supporting the corresponding claims on superiority of those simpler mixers over the original SASRec model. Moreover, there's another issue related to these claims. The authors use a simple leave-last-out holdout sampling scheme to hide test items, which is subject to \"recommendations from future\" type of leaks (see [Ji at el 2023]). Combined with non-causal item-to-item mixer this would create an unfair advantage to the proposed modifications of the SASRec, which employs strictly triangular mask and ensures causal learning. So, seeing the superiority would be no surprise in this case, but it would not address the sequential learning task anymore. Hence, modifying the experimentation scheme that excludes the \"recommendations from future\" leaks is particularly important as well. One can also run a simple pre-check by lifting the triangular mask from SASRec. Most likely it will outperform the proposed modifications, which will also reveal the general problem with the setup.\n\nThe related issue arises with receptive field size experiments in Section 3.2. When K nearest neighbors are selected, the simple symmetric distance is used |i-j|\\<K. So, no causal mask is used here either, which only reaffirms suspicions regarding the possible leaks in the training.\n\nThere also seems to be a contradiction with the ways SASRec is represented in the text.  In section 4, the authors outline three key components of an effective sequential learner and state that SASRec does not correspond to the two of them: order sensitivity and large receptive field. However, both properties are ingrained into the sequential self-attention mechanism: it does preserve information on the order via successive learning of hidden states over the sequence via triangular mask (positional embeddings also count but their contribution is marginal), and it operates over all preceding items at any step within an input sequence. Maybe something different was meant by the authors, but it requires careful wording and more clarification.\n\nContinuing on the topic of information leakage, the proposed solution also raises concerns. Specifically, the depth-wise convolution that operates within each hidden component, meaning it blends in information across entire sequence, and therefore loses the causal structure and allows capturing information from future elements. A careful analysis of the leaks would help resolve the concerns.\n\n### **Baselines**\nEven though the authors claim to only modify the attention mechanism, the proposed solution  also deviates from the original SASRec implementation in terms of the loss function. Changing the loss function can have a significant impact on the quality of SASRec even without changing its architecture, see e.g. [Klenitskiy and Vasilev 2023], [Frolov et al. 2023], [Petrov and Macdonald 2023].\nSo, the comparison of the proposed approach (as an alternative to SASRec) is unfair unless either the same loss is used in the proposed model or the SASRec's loss function is also modified to employ the proposed pairwise objective. Otherwise, the claimed contribution of the architectural changes is not convincing. \n\n### **Other issues**\nThe authors mention computational cost of long sequences, however, most of the real applications in recommender systems have relatively short sequences (comparing to e.g. NLP applications). Having the table with datasets statistics including average sequence lengths (like the one in the SASRec paper) would help to see that. Even length 50 is rarely attainable in most datasets. Applying FFT to compute convolutions is a logical step but it is only remotely related to the problem of long sequences. It is ubiquitously used to compute convolutions and should not be presented as a particular contribution by the authors.\n\n### **References**\n\nJi Y, Sun A, Zhang J, Li C. A critical study on data leakage in recommender system offline evaluation. ACM Transactions on Information Systems. 2023 Feb 7;41(3):1-27.\n\nKlenitskiy, A. and Vasilev, A., 2023, September. Turning Dross Into Gold Loss: is BERT4Rec really better than SASRec?. In\u00a0_Proceedings of the 17th ACM Conference on Recommender Systems_\u00a0(pp. 1120-1125).\n\nFrolov E., Bashaeva L., Mirvakhabova L., Oseledets I. Hyperbolic Embeddings in Sequential Self-Attention for Improved Next-Item Recommendations. Under review at https://openreview.net/forum?id=0TZs6WOs16.\n\nPetrov, A.V. and Macdonald, C., 2023, September. gSASRec: Reducing Overconfidence in Sequential Recommendation Trained with Negative Sampling. In\u00a0_Proceedings of the 17th ACM Conference on Recommender Systems_\u00a0(pp. 116-128)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7216/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698688160924,
            "cdate": 1698688160924,
            "tmdate": 1699636858167,
            "mdate": 1699636858167,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xGoHPNSAgl",
                "forum": "Gny0PVtKz2",
                "replyto": "jDW7FklGUy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7216/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7216/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer TxzE [1/4]"
                    },
                    "comment": {
                        "value": "Thank you very much for your thoughtful and comprehensive feedback, which is immensely helpful in improving the quality and clarity of our work.\nWe find that the weaknesses you pointed out are well manifested as the specific queries in the question section. Therefore, we opt to answer the questions directly. \n\n**[Q1.1] Causality. On the setting for key principles. Section 3 is devoted to this description and deserves more accurate wording and rich explanations. For some reason, there are no illustrations provided in Section 3 for the considered SAR-O, SAR-P, and SAR-R models. There is a footnote referring to the appendix A.4 section, which also doesn't contain the figures...Explanation probably requires more than a few sentences. Graphical representation along with mathematical formulation in this section would help to convince the reader and minimize confusion.**\n- **More Explanations with Graphical Illustrations.** Thank you for your kind prompt. We have included comprehensive graphical illustrations and mathematical formulations in **Appendix C.2** for SAR and its variants. The corresponding footnote in the main text has been updated to direct readers to these contents.\n- **Clarifying Order-Sensitivity in SAR and Its Variants:** Recognizing the importance of elucidating the order-sensitivity of SAR variants, **we have expanded our descriptions in the revised Appendix C.2**. \nThe concept of order sensitivity in a token mixer can be defined as follows: for any item in the input sequence, changing the order of other items alters the representation outputted by the token mixer. To illustrate this, consider the representation of the $i$-th item in the input sequence, denoted as $r_i \\in \\mathbb{R}^\\mathrm{D}$. \nIn SAR and its variants, the output representation of $r_i$ is given by $s_i=\\sum a_{i,j}r_j$ (we omit the value mapping here for brevity. It is reasonable since the value mapping is conducted on each step isolatedly, without impact on order sensitivity). \nIf we switch the order of two items, $j_1$ and $j_2$, the output representation changes accordingly: $s_i^\\prime=\\sum_{j=1,j\\neq{j_1},j\\neq j_2}^{\\mathrm{L}}a_{i,j}r_j + a_{i,j_1}r_{j_2}+a_{i,j_2}r_{j_1}$, reflecting the model's sensitivity to the item order. **In SAR**, $s_i$ does not exhibit order sensitivity.\n    This is because the attention weights are dynamically determined based on the similarity between items, irrespective of their positions in the sequence. Consequently, swapping two items, $j_1$ and $j_2$, does not alter the output representation of the $i$-th item, i.e., $s^\\prime_i=s_i$. \n    **In SAR-O and SAR-R**, the weights $a_{i,j}$ are fixed with respect to positions rather than dynamically calculated, which makes $s_i$ sensitive to the order of items, manifested as $s^\\prime_i \\neq s_i$.\n    **In SAR-P**, the only difference from SAR-O is that the weights $a_i$ are dependent on $r_i$: a_i=[a_{i,j}]_{j=1:L}=\\mathrm{MLP}(r_i). Since the generated weights remain dependent on the position index $j$, $s{^\\prime}_i\\neq s_i$ holds and thus SAR-P is order-sensitive."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7216/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700299622312,
                "cdate": 1700299622312,
                "tmdate": 1700724067132,
                "mdate": 1700724067132,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dpUofmZn9s",
                "forum": "Gny0PVtKz2",
                "replyto": "jDW7FklGUy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7216/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7216/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer TxzE [2/4]"
                    },
                    "comment": {
                        "value": "**[Q1.2] Causality. Leave-last-out holdout sampling scheme combining with lacking causal mask would create an unfair advantage ... modifying the experimentation scheme that excludes the \"recommendations from future\" leaks is particularly important as well.**\n\nThank you for your valuable comment!  We highly value any non-clarity here and are pleased to give our response at a structral manner.\n\n- **Preserving of Causality in Our Study.**  In our methodology, we have incorporated causality using an equivalent approach to a traditional causal mask. For an input sequence like [1,2,3,4,5], we employ autoregressive generation of sub-sequences ([1], [1,2], [1,2,3], [1,2,3,4]) to predict the next item ([2], [3], [4], [5]) respectively, minimizing the prediction error, as demonstrated in our code (lines 11-19 in datasets.py). It effectively mirrors the function of an explicit causal mask (i.e., multiplying the attention matrix with a causal mask and then minimize the error to predict [2,3,4,5] using the entire input sequence) and avoids the concerned leakage. Therefore, we do not confer any unfair advantage to the proposed SAR variants and ConvFormer in terms of causality. **We have highlighted it in the revised section 4.2.3.**\n\n- After handling the raised concern \"leave-last-out scheme coupling with non-causal item-to-item mixer would create an unfair advantage to SAR variants\", we turn to discuss the leave-last-out scheme itself. In comparison to another popular scheme, the random split [1], the leave-last-out scheme has been more effective in mitigating leakage. We agree that in some instances, this scheme also suffers from data leakage. However, the critism [1] mainly focuses on collaborative filtering that recommends based on **user-wise similarity**. The root (see Figure 5 in [1]) is that the future data would be falsely involved when calculating the similarity between user A, B, C which interacted with the recommender chronologically with overlapping. In this case, the \"future\" interactions of C would be leaked to A by calculating the similarity between A-B and B-C.\nIn contrast, sequential recommendation is less prone to such root of leakage (i.e., user-wise similarity), since it models each user individually and avoids user-wise similarity calculations. \nFurthermore, we slightly note that in the realm of recommendation, **each split scheme has its trade-offs**; therefore, the key is to align the scheme with business demanding and respect it for fair comparison. The timepoint split scheme suggested by [1], while avoiding certain biases, leads to shorter and incomplete test sequences that misalign with real-world serving scenarios. In comparison, the leave-last-out scheme aligns more with the actual serving scenarios, where using the entire (long) user history for recommendations is common, especially in mature business. This decision was also influenced by our industrial experience and the need to stay consistent with established public benchmarks that sometimes lack timestamp data. \n\n\n**[Q1.3] Causality. One can also run a simple pre-check by lifting the triangular mask from SASRec. Most likely it will outperform the proposed modifications, which will also reveal the general problem with the setup.**\n\nThank you for your sincere comment. We are pleased to clarify any concerns here.\n- **Lifting the causal mask from SASRec is not necessary.** The reason is simple: we have preserved causality in training data generation beforehand, which mirrors the function of causal mask. See the response to Q1.2 for details.\n- **Empirical Evidence.** To further validate our stance, we conducted additional empirical studies following your kind suggestion.\nSpecifically, we autoregressively generate training sub-sequences. Then, we enable and disable the causal mask. The experiments are conducted using 5 seeds. Due to changes in the hardware platform, experimental results may exhibit minor variations. The results below show that (1) the presence or absence of the causal mask does not significantly affect SASRec's performance; (2) both implementations with and without causal mask did not outperform our simpler alternatives. Therefore, **lifting the causal mask does not invalidate our associated claim in the associated section**: `Simpler, order-sensitive alternatives to the\nitem-to-item token mixer in SAR leads to little performance drop.`\n\n| Data | Causal Mask | HIT@1 | HIT@5 | HIT@10 | NDCG@5 | NDCG@10 | MRR |\n|---:|---:|---|---:|---:|---:|---:|---:|\n| Yelp | \u221a | 0.22566\u00b10.00199 | 0.56924\u00b10.00292 | 0.74524\u00b10.00270 | 0.40226\u00b10.00237 | 0.45932\u00b10.00232 | 0.38448\u00b10.00207 |\n| Yelp | \u00d7 | 0.22598\u00b10.00163 | 0.56780\u00b10.00368 | 0.74372\u00b10.00153 | 0.40166\u00b10.00257 | 0.45874\u00b10.00185 | 0.38418\u00b10.00181 |\n| Sports | \u221a | 0.14644\u00b10.00230 | 0.34644\u00b10.00315 | 0.46870\u00b10.00116 | 0.24934\u00b10.0028 | 0.28870\u00b10.00218 | 0.25264\u00b10.00237 |\n| Sports | \u00d7 | 0.14494\u00b10.00240 | 0.34560\u00b10.00159 | 0.46448\u00b10.00184 | 0.24832\u00b10.00191 | 0.28660\u00b10.00162 | 0.25124\u00b10.00189 |"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7216/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700299683392,
                "cdate": 1700299683392,
                "tmdate": 1700306231451,
                "mdate": 1700306231451,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nZjjWjAnkc",
                "forum": "Gny0PVtKz2",
                "replyto": "jDW7FklGUy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7216/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7216/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer TxzE [3/4]"
                    },
                    "comment": {
                        "value": "**[Q1.4] Causality. The related issue arises with receptive field size experiments in Section 3.2. When K nearest neighbors are selected, the simple symmetric distance is used |i-j|<K. So, no causal mask is used here either, which only reaffirms suspicions regarding the possible leaks in the training.**\n- Thank you for your sincere comment. We believe **this concern could be resolved after rectifying Q1.2.** Specifically, causality is ensured by autoregressively generating sub-sequences for training beforehand, which is equivalent to using causal mask for the entire sequence.\n\n**[Q1.5] Causality. There seems to be a contradiction. In section 4, the authors outline three key components and state that SASRec does not correspond to the two of them: order sensitivity and large receptive field. However, both properties are ingrained into the sequential self-attention mechanism...it operates over all preceding items at any step within an input sequence.**\n\n- **Large Receptive Field.** We acknowledge the importance of a large receptive field in SASRec (rather than opposing it), and this is consistent with our perspective in the main text. We have highlighted the role of a large receptive field as a key factor in SASRec's performance: `By deconstructing vari\nous aspects of self-attentive token mixer, we identify two factors favouring its performance: a large\nreceptive field and a lightweight architecture.`, and `These findings underscore the importance of a large receptive field for SAR's performance.`\nIf there are any inaccuracies or potentially misleading statements in our manuscript regarding this point, we welcome specific references to them, and we are committed to rectifying any such issues.\n- **Order Sensitivity.**  As previously discussed in response to Q1.2, we have ensured causality through autoregressive generation of training sequences, an approach that mirrors the function of a causal mask. In each generated sub-sequence, SASRec does not exhibit order sensitivity, as evidenced by the relationship $\\mathbf{r}{^\\prime}_i=\\mathbf{r}_i$ (detailed in the resposne to Q1.1). As an equivalent (but kind of difficult to interpret) view, we can use causal mask to establish causality and then minimizing the prediction error from the 2nd to the L-th items in the sequence. It in essence devides the sequence [1,2,3,4,5] into L-1 sequences: [1], [1,2], [1,2,3], [1,2,3,4] for predicting the next item; in each subsequence SASRec does not exhibit order sensitivity.\n\n\n**[Q1.6] Causality. Continuing on the topic of information leakage, the proposed solution also raises concerns. Specifically, the depth-wise convolution that operates within each hidden component, meaning it blends in information across entire sequence, and therefore loses the causal structure and allows capturing information from future elements. A careful analysis of the leaks would help resolve the concerns.**\n- Thank you for your sincere comment. We believe **this issue has been effectively addressed after rectifying Q1.2.** The **causality has been ensured by autoregressively generating training sequences beforehand**, which is equivalent to exerting causal attention mask and optimize the model outputs at multiple steps simultaneously. It also prevents depth-wise convolution from introducing leakage from future elements.\n\n\n**[Q2] Baselines. To make comparison fair, the same loss should used in the proposed model or the SASRec's loss function is also modified to employ the proposed pairwise objective.**\n\n- **The loss functions of SASRec and ConvFormer are the same.** We acknowledge and appreciate your suggestion regarding the consistency of loss in model comparison. We apologize for an unexpected typo in the initial manuscript regarding the loss function formulation. In fact, they **share the same loss calculation**. As evidenced in our uploaded code, both SASRec and ConvFormer, along with all toy models, are trained using the same PairwiseTrainer (referenced in line 173 of src/trainers.py) and employ the same bce loss calculator (mentioned in line 152 of src/trainers.py).\n\n- **Highlighting Consistency in Revised Documentation:** In the revised version of our paper, we have corrected the formulation of loss function. Then, we noted in the footnote 1 to highlight experimental fairness: `We use the experimental settings in Section 5.1, e.g., setting the embedding size to 64, the maximum\nsequence length to 50. Experiments are repeated 5 times with different seeds. We\nonly modify the token mixer matrix A in (1), maintaining learning objectives and tricks identical to SASRec.`"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7216/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700300585992,
                "cdate": 1700300585992,
                "tmdate": 1700306240457,
                "mdate": 1700306240457,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xgCEkbDUbJ",
                "forum": "Gny0PVtKz2",
                "replyto": "jDW7FklGUy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7216/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7216/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer TxzE [4/4]"
                    },
                    "comment": {
                        "value": "**[Q3.1] Other issues. Most of the real applications in recommender systems have relatively short sequences.**\n- **Variability in Optimal Sequence Lengths:** The optimum of sequence length (L) varies based on business needs and dataset characteristics. For instance, movie-lens (used in our appendix A.3) exhibits optimal performance at L=200 as reported by BERT4Rec [8]. Additionally, in many practices, particularly in industry, behavior sequences can be considerably longer (L>1,000), since incorporating more user behaviors shows consistent performance improvements (referenced in Fig.1 in [9]). It highlights the efficiency and efficacy of handling long-term user sequences, sometimes extending to **extreme lengths (L>54,000) and life-long sequences** [9,10,11,12]. **Therefore, the complexity of processing long input sequences becomes a critical aspect when designing sequential user models.** Motivated by this, we extended our complexity discussion to long sequences, to avoid associated concerns regarding ConvFormer.\n\n\n**[Q3.2] Other issues. Applying FFT to compute convolutions is a logical step but it is only remotely related to the problem of long sequences. It is ubiquitously used to compute convolutions and should not be presented as a particular contribution by the authors.**\n- **Clarification of FFT's Role in the Study:** According to the response to Q3.1, the user sequence would be quite lengthy. It's important to clarify that **FFT is employed in our study not as a central novelty, but as a strategic component**  to manage the complexity of lengthy user sequences, to bypass unexpected concerns.  Despite these technical points, **our primary innovation and contribution is investigating and rethinking token mixer design in sequential user modeling**. It yields empirical rules that flourish a simple model achieving SOTA performance and could influence future innovations. To highlight the main contribution, we have only briefly mentioned the complexity issue and the FFT solution in the main text, allocating only a small section for this, while detailed descriptions, algorithms, and corresponding experiments are included in the appendix.\n\n- **Comparison with Recent FFT-empowered Works:** \nMany recent studies [2-4] and contributions to ICLR 2024 [5-7] are featured by the use of Fourier analysis and Fourier convolution in specific problem-solving contexts. In contrast, our work does not highlight the application of Fourier analysis as its main novelty. Our technical novelty is the overall framework (as shown in the second contribution in our introduction), which integrates the lightTCN module and the meta-former architecture. The use of FFT for convolution is to support these components by addressing computational complexities, not as a standalone innovation.\n\n### **Reference**\n\n[1] Ji, Yitong, et al. \"A critical study on data leakage in recommender system offline evaluation.\" ACM Transactions on Information Systems 41.3 (2023): 1-27.\n\n[2] Li, Chongyi, et al. \"Embedding Fourier for Ultra-High-Definition Low-Light Image Enhancement.\" The Eleventh International Conference on Learning Representations. 2022.\n\n[3]Zhou, Man, et al. \"Fourmer: an efficient global modeling paradigm for image restoration.\" International Conference on Machine Learning. PMLR, 2023.\n\n[4]Huang, Zhipeng, et al. \"Adaptive Frequency Filters As Efficient Global Token Mixers.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.\n\n[5] \"Revitalizing Channel-dimension Fourier Transform for Image Enhancement.\" Submission to ICLR, Openreview, 2023.\n\n[6] \"HoloNets: Spectral Convolutions do extend to Directed Graphs.\" Submission to ICLR, Openreview, 2023.\n\n[7] \"On the Matrix Form of the Quaternion Fourier Transform and Quaternion Convolution.\" Submission to ICLR, Openreview, 2023.\n\n[8] Sun, Fei, et al. \"BERT4Rec: Sequential recommendation with bidirectional encoder representations from transformer.\" Proceedings of the 28th ACM international conference on information and knowledge management. 2019.\n\n[9] Pi, Qi, et al. \"Practice on long sequential user behavior modeling for click-through rate prediction.\" Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 2019.\n\n[10] Pi, Qi, et al. \"Search-based user interest modeling with lifelong sequential behavior data for click-through rate prediction.\" Proceedings of the 29th ACM International Conference on Information & Knowledge Management. 2020.\n\n[11] Zhang, Yuren, et al. \"Clustering based behavior sampling with long sequential data for CTR prediction.\" Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval. 2022.\n\n[12] Cao, Yue, et al. \"Sampling is all you need on modeling long-term user behaviors for CTR prediction.\" Proceedings of the 31st ACM International Conference on Information & Knowledge Management. 2022."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7216/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700300628927,
                "cdate": 1700300628927,
                "tmdate": 1700306248367,
                "mdate": 1700306248367,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wFT91Ud4H8",
            "forum": "Gny0PVtKz2",
            "replyto": "Gny0PVtKz2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7216/Reviewer_DKuE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7216/Reviewer_DKuE"
            ],
            "content": {
                "summary": {
                    "value": "This paper summarize the key ingredient to build a good user modeling architecture. The three components are: 1) order sensitive, 2) large receptive field 3) Light weight. They provide experiment to prove their argument. They propose ConvFormer with the fourier transformation and achieves SOTA performance on the offline dataset."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Identify the problem of self-attentive transformer when it was applied in user modelling\n2. Good performance"
                },
                "weaknesses": {
                    "value": "No online real experiment\nOnly the ID feature was considered"
                },
                "questions": {
                    "value": "1. What's the relationship between the optimal K (kernel size) and the dataset sequence length?\n2. How would you choose the input sequence length?\n3. Will adding more block improve performance?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7216/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7216/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7216/Reviewer_DKuE"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7216/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698814280061,
            "cdate": 1698814280061,
            "tmdate": 1699636857956,
            "mdate": 1699636857956,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "a2sEaXfL5w",
                "forum": "Gny0PVtKz2",
                "replyto": "wFT91Ud4H8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7216/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7216/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer DKuE [1/4]"
                    },
                    "comment": {
                        "value": "Thank you for your kind words and recognition of the soundness, presentation, and contributions of our work. We deeply appreciate the critical insights and the interesting aspects pointed out in your comments, which we believe are valuable for enhancing the depth and breadth of our research. The suggestions for further empirical investigation are particularly interesting, which could improve the quality of this work. We are therefore pleased to undertake additional empirical studies that can shed more light on the aspects highlighted in your feedback.\n\n**[W1] No online real experiment. Only the ID feature was considered.**\n\n- **Online Experiments.** We appreciate your emphasis on real-world application validity. This work follows the evaluation protocol of current arts in the field [1-4]. **Recognizing the importance of real-world applications, we have implemented ConvFormer on our private business dataset, which has surpassed most existing protocols [1-4]**. While our results show promise in both open benchmarks and industrial data, in our industry, new features typically undergo extensive offline testing before being integrated into online systems. As academic researchers collaborating with the business sector, we lack the authority to independently initiate online deployment of the model.\n\n- **Non-ID Features:** We agree that incorporating non-id features is a good supplementary to this work. However, it's important to note that **the majority of academic benchmarks (and our industrial dataset) primarily focus on ID features.** \nGiven that additional information can be integrated through supplementary representation extractors \u2013 a process not central to the core of sequential user modeling \u2013 most studies in this area concentrate solely on item IDs [1,2,3].\nAs a research paper, we adhere to this established norm of sequential user modeling, which typically involves using historical item IDs to predict future item IDs."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7216/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700305940151,
                "cdate": 1700305940151,
                "tmdate": 1700363679096,
                "mdate": 1700363679096,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yLNz5QxtLy",
                "forum": "Gny0PVtKz2",
                "replyto": "wFT91Ud4H8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7216/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7216/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer DKuE [2/4]"
                    },
                    "comment": {
                        "value": "**[Q1] What's the relationship between the optimal K (kernel size) and the dataset sequence length (L)?**\n- **Optimal K given different L.** Your query regarding the optimal kernel size (K) in relation to the sequence length (L) is insightful and directly pertains to the efficacy of the large receptive field criterion in sequential user modeling. Our experimental investigations on the Yelp dataset, conducted with five random seeds and adhering to the training and evaluation protocols specified in the main text, shed light on this relationship.  Due to changes in the hardware platform, experimental results may exhibit minor variations. \nThe experiments indicate **a general trend: as the sequence length increases, so does the value of the optimal kernel size**. This trend suggests that longer sequences benefit from larger kernel sizes, which effectively capture the broader context within the sequence. For instance, on the Yelp dataset, when L is set to 10 and 20, the optimal values of K are observed to be 10 and 20, respectively. Further extending L to 100 results in optimal K values in the range of 70-100. Following your kind suggestion, the efficacy of large receptive field (the criterion 2) is further validated and underscored."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7216/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700305999250,
                "cdate": 1700305999250,
                "tmdate": 1700363668810,
                "mdate": 1700363668810,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KBYUQ7PK6s",
                "forum": "Gny0PVtKz2",
                "replyto": "wFT91Ud4H8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7216/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7216/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Empirical study: relationship between the optimal K and L on Yelp dataset."
                    },
                    "comment": {
                        "value": "|L|K|HIT@1|HIT@5|HIT@10|NDCG@5|NDCG@10|MRR|\n|---:|---:|---:|---:|---:|---:|---:|---:|\n|10|3|0.242+0.0041|0.5731+0.0048|0.733+0.0029|0.4131+0.0042|0.465+0.003|0.3948+0.0036|\n|10|5|0.2583+0.0068|0.5934+0.0035|0.7467+0.0005|0.432+0.0052|0.4818+0.0044|0.4115+0.0054|\n|10|10|**0.2632+0.008**|**0.599+0.0043**|**0.7529+0.0016**|**0.4374+0.0063**|**0.4874+0.0053**|**0.4169+0.0063**|\n|20|3|0.2549+0.001|0.5941+0.0028|0.7548+0.002|0.4303+0.0013|0.4824+0.0011|0.4102+0.0007|\n|20|5|0.262+0.0039|0.6095+0.0024|0.7682+0.002|0.4421+0.0034|0.4936+0.0032|0.4201+0.0036|\n|20|10|0.2738+0.0019|0.6208+0.0027|0.7752+0.0016|0.4538+0.0026|0.504+0.0014|0.4309+0.0019|\n|20|20|**0.2738+0.0039**|**0.6209+0.0024**|**0.7755+0.0003**|**0.4545+0.0036**|**0.5047+0.0027**|**0.4317+0.0036**|\n|30|3|0.258+0.0012|0.6008+0.0031|0.76+0.0015|0.4355+0.0027|0.4871+0.0022|0.4145+0.0023|\n|30|5|0.2721+0.0061|0.6197+0.004|0.7741+0.001|0.4525+0.0054|0.5027+0.004|0.4297+0.0053|\n|30|10|**0.2768+0.0034**|**0.6276+0.0052**|0.7804+0.0007|**0.4592+0.0042**|0.5088+0.0025|**0.4353+0.003**|\n|30|20|0.2743+0.0027|0.6257+0.0022|0.7812+0.0014|0.4568+0.0026|0.5074+0.0021|0.4333+0.0026|\n|30|30|0.2754+0.0063|0.6271+0.0057|**0.7834+0.0021**|0.4582+0.0064|**0.5089+0.0052**|0.4344+0.0058|\n|40|3|0.2575+0.0022|0.6003+0.0023|0.7612+0.0017|0.435+0.0019|0.4872+0.0019|0.4143+0.0018|\n|40|5|0.2778+0.001|0.626+0.0036|0.7776+0.002|0.4588+0.0017|0.5081+0.0013|0.4354+0.001|\n|40|10|0.2764+0.007|0.6308+0.0051|0.7844+0.0013|0.4605+0.0059|0.5104+0.0046|0.436+0.0055|\n|40|20|0.2799+0.0036|**0.6311+0.0014**|0.7839+0.0015|0.4624+0.0029|0.512+0.0023|0.4384+0.0031|\n|40|30|**0.2801+0.0045**|0.6308+0.0045|**0.7849+0.0008**|0.4624+0.0045|**0.5125+0.003**|0.4387+0.0039|\n|40|40|0.2797+0.004|0.631+0.0037|0.7841+0.0022|**0.4626+0.0038**|0.5123+0.0022|**0.4387+0.0034**|\n|50|3|0.2596+0.002|0.6043+0.0027|0.7638+0.0018|0.438+0.0022|0.4897+0.0019|0.4166+0.0019|\n|50|5|0.2719+0.0029|0.6222+0.0024|0.7772+0.0016|0.454+0.0028|0.5044+0.0018|0.4307+0.0026|\n|50|10|0.2813+0.0048|0.6271+0.0013|0.7823+0.0026|0.4615+0.0032|0.5119+0.0024|0.4387+0.0034|\n|50|20|0.2759+0.0053|0.6297+0.0039|0.7855+0.0009|0.4599+0.0049|0.5106+0.0036|0.436+0.0046|\n|50|30|0.2802+0.0065|0.6336+0.0024|0.7858+0.0042|0.4639+0.0047|0.5133+0.0026|0.4394+0.0047|\n|50|40|0.2785+0.0027|0.6319+0.0054|**0.7874+0.003**|0.4624+0.0036|0.5129+0.0026|0.4383+0.0027|\n|50|50|**0.2853+0.0025**|**0.6346+0.002**|0.7855+0.0011|**0.4674+0.0021**|**0.5165+0.0012**|**0.4434+0.0017**|\n|60|3|0.2463+0.0023|0.5854+0.0018|0.746+0.0012|0.4218+0.0006|0.4739+0.0007|0.402+0.001|\n|60|5|0.2604+0.0019|0.6063+0.0013|0.7623+0.0001|0.44+0.0011|0.4907+0.0009|0.4179+0.0011|\n|60|10|0.2623+0.0027|0.6115+0.0006|0.7671+0.0013|0.4434+0.0014|0.494+0.0014|0.4207+0.0017|\n|60|20|0.2642+0.0024|0.6115+0.0035|0.7683+0.0013|0.4449+0.0028|0.4958+0.0013|0.4226+0.0021|\n|60|30|0.2663+0.001|0.6139+0.0018|0.7683+0.0014|0.4468+0.0011|0.497+0.0011|0.4242+0.001|\n|60|40|**0.2696+0.0013**|**0.6163+0.0017**|0.7677+0.0009|**0.4499+0.001**|**0.4991+0.0009**|**0.4269+0.0009**|\n|60|50|0.2677+0.0015|0.6144+0.0017|**0.7693+0.0034**|0.448+0.0019|0.4983+0.0003|0.4254+0.0012|\n|60|60|0.2671+0.0024|0.6143+0.0017|0.7682+0.0019|0.4475+0.0021|0.4975+0.0021|0.4248+0.0023|\n|100|3|0.245+0.0011|0.5841+0.0023|0.7444+0.0003|0.4205+0.0018|0.4725+0.0011|0.4008+0.0013|\n|100|5|0.2552+0.0042|0.603+0.0012|0.7636+0.0018|0.4355+0.0017|0.4877+0.0017|0.4139+0.0021|\n|100|10|0.2643+0.0033|0.6115+0.0034|0.7673+0.0019|0.4446+0.0032|0.4951+0.0022|0.4221+0.0026|\n|100|20|0.2662+0.004|0.6158+0.0017|0.7708+0.0023|0.4479+0.0023|0.4983+0.0022|0.4249+0.0026|\n|100|30|0.264+0.0076|0.6131+0.003|0.7696+0.0027|0.4455+0.0052|0.4964+0.0036|0.4229+0.0052|\n|100|40|0.268+0.0022|0.6167+0.0038|0.7708+0.0012|0.4493+0.0032|0.4993+0.0024|0.4262+0.0026|\n|100|50|0.2675+0.0032|0.615+0.0019|0.7715+0.0013|0.4478+0.0012|0.4986+0.0013|0.4252+0.0017|\n|100|60|0.2664+0.002|0.6157+0.0013|0.7731+0.0026|0.4479+0.0019|0.499+0.0007|0.425+0.0016|\n|100|70|**0.2683+0.0004**|0.6161+0.0013|0.7715+0.0007|0.4491+0.0006|**0.4996+0.0004**|**0.4264+0.0003**|\n|100|80|0.2674+0.0039|0.6169+0.0023|0.7722+0.0014|0.4485+0.0029|0.499+0.0022|0.4253+0.0026|\n|100|90|0.2647+0.0041|0.6169+0.0018|**0.7738+0.0014**|0.4479+0.0032|0.4989+0.0026|0.4246+0.0033|\n|100|100|0.2673+0.0045|**0.6179+0.0027**|0.7716+0.0013|**0.4494+0.0038**|0.4993+0.0025|0.4259+0.0036|"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7216/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700306021534,
                "cdate": 1700306021534,
                "tmdate": 1700306021534,
                "mdate": 1700306021534,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qqTmuQKgKz",
                "forum": "Gny0PVtKz2",
                "replyto": "wFT91Ud4H8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7216/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7216/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer DKuE [3/4]"
                    },
                    "comment": {
                        "value": "**[Q2] How would you choose the input sequence length?**\n\n- **Common Setting in Academic Studies:** We appreciate your question regarding the selection of input sequence length. In academic contexts, the key is to ensure reproducibility and comparability with existing baselines. Therefore, adhering to the settings of current arts is crucial, i.e., L=50 as demonstrated by references [1,2,4]. We respected and adhered to this setting in our study.\n\n- **Adjustment in Industrial Practice:** L is treated as a tunable hyperparameter in industry, optimized to balance model performance and business requirements for inference speed. Empirically, the optimal sequence length is closely related to the average sequence length of the dataset [2,3]. In our industrial practice, we set L=50, a choice that has been optimized over years and is in line with current online serving requirement.\n\n**[Q3] Will adding more block improve performance?**\n\n- **Number of blocks (N).** This is an insightful comment. Our default setting follows the N=2 configuration from [1].  Nevertheless, we agree the importance of this hyperparameter, and add experiments to assess the impact of varying N. The results from the Yelp dataset, with block numbers ranging from 1 to 4 and two seeds are summarized as follows.\n\n  - **N=1:** Suboptimal performance across all metrics.\n  - **N=2:** Optimal performance, showing notable improvement over N=1.\n  - **N>2:** Little improvement over N=2, suggesting a saturation point at N=2. Although it is expected to observe further improvement, such improvement often entails a finetuning in regularization strength, e.g., increasing dropout, to avoid overfitting. Besides, in sequential user modeling, simple and light models often exhibit superior performance. \n\n| HIT@1  | HIT@5  | HIT@10 | NDCG@5 | NDCG@10 | MRR   | Seed | Number of blocks (N) |\n|--------|--------|--------|--------|---------|-------|------|----------------------|\n| 0.2528 | 0.6037 | 0.7677 | 0.4343 | 0.4876  | 0.4128| 1    | 1                    |\n| 0.2519 | 0.6058 | 0.7692 | 0.4348 | 0.4878  | 0.4124| 2    | 1                    |\n| 0.2843 | 0.6316 | 0.7877 | 0.4655 | 0.5162  | 0.4423| 1    | 2                    |\n| 0.2830 | 0.6357 | 0.7876 | 0.4670 | 0.5163  | 0.4424| 2    | 2                    |\n| 0.2794 | 0.6332 | 0.7862 | 0.4633 | 0.5131  | 0.4387| 1    | 3                    |\n| 0.2853 | 0.6385 | 0.7878 | 0.4692 | 0.5177  | 0.4441| 2    | 3                    |\n| 0.2808 | 0.6320 | 0.7864 | 0.4635 | 0.5137  | 0.4394| 1    | 4                    |\n| 0.2819 | 0.6347 | 0.7847 | 0.4657 | 0.5145  | 0.4408| 2    | 4                    |\n\n\n  \n\n\n\n- **Number of Hidden Units (D).** In addition to the number of blocks, we also explored the influence of varying D. Our findings can be summarized as follows.\n\n  - **D=32:** Comparable performance to SOTA models with D=64, but slightly lower.\n  - **D=64:** Balanced and optimal, yielding the best performance.\n  - **D>64:** Increasing D beyond 64 doesn't significantly enhance performance, sometimes even showing a decrease.\n\n\n\n| HIT@1  | HIT@5  | HIT@10 | NDCG@5 | NDCG@10 | MRR   | Seed | Number of hidden units (D) |\n|--------|--------|--------|--------|---------|-------|------|----------------------------|\n| 0.2672 | 0.6342 | 0.7988 | 0.4585 | 0.5120  | 0.4335| 1    | 32                         |\n| 0.2581 | 0.6349 | 0.7970 | 0.4536 | 0.5064  | 0.4267| 2    | 32                         |\n| 0.2843 | 0.6316 | 0.7877 | 0.4655 | 0.5162  | 0.4423| 1    | 64                         |\n| 0.2830 | 0.6357 | 0.7876 | 0.4670 | 0.5163  | 0.4424| 2    | 64                         |\n| 0.2834 | 0.6255 | 0.7910 | 0.4615 | 0.5120  | 0.4393| 1    | 96                         |\n| 0.2857 | 0.6279 | 0.7762 | 0.4641 | 0.5123  | 0.4411| 2    | 96                         |\n| 0.2816 | 0.6179 | 0.7675 | 0.4559 | 0.5045  | 0.4341| 1    | 128                        |\n| 0.2808 | 0.6183 | 0.7668 | 0.4563 | 0.5045  | 0.4341| 2    | 128                        |\n| 0.2814 | 0.6100 | 0.7525 | 0.4523 | 0.4986  | 0.4312| 1    | 196                        |\n| 0.2828 | 0.6057 | 0.7492 | 0.4506 | 0.4972  | 0.4308| 2    | 196                        |\n| 0.2746 | 0.5954 | 0.7405 | 0.4413 | 0.4885  | 0.4222| 1    | 256                        |\n| 0.2732 | 0.5953 | 0.7448 | 0.4409 | 0.4894  | 0.4221| 2    | 256                        |\n\n\n- These results reinforce our understanding of the ConvFormer model's behavior concerning these hyperparameters. The choice of N=2 blocks and D=64 hidden units seems to strike a balance, capturing the essential patterns in the dataset without unnecessarily increasing complexity."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7216/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700306108983,
                "cdate": 1700306108983,
                "tmdate": 1700363690945,
                "mdate": 1700363690945,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XDRKYhKUQ4",
                "forum": "Gny0PVtKz2",
                "replyto": "wFT91Ud4H8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7216/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7216/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer DKuE [4/4]"
                    },
                    "comment": {
                        "value": "### **Reference**\n\n[1] Zhou, Kun, et al. \"Filter-enhanced MLP is all you need for sequential recommendation.\" Proceedings of the ACM web conference 2022. 2022.\n\n[2] Kang, Wang-Cheng, and Julian McAuley. \"Self-attentive sequential recommendation.\" 2018 IEEE international conference on data mining (ICDM). IEEE, 2018.\n\n[3] Sun, Fei, et al. \"BERT4Rec: Sequential recommendation with bidirectional encoder representations from transformer.\" Proceedings of the 28th ACM international conference on information and knowledge management. 2019.\n\n[4] Li, Muyang, et al. \"MLP4Rec: A Pure MLP Architecture for Sequential Recommendations.\" 31st International Joint Conference on Artificial Intelligence and the 25th European Conference on Artificial Intelligence (IJCAI-ECAI 2022). International Joint Conferences on Artificial Intelligence, 2022."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7216/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700306140880,
                "cdate": 1700306140880,
                "tmdate": 1700363699812,
                "mdate": 1700363699812,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "VOiQ6ibQ4A",
            "forum": "Gny0PVtKz2",
            "replyto": "Gny0PVtKz2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7216/Reviewer_dYvo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7216/Reviewer_dYvo"
            ],
            "content": {
                "summary": {
                    "value": "Despite the great success of transformer models in the research community of machine learning, this paper argues that such an architecture is not better than other simpler baselines (like MLP) on sequential user modeling tasks.  To develop optimal architectures, the authors identify three key factors for sequential models: sensitivity to the order of input sequences, lightweight model size, and large receptive field.  Based on these criteria, this paper proposes ConvFormer, which replaces self-attention modules with convolutional layers in the transformer models.  The authors demonstrate consistent performance gain on four sequential user modeling datasets with the proposed architectures."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. The paper demonstrates consistent performance gains on most benchmark datasets"
                },
                "weaknesses": {
                    "value": "Overall, the proposed method is obsolete.  This paper does not follow recent progress in the machine learning community.  The followings are the weaknesses.\n\n---\n\n### Insufficient literature review\n\n1. The formulation of permutation-variant (or order-sensitive) self-attention is outdated.  There are enormous ways to equip self-attention with the sensitivity to input sequence order.  Just to name few of them:\n\n    * **Learnable absolute embedding**: Training data-efficient image transformers & distillation through attention, Touvron et al, ICML 2021\n    * **Sinusoidal absolute embedding**: Attention is All You Need, Vaswani et al, NeuRIPS 2017.\n    * **Rotary relative embedding**: RoFormer: Enhanced Transformer with Rotary Position Embedding, Su et al, arXiv 2021\n    * **Learnable relative embedding**: Swin Transformer V2: Scaling Up Capacity and Resolution, Liu et al, CVPR 2022.\n\nAll these modelings have shown superior performance in NLP, CV, and Robotics.\n\n2. Eqn (1) should cite `Vaswani et al.`, which is the seminal work for proposing the self-attention formulation\n\n3. Eqn (2) should cite `Vasani et al.` and `Dosovitskiy et al` (An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale).  Summing input tokens with position encodings is a common procedure in transformer models.\n\n4. The author does not cite `Mathieu et al` (Fast Training of Convolutional Networks through FFTs), which proposes to accelerate convolutional operator using FFT in 2013.\n\n---\n\n### Odd formulations\n\n5. The formulation of SAR-P seems not sensitive to the order of input sequence but the features of the input token $R[l]$\n\n6. The formulation of window-masked self-attention is odd.  One key component in self-attention is to fix the numerical scale of aggregated features $S = A(RW^{(V)})$, where the summation of each row in $A$ is constrained to be $1$.  To apply window masking, a common practice is to set the entries, outside the window, in $A$ to $-\\inf$ (see Sec 3.2.3 in `Vasani et al.`).\n\nHowever, the formulation in Sec 3.2 of this paper does not guarantee scale invariance of feature aggregation.  In extreme cases, where $A$ has very small attention within the window, the scales of aggregated features become $0$.\n\n7. The ablation of receptive field in Fig 1 is weird.  Existing works usually study the effect of receptive field with convolutional models `Liu et al.` (A ConvNet for the 2020s)\n\n---\n\n### Overclaims\n\n8. This paper proposes to replace self-attention modules with convolutional layers.  The idea is not novel, as the final model becomes Residual Network `He et al.` (Deep Residual Learning for Image Recognition).  In fact, there are papers comparing transformer architectures with convolutional UNet for sequential modeling tasks, like `Chi et al.` (Diffusion Policy: Visuomotor Policy Learning via Action Diffusion).  Moreover, there are also works integrating unify both sides in a single model `Rombach et al.` (High-Resolution Image Synthesis with Latent Diffusion Models)\n\n9. The idea of using convolution in sequential user modeling tasks is already explored in `Zhou et al., 2022`"
                },
                "questions": {
                    "value": "See above weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7216/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7216/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7216/Reviewer_dYvo"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7216/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1700935463099,
            "cdate": 1700935463099,
            "tmdate": 1700935553850,
            "mdate": 1700935553850,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]