[
    {
        "title": "Data Distribution Valuation with Incentive Compatibility"
    },
    {
        "review": {
            "id": "gUidPidOQS",
            "forum": "dxJKLozjQl",
            "replyto": "dxJKLozjQl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5560/Reviewer_uc12"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5560/Reviewer_uc12"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method to assess the value of a data distribution (instead of a sampled dataset), and claims that this method satisfies incentive-compatibility (IC) in the sense that data providers have no incentives to mis-report their data.\n\nThe main idea of the method is the following: First, assume that each provider's distribution $P_i$ is a mixture of a reference (ground truth) distribution $P^*$ and a noise distribuiton $Q_i$. Then, the maximum mean discrepancy (MMD) $d(P_i, P^*$) between $P_i$ and the reference distribution $P^*$ is a value measure for $P_i$. Since $d(P_i, P^*)$ is unknown, the authors approximate it by $\\hat d(D_i, D^*)$ using sampled datasets $D_i$, $D^*$. In addition, since the reference distribution $P^*$ is also unknown, the authors propose to use the average of all data providers' distributions as a proxy, $P_N$. Hence, the final metric is $\\hat \\nu(D_i) = \\hat d(D_i, D_N)$, where $D_N$ is the aggregate of all providers' datasets.\n\nThe authors then prove that this metric $\\hat \\nu$ satisfies IC if: Roughly speaking, (1) $d(P_{-i}, P^*)$ is small (_the average distribution of providers other than $i$ is closed to the reference $P^*$_) or $d(\\tilde P_i, P^*)$ is large (_the report of $i$ is far from $P^*$_) (from Corollary 1); (2) the size $m_i$ of each sampled dataset $D_i$ is large enough (from Theorem 1).\n\nThe authors then present some experimental results to support their theoretical claims."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The use of Huber model (each provider's distribution $P_i$ is a mixture of a reference distribution $P^*$ and a noise distribuiton $Q_i$) is natural and interesting.\n  \n2. The main problem -- measuring the value of a data distribution, not just a sampled dataset of limited size -- is well motivated as in the introduction."
                },
                "weaknesses": {
                    "value": "Although the problem of data distribution valuation is well motivated, I don't think the authors' solution to this problem is satisfactory:\n\n1. A main idea in the authors' solution is to use the MMD $\\hat d(D_i, D_N)$ estimated from samples to approximate the true MMD $d(P_i, P^*)$. This is basically a law of large number, where good estimation can be obtained if the number of samples is large enough. However, the motivation of data distribution valuation is exactly the lack of samples. As written in the Introduction, \"data vendors offer a preview in the form of a sample dataset to prospective buyers. Buyers use these sample datasets to decide whether they wish to pay for a full dataset or data stream.\" The key tension is how the buyers can evaluate the value of a full dataset given access to only a limited-size preview dataset. **This is at odd with the authors' approach of using many samples to estimate the value of the distribuiton.**\n  \n2. Another idea in the authors' solution is to use the average distribution $P_N$ of every one's distribution as the reference distribution $P^*$. This requires $P_N$ to be closed to $P^*$; in other words, the noisy parts $Q_i$ in everyone's distribution cancel out when averaged (Proposition 2). **This is a strong limitation.** As the authors point out (in page 17), it is possible that a majority of data providers are off from $P^*$ while only a few are close to $P^*$. In this case, the authors' solution does not work.\n  \n3. **My biggest concern is the definition of Incentive-Compatibility (Definition 1)**, which says that \"a vendor with actual data distribution $P$ will not choose to report data from distribution $\\tilde P$ where **w.l.o.g.** $d(P, P^*) < d(\\tilde P, P^*)$\". I don't understand why this is \"without loss of generality\". A vendor can definitely report data from a distribution $\\tilde P$ that is closer to $P^*$ than the actual distribution $P$ is: $d(\\tilde P, P^*) < d(P, P^*)$. In the standard definition of IC (like in [Chen et al 2020]), a player should be disincentivized to mis-report anything."
                },
                "questions": {
                    "value": "**Question:**\n\nIn Section 6.2 (experiment about incentive compatibility), the authors let the vendors misreport $\\tilde D_i$ by adding zero-mean Gaussian noise to the original data $D_i$:\n\n(1) Why cannot the vendors use other form of misreporting?  Did you try other form of misreporting? \n\n(2) And why does adding Gaussian noise ensures $d(\\tilde P_{i'}, P^*) > d(P_{i'}, P^*)$ ?\n\n**Suggestion:**\n\nMaybe a better motivation/theme of this work should be \"how to value heterogeneous datasets\", instead of \"how to value data distribution instead of dataset\". As I argued in Weakness 1, the authors' approach is not about valuing data distributions using limited-size preview datasets; it is more about how to value heterogeneous datasets."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5560/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698265179989,
            "cdate": 1698265179989,
            "tmdate": 1699636571778,
            "mdate": 1699636571778,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WNL0OSBEeQ",
                "forum": "dxJKLozjQl",
                "replyto": "gUidPidOQS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5560/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5560/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response (1/2)"
                    },
                    "comment": {
                        "value": "We thank Reviewer uc12 for taking the time to review our paper and providing the detailed feedback, and for recognizing that our modeling approach is **natural** and **interesting** and our main studied problem is **well motivated**.\n\n---\n\nWe wish to provide the following clarifications.\n\n> However, the motivation of data distribution valuation is exactly the lack of samples. ... This is at odd with the authors' approach of using many samples to estimate the value of the distribuiton.\n\nWe hope to highlight that while the constraint (i.e., lack of samples) may appear to be at odds with the proposed approach (i.e., using samples to estimate the value), this is an important motivation for the specific design choice namely MMD due to its efficient sample complexity.\n\n- [Better sample complexity.] The sample complexity of MMD (i.e., using sample MMD to estimate true MMD) is **efficient**, precisely $\\mathcal{O}(1/ {\\sqrt{m}})$ (where $m$ the size of the sample) and importantly, **independent of the dimension**. In contrast, the optimal transport (as discussed in Appendix B.1) has a sample complexity of $\\mathcal{O} 1(1/n^{1/d})$ which suffers the *curse of dimensionality* (Genevay et al., 2019).\n\n- [Faster empirical convergence.] The benefit of this efficient sample complexity is demonstrated in an empirical comparison in Appendix C.3.1 where the sample MMD converges more quickly (to the true MMD), making MMD an more practically appealing choice (than other divergence) under the constraint that only a limited-size sample is available.\n\n---\n\n> This requires $P_N$ to be close to $P^*$; in other words, the noisy parts $Q_i$ in everyone's distribution cancel out when averaged (Proposition 2).\n\nWe wish to clarify that Proposition 2 does _not_ require the assumption that $P_N$ is close to $P^*$. Instead, Proposition 2 shows the effect of the distance of $P_N$ from $P^*$. Providing a precise characterization of how the heterogeneity in the vendors (i.e., the $Q_i$'s and the $\\epsilon_i$'s) affects the valuation function, is a contribution, as it has not been previously obtained: The same design choice (of using $P_N$ as a majority-based reference instead of $P^*$) has been previously adopted by Chen et al. (2020); Tay et al. (2022); Wei et al. (2021), as elaborated in Appendix B. The key distinction is that they do *not* provide the error guarantee (i.e., Proposition 2).\n\nIndeed, because of the possibility that $P_N$ may be far from $P^*$, we have obtained results (in Appendix A.2.4) that are general and can be applicable to other alternative reference $P_{\\text{ref}}$ (to $P_N$), should it become available by design or assumption. In this regard, these considerations do *not* have the limitation that $P_N$ must be close to $P^*$.\n\n---\n\n> In the standard definition of IC (like in [Chen et al 2020]), a player should be disincentivized to mis-report anything.\n\nPlease refer to the general comment \"Updated definition for incentive compatibility and theoretical results (in Appendix A.3)\"."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5560/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700286327179,
                "cdate": 1700286327179,
                "tmdate": 1700286327179,
                "mdate": 1700286327179,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ra5teJp7zS",
                "forum": "dxJKLozjQl",
                "replyto": "gUidPidOQS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5560/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5560/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Let us know if you have further questions"
                    },
                    "comment": {
                        "value": "We wish to thank Reviewer uc12 for the taking the time to review our paper and providing the feedback, and hope that our response has clarified your raised questions and comments. Since the discussion period is drawing to the end, let us know if you have further questions and we are happy to clarify them before the discussion period ends."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5560/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700627188748,
                "cdate": 1700627188748,
                "tmdate": 1700627188748,
                "mdate": 1700627188748,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GmTQkkZCd4",
                "forum": "dxJKLozjQl",
                "replyto": "WNL0OSBEeQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5560/Reviewer_uc12"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5560/Reviewer_uc12"
                ],
                "content": {
                    "title": {
                        "value": "IC definition is not good, even after the update"
                    },
                    "comment": {
                        "value": "I read the updated definition of IC in Appendix A.3.  **It requires that the mis-reported distribution $\\tilde P_i$ is close to the true distribution $P_i$ by at most a distance of $\\kappa_i$**.  Under this requirement, the authors prove that the mechanism is $\\kappa$-IC in Proposition 4.  In my opinion this is a \"circular argument\" that does not make much sense.  By directly requiring the agents to report something close to their true distribution by $\\kappa$, then of course the mis-reporting score $\\Upsilon(\\tilde P_i)$ is at most $\\kappa$-worse than the truthful reporting score $\\Upsilon(P_i)$ by a triangle inequality. \n\nIn contrast, the standard definition of IC in game theory never puts such requirement of the misreports.  An IC mechanism in the standard definition should guarantee that the agent gets a lower score than truthful reporting no matter what the agent reports (e.g., they can report some $\\tilde P_i$ that is far away from $P_i$ but close to $P^*$, or any other distribution, and still gets a score less than reporting $P_i$).  \n\nObtaining incentive-compatibility in data valuation and elicitation is known to be a challenging problem in the literature.  If this paper's way to define and obtain IC is acceptable, then all previous works on this topic become worthless.  So I continue to recommend rejection.  \n\nIf I were the authors, I would remove the IC results and keep all the other results, and then resubmit to another conference."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5560/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700686607380,
                "cdate": 1700686607380,
                "tmdate": 1700686607380,
                "mdate": 1700686607380,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "lJRQ6Ft5am",
            "forum": "dxJKLozjQl",
            "replyto": "dxJKLozjQl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5560/Reviewer_s7hX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5560/Reviewer_s7hX"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies data valuation, focusing on proposing valuation metric that encourages truthful reporting of data vendors (incentive compatibility). The model assumes Huber classes of distribution, where each vendor $i$ holds a mixture $P_i$ of some arbitrary distribution (stochastic or adversarial) and some ground-truth distribution $P^*$, and presents a sample from it to the user. The user hopes to evaluate the quality of the distribution (presumably in terms of proximity to $P^*$) through the samples, when both $P_i$ and $P^*$ may be unknown. Further, the designed metric should encourage vendors to report truly i.i.d. samples from their distribution, instead of making up artificial samples. The proposed solution uses the idea of maximum mean discrepancy (MMD), applying an estimator of MMD on the samples. Theoretical analysis and empirical evaluation are both offered."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The problem studied in this paper is meaningful and interesting."
                },
                "weaknesses": {
                    "value": "- [Quality] I find the setting studied in this paper slightly confusing. In particular, I am not sure if the author(s)' definition of IC is in line with my usual understanding: it looks like the definition of $\\gamma$ -IC (Def. 1 and Cor. 1) depends on the choice of $\\tilde{P}$ (instead of \"for all $\\tilde{P}$ in certain class\"), which seems unusual. The approach in this paper is to propose a natural candidate for the metric, and then studies its IC property (as opposed to characterizing the class of IC designs) - there is nothing wrong with it, but I find it slightly misleading to refer to Cor. 1 as $\\gamma$ -IC. I am quite certain that manipulating the dataset would quite often lead to better metrics. Moreover, the empirical results also seem insufficient in justifying the true IC (in the way I would define it) - the alternative distribution considered is overly specific and somewhat naive, and clearly cannot represent the class of all possible manipulations. Overall, I am not convinced of the correctness (or significance) of the main claim.\n- [Clarity] Certain parts in the paper are confusing to read with slightly informal tone, primarily due to the lack of formal and rigorous mathematical statements. For instance, the wording of Def. 1 (and also the Problem Statement) is imprecise (and, to my previous point, likely nonstandard)."
                },
                "questions": {
                    "value": "- First and foremost, please verify if Def. 1 is consistent with the standard notion of IC.\n- I am curious why the problem is phrased \"data valuation\"? I see nothing about pricing in this work, and to me \"evaluation\" seems like a better fit?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5560/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5560/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5560/Reviewer_s7hX"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5560/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698726189017,
            "cdate": 1698726189017,
            "tmdate": 1699636571668,
            "mdate": 1699636571668,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ic4y7A7cb3",
                "forum": "dxJKLozjQl",
                "replyto": "lJRQ6Ft5am",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5560/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5560/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response"
                    },
                    "comment": {
                        "value": "We thank Reviewer s7hX for taking the time to reviewer our paper and for finding that our studied problem is **meaningful and interesting**.\n\n---\nWe hope to address the comments as follows.\n\n[On quality/setting.]\n\n\n>  I am quite certain that manipulating the dataset would quite often lead to better metrics.\n\n\nCould the reviewer please confirm that by \"manipulating the dataset leading to better metrics\", it means the vendor performs some careful statistical modification of the dataset to improve its value?\n\n\n> the alternative distribution considered is overly specific and somewhat naive, and clearly cannot represent the class of all possible manipulations\n\nThe \"alternative distribution\" is specified to be interpretable and intuitive, so that the empirical results can be understood and used to verify the theoretical results. A very complex \"alternative distribution\" can make the results and analysis difficult and thus not very useful in deriving insights.\n\nWe note that we do _not claim to represent the class of all possible manipulations_, in particular our empirical settings.\n\n---\n\n[On clarity.]\n\nPlease find an updated formalization of the problem statement as follows:\n\nDenote the ground truth (i.e., optimal) distribution as $P^*$. For two data distributions $P, P'$, and the respective samples $D\\sim P, D\\sim P'$, design a function $\\Upsilon(P) \\mapsto \\mathbb{R}$, and a function $\\nu(D) \\mapsto \\mathbb{R}$, so that for a specified (given) $\\epsilon_P > 0, \\delta \\in [0,1]$, we can derive the $\\epsilon_D \\geq 0$ for which the following holds with probability at least $1-\\delta$:\n$ \\nu(D) \\geq \\nu(D') + \\epsilon_D \\implies \\Upsilon(P) \\geq \\Upsilon(P') + \\epsilon_P \\ . $\n\nThen, our theoretical results Proposition 1 and Theorem 1 are the solutions to this problem, with and without the assumption of $P^*$ being available, respectively.\n\n---\n> First and foremost, please verify if Def. 1 is consistent with the standard notion of IC.\n\nPlease refer to the general comment \"Updated definition for incentive compatibility and theoretical results (in Appendix A.3)\".\n\n\n\n> I am curious why the problem is phrased \"data valuation\"? I see nothing about pricing in this work, and to me \"evaluation\" seems like a better fit?\n\nWe note that the term \"data valuation\" is a relatively common phrase in similar existing works (Ghorbani & Zou, 2019; Kwon & Zou, 2022, Amiri et al., 2022, Wang & Jia, 2023, Just et al., 2023, Wu et al., 2022, Bian et al. 2021, Jia et al. 2018; 2019, Yoon et al. 2020).\n\nAs described in Section 1, one primary motivating application of this work (and these above existing works) is to use the value of data for the pricing of data in data marketplaces such as Datarade, Snowflake (Chen et al., 2022) (reference in main paper) and (Agarwal et al., 2019, Sim et al., 2022) (references below).\n\n---\n\nWe hope our response (and our updated definition of IC) have addressed your comments and helped improve your opinion of our work.\n\n**References**\n\nAgarwal, A., Dahleh, M., & Sarkar, T. (2019). A marketplace for data: An algorithmic solution. Proc. EC, 701\u2013726.\n\nRachael Hwee Ling Sim, Xinyi Xu and Bryan Kian Hsiang Low. (2022) Data Valuation in Machine Learning: \u201cIngredients\u201d, Strategies, and Open Challenges. IJCAI, survey track."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5560/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700286773065,
                "cdate": 1700286773065,
                "tmdate": 1700286773065,
                "mdate": 1700286773065,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9YBkcp6xxq",
                "forum": "dxJKLozjQl",
                "replyto": "lJRQ6Ft5am",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5560/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5560/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Let us know if we our response has clarified your comments"
                    },
                    "comment": {
                        "value": "We wish to thank Reviewer s7hX for the taking the time to review our paper and providing the feedback, and hope that our response has clarified your questions, in particular regarding the definition of IC. Since the discussion period is drawing to the end, let us know if you have further questions and we are happy to clarify them before the discussion period ends."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5560/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700627104216,
                "cdate": 1700627104216,
                "tmdate": 1700627104216,
                "mdate": 1700627104216,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "6EdRQOjljX",
            "forum": "dxJKLozjQl",
            "replyto": "dxJKLozjQl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5560/Reviewer_c1ZW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5560/Reviewer_c1ZW"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the problem of evaluating datasets from heterogeneous distributions. The paper adopts a Huber model on the dataset, assuming each dataset can be contaminated by samples from some noise distribution for a small factor. The principal aims to elicit truthful datasets from the data providers, and at the same time, the payment also distinguishes dataset qualities. The paper proposes to use the negative Maximum Mean Discrepancy (MMD) as the value of a distribution and to use an estimator of the MMD as the payment to incentivize truthful reports."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The paper studies an interesting problem and is well-motivated."
                },
                "weaknesses": {
                    "value": "* MMD as a valuation doesn\u2019t reflect data distribution\u2019s performance on a real decision problem / proper loss function. In contrast divergence-based valuations correspond to the value of dataset on a proper loss. \n\n* I suggest the author compare the results to previous results in information elicitation. Specifically, this following paper seems relevant. It studies elicitation with sample access to the distribution to be elicited. Suppose the principal has sample access to ground truth distribution, and sets it as a reference, it seems like the approach in this paper could be applied. \n\n    Frongillo, Rafael, Dhamma Kimpara, and Bo Waggoner. \"Proper losses for discrete generative models.\"\n\n* Suppose all N-1 distributions have the same deterministic bias, while only one distribution is the ground truth distribution. It seems like it is impossible in this case to discover or incentivize the most valuable dataset. \n\nMinor comments:\n* typo: Page 7, Subsequently, each Pi follows a Huber: Pi =(1\u2212\u03b5i)P\u2217 +Q , missing \u03b5i before Q."
                },
                "questions": {
                    "value": "* Impossibility results: as my comment 3 in Weakness, I wonder if the authors can prove impossibility results on discovering the truth. This can help justify the current approach in the paper. \n\n* Why is this bad? In the paper: \u201cTo elaborate, there is no need to explicitly learn the distributions P, P \u2032 to obtain their distribution divergence (e.g., in Kullback-Leibler divergence-based valuations\u201d. Explicitly learning the distribution doesn\u2019t sound like a drawback to me. The selection of MMD is not well-motivated. Are there other metrics that can achieve similar results?\n\n* Following the previous point, if the authors believe learning the explicit distribution is not feasible, is it possible to construct estimators of KL divergence from samples and directly estimate KL divergence similarly as the approach in this paper?\n\n* Definition of IC is unclear to me, especially the part with wlog."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5560/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5560/Reviewer_c1ZW",
                        "ICLR.cc/2024/Conference/Submission5560/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5560/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699776193468,
            "cdate": 1699776193468,
            "tmdate": 1699776381628,
            "mdate": 1699776381628,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6q2giBzJPj",
                "forum": "dxJKLozjQl",
                "replyto": "6EdRQOjljX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5560/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5560/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response (1/3)"
                    },
                    "comment": {
                        "value": "We thank Reviewer c1ZW for taking the time to review our work and providing the constructive feedback and for recognizing that our studied problem is **interesting** and **well-motivated**. We wish to address the feedback and comments as follows,\n\n---\n\n> MMD as a valuation doesn't reflect data distribution's performance on a proper loss function.\n\nIn (Si et al., 2023), the opposite is argued that the MMD can be used to provide a proper scoring rule (i.e., proper loss), because the **_continuous ranked probability score_ (CRPS) is a proper scoring rule, which can be shown to be a form of MMD**. Specifically, in Section 2 (Background) of (Si et al., 2023):\n\n> __A popular proper loss is the continuous ranked probability score (CRPS)__, defined for\ntwo cumulative distribution functions (CDFs) $F$ and $G$ as $\\text{CRPS}(F, G) = \\int (F(y) \u2212 G(y))^2 \\text{d}y$ ... The above CRPS can also be rewritten as an expectation relative to the distribution $F$:\n\\begin{equation}\n \\text{CRPS}(F, y') =  -\\frac{1}{2} \\mathbb{E}_F |Y - Y'| + \\mathbb{E}_F |Y -y'|\n\\end{equation}\nwhere $Y, Y'$ are independent copies of a random variable distributed according to $F$.\n\nAnd then in the last paragraph of the Section 2:\n\n> A special case of IPMs is maximum mean discrepancy (MMD) (Gretton et al., 2008), in which $\\mathcal{T} ={T : \\Vert T \\Vert_{\\mathcal{H} } \u2264 1}$ is the set of functions with a bounded norm in a reproducing kernel Hilbert space (RKHS) with norm $\\Vert \\cdot \\Vert_{\\mathcal{H}}$; the __CRPS objective can be shown to be a form of MMD__ (Gretton et al., 2008).\n\nIn this regard, we believe that MMD is indeed connected to a proper loss function.\n\nAs to the other part of this comment:\n\n> MMD as a valuation doesn't reflect data distribution's performance on a real decision problem.\n\nCould the reviewer elaborate on the definition of _a real decision problem_ and its potential benefits?\n\n\n>  In contrast divergence-based valuations correspond to the value of dataset on a proper loss.\n\nA divergence-based valuation does __not necessarily correspond__ to the value of dataset on a proper loss, because the result recalled by Kimpara et al., (2023), namely Theorem D.0.1 (in their Appendix D) is specific to the __Bregman divergence__. For data valuation, some divergences such as $f$-divergence and optimal transport have been considered (as compared and discussed in our Appendix B.1). To the best our knowledge, these divergences are not directly equivalent to the Bregman divergence, so the result in (Kimpara et al., 2023) is not immediately applicable.\n\n---\n\n> I suggest the author compare the results to previous results in information elicitation. Specifically, this following paper seems relevant.\n\n\nWe thank the reviewer for the reference and will include in our revision. We wish to highlight the following __connection and distinctions__:\n\nIn (Kimpara et al., 2023), Definition 3.3 (_Proper divergence_) is a key component used, and indeed MMD is __a stritcly proper divergence__. This can be proven using Theorem 5 of (Gretton et al., 2012), which ensures $\\text{MMD}(p,q) = 0 \\iff p=q$, and the fact that $\\text{MMD}(p,q) \\geq 0$ because MMD is equivalently a norm in the Hilbert space.\n\nThe distinctions are in the _goal and a technical setting_ where the goal of Kimpara et al., (2023) is to __evaluate generative models__ in the __discrete__ setting while our goal is to __characterize the sampling distributions__ for data in the __continuous__ setting.\n\nNote that we are citing the conference version of the paper, which has a different ordering of the authors from that reference (arXiv-version) raised by the reviewer.\n\n> Suppose the principal has sample access to ground truth distribution, and sets it as a reference, it seems like the approach in this paper could be applied.\n\nThis corresponds to Proposition 1 in Section 4 where we make this assumption of having access to the ground truth distribution. We subsequently relax this assumption in Section 5.\n\n---\n> Suppose all N-1 distributions have the same deterministic bias, while only one distribution is the ground truth distribution. It seems like it is impossible in this case to discover or incentivize the most valuable dataset.\n\nWe wish to highlight that our theoretical result on incentivization aims to provide a precise characterization of how the incentive compatibility is dependent on the $n$ distributions ($1$ for each of $n$ vendors), instead of trying to achieving the exact incentive guarantee for all possible cases. In the case described by the reviewer, our analysis **continues to apply**, the effect is that the effectiveness of the incentivization for the vendor with the ground truth distribution would be lower, and will decrease as the deterministic bias increases."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5560/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700285386782,
                "cdate": 1700285386782,
                "tmdate": 1700285433099,
                "mdate": 1700285433099,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iByzeh31fC",
                "forum": "dxJKLozjQl",
                "replyto": "6EdRQOjljX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5560/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5560/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Let us know if you have further questions"
                    },
                    "comment": {
                        "value": "We wish to thank Reviewer c1ZW for the taking the time to review our paper and providing the feedback, and hope that our response has clarified your comments and questions. Since the discussion period is drawing to the end, let us know if you have further questions and we will be happy to clarify them before the discussion period ends."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5560/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700627015369,
                "cdate": 1700627015369,
                "tmdate": 1700627015369,
                "mdate": 1700627015369,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KiMOTTwrQF",
                "forum": "dxJKLozjQl",
                "replyto": "6q2giBzJPj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5560/Reviewer_c1ZW"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5560/Reviewer_c1ZW"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks the authors for the clarification. I'd like to add that my comment on proper loss is more of a discussion, not a key weakness. The framework in this paper is two steps: 1) setting a reference point, either the ground truth distribution, or the average of all reported distributions; 2) estimating the divergence metric on the samples. The paper considers eps-IC, so the main difficulty is to select a divergence with good sample complexity, and there's no need for a proper loss. \n\nThe reason I mentioned proper loss is, a proper loss captures the decision loss from inaccurate data distribution. Suppose the principal faces a decision problem with an uncertain payoff-relevant state which is drawn from the ground truth distribution. The principal uses the data distribution to predict the payoff-relevant state. My question is how does she bound the loss from using inaccurate data distribution. For any decision problem, there's a proper loss calculating this loss. \n\nI'm not sure if the reference you provided shows a proper loss can be constructed with MMD. It seems like it's saying CRPS is proper, and it's a special case of MMD. \n\nTalking about exact IC, the paper I mentioned seems to solve the problem, assuming reference as the ground truth distribution. I assume their approach generalize to eps-IC when using the average of reports as a reference."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5560/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700632636688,
                "cdate": 1700632636688,
                "tmdate": 1700632636688,
                "mdate": 1700632636688,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]