[
    {
        "title": "Diffusion in Diffusion: Cyclic One-Way Diffusion for Text-Vision-Conditioned Generation"
    },
    {
        "review": {
            "id": "j8JOzYqmUY",
            "forum": "ePOjNlOjLC",
            "replyto": "ePOjNlOjLC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7628/Reviewer_45wn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7628/Reviewer_45wn"
            ],
            "content": {
                "summary": {
                    "value": "This manuscript proposes a new method called Cyclic One-Way Diffusion that integrates diffusion in physics and diffusion in deep learning, providing a learning-free manner by controlling the direction of diffusion in various customization application scenarios."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The article proposed a novel method from a new perspective to utilize the capabilities of the diffusion model.\n2. A learning-free manner can be widely used in personality customization with one or several conditions.\n3. The experiment results show good performance."
                },
                "weaknesses": {
                    "value": "1. The approach lacks a theoretical foundation, so it is not very intuitive to express why it can work.\n2. The results of comparison methods are a bit too bad. More information about the setting should be given."
                },
                "questions": {
                    "value": "I have some questions for the author to further improve this work.\n1. For the consideration of reproducibility, the code of the proposed method is suggested to be provided.\n2. Are the comparison methods learning-free? If so, it would be beneficial to provide additional details about the experimental settings. Furthermore, for the comparison experiments, it might be advisable to incorporate some learning-free methods, such as LIVR [1], to ensure a comprehensive evaluation.\n3. Sections 3.2 and 3.3 lack a theoretical foundation or pseudo-code to facilitate clearer understanding and reading.\n4. It is not clear in Section 3.3, paragraph 4. What is the difference between two steps/two ends and t-th diffusion step, and how to use them?\n5. The evaluation in Table 2 and in Section 4.2 is not matched."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7628/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698735789609,
            "cdate": 1698735789609,
            "tmdate": 1699636926779,
            "mdate": 1699636926779,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "657RdH8Jkq",
                "forum": "ePOjNlOjLC",
                "replyto": "j8JOzYqmUY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7628/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7628/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We sincerely thank you for providing your valuable suggestions and pointing out our oversights! Please see our response below:\n- **Q1:** The approach lacks a theoretical foundation, so it is not very intuitive to express why it can work.\n- **A1 :**\n    - Our method mainly builds upon the observed \"diffusion phenomenon\" of the pre-trained model as demonstrated in Fig. 2. This phenomenon is inspired by the nature that the diffusion model follows a stochastic Markovian chain between the data distribution and Gaussian distribution, characterized by a gradual entropy increase through Gaussian perturbations. In the reverse diffusion process, models focus on different levels of information at different phases, essentially embodying an extremely-noisy-to-semantic-formation-to-refine process.\n    - The two main proposed manners for utilizing the above properties are \"One-Way\" control and the \"Cyclic\" construction.  By constantly replacing in the Semantic Formation Phase, We exert influence from the given visual condition to the rest of the generated image while preserving the content of the visual condition, thus achieving a one-way conditioning. However,  the pretrained model can easily fail to generate a pleasant result with a roughly spliced starting point. So we design a cyclic \"disturb and reconstruct\" process to recursively exploit the former generation progress under the visual condition, thus achieving better generation results. \n    - Hope this additional explanation can help solve your concerns.\n---\n- **Q2:** The results of comparison methods are a bit too bad. More information about the setting should be given.\n- **A2:** \n    - We follow the corresponding official codes to conduct all the comparison methods. For ControlNet, we directly use the official webui of Stable Diffusion. We have included codes of other methods in the updated supplementary materials. \n    - The unsatisfying performance of compared customization methods (DreamBooth, TI) on semantically complex conditions like human face has also been reported in prior works [a, b, c]. One major drawback of these methods is that they are not training-free and thus tend to easily overfit the limited finetune data and incorrectly entangle object identity and spatial information. Instead, our work is learning-free, leveraging the property of diffusion denoising itself for the customized generation.\n    - The unsatisfying performance of SD-inpainting and ControlNet comes from the compulsively preserved visual condition, which may easily generate results with significant fragmentation (as shown in Fig. 1 of the main paper). Providing a small area as visual condition is a big challenge for these two methods. Meanwhile, these methods lack flexibility of fitting in multiple conditions, thus cannot be used for attribute editing or style transfer applications where the users use text prompts to slightly revise the visual conditions.\n\n[a] Jing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung. Instantbooth: Personalized text-to-image generation without test-time finetuning. arXiv preprint arXiv:2304.03411, 2023.\n\n[b] Yuheng Li, Haotian Liu, Yangming Wen, and Yong Jae Lee. Generate anything anywhere in any scene. arXiv preprint arXiv:2306.17154, 2023.\n\n[c] Ziyi Dong, Pengxu Wei, and Liang Lin. Dreamartist: Towards controllable one-shot text-to-image generation via contrastive prompt-tuning. arXiv preprint arXiv:2211.11337, 2022."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7628/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700409819616,
                "cdate": 1700409819616,
                "tmdate": 1700409819616,
                "mdate": 1700409819616,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "TNCqKGnuPr",
            "forum": "ePOjNlOjLC",
            "replyto": "ePOjNlOjLC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7628/Reviewer_4D12"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7628/Reviewer_4D12"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a ``diffusion in diffusion'' approach that leverages both physical diffusion and learning-based diffusion for the text-vision-conditioned generation, e.g., in painting, attribute editing and style transfer. The proposed method is based on inverse seed initialization and \"disturb\" and \"construct\" cycles for diffusion.  The proposed method is justified to be able to generate realistic image with higher ID-Distance and Face detection rates."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "(1) The idea of diffusion in diffusion for diffusing image to generate consistent background content in the diffusion process is an interesting idea, and can be combined with the pre-trained diffusion model without retraining.\n\n(2) The experiments on inpainting with visual condition, text-vision-conditioned generation showed that the proposed approach can produce realistic images."
                },
                "weaknesses": {
                    "value": "The overall idea of this approach is interesting. I have some questions mainly on the experimental justifications as follows.\n\n1. Most of these examples are based on putting an object in bounding box to a large image by adding backgrounds. This setting has applications, however, whether this approach can be applied to whole image generation/editing instead of pasting object on a larger image.\n\n2. In the main body of this paper, the authors should present failure cases if it has, and analyze the reasons.\n\n3. The inner diffusion cycles for \"disturb\", \"reconstruct\" may introduce additional computational overhead. More details on the computational balance on the number of cycles and its effect on the results should be given. \n\n4. There are typos in the paper, e.g., t around eq.(1). Please check the whole paper. \n\n5. On page 4, the cycles are divided into three phases. Is there strict division boundary between these phases, and why these discussions are introduced to the main body of this paper, and how to support these conclusions on the existing of three phases."
                },
                "questions": {
                    "value": "Please see above for my questions on the experiments and discussions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7628/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698803110445,
            "cdate": 1698803110445,
            "tmdate": 1699636926653,
            "mdate": 1699636926653,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oW1FMXA6WK",
                "forum": "ePOjNlOjLC",
                "replyto": "TNCqKGnuPr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7628/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7628/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your professional feedback, which played a crucial role in refining our manuscript. Please see our response below:\n- **Q1:** Most of these examples are based on putting an object in bounding box to a large image by adding backgrounds. This setting has applications, however, whether this approach can be applied to whole image generation/editing instead of pasting object on a larger image.\n- **A1:** Thanks for the valuable suggestion! We conduct experiments on the whole image generation/editing, achieving superior results. We have added those samples in the new appendix (Fig. 11 of the appendix).\n---\n- **Q2:** In the main body of this paper, the authors should present failure cases if it has, and analyze the reasons.\n- **A2:** \n    - Thanks for the valuable suggestion. We have added the failure cases and the analysis in 13 of the revised paper.\n    - We found the proposed method usually failed in two cases. First, there is a strong orientation mismatch between the visual condition face and the text prompt (e.g., back view text prompt but with front view face visual condition). Second, the model may fail when there is a clear text instruction of face coverings but the visual condition is a unobstructed face. In this case, the model may generate the covering at other random positions in the image.\n---\n- **Q3:** The inner diffusion cycles for \"disturb\", \"reconstruct\" may introduce additional computational overhead. More details on the computational balance on the number of cycles and its effect on the results should be given.\n- **A3:** \n    - The inner diffusion cycles for \"disturb\"and \"reconstruct\" are only conducted in part of the denoising path (20% of the whole process). The \"disturb\" step does not include additional computation overhead, since we just change the noise level from 500-th (low noise level) back to 700-th  (high noise level).  In this way, the denoising process can be viewed as returning back to the starting point of 700-th.\n    - Since there is redundancy in the cyclical process, we can apply skip steps strategy of DDIM to speed up the \"reconstruct\" progress by 100x. As a result, our computation cost is only 20% higher than the original Stable Diffusion inpainting model (6s versus 5s).\n    - To better address this concern, we have conducted more ablation studies to investigate the computation cost and performance of different cycle numbers and starting/ending points. The quantitive results are shown in the table below. As we can see, a cycle number of 10 is sufficient and achieves a good performance/speed balance for the inner diffusion process.\n\n| Cycle | Position &nbsp;&nbsp;&nbsp;           \\|  | Clip-T$\\uparrow$ | Human Rating$\\uparrow$ | Time Cost$\\downarrow$ |\n|:-----:|---------------------:|:----------------:|:----------------------:|:---------------------:|\n| 10    | 700$\\Rightarrow$500 &nbsp;\\| | 0.31             | 4.6                    | 5.21                  |\n| 10    | 900$\\Rightarrow$700 &nbsp;\\|   | 0.30             | 1.5                    | 5.52                  |\n| 10    | 800$\\Rightarrow$400 &nbsp;\\|   | 0.30             | 3.2                    | 7.96                  |\n| 10    | 500$\\Rightarrow$300 &nbsp;\\|   | 0.27             | 0.8                    | 4.85                  |\n| 5      | 700$\\Rightarrow$500 &nbsp;\\|   | 0.29             | 2.5                    | 3.32                  |\n| 20    | 700$\\Rightarrow$500 &nbsp;\\|   | 0.30             | 3.6                    | 8.83                  |\n\n---\n- **Q4:** There are typos in the paper, e.g., t around eq.(1). Please check the whole paper.\n- **A4:** Thanks for pointing it out! We have checked the whole paper and polished the writing again. The updated version has been uploaded in OpenReview.\n---"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7628/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700409191192,
                "cdate": 1700409191192,
                "tmdate": 1700409191192,
                "mdate": 1700409191192,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "nppLrKh1J7",
            "forum": "ePOjNlOjLC",
            "replyto": "ePOjNlOjLC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7628/Reviewer_S1EF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7628/Reviewer_S1EF"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a training-free method to better preserve the visual conditions in diffusion-based text-vision-conditioned image generation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is well-written and easy to follow. The idea is clearly illustrated.\n\n2. The proposed method is elegant and straightforward to preserve the visual condition."
                },
                "weaknesses": {
                    "value": "1. The proposed method repeatedly replaces part of the diffusion latent variable x_t with the corresponding visual condition, which strongly maintains the visual condition in the generated image. However, this method may have an intrinsic drawback: the visual condition may be too strong and conflict with other conditions. in which case, the generated image may be unrealistic.\n\n2. There are no quantitative analyses of the number of cycles, or positions of the start and end points. These experiments are important for us to understand the effectiveness of the proposed method.\n\n3. According to Table 1, the proposed method is inferior to SD inpainting on both performance and efficiency. The only superiority of the proposed method is training free. However, since it needs cyclical diffusion & denoising, its inference cost is higher than SD inpainting. The superiority may be weakened."
                },
                "questions": {
                    "value": "See the weakness above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7628/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698823457773,
            "cdate": 1698823457773,
            "tmdate": 1699636926536,
            "mdate": 1699636926536,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CAw7z1FJwK",
                "forum": "ePOjNlOjLC",
                "replyto": "nppLrKh1J7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7628/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7628/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We greatly appreciate your insightful and detailed feedback. The comments and suggestions made us think more deeply about our method and were very helpful in improving the quality of our work. Please see our point-to-point response below:\n- **Q1:** The proposed method repeatedly replaces part of the diffusion latent variable $x_t$ with the corresponding visual condition, which strongly maintains the visual condition in the generated image. However, this method may have an intrinsic drawback: the visual condition may be too strong and conflict with other conditions. in which case, the generated image may be unrealistic.\n- **A1:** \n    - In fact, the steps of \"repeatedly replace\" operation of the visual condition only occupy 20% of the generation process, while the text condition works throughout the whole process (from the layout to the detail). Our approach demonstrates a great ability to deal with different levels of conflicts between the visual and the textual conditions in a unified pipeline, as shown in Fig. 1 and Fig. 4 of our original submitted manuscript. \n    - In the meanwhile, we note the reviewer's concern on the visual-textual conflict is more of a problem for general customization tasks, but not specific to the methodological design of COW. Essentially, it is about how we would want a generative model to comprise between controversial given conditions. For instance, existing methods such as DreamBooth may also produce unrealistic images given very different conditioning prompts.\n    - In experiments we confirm COW is still able to handle different levels of conflicts.  In Fig. 7 of the new appendix, we show a gradual change in different conflict levels of textual and visual conditions. COW could even fit in the case where the visual condition has a very strong conflict with other conditions (e.g., cross-domain transformation from human to lion in Fig. 8). This further validates our model has the ability to fit in both conditions at the same time.\n---\n- **Q2:** There are no quantitative analyses of the number of cycles, or positions of the start and end points. These experiments are important for us to understand the effectiveness of the proposed method.\n- **A2:**\n    - We further conduct an ablation study on different hyper-parameters of COW. The table below provides quantitative analysis on the ablation of different settings of cycle numbers, the position of the starting and the ending points. We also included the results in Table 4 of the revised Appendix.\n    - Here we use three quantitative metrics to analyze different models. CLIP-T is the CLIP-text cosine-similarity, which is the similarity of generated image and text condition calculated by the prestrained CLIP (ViT-B/32) model. To better evaluate the visual quality of the generated samples, we also perform a human evaluation on those ablation models with 10 volunteers. Each volunteer carefully went through 216 images generated by the six model variants, and rated the best model variant as 5 and the worst as 0. Here we report the averaged ratings for each model. Since different starting and ending points would involve different inversion steps, the time cost is slightly difference according to the diffusion inversion cost. \n    - The results show the model performance is sensitive to the starting and ending points, which confirms our motivation that we should repeatedly go through the Semantic Formation phase to have a controllable and directional generation. \n\n    | Cycle | Position &nbsp;&nbsp;&nbsp;           \\|  | Clip-T$\\uparrow$ | Human Rating$\\uparrow$ | Time Cost$\\downarrow$ |\n    |:-----:|---------------------:|:----------------:|:----------------------:|:---------------------:|\n    | 10    | 700$\\Rightarrow$500 &nbsp;\\| | 0.31             | 4.6                    | 5.21                  |\n    | 10    | 900$\\Rightarrow$700 &nbsp;\\|   | 0.30             | 1.5                    | 5.52                  |\n    | 10    | 800$\\Rightarrow$400 &nbsp;\\|   | 0.30             | 3.2                    | 7.96                  |\n    | 10    | 500$\\Rightarrow$300 &nbsp;\\|   | 0.27             | 0.8                    | 4.85                  |\n    | 5      | 700$\\Rightarrow$500 &nbsp;\\|   | 0.29             | 2.5                    | 3.32                  |\n    | 20    | 700$\\Rightarrow$500 &nbsp;\\|   | 0.30             | 3.6                    | 8.83                  |"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7628/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700409563477,
                "cdate": 1700409563477,
                "tmdate": 1700409563477,
                "mdate": 1700409563477,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wbNquQwwrx",
            "forum": "ePOjNlOjLC",
            "replyto": "ePOjNlOjLC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7628/Reviewer_SshV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7628/Reviewer_SshV"
            ],
            "content": {
                "summary": {
                    "value": "This manuscript investigates the diffusion-in-diffusion processes\naiming to enable effective both pixel-level and semantic-level \nvisual conditioning. A cyclic one-way diffusion\nmethod is proposed. The cyclic method starts with an image and builds \nthe entire scene according to the text information, given a pre-trained \nfrozen diffusion model.\n\nExtensive experiments are provided for various applications.\nExperimental results, included human evaluation are provided.\nThe experiments and evaluations demonstrate that the proposed method \ncan generate images with high fidelity to both semantic-text\nand pixel-visual conditions."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The manuscript proposes a diffusion-in-diffusion process which is able to enable effective both pixel-level and semantic-level visual conditioning.\n- Extensive experiment results are provided for various applications.\n-The results indicate that the proposed method can generate images with high fidelity to both semantic-text and pixel-visual conditions."
                },
                "weaknesses": {
                    "value": "-"
                },
                "questions": {
                    "value": "What is the level of changes that may occur in the region on the seed image following the cyclic one-way diffusion?\nAccording to the results presented some changes occur sometimes in the region of the seed image from the generated image, but not in \nother cases presented."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7628/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7628/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7628/Reviewer_SshV"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7628/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699105968217,
            "cdate": 1699105968217,
            "tmdate": 1699636926420,
            "mdate": 1699636926420,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2uUOknmWID",
                "forum": "ePOjNlOjLC",
                "replyto": "wbNquQwwrx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7628/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7628/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for appreciating our contributions and providing valuable feedback! Please see our response below:\n\n**Q1:** What is the level of changes that may occur in the region on the seed image following the cyclic one-way diffusion? According to the results presented some changes occur sometimes in the region of the seed image from the generated image, but not in other cases presented. \n\n**A1:** The level of changes that may occur within the seed image depends on the discrepancy between textual and visual conditions. In most cases, the seed images can be well preserved given no explicit conflict between the visual-text conditioning pairs. At the same time, appropriate levels of text-guided changes can also be well reflected in the case of attribute editting, style transfer, and cross-domain transformation (as shown in Fig. 7 of the new supplementary file)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7628/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700406500771,
                "cdate": 1700406500771,
                "tmdate": 1700406500771,
                "mdate": 1700406500771,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]