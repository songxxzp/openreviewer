[
    {
        "title": "Understanding AI Cognition: A Neural Module for Inference Inspired by Human Memory Mechanisms"
    },
    {
        "review": {
            "id": "X6oVKNZTdX",
            "forum": "vBo7544jZx",
            "replyto": "vBo7544jZx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3652/Reviewer_4HGz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3652/Reviewer_4HGz"
            ],
            "content": {
                "summary": {
                    "value": "This work presents a novel architecture inspired by memory systems in cognitive science. The method improves performance across multiple reasoning tasks in both transformer and CNN architectures."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The proposed architecture improves performance across a diverse set of reasoning tasks.\n- A reasonable set of baseline comparisons are included.\n- An ablation study is performed to assess the impact of specific components."
                },
                "weaknesses": {
                    "value": "- The primary limitation concerns the framing of the architecture as instantiating both working memory and longterm memory. It is not clear to me that the architecture actually involves longterm memory in any meaningful sense. I think the approach would be better described as a form of relational working memory (utilizing a tensor product to capture relational information). This of course doesn't concern the method itself, which seems to perform well across multiple tasks. But I think the contribution would be much more clearly framed as a kind of working memory that exploits *relational* information. The role of relations in working memory is very well-studied in cognitive science (see references below), and I think this would make an interesting topic for discussion.\n- Is it possible to study an ablation model that includes the 'longterm' memory component but not the 'working' memory? It seems likely that the tensor product in the longterm memory component is primarily driving the gain in performance, and it would be nice if this could be isolated.\n- It would be good to cite work from cognitive science on the role of tensor product representations in working memory [1,2] as this is highly related to the outer product mechanism in the 'longterm' memory module.\n\n[1] Smolensky, P. (1990). Tensor product variable binding and the representation of symbolic structures in connectionist systems. Artificial intelligence, 46(1-2), 159-216.\n\n[2] Halford, G. S., Wilson, W. H., & Phillips, S. (1998). Processing capacity defined by relational complexity: Implications for comparative, developmental, and cognitive psychology. Behavioral and brain sciences, 21(6), 803-831.\n\nMinor comments:\n- It sounds like what is referred to as the transformer baseline in this work is actually a 'universal transformer' [3] in which parameters are shared across layers, and what is referred to as a 'high capacity transformer' is just a standard transformer (in which each layer has different parameters).\n\n[3] Dehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., & Kaiser, \u0141. (2018). Universal transformers. arXiv preprint arXiv:1807.03819."
                },
                "questions": {
                    "value": "- In what sense does the 'longterm' memory module involve long term memory more than the 'working' memory module? They both seem to operate over the same timescale, the only difference being the presence of the tensor product to capture relational interactions (which is not related to longterm vs. working memory).\n- Is it possible to ablate the 'working' memory module?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3652/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3652/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3652/Reviewer_4HGz"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3652/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698135256562,
            "cdate": 1698135256562,
            "tmdate": 1700711606544,
            "mdate": 1700711606544,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9KrXHQICRr",
                "forum": "vBo7544jZx",
                "replyto": "X6oVKNZTdX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3652/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3652/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Relational Working Memory/ Difference between WM and LTM+ Ablation Experiment Without Working Memory+ References+ Explanation of Terms"
                    },
                    "comment": {
                        "value": "We sincerely appreciate the reviewers' thorough reviewing of this paper and the valuable insights the reviewers have provided.\n\n**1.Relational Working Memory/ Difference between WM and LTM**\n\nWe are enthused that the reviewers gained a profound understanding of this paper. We would like to provide the following clarifications to address the concerns of the reviewer. In cognitive neuroscience, working memory and long-term memory differ in the following aspects: (i) purpose: working memory is primarily employed for processing and manipulating current information, while long-term memory is more geared towards storing and retrieving extensive information, including acquired knowledge and experiences. (ii) capacity: working memory is constrained by a limited capacity, whereas long-term memory boasts a significantly larger capacity. In our research, we utilize a content-based sparse cross-attention mechanism for updating working memory, where the current perception serves as both the key and value, while working memory acts as the query. In contrast, long-term memory is primarily updated through tensor outer product operations, which are not directly tied to the current perception but rather involve extracting higher-order relationships from the cumulative content of working memory. With a larger three-dimensional tensor, long-term memory can accommodate a greater volume of information. This can be interpreted as working memory paying more attention to current perceptual information, while long-term memory involves extracting higher-order relationships from the accumulated content of all previous working memories. During our research, we have indeed reviewed numerous relevant papers the role of relations in working memory. We would like to make appropriate modifications in the next version. Thank you once more for your insightful suggestions.\n\n**2.Ablation Experiment Without Working Memory**\n\nWe prefer to believe that the presence of the working memory component is justified, both from a cognitive science perspective and in the context of Transformers' attention mechanism. This is because it utilizes an attention mechanism to encode crucial information from the current perception into the working memory, which is consistent with the human cognitive process. The reviewers also acknowledge that long-term memory involves higher-order extraction of relations in working memory. If the structure of working memory were absent, it could require substantial changes to the research.\nHowever, to investigate whether the tensor product in long-term memory is the primary factor driving the gain in performance, we conducted an ablation experiment during the inference phase by removing the working memory (i.e., not utilizing the understanding of the current perception from the working memory, omitting formulas 8 and 11 in the paper, and directly extracting the understanding from long-term memory). The results are reported in the table below. Across various tasks, the absence of working memory led to a slight performance decline compared to the original best cases. However, this performance decline is far less than the impact of removing long-term memory, which indicates the importance of tensor outer product in long-term memory.\n\n| Tasks                              |Layers |MITR                  | MITR without WM      |\n| --------------------------------- | ----------------------- | ---------------------- |---------------------- |\n| SORT-OF-CLEVR                     |8| 99.34, 87.61, 62.45     | 99.26 , 84.72 , 61.86   |\n| bAbI                              |8| 2.55                    | 2.64                   |\n| DETECTING EQUILATERAL TRIANGLES   |8| 97.9                    | 97.2                   |\n| Enwik8  [1]                         |12| 0.96 bpc                | 1.01 bpc               |\n| WikiText-103 [2]                  |16   | 16.5 ppl                | 17.2 ppl               |\n\nHere, 99.34, 87.61 and 62.45 represent the accuracy of the unary, binary and ternary problems, respectively.\n\n**3.References**\n\nWe appreciate your valuable insights, and we plan to include these references in the next revision.\n\n**4.Explanation of Terms**\n\nWe would like to explain that, in this paper, transformer baseline is a standard transformer in which parameters are shared across layers, and a 'high capacity transformer' refers to a standard transformer, in which each layer has different parameters.\n\nReference\n\n[1] Mahoney, M. (2011). Large text compression benchmark.\n\n[2] Merity, S., Xiong, C., Bradbury, J., & Socher, R. (2016). Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3652/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700410172675,
                "cdate": 1700410172675,
                "tmdate": 1700466574039,
                "mdate": 1700466574039,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "to3Zp4Brk8",
                "forum": "vBo7544jZx",
                "replyto": "X6oVKNZTdX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3652/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3652/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Gentle reminder: less than one day left for the author-reviewer discussion"
                    },
                    "comment": {
                        "value": "Dear Reviewer 4HGz,\n\nWe sincerely appreciate the time and consideration you dedicate to our work. In response to your insightful comments, we have made extensive improvements in the latest version (accessible at https://openreview.net/pdf?id=vBo7544jZx). We would like to know if these modifications have effectively addressed any concerns on your part. We would be more than happy to provide further details during our reviewer-author discussion. Your response is of great importance to us, and we look forward to hearing from you.\n\nThank you sincerely!\n\nAuthors"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3652/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700670238592,
                "cdate": 1700670238592,
                "tmdate": 1700670238592,
                "mdate": 1700670238592,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "b0halvOGG6",
                "forum": "vBo7544jZx",
                "replyto": "9KrXHQICRr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3652/Reviewer_4HGz"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3652/Reviewer_4HGz"
                ],
                "content": {
                    "title": {
                        "value": "Response to rebuttal"
                    },
                    "comment": {
                        "value": "Thanks very much to the authors for these responses. I have a point-by-point reply below.\n\n## Long-term memory and working memory terminology\n\nI remain unconvinced that 'long-term memory' and 'working memory' are the appropriate terms for these modules. In cognitive psychology, long-term memory and working memory are defined primarily in terms of the timescale over which memories are maintained. While it is true that human working memory is capacity-constrained, this is not part of the definition of working memory. Therefore, I do not think it makes sense to describe one process as working memory and another as long-term memory simply because one is more capacity-constrained, when both operate over the same timescale. I also think that the contribution of the paper would be much clearer if the *relational* nature of the component referred to as 'long-term memory' were highlighted more, rather than confusingly giving the impression that these two components operate over different timescales.\n\n## 'Working memory' ablation\n\nThank you to the authors for carrying out an ablation of the 'working memory' module. While these results are informative, I think they imply that a more substantial revision of the paper is needed. The 'working memory' module does not appear to have any effect on performance (performance is better on some tasks and worse on others, and the differences are very small, it's unclear whether they are statistically significant). This suggests that the proposed model's performance is due entirely to the relational representation in the 'long term memory' module. In my opinion, the paper would be much clearer if the 'working memory + longterm memory' framing was abandoned, and the 'working memory' module was removed, highlighting instead the importance of using a relational memory representation for reasoning tasks.\n\n## Universal transformer\n\nThank you for adding the reference for universal transformers, but I still think it is confusing to describe these baselines as a 'transformer' and a 'high-capacity transformer'. The 'high-capacity transformer' is just a standard transformer, and the 'transformer' is a universal transformer. This is not clearly explained in the paper.\n\nOverall, I am still happy to vote for acceptance, but unless the issues regarding framing and clarity can be addressed I will stick with my current score (6) for now."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3652/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700684145603,
                "cdate": 1700684145603,
                "tmdate": 1700684145603,
                "mdate": 1700684145603,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qLZG1USo6p",
                "forum": "vBo7544jZx",
                "replyto": "nOqNl8elan",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3652/Reviewer_4HGz"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3652/Reviewer_4HGz"
                ],
                "content": {
                    "title": {
                        "value": "Followup"
                    },
                    "comment": {
                        "value": "Thanks very much to the authors for these final responses and revisions. I trust that they will update the framing of the final paper in line with our discussions. I will update my score to an 8."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3652/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700711591455,
                "cdate": 1700711591455,
                "tmdate": 1700711591455,
                "mdate": 1700711591455,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1mg7nWFHQz",
            "forum": "vBo7544jZx",
            "replyto": "vBo7544jZx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3652/Reviewer_AVcH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3652/Reviewer_AVcH"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a cognitive framework called PMI that consists of perception, memory, and reasoning modules. It is inspired by human memory mechanisms and aims to improve the understanding and handling of relational questions in AI systems. The memory module includes working memory (WM) and long-term memory (LTM), with LTM having a higher-order structure to retain accumulated knowledge. Current perceptions update WM through competitive write access and are merged with LTM via outer product associations. The inference module retrieves relevant information from both WM and LTM to generate comprehensive insights. The PMI enhancements consistently outperform their original counterparts in tasks such as question-answering, relation calculation, and image classification."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Integration of cognitive science and AI: The paper draws inspiration from multiple memory systems theory and global workspace theory in cognitive neuroscience, and applies these insights to develop the PMI framework for AI systems.\n\n- Novel memory module: The PMI framework introduces a dual-layer memory block with distinct communion principles, featuring working memory (WM) and long-term memory (LTM). This structure allows for efficient information filtering, storage, and knowledge consolidation.\n\n- Enhanced performance: The PMI enhancements consistently outperform their original counterparts in various tasks such as question-answering, and image classification. This demonstrates the effectiveness of the proposed framework in improving AI systems' understanding and reasoning abilities.\n\n- Clear experimental results: The paper provides detailed experimental results, including accuracy rates and convergence rates, to support the effectiveness of the PMI module. Visualizations of attention patterns further illustrate the model's ability to consolidate and integrate information from different memory sources.\n\n- Reproducibility: The authors plan to share their code once the review process is completed, ensuring the reproducibility of their experiments and allowing for further research and development in this area."
                },
                "weaknesses": {
                    "value": "- The text appears to be excessively embellished. I would like to encourage the author to employ conventional terminology, as exemplified by the authors referencing \"relation calculation\" in the abstract.\n\n- The paper includes visualizations of attention patterns between perceptions and memories, but it could benefit from providing more detailed explanations and interpretations of these visualizations. \n\n- Examining the qualitative impact of your modules on various types of tasks would provide valuable insights, rather than solely relying on quantitative results. This approach would enhance the paper's overall credibility. You can achieve this by employing various visualization techniques and similar methods.\n\n\n\n**Additional Feedback and Future Experiments:**\n\n- Enhance the clarity of the text to facilitate a deeper comprehension of the paper. Despite grammatical accuracy, the writing occasionally comes across as artificial\n\n- Incorporate additional visualizations to facilitate a clearer and more easily comprehensible text.\n\n- Consider expanding the scope of this research to include more memory-based and cognition-inspired tasks. You may find the paper titled \"Decoding the Enigma: Benchmarking Humans and AIs on the Many Facets of Working Memory\" by Sikarwar et al. to be a relevant reference in this context."
                },
                "questions": {
                    "value": "Please see weaknesses above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3652/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3652/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3652/Reviewer_AVcH"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3652/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698837493354,
            "cdate": 1698837493354,
            "tmdate": 1699636321355,
            "mdate": 1699636321355,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pHbvhHN6pe",
                "forum": "vBo7544jZx",
                "replyto": "1mg7nWFHQz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3652/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3652/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Conventional Terminology/Improve Clarity+ Explanations and Interpretations of These Visualizations+ Qualitative Impact/Incorporate Additional Visualizations+ More Memory-based and Cognition-inspired Tasks"
                    },
                    "comment": {
                        "value": "We thank the reviewers for their time in reviewing the paper and providing constructive feedback, and sincerely appreciate the reviewers for their recognition of our approach.\n\n**1.Conventional Terminology/Improve Clarity**\n\nWe sincerely appreciate the reviewer for giving us thorough feedback regarding the writing style of this paper, we will incorporate the feedback in the next revision of the paper to avoid being artificial. We will also make appropriate adjustments to the presentation structure, particularly in the visualization section, to enhance clarity.\n\n**2.Explanations and Interpretations of These Visualizations**\n\nWe would like to emphasize that we do provide a more detailed explanation in Appendix B.2 (given the limited length in the main text), illustrating the meaning of visualizations with specific examples. Detailed explanations for each case can be found in the legends of Figures 6a and 6b. Taking your valuable suggestions into account, we will diligently revise this presentation style in the next revision. We plan to appropriately integrate textual explanations into both the main text and the body of the appendix, rather than presenting them solely in the form of legends. If you find the explanations insufficient, we would like to provide additional explanations and interpretations in the next revision.\n\n**3.Qualitative Impact/Incorporate Additional Visualizations**\n\nFor this suggestion, we conduct a qualitative analysis of the proposed modules from a priori perspective on various types of tasks. We posit that as the computational steps increase, the content in working memory and long-term memory can serve as a prior for reasoning. Moreover, we use the bertviz library [14] to add visualizations of attention distributions for perception and memory on the bAbI dataset in the appendix. In addition, we add attention histograms for working memory and long-term memory on the WikiText-103 [2] dataset (new experiment) in the appendix. When predicting the next token, the attention histograms of these two different types are notably distinct. The attention histogram for long-term memory indicates its ability to focus on more long-range and relevant information, which is significantly longer than working memory.\n\n**4.More Memory-based and Cognition-inspired Tasks**\n\nWe conduct additional experiments by adding the PMI framework into the decoder of Transformers for language modeling, covering enwik8 [1], WikiText-103 [2] and PG-19 [3], to broaden the scope of this research. The results are as follows. \n\nIn addition, we thoroughly reviewed the paper \"Decoding the Enigma: Benchmarking Humans and AIs on the Many Facets of Working Memory\", which you kindly shared. We consider it an outstanding benchmark. In the next revision, we plan to conduct experiments on this dataset to further evaluate the performance of our PMI architecture.\n\n**Table 1**: Test set bits-per-character on enwik8.\n\n| Models| Layers|Params |BPC|\n|--|--|-|--|\n| Transformer-XL  [8]| 12 | 41M |1.06|\n| Transformer-XL  [8] | 24 | 277M | 0.99|\n| Compressive Transformer  [3]  | 24| - | 0.97|\n| Sparse Transformer [12]  | 30| 95M | 0.99|\n| Adaptive Transformer [13] |12| 39M| 1.02 |\n| RMT [9] | 12     | - | 1.222|\n| TIMS+HSW [5]*  | 12| 43M| 1.36|\n| MITR (ours) | 12| 45M| **0.96**|\n\n**Table 2**: Comparison with other models on WikiText-103.\n\n| Models | Layers | Params |Valid PPL |Test PPL |\n|--|-|-|--|--|\n| LSTM [6]|  - | - | -  | 48.7   |\n| RMC [7]|-| 30.8   | - | 31.6 |\n| Standard Transformer-XL [8] | - | 151M   | - | 24 |\n| RMT [9]  | 16|- |-  | 24.85|\n| Compressive Transformer [3] | 18| - | 16 | 17.1|\n| Transformer-XL Large [8]  | 18| 257M   | - | 18.3|\n| TIMS+HSW [5]* | 8| 112M   | 35.9 | 36.7|\n| MITR (ours)   | 8 | 116M | 24.9 | 23.8 |\n| MITR (ours)   | 16| 233M| 15.3| **16.5** |\n\n**Table 3**: Results on language modeling on PG-19 dataset.\n\n| Models | Layers|Valid PPL|Test PPL| \n|--|--|-|--|\n| Transformer-XL [8] | 36| 45.5| 36.3|\n| Compressive Transformer [3]   | 36 | 43.4| 33.6|\n| \u221e-former [10]  |12| - | 32.48|\n| Routing Transformer [11] | 12| - | 33.2|\n| TR+HSW [5]* | 12 | 39.46 |32.46|\n| MITR (ours) | 12 |37.12| **31.04**|\nHere, * signifies the results obtained from our experiments.\n\n[14] https://github.com/jessevig/bertviz"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3652/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700410145115,
                "cdate": 1700410145115,
                "tmdate": 1700410145115,
                "mdate": 1700410145115,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nYW2oxSSqn",
                "forum": "vBo7544jZx",
                "replyto": "1mg7nWFHQz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3652/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3652/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Gentle reminder: less than one day left for the author-reviewer discussion"
                    },
                    "comment": {
                        "value": "Dear Reviewer AVcH,\n\nWe are grateful for your valuable feedback. In response to your insightful comments, we have made extensive improvements in the latest version (accessible at https://openreview.net/pdf?id=vBo7544jZx). Specifically, we have provided additional clarity on visualization in Appendix B.2, and introduced new findings from qualitative analysis in Appendix C. \n\nWe are eager to ascertain if these modifications have effectively addressed any concerns on your part. We would be more than happy to provide further details during our reviewer-author discussion. Your response holds great importance for us, and we sincerely appreciate the time and consideration you dedicate to our work.\n\nThank you sincerely!\n\nAuthors"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3652/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700669835138,
                "cdate": 1700669835138,
                "tmdate": 1700670039523,
                "mdate": 1700670039523,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "jsZQpSJU2X",
            "forum": "vBo7544jZx",
            "replyto": "vBo7544jZx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3652/Reviewer_SKET"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3652/Reviewer_SKET"
            ],
            "content": {
                "summary": {
                    "value": "Inspired by human brain\u2019s memory system and cognitive architectures,this paper propose a PMI framework that consists of perception, memory and inference components. Notably, the memory module comprises working and long-term memory, with the latter endowed with a higher-order structure to retain more accumulated knowledge and experiences. \n\n\nIn my opinion, the motivation of this paper is meaningful because it comes from the human brain's memory. \nAnd the proposed memory module looks like powerful because it consists of working memory and long-term memory.\nHowever, the experiments may not be enough due to it not compare with other memory augment models, such as the memory augment language model. and this paper not take experiments on language generative task.\nBesides, the"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The motivation is sometimes novel and comes from human's brain memory. \n\n2. The proposed model is meaningful with its novel motivation\n\n3. The paper is well written, and the image is easy to understand."
                },
                "weaknesses": {
                    "value": "1. the experiments may not be enough to compare it with other memory-assisted language model\n\n2. The experiments is hard to understand, and i think it is not necessary to conduct experiments on image classification. And there is little work on the memory augment image model due to the image's too long context. \n\n3. I don't see any connection between your work and the title, the author maybe need change a title due to this model hard to help us underanding AI .\n\nI hope the author takes more experiments on the language model to solve the longer context challenge."
                },
                "questions": {
                    "value": "No"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3652/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699025910984,
            "cdate": 1699025910984,
            "tmdate": 1699636321280,
            "mdate": 1699636321280,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "C2gfQhuMZP",
                "forum": "vBo7544jZx",
                "replyto": "jsZQpSJU2X",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3652/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3652/Authors"
                ],
                "content": {
                    "title": {
                        "value": "More Experiments on Language Modeling+ Compare with Other Memory Augment Models+ Image Classification+ Title"
                    },
                    "comment": {
                        "value": "We thank the reviewers for their time in reviewing the paper and providing constructive feedback, and sincerely appreciate the reviewers for their recognition of our approach.\n\n**1.More Experiments on Language Modeling**\n\nTo further validate the effectiveness of the proposed approach in long-sequence language modeling, we applied MITR\u2014a decoder-only transformer embedded with our PMI module, to a variety of datasets on both character-level and word-level language modeling, including enwik8 [1], WikiText-103 [2] and PG-19 [3]. Enwiki8 is utilized for character-level language modeling and comprises 100M bytes of unprocessed Wikipedia text. Both WikiText-103 and PG-19 are benchmarks for word-level language modeling with long-term dependency, with the former containing 103M tokens from 28K English Wikipedia articles and the latter from English books published before 1919.\nPart of the implementation is based on Transformer-XL repository [4]. The experiment on Enwik8 employs a 12-layer MITR (8 heads, 512 hidden size, 2048 intermediate FF), WikiText-103 uses a 16-layer MITR (10 heads, 410 hidden size, 2100 intermediate FF), while PG19 uses a 12-layer MITR (8 heads, 1024 embedding size, 4096 intermediate FF). In addition, we use Adam optimizer with linear schedule learning rate starting from 0.00025 for 500,000 steps and set the working memory size M=8, the long-term memory size N=5 and top-k=5 on all the datasets. The complete hyperparameter settings are available in our repository as well as in the Appendix. \n\nThe test bits per character (BPC) on the Enwiki8 dataset and the perplexity (PPL) on WikiText-103 and PG-19 are reported in the table below. Notably, we improve the results of bpc/perplexity to 0.96 on enwiki8, 16.5 on WikiText-103 and 31.04 on PG19, which demonstrates the superiority of the PMI architecture. \n\n**Table 1**: Test set bits-per-character on enwik8.\n\n| Models| Layers|Params |BPC|\n|-|-|-|-|\n| Transformer-XL  [8]|12| 41M |1.06|\n| Transformer-XL  [8] |24| 277M|0.99|\n| Compressive Transformer  [3]|24|-|0.97|\n| Sparse Transformer [12]| 30| 95M |0.99|\n| Adaptive Transformer [13] |12| 39M|1.02 |\n| RMT [9] |12| - |1.222|\n| TIMS+HSW [5]*|12| 43M|1.36|\n| MITR (ours) |12|45M| **0.96**|\n\n**Table 2**: Comparison with other models on WikiText-103.\n\n|Models|Layers|Params|Valid PPL|Test PPL|\n|-|-|-|-|-|\n| LSTM [6]|-|-|-|48.7|\n| RMC [7]|-| 30.8| -|31.6 |\n| Standard Transformer-XL [8] |-|151M|-| 24 |\n| RMT [9]|16|-|-| 24.85|\n| Compressive Transformer [3] |18|-|16|17.1|\n| Transformer-XL Large [8]| 18| 257M| - |18.3|\n| TIMS+HSW [5]* | 8| 112M| 35.9 |36.7|\n| MITR (ours) |8|116M |24.9|23.8|\n| MITR (ours) |16| 233M|15.3|**16.5**|\n\n**Table 3**: Results on language modeling on PG-19 dataset.\n\n| Models | Layers|Valid PPL|Test PPL| \n|-|-|-|-|\n| Transformer-XL [8]|36|45.5| 36.3|\n| Compressive Transformer [3]| 36 |43.4| 33.6|\n| \u221e-former [10]|12|-|32.48|\n| Routing Transformer [11] |12|-| 33.2|\n| TR+HSW [5]*|12 |39.46 |32.46|\n| MITR (ours) |12|37.12|**31.04**|\nHere, * signifies the results obtained from our experiments.\n\n**2.Compare with Other Memory Augment Models**\n\nWe would like to point out that we do provide comparisons with other memory-augmented models. For example, TR+HSW model mentioned in this paper is a variant of Transformers with a working memory proposed by Goyal et al. [5]. In the text-based question-answering bAbI in Section 4.2 (Table 1), we compare with various memory-augmented models, including DNC, NUTM and H-Mem, etc. Moreover, in the newly introduced language modeling tasks, we compare with several memory-augmented models, such as LSTM [6], RMC [7], Transformers-XL [8], and Compressive Transformer [3], etc.\n\n**3.Image Classification**\n\nFor image classification, our aim is to demonstrate that the architecture embedded with our PMI, whether it belongs to the transformers or convolutional series, could outperform the original models. This is to establish the generality of the PMI, showcasing its applicability beyond Transformers. Additionally, the original intent of the paper is to prove that the introduction of PMI could enhance the model\u2019s inference ability, which we think maybe include the ability for image classification, while not approaching it from the perspective of long sequences. However, considering your constructive suggestion, we will place this experiment in the appendix, while adding language modeling experiments into the main text. What are your thoughts on this arrangement?\n\n**4.Title**\n\nAfter thoughtful consideration of your valuable feedback, we are planning to revise the title to \"A PMI Framework for Inference Inspired by Human Memory Mechanisms\", considering that the focal point of this paper lies in leveraging theoretical insights from cognitive science to construct a cognitive architecture named PMI, and embed it into mainstream architecture such as Transformers and convolutional networks to help us build AI systems for inference that align more closely with human cognition."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3652/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700410035842,
                "cdate": 1700410035842,
                "tmdate": 1700410035842,
                "mdate": 1700410035842,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sDsso65NQj",
                "forum": "vBo7544jZx",
                "replyto": "jsZQpSJU2X",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3652/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3652/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Gentle reminder: less than one day left for the author-reviewer discussion"
                    },
                    "comment": {
                        "value": "Dear Reviewer SKET,\n\nWe would like to express our gratitude for your valuable feedback. In the latest revision (available at https://openreview.net/pdf?id=vBo7544jZx), we have diligently addressed each of the concerns you raised. We are eager to learn if these changes have effectively alleviated any confusion on your part and more than willing to provide further details during the author-reviewer discussion. Your response is immensely important to us, and we genuinely appreciate the time and consideration you dedicate to our work.\n\nThank you sincerely!\n\nAuthors"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3652/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700669040373,
                "cdate": 1700669040373,
                "tmdate": 1700669040373,
                "mdate": 1700669040373,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Op59r1ZvID",
                "forum": "vBo7544jZx",
                "replyto": "jsZQpSJU2X",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3652/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3652/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Gentle reminder: less than six hours left for the reviewer-author discussion"
                    },
                    "comment": {
                        "value": "Dear reviewer SKET,\n\nWe sincerely apologize for taking up some of your time, and we greatly appreciate the time and effort you have dedicated to reviewing our work. However, we've noticed that the summary part of your initial comments appears to have been truncated (\"Besides, the\u2026\u2026\"), and we wonder if it's due to a system issue or other reasons. We genuinely hope to receive your complete comments. \n\nFurthermore, we have individually addressed the other issues you raised, visible to us in the latest reversion (accessible at https://openreview.net/pdf?id=vBo7544jZx). We are looking forward to your reply and at your disposal for any additional information.\n\nFinally, we wish you a happy Thanksgiving Day and may each day be delightful for you.\n\nBest Regards,\n\nAuthors"
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3652/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700719233448,
                "cdate": 1700719233448,
                "tmdate": 1700719233448,
                "mdate": 1700719233448,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]