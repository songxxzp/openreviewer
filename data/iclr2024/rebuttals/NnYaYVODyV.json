[
    {
        "title": "Perceptual Group Tokenizer: Building Perception with Iterative Grouping"
    },
    {
        "review": {
            "id": "Fnlnh4Nfwf",
            "forum": "NnYaYVODyV",
            "replyto": "NnYaYVODyV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission438/Reviewer_ABtJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission438/Reviewer_ABtJ"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes the Perceptual Group Tokenizer model (PGT), which utilizes grouping operations for visual feature extraction. It demonstrates competitive performance in self-supervised learning while reduces the computation complexity into O(n*m) compared with the complexity O(n^2) of vision transformer. The PGT also offers adaptive computation without re-training via flexible number of grouping tokens, and offers interpretability in feature representations.  Quantitative experiments and visualization demonstrate the PGT's effectiveness."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The idea of Perceptual Group Tokenized is intrersting. By alternatively refining the group tokens and image feature tokens, the computation complexity has been reduced comparing with self-attention. \n2. The adaptive computation ability of PGT is good, which could be used to meet the needs of different inference speeds. \n3. The interpretability of model is relatively good."
                },
                "weaknesses": {
                    "value": "1.  The experiment is not sufficiently comprehensive and the results are weak. Since a new model architecture is proposed, the performance under both supervised learning and self-supervised learning should be presented to show its\u2019 universal. In performance comparison, the listed comparable architectures should be compared with the same pre-training/supervised-training strategy, a similar amount of parameters. It will be better to do experiment with more model sizes(e.g, PGT-B, PGT-S,PGT-Ti or more ) and compare with counterparts under different sizes. The performance on downstream tasks are not clear, although segmentation performance is reported, more comparisons with relative methods are not sufficient.\n2.  Besides the number of parameters, for fair comparison, the computational cost(Gflops or MACs) should be shown. The inference time is missing, which is important for evaluating models\u2019 efficiency."
                },
                "questions": {
                    "value": "1. Do the group tokens in each block generate independently? If not, maybe the relavant description is ambiguous. If so, why don't the group tokens take its' output of the previous block as the next block's input?\n2.  Could the iterative grouping processes be considered multiple cross-attention between input tokens and group tokens? What's the difference or advantages of the iterative grouping processes compared with cross-attention? How much does the number of grouping iterations matter?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission438/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission438/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission438/Reviewer_ABtJ"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission438/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698495652361,
            "cdate": 1698495652361,
            "tmdate": 1699635970511,
            "mdate": 1699635970511,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SjeGIul6VQ",
                "forum": "NnYaYVODyV",
                "replyto": "Fnlnh4Nfwf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission438/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission438/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the feedback"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer\u2019s feedback and comments. Specific questions are addressed below.\n\n**W1.1. Multiple model sizes**\n\nWe agree and are working on the results with various model sizes, the performance of variants will be included in the final version. Providing one data point here, we develop a PGT-S with 34M parameters, the model is still in early training (1/3 course) and show similar performance (76.8%) with PGT-B (77.3%)\n\n**W1.2 Results and others**\n\nWe have updated our results in the paper. The table 1 is arranged for more clear fair comparison. Note that the performance of 80.3% is a strong result under the perceptual grouping paradigm, especially, we show that PGT has a much lower memory cost in inference. \n\n**W2. Profiling computational costs and stats**\n\nFollowing the reviewer\u2019s suggestion, we have updated our paper with following modifications: peak memory usage in table 3, inference time in A.2.3, and Gflops in A.2.4. Especially to highlight, PGT has a very low memory cost compared to ViT under fair comparison, summarized in table 3.\n\n\n**Q1. Independent group tokens**\n\nThis is a great question. Yes, each layer\u2019s group tokens are generated independently. Our paper aims to show that grouping operations can be used as a more general operator to exchange information among input tokens, compared to self-attention. Both operations are per-layer refinement. Adding cross layer connections can complicate the process. \n\nRegardless, we indeed have tried reusing the previous layer\u2019s group tokens as a variant (e.g., residual connections, or cross attention), and found that the performances are similar. This indicates that the grouping results are already stored in the refined patch token embedding vectors and can be easily recovered in the new round.\n\n**Q2. How to consider grouping**\n\nGrouping uses attention, but performs softmax in the query (group tokens) axis, different from cross attention (in the key axis). This ensures that each operation is an assignment. We have explored using key axis softmax and found the performance dropped a lot. \n\nGrouping iterations are very important in general. We explain some results in the appendix, A.2.2. Empirically, having a sufficient amount of binding process (e.g. 3) will always help on the performance, especially when the model depth is not too high. We\u2019ve also tried using learnable tokens each layer to perform one-step cross attention and found it leads to strong overfitting.\n\nInstead of viewing grouping as a deterministic procedure (like cross attention), we think it\u2019s better to view it from a probabilistic perspective, where there is a distribution over group tokens that summarize the tokens. With a better distribution, the performance can be boosted. For example, we add one more experiment where the initial Gaussian sampling distribution is replaced by a more advanced Normalizing Flow sampler. The normalizing flow models a more flexible initial distribution that better covers the space. We found that it can lead to an increase in the final performance. \n\nWe appreciate your comments and hope the above answers address the reviewer\u2019s questions and look forward to your response. Please let us know if you have more questions!"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission438/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700717489128,
                "cdate": 1700717489128,
                "tmdate": 1700717889458,
                "mdate": 1700717889458,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bKyKLgjGiG",
            "forum": "NnYaYVODyV",
            "replyto": "NnYaYVODyV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission438/Reviewer_TtGg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission438/Reviewer_TtGg"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a self-supervised learning method that relies only on grouping operations. They call their model the Perceptual Group Tokenizer (PGT). The model's linear probe performance is on par with other state-of-the-art models such as ViT. It also produces a highly interpretable representation and shows some interesting properties like being somewhat adaptable at inference time. Overall, the PGT demonstrates that grouping operations alone can produce a rich visual representation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The writing of this work is very clear. The authors do a great job of motivating the problem and contextualizing its significance. As the authors note, perceptual grouping has historically been an important concept in computer vision, but it had not been proven to be as powerful as feature detection. This work proposes a novel way to use grouping principles for self-supervised representation learning on large-scale natural image data. \n\nThe approach also appears to be well done. The analyses of number and size of grouping are also insightful, and demonstrate a thorough evaluation of their model. Making the connection to self-attention in ViT is also an important and original contribution for understanding why both the PGT and ViT work."
                },
                "weaknesses": {
                    "value": "One of the main weaknesses I see is that the PGT performs very similarly to the ViT. As the authors note, their method is in some sense a more general version of the self-attention approach so it is somewhat unsurprising that performance is on par with ViT. I think the paper could be strengthened by more discussion about why the perpetual grouping tokenizer might be better or more useful than other methods like ViT.\n\nAlong similar lines, I think the interpretability results in Figure 6 and section 4.5 could be better contextualized. It is not immediately obvious how the interpretability conveyed by the attention maps compares to other state-of-the-art models. In addition, this analysis could be strengthened by a discussion of how the attention maps at each stage do or do not agree with what we would expect of human vision."
                },
                "questions": {
                    "value": "As I mention in the weaknesses, could the authors touch on what are the benefits of the PGT over ViT given that they perform similarly on the ImageNet 1k linear probe?\n\nIn a similar vein, I would also like some of the model interpretability results to be expanded on in the context of other state-of-the-art methods as well as human visual perception.\n\nFinally, I thought adaptive computation (Section 4.3) was a bit too brief for me to appreciate. Could the authors explain what they mean by adaptive computation and why it is a benefit of PGT?\n\nGenerally, I enjoyed this paper. I am open to increasing my score if these concerns are addressed,"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission438/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission438/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission438/Reviewer_TtGg"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission438/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698620518557,
            "cdate": 1698620518557,
            "tmdate": 1699635970422,
            "mdate": 1699635970422,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LUjkd4f3L0",
                "forum": "NnYaYVODyV",
                "replyto": "bKyKLgjGiG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission438/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission438/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the constructive feedback!"
                    },
                    "comment": {
                        "value": "We appreciate your insightful feedback and would like to thank the reviewer for your comments and acknowledgement on our work. We address the questions below.\n\n**W1&Q1. Compared to ViT**\n\nThis is a great question. We answer in following perspectives: \n\n- ViT is a grouping with a fixed number of grouping slots by design (e.g. the patches), this limits its generalizability across other vision tasks, where larger resolution is needed, or many frames need to be processed (e.g. videos). PGT backbone separates the concept of \u201cgroup tokens\u201d out, making it much more capable of handling a broader class of vision tasks without the need to adapt the model to a more efficient version.\n\n- PGT has the potential to generate better tokens. ViT and ConvNet have shown similar results in classification, but ViT is more favored due to its meaningful token and easier alignment with language in multimodal training. PGT, as a more flexible model, can combine patches into regions and produce better tokens. This can be even more true in videos, where patches of objects can be grouped and tracked across frames. \n\n- PGT has a probabilistic nature in each grouping process, naturally adapts to different number of group tokens without re-training.\n\n**W2&Q2. Visualization interpretation.**\n\nThanks for the suggestion! We have added text in the paper, and also explain here for easier access.\n\nHighlighting a key difference compared to ViT, we find that the grouping process can automatically generate object groupings and segments corresponding to different group tokens, while a standard ViT in DINO can only extract a single foreground using [CLS] token without relying on other adaptations. Using multiple learnable embeddings recently show similar effects[a] with our model, but it can potentially be less flexible and adaptive with various number of group tokens due to a lack of probabilistic nature. \n\nOur model shows both human-like and fragmented-to-human grouping results. In the current SSL framework, the loss uses \u201cparts-to-whole\u201d or \u201cparts-to-parts\u201d matching and encourages alignment within the same 2D image. This either leads to object confusion when multiple objects appear, or a lack of sense of clear object boundaries since no temporal information is used. Human vision is sensitive to moving objects and trained with 4D space. We believe with a similar dataset, environment and optimization loss design, e.g. training vision models with time axis and interactions, our grouping model can potentially produce groupings more similar to human's.\n\n**Q3. Adaptive computation.**\n\nIn the general image domain, where images are not curated, the number of objects in the same image can vary. PGT has the adaptive computation ability, where, during training, only a fixed number of group tokens (e.g. 256) is used, in inference time the trained model can use a different number of group tokens to produce image features and group proposals.\n\nThis is because of the probabilistic nature of PGT: initial group tokens are sampled from a distribution, and one can sample as many tokens as possible given a distribution. If using a fixed number (e.g., K) of learnable embeddings, it cannot go above the fixed number (K). We also observe that this adaptivity and robustness can come from using sampling during training, which introduces random noises in the grouping procedure.\n\n[a]. Vision Transformer Needs Registers, Timoth\u00e9e Darcet, Maxime Oquab, Julien Mairal, Piotr Bojanowski \n\nWe appreciate your suggestions and hope the above answers address the reviewer\u2019s questions and look forward to your response. Please let us know if you have more questions!"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission438/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700717404398,
                "cdate": 1700717404398,
                "tmdate": 1700717404398,
                "mdate": 1700717404398,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "AY5Qg4MfL1",
            "forum": "NnYaYVODyV",
            "replyto": "NnYaYVODyV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission438/Reviewer_P2FR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission438/Reviewer_P2FR"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes the Perceptual Group Tokenizer (PGT), a ViT-like architecture that clusters tokens to implement the principle of perceptual grouping for visual recognition. Specifically, PGT combines a slot attention-like token grouping module within ViT blocks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- The paper is generally well-written.\n- Prior work on feature detection and perceptual grouping is well-discussed, although recent works on token grouping are missing."
                },
                "weaknesses": {
                    "value": "**Limited technical novelty**\n\nThe proposed method is essentially a combination of ViT and slot attention (or its token grouping variants).\nThe concept of token grouping has been extensively studied in prior work. Please refer to the related work section of ToMe [1] and CoCs [2] for examples.\nOn the other hand, DINOSAUR [3] combined a ResNet/ViT encoder with slot attention to scale up object-centric learning for real-world images, which is also relevant to this work.\n\n[1] Token merging: Your vit but faster. ICLR'23.\\\n[2] Image as Set of Points. ICLR'23.\\\n[3] Bridging the Gap to Real-World Object-Centric Learning. ICLR'23.\n\n---\n**Unclear empirical benefits**\n\nIn addition to the technical novelty, the empirical merit of the proposed method is unclear.\n1. Performance: It is not better than prior works, such as DINO.\n2. Efficiency: Several efficient ViT-based works exist, such as ToMe, DynamicViT [4], A-ViT [5], etc.\n3. Adaptive computation w/o retraining: ToMe claims to offer the same benefit.\n4. Grouping visualization: Other grouping methods also offer similar advantages (Fig. 4 of ToMe). CAST [6] is even better in this regard.\n\n[4] DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification. NeurIPS'21.\\\n[5] A-ViT: Adaptive Tokens for Efficient Vision Transformer. CVPR'22.\\\n[6] CAST: Concurrent Recognition and Segmentation with Adaptive Segment Tokens. arXiv'22."
                },
                "questions": {
                    "value": "There are many similar works in recent years. What are the substantial differences or unique advantages of this work?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission438/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission438/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission438/Reviewer_P2FR"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission438/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698782452430,
            "cdate": 1698782452430,
            "tmdate": 1700148401075,
            "mdate": 1700148401075,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "abt2UNZwke",
                "forum": "NnYaYVODyV",
                "replyto": "AY5Qg4MfL1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission438/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission438/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the review"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer\u2019s feedback and comments. A new revision is submitted with the updated draft. We address the specific questions below.\n\n**W1. Concerns about the novelty of the paper**\n\nWe disagree with this point. The mentioned papers are relevant, but we are delivering fundamentally different messages in our paper, which we clarify below:\n\n- The core message\n\nWe replace the self-attention operation with grouping operations, showing that grouping operations can perform feature learning in self-supervised learning. This alone separates our paper with other works [1,3,4,5,6], which still rely on ViT\u2019s self attention operations or convolution layers. \n\nSpecifically, [1,4,5] are ad-hoc adaptations for pruning ViTs and orthogonal to our work. The techniques like patch token drop can directly be applied to PGT input tokens x as an efficiency improvement method. CAST[6] uses object proposals from traditional vision methods as input, this makes it not comparable to most backbones, including PGT. [2] is indeed relevant, but focuses on supervised learning, uses non-standard operations (fixed center feature pooling, aggregation with sigmoid activations, patch pooling for reduction, etc.).  Instead, in our paper, we are the first to cleanly use *iterative grouping to replace the self attention* operation and show a strong performance.\n\nWe also show that the grouping tokens as communication channels among input tokens is a more general form of information-exchange operator. This generalizes self-attention and can potentially lead to more advances in vision models.\n\nWe do not think the mentioned works [1-6] deliver these messages.\n\n- Self-supervised learning\n\nPerceptual grouping is a general vision framework with a long history, and the root for many vision concepts and algorithms, *especially unsupervised*. There is a natural connection between pixel similarity measurement, grouping, and providing training signals from images themselves. Many classical vision tasks, such as segmentation, tracking, and 3D multiview problems can be formulated as grouping processes. Showing grouping works, especially under self-supervised learning framework, can be a critical stepstone to unify many vision tasks. \n\n- We do not combine ViT and Slot attention.\n\n The main components of ViT are tokenization, self attention, and mlp, where self attention is the core. We use tokenization, grouping and mlp. There is no ViT in our model.\n\nSlot attention combines the grouping procedure with ConvNet, showing the feature maps from the last layer of convolutional nets can be grouped. We instead show grouping can be used as a core operation to drive the entire feature learning.\n\n\nWe appreciate that the reviewer raises these relevant and inspiring works, and have added paragraphs discussing them in detail in section 2.\n\n\n\n**W2. Empirical benefits**\n\n  Note that we do not aim to propose an all-round model that beats every possible aspect of state-of-the-arts from all related works, and most papers do not either. As discussed above, the mentioned papers [1,4,5] are orthogonal to our work and can be applied to PGT. The intriguing part of PGT is that the performance, memory efficiency, adaptive computation and interpretability are all in a single model cleanly driven by one principle.\n\nWe hope the explanation and response above address the reviewer\u2019s questions. \n\n[1] Token merging: Your vit but faster. ICLR'23.\n\n[2] Image as Set of Points. ICLR'23.\n\n[3] Bridging the Gap to Real-World Object-Centric Learning. ICLR'23.\n\n[4] DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification. NeurIPS'21.\n\n[5] A-ViT: Adaptive Tokens for Efficient Vision Transformer. CVPR'22.\n\n[6] CAST: Concurrent Recognition and Segmentation with Adaptive Segment Tokens. arXiv'22."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission438/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700717389344,
                "cdate": 1700717389344,
                "tmdate": 1700723508685,
                "mdate": 1700723508685,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "kFuEEeuDa2",
            "forum": "NnYaYVODyV",
            "replyto": "NnYaYVODyV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission438/Reviewer_qQDA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission438/Reviewer_qQDA"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a perceptual group tokenizer, which just uses the grouping operations to extract visual features and perform self-supervised learning. The authors also explain the connection between the proposed perceptual group tokenizer and the self-attention. The experimental results show the performance is competitive with some state-of-the-art self-supervised methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The concept of perceptual group tokenizer is novel, and seems to enable the networks to have more good properties including interpretability and so on.\n\n2. Discussion between the perceptual group tokenizer and the self-attention is interesting, and can provide a new vision for vision transformer design."
                },
                "weaknesses": {
                    "value": "1. The motivation is not clear. Since we have powerful vision transformers already, what is the advantages of the proposed perceptual group tokenizer. The authors claim that the perceptual group tokenizer have good properties such as adaptive computation without re-training and interpretability. However, I don't see the experimental results or the visualization that can provides proof of this claim.\n\n2. The performance is still concerned. Because we should also focus on the accuracy despite some good properties, the results shown in Table 1 demonstrate that the method cannot beat the baselines. Moreover, the state-of-the-art methods proposed in 2023 are not compared.\n\n3. The explanations to the grouping operation should give more details. Since grouping seems to be a explicit operation, the implicit operations for grouping such as MLP should show the correlation of the term \"grouping\"."
                },
                "questions": {
                    "value": "See weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission438/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission438/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission438/Reviewer_qQDA"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission438/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699036947488,
            "cdate": 1699036947488,
            "tmdate": 1699635970240,
            "mdate": 1699635970240,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "00DaguKtGT",
                "forum": "NnYaYVODyV",
                "replyto": "kFuEEeuDa2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission438/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission438/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the review"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer\u2019s feedback and comments. A new revision is submitted with the updated draft. We address the specific questions below.\n\n**Q1.1. Motivation**\n\nAs mentioned in the introduction, our motivation and contribution is on the fundamental mechanisms of vision backbones. We would like to both refer the reviewer to our first half of introduction, and explain the motivation here. \n- Fundamental paradigms. \n\nPerceptual grouping in computer vision has deep roots and many vision concepts/methods are developed under that paradigm, for example, segmentation, object tracking, action recognition, etc. Showing that by simply applying one principle - perceptual grouping - can lead to state-of-the-art performance, is very encouraging. We believe this principle can be transferred to related fields and applications given their deep connections with grouping. \n\n- link to ViT\n\nAs discussed in the paper, PGT with grouping can be considered as a more generalized form of ViT. ViT will always have fixed number of grouping slots (equal to number of patches), while PGT separates the concept out and allows for flexible customization on number of groups. This makes it more generalizable to a broader class of vision tasks which require flexible designs in number of objects/groups (e.g., tracking). We also build connections between grouping and self-attention (section 3.4), serving as a generalization in the operator design and an explanation on why ViT works. This perspective can spur more variations of grouping ops to go beyond self attention and advance the vision backbone designs. \n\n- Generating better tokens. \n\nOne big advantage of ViT over ConvNet is its meaningful tokens[b], although it has been consistently shown that ConvNet can achieve similar performance[b]. The tokens can be easily aligned with language tokens in multimodal training[b]. Perceptual grouping models have the potential to produce even better tokens by design. The grouping process is constructed to produce a set of tokens that flexibly summarize the \u201cdetailed small patches\u201d into tokens with more semantic meanings. \n\n- Efficiency.  \n\nAs shown in table 3, the peak memory usage of perceptual grouping models is much less compared to ViT. The memory advantage is critical in larger resolutions, videos or multiview images.\n\n**Q1.2 Adaptive computation and grouping results**\n\nOur results of adaptive computation are in section 4.3, specifically table 3. \n\nThe grouping interpretability results are shown in figure 5 and 6. For example, in figure 6, each large attention image corresponds to a group token, an image is parsed into smaller meaningful regions, such as apple, jar and handle, or camel, camel legs and person. More explanation is in section 4.5 and figure 6 caption. \n\n**Q2. Performance**\n\nWe have updated the results of our model. We better aligned with DINO\u2019s preprocessing, where we previously omitted random blur. The current results have shown on par or better on top-1 accuracy.  Note that this performance is achieved with much less inference memory cost. \n\nWe also would like to point out that only the lower half of table 1 are  fair comparisons. We fully follow DINO\u2019s framework and only compare the backbone performance. The upper half are cited as reference for other backbone architectures. We cite works within the similar time range of DINO. With better loss functions and training procedure, PGT can potentially have better performance.\n\n**Q3. Grouping operation explanation**\n\nThe grouping process can be considered as a deterministic approximation for variational inference framework, where the group token embeddings are the latent variables **Z**. The grouping modules, including GRU, MLP, attention, and other layers are designed to better infer the embeddings (latent variables). The training signal is a pragmatic loss (instead of reconstruction loss), which has been demonstrated in [c,d]. The key differences are: (1) there is no sampling in each inference step; (2) the regularization from unit Gaussian distribution is set to zero. We do believe a full probabilistic treatment of the perceptual grouping architecture can be a very interesting next step. An explanation is added in the appendix in section A.2.5.\n\n[a]. Self-attention in Vision Transformers Performs Perceptual Grouping, Not Attention. Paria Mehrani, John K. Tsotsos\n\n[b]. ConvNets Match Vision Transformers at Scale. Samuel L. Smith, Andrew Brock, Leonard Berrada, Soham De\n\n[c] Pragmatic Image Compression for Human-in-the-Loop Decision-Making. Siddharth Reddy, Anca D. Dragan, Sergey Levine\n\n[d] Deep Variational Information Bottleneck. Alexander A. Alemi, Ian Fischer, Joshua V. Dillon, Kevin Murphy\n\nWe hope our answers address the reviewer\u2019s questions. Looking forward to your response and please let us know if you have more questions!"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission438/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700717376706,
                "cdate": 1700717376706,
                "tmdate": 1700717376706,
                "mdate": 1700717376706,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "JwgIMRkI9Z",
            "forum": "NnYaYVODyV",
            "replyto": "NnYaYVODyV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission438/Reviewer_8m3b"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission438/Reviewer_8m3b"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces the Perceptual Group Tokenizer (PGT), a novel vision model that relies on iterative grouping to extract visual features and learn representations in a self-supervised manner. PGT proposes to use grouping operations instead of self-attention layers in ViT, which yields a self-attention-free visual backbone. It demonstrates competitive performance on the ImageNet-1K benchmark. The model also shows properties like adaptive computation and high interpretability. The paper provides comprehensive analysis, ablation studies, and visualizations that underscore the model's capability and potential as a new paradigm in visual backbone architecture design."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. I like the idea of grouping operations only, without self-attention. Using perceptual grouping is innovative and theoretically sound, offering a fresh perspective on representation learning and architecture design. \n2. The adaptability of computation in inference mode is also interesting. Without re-training, the model could inference with different number of group tokens. Table 1 also shows the accuracy will increase as there are more group tokens\n3. The visualization of the attention map is very interesting. It not only shows more iterations yield clearer grouping, but also shows different grouping heads kind of learning disjoint visual representations."
                },
                "weaknesses": {
                    "value": "1. Only ViT-B level(70-80M parameter) model is reported. It would justify the effectiveness if the proposed architecture works when scaling the model size up. \n2. The computation cost, peak memory usage, and inference speed comparison with ViT-B are not reported. It would be informative for readers how fast the PGT is since PGT doesn't have memory/computation demanding self-attention operations."
                },
                "questions": {
                    "value": "1. The model uses patch size 4x4, which means the visual backbone only downsamples the image by 4. Intuitively, this model should be good at dense prediction tasks that require high feature resolution, e.g. semantic segmentation and object detection. The authors reported the results of semantic segmentation on ADE20K in Section 4.4, but it only outperforms the ViT-B by a smaller margin. I understand that the segmentation architecture is different. So it would be interesting to compare ViT-B vs PGT-B with the same segmentation architecture (linear classification layer). \n2. Since PGT is self-attention-free, the computation cost is not quadratically increasing with the input resolution. But it is still reasonable to compare to ViT under the same resolution, for example, 16x16 patch size and 8x8 patch size. I am wondering whether authors have done this ablation. \n3. As mentioned in the weakness section, it would be interesting to have the computation cost of PGT. As Table 1 shows, as we increase the number of group token from 256 to 768, the linear probe accuracy increases from 79.3 to 79.7 How about less than 256 tokens and more than 768 tokens? It would be insightful to have a graph of number of tokens vs accuracy and inference speed. \n4. In Mask Autoencoder paper, researchers find that higher linear probing accuracy may be not necessarily stand for better representation. It would be also interesting to compare with MAE-ViT against PGT under the fine-tuning setting."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission438/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission438/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission438/Reviewer_8m3b"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission438/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699344688163,
            "cdate": 1699344688163,
            "tmdate": 1699635970183,
            "mdate": 1699635970183,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oCgOrWjsJ6",
                "forum": "NnYaYVODyV",
                "replyto": "JwgIMRkI9Z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission438/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission438/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the constructive feedback!"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for the detailed and valuable comments. A new revision is submitted with the updated draft. We address the specific questions below.\n\n **W1. Scale up model sizes**\n\nWe agree. We are working on more experiments to scale up the model to 300M or larger level sizes and will add the results to the final version (the training takes some time). \n\n**W2. Profiling PGT\u2019s computational stats & Q3**\n\nThank you for this suggestion! We have updated the draft and added peak memory usage in table 3 and inference time in appendix section A.2.3. For easier read, the results are also listed here:\n\n- Peak memory usage\n\nWe show the percentage of peak memory usage in PGT compared to ViT-B with the same patch size (4\u00d74).  Even with a large number of group tokens, due to less number of heads and total attention tokens, we still only use a much smaller percentage of memory compared to the ViT-B backbone. Note that the usage is obtained using forward inference graph, as in practice the underlying complex hardware optimizer is a less accurate measurement and largely varies across infrastructures.\n\n\n| # group tokens        | 16  | 32  | 64  | 128 | 256 | 384 | 512 | 768  | 1024 | ViT-B |\n|-----------------------|-----|-----|-----|-----|-----|-----|-----|------|------|-------|\n| Peak memory usage (%) | 4.6 | 4.6 | 4.6 | 4.6 | 4.6 | 6.1 | 8.2 | 12.2 | 16.3 | 100   |\n\n- Inference time\n\nSimilarly, we profile our model\u2019s inference time against ViT-B/4. Our model is built upon a complex infrastructure with XLA and other hardware optimizers. We find that ViT has ~680 im/sec/core inference speed, while PGT has ~640 im/sec/core. Note that PGT has a heavier process (3 steps of iterative grouping), and can still maintain similar speed with ViT. With fewer grouping iterations (1 and 2), our model can achieve ~710 im/sec/core and ~820 im/sec/core.\n\nDue to the underlying hardware speed optimizers, changing the number of group tokens currently leads to negligible influence. Theoretically, with less number of group tokens, the inference time should still be reduced. We will look into how to profile the model in a more toy and simpler environment.\n\n**Q1. Segmentation**\n\nWe have an updated result of 45.1% for PGT. Using ViT with a linear layer, we got similar results (44.2). We completely agree on that having more dense patches should indeed lead to higher performance. One very likely reason is DINO\u2019s loss is a whole-image holistic loss, where parts of the same image are always matched with each other. The images in ImageNet, although curated, still contain multiple objects in most of them. This loss design might limit the power of our grouping backbone. We think a dense loss function that uses \u201cpixel-group assignment\u201d will further unleash the performance.\n\n**Q2. 8x8, 16x16**\n\nYes, we have been keeping track of model performance in different resolutions. Since one core of the grouping process is the ability of automatically grouping patches and generating more meaningful \u201cregions\u201d (compared to large strided patchification), we mainly test the model on small patches to allow bigger flexibility in the binding procedure. A simple attempt on 8x8 and 16x16 lead to 79.2 and 77.0. PGT-B uses 384 as patch embedding vector dimension and 384 as the hidden dimension for MLP (ViT uses 768 and 3072). Scaling it to the same dimension lead to a 0.4 improvement on each model. \n\n**Q4. Fine-tuning**\n\nThis is a great question. We fine-tuned our PGT and achieved 82.3% (DINO has 82.8%). Interestingly, we find that the standard data augmentation trick mixup negatively affects the model\u2019s performance. Mixup combines two images in a linear combination way and will blur the images. This seems to interfere the grouping process. PGT only uses randAugment[a] for finetuning. We believe perceptual grouping models have their own augmentation pipeline for achieving the best performance.\n\n[a] RandAugment: Practical automated data augmentation with a reduced search space. Ekin D. Cubuk, Barret Zoph, Jonathon Shlens, Quoc V. Le\n\nWe appreciate your suggestions and hope the above answers address the reviewer\u2019s questions and look forward to your response. Please let us know if you have more questions!"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission438/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700717364953,
                "cdate": 1700717364953,
                "tmdate": 1700717364953,
                "mdate": 1700717364953,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]