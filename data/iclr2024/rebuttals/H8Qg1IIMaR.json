[
    {
        "title": "Fool Your Large (Vision and) Language Models with Embarrassingly Simple Permutations"
    },
    {
        "review": {
            "id": "CmRCW5vlUO",
            "forum": "H8Qg1IIMaR",
            "replyto": "H8Qg1IIMaR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5690/Reviewer_FKE2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5690/Reviewer_FKE2"
            ],
            "content": {
                "summary": {
                    "value": "This paper explores and experiments with the permutation attack on current large vision-language models for multiple-choice question answering (MCQA). The authors show that these models are susceptible to adversarial permutations in the answer sets for multiple-choice prompts. This is concerning because the models should be invariant to the permutations."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Well written\n- Interesting and important area of work that many are overlooking \n- An important area to look at specially for industry wide adaptation of large vision-language models. \n- Experiments are easy to understand"
                },
                "weaknesses": {
                    "value": "1. I don't see any related work section. That would have made the work more sound\n2. The comparison tables on LLMs seems to be inconsistent. For example Table 3 has GPT-3.5 turbo but Table 5, 6 (that are also comparison on LLM) do not have GPT-3.5 turbo\n3. On the experimentation section, it would be good if the authors described some more details. For example - did they used model APIs or model weight's for testing? This would give us an idea on the consistency of experiments"
                },
                "questions": {
                    "value": "1. In Table 3 for GPT3.5 turbo, how did the authors do the testing? Is it using OpenAI API?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5690/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5690/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5690/Reviewer_FKE2"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5690/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698338598104,
            "cdate": 1698338598104,
            "tmdate": 1699636595176,
            "mdate": 1699636595176,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ALp8KAuucY",
                "forum": "H8Qg1IIMaR",
                "replyto": "CmRCW5vlUO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5690/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5690/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thank you very much for reviewing our paper and providing helpful comments. Here are our responses to your comments.\n\n### Q1. Related work section\nWe would like to clarify that a comprehensive review of related work is indeed present in our paper, specifically in Section 5. This section discusses the development of Large Language Models (LLMs) and Vision-and-Language Models (VLLMs), their wide adoption, and popular studies using multiple-choice question answering for evaluation. Additionally, it includes an analysis of the robustness of LLMs and VLLMs. We hope this section can draw parallels with previous studies, and highlight the distinct contributions and originality of our work.\n\n### Q2. Comparison of LLMs\nThank you for pointing this out. We acknowledge that GPT-3.5 Turbo is included in Table 3 but not in Tables 5 and 6. The reason for this is primarily due to budgetary constraints. To address this issue and maintain the integrity of our comparative analysis, we have now conducted additional experiments with GPT-3.5 Turbo for Table 5 and 6. The results can be seen in the updated tables. Additionally, we also incorporated Llama2-70B, a strong open-source model, in the revised version of our manuscript. This addition aims to provide another comparable level of insight and draw similar conclusions to what would have been achieved with GPT-3.5 Turbo.\n\n### Q3. Details of the experiments\nThank you for your suggestions. All of the models (except GPT-3.5-turbo) we used are open-sourced models. Their weights can be obtained from the official Huggingface model zoo (https://huggingface.co/models) or the original Github repositories. We have added additional descriptions of these details in the Reproducibility Statement section and we have uploaded the source code to reproduce these experiments.\n\n### Q4. Testing of GPT-3.5-Turbo\nThank you for your question. Yes, we used the official OpenAI GPT-3.5-turbo API for the experiments following their documents (https://platform.openai.com/docs/api-reference/chat)."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5690/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700228875281,
                "cdate": 1700228875281,
                "tmdate": 1700228875281,
                "mdate": 1700228875281,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fhdYbSsEbu",
                "forum": "H8Qg1IIMaR",
                "replyto": "CmRCW5vlUO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5690/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5690/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Sincerely expecting further discussions from Reviewer FKE2"
                    },
                    "comment": {
                        "value": "Dear Reviewer FKE2,\n\nWe want to thank you here, again, for the constructive comments and acknowledgment of this paper. We have conducted additional experiments and provided detailed explanations to try to address all of your concerns. Could you please kindly check our revised paper and our responses, to see if your concerns are solved? We would really like to hear if you have any further questions before the discussion window is over. And if no more questions, please could you consider updating the score?\n\nSincerely, \\\nAuthors"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5690/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700596306884,
                "cdate": 1700596306884,
                "tmdate": 1700596306884,
                "mdate": 1700596306884,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tShHQm6pbG",
                "forum": "H8Qg1IIMaR",
                "replyto": "ALp8KAuucY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5690/Reviewer_FKE2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5690/Reviewer_FKE2"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you to the authors for the explanation and conducting additional experiments, updating the paper. \nI think understanding the robustness of vision-language models in general is an important line of work as more and more usecases are built around it. While the actionable steps out of this finding is not clear in this paper, I think we need to encourage this line of investigative research to the community. \n\nI will increase my rating to 7"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5690/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700644876996,
                "cdate": 1700644876996,
                "tmdate": 1700644876996,
                "mdate": 1700644876996,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8uD5iPRv9o",
            "forum": "H8Qg1IIMaR",
            "replyto": "H8Qg1IIMaR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5690/Reviewer_EvuZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5690/Reviewer_EvuZ"
            ],
            "content": {
                "summary": {
                    "value": "The paper aims at the vulnerabilities of large language and vision-language models, specifically concerning permutations in multiple-choice question answering (MCQA). The authors conduct experiments that reveal performance degradation when models are exposed to permutation-based attacks, even though these models have shown capabilities in various tasks. The findings underscore the need for a deeper analysis of robustness before deploying such models in real-world applications."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper addresses the robustness of widely used models, a pertinent topic given the real-world deployment of these models."
                },
                "weaknesses": {
                    "value": "1. The methodology's depth and novelty are not entirely clear. More analysis could be provided on how natural variations in the way are ordered may impact models.\n\n2. While the paper focuses on permutation-based vulnerabilities, it might benefit from a broader discussion on other potential vulnerabilities or comparisons to other attack methods.\n\n3. The experiments provided are on a specific dataset. It would be beneficial to see how the models fare on diverse datasets to ensure the findings, which are not specific to one dataset's characteristics."
                },
                "questions": {
                    "value": "See the above weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5690/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698676030205,
            "cdate": 1698676030205,
            "tmdate": 1699636595065,
            "mdate": 1699636595065,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "av98vroZKF",
                "forum": "H8Qg1IIMaR",
                "replyto": "8uD5iPRv9o",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5690/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5690/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response (1/2)"
                    },
                    "comment": {
                        "value": "Thank you very much for reviewing our paper and providing helpful comments. Here are our responses to your comments.\n\n### Q1. Depth and Novelty\n*Novelty*: We would like to emphasize that the crux of our work's novelty does not lie in the complexity of the permutation method itself, but rather in the unexpected and significant findings that emerge from this straightforward yet effective approach. Our contribution is in uncovering a profound vulnerability in LLMs and VLLMs, which are otherwise regarded as highly advanced and robust. This vulnerability, triggered by simple permutations in MCQ options, reveals a critical aspect of these models that challenges their reliability in real-world applications. This phenomenon has not previously been reported in the field. \n*Depth*: The depth of our study lies in the extensive empirical analysis that such a basic manipulation can lead to drastic performance degradation. In over **100** experiments, we show that the vulnerability exists across more than 20 highly regarded foundation models and more than 9 established benchmarks. The fact that the phenomenon is so widely observed makes the result highly salient for both researchers and practitioners. \n\n### Q2. Other attacks\nThank you for your suggestions. In our original version, we compare our attack strategy with the position attack, i.e. always rotating the correct answers to a specific position (A/B/C/D) in the answer list. The results show that although the models also exhibit varying degrees of position bias, our adversarial permutation shows a much stronger effect (a larger proportion of predictions drop to below chance level).\nTo further address your concern, we conducted additional experiments to compare with three more types of attack methods. (1) ICL Permutation attack: Given a fixed set of few-shot in-context examples, permute the in-context learning examples to their worst-case order. (2) ICL Search attack: Search for the worst-case set of ICL examples to use from within the training set.  (3) Symbol attack: Consider different types of option symbols (e.g. A/B/C/D, 1/2/3/4, a/b/c/d), and choose the worst-case symbol set for each question. We present the results of these attacks on MMLU dataset below and the results of other datasets in the appendix Table 16-20.\n\n| **Models**   \t| **Original 0-shot** | **ICL** | **ICL Permutation** | **ICL Search attack** | **Symbol Attack** | **Adversarial Attack** |\n|------------------|---------------------|---------|---------------------|-----------------------|-------------------|------------------------|\n| Llama2-7B    \t| 40.91           \t| 45.67   | 35.09           \t| 34.46             \t| 25.70         \t| **6.17**           \t|\n| Llama2-13B   \t| 52.22           \t| 54.52   | 46.65           \t| 46.07             \t| 30.76         \t| **18.33**          \t|\n| Llama2-70B   \t| 64.68           \t| 68.25   | 59.82           \t| 59.68             \t| 47.40         \t| **33.16**          \t|\n| Vicuna-v1.5  \t| 48.57           \t| 50.27   | 40.85           \t| 41.92             \t| 33.85         \t| **18.09**          \t|\n| Vicuna-v1.5-13B  | 54.68           \t| 55.68   | 54.65           \t| 49.17             \t| 45.40         \t| **26.27**          \t|\n| WizardLM-13B \t| 48.60           \t| 48.93   | 39.98           \t| 39.90             \t| 29.07         \t| **15.87**          \t|\n| InternLM-7B  \t| 45.72           \t| 48.36   | 38.17           \t| 38.17             \t| 29.38         \t| **10.45**          \t|\n| InternLM-20B \t| 59.14           \t| 60.50   | 54.45           \t| 54.45             \t| 47.06         \t| **29.52**          \t|\n| Falcon-7B    \t| 31.66           \t| 32.95   | 27.18           \t| 26.79             \t| 14.38         \t| **2.49**           \t|\n| MPT-7B       \t| 35.60           \t| 38.73   | 30.51           \t| 27.33             \t| 21.62         \t| **3.52**           \t|\n\n\nAs can be seen from the table, while other attacks can also harm the performance, our adversarial attack has the biggest impact on the LLMs and causes the largest performance drop."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5690/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700228827191,
                "cdate": 1700228827191,
                "tmdate": 1700228827191,
                "mdate": 1700228827191,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fd88OFPtj7",
                "forum": "H8Qg1IIMaR",
                "replyto": "8uD5iPRv9o",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5690/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5690/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Sincerely expecting further discussions from Reviewer EvuZ"
                    },
                    "comment": {
                        "value": "Dear Reviewer EvuZ,\n\nWe want to thank you here, again, for the constructive comments and acknowledgment of this paper. We have conducted additional experiments and provided detailed explanations to try to address all of your concerns. Could you please kindly check our revised paper and our responses, to see if your concerns are solved? We would really like to hear if you have any further questions before the discussion window is over. And if no more questions, please could you consider updating the score?\n\nSincerely, \\\nAuthors"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5690/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700596269824,
                "cdate": 1700596269824,
                "tmdate": 1700596269824,
                "mdate": 1700596269824,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rwQM7vZu2N",
            "forum": "H8Qg1IIMaR",
            "replyto": "H8Qg1IIMaR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5690/Reviewer_RBKq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5690/Reviewer_RBKq"
            ],
            "content": {
                "summary": {
                    "value": "This paper examines permutation sensitivity in MCQA for generative language models and vision-language models. They show that a wide range of models display severe permutation sensitivity in a variety of MCQA benchmarks, and show that existing mitigation strategies do not solve permutation sensitivity."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "A large number of language models and vision language models are used in the experiments. The datasets chosen are diverse and there are enough datasets to draw strong conclusions. \n\nThere is concurrent work in the area (pointed out by the authors themselves), but the broadness of the experimental evaluation is original. While limited in scope to MCQA, it is significant, as it may point to underlying problems in LLM reasoning that cannot be easily fixed."
                },
                "weaknesses": {
                    "value": "I think it's overclaiming to call this an \"adversarial\" attack. Permuting the choices is done as a matter of course in evaluation (see Section 4.1 in [1]). \n\nAdditionally, I think important context is missing. We know parameters like the temperature and the sampling strategy have a significant effect on output. But I don't see these numbers reported. Do the numbers change if we reduce / increase the temperature?\n\nThere's no discussion on prompting. Does providing in-context examples effect permutation sensitivity? In-context examples for MCQA should always be available. They also provide a convenient way to alter the posterior distribution (for example, you could set the answer to always be the last answer in the in-context examples; would the positional bias then change?)\n\nAlso, no information is reported on the model perplexity / confidence during permutation. It may be possible that certain permutations have much lower perplexity / higher confidence, and hence the model's answer on those is more trustworthy. So in practice, we might evaluate the model on all the permutations, then select the model's most confident answer. Of course, if we can do this, it is better to avoid MCQA entirely and have the model assess the answers one-by-one.\n\n[1] MMBench: Is Your Multi-modal Model an All-around Player?"
                },
                "questions": {
                    "value": "Please see the weaknesses section. I am willing to raise my rating if the weaknesses are addressed."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5690/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5690/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5690/Reviewer_RBKq"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5690/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698716254074,
            "cdate": 1698716254074,
            "tmdate": 1700715162695,
            "mdate": 1700715162695,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5TDabG4G0O",
                "forum": "H8Qg1IIMaR",
                "replyto": "rwQM7vZu2N",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5690/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5690/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response (1/2)"
                    },
                    "comment": {
                        "value": "Thank you very much for the thoughtful review and excellent questions. Here are our responses to your comments.\n\n### Q1. Adversarial attack v.s. Permutation\nThe term \u201cadversarial attack\u201d in the context of machine learning generally connotes any deliberate attempt to deceive or manipulate a model's output. Adversarial examples refer to an input that is crafted to look normal to humans while triggering erroneous behaviour from the model. We argue that MCQ permutation falls within this scope, as humans do not care about the permutation of the candidate choices, but we show that a worst-case permutation dramatically worsens model performance in a way that is not anticipated or intended by the model's design.\n\nSecondly, we would like to emphasize that our choice of terminology was not meant to suggest a sophisticated or elaborate attack mechanism. On the contrary, the core message of our paper is to highlight the surprising fact that even the most straightforward forms of manipulation, such as the proposed permutation, can significantly impair the performance of advanced LLMs and VLLMs in MCQ scenarios. The fact that such a simple attack is successful only enhances the salience of our result. This finding is crucial as it underscores the need for future work to improve the robustness of these models against even the simplest forms of potential manipulation.\n\nLastly, we are grateful for your suggestion of MMBench. We emphasize that MMbench performs *circular rotation* of answer candidates, not *permutation* of answer candidates. I.E: For N candidate answers, MMBench tries $N$ orders, preserving the relative order of the options, while we consider all possible $N!$ permutations, breaking the relative order of the options. In the revised version, we have added extra experiments comparing our permutation approach with MMBench circular evaluation (updated Table 6). The results show that our permutation results in much worse performance, demonstrating a stronger attack. The distinct outcomes of rotation and permutation are a critical step toward understanding this vulnerability and ultimately defending against it.\n\n### Q2. Temperature and sampling strategies\nThank you for your question. For all of the experiments, we consistently set the temperature to 1 and used greedy decoding for the generation to ensure reproducibility (specified in section 2.1 \u201cEvaluation\u201d paragraph). Specifically, we follow the common practice for MCQ evaluation [1][2] to obtain the probabilities of the first token and select the one with the highest probability, comparing it against the ground truth answer. (Note that in the MCQ context, we only need to select one token to answer.) Under this greedy evaluation scheme, variations in temperature or the adoption of different sampling strategies do not impact the results. This actually further enhances the generalizability and importance of our results for MCQ evaluation.\n\nHowever, to address your concern, we performed additional experiments using different temperatures (0.5, 1.5) and two commonly used sampling strategies (top k sampling and nucleus Sampling). As can be seen from the table below, the greedy decoding strategy we adopted achieves the best accuracy before and after the permutation among different decoding strategies. And the performance of the other decoding strategies are even worse after the permutations. This confirms both that our initial experiments were optimally configured, and also that the findings generalize to other decoding strategies. This analysis has been added to Appendix A.2 of the paper. \n\n| **Model**         \t| **Greedy Decoding** \t| **Temperature=0.5**  | **Temperature=1.5**  | **Top-k Sampling**  | **Nucleus Sampling** |\n|-----------------------|----------------|------------------|------------------|-----------------|------------------|\n| Llama2-7B     \t| **40.91/6.17** | 28.39/0.03   \t| 10.35/0.00   \t| 21.71/0.00  \t| 21.95/0.00   \t|\n| Llama2-13B   \t| **52.22/18.33**| 44.00/3.67   \t| 13.94/0.00   \t| 32.54/0.00  \t| 32.42/0.02   \t|\n| Llama2-70B    | **64.68/33.16**| 58.13/12.56  \t| 17.66/0.00   \t| 44.21/0.07  \t| 44.44/0.42   \t|\n| Vicuna-v1.5    | **48.57/18.09**| 47.64/12.29  | 34.43/0.04   \t| 42.71/3.60  \t| 44.77/8.10   \t|\n| Vicuna-v1.5-13B  | **54.68/26.27**| 53.71/21.65  \t| 38.18/0.11   \t| 49.24/7.34  \t| 51.99/17.10  \t|\n| WizardLM-13B     | **48.60/15.87**| 47.56/12.43  \t| 38.11/0.57   \t| 44.61/5.86  \t| 45.83/10.30  \t|\n| InternLM-7B   | **45.72/10.45**| 0.01/0.00   | 0.53/0.00    \t| 0.16/0.00   \t| 0.07/0.00    \t|\n| InternLM-20B     \t| **59.14/29.52**| 33.53/3.89   \t| 19.81/0.00   \t| 30.82/0.31  \t| 31.91/1.24   \t|\n| Falcon-7B         | **31.66/2.49** | 0.02/0.00    \t| 0.46/0.00    \t| 0.06/0.00   \t| 0.01/0.00    \t|\n| MPT-7B            \t| **35.60/3.52** | 0.01/0.00    \t| 0.67/0.00    \t| 0.12/0.00   \t| 0.04/0.00    \t|\n\n[1] Measuring Massive Multitask Language Understanding. Hendrycks et al., ICLR 2021.\\\n[2] Holistic evaluation of language models. Liang et al., 2022."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5690/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700228686048,
                "cdate": 1700228686048,
                "tmdate": 1700228686048,
                "mdate": 1700228686048,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rCCzYlwfam",
                "forum": "H8Qg1IIMaR",
                "replyto": "rwQM7vZu2N",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5690/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5690/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Sincerely expecting further discussions from Reviewer RBKq"
                    },
                    "comment": {
                        "value": "Dear Reviewer RBKq,\n\nWe want to thank you here, again, for the constructive comments and acknowledgment of this paper. We have conducted additional experiments and provided detailed explanations to try to address all of your concerns. Could you please kindly check our revised paper and our responses, to see if your concerns are solved? We would really like to hear if you have any further questions before the discussion window is over. And if no more questions, please could you consider updating the score?\n\nSincerely,\\\nAuthors"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5690/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700596227029,
                "cdate": 1700596227029,
                "tmdate": 1700596323523,
                "mdate": 1700596323523,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TLLuOj9Lzk",
                "forum": "H8Qg1IIMaR",
                "replyto": "rwQM7vZu2N",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5690/Reviewer_RBKq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5690/Reviewer_RBKq"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "The authors have answered my questions. I have no more questions. As promised, I am increasing my score."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5690/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700715295606,
                "cdate": 1700715295606,
                "tmdate": 1700715295606,
                "mdate": 1700715295606,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YSdj47oHH2",
            "forum": "H8Qg1IIMaR",
            "replyto": "H8Qg1IIMaR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5690/Reviewer_d9tg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5690/Reviewer_d9tg"
            ],
            "content": {
                "summary": {
                    "value": "This paper reveals an interesting phenomenon of existing LLM and VLLMs: they are vulnerable to adversarial permutation in answer sets for multiple-choice prompting, which is surprising as models should ideally be as invariant to prompt permutation as humans are."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper reveals an interesting phenomenon:  they are vulnerable to adversarial permutation in answer sets for multiple-choice prompting, which they should not be ideally.\n\n2. Experiments are conducted across different multiple LLM / VLLMs, demonstraing the universality of the phenomenon."
                },
                "weaknesses": {
                    "value": "While the finding is interesting, I wonder if authors could provide any intuitive explanations on the observation? Position bias (Zheng et al., 2023a) may not be able to fully explain this phenomenon, but it can be potentially one of the reasons why they are vulnerable to the adversarial permutation in answer sets from my understanding. I am willing to see more analysis on the potential causes of this observation. Can the explanations on similar problems in other tasks besides MCQA apply to this case\uff1fAuthors may put more efforts on it."
                },
                "questions": {
                    "value": "Please refer to Weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5690/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698814841908,
            "cdate": 1698814841908,
            "tmdate": 1699636594835,
            "mdate": 1699636594835,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KqtZl4BuXt",
                "forum": "H8Qg1IIMaR",
                "replyto": "YSdj47oHH2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5690/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5690/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thank you very much for your review and questions. Here are our responses to your comments.\n\n### Q1. Intuitive explanations of the phenomenon\nWe have further delved into it and would like to present our additional findings:\n1. **Option Symbol-Answer Content Shortcut**: Beyond position bias, we hypothesized another contributing factor: the model may suffer from shortcut [1] or spurious correlation learning [2] between option symbols and answer content. To investigate this, we conducted three experiments using different answer symbol sets (A/B/C/D, a/b/c/d, and I/II/III/IV). We examined if models exhibit similar patterns in response to the same permutations but only with different symbols, keeping the answer content constant. We conducted experiments on ARC-Challenge dataset with all models. Interestingly, our results revealed a high correlation for similar symbol pairs (A/B/C/D and a/b/c/d) with a Pearson correlation score of 0.76. However, for more distinct symbol pairs (A/B/C/D and I/II/III/IV), the correlation score dropped to 0.36, indicating that the models\u2019 response to permutation differs significantly when only the symbols varied. In other words, the baseline accuracy and permuted accuracy are almost the same for different symbol sets, but they respond very differently to permutation. This discrepancy points to the models having learned shortcuts between symbols and answer content, which may give extra vulnerability to the permutations of the models. This offers an additional explanation for the observed phenomenon. This analysis has been added to Appendix A of the paper. \n\n| Symbol Set              \t| Correlation | Original Accuracy | Permuted Accuracy |\n|-----------------------------|-------------|-------------|-------------|\n| Capital Letters vs. Lowercase Letters | 0.76  |  55.06 vs. 54.87 \t|   23.73 vs. 21.68\t|\n| Capital Letters vs. Roman Numerals\t| 0.36  |  55.06 vs. 52.49\t|   23.73 vs. 19.33 |\n\n2. **Bias from the Training Set**: We also considered data in the training set might lead to the observed vulnerability in MCQA [3]. Training data biases can arise from the predominance of certain answer patterns or thematic content. For instance, if the training set disproportionately represents specific types of questions or contextual themes, the model may develop an over-tuned response to these patterns. This over-tuning could result in a skewed performance when the model encounters permutations in MCQs that deviate from these familiar patterns. However, given that current LLMs and VLLMs are trained on vast and varied datasets, quantifying the exact impact of these training data biases on model vulnerability remains a challenging endeavor and we leave it for future work.\n\n\n[1] Geirhos, Robert, et al. \"Shortcut learning in deep neural networks.\" Nature Machine Intelligence 2020.\\\n[2] Sagawa, Shiori, et al. \"An investigation of why overparameterization exacerbates spurious correlations.\" ICML 2020.\\\n[3] Du, Mengnan, et al. \"Shortcut learning of large language models in natural language understanding: A survey.\" arXiv 2022."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5690/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700228552412,
                "cdate": 1700228552412,
                "tmdate": 1700228552412,
                "mdate": 1700228552412,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SbpILlNViL",
                "forum": "H8Qg1IIMaR",
                "replyto": "YSdj47oHH2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5690/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5690/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Sincerely expecting further discussions from Reviewer d9tg"
                    },
                    "comment": {
                        "value": "Dear Reviewer d9tg,\n\nWe want to thank you here, again, for the constructive comments and acknowledgment of this paper. We have conducted additional experiments and provided detailed explanations to address your concerns. Could you please kindly check our revised paper and our responses, to see if your concerns are solved? We would really like to hear if you have any further questions before the discussion window is over. And if no more questions, please could you consider updating the score?\n\nSincerely,\\\nAuthors"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5690/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700596137291,
                "cdate": 1700596137291,
                "tmdate": 1700596165688,
                "mdate": 1700596165688,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]