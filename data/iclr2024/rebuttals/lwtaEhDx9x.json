[
    {
        "title": "Elephants Never Forget: Testing Language Models for Memorization of Tabular Data"
    },
    {
        "review": {
            "id": "WzTKlCpFu8",
            "forum": "lwtaEhDx9x",
            "replyto": "lwtaEhDx9x",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7826/Reviewer_YJg3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7826/Reviewer_YJg3"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims at inspecting data contamination which happens when training a large language model. Specifically, they inspect whether some tabular datasets were used to train the language models. For this purpose, they propose several approaches to testing the language models.\n- Testing for Knowledge and Learning:\n    - Meta data: Testing whether the model can predict the name of the fields in the datasets.\n    - Conditional completion: Testing whether the model can predict the value of a feature of a sample based on some other features.\n    - Unconditional zero-knowledge samples: Testing whether the model produces the statistics of the features in a datasets.\n- Testing for memorization:\n    - Testing whether the model can generate rows based on the previous rows (either the first a few rows or a few rows starting from a random line) in the dataset csv file.\n    - Testing whether the model can generate the correct value of a feature based on the value of the other features. Here the value is \u201cunique\u201d.\n    - Testing whether the model can generate the first token of the next row.\n\nFinally, they compare the language models\u2019 performance on several datasets with some baseline models\u2019 performance, and conclude that some models\u2019 performance can be attributed to their memorization of the datasets during pretraining."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- They propose several methods to test whether the language models were trained with those datasets.\n- The idea of comparing the distribution generated by the model and the distribution in the datasets (Sec 3.3) is novel and interesting.\n- The claim that they show that some language models are pretrained with some tabular datasets is somewhat convincing and interesting."
                },
                "weaknesses": {
                    "value": "Main concerns:\n\n1. This work does not provide strong evidence supporting the validity of their proposed approaches. I think one main takeaway of this paper is that some models are pretrained with some datasets, so their performance is not indicative of. But this takeaway is based on the validity of their proposed approaches. I think the authors need to address this more.\n2. I can\u2019t understand the purpose of having these many different testing approaches, probably because the structure of this paper is hard to follow. The authors propose many approaches, some of them are interesting, but they do not provide a holistic interpretation of the results from these many approaches.\n3. The descriptions of the testing approaches are vague and not rigorous. Writing down the testing approaches with simple math equations could help. For example, in page 6, I can\u2019t understand what it means by \u201cwe can perform a t-test between the similarity of model completions with actual vs. random rows.\n4. Knowledge, learning, memorization should be defined more specifically.\n5. The authors (claims to) show data contamination exists in some datasets. However, I am not sure whether those datasets are commonly used to benchmark the language model. Thus I am not sure whether the findings are important (if they are valid).\n\nMore specific (writing) issues:\n\n\n2. The second paragraph in Sec 3.2: Here 4 possible causes are provided, but I don\u2019t see how they are discussed in the following experimental designs.\n3. The last sentence in page 5: \u201cEmpirically, we find \u2026 a very intuitive test \u2026\u201d. I don\u2019t understand how your empirical results support this.\n4. Page 8: \u201cIt might be that this learning task is relatively simple, that our memorization test are not sensitive enough\u201d. I can\u2019t understand why it is the case.\n\n\nGrammar:\n\nThere are many grammar errors. I suggest that the authors do some proofreading."
                },
                "questions": {
                    "value": "1. Figure 1: I suggest to use bar charts is more reasonable because your x-axis is not continuous."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7826/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697866562424,
            "cdate": 1697866562424,
            "tmdate": 1699636958544,
            "mdate": 1699636958544,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "r5YdngTVCH",
                "forum": "lwtaEhDx9x",
                "replyto": "WzTKlCpFu8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7826/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7826/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer YJg3"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the detailed review and many helpful comments.\n\n- *\u201cThis work does not provide strong evidence supporting the validity of their proposed approaches.\u201d*\n\nWith regard to the validity of our tests, please see the global comment \u2018The validity and relevance of our tests\u2019. \n\nIn addition, note that we have been careful to use (1) datasets that are public and overwhelmingly likely to have been used in pre-training, and (2) datasets that are very unlikely or impossible to have been used in pre-training.  There is a clear difference in the results for these two different kinds of datasets.  We also intentionally include FICO as a dataset where there is significant public discussion of the data and some of its properties, but where the data table itself has not been made public. As might be expected, on this dataset our tests detect prior experience but fail to detect verbatim memorization.\n\n- *\u201cI can\u2019t understand the purpose of having these many different testing approaches\u201d*\n\nWe acknowledge that the flow of the paper can be overwhelming due to the many different proposed approaches. Under the limitations we impose upon our testing framework, we believe it is impossible to design a single test that will work perfectly under all possible circumstances. Our design philosophy therefore is to design a battery of tests that offer empirical evidence of memorization in concert. Under stricter assumptions, a smaller number of tests may likely be sufficient, but making stronger assumptions would compromise the utility of our tests by working either only with fewer LLMs or fewer tabular datasets. Overall, we believe that the proposed tests allow us to provide a nuanced picture of different types of pre-training contamination.\n\n- *\u201cThe descriptions of the testing approaches are vague and not rigorous.\u201d*\n\nWe agree that there is room for improvement and will rewrite the sections describing the different tests. For details on the different tests, please also see Supplement Section E which gives the prompt structures of all the tests, and the revised Supplement Section F which provides the test results in a more explicit form.\n\n- *\u201cI am not sure whether those datasets are commonly used to benchmark the language model. Thus I am not sure whether the findings are important\u201d*\n\nYes, the datasets in our paper have been used to benchmark language models. Please see the papers referenced under \u2018The relevance of our work\u2019 in the global comment."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7826/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700651353224,
                "cdate": 1700651353224,
                "tmdate": 1700651353224,
                "mdate": 1700651353224,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qZSAjmfZao",
            "forum": "lwtaEhDx9x",
            "replyto": "lwtaEhDx9x",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7826/Reviewer_aXk7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7826/Reviewer_aXk7"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a discussion on data contamination and memorization in large language models concerning tabular data. The authors propose multiple methods to examine whether an LLM has memorized specific tabular datasets during training. The paper also proposes methods to examine knowledge, learning, and memorization separately and discuss the distinction between them. Finally, the paper analyzes the influence of learning and memorization on the performance of downstream tasks, and advocates checking memorization as a crucial step in evaluating LLM on tasks with tabular data."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper presents several novel methods to evaluate memorization of tabular data in LLMs, and evaluation results on a series of datasets correlate well with the publication time and availability of the data, confirming the effectiveness of the proposed methods in identifying memorization. The different evaluation methods also complement each other, elucidating the different aspects of memorization of tabular data.\n\nThe paper is overall well-written and very easy to read, the visualizations present the main findings nicely.\n\nThe contamination and memorization of training data by LLMs is a critical issue. The findings provoke essential discussions on the evaluation of LLMs on tabular data, which is likely to become more relevant given the rising usage of LLMs in diverse tasks. \n\nThe introduced tools and code potentially provide easy and accessible ways to evaluate memorization of tabular data, reusable in future research."
                },
                "weaknesses": {
                    "value": "Some important details in the experiment design may be missing or incomplete: \n\n* Evaluation metric for knowledge, learning, and memorization is unclear. In Table 1, the evaluation results are categorized into three categories (\u2713,X, and ?), but the metric for the categorization is not given. It is probably a better idea to show the raw values (e.g., accuracy) than using categories to give the reader a direct comprehension of the degree of memorization on each dataset. Notations such as \"\u2713\" could be misleading as it may be confused as perfect memorization.\n\n  The appendix gives raw accuracies for Row Completion Test, Feature Completion Test, and First Token Test, why raw accuracies for Feature Names, Feature Values, and Header Test are not provided as well?\n\n* The differentiation between learning and memorization is not clear: the authors use feature distributions to examine learning, but memorization can also result in a high similarity of the generated data's feature distributions to the original data. Learning is defined as the model's ability to perform tasks in the current paper, but task performance is heavily affected by memorization and may fail to reflect true learning. Even with considerable discussion, the paper does not seem to arrive at a conclusion about how learning can be clearly assessed.\n\n* Evalulation of memorization needs to take the nature of data fields into consideration. Some data fields in the tabular dataset are considerably harder to memorize verbatim or to predict exactly (such as measurement values) than other simpler fields (categorical values such as sex, occupation, nation). For numerical values, it may be more reasonable to measure the relative distance from the predicted value to the true value than using exact match (perhaps in a similar vein as the \"first token test\" in the paper but more principled).\n\n  Under the current evaluation protocol, it is likely that datasets containing more easy fields are more likely to be judged as memorized. To compare the degree of memorization across datasets, it seems necessary to perform some kind of \"normalization\" before measuring memorization, for example, selecting a fixed number of categorical and numerical fields from each dataset. Results in Table 3 could suffer from this limitation as well.\n\n* Evaluation of memorization needs to be evaluated separately for the training and test split. It may be possible that the training sets are memorized more than the test set due to more exposure on the internet. Memorizing the test set definitely compromises evaluation, but memorizing the training set may not always compromise evaluation.\n\n* Connection between memorization and downstream performance is not reliably established. The main observation from Section 5 is that for datasets with a high degree of memorization, LLM performs better than decision tree and logistic regression, while for datasets with a low degree of memorization the reverse is true. Such observation alone may not be sufficient to conclude that memorization compromises evaluation, because there is no evidence that LLM cannot perform better than decision tree and logistic regression under no memorization. It would be much better to solicit new test sets for the tasks to use in evaluation, which can be used to show exactly how much performance gap is caused by memorization. In case finding new examples is difficult, perhaps one can modify the values of the fields known to be irrelevant to the label in existing examples, and that may break the reliance on memorization in LLMs.\n\nSome main conclusions of the paper are compromised because of the above limitations:\n\n* \"We emphasize the importance of verifying data contamination before applying LLMs\": the implication of data contamination is not reliably demonstrated in Section 5.\n\n  Also, from the current discussion, it is not very clear how to interpret the test results on knowledge, learning, and memorization together. For example, if knowledge and learning show positive results and memorization show negative results, should we conclude that there is data contamination or not? And could the performance on downstream tasks be trusted in this situation?\n\n  It can be argued that knowledge and learning will not directly compromise evaluation on downstream tasks, so there may not be as much need to evaluate them compared to memorization. I would suggest allocating more space in the paper for extended experiments and discussions on memorization, which is the ultimate reason why people are concerned about data contamination. \n\n* \"... and propose practical methods to do so\": the proposed method verifies memorization of data, but does not give a definite metric to judge when memorization is severe enough to compromise evaluation.\n\n* \"We offer a principled distinction between learning and memorization in LLMs\": the distinction is not given clearly enough. One can tell whether there is memorization from the proposed test, but it is not clear how to tell whether learning exists (especially when memorization is present)."
                },
                "questions": {
                    "value": "What is the difference in the remembering behavior of LLM between tabular data and non-tabular data? The question may help strengthen the original contribution of the paper.\n\nIn the beginning of the discussion section, the use of the term \"representation learning\" may be confusing to some people. Representation learning usually refers to the process of learning useful features from raw data (wikipedia), which does not include memorization by definition."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7826/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7826/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7826/Reviewer_aXk7"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7826/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697977363042,
            "cdate": 1697977363042,
            "tmdate": 1699636958422,
            "mdate": 1699636958422,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "eHCDXcnxpt",
                "forum": "lwtaEhDx9x",
                "replyto": "qZSAjmfZao",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7826/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7826/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer aXk7 (Part 1)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the detailed review and many helpful comments.\n\n- *\u201craw accuracies for Feature Names, Feature Values, and Header Test are not provided \u201d*\n\nWe empirically found that the language models either (1) correctly list all the feature names, or (2) completely fail to list the feature names. The same holds true for the Feature Values test (third row of Table 3) and also for the header test. This is why we decided to depict the test results qualitatively. We agree that we did not make this clear enough in the paper. The revised Supplement Section F now depicts detailed results of the different tests.\n\n- *\u201cthe authors use feature distributions to examine learning, but memorization can also result in a high similarity of the generated data's feature distributions to the original data. Learning is defined as the model's ability to perform tasks in the current paper, but task performance is heavily affected by memorization and may fail to reflect true learning. Even with considerable discussion, the paper does not seem to arrive at a conclusion about how learning can be clearly assessed.\u201d*\n\nGreat point. We find it important to introduce the learning/memorization distinction to make clear that there are many different tasks that the model might be able to perform with the data that are not indicative of memorization (see also \u2018The distinction between Knowledge, Learning and Memorization\u2019 above). We are reasonably confident to have identified cases where the LLMs have learned, but not memorized. The reviewer asks whether we are able to test for learning under the assumption that there is memorization. This is a very interesting question (and also a question that is hard to answer). With the tests that we propose in this paper, we do not claim to be able to do this. \n\n- *\u201cEvaluation of memorization needs to take the nature of data fields into consideration. [...] For numerical values, it may be more reasonable to measure the relative distance from the predicted value to the true value than using exact match (perhaps in a similar vein as the \"first token test\" in the paper but more principled). Under the current evaluation protocol, it is likely that datasets containing more easy fields are more likely to be judged as memorized. To compare the degree of memorization across datasets, it seems necessary to perform some kind of \"normalization\" before measuring memorization, for example, selecting a fixed number of categorical and numerical fields from each dataset. Results in Table 3 could suffer from this limitation as well.\u201d*\n\nThis is an interesting observation, but we tend to disagree. In fact, it is precisely the exact matches between rare numerical values in the data and the generations by the LLM that allow us to develop valid tests for memorization. The fact that tabular datasets often contain such values (as opposed to free-form text) is what makes tabular data such an interesting testbed for memorization (and this is also why the results in Table 3 are valid). For a dataset that only contains easy fields such as categorical variables with few different values, it would be very hard to distinguish a powerful learner and memorization. Measuring the relative distance from the predictive value to the true value could result in a test for learning, but would be unlikely to result in a valid test for memorization. \n\n- *\u201cEvaluation of memorization needs to be evaluated separately for the training and test split. \u201d*\n\nVery good point. The revised Supplement Section F now contains separate evaluations for the datasets where separate train and tests splits are available. We do not find any significant differences.\n\n- *\u201cConnection between memorization and downstream performance is not reliably established. [...] It would be much better to solicit new test sets for the tasks to use in evaluation, which can be used to show exactly how much performance gap is caused by memorization.\u201d*\n\nThe SOTA accuracy for Kaggle Titanic using ensembling is approximately 0.85 (see https://www.kaggle.com/code/pliptor/how-am-i-doing-with-my-score/report). GPT-4 achieves an accuracy of 0.98. We think that this is fairly indicative of memorization. We agree, however, that the approach in Section 5 is only a first step, and that one could think of more advanced evaluation strategies (such as the one suggested by the reviewer). In general, however, there are many possible downstream tasks, and it is not possible for tests of memorization and prior exposure to consider (or even be relevant) for all of these.  Because of this, we believe it is important to have tests that are also reasonably task-agnostic."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7826/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700643609406,
                "cdate": 1700643609406,
                "tmdate": 1700643609406,
                "mdate": 1700643609406,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "MAjYHtyiHz",
            "forum": "lwtaEhDx9x",
            "replyto": "lwtaEhDx9x",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7826/Reviewer_ELKr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7826/Reviewer_ELKr"
            ],
            "content": {
                "summary": {
                    "value": "LLMs are increasingly being applied to various types of data, including tabular data. Since, at the moment, the most advanced LLMs are essentially black-boxes w/ restricted APIs with little details available about their training data, it is hard to tell a priori whether the tabular data was leaked into the training and whether the models have memorized it.\nThis paper proposes four tests that probe an LLM for the training data contamination and estimate the degree of the contamination (\u201cknowledge\u201d, \u201clearning\u201d, \u201cmemorization\u201d).\n\nIn the experimental study, the paper focuses on ChatGPT 3.5 & 4 and 10 tabular datasets, which have a high chance of being in the training data obtained by crawling the Internet (Iris, Kaggle Titanic, \u2026). \n\nFirst, the authors show that both LLMs have memorized basic meta-data from the datasets. Next, the LLMs are probed for an ability to reproduce a dataset example, conditioned on a part of its features. As an example, on the Adult Income dataset, the LLMs completed EduNum feature significantly better than a marginal distribution baseline. Further, the authors propose \u201czero-knowledge\u201d prompting technique where the model is prompted to sample samples from a dataset (unconditionally or conditionally), provided samples from other datasets. Using that approach, the authors show that the models often can reproduce the distribution of the data in some datasets (approximately). Finally, the models are probed to reproduce parts of the datasets verbatim; for some datasets that happens extremely often.\n\nAdditionally, S5 provides a comparative analysis of the LLMs performing few-shot classification tasks on a subset of datasets, in comparison to standard ML baselines. It turns out that the LLMs have marked drop in performance on some datasets that are likely absent in training (Pneumonia & Spaceship Titanic); at the same time very high performance on datasets that are likely to be memorized (Kaggle Titanic)."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* I believe this work (a) raises an important overlooked question, (b) addresses it, (c) by proposing an original technique. I particularly like the zero-shot prompting technique that allows sampling from a dataset w/o leaking information in the prompt.\n* The paper disentangles a few levels of training data contamination and comprehensively tests for those.\n* The paper showcases the potential impact of the contamination on the downstream comparisons, hence proving a strong motivation to the work. \n* The text and the story are clear.\n* The code is made public."
                },
                "weaknesses": {
                    "value": "* The paper only studies ChatGPT-3.5 and 4. Those are very likely to be strongly correlated in terms of the data used, which harms the representativeness of the study.\n* As there is no ground-truth knowledge on whether a particular dataset was seen at training, it is impossible to strictly verify the findings. Including an LM trained on a known dataset would allow us to verify the used methods.\n* Another related issue: the work is mostly relevant when we consider closed-data models w/ a black-box API access. This scenario reflects a dominant situation at the moment, but it is not given that this will not/should not change.\n\nMinor:\n* Table 1 is mentioned on page 3, yet only appears on page 6. Is there a way to bring it closer?\n* Would it make sense to consider swapping sections 5 and 6? I feel the S6 is more connected to the S3-4 than S5."
                },
                "questions": {
                    "value": "I wonder if authors would be willing to address the first two points in \u2018weaknesses\u2019."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7826/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698408062111,
            "cdate": 1698408062111,
            "tmdate": 1699636958285,
            "mdate": 1699636958285,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qOlSRiWjjL",
                "forum": "lwtaEhDx9x",
                "replyto": "MAjYHtyiHz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7826/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7826/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer ELKr"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the detailed review and appreciation of our paper. We also thank the reviewer for the minor comments that we will incorporate into the paper.\n\n- *\u201cThe paper only studies ChatGPT-3.5 and 4. \u201d*\n\nWe have now extended our code and prompts to work with arbitrary language models (some adaptation of prompts is required between base models and chat models, and the capabilities of chat models to follow instructions vary). The revised Supplement Section F contains results with Llama2-7b and Llama2-70b. \n\n- *\u201cIncluding an LM trained on a known dataset would allow us to verify the used methods.\u201d*\n\nWe agree that this is an interesting direction in which our work could be extended. Unfortunately, the LLMs for which the training data and schedule are transparent are rather small (<100b Parameters). At the same time, memorization is known to occur especially for very large models. This is evident, for example, in Supplement Table 4, which shows that GPT-4 has memorized significantly more than GPT-3.5. \nIt is also evident in the Figures in the revised Supplement Section F, where it can be seen that Llama2-7b is the model that has memorized the least.\n\n- *\u201cthe work is mostly relevant when we consider closed-data models w/ a black-box API access. \u201c*\n\nWe completely agree. We also believe that this is a very important and relevant scenario (see also \u2018The validity and relevance of our tests\u2019 above)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7826/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700643331933,
                "cdate": 1700643331933,
                "tmdate": 1700643331933,
                "mdate": 1700643331933,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "2QzbpIB3Le",
            "forum": "lwtaEhDx9x",
            "replyto": "lwtaEhDx9x",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7826/Reviewer_8936"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7826/Reviewer_8936"
            ],
            "content": {
                "summary": {
                    "value": "This paper specifically targets the issue of contamination in training sets when evaluating LLMs on tasks with tabular data.\nCompared to previous work on LLMs for tabular data, the authors propose methods to test the LLM for memorization (in addition to the dimensions \"knowledge\" and \"learning\").\nThese novel tests help to better analyze and understand the performance on downstream tasks, such as deciding if the data has been seen in training or not."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* LLMs are pervasive currently, and it's important to understand and control their behavior. The authors emphasize the importance of verifying data contamination before applying LLM.\n* Their setup based on tabular data is an elegant way to test \u201cknowledge\u201d, \u201clearning\u201d, and \u201cmemorization\u201d of an LLM.\n* Moreover, they assume only blackbox API access, without assuming access to the probability distributioin over tokens or the ability to re-train the model.\n* Release of an open-source tool that can perform various tests for memorization."
                },
                "weaknesses": {
                    "value": "* My main point of criticism is that the paper feels a bit like a collection of remarkable examples and the analysis largely confirms known concerns/behavior of LLMs. \n* \"we also identify a regime where the language model reproduces important statistics of the data, but fails to reproduce the dataset verbatim\": It's not clear to me what this statement means. See also Question 1 below.\n\n* Just echoing the authors: \"A limitation of our work is that we do not have access to the training data of GPT-3.5 and GPT4.\" I.e., the interpretation of results often remains speculative.\n* Figure 3: Why are some results with gpt-3.5 and some with gpt-4?\n* Typo: \"two publicly available dataset that are highly memorized\"\n* Typo: \"UCI repository athttps://\""
                },
                "questions": {
                    "value": "1. \"\u200b\u200bAn important result of our investigation is to identify a regime where the LLM has seen the data during training and is able to perform complex tasks with the data\": Don't LLMs behave as expected on some data and not on other? How does this work help to control the behavior of LLMs?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7826/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7826/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7826/Reviewer_8936"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7826/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698827434789,
            "cdate": 1698827434789,
            "tmdate": 1699636958160,
            "mdate": 1699636958160,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "q4PC63SuZG",
                "forum": "lwtaEhDx9x",
                "replyto": "2QzbpIB3Le",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7826/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7826/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer 8936"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the detailed review and many helpful comments.\n\n- *\u201cMy main point of criticism is that the paper feels a bit like a collection of remarkable examples and the analysis largely confirms known concerns/behavior of LLMs. \u201d*\n\nWe agree that dataset contamination is a general concern in LLMs, and that it is widely known that LLMs can memorize their training data. Despite this, many papers at the top ML conferences evaluate LLMs on datasets that potentially are part of the LLM\u2019s pre-training corpus without performing any systematic tests for memorization (see \u2018The relevance of our work\u2019 above) . We suspect that a main reason for this is that while contamination is a known concern, there are few tools available that allow researchers to systematically test for memorization and other forms of prior contamination within the timeframe of a single research project. With this paper, we attempt to develop testing approaches that will allow future research to address this question in a systematic manner. \n\n- *\u201cwe also identify a regime where the language model reproduces important statistics of the data, but fails to reproduce the dataset verbatim\u201d*\n\nHere we are referring to the behavior of GPT-3.5 on the California Housing dataset. On this dataset, the samples provided by the model match the correlations in the training data (Figure 3). At the same time, the average n-gram match between the samples and the training data is only 3.1/10 (Table 3) and there is no evidence for verbatim memorization except for the header of the dataset (Table 1).\n\n- *\u201cthe interpretation of results often remains speculative.\u201d*\n\nWe would not call our results speculative. We don\u2019t think that there is any plausible way in which the model could reproduce tabular datasets verbatim (Table 1) or with precisely matching statistics (Figure 3) without having seen those datasets during pre-training. \n\n- *Figure 3: \u201cWhy are some results with gpt-3.5 and some with gpt-4?\u201d*\n\nFigure 3 depicts selected examples. The full results, for both gpt-3.5 and gpt-4, are in Supplement D. \n\n- *\u201c\"\u200b\u200bAn important result of our investigation is to identify a regime where the LLM has seen the data during training and is able to perform complex tasks with the data\": Don't LLMs behave as expected on some data and not on other? How does this work help to control the behavior of LLMs?\u201d*\n\nWe don\u2019t know what the reviewer considers the expected behavior of LLMs to be. We would interpret our results as showing that there is a fair degree of variation in LLM behavior, both across different models, and also across datasets for a single model."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7826/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700643200147,
                "cdate": 1700643200147,
                "tmdate": 1700643200147,
                "mdate": 1700643200147,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rKzhZ8fS1n",
                "forum": "lwtaEhDx9x",
                "replyto": "q4PC63SuZG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7826/Reviewer_8936"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7826/Reviewer_8936"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your clarifications.\n\n> \u201c\u200b\u200bAn important result of our investigation is to identify a regime where the LLM has seen the data during training and is able to perform complex tasks with the data.\"\n\nI'm still not sure if I understand this statement: Is it (a) You identified \"knobs\" to control the behaviour of the LLM and tested it on a number of data sets or rather (b) You use an existing LLM and find data sets that exhibit the ability to perform complex tasks with the data?"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7826/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700729501161,
                "cdate": 1700729501161,
                "tmdate": 1700729501161,
                "mdate": 1700729501161,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]