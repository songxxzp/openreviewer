[
    {
        "title": "Motif: Intrinsic Motivation from Artificial Intelligence Feedback"
    },
    {
        "review": {
            "id": "ybgpiwvG1G",
            "forum": "tmBKIecDE9",
            "replyto": "tmBKIecDE9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4172/Reviewer_j8Na"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4172/Reviewer_j8Na"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes Motif: 1) replacing human labeling in preference-based RL with LLM labeling, and 2) joint optimization of preference-based and extrinsic rewards to solve the NetHack environment. After collecting sufficiently covered offline data from the existing RL methods, preference labels are annotated using LLaMA 2. Motif trains the preference reward model from those data and leverages it for online training from scratch, jointly maximizing preference-based and extrinsic rewards. Motif exhibits strong performance in staircase tasks from NetHack."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "### quality and clarity\n- This paper is well-written and easy to follow.\n\n### significance\n- The empirical results are strong. It would be notable to solve the difficult, sparse-reward NetHack environments that previous intrinsic-motivation methods cannot solve by leveraging preference-based reward."
                },
                "weaknesses": {
                    "value": "- I think the point of this paper is that \"joint optimization of preference-based and extrinsic reward helps resolve the sparse reward problems\". As the source of feedback, either humans or LLMs are OK. I think describing this as LLM's contribution might be an overstatement.\n- As a preference-based RL method, I guess there are no differences from the original paper [1]. In the LLM literature, [2] leverages GPT-4 to solve game environments, and [3] incorporates LLM-based rewards for RL pretraining.\n- Terminology: I'm not sure if a preference-based reward should be treated as an \"intrinsic\" reward. I think it is extrinsic knowledge (from humans or LLM).\n\n[1] https://arxiv.org/abs/1706.03741\n\n[2] https://arxiv.org/abs/2305.16291\n\n[3] https://arxiv.org/abs/2302.06692"
                },
                "questions": {
                    "value": "- Which RL algorithm is used for Motif? I may miss the description in the main text.\n- Are there any reason why employ LLaMA 2 rather than GPT-3.5 / 4?\n\n(Minor Issue)\n- In Section 2, `... O the observation space ...` might be `... O is the observation space ...`."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4172/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698610293239,
            "cdate": 1698610293239,
            "tmdate": 1699636383241,
            "mdate": 1699636383241,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ApQ9CkTgz4",
                "forum": "tmBKIecDE9",
                "replyto": "ybgpiwvG1G",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4172/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4172/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your feedback!\n\n> I think the point of this paper is that \"joint optimization of preference-based and extrinsic reward helps resolve the sparse reward problems\". As the source of feedback, either humans or LLMs are OK. I think describing this as LLM's contribution might be an overstatement. [...] As a preference-based RL method, I guess there are no differences from the original paper [1].\n\nWe never claim that an LLM's feedback is inherently better than the feedback coming from humans, even though we believe assessing whether that could be the case is an interesting avenue for future work. Instead, we simply leverage an LLM's feedback because of its scalability: in just a few hours on a small-sized cluster, one can annotate thousands of pairs of games events, which would require significant amounts of human NetHack experts's labour otherwise. This scalability, leveraged also in recent work on chat agents (e.g., Constitutional AI from Bai et al, 2022), allows for a method like Motif to fully leverage human common sense to bootstrap an agent's knowledge.\n\n> In the LLM literature, [2] leverages GPT-4 to solve game environments, and [3] incorporates LLM-based rewards for RL pretraining.\n\nIn [2] the Voyager algorithm uses complex prompt engineering involving significant amounts of human knowledge and engineering (such as deliberately prompting the LLM to use the \u201cexplore\u201d command regularly). Additionally, Voyager relies on a Javascript API to bridge the gap between the LLM's language outputs and the high dimensional observations and actions of Minecraft. Finally, Voyager relies on perfect state information about certain features of the game (e.g. agent position and neighbouring objects). Voyager also builds on GPT-4's strong coding abilities, which would likely not be the case of current open models. Altogether, these factors strongly limit Voyager's general applicability and reproducibility. On the other hand, Motif relies on very limited human knowledge, being able to get significant performance even without any information about NetHack. Moreover Motif is way simpler to implement, with very few, clearly separated, moving pieces, providing a robust solution for leveraging prior knowledge from large models. This makes our approach a significantly more general method that has the potential to be applied to multiple domains or be possibly combined with powerful Large Vision Language Models.\n\nWe have additionally compared Motif to the ELLM approach of [3], adapted as described in the general response, showing that Motif significantly outperforms it in all NLE tasks. Please see the common response for a detailed description of the experiment. The paper also highlights in Section 5 (Related Work) the important differences between Motif and the ELLM algorithm. In particular, ELLM's reward function cannot, by design, exhibit the exploration and credit assignment properties of the one produced by Motif (see Section 4.2 of the paper, \"Alignment with human intuition\"). We believe those differences are key to the strong performance of Motif.\n\n> Terminology: I'm not sure if a preference-based reward should be treated as an \"intrinsic\" reward. I think it is extrinsic knowledge (from humans or LLM).\n\nAs standard, we refer to extrinsic reward as the reward that comes from the task to be performed in the environment, whereas intrinsic rewards are provided by the algorithm. From that point of view, the reward provided by the LLM is intrinsic (as opposed to extrinsic) to the agent. Please notice that this terminology has previously been used in the literature (see, for instance, the ELLM [3] paper).\n\n\n> Which RL algorithm is used for Motif? I may miss the description in the main text.\n\n\nWe use the asynchronous PPO implementation of Sample Factory (Petrenko et al., 2020). This information is available in the paper on the bottom of page 4. We chose this implementation as it extremely fast: we can train an agent on 2B steps of interactions in only 24h using only one V100 GPU.\n\n\n> Are there any reason why employ LLaMA 2 rather than GPT-3.5 / 4?\n\nYes, we believe there are important and significant reasons to prefer using Llama 2 rather than GPT3.5 or GPT4. GPT3.5 and GPT4 are subject to changes over time, require significant financial efforts to be used at scale, and rely on unknown methodologies and practices. Despite the fact that they might provide better performance, they are problematic for rigorous scientific reproducibility, and thus they are significantly less preferrable than Llama 2 for conducting scientific research. We explicitly made this decision for the benefit of the scientific community, and we will also release our code and dataset to ease experimenting with a method like Motif for other members of the community.\n\n> (Minor Issue)\n\nWe thank the reviewer for spotting the typo. We corrected it in the updated version of the paper."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4172/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700090836876,
                "cdate": 1700090836876,
                "tmdate": 1700090836876,
                "mdate": 1700090836876,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pKsLAl1StP",
                "forum": "tmBKIecDE9",
                "replyto": "ybgpiwvG1G",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4172/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4172/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer,\n\nWe believe our response addresses your concerns. In particular, we have now a total of 7 different baselines, including two additional baselines using language model-based rewards (sentiment analysis and ELLM), which are all significantly outperformed by Motif.\n\nIf there are any remaining concerns which prevent you from increasing your score, please let us know in the discussion as soon as possible and we will do our best to address them.\n\nThank you,\n\nThe Authors"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4172/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700255838481,
                "cdate": 1700255838481,
                "tmdate": 1700255838481,
                "mdate": 1700255838481,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZAbrclpAKi",
                "forum": "tmBKIecDE9",
                "replyto": "ybgpiwvG1G",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4172/Reviewer_j8Na"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4172/Reviewer_j8Na"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the detailed response.\n\nMy questions on (1) the difference between yours and voyager/ELLM, (2) the RL algorithm, and (3) the choice of LLMs become clear now. Here are the remaining comments:\n\n**Contribution:**\n\nThe response to my concern seems to be reasonable, but not explicitly stated in the main text. So, the reader should infer the hidden intention why LLM is chosen rather than humans as the preference labeler (though alignment with *human* intention is emphasized many times). It is necessary to include your explanation (the scalability of LLM allows Motif to fully leverage human common sense to bootstrap an agent's knowledge, etc.) into the paper in this revision phase.\n\n**Terminology:**\n\nI cannot agree with your explanation because, in deep RL literature, the preference-based reward should be distinguished from \"intrinsic motivation\".  For instance, The strong performance of Motif with \"intrinsic reward only\" in Figure 1 causes confusion as to why the agent can learn decent behavior only with \"intrinsic motivation\",  since \"intrinsic reward\" often reminds the community of exploration. However, if the reader understands that Motif learns preference-based reward as a proxy for extrinsic reward, the results in Figure 1 will make sense. I express strong concerns about the description that equates \"intrinsic reward\" with a preference-based reward that should be a proxy for extrinsic reward and be learned from external knowledge."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4172/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700301480780,
                "cdate": 1700301480780,
                "tmdate": 1700301480780,
                "mdate": 1700301480780,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kQmpKvN2AD",
                "forum": "tmBKIecDE9",
                "replyto": "ybgpiwvG1G",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4172/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4172/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Request for re-evaluation"
                    },
                    "comment": {
                        "value": "Dear reviewer,\n\nWe hope our last answer addressed your remaining comments. Let us know if you have any remaining concerns, or if you could now recommend our paper for acceptance.\n\nBest,\n\nThe Authors"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4172/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700679249003,
                "cdate": 1700679249003,
                "tmdate": 1700679249003,
                "mdate": 1700679249003,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QAyb3tGtSA",
            "forum": "tmBKIecDE9",
            "replyto": "tmBKIecDE9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4172/Reviewer_2GW2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4172/Reviewer_2GW2"
            ],
            "content": {
                "summary": {
                    "value": "This paper provides Motif, a method for training a reinforcement learning (RL) agent with AI preferences. The main idea of Motif is to use AI preferences (or Large Language Model (LLM) preferences), instead of human preferences, for the preference-based RL. More specifically, Motif trains an intrinsic reward model on a LLM preference dataset, and then trains a RL agent by harnessing the intrinsic reward model. In summary, Motif consists of three phases: (1) dataset annotation by a LLM, (2) reward training on a LLM preference dataset, and (3) RL training with the reward model. This paper applies Motif to the NetHack Learning Environment (NLE). The paper uses Llama-2-70B as a preference annotator, and CDGPT5 as a baseline NetHack agent. This paper shows that RL agents trained with Motif's intrinsic reward surprisingly outperform agents trained using the score itself."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- S1. First of all, this paper is well-written and well-organized.\n- S2. The idea of using a LLM as a preference annotator for preference-based RL is interesting and promising.\n- S3. This paper provides a loss function (equation 1) to train an intrinsic reward model.\n- S4. This paper shows that training agents with intrinsic rewards is very effective."
                },
                "weaknesses": {
                    "value": "- W1. One of my main questions is whether Motif can be generally applied to other environments. Even though the NetHack Learning Environment (NLE) is a very challenging environment, it seems that the NLE may be one of environments that a LLM can easily annotate preferences."
                },
                "questions": {
                    "value": "- Q1. Can Motif be applied to other environments beyond the NetHack Learning Environment (NLE)?\n- Q2. What a RL algorithm is used for RL fine-tuning?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4172/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4172/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4172/Reviewer_2GW2"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4172/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698689381017,
            "cdate": 1698689381017,
            "tmdate": 1699636383170,
            "mdate": 1699636383170,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RqEDE8b7r6",
                "forum": "tmBKIecDE9",
                "replyto": "QAyb3tGtSA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4172/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4172/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your feedback!\n\n>  Can Motif be applied to other environments beyond the NetHack Learning Environment (NLE)?\n\nWe could not investigate this question in our current paper, as it is already compact with detailed analysis on the behavior, the risks of using LLMs and the possibilities for defining diverse rewards. By adding other environments, we could not provide such in-depth analysis. \n\nWe strongly believe that Motif is a general method, and it can applied to other environments after reasonably-sized efforts, when its main assumptions are satisfied. In particular, Motif's LLM needs to have enough knowledge about the environment, which is related to the presence on the Internet of text related to it, and the availability of an event captioning system. These assumptions are realistic in many environments, both when dealing with a physical system (e.g., a robot accompanied by a vision captioner) and a simulated/virtual world (e.g., a commonly-played videogames or Web browsing). Additionally, one could apply the general architecture of Motif to any environments based on visual observations, by just substituting a VLM in place of the LLM. We believe this is an exciting direction for future work.\n\nTo give some context on our choice of environment, NetHack is a challenging and illustrative domain to deploy an algorithm like Motif. Captions in NetHack are non-descriptive: they do not provide a complete picture on the underlying state of the game. Moreover, these captions are sparse, appearing in only 20% of states. This means that overall there is a high degree of partial observability. Despite this challenge Motif is able to thrive and show results that we have not witnessed in the literature previously. \n\nWe believe that if we were to apply Motif in other environments with more complete descriptions we could see even stronger performance. This would bring important questions to be studied: what exactly is the impact of partial observability on preferences obtained from an LLM? Do more detailed captions unlock increasingly more refined behaviors from the RL agent? Such important questions could be investigated by future work.\n\n> What a RL algorithm is used for RL fine-tuning?\n\nWe use the asynchronous PPO implementation of Sample Factory (Petrenko et al., 2020). This information is available in the paper on the bottom of page 4. We chose this implementation as it extremely fast: we can train an agent on 2B steps of interactions in only 24h using only one V100 GPU."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4172/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700090819666,
                "cdate": 1700090819666,
                "tmdate": 1700090819666,
                "mdate": 1700090819666,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gHJEddULkW",
                "forum": "tmBKIecDE9",
                "replyto": "RqEDE8b7r6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4172/Reviewer_2GW2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4172/Reviewer_2GW2"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for providing thoughtful responses to my questions. It helped me to understand this work more concretely."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4172/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700742030141,
                "cdate": 1700742030141,
                "tmdate": 1700742030141,
                "mdate": 1700742030141,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YmGIe1M2XC",
            "forum": "tmBKIecDE9",
            "replyto": "tmBKIecDE9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4172/Reviewer_VWkY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4172/Reviewer_VWkY"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces Motif, a method for integrating the common sense and high-level knowledge of LLMs into reinforcement learning agents. Motif works by eliciting preferences from the LLM based on pairs of event captions. These preferences are then translated into intrinsic rewards for training agents. The authors test Motif on the NetHack Learning Environment, a complex, open-ended, procedurally-generated game. The results show that agents trained with Motif's intrinsic rewards outperform those trained solely to maximize the game score. The paper also delves into the qualitative aspects of agent behavior, including alignment properties and the impact of prompt variations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is clear and well presented.\n2. The idea of using intrinsic rewards generated from an LLM's preferences is both innovative and practically useful, potentially paving the way for more human-aligned agents.\n3. The method scales well with the size of the LLM and is sensitive to prompt modifications, offering flexibility and adaptability.\n4. The paper provides a comprehensive analysis, covering not just the quantitative but also the qualitative behaviors of the agents."
                },
                "weaknesses": {
                    "value": "1. The paper could benefit from a more extensive comparison to other methods, especially those that also attempt to integrate LLMs into decision-making agents.\n2. There is a lack of discussion on the computational cost and efficiency aspects of implementing Motif.\n3. While the paper makes a strong case for Motif, it doesn't delve deeply into the limitations or potential drawbacks of relying on LLMs for intrinsic reward generation."
                },
                "questions": {
                    "value": "1. Could the authors offer insights into why agents trained  on extrinsic only perform worse than those trained on intrinsic only rewards?\n2. What's the best strategy to optimally balance intrinsic and extrinsic rewards during training?\n3. Can the authors elaborate on the limitations of using LLMs for generating intrinsic rewards? Are there concerns about misalignment or ethical considerations?\n4. How robust are agents trained with Motif against different types of adversarial attacks or when deployed in varied environments?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4172/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698723817523,
            "cdate": 1698723817523,
            "tmdate": 1699636383094,
            "mdate": 1699636383094,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EMuSNhvpLq",
                "forum": "tmBKIecDE9",
                "replyto": "YmGIe1M2XC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4172/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4172/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for you feedback!\n\n> The paper could benefit from a more extensive comparison to other methods, especially those that also attempt to integrate LLMs into decision-making agents.\n\nFirst of all, we refer the reviewer Figure 7 in the Appendix F, in which we show that Motif outperforms four competitive baselines, including E3B (Henaff et al, 2022) and NovelD (Zhang et al., 2021), two state-of-the-art approaches specifically created for procedurally-generated domains such as NetHack. \n\nIn the updated paper, we have now additionally added a comparison to ELLM (Du et al., 2023), a recent approach for deriving reward functions from LLMs, showing that Motif's performance significantly surpasses such LLM-based baselines across all tasks. This is due to the peculiar features of Motif's intrinsic reward (e.g., its anticipatory nature), which, by design, are not implied by a reward function based on the cosine similarity between a goal and a caption. Our implementation is described in detail in the general answer above.\n\n> There is a lack of discussion on the computational cost and efficiency aspects of implementing Motif.\n\nDue to our implementation being modular and asynchronous (that will be publicly released), dataset annotation is not especially expensive. Please see the general response for complete computational considerations. In addition, in Figure 15 we show that the performance of our method is particularly robust to the size of the dataset of annotations: Motif is able to outperform the baselines even with a dataset that is five times smaller (i.e., ~100k annotations).\n\n> While the paper makes a strong case for Motif, it doesn't delve deeply into the limitations or potential drawbacks of relying on LLMs for intrinsic reward generation.\n\nWe believe, as the reviewer does, that addressing limitations in LLM-based work is critical: this is why a substantial fraction of our paper is devoted to analyzing limitations and pitfalls of intrinsic motivation from an LLM's feedback. In particular, we want to highlight that we dedicated a full page of the paper to demonstrating evidence for, explaining, and characterizing _misalignment by composition_, a negative phenomenon relevant to our framework, whose emergence is a current limitation of Motif. In addition, we studied the sensitivity of Motif to different prompts, showing in Figure 6c that semantically-equivalent prompts can lead, in complex tasks, to drastically different behaviors.  We believe this is a limitation of current approach based on an LLM's feedback, and hope that future work will be able to address it. Finally, we also included in Appendix H.3 a study on the impact of the data diversity of the dataset (through which we elicit preferences) and the resulting final performance.\n\nPlease notice that we are also very upfront about the fundamental assumption behind Motif, which is also a fundamental assumption behind the zero-shot application of LLMs to new tasks: that the LLM contains prior knowledge about the environment of interest. Our Introduction is centerered around this assumption.\n\n> Could the authors offer insights into why agents trained on extrinsic only perform worse than those trained on intrinsic only rewards?\n\nOur paper already provides some insights in the \"Alignment with human intuition\" paragraph of Section 4.2, but we will now provide an additional perspective that can be beneficial to understand this result. By inspecting the messages preferred by the intrinsic reward function, one can quickly realize that the agent will receive from the LLM's feedback three kinds of rewards: _direct rewards_, _anticipatory rewards_ and _exploration-directed rewards_. Direct rewards (e.g., for \"You kill the cave spider!\") leverage the LLM's knowledge of NetHack, implying a reward very similar to the score (i.e., the extrinsic reward). Motif's reward, however, goes beyond this. Anticipatory rewards (e.g., for \"You hit the the cave spider!\") implicitly transport credit from the future to the past, encouraging events not rewarded by the extrinsic signal and easing credit assignment. Finally, exploration-directed rewards (e.g., for \"You find a hidden door.\") directly encourage explorative actions that will lead the agent to discover information in the environment. Together, these three types of rewards allow the agent to maximize a proxy for the game score that is way easier to optimize compared to the actual game score, explaining the increased performance.\n\n\n> What's the best strategy to optimally balance intrinsic and extrinsic rewards during training?\n\nWe show in Figure 10c in Appendix that Motif is quite robust to how the two rewards are balanced. Broadly speaking, given that the nature of Motif's intrinsic reward brings it closer to a value function, future work can explore potentially more effective ways to leverage such type of intrinsic reward, for instance via potential-based reward shaping (Ng et al., 1999)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4172/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700090791008,
                "cdate": 1700090791008,
                "tmdate": 1700090791008,
                "mdate": 1700090791008,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6yN80vkoPp",
                "forum": "tmBKIecDE9",
                "replyto": "Hs103Suayb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4172/Reviewer_VWkY"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4172/Reviewer_VWkY"
                ],
                "content": {
                    "comment": {
                        "value": "Thank the authors for their clarifications and the additional experiments. Most of my concerns have been addressed. I will maintain my positive opinion towards accepting the paper."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4172/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700629740227,
                "cdate": 1700629740227,
                "tmdate": 1700629740227,
                "mdate": 1700629740227,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "VZz21uJuD5",
            "forum": "tmBKIecDE9",
            "replyto": "tmBKIecDE9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4172/Reviewer_wqBP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4172/Reviewer_wqBP"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method called Motif for tackling the exploration problem in RL by making use of an LLM to learn an intrinsic reward function, which is used to learn a policy using a standard RL algorithm. The testbed for this approach is the NetHack Learning Environment, a popular procedurally generated environment with sparse reward. This approach leads to strong improvements in performance over prior approaches without demonstrations and also results in human understandable behaviours."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Figure 1 provides a nice clean bird's eye view of the overall approach and helps with readability.\n- The evaluation of the agent for not just the game score but also other dimensions provides a helpful qualitative assessment of the proposed approach and baselines through the spider graph in Figure 4.\n- The ablation experiments for the approach are quite exhaustive, covering scaling laws, prior v/s zero knowledge, rewordings of the prompts, etc."
                },
                "weaknesses": {
                    "value": "- Using a 70-billion LLM to generate a preference dataset from given captions is quite expensive; while I understand this is out of the scope of the paper, perhaps using a large VLM to annotate frames without captions might have been more economical?\n- Given that one of the key contributions of the paper is the intrinsic reward function that is learnt from preferences extracted from the LLM, it might be worthwhile having a baseline that gives preferences using a simpler model (say sentiment analysis) and learn the RL policy using this intrinsic reward model."
                },
                "questions": {
                    "value": "- In the part on \"Alignment with human intuition\", the paper mentions that the agent exhibits a natural tendency to explore the environment by preferring messages that would also be intuitively preferred by humans. Is this a consequence of having a strong LLM, or is it due to the wording of the prompt?\n- An ablation over $\\alpha_2$ has been provided in the appendix, but the value of the coefficient for the intrinsic reward $\\alpha_1$ is kept fixed at 0.1; could you explain the reason behind that?\n- In Figure 6c, the score for the reworded prompt is quite low but its dungeon level keeps steadily rising compared to the default prompt. Is this a case of the agent hallucinating its dungeon level due to a very high intrinsic reward, or is it something else?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4172/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4172/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4172/Reviewer_wqBP"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4172/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699223413133,
            "cdate": 1699223413133,
            "tmdate": 1700668318869,
            "mdate": 1700668318869,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1d4oypRs36",
                "forum": "tmBKIecDE9",
                "replyto": "VZz21uJuD5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4172/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4172/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your feedback!\n\n> Using a 70-billion LLM to generate a preference dataset from given captions is quite expensive\n\nDue to our implementation being modular and asynchronous (that will be publicly released), dataset annotation is not especially expensive. Please see the general response for complete computational considerations. In addition, in Figure 15 we show that the performance of our method is particularly robust to the size of the dataset of annotations: Motif is able to outperform the baselines even with a dataset that is five times smaller (i.e., ~100k annotations).\n\n> while I understand this is out of the scope of the paper, perhaps using a large VLM to annotate frames without captions might have been more economical?\n\nThe question of whether annotations extracted from a VLM would be more efficient than the ones extracted from running an LLM on captions is interesting. Unfortunately, our experiments with current open VLM models suggest that none of them are able yet to interpret visual frames well enough to provide an effective evaluation or even accurate captions (most likely because current open models are predominantly trained on natural images). However, given the current pace of VLM research, this may change very soon. Thus, investigating the difference in the efficiency of various types of feedback will be an interesting avenue for future research.\n\nIn general, the question of large VLMs operating on images vs LLMs operating on captions brings interesting tradeoffs. Large VLM are more general, since they do not assume access to captions, but are faced with a more challenging task since they work with complex images rather than compressed text descriptions. From a purely computational standpoint, if captions are available and are of high quality then LLMs are likely more economical since their inputs are smaller. \n\n> it might be worthwhile having a baseline that gives preferences using a simpler model (say sentiment analysis) and learn the RL policy using this intrinsic reward model.\n\nWe added the results of an experiment using a sentiment analysis model as a preference model in the updated paper (Figure 8 of Appendix A.6). We use a [T5 model](https://huggingface.co/mrm8488/t5-base-finetuned-imdb-sentiment) fine-tuned for sentiment analysis, and extract, for each message, a score computed as the sigmoid of the confidence of the model in its positive or negative sentiment prediction. Then, for each pair in the dataset, we assign a preference based on the message with higher sentiment score. Finally, we execute reward training and RL training as with Motif. \n\nResults on the `score` task show performance close to zero, both with and without extrinsic reward. This poor performance can be easily explained: a generic sentiment analysis model cannot capture the positivity or negativity of NetHack-specific captions. For instance, killing or hitting are generally regarded as negative statements, but they become positive in the context of killing or hitting monsters in NetHack. Llama 2 can understand this out-of-the-box without any fine-tuning, as demonstrated by our experiments. Also note that such a vanilla sentiment analysis model cannot be easily steered, thus losing any opportunity for the controllability offered by Motif.\n\nTo attest to Motif's strong performance, we also compared with an additional LLM-based baseline (as requested by Reviewer VWkY). The details of this additional experiment are presented in the common response above.\n\n\n> the paper mentions that the agent exhibits a natural tendency to explore the environment by preferring messages that would also be intuitively preferred by humans. Is this a consequence of having a strong LLM, or is it due to the wording of the prompt?\n\nWe believe this is due to the fact that the LLM was pretrained on massive amounts of human data, and then fine-tuned on human preferences. Indeed, even when using the zero-knowledge prompt presented in Prompt 2 of the Appendix B, Motif's reward function allows agents to play the game effectively even without any reward signal from the environment (see Figure 6b)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4172/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700090740907,
                "cdate": 1700090740907,
                "tmdate": 1700090740907,
                "mdate": 1700090740907,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xRWJ4Emiz7",
                "forum": "tmBKIecDE9",
                "replyto": "3bxb4Ri8VW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4172/Reviewer_wqBP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4172/Reviewer_wqBP"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the responses to my questions and also for the additional experiments. I also appreciate the overall note to all reviewers in which compute-related issues raised by all reviewers were addressed. I am satisfied with the responses and am increasing my score."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4172/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700668306931,
                "cdate": 1700668306931,
                "tmdate": 1700668306931,
                "mdate": 1700668306931,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]