[
    {
        "title": "COSA: Concatenated Sample Pretrained Vision-Language Foundation Model"
    },
    {
        "review": {
            "id": "NdYW13uAEI",
            "forum": "bDkisS75zy",
            "replyto": "bDkisS75zy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5044/Reviewer_bdF4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5044/Reviewer_bdF4"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a concatenated sample pretrained vision-language foundation model. By sequentially connecting multiple image-text pairs as pre-training inputs, it can jointly model visual content and event-level temporal cues using only image-text corpora. Extensive experiments show the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tMotivations: This paper presents a very important problem of how to capture time-level event clues using only image-text data. In the case of insufficient quality and quantity of video data, it provides a very important help for the pre-training of vision-language foundation model. The proposed method is very simple and effective. I think this work is easy to follow and most of the techniques are correct.\n\n2.\tExtensive experiments: A large amount of experimental evidence is provided in this paper, which fully verifies the effectiveness of the proposed method."
                },
                "weaknesses": {
                    "value": "1.\tTechnical contributions: The proposed method is simple and effective. However, the proposed method is not surprising enough, because using pictures to enhance video pre-training has been quite explored in the field of CV/ vision-language pre-training field. This paper combines many existing pre-training methods, so the technology sharing is limited.\n\n2.\tMore explanation: The continuous frames in the video are similar, and there are relations between different events. Establishing event-level correlation includes two parts: (1) the first part is to distinguish between different events. (2) the second part is to make temporal inferences between similar or related frames. The proposed method randomly splices several pictures, but there is no correlation between these pictures, so the model can only distinguish different events, but can not make the model time sequence inference between frames. Therefore, I do not think that the proposed method fully corresponds to its motivations."
                },
                "questions": {
                    "value": "1. In Table 9, why is it better to concatenate random images for training than to concatenate only semantically similar images?\n2. It is better to give the weights of the 6 losses (training objectives) and the size of the input image in the implementation details."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5044/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5044/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5044/Reviewer_bdF4"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5044/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697787933590,
            "cdate": 1697787933590,
            "tmdate": 1699636494101,
            "mdate": 1699636494101,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wktH4jiyHJ",
                "forum": "bDkisS75zy",
                "replyto": "NdYW13uAEI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5044/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5044/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer bdF4"
                    },
                    "comment": {
                        "value": "## **Q1:  Using pictures to enhance video pre-training has been quite explored in the field of CV/ vision-language pre-training field**  \n## **A1:** \n Indeed many video-language pretraining methods also using image-text data (CC3M) as their training corpora, **but they view each picture as individual training sample.** By contrast, **COSA is the first work to utilize pictures to compose pesudo video-paragraph training data and proved that can  further improve performances of series of understanding and generation tasks**. In other words, we believe that our technical contribution lies in COSA's proposal of a novel approach to effectively utilize image data for video-language pretraining, rather than just employing both image and video data in pretraining, which has already been widely explored.\n\n## **Q2: Proposed method does not fully correspond to its motivations.**\n## **A2:**\n We also agree on your analysis of two parts of temporal modeling, i.e., inter-scene temporal correlation and intra-scene temporal correlations. However, **as wrote in the \"Temporal Learning in Video-Language Pretraining (VidLP)\" section of Related Work** in the paper, we have attempted to let **\"event-level temporal\" refer explicitly to the first part, i.e. inter-scene temporal, instead of both parts**, and we use \"action-level temporal\" to refer to the second part, i.e. intra-scene temporal.  There are some works like OmniVL and HiTeA tried to model action-level temporal during pretraining, while COSA emphasizes more at event-level temporal learning, considering that action-level (intra-scene) needs consecutive images in a scene, which is hard to realize using pure image-text data. We also think that strengthening both action-level and event-level temporal modeling in a unified framework could be a meaningful future video-language research direction. **The motivation of current version of COSA  is to strength \"event-level\" (first part) temporal modeling only instead of both \"action-level\" (second part) and \"event-level\" temporal modeling.** **And we have made the illustration more clear in the updated version of Related Work**.\n\n## **Q3: Why is it better to concatenate random images for training than to concatenate only semantically similar images**\n## **A3:**\nIn our experiments, COSA trained with random sampling works better than the one trained with relevant sampling. **We make visualizations of different sampling methods and add explanations in the updated version of supplementary materials.** \n\nSpecifically, as figure 1 shown in the supplementary material, when we observe a real video-language sample from ActivityNet caption dataset (the top green part of figure), we can find that the 1) sampled vision frames are relevant and coherent, and 2) the corresponding texts are very informative and can strictly match to different frames in temporal order. \n\nMotivated by this, an ideal  large-scale  pretraining dataset for video-language tasks should also possess those two characteristics to help model learn the capabilities of perceiving events happend in videos and their relative order.  In this work, we propose COSA to imitate those pretrain dataset from image-text data. And the examples of COSA with different sampling methods are shown in the bottom pink part of the figure, from which we can find that even though the random sampling example shows little relevance between four independent scenes, but it has the strength that each sentance can strictly correspond to one image, which satisfies the second charatersic. \n\nBy contrast, even though relevant (vision/text similarity) sampling examples have shown vision relevance  to some extent, they have strong semantic redundancy, and one sentence can correspond to many frames, which could confuse models' event-level temporal learning. And we think that's why it performs weaker than random sampling. In addition, considering that the scene changes are somewhat hard in random sampling, we think other soft sampling methods could be a future research direction.\n\n## **Q4: It is better to give the weights of the 6 losses (training objectives) and the size of the input image in the implementation details.**\n## **A4:** \nWe haven't intended to tune the 6 training losses, and they  are simply equally weighted. With regards to input image size, as presented in Table 3 of Appendix, for pretraining and video tasks finetuning, we use 224 resolution, and for image-text tasks finetuning, we use 384 for QA and retrieval, and 480 for captioning. **We have added loss weights and input image size to the implementation details at the updated version of paper, thanks for your advice**."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5044/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700310636898,
                "cdate": 1700310636898,
                "tmdate": 1700652642373,
                "mdate": 1700652642373,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "n8rE4gLplm",
                "forum": "bDkisS75zy",
                "replyto": "NdYW13uAEI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5044/Reviewer_bdF4"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5044/Reviewer_bdF4"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the responses given by the authors. The newly conducted experiments and visualizations have addressed my concerns. I would like to keep my positive rating."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5044/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700652425034,
                "cdate": 1700652425034,
                "tmdate": 1700652540263,
                "mdate": 1700652540263,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "jyfSb5qPmA",
            "forum": "bDkisS75zy",
            "replyto": "bDkisS75zy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5044/Reviewer_gWLj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5044/Reviewer_gWLj"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a new vision-language pre-training framework, called COSA. In particular, COSA augmented original image-text pairs by concatenating multiple examples as pseudo video-text pairs. Extensive experiments were conducted covering both video-language and image-language tasks, and demonstrated the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is well written and easy to follow. In addition, the proposed method was supported by comprehensive experiments together with ablation studies, which made the paper a complete work.\n- The method COSA itself was simple yet effective to improve the learned representations for downstream tasks, and at the same time, it did not introduce extra computational costs."
                },
                "weaknesses": {
                    "value": "- The method was more like a trick of data augmentation instead of a significant technical contribution, as it just simply concatenated images and their corresponding captions and it was not very surprising to observe performance improvements.\n- As it was mentioned in the paper that apart from modified objectives, COSA also included original objectives for pre-training on image-text pairs. It was a complicated design to have so many training objectives and it was unclear how they were weighted (seemed to be equally weighted). Even though there was an ablation study of training objectives in Table 7, it still did not explain well the contributions of each item.\n- The method leveraged the average pooled [CLS] token for each image as the final representation for the pseudo video. In this way, there was actually no temporal information considered. And the selected downstream tasks were less dependent on temporal information in the meanwhile. It would be better if tasks such as temporal action localization were included to show whether COSA can improve those tasks. In addition, since temporal information did not play any role in current method, I am afraid that using augmentations like mixup for videos/images might lead to similar performance gain, as shown in [1].\n- Previous works showed that using CLIP initialization could lead to better performance. Among compared baseline methods, some of them such as MILES [2] actually used ViT trained on ImageNet for image classification and it was not a fair comparison to COSA with CLIP initialization.\n\n[1] Hao, Xiaoshuai, et al. \"Mixgen: A new multi-modal data augmentation.\" Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 2023.\n\n[2] Ge, Yuying, et al. \"Miles: Visual bert pre-training with injected language semantics for video-text retrieval.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022."
                },
                "questions": {
                    "value": "- Is it possible for the authors to include tasks which rely much on temporal information like temporal action localization? This would provide better understanding of the proposed method.\n- It would be better if results with different initializations can be presented to remove my concern about better CLIP initialization.\n- It was worth trying data augmentations like mixup and it might lead to similar performance gain as demonstrated in the paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5044/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5044/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5044/Reviewer_gWLj"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5044/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698552787035,
            "cdate": 1698552787035,
            "tmdate": 1700678123487,
            "mdate": 1700678123487,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "InjuDQ6847",
                "forum": "bDkisS75zy",
                "replyto": "jyfSb5qPmA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5044/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5044/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer gWLj - Part1"
                    },
                    "comment": {
                        "value": "## **Q1: Novelty and comparison with other data augmentation methods.**\n## **A1:**\n- We think viewing COSA from the data augmentation perspective also makes sense, considering that the core operation of COSA is to concatenate image-text data into pseudo video-paragraph data, and learn event-level temporal correspondences which are needed for video-text downstream tasks. **However, as far as we know, we are the first to compose pseudo video-paragraph data from image-data, demonstrating its effectiveness across abundant both image-text and video-text tasks, making detailed comparison with the common SST baseline, and verifying its generality across different dataset and model scales, so we think COSA has its novelty.** \n\n- In addition, as suggested, we add experiments to make comparisons with other data augmentation method. Mixup is first proposed for image classification tasks which sample two images and interpolate both raw images and their labels, in equal weight. Motivated by Mixup, Mixgen promotes it from image field to image-text field, replacing labels with captions and keeping image processing remained (average raw pixels of two images). We add the comparison results as follows, in which we find that compared to SST baseline, mixgen severely decreases the performance of downstream video-text tasks, while COSA improves SST baseline evidently. We attribute this phenomenon to that Mixgen simply averages different images into one single image without any temporal modaling, leaving event-level temporal learning unconsidered, thus the more images averaged (4 vs 2), the more performance degrades. By contrast, COSA concatenates multiple images  into videos and encodes temporal information via temporal position embeddings, which can effectively learn event-level temporal correspondence, and more images can further improve performances. These results demonstrate that COSA is more general and effective compared to Mixgen. \n| Method | # Concatenated or mixed Images | DiDeMo-RET (R@1) | ActivityNet-RET (R@1) | VIST-CAP(CIDEr) | TVC-CAP(CIDEr) | TGIF-QA(Acc) |\n| --- | --- | --- | --- | --- | --- | --- |\n| SST | - | 49.2 | 43.8 | 14.8 | 53.9 | 73.0 |\n| COSA(Ours) | 2 | 49.1 | 45.5 | 15.3 | 54.5 | 73.5 |\n| COSA(Ours) | 4 | 54.8 | 51.2 | 20.7 | 55.4 | 73.7 |\n| Mixgen | 2 | 46.5 | 42.9 | 11.9 | 53.6 | 73.1 |\n| Mixgen | 4 | 40.8 | 37.2 | 10.0 | 52.9 | 72.3 |\n\n## **Q2: Different model initialization beyond CLIP.**\n## **A2:**\n- In fact, for fair comparison with state-of-the-art methods, we train different scales of COSA model with different vision encoders and training corpus. As depicted in the following table (the same as Table 1 in the paper), COSA-B takes **Swin-Base** trained on ImageNet as vision encoder, **instead of CLIP** pretrained on 400M WebDataset.\n| Model |  Vision Encoder |\n| --- | --- |\n| COSA-B | Swin-B |\n| COSA-L | CLIP/ViT-L/14 |\n| COSA |  EVAClip/VIT-G/14 |\n\n- **We copy the comparison results of COSA-B from Table 2 in the paper below, and additionally add a vision encoder column**. It is noted that **all methods use vision backbones pretrained on ImageNet, and thus we believe a fair comparison is warranted.** Methods using Swin as backbone generally outperform the ones using ViT as backbone, and COSA surpasses other methods like Clover, VIOLETv2, LF-VILA, which also use Swin. Considering that Swin may be marginlly better than ViT backbone, we are training a COSA-B(ViT) model for a absolute fair comparison with methods initialized with ViT, such as MILES. However, this experiment requires a longer training period compared to our other experiments conducted for this rebuttal, due to the increased number of training iterations and the non-fixed vision backbone. Consequently, we are unable to complete it before the end of the rebuttal period. We will add this comparison results at the final version of paper.\n| Method | Vision encoder | Example | MSRVTT | DiDeMo | LSMDC | ActivityNet |\n| --- | --- | --- | --- | --- | --- | --- |\n| ClipBert | ResNet50 | 5.4M | 22.0 /46.8/59.9  | 20.4/48.0 /60.8 | - | 21.3/49.0/63.5 |\n| Frozen | ViT-B | 5M | 31.0/59.5/70.5  | 34.6/65.0/74.7 | 15.0/30.8 /39.8 | - |\n| BridgeFormer | ViT-B | 5M | 37.6/64.8/75.1 | 37.0/62.2/73.9 | 17.9/35.4/44.5 | - |\n| MILES | ViT-B | 5M | 37.7/63.6/73.8 | 36.6/63.9/74.0  | 17.8/35.6/44.1  | - |\n| OA-Trans | ViT-B | 5M | 35.8/63.4/76.5 | 34.8/64.4/75.1 | 18.2/34.3/43.7 | - |\n| Clover | Swin-B | 5M | 38.6/67.4/76.4 | 45.1/74.3/82.2 | 22.7/42.0/52.6 | - |\n| VIOLETv2 | Swin-B  | 8.5M | 37.2/64.8/75.8  | 47.9/76.5/84.1  | 24.0/43.5/54.1 | - |\n| LF-VILA | Swin-B | 5M |-  | 35.0/64.5/75.8 | - | 35.3/65.4/- |\n| COSA-B | Swin-B | 5M | **42.2/69.0/79.0** | **57.8/80.6/87.9** | **27.3/46.7/55.2** | **55.6/80.7/89.1**  |"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5044/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700309347952,
                "cdate": 1700309347952,
                "tmdate": 1700668225617,
                "mdate": 1700668225617,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QNvlvmgfm4",
                "forum": "bDkisS75zy",
                "replyto": "jyfSb5qPmA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5044/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5044/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer gWLj - Part2"
                    },
                    "comment": {
                        "value": "## **Q3: Evaluation on temporal action localization benchmark.**\n## **A3**:\n - We evaluate COSA and SST on **temporal action localization (TAL)** task. Specifically, we take **actionformer** as baseline methods and evaluate on **ActivityNet dataset** with features extracted from vision encoders of COSA-B and SST-B respectively. It is noted that they are both trained with the same datasets and iterations. The results are shown in the following table, from which we can find that both SST and COSA improve performance over baseline. Furthermore, COSA surpasses SST, demonstrating its efficacy on the localization task.\n| Method | Vision feature | mAP |\n| --- | --- | --- |\n| Actionformer | Swin-B | 33.40 |\n| Actionformer | SST-B | 34.76 |\n| Actionformer | COSA-B | 35.10 |\n\n- However, the improvements appear limited, which we believe may be due to the fact that COSA is pre-trained as an integrated vision-language framework, but when fine-tuned on TAL, only its vision encoder is utilized. This approach might not fully leverage the strengths of the COSA framework. So we additionally evaluate COSA on **text-guided video grounding task** (i.e., moment retrieval). \nSpecifically, inspired by **MMN**, we sparsely sample 16 frames per video and forward them to the vision encoder,  getting a 1d feature map whose size equals to 16\\*C (C is the hidden size of channel). After that we transform the 1d feature map to 2d map whose size is 16\\*16*C. Each feature located at (i, j) in the feature map represents a pre-defined anchor with fixed start and end timestamps (which are the timestamps of i-th and j-th frames), and they are computed by averaging the features of i-th and j-th frames along channel. It is noted that only the upper triangle part of 2d map is valid due to that start time must be smaller than end time. Input querys are processed by the text encoder of COSA.  The whole training losses consist of contrastive loss and IOU loss, following MMN (constrastive head is inherited from pretrained COSA model and IOU head is newly initialized). All parameters of networks are updated during finetuing. We conducted experiments on the **Charades dataset**. As presented in the following table, COSA-B surpasses SST-B with large margins on the R@1 metric of different IOU scores, strongly proving that COSA can also benefit localization tasks.\n| Method | R@1_IOU0.3 | R@1_IOU0.5 | R@1_IOU0.7 |\n| --- | --- | --- | --- |\n| Baseline(w/o pretrain) | 67.69 | 54.49 | 32.15 |\n| SST-B | 68.66 | 56.45 | 34.84 |\n| COSA-B | 69.62 | 58.01 | 36.40 |\n\n## **Q4: More explanations of training losses.**  \n## **A4:**   \nSince there are up to six losses (ITC/ITM/CITC/CITM/CMLM/CGM) used in COSA, we do not tune their weights considering the complexity. Using identical weights already achieves good performance. We also make experiments to analyze the function of each loss. For clarity, we group ITC and ITM into L_align (L_align=L_itc+L_itm), and group MLM and GM into L_mask (L_mask = L_mlm+L_gm). The analysis is as follows:\n- ITC improves retrieval task (line1 vs line0), and ITM make further improvements via fine-grained multimodal fusion and rereanking (line2 vs line1).\n - MLM (bi-direction self-attention masks) can improve caption and QA tasks (line3 vs line0), and adding GM (causal self-attention mask) can further enhance performance (line4 vs line3).\n-  Using L_align and L_mask together leads to notable improvements on all tasks (line5 vs line4).\n-  Regarding the 'Concatenate' operation in COSA, replacing L_mask with L_Cmask and incorporating L_Calign both contribute to improved performance across all tasks (line6 vs line5, line7 vs line6).\n\n| Line | Losses  | DiDeMo-RET | MSRVTT-RET | TVC-CAP | TGIF_QA |\n| --- | --- | --- | --- | --- | --- |\n| 0 | - | 26.9 | 34.1 | 51.0 | 69.4 |\n| 1 | L_itc | 38.4 | 37.1 | 49.7 | 66.6 |\n| 2 | L_align (L_itc+L_itm) | 48.4 | 38.3 | 50.3 | 69.2 |\n| 3 | L_mlm | 26.0 | 31.6 | 52.1 | 72.1 |\n| 4 | L_mask (L_mlm+L_gm)  | 30.9 | 33.5 | 53.3 | 72.9 |\n| 5 | L_align+L_mask | 49.2 | 39.4 | 53.9 | 73.0 |\n| 6 | L_align+L_Cmask | 53.0 | 42.8  | 55.7  | 73.7 |\n| 7 | L_align+L_Calign+L_Cmask | 54.8  | 43.5  | 55.4 | 73.7 |"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5044/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700319198369,
                "cdate": 1700319198369,
                "tmdate": 1700665219232,
                "mdate": 1700665219232,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HtIAaf7sUy",
                "forum": "bDkisS75zy",
                "replyto": "jyfSb5qPmA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5044/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5044/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer gWLj - Part3"
                    },
                    "comment": {
                        "value": "## Reference\n- Hao et al. Mixgen: A new multi-modal data augmentation.\n- Zhang et al. Actionformer: Localizing moments of actions with transformers\n- Wang et al. Negative Sample Matters: A Renaissance of Metric Learning for Temporal Grounding"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5044/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700320711137,
                "cdate": 1700320711137,
                "tmdate": 1700320711137,
                "mdate": 1700320711137,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bt40P9BpTJ",
                "forum": "bDkisS75zy",
                "replyto": "InjuDQ6847",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5044/Reviewer_gWLj"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5044/Reviewer_gWLj"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your detailed response. I acknowledge that the paper demonstrated a simple and effective method to improve video-language pre-training empirically but the technical contribution is limited. Considering the completeness of the paper, I would increase my score to 6."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5044/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700678085049,
                "cdate": 1700678085049,
                "tmdate": 1700678085049,
                "mdate": 1700678085049,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "OUYXw0B4aO",
            "forum": "bDkisS75zy",
            "replyto": "bDkisS75zy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5044/Reviewer_vqzh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5044/Reviewer_vqzh"
            ],
            "content": {
                "summary": {
                    "value": "The work proposes the vision-language foundation model, which can jointly model visual contents and event-level temporal cues using only image-text corpora. Extensive experiments demonstrate that COSA consistently improves performance across a broad range of semantic vision-language downstream tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper proposes the effective method for video-text and image-text tasks.\n2. The experiment is very adequate.  The model consistently improves performance\nacross a broad range of semantic vision-language downstream tasks."
                },
                "weaknesses": {
                    "value": "1. The reasons for the improvement brought by Concatenation lack detailed analysis. Why is there also improvement for image-text tasks? Why is it necessary to include the video dataset (web2vid)? Why wasn't the 1.2B model included in the video dataset?\n2. The data shown in Table 1 is confusing. The data for COSA-L is 417M, while the data volume for COSA is 415M.\n3. The results in Table 7 and Table 7 are also confusing. The best performance is based on 6 pretraining task? Which pre-training tasks were used in the overall experimental results of COSA? Is the WebVid2.5M dataset more important\uff0cthe results for COSA 4 frames?"
                },
                "questions": {
                    "value": "see weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5044/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698653519545,
            "cdate": 1698653519545,
            "tmdate": 1699636493723,
            "mdate": 1699636493723,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IzG4Tn9r3U",
                "forum": "bDkisS75zy",
                "replyto": "OUYXw0B4aO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5044/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5044/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer vqzh - Part1"
                    },
                    "comment": {
                        "value": "## **Q1: Why is there also improvement for image-text tasks?**\n## **A1:** \nThrough randomly concatenating image-text or video-text samples, COSA can learn explicit and accurate event-level temporal relations through large-scale pretraining, and this brings improvements on downstream video-language understanding tasks. With regard to the improvements for image-text benchmarks as shown in Table 6 of main paper (COCO retrieval, COCO caption and VIST story telling), we explain from the following perspectives.\n- **Learning difficulty**. Building the relations of image objects and text tokens are the core of image-text pretraining, the stronger relations have been built, the better models can handles cross-modality tasks. During pretraining, SST (single sample training) gives model one image and one sentence once, and model needs to connect objects and textual words. By contrast, COSA give models multiple images and sentences, and the numbers of both objects and words become around four times more (if four samples are concatenated). This makes the relations learning process harder, because for one object, the positive concepts remains the same while negative words becomes more. Similarly, for a meaningful word, there are four times more negative visual objects. The relation building process becomes more difficult, which may benefit in that learned features and relations are more robust and discriminative.\n- **Data augmentation**. During SST training, each image-text pair is seen multiple times (=epochs). By contrast, For COSA, when the concatenation number is equal to 4, with a larger batch size and random sampling, there is negligible possibility that concatenated samples are the same, which means at every iteration, the input cross-modality pairs are different, which could be viewed as a kind of data augmentation to avoid model overfitting.\n\n## **Q2:  Necessity of including the video dataset (web2vid).**\n## **A2:**\n- In abalation study (Table 8), we include the video dataset for two purposes. 1) Proving the generality of COSA method, which could be fitted to both image-text and video-text data. 2) Proving applying COSA method to video dataset works better than SST baseline with sampling more frames, further demonstrate the strength of COSA. \n- In SOTA comparisons (COSA-B and COSA-L), they are trained with video dataset, mainly for **fair comparison**, because most compared methods also train their model on both image and video datasets. \n- In SOTA comparisons (COSA), it is trained without using any video dataset. It is noted that we assume adding video datasets like webvid could further improve COSA's performance, but we want to convey that **even using image-text data only**, we can still outperform other large models using video datasets (videoCoCa, Flamingo) or other models also using image data only (GIT), and that is **aligned with COSA's motivation to relief the scarcity of high-quality video data**, by using concatenated image-text samples."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5044/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700308725310,
                "cdate": 1700308725310,
                "tmdate": 1700711387063,
                "mdate": 1700711387063,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "usb1hDBHUe",
                "forum": "bDkisS75zy",
                "replyto": "OUYXw0B4aO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5044/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5044/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer vqzh - Part2"
                    },
                    "comment": {
                        "value": "## **Q3: Training data volume of COSA-L and COSA is confusing.**\n## **A3:**\nSorry for the confusion caused. For clarity, the details of the data used in training COSA/COSA-L are listed in the table below. The difference in data volume is due to the **exclusion of the Webvid-2.5M dataset in the COSA training**. The reason of this exclusion is explained in our response (A2) to \"Q2: Necessity of including the video dataset (Web2Vid)\". As noted in the caption of Table 1, since the LAION-102M used in COSA is sampled from the training corpus of the EVAClip vision backbone (LAION-400M), we count them collectively as 400M.\n\n| Model | Vision-text pairs utilized in vision-language pretraining | Vision-text pairs utilized in vision encoder pretraining | Total |\n| --- | --- | --- | --- |\n| COSA-L | CC14M (**15M**) + Webvid2.5M(**2.2M**)  | Webdata400M(**400M**) | **417M** |\n| COSA | CC14M (**15M**) + LAION102M(102M, overlapped with LAION400M) | LAION400M(**400M**) | **415M** |\n\n- \"CC14M\" combines the CC3M, CC12M, COCO, VG, and SBU caption datasets, and after the removal of invalid links, totaling 15M  pairs remain accessible.\n\n- \"Webvid2.5M\" initially contains 2.5M vision-text pairs, and after filtering invalid links, we achieve 2.2M pairs for training.\n  \n## **Q4:  The results in Table 7 and Table 8 are also confusing. The best performance is based on 6 pretraining task? Which pre-training tasks were used in the overall experimental results of COSA? Is the WebVid2.5M dataset more important, the results for COSA 4 frames?**\n## **A4:**\n- Yes.  **As written in Table 7 and \"Training Objectives\" in Section 4.3**, the best performance is based on 6 pretraining tasks, and they  are also used in all other COSA experiment tables.\n- **Table 8** targets at demonstrating COSA's generality to different datasets and backbones. We want to **emphasize the comparison between methods (i.e. SST baseline and COSA), instead of datasets (i.e., cc3m and webvid)**. **In addition, we compare webvid and cc3m in the table below, and conduct an additional experiments that use both dataset together**. From the results we can find that separately using one dataset can achieve similar performance, and using them together further improves performance on all tasks.\n\n| Method | Enc  | Dataset | Frame | DiD(R) | ANET(R) | MSR(R) |  YOU(C) | MSR(Q) |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| COSA  | CLIP-B | WebVid2.5M | 1 | 54.7 | 52.2 |  44.0 | 93.4 | 46.4 |\n| COSA  | CLIP-B | CC3M | 1 | 54.8 | 51.2 | 43.5 | 91.9 | 46.5 |\n| COSA  | CLIP-B | WebVid2.5M+CC3M | 1 | 55.8 | 54.1 | 46.2 | 100.5 | 47.0 |"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5044/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700711427227,
                "cdate": 1700711427227,
                "tmdate": 1700715411652,
                "mdate": 1700715411652,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "gXWVrc0GdP",
            "forum": "bDkisS75zy",
            "replyto": "bDkisS75zy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5044/Reviewer_kord"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5044/Reviewer_kord"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed to concatenate image-text samples to mimic video-paragraph corpus in vision-language pre-training. The method is simple and the evaluation is conducted on various image/video datasets to demonstrate impressive performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The idea is simple and easy to reproduce. Meanwhile, the performance gain is impressive.\n2. The experiments are conducted on many benchmarks across image-text and video-text tasks, as well as different data scales. Also the ablation is comprehensive and covers most of the aspects of this method."
                },
                "weaknesses": {
                    "value": "1. It makes sense that pseudo video-paragraph data in pre-training can mitigate the gap between pre-training and fine-tuning in image-text pertaining. However, intuitively, the discontinuity of semantics in pseudo video-paragraph data should hurt compared with relevant video-paragraph data because in downstream videos, image and text are indeed relevant. But in Tab9, it seems random sampling is better than relevant sampling, which is kind of counter-intuitive. Can the authors explain more about it?\n\n2. When having seen the same number of samples, whether COSA is better than SST in `image-text downstream tasks`? Basically, I want to see the comparison like Figure 4 in image-text downstream tasks. I am okay with this observation not holding anymore in image-text downstream tasks because essentially video-paragraph and image-text are different domains.\n\n3. I want to see how this method performs in zero-shot image-text tasks. Considering the domain gap, I suspect it might perform worse than some image-text pre-trained methods that COSA can outperform when finetuning."
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5044/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698819270444,
            "cdate": 1698819270444,
            "tmdate": 1699636493619,
            "mdate": 1699636493619,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YI2zlGH6jA",
                "forum": "bDkisS75zy",
                "replyto": "gXWVrc0GdP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5044/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5044/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer kord - Part1"
                    },
                    "comment": {
                        "value": "## **Q1: Why random sampling is better than relevant sampling?**\n\n## **A1**: \nIn our experiments, COSA trained with random sampling works better than the one trained with relevant sampling. **We make visualizations of different sampling methods and add explanations in the Figure 1 of updated version of supplementary materials**. \n\nSpecifically, as Figure 1 shown in the supplementary material, when we observe a real video-language sample from ActivityNet caption dataset (the top green part of figure), we can find that the 1) sampled vision frames are relevant and coherent, and 2) the corresponding texts are very informative and can strictly match to different frames in temporal order. \n\nMotivated by this, an ideal  large-scale  pretraining dataset for video-language tasks should also possess those two characteristics to help model learn the capabilities of perceiving events happened in videos and their relative order.  In this work, we propose COSA to imitate those pretrain dataset from image-text data. And the examples of COSA with different sampling methods are shown in the bottom pink part of the figure, from which we can find that even though the random sampling example shows little relevance between four independent scenes, but it has the strength that each sentence can strictly correspond to one image, which satisfies the second characteristic. \n\nBy contrast, even though relevant (vision/text similarity) sampling examples have shown vision relevance  to some extent, they have strong semantic redundancy, and one sentence can correspond to many frames, which could confuse models' event-level temporal learning. We believe that's why it performs worse than random sampling. In addition, considering that the scene changes are somewhat abrupt in random sampling, we think smoother sampling methods could be a future research direction.\n\n## **Q2: Comparison of COSA and SST on image-text tasks when seen the same number of samples.**  \n## **A2:**\nIn fact, **Figure 4 already contains two image tasks**, including MSCOCO-RET (text-to-image retrieval) and VIST-CAP (image story telling). We here represent the comparison on those two benchmarks  in table below.\n\n\n| Method    | Iteration | MSCOCO-RET (Recall@1) | VIST-CAP (Cider) |\n|-----------|-----------|-----------------------|------------------|\n| SST-30K   | 30K       | 53.9                  | 14.8             |\n| **COSA-30K**  | 30K       | 54.7                  | 20.7             |\n| SST-60K   | 60K       | 54.8                  | 18.6             |\n| COSA-60K  | 60K       | 55.3                  | 24.1             |\n| **SST-120K**  | 120K      | 54.6                  | 19.1             |\n| COSA-120K | 120K      | 55.8                  | 26.0             |\n\nFrom the table we can find that when comparing COSA and SST under the same examples seen setting (COSA-30K vs SST-120K), COSA-30K achieves 54.7 R@1 and 20.7 CIDEr, which outperforms SST-120K who achieves 54.6 R@1 and 19.1 CIDEr. This demonstrates that COSA is both more effective and efficient than SST (COSA-30K only takes less than half the training time of SST-120K), for both image-text and video-text tasks, showing its generality as a vision-language foundation model. It is noted that when compared under the same iteration setting (both methods are converged), COSA-120K outperforms SST-120K by a significant margin."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5044/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700307945538,
                "cdate": 1700307945538,
                "tmdate": 1700662522337,
                "mdate": 1700662522337,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "f14MarPhzn",
                "forum": "bDkisS75zy",
                "replyto": "gXWVrc0GdP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5044/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5044/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer kord - Part2"
                    },
                    "comment": {
                        "value": "## **Q3: Compare COSA with state-of-the-arts on zero-shot image-text tasks.**  \n## **A3:**  \nConsidering MSCOCO is included in CC4M dataset which has been used  in the pretraining process of COSA, we test COSA on two image-text zero-shot benchmarks including **Flickr-30K image-text retrieval** and **Nocaps image caption**, and compare COSA with other pretraining methods.  \nFor Flickr-30K retrieval, we test in two settings: \n- direct zero-shot evaluation. From the results we can find that COSA outperforms methods including CLIP/ALIGN/FILIP/Florence/CoCa with evident strengths. Considering that compared methods are all two-tower models while COSA use ITM to rerank, we test COSA-ITC that removes the ITM refining process, we can find that COSA-ITC also achieves better text-to-image and decent image-to-text zero-shot performances.\n| Method   | Finetune on MSCOCO | Text-to-Image R@1/R@5/R@10 | Image-to-Text R@1/R@5/R@10 |\n|----------|--------------------|----------------------------|----------------------------|\n| CLIP     | N                  |   68.7 90.6 95.2           | 88.0 98.7 99.4             |\n| ALIGN    | N                  | 75.7/93.8/96.8             | 88.6/98.7/99.7             |\n| FILIP    | N                  |  75.0 93.4 96.3            | 89.8 99.2 99.8             |\n| Florence | N                  | 76.7/93.6/-                | 90.9/99.1/-                |\n| CoCa     | N                  | 80.4/95.7/97.7             | 92.5/99.5/99.9             |\n| COSA-ITC | N                  | 84.4/**97.2**/**98.5**            | 88.0/99.7/99.9             |\n| COSA     | N                  | **87.2**/97.0/97.9             | **96.8**/**100.0**/**100.0**           |\n\n- finetuning on MSCOCO retrieval dataset and then tested on Flickr30K. In this setting, all compared methods use ITM reranking and have an intermediate finetuning process on MSCOCO dataset. COSA surpasses ALBEF/BLIP/mPLUG-2/BLIP-2 with evident margins.\n| Method | Finetune on MSCOCO | Text-to-Image R@1/R@5/R@10 | Image-to-Text R@1/R@5/R@10 |\n| --- | --- | --- | --- |\n| ALBEF | Y | 85.6/97.5/98.9 | 95.9 99.8 100.0 |\n| BLIP  | Y | 86.7/97.3/98.7  | 96.7/100.0/100.0 |\n| mPLUG-2  | Y | 88.1/97.6/99.1 | 97.2 100.0 100.0 |\n| BLIP-2 | Y | 89.7/98.1/98.9 | 97.6/100.0/100.0 |\n| COSA (Ours) | Y | **90.2**/**98.4**/**99.4**  | **98.3**/**100.0**/**100.0**  |\n\nFor the Nocaps zero-shot caption dataset, it is noted that there are models with LLM as a multimodal decoder achieving higher performance, such as BLIP-2. Considering that COSA utilizes BERT-base as its multimodal decoder, we only compare it with similar models. Specifically, we find that COSA outperforms OSCAR and VinVL by large margins and achieves comparable performance with SimVLM and BLIP.\n|  | Use LLM | Cider | Spice |\n| --- | --- | --- | --- |\n| BLIP-2 | Y | 119.7  | 15.4 |\n| OSCAR | N | 80.9  | 11.3  |\n| VinVL | N | 95.5  | 13.5 |\n| BLIP | N |  **113.2**  | 14.8 |\n| SimVLM | N |  112.2 | - |\n| COSA(Ours) | N | 113.0 | **15.0** |"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5044/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700308124193,
                "cdate": 1700308124193,
                "tmdate": 1700668104879,
                "mdate": 1700668104879,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "F3ZMCzedg5",
                "forum": "bDkisS75zy",
                "replyto": "gXWVrc0GdP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5044/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5044/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer kord - Part3"
                    },
                    "comment": {
                        "value": "## Reference \n-  Lei et al. Less is more: Clipbert for video-and-language learning via sparse sampling\n- Radford et al. Learning transferable visual models from natural language supervision.\n- Jia et al.  Scaling up visual and vision-language representation learning with noisy text supervision.\n- Yao et al. FILIP: fine-grained interactive language-image pre-training.\n- Yuan et al. Florence: A new foundation model for computer vision.\n- Yu et al. Coca: Contrastive captioners are image-text foundation models.\n- Li et al. Align before fuse: Vision and language representation learning with momentum distillation.\n- Li et al.  BLIP: bootstrapping language-image pre-training for unified vision language understanding and generation.\n- Xu et al. mplug-2: A modularized multi-modal foundation model across text, image and video.\n- Li et al. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models\n- Li et al. Oscar: Object-semantics aligned pre-training for vision-language tasks\n- Zhang et al. Vinvl: Making visual representations matter in vision-language models\n- Wang et al. Simvlm: Simple visual language model pretraining with weak supervision."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5044/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700320377844,
                "cdate": 1700320377844,
                "tmdate": 1700320377844,
                "mdate": 1700320377844,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Gqg5Xl5Tb3",
                "forum": "bDkisS75zy",
                "replyto": "f14MarPhzn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5044/Reviewer_kord"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5044/Reviewer_kord"
                ],
                "content": {
                    "comment": {
                        "value": "Thank the author for the response. My first concern is solved.\n\nFor the second concern, it's weird that in MSCOCO-RET, SST-120K's performance is worse than SST-60K. In the table, SST-60k already outperforms COSA-30K in MSCOCO-RET, why would SST-120K turn out to underperform COSA-30K? Explanation from the author(s) is needed here. \n\n As for my third concern, I think it's better to be validated by a fair comparison such as SST-120k vs COSA-30K with the same number of seen data conducting zero-shot tasks. This is to remove the difference brought by training datasets/recipes in different works. I still think that there should be a domain shift between pre-training on random concatenated image-text and the single image-text scenario in downstream tasks if directly applying zero-shot inference. I am okay to see the negative results. This is not a weakness of this paper either."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5044/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700723684015,
                "cdate": 1700723684015,
                "tmdate": 1700723684015,
                "mdate": 1700723684015,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]