[
    {
        "title": "Universal Metric Learning with Parameter-Efficient Transfer Learning"
    },
    {
        "review": {
            "id": "fIDpUjGBkK",
            "forum": "Jr3X3BTwsj",
            "replyto": "Jr3X3BTwsj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1349/Reviewer_f9dZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1349/Reviewer_f9dZ"
            ],
            "content": {
                "summary": {
                    "value": "Prior metric learning models were typically trained independently on individual i.i.d. datasets. This paper introduces the concept of Universal Metric Learning (UML), which involves the joint training of models using multiple distinct i.i.d. datasets. UML poses two primary challenges: data distribution imbalance and model bias towards dominant distributions. To tackle these challenges, the authors present two novel methods: the Stochastic Adapter method and Conditional Prompt Learning. The experimental results on benchmark datasets demonstrate the exceptional performance of these proposed techniques."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper is the first to tackle the Universal Metric Learning (UML) problem, introducing challenges related to imbalanced data distribution and bias towards dominant distributions.\n\n2. The study includes an extensive series of experiments."
                },
                "weaknesses": {
                    "value": "It remains unclear how the stochastic adapter successfully mitigates bias in the embedding space toward the major data distribution. The utilization of generalizable features from a pretrained model is noteworthy. Notably, the ViT-S model used for pretraining is pretrained on ImageNet-21K, which already encompasses all the categories within the experimental datasets. Therefore, the performance boost might be primarily attributed to the pretrained model itself rather than the novel method proposed in this paper. Additionally, it's important to consider the scenario of $\\textbf{training the model from scratch}$ and how the bias towards the dominant distribution of data can be effectively avoided. This aspect needs further elaboration and exploration in the paper."
                },
                "questions": {
                    "value": "The proposed method lacks an in-depth analysis, and there is a concern regarding potential data leakage in pre-trained models."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1349/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1349/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1349/Reviewer_f9dZ"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1349/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698304479121,
            "cdate": 1698304479121,
            "tmdate": 1699636062121,
            "mdate": 1699636062121,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "nUkTTiAbfm",
            "forum": "Jr3X3BTwsj",
            "replyto": "Jr3X3BTwsj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1349/Reviewer_xsqK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1349/Reviewer_xsqK"
            ],
            "content": {
                "summary": {
                    "value": "This paper explores parameter-efficient transfer learning in deep metric learning problems under a condition where the model was primely trained on a large dataset that was combined with different image retrieval datasets.\n\nThe authors explore both parameter adaptation and prompt-related approaches and propose a framework to combine these techniques to improve the capability of transfer learning."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "Techniques of parameter-efficient fine-tuning (PEFT) are popular in recent research literature. It is reasonable to explore a way to optimize the PEFT method for image retrieval tasks.\n\nThe authors did extensive experiments on many different datasets to demonstrate their proposed framework. They show some improvement in model generalization, comparing the full fine-tuning baseline and other methods."
                },
                "weaknesses": {
                    "value": "1. Unsolid motivation: the concept \"universal\" is not really universal\n\nI believe the major problem with this paper is the motivation and proposed challenge in DML tasks. The authors promote the concept of \"universal\" DML model by simply fine-tuning it on a combined dataset. However, I believe what the community expects from PEFT research is some efficient transfer learning approach directly from the pre-trained large-scale model, like CLIP, which was trained in an unsupervised way on general large-scale datasets. Thus, we can develop methods for fine-tuning this general model for some specific downstream tasks, like image retrieval. However, the authors' proposed paradigm evaluates its transfer learning capability on a combined DML dataset, which I believe is not meaningful. In other words, if we have a pre-trained model and we want to fine-tune it on a small image retrieval task, like Cars, why do we need to care about its transfer ability on a united dataset that combines many small image retrieval datasets?\n\n\n2. The contribution is not novel.\n\nThe author proposes a framework that combines two popular PEFT methods (the Adapter and the Prompts), which have been widely explored in recent research works.\n\nMore importantly, I didn't see any contributions specifically designed for DML tasks. That means the proposed framework can also be applied to other tasks, like general image classification. Why it benefits the performance of DML tasks is still unclear. The performance of the proposed framework on other computer vision tasks may also need to be discussed. In addition, the technique improvements on the Adapter (add the random mask) and the Prompts (add the conditional attention) are incremental and cannot be regarded as novel.\n\nBased on the unsolid motivation and incremental technique contribution, I believe this paper is under the board line in its current state."
                },
                "questions": {
                    "value": "1. Why do we need to care about a united dataset that is combined with different image retrieval datasets, knowing that we already have pre-trained models from much larger and more \"universal\" datasets, like CLIP?\n\n2. In conditional prompt learning, it seems both the key (K) and the value (prompt itself) of the attention network need to be learned from the task. How does the learning object ensure learning distinct representations for each member of the prompts in the pool?\n\n3. Why would the query feature grasp the data distribution from the patch of a single image? Why don't pool the patches of all images in the datasets instead?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1349/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1349/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1349/Reviewer_xsqK"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1349/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698763792488,
            "cdate": 1698763792488,
            "tmdate": 1699636062031,
            "mdate": 1699636062031,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "BVUrRONtmw",
            "forum": "Jr3X3BTwsj",
            "replyto": "Jr3X3BTwsj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1349/Reviewer_W7ZY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1349/Reviewer_W7ZY"
            ],
            "content": {
                "summary": {
                    "value": "This paper tries to introduce and solve a new metric learning paradigm, called Universal Metric Learning (UML).   \nIn a real-world scenario, UML consists of multiple different data distributions and might suffer from imbalanced and dominant distribution problems.   \n\nTherefore, this paper proposed the Parameter-efficient Universal Metric leArning (PUMA) method to deal with the problem. Specifically, stochastic adapters and a prompt pool are designed on a pre-trained Vision Transformer. \n\nMoreover, a new universal metric learning benchmark is compiled with 8 different datasets.   \nIn the experiments, PUMA outperforms several baseline methods and requires 69 times fewer trainable parameters."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "### (1) Reasonable setting\nThis paper tries to solve the metric learning problem in a realistic setting, which involves diverse data distributions. \n\n### (2) Technical contribution\n Built upon a pre-trained Vision Transformer (ViT), this paper designed two modules for universal metric learning: stochastic adapters and prompt pool. These modules are lightweight and effective in dealing with the universal metric learning problem. \n\n### (3) Experiment \nA new benchmark is constructed, and the experimental results show that PUMA performed better than data-specific and conventional methods."
                },
                "weaknesses": {
                    "value": "### (1) Lack of related work and discussion \nUniversal metric learning considers multiple data distributions (datasets) in a real-world scenario, which is quite related to cross-domain/multi-domain feature learning methods. In particular, the motivation in Figure 1 is similar to existing methods, such as [R1-R5]. \nAnd the idea of utilizing a pre-trained feature extractor has also been explored [R4, R5]. \n\n### (2) Technical contribution\nThis paper designed two additional modules on a pre-trained ViT: stochastic adapter and prompt pool. The stochastic adapter mainly adopted some ideas from low-rank methods such as LoRA. And prompt pool has been explored in VPT methods. The utilization of the two modules generally makes sense in the universal metric learning problem.   \nMy concerns are:\n>2.1 From the ablation study in Table 3, it seems most improvements come from stochastic adapters. The prompt pool seems to only marginally increase the performance.   \n>2.2 Is there an ablation study about M?   \n\n\n\n\n#### References\n[R1] Triantafillou E, Zhu T, Dumoulin V, Lamblin P, Evci U, Xu K, Goroshin R, Gelada C, Swersky K, Manzagol PA, Larochelle H. Meta-dataset: A dataset of datasets for learning to learn from few examples. arXiv preprint arXiv:1903.03096. 2019 Mar 7.\n\n[R2] Hung-Yu Tseng, Hsin-Ying Lee, Jia-Bin Huang, and MingHsuan Yang. Cross-domain few-shot classification via\nlearned feature-wise transformation. In International Conference on Learning Representations, 2019.\n\n[R3] Lu Liu, William Hamilton, Guodong Long, Jing Jiang, and Hugo Larochelle. A universal representation transformer\nlayer for few-shot image classification. arXiv preprint arXiv:2006.11702, 2020.\n\n[R4] Triantafillou, Eleni, Hugo Larochelle, Richard Zemel, and Vincent Dumoulin. \"Learning a universal template for few-shot dataset generalization.\" In International Conference on Machine Learning, pp. 10424-10433. PMLR, 2021.\n\n[R5] Liu Y, Lee J, Zhu L, Chen L, Shi H, Yang Y. A multi-mode modulator for multi-domain few-shot classification. InProceedings of the IEEE/CVF International Conference on Computer Vision 2021 (pp. 8453-8462)."
                },
                "questions": {
                    "value": "Please see above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1349/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699015917353,
            "cdate": 1699015917353,
            "tmdate": 1699636061926,
            "mdate": 1699636061926,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]