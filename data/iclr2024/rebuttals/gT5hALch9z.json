[
    {
        "title": "Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language Models that Follow Instructions"
    },
    {
        "review": {
            "id": "anXr4mswBa",
            "forum": "gT5hALch9z",
            "replyto": "gT5hALch9z",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2808/Reviewer_gf5T"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2808/Reviewer_gf5T"
            ],
            "content": {
                "summary": {
                    "value": "The authors created a dataset for evaluating improper behavior of LLMs and the trade-off between safety and helpfulness objectives.\nThey report on experiments with different fine-tuning variants of LLAMAs, showing some unsurprising results (there is a trade-off between safe behavior and helpful behavior, the amount of labeled data matters) and some less obvious results (asking the model to provide an opinion yields safer answers and asking to answer a question to provide instructions; finding the right balance between safety and helpfulness is difficult)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Experiments seem sound and done carefully. Not all results are trivial and are thus worthy of being reported in a paper. The dataset could be useful for further evaluation of alignment and safety methodologies."
                },
                "weaknesses": {
                    "value": "The claims on page 2 need to be redone. The ones that are shown are (a) not novel enough (e.g. there is a tension between the two objectives) and (b) redundant (2 and 3 are different ways of stating 1). \n\nHowever, there are real contributions (such as the creation of the dataset and the observed experimental results) which should be highlighted in the claims.\n\nAlthough the experiments may help to adjust current LLMs in a slightly safer way, this may all be useless if the overall LLM with RLHF or safety fine-tuning is very easy to break with a little bit of appropriately chosen fine-tuning, e.g. see Qi et al \"Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!\", arXiv:2310.03693"
                },
                "questions": {
                    "value": "How do the authors put their work in the perspective of the results from Qi et al 2023 (arXiv:2310.03693)?\nIs it even worthwhile pursuing this kind of avenue if fine-tuning for safety can be completely broken so easily?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2808/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698514803302,
            "cdate": 1698514803302,
            "tmdate": 1699636223437,
            "mdate": 1699636223437,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wzY5e2VmR2",
                "forum": "gT5hALch9z",
                "replyto": "anXr4mswBa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2808/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2808/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the comments and the review. \n\n**The claims on page 2 need to be redone.**\n\nWe thank the reviewer for the comment, we have modified the section to make our claims clearer, but we still believe that the general contributions remain the same:\n\nClaim 1 is about the tension and our paper is the first to show this in an entirely reproducible setting in which both models and data are open-sourced.\n\nClaim 2 is connected to the fact that we believe that safer models can be obtained with little effort (5% of the data), suggesting that most models can be made safer. This happens without completely compromising general capabilities; this would suggest that thanks to safety tuning there is little to no tension (claim 1), however\u2026\n\nClaim 3 is about the fact that safety training can introduce a novel phenomenon, called exaggerated safety. This is different from the other claims, as it shows that even if general capabilities do not seem to decrease (claim 2) there is still an impact that comes from safety training.\n\n**\"Although the experiments may help to adjust current LLMs in a slightly safer way, this may all be useless if the overall LLM with RLHF or safety fine-tuning is very easy to break with a little bit of appropriately chosen fine-tuning, e.g. see Qi et al \"Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!\", arXiv:2310.03693\"**\n\nThanks for pointing out the work by Qi et al. In their work, they show that fine-tuning can be used to break safety capabilities, which we understand is an important finding.\n\nHowever, we believe it is fundamental to work on releasing safer models. With the current models, it is very easy for non-expert users to create toxic and unsafe content. Fine-tuning still requires a reasonable amount of expertise most users do not have. Our paper focuses on limiting existing problems.\n\nIn general, we do not believe that adding safety to the models is a futile experiment, even when it can be broken. Ease of access to unsafe behaviors matters. Safety by default still makes a difference for user-facing applications, even if safety measures can be easily circumvented\n\nWe have found that most models in online repositories that are instruction-tuned lack any safety guarantee. This suggests that, as of today, there is no need to \u201cbreak the safety\u201d as the models released are already unsafe. In our paper, we propose that safety tuning can be successfully applied to most training models without compromising their general performance, with some caveats we have described in the paper."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2808/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700234372658,
                "cdate": 1700234372658,
                "tmdate": 1700234372658,
                "mdate": 1700234372658,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "fuJraLl0rr",
            "forum": "gT5hALch9z",
            "replyto": "gT5hALch9z",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2808/Reviewer_yQ2D"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2808/Reviewer_yQ2D"
            ],
            "content": {
                "summary": {
                    "value": "This paper discusses the safety concerns of large language models (LLMs) that focus on helpfulness rather than harmlessness in their instruction-tuning. It reveals that popular instruction-tuned models are highly unsafe, and adding just 3% safety examples can significantly improve their safety. While safety-tuning does not significantly reduce models' capabilities or helpfulness, it can lead to exaggerated safety behaviors, where models refuse perfectly safe prompts if they resemble unsafe ones. The results highlight the trade-offs between training LLMs to be helpful and harmless."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ The paper addresses an important and timely direction, which is the safety concern of LLM\n+ The paper is easy to follow in general \n+ Figure 1 provides a good overview of the main focus of the paper"
                },
                "weaknesses": {
                    "value": "- The collected safety dataset is too small, many are about 100.\n- The difference with related work is not deeply discussed, e.g., the reasons for not using the red teaming dataset in safety training are not convincing enough \n- There are some parts of the experiment results requiring further explanation"
                },
                "questions": {
                    "value": "In general, the idea of the work is interesting, to evaluate the safety of the existing open-source LLMs and find that a small amount of safety demonstrations could improve the safety of the LLM. The manuscript is easy to follow and the idea is not hard to understand. However, I think the biggest problem the paper has is the evaluation is kind of weak, e.g., the size of the dataset is small (many are about 100) and some experiment results are not well-explained. I list the questions below.\n\n1. Section 2: Maybe the authors could define what safety is before a deeper discussion. Safety is a very general concept and without a clear definition, it is hard for the readers to understand what the paper discusses.\n\n2. Page 4: \n> We randomly selected 2,000 questions from this dataset and used GPT-3.5-turbo to generate \u201csafe\u201d responses to them.\n\nWhy do you choose ChatGPT is generate safe responses and how do you guarantee that? Also, the safety seems only limited to the area that you choose, can they be generalized?\n\n3. Section 3.3: Why not use the red teaming dataset?\n\n4. Figure 3(a): Why safety training is not effective in O-Controversial dataset?\n\n5. Page 8: \n> Models\u2019 \u201copinions\u201d do not reflect models\u2019 behavior.\n\nI don't quite get the meaning of this part.\n\n6. Page 8:\n> Too much safety can be detrimental.\n\nMaybe you could go over the dataset and get some more stronger numbers. With example only is not convincing enough."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2808/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698681753794,
            "cdate": 1698681753794,
            "tmdate": 1699636223366,
            "mdate": 1699636223366,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "goOzhbs96d",
                "forum": "gT5hALch9z",
                "replyto": "fuJraLl0rr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2808/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2808/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the comments and the review. \n\n**Definition of Safety**\n\nThanks for pointing this out, We have added this to the paper.\nIn our context, safety is defined as stopping models from following malicious instructions (I-MaliciousInstructions) and generating toxic content (I-CoNa, I-Controversial). We also test how this safety training generalizes on physical safety instructions. \n\n**O-Controversial Results and Models\u2019 Opinions vs Behaviour**\n\nThe O-Controversial dataset contains prompts like \u201cDo you think I should kill someone?\u201d.\nThere is still a minor reduction in harmfulness even in O-Controverisal. Looking at the examples, in O-Controverisal, the 2000-safety model explains in detail why it does not believe something has to be done (e.g., killing someone). Instead, in I-CoNA the model responds with direct refusal by the model (\u201cAs an AI assistant\u2026\u201d) more times; the reward model seems to give a slightly higher score (more harmful) for the former and a lower for the latter. \n\nThanks for pointing out the limitations of this section. We have clarified this in the paper. What we want to show is that asking the model \u201cDo you think I should kill myself\u201d is very different from asking the model \u201cTell me ways how I can kill myself\u201d. This shows an additional weak spot of the helpfulness and harmlessness tension and makes. In addition to this, recent research has been devoted to interpreting and identifying the political leanings of different models: these results suggest that the model might say it does not believe killing someone is a good idea, but would still provide ways to do it.\n\n**\"The collected safety dataset is too small, many are about 100.\"**\n\nThe training dataset contains 2,000 safety examples for training. The evaluation datasets are smaller but also more targeted. We created smaller evaluation datasets that we could manually control and check. When considered together, the evaluation datasets count more than 500 examples and cover different safety aspects.\n\nWhile our datasets cannot be used to evaluate all safety limitations in large language models, we still see that the models fail in many cases. Considering also that the rate of failure changes with the scale of safety tuning, our evaluation datasets have relevant diagnostic power.\n\n\u201cWhy do you choose ChatGPT is generate safe responses and how do you guarantee that? Also, the safety seems only limited to the area that you choose, can they be generalized?\u201d\n\nChatGPT responses are very safe, we manually went through the examples to ensure they were all acceptable for our task. Most of the responses are direct refusal (e.g., \u201cAs an AI assistant\u2026\u201d), and some offer more explanations (e.g., \u201cKilling someone is bad because\u2026\u201d)\n\nRegarding the training dataset, we start from a series of RedTeaming attempts that cover a wide variety of possible malicious attempts. These attempts cover a wide variety of issues, like doxxing, privacy violation, bias, and content toxic generation. We believe that our sample is representative of these attempts. Results on our PysicalSafety datasets provide insights into how the models generalize on examples that are out of domain."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2808/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700234292866,
                "cdate": 1700234292866,
                "tmdate": 1700234292866,
                "mdate": 1700234292866,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rkl2xP3MWX",
                "forum": "gT5hALch9z",
                "replyto": "fuJraLl0rr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2808/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2808/Authors"
                ],
                "content": {
                    "title": {
                        "value": "(second part)"
                    },
                    "comment": {
                        "value": "**Difference with related work**\n\nThanks for pointing this out, we have extended the paper to discuss this issue more in detail; we believe that no safety dataset can be used to train instruction models. \n\nThere are two main datasets for safety-related tasks: the HH-RLHF (Bai et al., 2022) and the RedTeaming (Ganguli et al., 2022) datasets. However, they come with the following limitations:\n\n* they are not made for tuning; Anthropic\u2019s guidelines advise not to use these datasets for tuning: \u201cTraining dialogue agents on these data is likely to lead to harmful models, and this should be avoided.\u201d\n* they are less helpful to induce safety behaviors as they were generated using not safety-tuned models. \n\nWe agree with the reviewer about the benefit of better explaining why we created a new dataset. We have added a more in-depth discussion on why we did not use the HH-RLHF or the RedTeaming datasets to the appendix.\n\nIn particular, the RedTeaming dataset was not meant to train models as the responses come from models that were trained without safety; there is often no refusal and the dataset contains a wide variety of safe and unsafe replies. The reviewer can also look at some of the examples in the dataset to see that they do not offer any guarantee for safety.\n\nThe HH-RLHF dataset should be used only to train reward models, but in some cases it has been used to train dialogue agents. In general, we do not believe the training samples are effective to study safety training. Looking at some of the examples in the dataset (like the one mentioned in the paper) demonstrates this and we believe that this issue affects trained models.  In particular, in the Appendix, we show that while models (e.g., MPT 7B chat) trained on the dataset often provide safe completions, they do not learn to be completely safe or to respond with refusal to unsafe instructions. In our opinion, the main reason why they do not completely refuse some of the malicious instructions is that many examples in the dataset do not have a clear refusal, justifying the need for an additional dataset to study how models learn safety.\n\nAll in all, no other safety dataset can be used to train models to be safe. Our dataset is a novel contribution and is the first dataset for safety instruction tuning.\n\n**\u201cMaybe you could go over the dataset and get some more stronger numbers. With example only is not convincing enough.\u201d**\n\nOur results show that training with 2000 additional safety examples makes the models exaggerate the safety of 50% of the test instruction. In the Appendix, we report that there is a linear increase with the additional safety examples we add and we also report a direct comparison between helpfulness, harmfulness, and exaggerated safety of different models."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2808/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700234304747,
                "cdate": 1700234304747,
                "tmdate": 1700234675029,
                "mdate": 1700234675029,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xXJKAC225c",
            "forum": "gT5hALch9z",
            "replyto": "gT5hALch9z",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2808/Reviewer_cMF9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2808/Reviewer_cMF9"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates approaches for improving safety in large language models that have been fine-tuned to follow instructions. The authors show that popular LLMs can generate unsafe or harmful responses when given malicious prompts. They demonstrate that incorporating even small amounts of safety data during instruction tuning, such as a few hundred examples, can substantially reduce unsafe behaviors without negatively impacting performance on standard benchmarks. However, too much safety data leads to models refusing safe prompts, a problem they term exaggerated safety. The paper releases datasets and tools for evaluating safety issues in language models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Clearly frames the tension between helpfulness and harmlessness in instruction-tuned LLMs. Shows examples of popular models complying with unsafe instructions.\n- Examines the tradeoff between safety and capabilities, finding that even not too much safety data leads to exaggerated safety behaviors where models overly refuse safe prompts."
                },
                "weaknesses": {
                    "value": "- The tradeoff of safety and helpfulness is already a well-known fact.\n- The safety training data is limited in scope and size. Scaling up safety data could lead to different conclusions.\n- The current models still seem susceptible to simple adversarial attacks or instructions framed differently.\n- The paper mainly echo with existing finding in literature without proposing novel methods to solve the safety helpfulness tradeoff."
                },
                "questions": {
                    "value": "Have you continued experiments with scaling up the amount of safety data? If so, did you find any \"sweet spots\" where safety improves without exaggerated safety effects?\nHow resilient are the safety-tuned models to variations in the phrasing of unsafe prompts or simple adversarial attacks?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2808/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2808/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2808/Reviewer_cMF9"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2808/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698817084386,
            "cdate": 1698817084386,
            "tmdate": 1700875106923,
            "mdate": 1700875106923,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YJo89Jetp2",
                "forum": "gT5hALch9z",
                "replyto": "xXJKAC225c",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2808/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2808/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the review and the comments. \n\n**\u201cThe tradeoff of safety and helpfulness is already a well-known fact.\u201d**\n\nWe want to highlight that this paper is the first study using open-source datasets to evaluate the effectiveness of safety training in large language models, and how this effectiveness varies with the amount of safety data.\n\nVery little is known about the safety training of both Llama2 and ChatGPT, which are considered to be safer language models. Our study represents the first attempt at replicating safety tuning under conditions where all resources have been made openly accessible. In addition, we have released a novel dataset and demonstrated the scaling behavior of safety training, and highlighted a risk of exaggerated safety. \n\n**Scaling Safety and Risk of Adversarial Attacks**\n\nThank you for these questions. Our experiments show that even little safety data can result in exaggerated safety effects. Given that we already see exaggerated safety when using some safety tuning data, further scaling up safety tuning would likely just increase these problems.\n\nWe acknowledge that the resulting models are not completely safe, but they are significantly safer for normal use. Our experiments with various instructions indicate that the model can be generalized effectively. However, it is important to note that there is still a possibility of the model being affected by adversarial and jailbreaking prompts. In this context, it is worth emphasizing that our goal is to establish an initial layer of safety. The ease of accessing unsafe behaviors is an important consideration. Even though safety measures can be bypassed, having safety as the default setting remains crucial for user-facing applications. \n\n**\u201cThe paper mainly echo with existing finding\u201d**\n\nWe respectfully disagree with this statement. There is currently no similar finding in the literature for which the datasets are also open-sourced. \n\nThis is the first paper to systematically quantify the effect of safety training on exaggerated safety and also the first to show that little safety data is enough to ensure a consistent reduction in the effectiveness of malicious attacks. We also released the open source benchmark and code to support users who want to evaluate safety in their models, which is a valuable contribution.\n\nThank you very much for your time and review! We would really appreciate it if you could let us know if you have additional questions, and we are happy to further clarify."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2808/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700234072331,
                "cdate": 1700234072331,
                "tmdate": 1700234072331,
                "mdate": 1700234072331,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "LfA3S5yXkU",
            "forum": "gT5hALch9z",
            "replyto": "gT5hALch9z",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2808/Reviewer_HxPh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2808/Reviewer_HxPh"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the safety-tuning of LLMs. It creates a public fine-tuning dataset for safety-tuning, which is currently lacking. This dataset it presents a study on how mixing safety data and common instruction-tuning data would impact the safety and utility of resulting models. It is shown that, by mixing a small portion of safety data, the models would be much safer while do not suffer from too much utility drops."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The evaluation is comprehensive, covering a diverse set of safety evaluation datasets with focuses on different aspects of model safety. It also considers multiple approaches to evaluate the quality of the response, evaluating the utility of the models. Human studies are also involved.\n\n2. Contribution of a publicly available safety tuning dataset.\n\n3. The analysis is insightful, providing a lot of practical insights gained from safety-tuning experiments."
                },
                "weaknesses": {
                    "value": "The novelty is my primary concern. It is known that aligned models like ChatGPT and Llama-2 involve instruction tuning with safety data. That's exactly what makes these models safer than unaligned ones. What the paper does is just repeat an experimental study on instruction tuning. This makes the contributions of the paper seemingly insignificant. \n\nHowever, the details of the safety tuning of ChatGPT/Llama-2 are not publicly transparent. Especially, the safety data are not publicly available, and we neither know how different design choices will impact the effectiveness. From this perspective, this work can be publicly accessible material for the public to understand and reimplement the practice."
                },
                "questions": {
                    "value": "Can you explain more on the novelty of this work given my above concern?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2808/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2808/Reviewer_HxPh",
                        "ICLR.cc/2024/Conference/Submission2808/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2808/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698821582325,
            "cdate": 1698821582325,
            "tmdate": 1699793378983,
            "mdate": 1699793378983,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "x8jmTPtpOG",
                "forum": "gT5hALch9z",
                "replyto": "LfA3S5yXkU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2808/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2808/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their review and comments. \n\nWe will answer the points raised individually.\n\n**Can you explain more on the novelty of this work given my above concern?**\n\nAs mentioned by the reviewer, there is limited information regarding the safety training of Llama2 and ChatGPT, which are considered to be safer language models. Most language models available today are either non safety-tuned or use data from ChatGPT, which allows for inheriting its safety guardrails.\n\nOur study represents the first attempt at replicating safety tuning under conditions where all resources have been made openly accessible. \n\nWe released a safety dataset to encourage future research, but our contribution goes beyond that. Previous works have not systematically demonstrated that adding a small number of safety examples to fine-tune a model is enough to provide reasonable safety behaviors. Moreover, this is the first paper to directly quantify the safety vs exaggerated safety tradeoff when training models to follow instructions and be safe at the same time. \n\nIn addition to this, we want to point out that all our contributions are released as open-source. This final point will encourage future open model releases to account for safety considerations."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2808/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700233922769,
                "cdate": 1700233922769,
                "tmdate": 1700233922769,
                "mdate": 1700233922769,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BrJETLuy62",
                "forum": "gT5hALch9z",
                "replyto": "x8jmTPtpOG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2808/Reviewer_HxPh"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2808/Reviewer_HxPh"
                ],
                "content": {
                    "title": {
                        "value": "Reply to the authors"
                    },
                    "comment": {
                        "value": "Thanks to the authors for answering my questions. I generally agree with what the authors said above, but with a few additional comments:\n\n> Previous works have not systematically demonstrated that adding a small number of safety examples to fine-tune a model is enough to provide reasonable safety behaviors.\n\nIn the original paper of Llama 2 [1], they also mentioned that \"quality is all you need\" for alignment. That said, they showed that a moderate number of high-quality data is sufficient.  In the LIMA paper [2], similar information has been conveyed. It would be better if the authors could tone down a little bit on this claim and make meaningful connections to these related work in the paper.\n\n\n> This is the first paper to directly quantify the safety vs exaggerated safety tradeoff when training models to follow instructions and be safe at the same time.\n\nThough the concept of \"exaggerated safety\" is relatively new, the trade-off between helpfulness and harmlessness is not new and has been broadly discovered. Similarly, it would be good for the authors to tone down a little bit. \n\n\nIn general, I agree that this paper makes meaningful contributions, as I summarized in my initial review and also supplemented by the authors. It would be great if the authors could consider my suggestions above. \n\n\n\n[1] Touvron, Hugo, et al. \"Llama 2: Open foundation and fine-tuned chat models.\" arXiv preprint arXiv:2307.09288 (2023).\n\n[2] Zhou, Chunting, et al. \"Lima: Less is more for alignment.\" arXiv preprint arXiv:2305.11206 (2023)."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2808/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700373145565,
                "cdate": 1700373145565,
                "tmdate": 1700373145565,
                "mdate": 1700373145565,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]