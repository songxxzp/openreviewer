[
    {
        "title": "Primal-Dual Continual Learning: Stability and Plasticity through Lagrange Multipliers"
    },
    {
        "review": {
            "id": "Fru2jc5ts1",
            "forum": "GicZtgSlJW",
            "replyto": "GicZtgSlJW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6337/Reviewer_aK2g"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6337/Reviewer_aK2g"
            ],
            "content": {
                "summary": {
                    "value": "This paper formulates the no-forgetting objective of Continual-Learning (CL) as a constrained optimization problem w.r.t the population risks. Given the forgetting tolerance $\\epsilon_{1:T}$, it focuses on two important aspects of the memory-based methods: 1. how to partition the memory buffer for different tasks. 2. For each task, which subsamples should be stored? The first point is addressed by deciding the sample size of each task through minimizing the generalization gap weighted by the optimal dual variables of the CL objective. The second is to select the samples with the highest associated per-sample dual variable from each task."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is well-written and the motivation is clear.\n2. Relating the generalization gap with the dual variables to obtain the optimal memory partition in CL is novel to me. \n3. Experimental results validate the effectiveness of the proposed method compared to previous memory-based approaches."
                },
                "weaknesses": {
                    "value": "My primary concerns lie in the following aspects:\n  * The convergence of $\\mathbf{\\lambda}$ is highly sensitive to the setting of the forgetting tolerance $\\epsilon$, the number of tasks $T$, and the hardness of the tasks, which will affect the memory partition.\n  *  At every timestep, the memory partition changes. Not just the problem mentioned in the discussion exists, where the optimal partition size of a previous task grows at the current timestep. For the tasks that have a smaller size at the current timestep, it needs to reselect the samples to store, which would cause additional computation costs.  \n  * The growing and large number of constraints."
                },
                "questions": {
                    "value": "Please see the previous section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6337/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6337/Reviewer_aK2g",
                        "ICLR.cc/2024/Conference/Submission6337/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6337/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698008383791,
            "cdate": 1698008383791,
            "tmdate": 1700540846248,
            "mdate": 1700540846248,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "H56P8o6brn",
                "forum": "GicZtgSlJW",
                "replyto": "Fru2jc5ts1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6337/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6337/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer aK2g"
                    },
                    "comment": {
                        "value": "We sincerely thank the reviewer for their time and feedback, and for their positive evaluation of our work. \n\n- Sensitivity of $\\lambda$\n\nWe believe that the sensitivity of $\\lambda$ with respect to the forgetting tolerance $\\epsilon$ and the hardness of the tasks is indeed a *positive* aspect of our approach. As mentioned in the general response, the regularization parameter associated to each rehearsal loss is meant to be adaptive. For instance, if the forgetting tolerance $\\epsilon_k$ associated to task $k$ is relaxed, the effective difficulty of this task is reduced, decreasing its associated dual variable and leading to a smaller buffer partition. This adaptivity is the insight leveraged to partition the buffer and select samples.\n\n- Re-computation of buffer partition.\n\nIndeed, the buffer partition is computed at every iteration, which creates an additional cost with respect to fixed partitions. However, the computational overhead of running the Sequential Quadratic Programming algorithm is small (see Appendix A.8). More importantly, as mentioned in the previous response, the flexibility of recomputing the buffer partition after every iteration allows us to dynamically assess the relative difficulty of the observed tasks, which is a key positive aspect of the proposed approach.\n\nAs mentioned in Section 7, there are two non-efficient ways of addressing the problem of dual variable underestimation. Namely, one can fill the empty portion of the buffer with either augmented samples from the previously selected ones or samples from the current task, whose dataset is entirely available.\n\n- Number of constraints\n\nThe number of constraints in our formulation of continual learning can be equal to the number of tasks or the number of samples. In the sample-level setting, our experiments show that one can effectively train constrained continual learners with up to 100000 constraints (Tiny-ImageNet). In view of the challenges and novel techniques deployed in this paper, we leave the experimental validation on larger datasets (e.g., ImageNet) as future work."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6337/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700494234090,
                "cdate": 1700494234090,
                "tmdate": 1700505572929,
                "mdate": 1700505572929,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Li8KKjaJcq",
                "forum": "GicZtgSlJW",
                "replyto": "H56P8o6brn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6337/Reviewer_aK2g"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6337/Reviewer_aK2g"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your clarification. \n\nMy concerns are partly resolved. Overall, even though it's not as perfect as I expected, I believe it's an interesting work that should be seen by the community. \n\nI raised my score accordingly."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6337/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700540832657,
                "cdate": 1700540832657,
                "tmdate": 1700540832657,
                "mdate": 1700540832657,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ky8lQo6AJR",
            "forum": "GicZtgSlJW",
            "replyto": "GicZtgSlJW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6337/Reviewer_CG5c"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6337/Reviewer_CG5c"
            ],
            "content": {
                "summary": {
                    "value": "This paper views continual learning as a constrained learning problem: to learn the new task without forgetting the old tasks (too much). Some previous work took this perspective as well, but in those cases this way of formulating the continual learning problem only motivated the proposed approach. In this paper, the authors directly address continual learning as a constrained learning problem by making use of recent advances in Lagrangian duality as tool address constrained optimization. In particular, the paper demonstrates that by adopting such a primal-dual method, a principled approach emerges for deciding how to fill the memory buffer."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "As far as I am aware, this is the first work that directly addresses continual learning as a constrained learning problem. The paper proposes a principled framework for this by means of optimizing the Langrangian empirical dual, and it provides clear theoretical justification for its propositions.\n\nA neat theoretical demonstration of the paper is showing that the Lagrangian dual variables can be interpreted as signaling the difficulty of their corresponding task.\n\nThe paper then demonstrates that the Lagrangian dual variables can be used to select which samples to store in the memory buffer, and that empirical benefits can be obtained by doing so."
                },
                "weaknesses": {
                    "value": "Although I think this paper already makes some important and neat contributions, to realize its full potential, I think it is important to improve and clarify the empirical comparisons.\n\n**Indirectness of empirical comparisons**\n\nIn my opinion, from a practical perspective, this paper proposes three \u201cnovel aspects\u201d compared to the standard experience replay approach that is commonly used in continual learning:\n\n{1} the weighing of the replayed losses relative to the loss on the current task is determined by the Lagrangian dual variables (rather than, as is currently done in continual learning, either by a hyperparameter or as a function of how many tasks have been seen so far)\n\n{2} the selection of samples to be stored in the buffer at the task level (buffer partition)\n\n{3} the selection of samples to be stored in the buffer at the sample level\n\nHowever, it seems only the impact of the last two aspects are evaluated empirically. Why do the authors not include a direct comparison to assess the effect of {1}? (That is, a comparison between \"standard ER\" and the approach proposed by this paper except without buffer partition at task level or individual sample selection.) I think doing so could substantially strengthen this paper. Moreover, it is not clear to me whether the comparisons to assess the effect of {2} are direct. For example, in Figure 1 (but a similar question applies to Figure 4), when \u201cPDCL\u201d is compared with \u201cReservoir\u201d, it is not clear how the replayed losses are weighed in the case of \u201cReservoir\u201d. Are they weighed in the same way as in \u201cPDCL\u201d? Or are they weighed in another way? This should be clearly described. If it is the second option, then I do not think that Figure 1 provides a comparison that \u201cisolates the effect of buffer partition\u201d.\n\n**Distinction task- versus class-incremental learning**\n\nThe way the paper describes the difference between task- and class-incremental learning suggests that the authors *train* their models in these two scenarios in the same way, and that there is only a difference between these scenarios in the way the models are *evaluated* at test time. Is this indeed the way the authors implemented their experiments? Because to me it seems there should also be a difference in how models are trained in task-incremental versus class-incremental learning. For example, when training on samples from the second task, with task-incremental learning the models only need to be trained on distinguishing between classes from the current task, while with class-incremental learning the models should also learn that those current samples do not belong to classes from the first task. To clarify this, the authors should provide more details regarding how they implemented the difference between task- and class-incremental learning. When discussing the distinction between task- and class-incremental learning, I think it is also important to cite the original paper (van de Ven et al., 2022; https://www.nature.com/articles/s42256-022-00568-3).\n\n**Minor issues:**\n- top of p9: a reference is made to Figure 9, but I think Figure 4 might be meant?\n- in the reference list, the paper Buzzega et al. (2020) is included twice\n- for a number of papers in the reference list, no venue is included (e.g., Gentile et al., 2022; but there are several others as well)\n- there are several formatting issues with in-text citations in the Appendix\n- on p19, Task Incremental Learning is abbreviated as CIL"
                },
                "questions": {
                    "value": "Although I think this paper already makes some important and neat contributions, to realize its full potential, I think the authors should [1] include empirical comparisons that more directly assess the impact of the three different novel aspects that the authors propose, and [2] provide more details regarding the difference between the task- and class-incremental learning experiments.\n\nPlease see under \u201cWeaknesses\u201d for details on both.\n\nWhile I think it is already a paper that could be accepted, if these two issues can be satisfactorily addressed, I think it could be a strong or very strong paper.\n\nI would be happy to actively engage in the discussion period."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6337/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698662605995,
            "cdate": 1698662605995,
            "tmdate": 1699636697552,
            "mdate": 1699636697552,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "x015bKW9iH",
                "forum": "GicZtgSlJW",
                "replyto": "ky8lQo6AJR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6337/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6337/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer CG5c"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their feedback and for the positive evaluation of our work.\n\n- On the indirectness of empirical comparisons.\n\nIndeed, the effect of adaptive weights for replay losses (as opposed to fixed hyperparameters) is evaluated only indirectly, along with the buffer partition strategy. We agree with the reviewer in that adding the explicit comparison against a primal-dual method that uses the same buffer partition as ER is pertinent (we refer to it as PD w. Reservoir). We provide the preliminary results of this comparison here (CIL setting) and will add the full experiments to the revised version. The main conclusion appears to be that adaptive weights alone provide an improvement with respect to fixed ones. Indeed, in the case of SpeechCommands with a buffer size of 2000, virtually all the gains are explained by the adaptivity of the task-loss weights. However, in many cases, this does not explain the performance improvement entirely. \n\n\n| Test Accuracy  | SeqMNIST |       | SpeechCommands  |       |\n|-------------|-------|-------|-------|-------|\n| Buffer Size | ER   | PD w. Reservoir   | ER   | PD w. Reservoir   |\n| 400         | 70.28 | 82.1 | 45.75 | 49.43 |\n| 2000        | 87.02  | 88.97 |  68.54 | 72.28 |\n\n \nRegarding the baseline Experience Replay, we use the standard implementation from (Chaudhry et al, 2019). At each iteration, a batch of samples from the buffer and a batch of samples from the current task are merged. This larger batch is used to perform a forward pass and to compute the gradient of the loss with respect to $\\theta$. Thus, the relative weight associated to each task loss is proportional to the number of samples of that task available in the buffer. For instance, when using a uniform partition, the relative weights are also uniform. \n\nChaudhry, A., Rohrbach, M., Elhoseiny, M., Ajanthan, T., Dokania, P.K., Torr, P.H. and Ranzato, M.A., 2019. On tiny episodic memories in continual learning. arXiv preprint arXiv:1902.10486.\n  \n- Distinction task versus class incremental learning. \n\nWe follow the formulation of class and task incremental learning of (Boschini et al. 2022), as implemented by the continual learning library Mammoth (https://github.com/aimagelab/mammoth). In this implementation, only the evaluation procedure is affected. The reviewer is correct in that this implementation provides an approximation to the TIL setting since it does not affect the training procedure. When evaluating the model in the TIL setting, only the relevant classes are taken into account. In CIL, however, the predictor can select any of the previously observed categories. We will clarify this distinction in the revised version. We will add the citation to the mentioned reference (van de Ven et al., 2022) and clarify this in the revised version. \n\nBoschini et al, 2022. Class-Incremental Continual Learning into the eXtended DER-verse\n\n- Other comments\n\nThank you very much for pointing these out. We will correct the formatting issues and appendix references in the revised version."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6337/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700494110048,
                "cdate": 1700494110048,
                "tmdate": 1700505109908,
                "mdate": 1700505109908,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OV3CE7nyaH",
                "forum": "GicZtgSlJW",
                "replyto": "ky8lQo6AJR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6337/Reviewer_CG5c"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6337/Reviewer_CG5c"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for responding to my review. I appreciate the additional experiment in which a direct comparison is performed between \"standard ER\" and \"adaptive weights for replay\". I note that the outcome of this experiment seems to challenge, at least to a certain degree, the conclusions from the original submission regarding the benefit of buffer partitioning.\n\nAlthough I think that a difference in performance between \"standard ER\" and \"adaptive weights for replay\" would also be an interesting finding, I think it is unfortunate that based on the currently communicated results it is unclear which conclusions can be drawn from the experiments.\n\nBy proposing a principled framework for directly addressing continual learning as a constrained optimisation problem, I still think this paper makes an interesting and insightful contribution, and therefore has enough merit to be accepted. However, the current incompleteness of the results, and the resulting ambiguity with regards to what conclusions the authors will be able to draw, prevent me from raising my score further.\n\nFurther, reading the other reviews and the author rebuttals to them, I was surprised that the authors seem to find that A-GEM performs substantially better than \"standard ER\". (Comparing the results reported in the rebuttal to reviewer \"Heyc\" with the results reported in the rebuttal to my review.) I have not come across experiments before in which A-GEM clearly outperforms \"standard ER\". Based on my reading of the literature, it seems to be usually the other way around (e.g., De Lange et al., 2022; https://ieeexplore.ieee.org/abstract/document/9349197 or Van de Ven et al., 2022; https://www.nature.com/articles/s42256-022-00568-3). I think it would be good if the authors double-check their results, and if they are indeed correct, it would be useful to discuss why their results might differ from the ones in the above articles."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6337/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700666991458,
                "cdate": 1700666991458,
                "tmdate": 1700667148602,
                "mdate": 1700667148602,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hrlxUbo19Z",
                "forum": "GicZtgSlJW",
                "replyto": "ky8lQo6AJR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6337/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6337/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their comments and acknowledgment of the contribution of the principled constrained learning framework. We are in agreement about the benefits of analyzing a Primal Dual method with no buffer partition to further assess the impact of adaptive weights. Regarding the extra baseline A-GEM. Indeed, it is well-known that the original version of A-GEM is outperformed (in many cases by a large margin) by ER, both with reservoir and ring buffer, as reported in: [On Tiny Episodic Memories in Continual Learning. Chaudhry et al. 2019]. Thus, it is not uncommon to use A-GEM gradient updates but to augment its loss with a replay term. This can be thought of as a mixture of strategies (AGEM with ER), present in some Continual Learning code bases, and recently formalized in [Two Complementary Perspectives to Continual Learning: Ask Not Only What to Optimize, But Also How. Hess, Tuytelaars, van de Ven]. We apologize for the confusion and will revise our response to clarify that this is the version of A-GEM used."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6337/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700679382566,
                "cdate": 1700679382566,
                "tmdate": 1700679417403,
                "mdate": 1700679417403,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "h5f1t9dydJ",
                "forum": "GicZtgSlJW",
                "replyto": "hrlxUbo19Z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6337/Reviewer_CG5c"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6337/Reviewer_CG5c"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the clarification regarding A-GEM. That makes sense to me."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6337/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700680375231,
                "cdate": 1700680375231,
                "tmdate": 1700680375231,
                "mdate": 1700680375231,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "6aONVutOXV",
            "forum": "GicZtgSlJW",
            "replyto": "GicZtgSlJW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6337/Reviewer_ipX4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6337/Reviewer_ipX4"
            ],
            "content": {
                "summary": {
                    "value": "This work directly leverages the constrained optimization framework to solve a continual learning problem. \nBased on the renowned sensitivity analysis with Lagrangian dual variables, this work tackles the continual learning problem in two different aspects, at the level of tasks and data. \n* At the task level, the Primal-Dual Continual Learning (PDCL) algorithm allocates more datapoints to task that is sensitive to constraint perturbation (i.e. large per-task dual variable)\n* At the data level, their indirect sample selection algorithm prefers to choose datapoints that are sensitive to constraint perturbation (i.e. large per-datum dual variables)"
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The authors carefully motivate the readers to understand Lagrangian sensitivity analysis and its application in the context of continual learning. \n* Their experiments show that the idea of Lagrangian sensitivity analysis can be effectively applied to both buffer allocation and data selection for memory-based continual learning."
                },
                "weaknesses": {
                    "value": "* **Regarding theoretical contributions**\n  - In abstract, it is claimed that there are sub-optimality bounds. At first glance, I was expecting learnability guarantee (e.g., PAC) for the actual continual learning problem. However, it turns out that the sub-optimality bound was for estimation of dual variables. Since the estimation of dual variables is the main spirit of the proposed algorithm, I don\u2019t want to say this is not an enough contribution. Rather, I would say the expression \u2018sub-optimality bound\u2019 is quite misleading in some sense.\n  - The paper defers the discussion on the *strong* concavity constant $c$ to Appendix A.5. However, I think this hides several important dependencies. For example,\n    1. It intrinsically assumes the usage of (might be a large amount of) weight decay to induce strong concavity of the objective function;\n    2. The loss function should be $G$-smooth, and the sub-optimality bound in Theorem 4.2 turns out to be depends quadratically on $G$.\n    3. The constraint Jacobian (question: what is it exactly?) must be full rank, and the sub-optimality bound depends quadratically on the inverse of minimum singular value of this matrix, which can be arbitrarily large.\n\n    For these, I think the paper should be more clear and honest on several hidden dependencies.\n  - The last paragraph of Section 4 claims that the weakness of the sub-optimality bound \u201ccan be fixed by replacing the minimum with the average sample complexity\u201d, but I cannot find any detailed discussion on this, throughout the paper.\n  - Although the proof would be similar to that of Theorem 3.2, I think the full proof of Proposition 5.1 should be added, or at least a set of necessary modifications in the proof to prove the proposition must be added.\n* **Regarding Theorem 3.2 and the notation \u201c$\\partial P^{\\star}_t (\\epsilon_k)$\u201d**\n  - Is $\\partial P^{\\star}_t (\\cdot)$ a convex function? I think this should be clarified in order to use the notion of sub-differential.\n  - Also, I think the notation is quite confusing. I would like to suggest the notation like \u201c$\\partial_{\\epsilon_k} P^{\\star}_t (\\epsilon)$\u201d where $\\epsilon = (\\epsilon_1, \u2026, \\epsilon_t)$. \n  - In a higher level of discussion, does the paper ever require such a **local** sensitivity result to give a motivation?\n* **There are several but minor typos and misleading usages of symbols:**\n  - Equation $(P_t)$: I think this should be $\\min_{f\\in\\mathcal{F}}$, not $\\arg\\min_{f\\in\\mathcal{F}}$. This also applies to the equation at the beginning of Appendix A.2.\n  - In Assumption 2.1, $\\delta$ is used for task similarity. Throughout Section 4, however, $\\delta$ is used as a probability parameter.\n  - Assumption 2.4: \u201cThere exists $R, M >0$ such that \u2026\u201d\n  - Page 3, below Equation $(1)$: \u201c\u2026 two-player gamer \u2026\u201d $\\rightarrow$ \u201c\u2026 two-player game \u2026\u201d\n  - Equation $(3)$: Why do we need an inner product between two scalars $-\\lambda_k^\\star$ and $\\gamma$? I don\u2019t think this is necessary.\n  - Proposition 4.1: The order 2.3 and 2.2 must be flipped.\n  - Theorem 4.2: \u201c$\\\\|\\lambda\u2019\\\\|_1 = \\max\\\\{\\\\|\\lambda_p^\\star\\\\|, \\\\|\\hat{\\lambda}_p^\\star\\\\|\\\\}$\u201d $\\rightarrow$ Are all the norms $\\ell_1$-norms?\n  - Page 7, below Equation $(6)$: \u201c$\\mathfrak{B}_t(x,y) \\ne D_t(x,y)$\u201d $\\rightarrow$ \u201c$\\mathfrak{B}_t(x,y) \\ne \\mathfrak{D}_t(x,y)$\u201d\n  - Section 6: there are some inconsistencies of using the word \u201cTiny-ImageNet\u201d, which should be fixed throughout the section.\n  - Appendix A.2, page 15, the equation starts with $L(f,\\lambda;\\epsilon)$: What is $z$ at the end of the equation? I think it should be removed.\n  - Appendix A.5: the letter $\\ell$ is both loss function and the minimum singular value of constraint Jacobian matrix. \n* **Minor comments**\n  - Around Assumption 2.3, it would be great if the authors put some citations on universal approximation results for neural networks, which explains (with examples) the richness of (modern) machine learning model parametrization.\n  - Below Equation $(1)$: \u201c\u2026 the forgetting tolerances $\\\\{\\epsilon_k\\\\}$ need to \u2026\u201d $\\rightarrow$ \u201c\u2026 the forgetting tolerances $\\\\{\\epsilon_k\\\\}$ *suffice* to \u2026\u201d\n  - Page 4: This sentence is quite weird: \u201c\u2026 it is sensible to partition the buffer across different tasks as an increasing function of $\\mathbf{\\lambda}^\\star$...\u201d, because it says we can say that a function is increasing in terms of a vector variable.\n\nOverall, I believe the writing could be much more improved than the current draft."
                },
                "questions": {
                    "value": "Please see **Weaknesses**."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6337/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6337/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6337/Reviewer_ipX4"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6337/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698676317256,
            "cdate": 1698676317256,
            "tmdate": 1699636697429,
            "mdate": 1699636697429,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QCdJWlUn1v",
                "forum": "GicZtgSlJW",
                "replyto": "6aONVutOXV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6337/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6337/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer ipX4"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their time and detailed feedback. In what follows, we provide answers and clarifications to the weaknesses and questions raised.\n\n- Regarding Theorem 3.2/5.1 and the sub-differential $\\partial P^*_t(\\epsilon_k)$. \n\nIn this setting, the perturbation function $\\partial P^{\\star}\\_t(\\epsilon_k)$ is a convex function of the constraint upper bounds $\\epsilon$. This is a well-known result from convex optimization (see e.g., (Bonnans & Shapiro, 1998) or (Rockafellar, 1997, Theorem 29.1)), and we agree with the reviewer in that it is pertinent to mention it before Theorem 3.2. We will also update the notation of the subdifferential to $P^*_{\\epsilon_k}(\\epsilon)$ as suggested to avoid confusion.\n\nOne could motivate the buffer partition approach without this sensitivity result. Note that dual variables accumulate the slacks (task-level constraint violations) over the continual learning procedure. Thus, it is not surprising that they carry information about task difficulty and that they can be used to manage the buffer. However, Theorem 3.2 provides a complete description of the induced task difficulty metric. Theorem 3.2 tells us that dual variables capture the minimum rate of degradation of the performance on the current task, when tightening the forgetting requirement in a previous task. This translates into a principled measure of how hard maintaining the performance in a past task is, relative to other observed tasks. Thus, we believe that Theorem 3.2 is a key component of this paper. We also provide an experimental interpretation of Proposition 5.1 (see Figure 3.b).\n\n- On the estimation of dual variables.\n\nSince we only have access to samples (not probability distributions) and we optimize over model parameters (not functions), in practice, we solve the empirical problem $(\\hat{D}_t)$ instead of $P_t$. This corresponds to analyzing estimation and approximation errors of constrained learning, as done by e.g., (Vapnik 1999) or (Shai-Shalev Schwarz 2014) for supervised learning. For constrained learning, these errors were characterized, to a certain extent, in (Chamon et al. 2023). Theorem 4.2 bridges the gap between the optimal dual variables of $(P_t)$ and $(\\hat{D}_t)$.\n\nFor the distance between these dual variables to be bounded, the dual function $g_u$ can not be flat. Observe that $g_u$ is the minimum of a family of affine functions; thus, it is concave, irrespective of whether the primal problem is convex. For it to be strongly-concave, we require three extra assumptions. Namely, strong convexity and smoothness of the loss and full-rankness of the constraint Jacobian. \n\nThe first two (strong convexity and smoothness) are widely used to prove convergence results for gradient descent algorithms even in the convex setting and are satisfied by typical losses used in ML when, e.g., the hypothesis class $\\mathcal{F}$ is compact. Note that we require convexity of the objective with respect to the functionals, but not model parameters, which holds for both mean squared error and cross-entropy loss. \n\nAlthough harder to guarantee, Assumptions 3.7 is a constraint qualification called LICQ (Linear Independence Constraint Qualification), which is ubiquitous in constrained optimization (Bertsekas 1995). The independence of the columns of $D_f \\ell (f^*)$ reflects that, at the optimum, the constraints are not redundant and that they contribute in different ways to reach the optimal point. Although this is probably the stricter assumption, we believe that it can be lifted, being replaced by a stronger version of strict feasibility, but this is not immediate. With these assumptions, we can guarantee that $g_u$ is not linear, and that the optimal dual variables of $(P_t)$ and $(\\hat{D}_t)$ are not too far away. \n\nAlthough improvements to this bound could be obtained, we leave them for future work. We believe that the current version of the bound theoretically grounds the use of dual variables as sensitivity indicators. \n\n\n - Other comments \n\nWe agree with the reviewer regarding the lack of comments on how the proof of Theorem 3.2 relates to the proof of Proposition 5.1 (i.e., sample-level sensitivity). These modifications are very minor since the only difference between Theorem 3.2 (task-level sensitivity) and Proposition 5.1 (sample-level sensitivity) is the dimensionality of the input to $P^*_t(\\epsilon)$. We will add these comments to the revised version. We will also fix the typos mentioned in the revised version and add citations on universal approximation results for neural networks (e.g., Hornik 1989).\n\nHornik, K., Stinchcombe, M. and White, H., 1989. Multilayer feedforward networks are universal approximators. Neural networks, 2(5), pp.359-366.\n\nChamon, Luiz FO, et al. \"Constrained learning with non-convex losses.\" IEEE Transactions on Information Theory 69.3 (2022): 1739-1760."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6337/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700494047988,
                "cdate": 1700494047988,
                "tmdate": 1700505636792,
                "mdate": 1700505636792,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Hbp4JoSCtS",
            "forum": "GicZtgSlJW",
            "replyto": "GicZtgSlJW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6337/Reviewer_Heyc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6337/Reviewer_Heyc"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a theoretical analysis of memory-based continual learning based on the recent advances in constrained optimization.\n\nIn terms of constrained optimization, preventing forgetting previously learned tasks becomes the constraint of the optimization problem, and the emprical risk with finite samples should be bounded by the forgetting tolerance as the constraints.\n\nMotivated by the theoretical result of the constrained learning through Lagrangian duality (Chamon et. al. 2020), the authors provide a theoretical plausible Lagrange multiplier $\\lambda_k$ and the buffer size $n_k$  for each task $k.$\n\nIn the experiment, the paper provides some toy benchmark results, such as seq-MNIST with several memory-based baselines."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "In the research of continual learning, there is few optimization-based analysis to mitigate catastrophic forgetting.\n\nThis paper provides a new theoy-based algorithm from scratch, which helps to understand which Lagrange multiplier  and buffer size are used totrain the neural networks for continual learning."
                },
                "weaknesses": {
                    "value": "Despite the theoretical result, the proposed algorithm does not fit the online continual learning scenario because the process \"fill buffer\" is done after visiting samples in line 11 of Algorithm 1.\n\nThis implies that the buffer should keep all encountered data points during $n_{iter}$ iterations, and then the buffer drops some samples to satisfy the buffer size condition, which has already been violated in lines 5-10 in Algorithm 1.\n\nIt seems that this contradiction occurs because Algorithm 1 needs to access the information of $\\lambda_k$ at the end of each task to compute the optimal buffer size. However, we should have at least the upper bound of the buffer size for the current task $k$ to save encountering samples in the online stream.\n\nIn addition, the loss landscape on the parameter $\\theta$ is non-convex, as the authors stated in Section 3. The local-optimal setting for a given local minimal point and the Lagrange multiplier do not guarantee remarkable performance in the empirical result. The existing heuristic methods based on constrained optimization, such as A-GEM, have already shown remarkable performance in more complex benchmarks, such as split-CIFAR100 and split-MiniImagenet.\n\nConsidering the recent advances in continual learning, I think that a new constrained optimization-based CL algorithm should be either theoretically solid or empirically outstanding."
                },
                "questions": {
                    "value": "1. The reported metric is not standard in continual learning. Can the authors report the experiemntal result in terms of the average test accuracy and FWT?\n2. I think the constrained optimizaiton based CL baselines, such as GEM and A-GEM should be included in the experiemt section to analyze the novelty of the proposed method. Is there any reason why the authors does not contain these algorithms?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6337/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698976634453,
            "cdate": 1698976634453,
            "tmdate": 1699636697316,
            "mdate": 1699636697316,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RRmNsjnxPQ",
                "forum": "GicZtgSlJW",
                "replyto": "Hbp4JoSCtS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6337/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6337/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer Heyc"
                    },
                    "comment": {
                        "value": "We greatly appreciate the reviewer's feedback. We strongly encourage them to read our general response to all reviewers as it will clarify the main motivations and contributions tackled by our work. We hope that our answers below will clarify specific points that might have been missed.\n\n- On the availability of samples during training. \n\nThe notation used could have led to some confusion regarding the availability of samples during the execution of the Primal-Dual iterations and buffer partition. As explained in section 2, at iteration $t$, the following are available:\n\n1. Samples associated to task $t$.\n\n2. Samples from tasks $1, \\dots, t-1$ that where stored in the buffer: $\\mathcal{B}\\_t = \\cup_{k=1}^{t-1} \\mathcal{B}\\_t^k$  where  $\\mathcal{B}_t^k$ denotes the subset of the buffer allocated to task $k$ at iteration $t$.\n\nThis is the standard memory-based continual learning setting [1], where one can store a subset of the samples from previous tasks, revisiting them at future iterations. We will clarify this in the algorithm.\n\n[1] GM van de Ven, Three types of incremental learning, 2022.\n\n- On the computation of $\\mathbf{\\lambda}$ at each iteration.\n\nIndeed, at each iteration $t$, we undertake the dual problem $\\hat{D}_t$, from which we obtain a vector of dual variables $\\mathbf{\\lambda}$. This means that we assess the relative difficulty of the observed tasks and compute a new buffer partition at each iteration. However, we do not require access to the entire dataset of a previous task to compute this partition. Instead, it is estimated using the set of samples $\\mathcal{B}_t$ stored in the buffer. As mentioned in section 7, this can give rise to the issue of dual variable underestimation.\n\n- On the sub-optimality of $\\theta^*$.\n\nIndeed, the loss landscape on the parameter $\\theta$ is non-convex, creating a duality gap. In other words, the value of the dual problem $\\hat{D}^{\\star}_t$ need not be a good approximation of the value of the primal problem $P^\\star_t$. However, as mentioned in section 3, this was tackled in (Chamon et al, 2022, Theorem 1):\n\n$$ | P^\\star_t - \\hat{D}^{\\star}_t | \\leq (M\\nu+\\zeta)(1+ \\Delta ) \\text{,}$$ where $\\Delta= \\text{max} ( \\Vert \\tilde{\\lambda}^* \\Vert, \\Vert \\hat{\\lambda^*} \\Vert )$ and $\\zeta = \\text{max} \\zeta_i(N_i, \\delta)$ (see section 4.2 for the definition of $\\zeta_i$ ).\n\nConstrained Learning with Non-Convex Losses, 2022. Luiz F. O. Chamon, Santiago Paternain, Miguel Calvo-Fullana, and Alejandro Ribeiro.\n\n- On the reported metric.\n\nWe report the Top-1 Error, which is simple 100-Average Test Accuracy. We report this both in the Task Incremental Setup and Class Incremental Setup, both of which are standard in the continual learning literature. For completeness, we will add accuracy to the revised version in the Additional Experiments Appendix.\n\n\n- On the baselines. \n\nWe agree with the reviewer that A-GEM is a relevant baseline. We attach the results of A-GEM on SpeechCommands here and will include them on the rest of the datasets in the camera-ready version. A-GEM appears to perform worse than PDCL on the Task Incremental setting (~2%), and slightly better on the Class Incremental setting.\n\n|             | A-GEM |       | PDCL  |       |\n|-------------|-------|-------|-------|-------|\n| Buffer Size | TIL   | CIL   | TIL   | CIL   |\n| 400         | 90.07 | 55.75 | 92.94 | 54.73 |\n| 2000        | 92.5  | 72.54 | 94.79 | 72.36 |"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6337/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700493777347,
                "cdate": 1700493777347,
                "tmdate": 1700504742502,
                "mdate": 1700504742502,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]