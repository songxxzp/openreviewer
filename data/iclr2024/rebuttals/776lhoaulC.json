[
    {
        "title": "Exploring the Common Appearance-Boundary Adaptation for Nighttime Optical Flow"
    },
    {
        "review": {
            "id": "2jUKwPaBnb",
            "forum": "776lhoaulC",
            "replyto": "776lhoaulC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4688/Reviewer_R9ad"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4688/Reviewer_R9ad"
            ],
            "content": {
                "summary": {
                    "value": "The study addresses nighttime optical flow, hampered by low texture and high noise. Traditional methods employ domain adaptation from auxiliary to nighttime domains but struggle due to domain gaps. To overcome this, the study introduces a common-latent space, using daytime and event domains. It shows motion appearance knowledge can be transferred effectively in reflectance-aligned spaces, and theoretical derivations emphasize substantial correlations between spatiotemporal gradients. Appearance and boundary adaptation are complementary and effectively transfer global motion and local boundary knowledge to nighttime domains, as validated by extensive experiments."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1) The paper is clearly written and easy to follow.\n2) The novelty of this paper, in my opinion, is significant to the community. The concept of \u201ccommon space adaptation\u201d is very effective to reinforce feature alignment between domains, and it has great potentials to be applied for any degraded scene understanding tasks. \n3) The constructed common spaces are from both appearance (reflectance) and boundary (gradient) sides. They are complementary and sound reasonable for the performance improvement."
                },
                "weaknesses": {
                    "value": "While the proposed method appears promising, the complexity of the loss function involving seven balance weights raises concerns. Identifying the appropriate values for these seven weights efficiently is a challenge. Performing a grid search to determine these values might be time-consuming and resource-intensive."
                },
                "questions": {
                    "value": "Please refer to the weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "10: strong accept, should be highlighted at the conference"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4688/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4688/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4688/Reviewer_R9ad"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4688/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698284430789,
            "cdate": 1698284430789,
            "tmdate": 1699636450391,
            "mdate": 1699636450391,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FMlJcnWjtT",
                "forum": "776lhoaulC",
                "replyto": "2jUKwPaBnb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4688/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4688/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q1:** Complexity of determining the balance weights for the loss function.\n\n**A:** We deeply thanks for your strong recommendation of the proposed framework. We do acknowledge that, if the whole framework is directly trained together, determining the balance weights of the loss function can be time-consuming. However, in Fig. 2 of this paper, each component has its own specific physical meaning. In appearance adaptation, the common latent reflectance space bridges the daytime-nighttime domain gap, and assists the motion distribution alignment module in transferring motion appearance knowledge from daytime to nighttime domain. In boundary adaptation, the common latent boundary space closes the event-image domain gap, thus guiding the motion boundary contrastive module to transfer local boundary knowledge from event to nighttime image domain. Therefore, the whole framework can be divided into different components for separate training (seeing the \"Implementation Details\" part of this paper). When each module is trained separately, only 1-2 balance weights need to be determined, which is easy to adjust. In the final joint optimization, we fine-tune the whole framework using the preset balance weights. In addition, the weight sensitivity of the loss function has been analyzed in the \"Implementation of Training Details\" subsection of the proposed supplementary material, where we have discussed four balance weights of the losses that are related to domain adaptation. We will further add the weight sensitivity experiments of the other balance weights for the loss function in the revised manuscript."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4688/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700055471819,
                "cdate": 1700055471819,
                "tmdate": 1700055471819,
                "mdate": 1700055471819,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HTugY6DpSr",
                "forum": "776lhoaulC",
                "replyto": "2jUKwPaBnb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4688/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4688/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer R9ad,\n\nThank you for your recognition and recommendation of our work. We have explained the question raised by you. We are expecting for your reply.\n\nBest wishes."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4688/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700392835904,
                "cdate": 1700392835904,
                "tmdate": 1700392835904,
                "mdate": 1700392835904,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AYzvzRwtEk",
                "forum": "776lhoaulC",
                "replyto": "2jUKwPaBnb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4688/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4688/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer R9ad,\n\nWe express our sincere gratitude for your recognition and recommendation of our work. We have actively discussed your valuable queries. And, we eagerly anticipate your prompt feedback.\n\nBest wishes."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4688/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700565755740,
                "cdate": 1700565755740,
                "tmdate": 1700565755740,
                "mdate": 1700565755740,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "op5OI438ha",
                "forum": "776lhoaulC",
                "replyto": "AYzvzRwtEk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4688/Reviewer_R9ad"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4688/Reviewer_R9ad"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Authors\uff1a\n\nI've read the feedback and that addresses my concerns. I would maintain the initial scores.\n\nGood Luck!"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4688/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700615594105,
                "cdate": 1700615594105,
                "tmdate": 1700615594105,
                "mdate": 1700615594105,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QnTXu5bVC6",
            "forum": "776lhoaulC",
            "replyto": "776lhoaulC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4688/Reviewer_hzUU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4688/Reviewer_hzUU"
            ],
            "content": {
                "summary": {
                    "value": "This paper  focuses on the nighttime optical flow task, which is a challenging.  This paper  expolit two auxiliary daytime adn event domains, and present a common apperance-boundary adaption framework for nighttime optical flow. For apperance and boundary adaption, this paper have the new exploration, and both them are complementary to each other.  Extensive experiments  are conducted on various datasets, showing the SOTA performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper is well-presented, including the figures and tables.\n\n2. The experiments are well-conducted, including  main comparsions, ablation studies and viual results. The method achieves the sota performance.\n\n3. From the provided video demo, the proposed method genelizes well across many scenes."
                },
                "weaknesses": {
                    "value": "It would be better to add some analyses on the failure cases and show the limitations of the method and give some discussions."
                },
                "questions": {
                    "value": "Hope authors to release the codes to benefit the community on this field."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4688/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698770270412,
            "cdate": 1698770270412,
            "tmdate": 1699636450292,
            "mdate": 1699636450292,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gEigPHFVlp",
                "forum": "776lhoaulC",
                "replyto": "QnTXu5bVC6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4688/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4688/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q1:** Limitation of the proposed method.\n\n**A:** Thanks for appreciating our work. We have specifically discussed the limitation of the proposed method in the \"3.7 Limitation'' subsection of the supplementary material. The proposed method can model the x, y-axis motion pattern, but fails to estimate the radial motion along the z-axis. The key to z-axis radial motion estimation is to obtain the accurate scene depth information. From the perspective of imaging mechanism, the frame camera records absolute luminance in the x, y-axis via global scan, while the event camera mainly triggers x, y-axis events due to brightness change, and is difficult to respond to z-axis motion in a short time. Therefore, it is difficult for frame and event cameras to directly perceive z-axis motion information. For example, when imaging that a vehicle is approaching us from far to near along the center of the camera, the frame camera cannot capture the relative motion of the vehicle, and the event camera also fails to trigger the corresponding events. From the perspective of knowledge transfer, the common space we constructed follows the x, y-axis basic optical flow model, serving as an intermediate bridge to guide the knowledge transfer, which lacks the constraint on z-axis motion knowledge. Both aspects limit the ability of the proposed method to model z-axis radial motion. In the future, we will further introduce LiDAR which is robust to illumination, to directly measure the depth information to assistantly estimate the 3D motion."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4688/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700055407826,
                "cdate": 1700055407826,
                "tmdate": 1700055407826,
                "mdate": 1700055407826,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gV4hy0f9sW",
                "forum": "776lhoaulC",
                "replyto": "QnTXu5bVC6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4688/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4688/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer hzUU,\n\nThank you very much for reviewing our work. We have discussed the limitation raised by you. We are looking forward to your response.\n\nBest wishes."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4688/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700392775531,
                "cdate": 1700392775531,
                "tmdate": 1700392775531,
                "mdate": 1700392775531,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bsPFGss3qs",
                "forum": "776lhoaulC",
                "replyto": "QnTXu5bVC6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4688/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4688/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer hzUU,\n\nThank you for your detailed review. We have taken your valuable feedback into consideration and made some discussions to address your concerns about the limitation. As the discussion deadline is approaching, we eagerly await your timely response.\n\nBest wishes."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4688/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700565437937,
                "cdate": 1700565437937,
                "tmdate": 1700565437937,
                "mdate": 1700565437937,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "GNUbIcw471",
            "forum": "776lhoaulC",
            "replyto": "776lhoaulC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4688/Reviewer_koka"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4688/Reviewer_koka"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel common appearance-boundary adaptation framework for nighttime optical flow estimation. The authors explore a common latent space as an intermediate bridge to reinforce feature alignment between auxiliary and nighttime domains. They construct two common spaces: a reflectance-aligned common space between daytime and nighttime domains, and a spatiotemporal gradient-aligned common space between nighttime frame and accumulated events. The appearance adaptation transfers global motion knowledge from daytime to nighttime domain, while the boundary adaptation transfers local motion boundary knowledge from event to nighttime domain. The proposed method, ABDA-Flow, achieves state-of-the-art performance for nighttime optical flow."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The common latent space approach effectively mitigates the distribution misalignment issue between source and target domains.\n2. The appearance and boundary adaptations complement each other, jointly transferring global motion and local boundary knowledge to the nighttime domain.\n3. The proposed method achieves state-of-the-art performance for nighttime optical flow estimation."
                },
                "weaknesses": {
                    "value": "1. The method may be more complex to implement and train compared to simpler optical flow estimation techniques.\n2. The effectiveness of the proposed method may be limited to specific nighttime optical flow tasks and datasets.\n3. The runtime of the method may be slower than some other optical flow estimation techniques due to the Transformer architecture."
                },
                "questions": {
                    "value": "1. How does the proposed common appearance-boundary adaptation framework compare to other domain adaptation techniques in terms of computational complexity and efficiency?\n2. Can the proposed method be applied to other challenging optical flow estimation tasks, such as low-light or high-speed scenarios?\n3. How does the proposed method handle the trade-off between model size and computational efficiency? Are there any plans to further optimize the runtime or explore other efficient optical flow estimation techniques?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4688/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698846474260,
            "cdate": 1698846474260,
            "tmdate": 1699636450198,
            "mdate": 1699636450198,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "G8oG0tmuaE",
                "forum": "776lhoaulC",
                "replyto": "GNUbIcw471",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4688/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4688/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q1:** How does the proposed common appearance-boundary adaptation framework compare to other domain adaptation techniques in terms of computational complexity and efficiency?\n\n**A:** Thanks for your recognition of our work. Compared with other domain adaptation techniques, the proposed common appearance-boundary adaptation framework only introduces additional computational consumption in the training phase, but can greatly improve the optical flow performance. During the inference phase, the final model is the same as the typical optical flow backbone, which only needs the nighttime optical flow encoder $E_n$ and $GRU$ in Fig. 2 of this paper for testing. To fairly compare the computational complexity and efficiency, with the same optical flow backbone and training data, we compare the proposed common space adaptation with the visual adaptation (\"CycleGAN+our flow baseline\" in Table 5 of this paper) and the motion adaptation (\"CycleGAN+our flow baseline+E-RAFT\" in Table 5 of this paper) in complexity and performance as shown in the following table:\n\n|       Training data       |                Method                 | FLOPs (G) | Parameters (M) |   EPE    |\n| :-----------------------: | :-----------------------------------: | :-------: | :------------: | :------: |\n|    daytime, nighttime     |      CycleGAN+our flow baseline       |   1.85    |     31.28      |   1.41   |\n|                           |     Common appearance adaptation      |   1.87    |     35.84      | **0.87** |\n| daytime, nighttime, event |   CycleGAN+our flow baseline+E-RAFT   |   2.58    |     46.72      |   1.33   |\n|                           | Common appearance-boundary adaptation |   2.42    |     56.26      | **0.74** |\n\n\u200b\tThe results indicate that the complexity (i.e., FLOPs and Parameters) does not increase much, but the optical flow performance (i.e., EPE) is significantly improved. Therefore, the proposed common space adaptation only sacrifices a slight computational cost to transfer knowledge in the training stage, but can greatly improve the nighttime optical flow in the inference stage.\n\n**Q2:** The impact of common space adaptation on the model size and efficiency of the optical flow model for inference.\n\n**A:** It should be emphasized that the focus of this work is not the single optical flow backbone in the inference stage, but the knowledge transfer framework in the training stage for the nighttime optical flow task. The final optical flow model only needs the nighttime optical flow encoder $E_n$ and $GRU$ in Fig. 2 of this paper for testing, which can be replaced with any optical flow backbone. The model size and runtime are related to the optical flow backbone, while the accuracy of nighttime optical flow mainly depends on the knowledge transfer framework during the training phase. In the \"3.5 Inference Time'' subsection of the proposed supplementary material, we have compared the runtime and performance of different optical flow backbones that before and after being trained under the proposed framework. In the table below, we further select the latest lightweight optical flow network EMD-S [1] to compare the model size, runtime, and accuracy without and with knowledge transfer:\n\n|              Method               | Model size (M) | Runtime (ms) | EPE  |\n| :-------------------------------: | :------------: | :----------: | :--: |\n|                GMA                |      5.9       |      79      | 1.48 |\n|     GMA w/ common adaptation      |      5.9       |      79      | 0.79 |\n|            Transformer            |      18.2      |     273      | 1.43 |\n| Transformer  w/ common adaptation |      18.2      |     273      | 0.74 |\n|               EMD-S               |      4.5       |      42      | 1.68 |\n|    EMD-S w/ common adaptation     |      4.5       |      42      | 0.81 |\n\n\u200b\tWe have two conclusions. First, model size and runtime are only related to the optical flow backbone. Second, the proposed common space adaptation framework can significantly improve the nighttime optical flow performance of different optical flow backbones. Therefore, the proposed common space adaptation is a plug-and-play training framework that can solve the nighttime optical flow problems.\n\n[1] Deng C, et al. Explicit motion disentangling for efficient optical flow estimation. ICCV, 2023."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4688/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700055168659,
                "cdate": 1700055168659,
                "tmdate": 1700055168659,
                "mdate": 1700055168659,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OyshQwMaoJ",
                "forum": "776lhoaulC",
                "replyto": "GNUbIcw471",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4688/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4688/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q3:** Application for low-light and high-speed scenarios.\n\n**A:** Low light and high speed are two of the more challenging scenarios for nighttime scenes. In terms of imaging mechanism, conventional frame camera records the absolute luminance with a fixed exposure time via global scan. In nighttime scenes, conventional frame camera would inevitably face a dilemma between the long exposure time of low-light scenarios and motion blur of high-speed scenarios. In contrast, the event camera reacts to changes in light intensity, rather than integrating photons during the exposure time of each frame [2]. Each pixel works independently and returns a signal only when an intensity change is detected. Compared with the conventional frame camera, the event camera can sense the dynamic changes with higher temporal resolution (microsecond) and higher dynamic range, thus compensating for frame camera in nighttime scenes, such as, nighttime image enhancement [3] and nighttime deblurring [4].\n\nSimilarly, the event camera can also assist the frame camera to learn the motion patterns in nighttime low-light and high-speed scenarios. To discuss the impact of the event on the optical flow in nighttime low-light and high-speed scenes, as shown in the following table, we use the coaxial optical system (seeing Fig. 1 in the supplementary PDF) to collect the spatiotemporally-aligned image sequences and events stream with various illumination (e.g., 3.5 lux, 9.2 lux and 12.7 lux) and various driving speed (e.g., 50 km/h, 70 km/h and 80 km/h) to quantitatively compare the optical flow performance. As for the optical flow label, since it is difficult to directly obtain the dense optical flow labels, we manually mark 100 pairs of corresponding corner points for each two adjacent images, and calculate the relative displacement between the corner points as the sparse optical flow labels. In addition, we choose EPE as the evaluation metric.\n\n|                                              | Low light |         |          | High speed |         |         |\n| :------------------------------------------: | :-------: | :-----: | :------: | :--------: | :-----: | :-----: |\n|                    Method                    |  3.5 lux  | 9.2 lux | 12.7 lux |  50 km/h   | 70 km/h | 80 km/h |\n|              our flow baseline               |   4.26    |  3.98   |   3.67   |    3.65    |  4.53   |  5.74   |\n|       our flow baseline, w/ only event       |   2.05    |  2.01   |   1.63   |    1.75    |  1.75   |  1.80   |\n| our flow baseline, w/ event, w/ common space |   1.52    |  1.44   |   1.10   |    1.14    |  1.15   |  1.19   |\n\nWe have two observations. First, the event camera can greatly improve optical flow performance in both nighttime low-light and nighttime high-speed scenes. In high-speed scenes, the proposed method is robust to various speeds, and the optical flow performance remains unchanged, demonstrating the advantage of high temporal resolution of the event camera. In low-light scenes, as the illumination becomes lower, the optical flow metric (EPE) trend becomes larger obviously. This shows that, although the event camera has the advantage of high dynamic range, too low illumination would also interfere with the optical flow performance. The main reason is that, under low light conditions, the event noise is intensified. The event noise and the valid signal event are both 0-1 pulses, and their difference is very small, which affects the optical flow. In the future, we will further consider the impact of noise and achieve optical flow estimation under extremely low-light scenes. Second, the proposed common space can further improve the upper limit of optical flow, indicating that common space can serve as a bridge to reinforce the feature alignment between event and nighttime image domains.\n\n[2] Cabriel C, et al. Event-based vision sensor for fast and dense single-molecule localization microscopy. Nature Photonics, 2023.\n\n[3] Liang J, et al. Coherent Event Guided Low-Light Video Enhancement. ICCV, 2023.\n\n[4] Qi Y, et al. E2NeRF: Event Enhanced Neural Radiance Fields from Blurry Images. ICCV, 2023."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4688/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700055276171,
                "cdate": 1700055276171,
                "tmdate": 1700055276171,
                "mdate": 1700055276171,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QwzVOAfHa6",
                "forum": "776lhoaulC",
                "replyto": "GNUbIcw471",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4688/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4688/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer koka,\n\nSincere gratitude for your comment. We have addressed all the comments raised by you. Item by item responses to the your comments are listed above this response. We are looking forward to your feedback.\n\nBest wishes."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4688/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700392734825,
                "cdate": 1700392734825,
                "tmdate": 1700392734825,
                "mdate": 1700392734825,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "A3WwL2UjST",
                "forum": "776lhoaulC",
                "replyto": "GNUbIcw471",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4688/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4688/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer koka,\n\nWe appreciate your thorough review. We have actively conducted various experiments to address your valuable questions. As the deadline for discussions is approaching, we look forward to your prompt feedback.\n\nBest wishes."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4688/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700565289103,
                "cdate": 1700565289103,
                "tmdate": 1700565289103,
                "mdate": 1700565289103,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]