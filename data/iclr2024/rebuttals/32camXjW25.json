[
    {
        "title": "Covariance-corrected Whitening Alleviates Network Degeneration on Imbalanced Classification"
    },
    {
        "review": {
            "id": "8GjwQtT2Df",
            "forum": "32camXjW25",
            "replyto": "32camXjW25",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission720/Reviewer_xPQZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission720/Reviewer_xPQZ"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a normalization method with class-aware sampling to cope with class imbalance in long-tailed classification. By analyzing covariance matrices of features trained on an imbalanced dataset, the authors find out the issue of long-tailed learning that learnt feature components heavily correlate with each other, degrading rank of feature representation. To mitigate it, the proposed method embeds a whitening module to decorrelate features and applies a sampling strategy based on class distribution toward stable training. In the experiments on long-tailed image classification, the method exhibits competitive performance with the other approaches."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ Analysis about feature covariance is interesting and effectively inspires the authors to embed feature decorrelation into neural networks.\n+ Performance is empirically evaluated on several benchmark datasets to demonstrate the efficacy of the method in comparison to the others."
                },
                "weaknesses": {
                    "value": "### - Sampling technique.\nWhile the whitening is well introduced into long-tailed classification in Secs.3.1-3.3, the sampling strategy (Sec.3.4) is presented in a heuristic manner without providing detailed (theoretical) analysis nor motivation.\n\nIt is unclear why classes are first divided into several groups (in Fig.4). The concept of \"group\" as a superset of classes is introduced in a procedural way, lacking discussion about its effect on sampling.\n\nThen, ad-hoc sampling rule is defined by using lots of hyper-parameters in Eqs.(4,5). What is a key difference from the standard class-balanced sampling? For realizing such a class-aware sampling, it is more straightforward to control the class frequency in sampling between the uniform (class-balanced sampling) and the ratio of class samples (instance-balanced sampling), though it is hard to grasp the purpose of GRBS in this manuscript.\n\nBesides, BET is just a simple technique to control frequency of the GRBS.\nAs shown in Table 4, the GRBS itself degrades performance, while it is improved by BET. Following this direction, class-balanced sampling (CB) could also be improved by applying such an ad-hoc control of sampling frequency.\nPile of these ad-hoc techniques makes the method theoretically unclear.\n\n### - Feature analysis.\nThis paper lacks in-depth analysis about feature co-variance (Fig.2). Qualitative discussion/analysis is required to clarify why such a rank reduction happens in the scenario of class imbalance. SVD results in the bottom row of Fig.2 are less discussed; add more comments on it such as by clarifying what the two axes mean.\nThere are also several works to cope with class imbalance by means of feature co-variance [R1,R2] and normalization [R3][Zhong+21]. The authors should discuss the proposed method in those frameworks for clarifying its novelty.\n\nFig.3 provides a confusing analysis based on trace norm of covariance matrices. The right-hand figure shows that the proposed method increases \"instability\", possibly leading to unfavorable training.\nIt is a confusing result and makes it hard to understand the authors' claim. The trace norm is dependent on the feature scale (magnitude) which is less relevant to instability of training. Thus, it seems not to be a proper metric for measuring the instability in this case; the authors should apply the other metric invariant to feature scales.\n\n### - Experimental results.\nThe performance results in Sec.4 are inferior to SOTAs reported, e.g., in [R3]. For fair comparison to the SOTAs, the method should be embedded into the popular backbone networks, e.g., ResNet-50 for ImageNet-LT and iNat18.\nIt is also valuable to check whether discriminative features of deeper backbones behaves in the similar way to Fig.2 or not.\n\nIn Fig.4 right, it is meaningless to compare training losses among different sampling strategies since even an identical training dataset can be regarded as different ones by varying sampling rules.\n\n\n[R1] Xiaohua Chen et al. Imagine by Reasoning: A Reasoning-Based Implicit Semantic Data Augmentation for Long-Tailed Classification. In AAAI22.\n\n[R2] Yingjie Tian et al. Improving long-tailed classification by disentangled variance transfer. Internet of Things 21, 2023.\n\n[R3] Lechao Cheng et al. Compound Batch Normalization for Long-tailed Image Classification. In MM22.\n\n### - Minor comments:\nIn p.8: Table ?? -> Table 2"
                },
                "questions": {
                    "value": "Please provide responses to the above-mentioned concerns about sampling technique and analysis about features."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission720/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698135767532,
            "cdate": 1698135767532,
            "tmdate": 1699635999256,
            "mdate": 1699635999256,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tIh74pPJmd",
                "forum": "32camXjW25",
                "replyto": "8GjwQtT2Df",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission720/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission720/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xPQZ [1]"
                    },
                    "comment": {
                        "value": "Thank you for the review and constructive comments.\n\n\n\n**Q1: While the whitening is well introduced into long-tailed classification in Secs.3.1-3.3, the sampling strategy (Sec.3.4) is presented in a heuristic manner without providing detailed (theoretical) analysis nor motivation.**\n\nA1: As shown in Figure 3, the batch covariance statistic exhibits significant fluctuations, impeding the convergence of the whitening operation. Our proposed covariance-corrected modules are designed to obtain more accurate and stable batch statistic estimation for whitening to avoid its non-convergence and reinforce its capability in imbalanced scenarios. The results on Table 4 also verify their effectiveness. \n\n\n**Q2: It is unclear why classes are first divided into several groups (in Fig.4). The concept of \"group\" as a superset of classes is introduced in a procedural way, lacking discussion about its effect on sampling.**\n\nA2: We introduced the reason why sample categories are divided into different groups, \"In order to make the categories in each group relatively balanced, we select from N sorted categories at equal intervals to form G groups.\". This also makes the distribution of samples in each batch relatively stable, because batch samples are collected in a group. Please refer to Figure 4 for details.\n\n\n\n**Q3: Then, ad-hoc sampling rule is defined by using lots of hyper-parameters in Eqs.(4,5). What is a key difference from the standard class-balanced sampling? For realizing such a class-aware sampling, it is more straightforward to control the class frequency in sampling between the uniform (class-balanced sampling) and the ratio of class samples (instance-balanced sampling), though it is hard to grasp the purpose of GRBS in this manuscript.**\n\nA3: 1) Our results in Table 4 demonstrate that class-balanced sampling can cause model performance to degrade, because class balancing can make the model overfit to classes with few samples. This is a well-known conclusion. 2) Our proposed GRBS controls the sampling probability of the category, and combined with the BET training strategy prevents the model from overfitting to tail classes.\n\n\n**Q4: Besides, BET is just a simple technique to control frequency of the GRBS. As shown in Table 4, the GRBS itself degrades performance, while it is improved by BET. Following this direction, class-balanced sampling (CB) could also be improved by applying such an ad-hoc control of sampling frequency. Pile of these ad-hoc techniques makes the method theoretically unclear.**\n\nA4: 1) The results in Table 4 show that our GRBS performs better than CB before and after using whitening operation. 2) The purpose of GRBS and BET is to let the tail classes participate in more iterations without affecting the representation learning of head classes. More importantly, their combination can obtain stable batch statistics, thereby avoiding the non-convergence of whitening.\n\n\n\n**Q5: This paper lacks in-depth analysis about feature co-variance (Fig.2). Qualitative discussion/analysis is required to clarify why such a rank reduction happens in the scenario of class imbalance. SVD results in the bottom row of Fig.2 are less discussed; add more comments on it such as by clarifying what the two axes mean. There are also several works to cope with class imbalance by means of feature co-variance [R1,R2] and normalization [R3][Zhong+21]. The authors should discuss the proposed method in those frameworks for clarifying its novelty.**\n\nA5: 1) Thanks for your comment, we have introduced the meaning of coordinate axes. 2) In section 3.2, we discussed that the whitened features in the last hidden layer have more large singular values to avoid feature concentration. It is a well-known phenomenon that highly correlated features can lead to model degradation. We discussed it in section 3.1. We observed network degradation in imbalanced classification and proposed an effective solution. 3) Our method is not directly related to [R1,R2,R3], but we have cited them in the updated version."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission720/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699701880085,
                "cdate": 1699701880085,
                "tmdate": 1699717032523,
                "mdate": 1699717032523,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jodVDa4LIn",
                "forum": "32camXjW25",
                "replyto": "8GjwQtT2Df",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission720/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission720/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xPQZ [2]"
                    },
                    "comment": {
                        "value": "**Q6: Fig.3 provides a confusing analysis based on trace norm of covariance matrices. The right-hand figure shows that the proposed method increases \"instability\", possibly leading to unfavorable training. It is a confusing result and makes it hard to understand the authors' claim. The trace norm is dependent on the feature scale (magnitude) which is less relevant to instability of training. Thus, it seems not to be a proper metric for measuring the instability in this case; the authors should apply the other metric invariant to feature scales.**\n\nA6: 1) In section 3.3, we have discussed that previous work proved that large stochasticity (e.g., unstable) of whitening matrix would cause slow training and degenerated performance. We observe instability in the covariance matrix in imbalanced data sets, and it causes the whitening operation to not converge. Our modules can obtain stable batch statistics to solve the above problems. 2) E refers to the sum of the variances of all channels. We just represent it as the sum of the diagonal elements on the covariance matrix.\n\n\n**Q7: The performance results in Sec.4 are inferior to SOTAs reported, e.g., in [R3]. For fair comparison to the SOTAs, the method should be embedded into the popular backbone networks, e.g., ResNet-50 for ImageNet-LT and iNat18. It is also valuable to check whether discriminative features of deeper backbones behaves in the similar way to Fig.2 or not.**\n\nA7: 1) [R3] uses AutoAugment to train the model, which is unfair comparison. 2) The visualization results in the appendix demonstrate that deeper backbones (ResNet-110, EfficientNet-B0 and DenseNet121, Figures 7-9) and model trained on large-scaled iNaturalist-LT datasset (Figure 10) can obtain high correlated features.\n\n\n**Q8: In p.8: Table ?? -> Table 2**\n\n\nA8: Thanks for your comment. We have revised it in the new version."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission720/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699701934292,
                "cdate": 1699701934292,
                "tmdate": 1699717113905,
                "mdate": 1699717113905,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "C5k9UlC4Kv",
            "forum": "32camXjW25",
            "replyto": "32camXjW25",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission720/Reviewer_piR7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission720/Reviewer_piR7"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the imbalance class problem using DNNs trained end-to-end. It firstly finds that he highly correlated features fed into the classifier is a main factor of the failure of end-to-end training DNNs on imbalanced classification tasks. It thus proposes Whitening-Net, which uses ZCA-based batch whitening to help end-to-end training escape from the degenerate solutions and further proposes two mechanisms to alleviated the potential batch statistic estimation problem of whitening in the class-imbalance situation. Experimental results on the benchmarks CIFAR-LT-10/100, ImageNet-LT and iNaturalist-LT, demonstrate the effectiveness of the proposed approaches."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ This paper is a well-motivated paper and the solution is well-supported. This paper addresses the imbalance class problem using DNNs trained end-to-end, and empirically finding that the highly correlated features fed into the classifier makes the failure of end-to-end training on imbalanced classification. Based on this, it uses batch whitening (termed channel-whitening in this paper) to decorrelate the features before the last linear layer (classifier), and further proposes two mechanisms to alleviated the potential batch statistic estimation problem of whitening in the class-imbalance situation.\n\n+ The imbalance classification problem is common in the learning and vision community, especially in the situation using DNNs. The main line of methods is decoupled training. It is glad to see the proposed end-to-end trained whitening-Net outperformed the decoupled training methods, showing great potentiality.   \n\n+ The presentation of this paper is clear and easy to follow."
                },
                "weaknesses": {
                    "value": "1.The descriptions of this paper should follow the common specification. This paper uses the Batch whitening (whitening over the batch dimension, like its specification, batch normalization (standardization) ) [Huang CVPR 2018, Huang CVPR 2020], but it terms as channel whitening. I understand this paper want to address the \u201cchannel\u201d decorrelation, but the method is commonly said as \u201cbatch\u201d whitening (v.s., batch normalization).   \n\n2.I am not confident to the novelty. Indeed, batch whitening is a general module proposed in [Huang CVPR 2018, Huang CVPR 2020], and is also plugged in before the last linear layer to learn decorrelated representation for normal class distribution. I recognize the novelty of this paper using BW for imbalance classification, but overall, the novelty seems not to be significant.  \n\nOther minors:\n\n-It is better to proofreading the paper. E.g., \u201cre-samplingPouyanfar et al\u201d in Page 2, \u201cAditya et al. Menon et al. (2020) propose\u201d in Page 3. \u201cTable ??, \u201d in Page 8. \n\n-I am not sure this paper whether use a correct reference, e.g., \u201c. Cui et al. Cui et al. (2019)\u201d \u201cCao et al. Cao et al. (2019) \u201d \u2026., Besides, There provide too much reference in the first paragraph, and most of words in the first paragraph is the reference. I personally suggest only preserve the representative references in the first paragraph, and leave the others in the related work for details."
                },
                "questions": {
                    "value": "Well proofreading and responding the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission720/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698330078461,
            "cdate": 1698330078461,
            "tmdate": 1699635999185,
            "mdate": 1699635999185,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1OJRoOc1Bt",
                "forum": "32camXjW25",
                "replyto": "C5k9UlC4Kv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission720/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission720/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer piR7"
                    },
                    "comment": {
                        "value": "Thank you for the review and constructive comments.\n\n\n**Q1: The descriptions of this paper should follow the common specification. This paper uses the Batch whitening (whitening over the batch dimension, like its specification, batch normalization (standardization) ) [Huang CVPR 2018, Huang CVPR 2020], but it terms as channel whitening. I understand this paper want to address the \u201cchannel\u201d decorrelation, but the method is commonly said as \u201cbatch\u201d whitening (v.s., batch normalization).**\n\nA1: Thanks for your comment. \"Batch\" refers to the sample dimension, and we reduce the correlation between channels. We can revise it in the next version. \n\n\n\n**Q2: I am not confident to the novelty. Indeed, batch whitening is a general module proposed in [Huang CVPR 2018, Huang CVPR 2020], and is also plugged in before the last linear layer to learn decorrelated representation for normal class distribution. I recognize the novelty of this paper using BW for imbalance classification, but overall, the novelty seems not to be significant.**\n\nA2: Our contribution is not only to use whitening operation before the classifier. 1) Our extensive experimental results (Fig.2, Fig.6-10) show that the representations learned by neural networks on imbalanced data sets have higher correlations, causing the network to fall into a degenerate solution. We think this is an interesting observation. Reviewer iY3M also agrees that the findings are not limited to solving imbalanced classification problem. 2) We propose to use whitening operation before the classifier to remove the correlation between channels. We find that unstable batch statistics will cause whitening to not converge, so we introduce two covariance correction modules to obtain stable batch statistics and thereby reinforcing the capability of whitening. 3) Extensive experimental results on four imbalanced benchmarks demonstrate effectiveness of our proposed method. 4) The [Huang CVPR 2018] applied Group Whitening to avoid large stochasticity (e.g., unstable) of covariance matrix. In contrast, our proposed biased covariance-corrected modules can get more accurate and stable batch statistics to avoid non-convergence of whitening in imbalanced scenarios. More importantly, if we replace channel whitening with group whitening, the results in the following Table A show that [Huang CVPR 2018] is invalid on imbalanced classification. The results are obtained based on codebase https://github.com/kaidic/LDAM-DRW.\n\n    Method                               |  Accuracy\n    -------------------------------------------------\n    ERM                                  |   66.4\n    Ours                                 |   76.4\n    -------------------------------------------------\n    [Huang CVPR 2018]                    |   66.6\n\n   Table A: Test accuracy on CIFAR-10-LT dataset with imbalance factor 200.\n\n\n**Q3: It is better to proofreading the paper. E.g., \u201cre-samplingPouyanfar et al\u201d in Page 2, \u201cAditya et al. Menon et al. (2020) propose\u201d in Page 3. \u201cTable ??, \u201d in Page 8.**\n\nA3: Thanks for your comments. We have revised them in the new version.\n\n\n**Q4: I am not sure this paper whether use a correct reference, e.g., \u201c. Cui et al. Cui et al. (2019)\u201d \u201cCao et al. Cao et al. (2019) \u201d \u2026., Besides, There provide too much reference in the first paragraph, and most of words in the first paragraph is the reference. I personally suggest only preserve the representative references in the first paragraph, and leave the others in the related work for details.**\n\nA4: Thanks for your detailed review. 1) \"Cui et al. Cui et al. (2019)\" is because we manually wrote the author's name before citing it, which resulted in duplication with the ICLR citation format. 2) We have revised the Introduction section and moved the references to related works."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission720/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699701543733,
                "cdate": 1699701543733,
                "tmdate": 1700156301007,
                "mdate": 1700156301007,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "61UIaRxxHC",
            "forum": "32camXjW25",
            "replyto": "32camXjW25",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission720/Reviewer_iY3M"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission720/Reviewer_iY3M"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses the problem of image classification and claims  two-fold contributions: first it identifies that in imbalanced problems high correlation between features is an indicator of poor performance and secondly it proposes a whitening algorithm to address the problem. The method is evaluated on 4 datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The observation about correlation  is interesting and I believe it has applications beyond this paper\n2. The algorithm proposed is explained clearly although innovation is limited\n3. Evaluation is strong. I appreciated evaluation on a dataset which is naturally imbalanced as evaluation on synthetic sets have limitation in practice."
                },
                "weaknesses": {
                    "value": "1. Some clarification in evaluation would be beneficial: It is not clear to me, given this form of the paper, if the improvement in the performance is on the frequent classes or the ones that have little representation. I believe that is a relevant question especially when focusing  on imbalance problems.\n2. (Minor) Paper needs some revision:\n    - page 8 \"Table ??,\"\n    - what does \"Many\" \"Medium\" \"Few\" \"All\" refer to in table 3"
                },
                "questions": {
                    "value": "Please see Weaknesses\n\n============================\nPost rebuttal comment:\nI have read other reviews and authors response. As mentioned in a message bellow, I appreciated that the issue I have raised has been properly dealt with. Therefore I view the paper as being on the \"acceptable\" side and I am keeping my initial recommendation."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission720/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission720/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission720/Reviewer_iY3M"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission720/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698664910166,
            "cdate": 1698664910166,
            "tmdate": 1700914509923,
            "mdate": 1700914509923,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "E8ZvmsMDGo",
                "forum": "32camXjW25",
                "replyto": "61UIaRxxHC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission720/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission720/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer iY3M"
                    },
                    "comment": {
                        "value": "Thank you for the review and constructive comments.\n\n**Q1: Some clarification in evaluation would be beneficial: It is not clear to me, given this form of the paper, if the improvement in the performance is on the frequent classes or the ones that have little representation. I believe that is a relevant question especially when focusing on imbalance problems.**\n\nA1: The results in Table 3 show that our method can greatly improve the test accuracy of the model on few shot, while ensuring that the test accuracy on many shot does not drop much.\n\n\n**Q2: Paper needs some revision: page 8 \"Table ??,\".**\n\nA2: Thanks for your comment. We have revised it in the new version.\n\n\n**Q3: What does \"Many\" \"Medium\" \"Few\" \"All\" refer to in table 3?**\n\nA3: We follow the description of previous works and divide the categories in the dataset into many shot, medium shot and few shot according to the number of samples. For example, the large-scale ImageNet-LT consists of 115.8K training images from 1000 classes and the number of images per class is decreased from 1280 to 5. Many shots have more than 100 images, medium shots have 20-100 images and few shots have less than 20 images."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission720/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699701206284,
                "cdate": 1699701206284,
                "tmdate": 1699701601119,
                "mdate": 1699701601119,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SnlgdHkZYz",
                "forum": "32camXjW25",
                "replyto": "E8ZvmsMDGo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission720/Reviewer_iY3M"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission720/Reviewer_iY3M"
                ],
                "content": {
                    "title": {
                        "value": "Frequent classes vs in rare classes"
                    },
                    "comment": {
                        "value": "While the answer is appreciated, the question has not been answered. \n\nLet me explain into more detail: The title of the paper contains \"Imbalanced\" Classification. This means that from the total number of classes, some have more examples (frequent classes), some have few examples (rare). The question is: the increase in performance (the proposed method with respect to the baseline) shows an increase in the recognition rate for  frequent classes or for rare one? This question basically requires some data from the confusion matrix\n\nThe provided answer refers to the case when the number on question is about  how many examples from the total available are taken as being labelled. This is a different thing.\n\nThank you."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission720/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700125239229,
                "cdate": 1700125239229,
                "tmdate": 1700125239229,
                "mdate": 1700125239229,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Z6PwAiMTgs",
                "forum": "32camXjW25",
                "replyto": "61UIaRxxHC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission720/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission720/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your feedback!"
                    },
                    "comment": {
                        "value": "Dear Reviewer iY3M,\n\nWe appreciate you for acknowledging our response. We can give more expressions to the remaining concern.\n\na)  We adhere to the approach outlined in [1] for reporting accuracy across three class splits:: Many-shot (more than 100 images), Medium-shot (20-100 images) and Few-shot (less than 20 images). The many-shot classes are the frequent classes you mentioned, while the few-shot classes represent the rare ones.\n\nb) Our previous A1 has actually introduced the issues you are concerned about. For your reference, we provide results in Table B below. In comparison to the baseline method, our approach demonstrates significant improvements in preserving accuracy for many-shot classes while also enhancing accuracy for few-shot classes.\n\n    Method        |  Many-shot     Medium-shot    Few-shot   |   All\n    --------------------------------------------------------------------\n    ERM           |    55.7           45.5          40.6     |   44.6\n    LWS           |    44.3           51.0          52.9     |   51.1\n    Ours          |    49.3           53.4          53.8     |   53.2\n\n                   Table B: Top 1 accuracy on iNaturalist-LT.\n\n\n[1] Liu, Ziwei and Miao, Zhongqi, et al. Large-Scale Long-Tailed Recognition in an Open World.\n\nc) We hope this explanation addresses your concerns. If there are any further comments, please feel free to raise them and we will always be happy to answer your questions.\n\n\nAuthors"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission720/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700142220868,
                "cdate": 1700142220868,
                "tmdate": 1700156246309,
                "mdate": 1700156246309,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MyI6xoOa5I",
                "forum": "32camXjW25",
                "replyto": "Z6PwAiMTgs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission720/Reviewer_iY3M"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission720/Reviewer_iY3M"
                ],
                "content": {
                    "title": {
                        "value": "Frequent classes vs in rare classes"
                    },
                    "comment": {
                        "value": "Thank you for your answer! My question has been answered."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission720/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700143719247,
                "cdate": 1700143719247,
                "tmdate": 1700143719247,
                "mdate": 1700143719247,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "AzCUPTBe2q",
            "forum": "32camXjW25",
            "replyto": "32camXjW25",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission720/Reviewer_bhjW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission720/Reviewer_bhjW"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses imbalanced image classification task with a Whitening-Net. Specifically, the authors first show that the reason of model degeneration lies on the correlation coefficients among sample features before the classifier, i.e., large correlated coefficients lead to model degeneration. Thereby, they proposes to use ZCA whitening before classifier to remove or decrease the correlated coefficients between different samples. For stable training, the also present the Group-based Relatively Balanced Sampler (GRBS) to obtain class-balanced samples, and a covariance-corrected module."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Good results on most of the datasets."
                },
                "weaknesses": {
                    "value": "The novelty is limited. The so-called Whitening-Net is more like a batch normalization before the classifier. \n\nThere are many works that use BN layer before classifier to address imbalanced image classification task. Some are as follows when searching in Google. Please clarify if there are similar or not, and provide some results by BN at least. \n[1] Improving Model Accuracy for Imbalanced Image Classification Tasks by Adding a Final Batch Normalization Layer: An Empirical Study. ICPR,2020.\n[2] Consistent Batch Normalization for Weighted Loss in Imbalanced-Data Environment\n\nThere are some typos, e.g., \u201cAs show in Table ??\u201d in page 8. \"ERM\" is not explained in page 1."
                },
                "questions": {
                    "value": "See the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission720/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission720/Reviewer_bhjW",
                        "ICLR.cc/2024/Conference/Submission720/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission720/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698758174002,
            "cdate": 1698758174002,
            "tmdate": 1700101949875,
            "mdate": 1700101949875,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ADPhXG0cYh",
                "forum": "32camXjW25",
                "replyto": "AzCUPTBe2q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission720/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission720/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer bhjW"
                    },
                    "comment": {
                        "value": "Thank you for the review and constructive comments.\n\n**Q1: The novelty is limited. The so-called Whitening-Net is more like a batch normalization before the classifier.**\n\nA1: Our contribution is not only to use whitening operation before the classifier. 1) Our extensive experimental results (Fig.2, Fig.6-10) show that the representations learned by neural networks on imbalanced data sets have higher correlations, causing the network to fall into a degenerate solution. We think this is an interesting observation. Reviewer iY3M also agrees that the findings are not limited to solving imbalanced classification problem. 2) We propose to use whitening operation before the classifier to remove the correlation between channels. 3) We find that unstable batch statistics will cause whitening to not converge, so we introduce two covariance correction modules to obtain stable batch statistics and thereby reinforcing the capability of whitening. 4) Extensive experimental results on four imbalanced benchmarks demonstrate effectiveness of our proposed method.\n\n\n\n**Q2: There are many works that use BN layer before classifier to address imbalanced image classification task. Some are as follows when searching in Google. Please clarify if there are similar or not, and provide some results by BN at least.**\n\nA2: 1) Paper [1] uses BN after the classifier (Fig.2 of [1]), which is different from us using whitening before the classifier to alleviate degenerate solution. 2) Paper [2] proposes a Weighted Batch Normalization (WBN) by re-weighting the batch statistics to slove the size-inconsistency problem. Their motivations and methods are completely different from ours. 3) The following experimental result (Table A) shows that the model performance is lower than the ERM baseline, after replacing whitening with BN. The results are obtained based on codebase https://github.com/kaidic/LDAM-DRW.\n\n    Method                              |  Accuracy\n    ------------------------------------------------\n    ERM                                 |   38.6\n    Ours                                |   47.2\n    ------------------------------------------------\n    ERM w/ BN                           |   37.4\n\n Table A: Test accuracy on CIFAR-100-LT dataset with imbalance factor 100.\n\n\n\n\n**Q3: There are some typos, e.g., \u201cAs show in Table ??\u201d in page 8. \"ERM\" is not explained in page 1.**\n\nA3: Thanks for your comments. We have revised them in the new version."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission720/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699701096523,
                "cdate": 1699701096523,
                "tmdate": 1700156329500,
                "mdate": 1700156329500,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rhw5Oy9uSu",
                "forum": "32camXjW25",
                "replyto": "AzCUPTBe2q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission720/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission720/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your feedback!"
                    },
                    "comment": {
                        "value": "Dear Reviewer bhjW,\n\n**We appreciate your willingness to improve the rating score.**\n\nWe kindly request your guidance on how we can address any remaining concerns to ensure their resolution. If you have any further suggestions, please don't hesitate to share them with us. We are always open to feedback and value your insights.\n\nThank you for your dedicated efforts in helping us refine this paper.\n\nAuthors"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission720/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700120627013,
                "cdate": 1700120627013,
                "tmdate": 1700120712106,
                "mdate": 1700120712106,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]