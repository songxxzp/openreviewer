[
    {
        "title": "Dissecting Language Models: Machine Unlearning via Selective Pruning"
    },
    {
        "review": {
            "id": "MTC399GUl7",
            "forum": "8SPSIfR2e0",
            "replyto": "8SPSIfR2e0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5824/Reviewer_Q3q4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5824/Reviewer_Q3q4"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents the application of pruning techniques to analyze neuron behavior within large language models. Introducing a method termed \"selective pruning,\" the approach gauges the significance of each neuron based on its importance in both retained and forgotten datasets. From their experimental findings, the paper highlights that: (1) neurons exhibit high specialization, (2) larger models demonstrate more selective tendencies, and (3) feed-forward neurons show greater specialization compared to attention neurons."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper delves into an interesting topic not previously addressed: examining the behaviour of neurons in large language models to comprehend their functionality.\n2. The article introduces a method termed \"selective pruning\" to undertake machine unlearning on large language models, subsequently employing it to analyze neuronal functions.\n3. The research offers intriguing findings from its experiments, suggesting that neurons in FFN hold greater significance than in Attention module for specialized tasks. Such insights could potentially inspire further research and insights into large language models within the community."
                },
                "weaknesses": {
                    "value": "1. The experimental results presented by the author do not fully support the conclusion this paper wants to draw. For example, the authors mentioned in the introduction that, \"If capabilities can be separated on the level of neurons, then this can lead to modularity inside models.\" However, based on the experimental results, it appears that the entire LLM behaves highly in coupling and cannot be separated. For instance, in Figure 1(d), the performance loss on the 'code' dataset seems to be mirrored closely by a performance loss on the 'python' dataset, where the model drops close to the retain dataset and the forget dataset.\n\n2. The paper lacks comprehensive and comparable comparisons with previous methods. The authors only show the results with one baseline method (Task Arithmetic), and the comparison does not seem equitable.  It's unclear from the presented data whether their approach outperforms the baseline method. Using varying scales for the reduction in perplexity complicates the evaluation,  and it's challenging to determine whether a reduction from 4.8 to 0.8 is significant, or if a drop from 2.2 to 0.3 is more significant.\n\n3. Given that the experiments were solely conducted on datasets related to code, I am uncertain about the generalizability of the experimental results presented in the paper. For instance, the conclusion that FFN outperforms attention\u2014might it be possible that a different task could yield an opposing conclusion.\n\n4. The readability of the entire article is not good. For instance, in Section 3.1, the author describes the distribution characteristics of the \"**attention pre-out neuron**\" activations. However, the definition of this unfamiliar term, \"attention pre-out neuron,\" is only introduced in Section 3.3. This leads to confusion for me when initially encountering the term. Additionally, the article frequently places experimental results in the appendices, referencing them in the main text and using the conclusion from these experiments in appendices to support further observation in the main text. This approach disrupts the reading flow, often requiring to flip back and forth for context. While thorough analyses and experiments are commendable, the structure of the article still needs further refinement to enhance its logical flow."
                },
                "questions": {
                    "value": "1. Please answer the questions mentioned in Weaknesses.\n\n2.  In section 3.2: As a baseline we also randomly pruned layers. Where is this baseline?\n\n3. A prior study [1] demonstrated that, depending on the specific input, it's possible to achieve a high pruning ratio without negatively affecting performance and without the need for retraining. This suggests that there is redundancy in the neurons of the LLM when it's tasked with executing a singular function (equating a single sentence to a minor task, for instance). In light of these findings, what novel insights or observations does your paper offer in comparison to that study?\n\n[1] Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5824/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5824/Reviewer_Q3q4",
                        "ICLR.cc/2024/Conference/Submission5824/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5824/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698658016232,
            "cdate": 1698658016232,
            "tmdate": 1700547127595,
            "mdate": 1700547127595,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ycmSws0AOh",
                "forum": "8SPSIfR2e0",
                "replyto": "MTC399GUl7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5824/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5824/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to thank the reviewer for their helpful and thorough review, and for their positive comments. We\u2019re glad you think some of our insights could potentially inspire further research and insights into large language models within the community. We address the concerns raised in the review below.\n\n> 1. However, based on the experimental results, it appears that the entire LLM behaves highly in coupling and cannot be separated. For instance, in Figure 1(d), the performance loss on the 'code' dataset seems to be mirrored closely by a performance loss on the 'python' dataset\n\nWe think that the extent to which different capabilities are coupled or intertwined depends on the capabilities in question. By comparing the results in Figure 2(a) and 2(b) we see that \u2018python\u2019 ability and \u2018code\u2019 ability are more intertwined than \u2018code\u2019 ability and \u2018pile\u2019 (i.e. general text) ability. This is in line with what one might intuitively expect: coding ability in general and coding in the python programming language seem like similar skills. \n\n> 2. Using varying scales for the reduction in perplexity complicates the evaluation, and it's challenging to determine whether a reduction from 4.8 to 0.8 is significant, or if a drop from 2.2 to 0.3 is more significant.\n\nWe agree that it is inconvenient that we were unable to replicate their results. In an attempt to resolve this we have contacted the authors in August, but unfortunately they have not replied.\n\nWe have tried to further vary text generation parameters that were not stated in the paper, and by increasing the temperature and generation length, we managed to rerun the experiment to find pre-intervention toxicity levels that are closer to 4.8. The updated paper now reports a drop in toxic generations from 3.5% -> 0.3%. \n\n> 3. Given that the experiments were solely conducted on datasets related to code, I am uncertain about the generalizability of the experimental results presented in the paper. For instance, the conclusion that FFN outperforms attention\u2014might it be possible that a different task could yield an opposing conclusion.\n\nWe agree that the degree to which attention and feed forward pruning may be specialized to different tasks may differ. Thank you for pointing this out. We have updated the pdf to reflect this.\n\n> 4. The readability of the entire article is not good. For instance, in Section 3.1, the author describes the distribution characteristics of the \"attention pre-out neuron\" activations. However, the definition of this unfamiliar term, \"attention pre-out neuron,\" is only introduced in Section 3.3. This leads to confusion for me when initially encountering the term. \u2026 While thorough analyses and experiments are commendable, the structure of the article still needs further refinement to enhance its logical flow.\n\nThank you for the feedback. We have resolved the issue of using the term \u201cattention pre-out neuron\u201d before we explain the term. We have also made other updates to the paper to improve readability, such as improving the wording and adding a diagram describing our method on page 3. \n\n**Question 1 and Question 2:** Question 1 is addressed in the comments above and question 2 is answered in the common concerns section.\n\n**Question 3:**\n\n> A prior study [1] demonstrated that, depending on the specific input, it's possible to achieve a high pruning ratio without negatively affecting performance and without the need for retraining. \u2026 In light of these findings, what novel insights or observations does your paper offer in comparison to that study?\n\nThis paper is very interesting indeed! We have now cited this paper. Their findings are similar to but more general than the ones in [A] which motivate our importance functions. The paper shows contextual sparsity per input sentence, whereas we show something similar per dataset.\n\nThe paper shows that neurons specialize locally on the scale of tokens and sentences. We show that neurons specialize more globally on the scale of broader tasks and datasets.\n\nIn Figure 1 (now Figure 2) we have now added imagenet data, and we think it is interesting that the three different pairs Pile-Coding, Coding-Python, Imagenet-Bird all have very different amounts of separability. This shows that the separability of neurons is task dependent. \n\n[A] MoEfication: Transformer Feed-forward Layers are Mixtures of Experts, Zhang et al. https://arxiv.org/abs/2110.01786\n\n### **Conclusion**\n\nWe hope the above clarifications and additional results have changed your view on the paper and successfully addressed all the questions and limitations you mentioned. Please let us know if there are other concerns stopping you from increasing your score and recommending acceptance of the paper."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5824/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700175606561,
                "cdate": 1700175606561,
                "tmdate": 1700175606561,
                "mdate": 1700175606561,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ARYRgfZ9Ro",
                "forum": "8SPSIfR2e0",
                "replyto": "ycmSws0AOh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5824/Reviewer_Q3q4"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5824/Reviewer_Q3q4"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your detailed response. I would increase my score to 6."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5824/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700547100985,
                "cdate": 1700547100985,
                "tmdate": 1700547100985,
                "mdate": 1700547100985,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5V3JTm28ff",
            "forum": "8SPSIfR2e0",
            "replyto": "8SPSIfR2e0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5824/Reviewer_sDUD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5824/Reviewer_sDUD"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an unlearning algorithm based on pruning for removing \u2018capabilities\u2019 from pretrained language models. Specifically, they provide different definitions for quantifying the importance of a neuron for a particular dataset based on the value of its activation on that the points in that dataset. Then, they define a \u2018score\u2019 for a neuron as the ratio of importance of that neuron for the retain versus the forget datasets. They prune a chosen percentage of nodes from a ranked list according to this score. Empirically, they experiment in a setting where a general purpose model is caused to forget its \u2018coding ability\u2019 (and the reverse of forgetting all but its coding ability). They also look at a more finegrained scenario of removing python coding ability while retaining the ability to code in other languages (and its reverse). The way that forgetting is measured is by inspecting the relative drop in accuracy on the forget set (relative to the relative drop on the retain set). My understanding is that it is desired according to this evaluation to have a large drop in the forget accuracy without having a large drop in the retain accuracy. They investigate empirically these trade-off curves obtained by their method in different types of language models. Then, on a different task/dataset (where the goal is to remove toxicity from a trained language model), they compare against one baseline and claim that they get similar results."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- the paper studies an interesting and important problem\n- the proposed method is a reasonable idea and well motivated\n- the proposed method is efficient and requires no gradient updates \n- the paper is for the most part well-written (though see some exceptions below)"
                },
                "weaknesses": {
                    "value": "- the related work section is weak, missing a lot of literature both from unlearning and pruning. For example, [A-E] are recent papers on unlearning, with [A, B] particularly related to the methodology of this paper (see References below). I\u2019m less familiar with the sparsity and pruning literature but the authors should conduct a thorough review.\n- the authors compare against only one baseline, and only for one task. Other common baselines include finetuning on the retain set, gradient ascent on the forget set, and comparing against other recent unlearning methods is also important (see the references below)\n- the particular problem setting of unlearning is not clearly defined. What defines successful unlearning here? In section 3 the authors describe the forget set as the \u201ctask or dataset that we aim to reduce performance on\u201d. This isn\u2019t precise enough. How much do we want to reduce performance? Is it that the greater the reduction, the better? For context, unlearning papers (e.g. Golatkar et al, which the authors cite) usually consider that the accuracy on the forget set should be reduced only up to a reference point and no further (where the reference point is given by the oracle unlearning algorithm of retraining the model from scratch without the forget set). Is the setup here different, and if so, how is the goal defined in this case?\n- [clarity] It seems that \u2018task\u2019 and \u2018dataset\u2019 are used interchangeably in the paper which causes confusion. For example, \u2018the task that we are optimizing for as the retain dataset\u2019. To me, a \u2018task\u2019 includes a particular training objective whereas \u2018dataset\u2019 refers to raw data.\n- [clarity] fundamental problem setting details are missing. For example, were Pile and Code included in the dataset that the various language models were trained on? If not, the terms \u201cforgetting\u201d or \u201cunlearning\u201d may be ill-suited for this application (as they usually refer to forget parts of the training dataset). At the very least, the problem setting targeted by this paper should be clearly defined. \n- [soundness]: can decreased performance (accuracy, perplexity) on a particular dataset support claims of removal of a capability? Generally, \u2018capability removal\u2019 is not precisely defined.\n- [soundness]: when it comes to forgetting or unlearning, several metrics have been proposed by the community to measure this. Simply inspecting the accuracy / perplexity is likely a poor proxy for forgetting quality. For example, Membership Inference Attacks are an important category of methods (see e.g. Golatkar et al and reference [E] below). Are these not applicable here. If not then why not?\n- [presentation, soundness] the authors use the term \u201cselective\u201d to describe an unlearning method, without defining this term clearly in this context. My understanding of what \u201cselective\u201d means in this context is the ability of reducing accuracy / performance on the forget dataset without (really) damaging the accuracy / performance on the retain dataset. If my understanding is correct, I don\u2019t agree with the claim that Figure 1a shows that the larger the model, the more selective it is. I can see this being true for the Opt family but not the Pythia family, for example.\n- [presentation] In the paragraph under Definition 1, the authors give intuition for the different influence functions but they omit I_{abs}. Please add.\n- [presentation] Above Table 1, the authors describe the models they use (which correspond to columns in Table 1) but they omit Roberta. Please add.\n- [presentation] \u201cAs a baseline, we also randomly pruned layers\u201d \u2013 the authors claim this but I don\u2019t see this baseline in their experiments (at least in the main paper), unless I\u2019m missing it. \n- [empirical results] It would greatly strengthen the paper to conduct analyses of: the effect of the number of pruning iterations, the effect of the % pruned (in each iteration or overall), the effect of the choice of the influence function. It sounds like the authors have some results on some of these in the Appendix but not all of them. It would also be great to summarize in the main paper all of the findings (so that one doesn\u2019t need to read the entire Appendix). \n- [empirical results] please include confidence intervals in all tables and e.g. in Figure 3. It is currently challenging to tell if the differences are significant\n- [empirical results] why is there such a large difference between \u201cBase (quoted)\u201d and \u201cBase (replicated)\u201d in Table 3? This makes me concerned about whether \u201cTask Arithmetic (quoted)\u201d and \u201cPruned\u201d are comparable.\n\nReferences\n=========\n- [A] Fast Machine Unlearning Without Retraining Through Selective Synaptic Dampening. Foster et al 2023.\n- [B] Model Sparsity Can Simplify Machine Unlearning. Jia et al. NeurIPS 2023.\n- [C] Unrolling SGD: Understanding Factors Influencing Machine Unlearning. Thudi et al.\n- [D] Prompt Certified Machine Unlearning with Randomized Gradient Smoothing and Quantization. Zhang et al. NeurIPS 2022.\n- [E] Towards Unbounded Machine Unlearning. Kurmanji et al. NeurIPS 2023."
                },
                "questions": {
                    "value": "- the authors claim that their method is specifically designed for LLMs but it\u2019s unclear to me why that\u2019s the case. Can\u2019t this method be applied out of the box e.g. to vision transformers? If not, then why not?\n- In Section 3.1, the authors discuss some observations about the distributions of activations that motivated their use of importance functions. Which dataset / setting were these observations made in? Have the authors made an effort to confirm that they are generalizable beyond a certain setting?\n- I don\u2019t really understand the statement that \u201cWe also do not want to directly modify specific dimensions of the embedding space\u201d. Did not understand the provided rationale. If it\u2019s not the right level of granularity for pruning, as the authors claim, would the proposed scoring function not capture this? If not, does this suggest we need to design better scoring functions?\n- In the Discussion, the authors hypothesize that their method is more likely to \u201cactually remove the undesired behaviour\u201d compared to other unlearning methods. Similarly, in the Broader Impacts section, they claim that (compared to other unlearning methods) their method is unlikely to generate systems that are more harmful than the base model. What is the evidence used for making these claims?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5824/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698676156164,
            "cdate": 1698676156164,
            "tmdate": 1699636614546,
            "mdate": 1699636614546,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "55VND6AE3C",
                "forum": "8SPSIfR2e0",
                "replyto": "5V3JTm28ff",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5824/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5824/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their detailed and helpful review, and for their positive comments. We\u2019re glad you find we study an interesting and important problem and that the proposed method is well motivated, efficient and requires no gradient updates. \n\n> [A-E] are recent papers on unlearning\n\n> Can\u2019t this method be applied out of the box e.g. to vision transformers?\n\n[A] This Arxiv paper from August 15th is related and interesting!\nIn a quick trial we applied our method to the Vision Transformer (ViT) mushroom (MR) row in their Table 2. On the test split (100 datapoints) of CIFAR100 we find a forget drop from 72\u00b18% to 0\u00b13% and a retain drop from 90.0\u00b10.3% to 89.5\u00b10.3%.\n[A-E] Cited.\n\n> Common baselines include finetuning on the retain set, gradient ascent on the forget set\n\nReference [B] states that fine-tuning and gradient ascent based machine unlearning methods cost between 2% and 6% the cost of retraining a model. For reference, training Llama-2-70b used 1.7million GPU hours.\n\n> How much do we want to reduce performance? \n\nWhile we agree that retraining is a good standard, retraining an LLM is too expensive. We simply try to maximise drop in forget while keeping performance in retain, seen with a \"curvier\" graph being better. We do not insist on a specific goal, but make a cheap reference, and allow the user to decide where they would like to be on the trade-off between a high retain & low forget accuracy.\n\n> Were Pile and Code included in the dataset that the various language models were trained on? If not, the terms \u201cforgetting\u201d or \u201cunlearning\u201d may be ill-suited for this application. \n\nIt's uncertain which specific data points were used in the training of all models (e.g., LLaMA 2). Some models, such as Pythia and OPT, do incorporate parts of the Pile dataset. \n\nThe terms 'retain' and 'forget' are existing terminologies that mostly capture the meaning we want. Though it may slightly deviate from normal usage, inventing new terminology would likely be more confusing. Rather than referring to the forgetting of specific instances encountered during training, we're focusing on the loss or retention of underlying skills as exemplified by a dataset.\nOur goal is to remove harmful behavior as exemplified by a (forget) dataset. For example, GPT might not have seen \u201cTo make a bomb you need x, which you can get at location y\u201d, but the model may still output such strings.\n\n> Membership Inference Attacks are an important category of methods...\n\nIn this paper we have tried to show that harmful behavior can be reduced by applying selective pruning. We are less focused on removing specific datapoints from the model than we are in removing the model\u2019s ability to generate code or toxic data. Studying membership inference attacks (which requires full knowledge of training data) is beyond the scope of our current analysis. \n\n> I don\u2019t agree \u2026 the larger the model, the more selective it is.\n\nThank you for pointing this out. We agree and we have updated the pdf to reflect this.\n\n> [presentation] \u2026 I_{abs} \u2026 Table 1,\n\nThanks! Added.\n\n> It would greatly strengthen the paper to conduct analyses of: the effect of the number of pruning iterations, the effect of the % pruned \n(in each iteration or overall), the effect of the choice of the influence function.\n\nWe have chosen not to do a grid-search for the hyperparameter \u201cpercentage pruned per iteration\u201d, but instead report on all the hyper-parameters that we tried.\nThe effect of how much has been pruned overall can be seen in Figure 2. Every dot here reflects another pruning step. The more pruning steps the bigger the reduction in accuracy.\nFigure 4 shows the effect of the choice of influence function.\n\n> please include confidence intervals \n\nIn our tables, we apply the 'significant digits' convention, where we round our values to the most relevant figure and omit digits that represent a substantial error margin. We have taken the suggestion to add error bars to Figure 4.   \n\n> Which dataset / setting were these observations made in? \n\nThese were taken using OPT on the Pile dataset, we have verified the observations for Pythia and LLaMA.\n\n> \u201cWe also do not want to directly modify specific dimensions of the embedding space\u201d \n\nAfter every layer, the model writes into the residual stream which has the same dimension as the embedding space (and which at the last layer turns into the model output after the unembedding). Investigating how to alter these dimensions that are being continuously read from and written into would be an interesting investigation, but is beyond the scope of this paper.\n\n> In the Discussion, the authors hypothesize that their method is more likely to \u201cactually remove the undesired behaviour\u201d compared to other unlearning methods...\n\nWe compare to other model control methods more broadly, many of which have been shown to be brittle under jailbreaking and finetuning. We hypothesize that (our) machine unlearning methods are less brittle than these control methods."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5824/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700175188981,
                "cdate": 1700175188981,
                "tmdate": 1700175188981,
                "mdate": 1700175188981,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Z6mpvEP4Ek",
                "forum": "8SPSIfR2e0",
                "replyto": "55VND6AE3C",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5824/Reviewer_sDUD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5824/Reviewer_sDUD"
                ],
                "content": {
                    "title": {
                        "value": "thank you for the responses"
                    },
                    "comment": {
                        "value": "Hi authors,\n\nThank you for your responses! here are some additional thoughts after reading the rebuttal and other reviews:\n\nre: \"We simply try to maximise drop in forget while keeping performance in retain, seen with a \"curvier\" graph being better. We do not insist on a specific goal, ...\" -- it's not clear to me what sort of application this setup targets exactly. In the intro, the authors motivate unlearning by the need to protect against misuse or misalignment of LLMs or remove sensitive user information. Without a clear reference point for e.g. how much the accuracy should drop on a \"task\", it seems impossible to assess if these desiderata can be reached. For example, when it comes to protecting user privacy, there is discussion in the literature that reducing the accuracy on such user data 'too much' might 'backfire' and make those data points more vulnerable to e.g. membership inference attacks (which the authors argue in their rebuttal is beyond the scope of their exploration). I think it would be useful for the authors to specifically motivate their problem setup by some realistic scenario and adopt metrics that reflect that. \n\nre: other baselines: i don't think it's valid to dismiss comparing against any other baselines or methods from the unlearning literature due to claims that they are too computationally expensive. These comparisons could be run, for instance, on smaller datasets (e.g. the vision dataset included in the rebuttal -- which btw is a great addition!). Generally, comparing to prior work is a crucial step to understand how the proposed method differs and inform practitioners when they should use it over other methods.\n\nre: \"We compare to other model control methods more broadly, many of which have been shown to be brittle under jailbreaking and finetuning\" -- could you please include some reference for this? in addition, i'm curious about this hypothesis that the proposed method is less brittle. what is the reasoning and evidence there?\n\noverall, I find this work really interesting and the proposed method seems promising. but I feel that another round of reviews is needed to clarify the motivation / application scenario, and expand the experimental section to include comparisons to previous works."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5824/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700586863326,
                "cdate": 1700586863326,
                "tmdate": 1700586863326,
                "mdate": 1700586863326,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "z9KsZ0NjmC",
            "forum": "8SPSIfR2e0",
            "replyto": "8SPSIfR2e0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5824/Reviewer_9FMg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5824/Reviewer_9FMg"
            ],
            "content": {
                "summary": {
                    "value": "This work presents a novel selective pruning method in order to allow trained models to 'unlearn' specific capabilities. Related work and the details of the method are explicated. Experiments on different models with different sizes are presented on two data splits (code/pile and code/python) showing generally good performance at 'unlearning' the forgetting dataset and retaining the other. Analysis experiments compare the efficacy of pruning feed-forwards vs attention neurons. Experiments on toxicity show good results for the method on allowing the unlearning of toxic text."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is clearly written. The experiments are thorough, and the presented results convincingly demonstrate the utility of the method. The results on toxicity are particularly nice, as this is a highly relevant domain for which related techniques are well-motivated. The presented method is novel."
                },
                "weaknesses": {
                    "value": "In Limitations: \"Our method can only be applied to remove a capability when that capability is neatly captured by a\ndataset.\" The authors rightly point out that this method of evaluation of unle'arning is dependent upon dataset-level perplexity. The extent to which this metric for any constructed dataset sufficiently \"neatly captures\" whatever capability is desired to be forgotten is difficult to asses without further analysis not presented in this work. It may be the case that for the practical scenarios which motivate the method in the first place, no such \"neat\" dataset is possible to produce. It is fine for this analysis to be out off scope of this work.\n\nGiven this evaluative dependence upon specific datasets, it would significantly strengthen the results of the paper to present more diverse experiments. While it is nice to have many different models at many different sizes, there does not seem to be much addition knowledge gleaned from that diversity, whereas having more tasks would demonstrate a broader efficacy across a more speculative dimension. \n\nIn this paper, the authors present experiments pruning either the feed-forward or the attention blocks, and leave \"Embedding, Positional\nEmbedding and Output Unembedding unmodified.\" (3.3). This is a significant constriction of the application of the method which is not more than intuitively justified. Further experiments, even just to show that this restriction is well-motivated, would strengthen the work."
                },
                "questions": {
                    "value": "See weaknesses.\n\nIn Table 3: it would help readability to label \"Task Arithmetic (quoted)\" as being a finetuning task. Also, the gap between the baseline (quoted) and (replicated) is fairly large, which makes actually comparing the finetuning vs presented pruning method difficuly. Would it be possible to replicate the task-arithmetic finetuning? That would significantly improve the comparability of these results.\n\n\nsmall issue:\nThe details of iterative pruning are not specified. It seems like it would be important to the function of the method to set the proportion of nodes pruned per iteration well, but this is not discussed. Even if this is not and important hyper-parameter of the method, a clearer explanation of the iterative pruning method is necessary to fully elucidate the applied method.\n\n6.3: If an LLM were trained not to answer questions about a dangerous topic, say, bomb-building, could the presented method not be used to unlearn that guardrail? I don't think this is a concern specific to the presented method, but I do not follow why this method is less likely to generate harmful systems than other methods. Can the authors clarify?\n\nNit:\nin discussion: hypothesise -> hypothesize"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5824/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5824/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5824/Reviewer_9FMg"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5824/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698822401760,
            "cdate": 1698822401760,
            "tmdate": 1699636614429,
            "mdate": 1699636614429,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Nb4Q9IUjNl",
                "forum": "8SPSIfR2e0",
                "replyto": "z9KsZ0NjmC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5824/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5824/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to thank the reviewer for their detailed review, and for their positive comments. We\u2019re glad you find the presented method novel and our experiments  thorough, and that the presented results convincingly demonstrate the utility of the method. We are also happy you found toxicity removal a highly relevant domain. We address the concerns raised in the review below.\n\n> Given this evaluative dependence upon specific datasets, it would significantly strengthen the results of the paper to present more diverse experiments. While it is nice to have many different models at many different sizes, there does not seem to be much additional knowledge gleaned from that diversity, whereas having more tasks would demonstrate a broader efficacy across a more speculative dimension.\n\nWe have now included results for an image classification task. Please see them in the updated version of Figure 1 (now moved to be Figure 2) of the updated pdf.\n\n> In this paper, the authors present experiments pruning either the feed-forward or the attention blocks, and leave \"Embedding, Positional Embedding and Output Unembedding unmodified.\" (3.3). This is a significant constriction of the application of the method which is not more than intuitively justified.\n\nThe Embedding, Positional Embedding and Output Unembedding are one layer / weight matrix each. Excluding three layers from pruning out of the 30-100 layers that LLMs typically have seems like a minor constriction. The embedding and output unembedding exist to translate from the input (or output) tokens to an internal model representation and almost function like a dictionary. In LLama the positional embedding is not even learned, but is instead hard-coded. This positional embeddings seem like they would be essential for most tasks that one might think of. \n\n> In Table 3: it would help readability to label \"Task Arithmetic (quoted)\" as being a finetuning task.\n\nThank you for the suggestion! We have updated the label.\n\n> The details of iterative pruning are not specified. It seems like it would be important to the function of the method to set the proportion of nodes pruned per iteration well, but this is not discussed.\n\nWe have added some clarification in Section 3.2 in the updated pdf. We have not done any hyper-parameter tuning for the proportion of nodes pruned per iteration. \n\n> 6.3: If an LLM were trained not to answer questions about a dangerous topic, say, bomb-building, could the presented method not be used to unlearn that guardrail? I don't think this is a concern specific to the presented method, but I do not follow why this method is less likely to generate harmful systems than other methods. Can the authors clarify?\n\nSomething that was not clear in our original formulation is that we think machine unlearning methods (such as ours) will be more thorough than other model control methods.\n\nFor example, RLHF (a control method) guard rails are fragile to things like jailbreaking [1] and finetuning [2]. This is possible because the information about how to build a bomb is still in the model. This method is an attempt to remove much of that information so that even if one unlearns the guard rails, one can still not learn how to make the bomb.\n\n[1] Deep reinforcement learning from human preferences, Christiano et al. https://arxiv.org/abs/1706.03741\n\n[2] LoRA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B, Lermen et al. https://arxiv.org/abs/2310.20624 \n\n### **Conclusion**\n\nWe hope the above clarifications and additional results have changed your view on the paper and successfully addressed all the questions and limitations you mentioned. Please let us know if there are other concerns stopping you from increasing your score."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5824/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700174350469,
                "cdate": 1700174350469,
                "tmdate": 1700174350469,
                "mdate": 1700174350469,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "w8ESNORUvA",
            "forum": "8SPSIfR2e0",
            "replyto": "8SPSIfR2e0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5824/Reviewer_q42J"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5824/Reviewer_q42J"
            ],
            "content": {
                "summary": {
                    "value": "This article presented an approach for the targeted removal of neurons, which is based on their comparative significance across two datasets. This technique of machine unlearning demonstrates its effectiveness through the quantifiable decrease in accuracy differentials and perplexity measurements. Additionally, it establishes a cost-effective foundation for forthcoming research comparisons. Its theory posits that the approach is more inclined to eliminate undesired model behaviors, as opposed to merely concealing them, in contrast to fine-tuning.\nThis approach is a compute- and data-efficient method for identifying and removing neurons that enable specific behaviours. The findings of this method reveals that both feed-forward and attention neurons in LLMs are specialized; that is, for specific tasks, certain neurons are more crucial than others."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1- The model presented here tackles an intriguing and complex issue within large language models (LLMs), focusing on the significance scores assigned to individual neurons with respect to a specific target dataset.\n\n2- The concept of machine unlearning is implemented across both types of neural networks, including feedforward and attention-layer-based networks. By eliminating unwanted neurons for any given target dataset, the process is swift and leads to a reduction in the network's computational load.\n\n3- The efficacy of the unlearning process is demonstrated through experiments conducted on three distinct datasets."
                },
                "weaknesses": {
                    "value": "1- The proposed model can effectively eliminate the information captured by the target dataset. However, it is unable to unlearn knowledge that lies beyond the representation of the datasets. Please provide a clear justification.\n\n2- Including a visual representation of the suggested concept could enhance comprehension. Thus, kindly incorporate a diagram that presents a general outline of the proposed concept.\n\n3- The evaluation of the proposed model focuses on text datasets, yet the experiments for the image dataset are absent. Implementing machine unlearning for the image dataset would be highly beneficial."
                },
                "questions": {
                    "value": "Please address all the question raised in the weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5824/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698865285067,
            "cdate": 1698865285067,
            "tmdate": 1699636614299,
            "mdate": 1699636614299,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5TuzHVRC8x",
                "forum": "8SPSIfR2e0",
                "replyto": "w8ESNORUvA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5824/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5824/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to thank the reviewer for their helpful review, and for their positive comments. We\u2019re glad you found the goal of removing capabilities from LLMs an intriguing and complex issue, and our method a cost-effective foundation for forthcoming research comparisons, the efficacy of which was demonstrated through experiments on three distinct datasets. We address the concerns raised in the review below.\n\n> 1- The proposed model can effectively eliminate the information captured by the target dataset. However, it is unable to unlearn knowledge that lies beyond the representation of the datasets. Please provide a clear justification.\n\nYes we think this is an important question! However, we think that the general question of capturing (un)desired behaviour in a dataset is one of the key problems in machine learning. It is outside the scope of this paper to solve this problem here.\n\n> 2- Including a visual representation of the suggested concept could enhance comprehension. Thus, kindly incorporate a diagram that presents a general outline of the proposed concept.\n\nWe have now added a visual representation (new Figure 1) of selective pruning to Section 3.2 on page 3 of our updated pdf.\n\n> 3- The evaluation of the proposed model focuses on text datasets, yet the experiments for the image dataset are absent. Implementing machine unlearning for the image dataset would be highly beneficial.\n\nWe have taken your suggestion to implement selective pruning for an image dataset on board! Please see our updated Figure 1 (now Figure 2) in the updated pdf.\n\n### **Conclusion**\n\nWe hope the above clarifications and additional results have changed your view on the paper and successfully addressed all the questions and limitations you mentioned. Please let us know if there are other concerns stopping you from increasing your score."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5824/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700174184492,
                "cdate": 1700174184492,
                "tmdate": 1700174184492,
                "mdate": 1700174184492,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]