[
    {
        "title": "Real-Fake: Effective Training Data Synthesis Through Distribution Matching"
    },
    {
        "review": {
            "id": "3OUpbqV6EG",
            "forum": "svIdLLZpsA",
            "replyto": "svIdLLZpsA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1261/Reviewer_wBAQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1261/Reviewer_wBAQ"
            ],
            "content": {
                "summary": {
                    "value": "The authors introduce a distribution matching (MMD) framework and implements it within the Stable Diffusion to improve data synthesis. In particular, feature distribution alignment, conditional visual guidance and latent prior initialization are introduced to finetune the Stable Diffusion.  Experiments demonstrate the effectiveness of their synthetic data across image classification, OOD generalization and privacy preservation tasks."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is easy to follow.\n2. Experiments on various tasks are provided and the results appear promising."
                },
                "weaknesses": {
                    "value": "1. The overall idea of Maximum Mean Discrepancy (MMD) for learning data distribution is not novel, which has been explored before, like [1].   \n2. The derived MMD loss based on diffusion model (Eq. (7)) is the same as the simplified diffusion model loss (Ho et al. 2020). This means the proposed loss does not bring anything new for the current diffusion model. \n3. The effectiveness of the proposed MMD loss (Eq. (7)) is doubtful. From Table 2, the performance gain brought by MMD is not significant compared to finetuning only. \n4. The promising performance gains are significantly attributed to the latent prior (Table 2). Given that the latent prior improvement is not introduced by this work but has been already explored by the literature (Section 3.3), the contribution of this work is marginal.\n\nMinor:\n1. $\\epsilon_0$ is not defined in Eq. (7)."
                },
                "questions": {
                    "value": "See the weakness above. Other questions:\n\n1. For the finetune only in Table 2, does it mean finetuning the Stable Diffusion with original losses on the target datasets? \n2. For the latent prior only in Table 2, does it mean simply adopting the proposed prior for the pretrained Stable Diffusion to synthesiz the target images?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1261/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1261/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1261/Reviewer_wBAQ"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1261/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698753739913,
            "cdate": 1698753739913,
            "tmdate": 1700487117257,
            "mdate": 1700487117257,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JvmcEeXGah",
                "forum": "svIdLLZpsA",
                "replyto": "3OUpbqV6EG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1261/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1261/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer wBAQ,\n\n\n\n\nThank you for your valuable feedback and constructive comments. We appreciate you recognizing our work is easy to follow and results appear promising. We now address your concerns and questions below.\n\n\n\n\n**Q1: Maximum Mean Discrepancy (MMD) for learning data distribution is not novel, which has been explored before.**\n\n\n**A1:** We kindly note the main idea of our work is not leveraging MMD loss as a standalone technique for learning data distribution but to formulate the training data synthesis as a distribution matching problem (Eq. 3 and Eq. 4). Our specific implementation of this framework on latent diffusion model involves using distribution matching loss (MMD) **along with latent prior initialization and conditional visual guidance**, integrated in Stable Diffusion for synthesizing informative training data. The significant performance improvements brought by our method empirically indicate the effectiveness of the proposed framework. We have also **validated the importance of all three components** of our method. We have revised our manuscript to emphasize and clarify our main idea.\n\n\n\n\n**Q2: The derived MMD loss based on diffusion model (Eq. (7)) is the same as the simplified diffusion model loss (Ho et al. 2020).**\n\n\n**A2:** We would like to clarify that our derived MMD loss is **not** identical to the diffusion model loss presented by Ho et al. (2020). Their simplified diffusion model loss is a mean square error calculated on top of the predicted noise and added noise whereas the MMD loss, Eq. (7), is the L2-norm of the averaged difference between the predicted noise and added noise. \n\n\n$$\nL_{DM} := ||\\frac{1}{|N|} \\sum_{i=1}^{|N|} (\\epsilon - \\epsilon_{\\theta}(x_t, t))||^2 \\leq \n\\frac{1}{|N|} \\sum_{i=1}^{|N|} ||(\\epsilon - \\epsilon_{\\theta}(x_t, t))||^2 = L_{Diffusion}.\n$$\n\n\n\n\n\n\n**The diffusion model loss is an upper bound of the MMD loss** by Jensen\u2019s inequality. The derived MMD loss is thus a tighter bound on the distribution discrepancy between real and synthetic data under the MMD measure. We have revised the relevant sections to better clarify this distinction.\n\n\n\n\n[1] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems 33:6840\u20136851, 2020.\n\n\n\n\n**Q3: From Table 2, the performance gain brought by MMD is not significant compared to finetuning only.**\n\n\n**A3:** As mentioned above, we show that the diffusion loss (Ho et al. 2020) is an upper bound of the MMD loss. Therefore, finetuning with the diffusion model loss still aligns with our distribution matching framework for training data synthesis. Our motivation (Section 3.1) to further include MMD loss rather than simply diffusion model loss is due to the known looseness problem of diffusion loss (Kingma et al. 2021). The results in Table 2 also show that the extra MMD loss can bring notable improvement in some settings, which are selected in the following table for illustration. MMD loss leads to consistent improvements of 0.6/1.1%, and 1.0/0.7%  on ImageNette/ImageNet100, on top of vanilla diffusion loss, accordingly. We revised our manuscript to clarify it and provided further analysis in Appendix G.5.\n\n\n\n\n| Latent Prior | Visual Guidance | Distribution Matching | Finetune | ImageNette | ImageNet100 |\n|--------------|-----------------|-----------------------|----------|------------|-------------|\n|              | \u2713               |                       | \u2713        | 82.3       | 74.0        |\n|              | \u2713               | \u2713                     | \u2713        | 82.9       | 75.1        |\n| -  |         -        |          -             |     -     | $\\Delta$ + 0.6 | $\\Delta$ + 1.1 |\n| \u2713            | \u2713               |                       | \u2713        | 89.5       | 79.3        |\n| \u2713            | \u2713               | \u2713                     | \u2713        | 90.5       | 80.0        |\n|     -             |        -         |             -          |     -     | $\\Delta$ + 1.0 | $\\Delta$+ 0.7 |\n\n\n\n\n[2] Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. Advances\nin neural information processing systems, 34:21696\u201321707, 2021."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1261/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700136470650,
                "cdate": 1700136470650,
                "tmdate": 1700136470650,
                "mdate": 1700136470650,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9MYdplFBIN",
                "forum": "svIdLLZpsA",
                "replyto": "3OUpbqV6EG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1261/Reviewer_wBAQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1261/Reviewer_wBAQ"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the reponses."
                    },
                    "comment": {
                        "value": "1. I think it is trivial to formulate training data synthesis as a problem of learning data distribution, in particular a joint data distribution $q(x, y)$. This type of joint data distribution learning has been well established in the literature, e.g., [1]. The author emphasizes the significance of this contribution, with which I do not agree. Implementing the joint distribution learning through feature distribution alignment and conditional distribution alignment is a standard procedure, not novel.\n\n2. From the description in Section 3.3, the component of latent prior initialization is not derived from the joint distribution learning framework but rather from the literature on the diffusion model. Therefore, I am not convinced by the claim in A4 that the proposed approach \u201cintegrates latent prior within a theoretical framework, offering a novel perspective in this field\u201d.\n\n3. I was misled by Eq. (21) before, erroneously thinking that the work employed the upper bound of $L_{DM}$ as the objective for DM. Thanks for the clarification in A2. It is clear to me now. I suggest polishing Eq. (21) to avoid confusion.\n\n4. The superiority of MMD loss over original diffusion model loss is not well verified. The authors claim that \"Our motivation (Section 3.1) to further include MMD loss rather than simply diffusion model loss is due to the known looseness problem of diffusion loss (Kingma et al. 2021).\" Then a comparison of the MMD loss solely with the simplied diffusion model loss should be included.\n\n5. Given the responses, now I believe deriving MMD loss under the diffusion model introduces some novelty. The authors may consider re-organize the work for delivering the contributions of this work. I think the current emphasis on a principled theoretical framework from the joint distribution matching perspective overstate the contributions of the work.\n\n\n[1] Li, C., Xu, T., Zhu, J., & Zhang, B. (2017). Triple generative adversarial nets. Advances in neural information processing systems, 30."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1261/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700210967176,
                "cdate": 1700210967176,
                "tmdate": 1700223745501,
                "mdate": 1700223745501,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sRShpc6Yyx",
                "forum": "svIdLLZpsA",
                "replyto": "ee0uAFm9SZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1261/Reviewer_wBAQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1261/Reviewer_wBAQ"
                ],
                "content": {
                    "title": {
                        "value": "I increase the score, but still not vote for acceptance."
                    },
                    "comment": {
                        "value": "I increase the score for the novelty of MMD loss under the context of diffusion models. However, I still maintain my opinions regarding the points #1, 2, 4, 5 in my last responses."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1261/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700488079410,
                "cdate": 1700488079410,
                "tmdate": 1700488079410,
                "mdate": 1700488079410,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "614G38pyQf",
            "forum": "svIdLLZpsA",
            "replyto": "svIdLLZpsA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1261/Reviewer_BpUK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1261/Reviewer_BpUK"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a principled theoretical framework for training data synthesis from a distribution matching perspective. The key idea is to frame training data synthesis as matching the joint distribution of data and labels between real and synthetic datasets. This involves two aspects: (1) matching the marginal data distribution, and (2) matching the conditional label distribution given data.\n\nBased on this framework, the authors improve upon the state-of-the-art Stable Diffusion model for image synthesis through three strategies: (1) adding an MMD loss to better align feature distributions, (2) incorporating visual cues along with text prompts for conditioned sampling, and (3) using a informative prior for latent diffusion sampling.\n\nExperiments demonstrate improved image classification performance using the proposed synthetic data, outperforming previous methods. Advantages are shown in replacing real data, augmenting real data, scaling up synthetic data, and for OOD generalization and privacy preservation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The paper offers a theoretical perspective on training data synthesis, formalizing the objective as distribution matching between real and synthetic datasets. The framework clearly decomposes the overall problem into marginal data distribution matching and conditional label distribution matching. This exposes the different requirements.\n* The proposed improvements are well-motivated based on aligning with the principles of the theoretical framework. \n* Thorough experiments validate the utility of the proposed synthetic data on diverse image classification benchmarks. Gains are shown over state-of-the-art baselines. The work highlights promising advantages of synthetic data for OOD generalization and privacy preservation."
                },
                "weaknesses": {
                    "value": "* The improvment over Imagen seems limited. The comparisons between the proposed method and Imagen are limited to ImageNet-1K, making the superiority unclear on other datasets. \n\n* This work utilizes the prompt in format of \"photo of [classname], [Image Caption], [Intra-class Visual Guidance]\". Are there any other comparative methods that also use the same \"[Image Caption]\"? Whether the improvment are mainly from \"[Image Caption]\" instead of \"[Intra-class Visual Guidance]\"?"
                },
                "questions": {
                    "value": "see weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1261/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698811469746,
            "cdate": 1698811469746,
            "tmdate": 1699636052688,
            "mdate": 1699636052688,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xbRyUqaYi5",
                "forum": "svIdLLZpsA",
                "replyto": "614G38pyQf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1261/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1261/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer BpUK,\n\n\nThank you for your insightful comments on our paper. We appreciate that you consider our work as a well-motivated theoretical framework, and recognize the comprehensive experiments with gains over the state of the art and promising advantages for OOD generalization and privacy preservation. We now address your concerns below.\n\n\n\n\n**Q1: Improvement over Imagen is limited. Only the comparison on ImageNet-1K is provided.**\n\n\n**A1:** Thank you for raising this point regarding the comparison to Imagen results (Azizi et al. 2023). We would like to kindly highlight several key aspects to consider Imagen (Saharia et al. 2022) in this context. \n\n\nFirstly, Imagen is a **closed-source** model trained on **private data**, which makes it impossible for us to obtain more experiment results of Imagen to compare except their reported ones on ImageNet-1K dataset in (Azizi et al. 2023). \n\n\nSecondly, the Imagen, trained on **private data**, involves a **more powerful generative model than Stable Diffusion** and is **fine-tuned with full parameters** on ImageNet-1K, which is computationally unaffordable by general researchers. In contrast, our approach leverages the open-source Stable Diffusion model coupled with a low-budget tuning (LoRA) and synthesis framework. \n\n\nThirdly, despite the resource constraints, our method still achieves **notable performance improvements (1.7%)** on the very challenging large-scale dataset ImageNet-1K, underlining the effectiveness of our techniques. \n\n\nLast but not least, with a fair comparison with other baselines using the same generative model backbone, Stable Diffusion, our synthesis framework brings significant (10% ~ 35%) performance improvements on other datasets. \n\n\n\n\n[1] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35:36479\u201336494, 2022\n\n\n[2] Shekoofeh Azizi, Simon Kornblith, Chitwan Saharia, Mohammad Norouzi, and David J Fleet. Synthetic data from diffusion models improves imagenet classification. arXiv preprint arXiv:2304.08466, 2023\n\n\n\n\n**Q2: Are there any other comparative methods that also use the same \"[Image Caption]\"? Whether the improvement are mainly from \"[Image Caption]\" instead of \"[Intra-class Visual Guidance]\"?**\n\n\n**A2:** Thanks for the question. Actually, the CiP (Lei et al. 2023) method generates images based on the \"[Image Caption]\". Specifically, they use the template \u201cA photo of {class name}, {image caption}\u201d to generate prompts. We provided a comparison to their results in Table 1 in the submission. Our method outperforms the baseline by 10%, 18%, and 17% on ImageNette, ImageNet-100 and ImageNet-1k datasets respectively. \nFurthermore, our ablation study in Table 2 has proven the importance of \"[Intra-class Visual Guidance]\" component. We further clarify it by showing the following selected results from Table 2. As shown, \"[Intra-class Visual Guidance]\" brings 1.5% and 0.7% performance improvements on ImageNette and ImageNet100 respectively, when cooperating with \u201cDistribution Matching\u201d components. To clarify it, we revised our manuscript and provided further analysis in Appendix G.5.\n\n\n\n\n| Latent Prior | Visual Guidance | Distribution Matching | Finetune | ImageNette | ImageNet100 |\n|--------------|-----------------|-----------------------|----------|------------|-------------|\n|              |                 |                       | \u2713        | 80.8       | 73.3        |\n|              | \u2713               |                       | \u2713        | 82.3       | 74.0        |\n|      -        |        -         |             -          |   -       |$\\Delta$ + 1.5 |$\\Delta$ +0.7 |\n\n\n\n\n[3] Shiye Lei, Hao Chen, Sen Zhang, Bo Zhao, and Dacheng Tao. Image captions are natural prompts for text-to-image models. arXiv preprint arXiv:2307.08526, 2023."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1261/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700136375582,
                "cdate": 1700136375582,
                "tmdate": 1700136375582,
                "mdate": 1700136375582,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "x31k50f2Wt",
            "forum": "svIdLLZpsA",
            "replyto": "svIdLLZpsA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1261/Reviewer_8NDL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1261/Reviewer_8NDL"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the problem of synthesizing data which better matches the distribution of real data, and further improve the performance of model training. It comes up with a framework containing two parts of emphasizing the synthetic data distribution. Based on the experimental and theoretical analysis, the performance improves by a significant number on different tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. The study problem is interesting. As the synthetic data is easier to obtain and the real data often comes with privacy concerns, it is useful to understand how synthetic data can contribute, as well as the training process;\n2. The proposed framework is sound and easy to understand;\n3. The experiments are extensive, and different types of tasks are considered."
                },
                "weaknesses": {
                    "value": "1. It seems there is only 1 run on all the experiments. It is beneficial to include more than 1 repeats for the experiments to exclude the confounders;\n2. The experiments are extensive, and I'd also like to see how the proposed framework work on more complex tasks, for example tasks on human faces;\n3. Some figures can be improved, for example, Fig. 3, the text overlaps with the curves;"
                },
                "questions": {
                    "value": "Please refer to the previous sections"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1261/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699158798064,
            "cdate": 1699158798064,
            "tmdate": 1699636052612,
            "mdate": 1699636052612,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RfgJXmECHV",
                "forum": "svIdLLZpsA",
                "replyto": "x31k50f2Wt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1261/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1261/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer 8NDL,\n\n\n\n\nThank you for your insightful feedback and constructive comments on our manuscript. We appreciate that you find the study problem interesting, the proposed framework sound, and the experiments extensive. We now address your concern below: \n\n\n\n\n**Q1: More rounds of experiments**\n\n\n**A2:** Thank you for reminding us of this problem. We have included additional multi-runs for ImageNet (IN-10, 1N-100, 1N-1K) experiments as shown in the table below, where we find the standard deviation of the three-seed experiment on all scales stays within a small range. This validates the stability of our method. We have added these results in our revised manuscript in Appendix G.6. \n\n\n\n\nMulti-Seed Experiment Results: Top-1 Classification Accuracy across various datasets (IN-10, IN-100, IN-1k) over multiple runs. *Average* shows the mean and standard deviation of multiple run results.\n\n\n\n\n|       | IN-10      | IN-100     | IN-1k      |\n|-------|------------|------------|------------|\n| Run 1 | 90.5       | 80.0       | 70.9       |\n| Run 2 | 91.2       | 79.9       | 70.6       |\n| Run 3 | 90.7       | 80.2       | 70.9       |\n| Average | 90.8 \u00b1 0.29 | 80.0 \u00b1 0.12 | 70.8 \u00b1 0.14 |\n\n\n\n\n**Q1: Application to more complex task, for example tasks on human face**\n\n\n**A2:** Thank you for suggesting testing our framework on more complex tasks. We conducted additional experiments with a subset of the human face dataset CelebA (Liu et al. 2015) that included a general facial attribution classification task. As shown in the table below, the training data synthesized by our framework adapts well to various facial attributes, with close performance to the real data and notable improvement against the FakeIt baseline. We leave more complex face recognition tasks as future work. We have added the experiment settings, results, and discussion in our revised manuscript in Appendix G. 7.\n\n\n\n\n**CelebA Facial Attribute Classification Results on Three Attributes**\n|               | Smiling | Attractive | Heavy Makeup |\n|---------------|---------|------------|--------------|\n| Real Data     | 89.65   | 86.22      | 85.60        |\n| Baseline          | 76.90  |65.11      | 73.35        |\n| OURS          | 86.93   | 74.74      | 82.55        |\n\n\n[1] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In Proceedings of International Conference on Computer Vision (ICCV), 2015.\n\n\n**Q3: Some figures can be improved**\n\n\n**A3:**  We appreciate your helpful suggestions on the figures, especially Fig. 3. We have polished Figure 3 to ensure that all elements are clearly visible and the information is conveyed effectively. We also polished other figures in the manuscript, e.g., adjusting the colors in Figure 1."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1261/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700136329733,
                "cdate": 1700136329733,
                "tmdate": 1700136329733,
                "mdate": 1700136329733,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "MuzivOibHp",
            "forum": "svIdLLZpsA",
            "replyto": "svIdLLZpsA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1261/Reviewer_WEFi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1261/Reviewer_WEFi"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a novel diffusion-based approach for generation of synthetic training data. The key innovations of the method are three-fold. Firstly, an MMD-based distribution matching objective between the synthetic and target datasets is employed as fine-tuning step for the diffusion model. Second, visual guidance in the form of intra-class mean features from CLIP plus image captions and class names are used to condition the diffusion model. Lastly, a VAE is employed to obtain the latent codes of real samples for initializing the latent prior of the diffusion model. Experimental evaluations validate the efficacy of the proposed approach across a range of settings and datasets."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "**Clarity.** The writing, figures, and tables are exceptionally clear.\n\n**Originality.** The paper contains three innovations: feature distribution alignment via MMD, conditioned visual guidance, and VAE-based latent prior initialization. While not algorithmically revolutionary, these innovations are well-motivated and highly effective. \n\n**Quality.** The paper provides interesting theoretical justifications for the proposed innovations and a thorough evaluation of the proposed method, including adequate comparisons to the SoTA and a carefully conducted ablation study.\n\n**Significance.** The proposed approach outperforms the SoTA by huge margins (10%, 17%, 30%, etc.) across a range of datasets for both in and out of distribution data."
                },
                "weaknesses": {
                    "value": "Details of privacy analysis are missing."
                },
                "questions": {
                    "value": "Why are so many of the cells in table 1 empty?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1261/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699425869679,
            "cdate": 1699425869679,
            "tmdate": 1699636052545,
            "mdate": 1699636052545,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BqqSACLgtE",
                "forum": "svIdLLZpsA",
                "replyto": "MuzivOibHp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1261/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1261/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer WEFi,\n\n\n\n\nThank you for your insightful review and the affirmative evaluation of our work. We appreciate that you recognize the good clarity, originality, quality, and significance of our paper. We now address your concern below:\n\n\n\n\n**Q1: Details of privacy analysis are missing.**\n\n\n**A1:**  Thank you for pointing out the insufficiency of details. We addressed this by updating a comprehensive privacy analysis details in Appendix G.4 in the revised manuscript.\n\n\n\n\n**Q2: Missing Entry in Tab1**\n\n\n**A2:** We would like to kindly note a few reasons for the empty entries in Table 1. Firstly, direct evaluations against the Imagen model on diverse datasets are challenging due to its closed-source nature. Secondly, relatively older baselines in Table 1 have been surpassed by more advanced methods, making them less competitive for current benchmarks. Therefore, we focused our evaluations on the large-scale ImageNet-1K dataset, considering it the most representative and classical benchmark for evaluation in this domain. Also, regarding the benchmarking on other datasets, we compare with the current state-of-the-art *FakeIt*. It delivers a fair comparison given the same generative model backbone and demonstrates significant performance improvement."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1261/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700136123697,
                "cdate": 1700136123697,
                "tmdate": 1700136123697,
                "mdate": 1700136123697,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]