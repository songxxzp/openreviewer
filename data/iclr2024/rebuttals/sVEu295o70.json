[
    {
        "title": "Understanding when Dynamics-Invariant Data Augmentations Benefit Model-free Reinforcement Learning Updates"
    },
    {
        "review": {
            "id": "0t29LKnoEH",
            "forum": "sVEu295o70",
            "replyto": "sVEu295o70",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4132/Reviewer_Mi19"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4132/Reviewer_Mi19"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the effects of data augmentation in off-policy RL, notably the number of augmented transitions per observed transition, the number of augmentations per update, and the ratio of real data vs augmented data used in updates. The intermediate effects studied are the resulting diversity on state-actions and the diversity of rewards. They find that the state-action coverage is the most important outcome of data augmentation that leads to success, and this effect can provide equal (or even better) performance than drawing more real (diverse) data from the environment."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is very well presented and clear: I wasn't able to find any flaws in the story nor the arguments. Motivation is very clear, and they do an excellent job isolating various factors in augmentation-based (off-policy) algorithms and provide clear experimentation. The experiments seem insightful and I believe that the conclusions are sound."
                },
                "weaknesses": {
                    "value": "Very little weaknesses, though I think mostly I would like to see a little more clarity wrt ensuring f generates valid transitions."
                },
                "questions": {
                    "value": "My primary question is wrt f, e.g., how is this guaranteed that f generates valid transitions, what are the consequences of f not generating valid transitions, what are the properties of f in the experiments provided, etc. Could you please clarify these points in the experimentation section as well as clarify earlier in the work whether or not f is chosen to have the characteristics noted (validity, etc). But how much does this matter? Is it ok to have some invalid transitions in the augmented data, how might these change the results, etc?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4132/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698815180511,
            "cdate": 1698815180511,
            "tmdate": 1699636378709,
            "mdate": 1699636378709,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "n0ViIhKZgy",
                "forum": "sVEu295o70",
                "replyto": "0t29LKnoEH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4132/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4132/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to thank the reviewer for their kind comments! We\u2019re quite pleased to see that you found our work to be well-presented and that you appreciate the insights we draw from our empirical analysis.\n\nBelow, we address your questions regarding how and why we generate \u201cvalid\u201d augmented data. We would also like to point the reviewer to Appendix C in the supplemental material for a more in-depth description of all DAFs in our analysis.\n\n## **Ensuring we generate valid augmented data:**\nEnsuring we generate valid augmented data requires domain knowledge; we know that the dynamics of the robot and objects in Panda tasks are independent of the goal state, and we know that translating and rotating the agent in Goal2D will generate data that respects the task\u2019s dynamics. We want to note that while domain knowledge may seem like a limitation, we observe in the literature and real world RL applications that valid data augmentation functions are incredibly common and often require very little prior knowledge to specify. For example:\n1. Transition dynamics are often independent of the agent\u2019s goal state [1].\n2. Objects often have independent dynamics if they are physically separated [2,3], which implies translational invariance conditioned on physical separation.\n3. Several works focus on rotational symmetry of 3D scenes in robotics tasks [4,5], and many real-world robots are symmetric in design and thus have symmetries in their transition dynamics [6,7].\n\nWe chose to focus on dynamics-invariant augmentations because they have already appeared so widely in the literature. Furthermore, as RL becomes an increasingly widely used tool, we anticipate that domain experts will be able to identify new domain-specific augmentations and use them to further lower the data requirements of RL. Hence, the importance of identifying when and why different general properties of data augmentation will benefit RL.\n\n## **What if we generate non-valid augmented data?**\n\nNon-valid data, i.e. data that does disagrees with the task\u2019s dynamics and/or reward function, can bias learning and reduce data efficiency (similar to how model uncertainty biases learning in model-based algorithms [8, 9]). Since our analysis aims to understand which aspects of DA contribute to observed improvements in data efficiency, our focus on dynamics-invariant DAFs removes a potential confounding factor in our analysis. An interesting future research question to consider might be \u201cHow much non-valid data can an agent tolerate without harming data efficiency?\u201d\n\nSeveral prior works have used non-valid augmentations \u2013 especially those focusing on visual augmentations [*e.g.* 10, 11] \u2013 and we do think it is worth studying this class of DAFs. However, visual augmentations primarily aid representation learning, so such a study will likely need to focus on different aspects of DA than the ones we considered in our work and is thus beyond the scope of our analysis. \n\nPlease let us know if our response clarifies your comments! If you have follow-up questions or comments, we are more than happy to discuss them with you.\n\n## **References**\n\n[1] Adrychowicz et. al. \"Hindsight Experience Replay.\" NeurIPS 2017.\n\n[2] Pitis et. al. Counterfactual Data Augmentation using Locally Factored Dynamics.\" NeurIPS 2020.\n\n[3] Pitis et. al. \"MoCoDA: Model-based Counterfactual Data Augmentation.\"  NeuIPS 2022.\n\n[4] Wang et. al.  \"On-Robot Learning with Equivariant Models.\" CoRL 2022.\n\n[5] Wang et. al. \"The Surprising effectiveness of Equivariant Models in Domains with Latent Symmetry.\" ICLR 2023.\n\n[6] Pavlov et. al. \"Run, Skeleton, Run: Skeletal Model in a Physics-Based Simulation.\" AAAI 2018.\n\n[7] Abdolhosseini et. al.  \"On Learning Symmetric Locomotion.\" ACM SIGGRAPH 2019.\n\n[8] Moerland, Thomas M., et al. \"Model-based reinforcement learning: A survey.\" Foundations and Trends in Machine Learning 16.1, 2023.\n\n[9] Polydoros and Nalpantidis. \"Survey of model-based reinforcement learning: Applications on robotics.\" Journal of Intelligent & Robotic Systems 86.2, 2017.\n\n[10] Raileanu et al. \u201cAutomatic data augmentation for generalization in deep reinforcement learning.\u201d arXiv:2006.12862, 2020.\n\n[11] Laskin et. al. \"Reinforcement Learning with Augmented Data.\" NeurIPS 2020."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4132/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700169316098,
                "cdate": 1700169316098,
                "tmdate": 1700169316098,
                "mdate": 1700169316098,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "iw1rKahfmk",
            "forum": "sVEu295o70",
            "replyto": "sVEu295o70",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4132/Reviewer_Y7KF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4132/Reviewer_Y7KF"
            ],
            "content": {
                "summary": {
                    "value": "This paper is an empirical study of data augmentation in reinforcement learning, which has been shown to improve results. The authors hypothesize three possible causes for the improvement, 1) state-action coverage, 2) reward density, and 3) augmented replay ratio. For the off-policy and sparse-reward setting, a study is designed that disentangles these three causes as much as possible, on environments from panda-gym and 2D navigation. The results indicate that increasing state-action coverage and decreasing augmented replay ratio are more important, while increasing reward density is less important, although specific conclusions are task-dependent."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "# Originality and significance #\n\nAs far as I know, this is the first systematic study on the underlying causes of the improvements from data augmentation in RL. As such, I think the results would be helpful for researchers designing similar algorithms in the future.\n\n# Clarity #\n\nThe paper is written very clearly and the approaches are straightforward to understand. The separation of results into the main text and supplement is reasonable.\n\n# Quality #\n\nThe approach used to disentangle the three hypothesized causes are clever. Uncertainties are also presented in the graphs."
                },
                "weaknesses": {
                    "value": "The authors state many of the weaknesses of the work in the last paragraph of the paper: limited range of environments, restricted data augmentation framework, and incomplete set of properties investigated. One that they did not state was the limited range of algorithms. However, I think aside from the limited range of environments, those weaknesses are a reasonable consequence of the approach used to disentangle the three causes. Addressing them would make the disentanglement much harder."
                },
                "questions": {
                    "value": "1. How were the algorithms used in the study chosen?\n2. Is there intuition about how the differences in the tasks affects the results? For example, in figure 3 the \"x2\" and \"x4\" results for policy data and augmented data are similar, but this is not true in figure 1."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4132/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698956257305,
            "cdate": 1698956257305,
            "tmdate": 1699636378639,
            "mdate": 1699636378639,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6xWNBr9GD4",
                "forum": "sVEu295o70",
                "replyto": "iw1rKahfmk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4132/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4132/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "First, we would like to thank the reviewer for their positive review. We\u2019re pleased to see that you found our analysis to be well-written and clever, and our empirical findings to be useful to the research community. Below, we address your comments.\n\n# **Clarification on limitations:**\n\nThank you for pointing out the additional limitation with respect to our RL algorithm choices \u2013 we\u2019ll include it in our revisions. \n\nDifferent algorithms may be more or less capable of learning from augmented experience. We do think that it would be interesting to conduct another empirical study investigating how different algorithmic advances (e.g. multiple target networks [1], entropy regularization [2], n-step returns [3], etc.) may affect learning from augmented data. Several recent works have shown that n-step returns are critical to data efficient RL when learning with only observed data [4,5], and we hypothesize this may also be the case when learning with augmented data.\n\n# **How did we choose our algorithms?**\n\nWe initially wanted to use TD3 throughout the paper; TD3 often performs much better than DDPG, and is much less expensive to run than SAC. However, we found that TD3 performed noticeably worse than DDPG on all Panda tasks. In fact,  Gallouedec et. al [6] (the creators of Panda robot tasks) made this same observation. Thus, we decided to use DDPG for Panda tasks and TD3 for Goal2D as well as all MuJoCo tasks in Appendix F.\n\n# **Why does data augmentation improve data efficiency more in Figure 1 than in Figure 3?**\n\nIn the Goal2D task, we do see that additional augmented data improves data efficiency more than additional policy-generated data. In Panda tasks, augmented data is either just as good or slightly worse. We believe this difference arises because Goal2D is much simpler than the Panda tasks. \n\nThe *absolute* improvement to data efficiency provided by DA is not a critical point in our analysis; we sought to understand how much different aspects of DA contribute to the observed *relative improvements* in data efficiency.\n\nPlease let us know if we\u2019ve addressed all of your comments. We are more than happy to discuss any follow-up questions or comments you might have!\n\n# **References**\n[1] Fujimoto et. al. \"Addressing Function Approximation Error in Actor-Critic Methods.\" ICML 2018.\n\n[2] Haarnoja, Tuomas, et al. \"Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor.\" ICML, 2018.\n\n[3] Sutton and Barto. \"Reinforcement Learning: An Introduction.\" 2018.\n\n[4] Fedus et. al. \u201cRevisiting fundamentals of experience replay.\u201d ICML 2020.\n\n[5] Obando-Ceron et. al. \"Small batch deep reinforcement learning.\" To appear in NeurIPS 2023.\n\n[6] Gallouedec et. al. \u201cpanda-gym: Open-Source Goal-Conditioned Environments for Robotic Learning\u201d 4th Robot Learning Workshop: Self-Supervised and Lifelong Learning at NeurIPS 2021"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4132/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700168826877,
                "cdate": 1700168826877,
                "tmdate": 1700168826877,
                "mdate": 1700168826877,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wm8Tx20Fqy",
                "forum": "sVEu295o70",
                "replyto": "6xWNBr9GD4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4132/Reviewer_Y7KF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4132/Reviewer_Y7KF"
                ],
                "content": {
                    "title": {
                        "value": "Thanks to the authors for your response"
                    },
                    "comment": {
                        "value": "After reading the other reviews and responses, I have decided to keep my score. Although I agree with some of the other reviewers that the scope of the experiments is somewhat limited, I feel that the novelty and relevance of the work outweighs it and that the paper would be of interest to the community. I would encourage the authors to expand the variety of the experimental setup."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4132/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700685892593,
                "cdate": 1700685892593,
                "tmdate": 1700685892593,
                "mdate": 1700685892593,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "hqfdztLYAa",
            "forum": "sVEu295o70",
            "replyto": "sVEu295o70",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4132/Reviewer_UZG5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4132/Reviewer_UZG5"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the effective use of the dynamics-invariant data augmentation (DA) in reinforcement learning (RL). The authors propose an empirical framework to investigate the effectiveness of DA in RL, where they particularly focus on the impact of the augmented replay ratio, governing the frequency of using augmentations. Based on this, they identify the ratio as one of critical hyperparameters in RL, and demonstrate that it is sometimes better to set the replay ratio lower, whereas, intuitively, it seems always better to use higher ratio as a part \nof maximizing the benefit from the augmentation. Besides this, the authors provide a set of insights on using DA in RL, in particular, when reward is sparse."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper identifies a critical hyperparameter, the augmented relay ratio, in RL, which may overlooked before.\n\nThe authors propose a framework to empirically study the impact of data augmentation in RL.\n\nThe paper is well-written and easy to follow."
                },
                "weaknesses": {
                    "value": "My major concerns are the weak messages of the empirical findings. The authors provide some useful (yet somewhat intuitive) insights on using data augmentation in RL, but no effective usages in practice. It would be better to clarify the use of their findings. In my understanding, the most useful message in practice is to consider tuning the augmented reply ratio. It would be helpful to provide practical strategies to use DA effectively, based on the empirical findings.\n\nThere has been a line of works to study the impact of data augmentation in RL, and maximize its efficacy in RL, e.g., [A,B,C]. It has been observed that maximally using the data augmentation in RL training is sometimes worse than not using the augmentation. In some sense, this coincides with this paper's major finding (low augmented replay ratio is better), although they focused on the vision-based RL, that is different from the one considered in this paper. It is necessary to discuss differences and coincidences between this work and existing ones.\n\nIn addition, the proposed framework to study the effectiveness of data augmentation seems straightforwardly designed. It would be helpful to clarify the technical novelty of the framework.\n\n[A] Laskin, Misha, et al. \"Reinforcement learning with augmented data.\" Advances in neural information processing systems 33 (2020): 19884-19895.\n[B] Raileanu, Roberta, et al. \"Automatic data augmentation for generalization in deep reinforcement learning.\" arXiv preprint arXiv:2006.12862 (2020).\n[C] Ko, Byungchan, and Jungseul Ok. \"Efficient Scheduling of Data Augmentation for Deep Reinforcement Learning.\" Advances in Neural Information Processing Systems 35 (2022): 33289-33301."
                },
                "questions": {
                    "value": "It would be great if additional experiments, further discussion, or navigation to what I\u2019ve missed can be provided to address the comments and questions in the weakness and what follows.\n\n- Do the findings hold if DA is used for more than just augmenting data, e.g., policy distillation or representation learning? The other uses of DA may extract the full potential of DA in RL, and show somewhat different observations.\n\n- As we all know, choosing hyperparameters is critical in any machine learning. Hence, I want to ensure that every hyperparameter has been optimized for each scenario."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4132/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699007224302,
            "cdate": 1699007224302,
            "tmdate": 1699636378574,
            "mdate": 1699636378574,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VvZgqecl8t",
                "forum": "sVEu295o70",
                "replyto": "hqfdztLYAa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4132/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4132/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Rebuttal (1/2)"
                    },
                    "comment": {
                        "value": "We would first like to thank you for your review! We\u2019re pleased that you found our work well written and noted the importance of our findings regarding the augmented replay ratio. We believe we can address your comments and questions with a few minor clarifications in our revisions, and we hope to discuss these clarifications with you.\n\n## **Core messages of our empirical analysis:**\n\nIn our submission, we provide two practical guidelines:\n\n1. **Data augmentation (DA) should focus less on generating reward signal and more on increasing state-action coverage.** In our experiments, most \u2013 and sometimes all \u2013 of the benefits of a DAF can be explained by an increase in state-action coverage alone, and increasing reward density beyond a small threshold typically provides no further benefit. This guideline contrasts with the core motivation of HER and its follow-up works [1, 2] which focus on generating augmented reward signal to improve data efficiency.\n2. **When it is possible to generate a large amount of augmented data, use this data to decrease the replay ratio** (rather than to increase the batch size used for updates.)\n\nWe\u2019re pleased that our second guideline was clear, and we hope our response clarifies the first guideline. In our revisions, we will better emphasize these guidelines.\n\n## **Differences between our work and prior works studying visual augmentations:**\n\nThis is a great question that we can clarify with a few minor additions to the related work section. There are a few fundamental differences between our work and the related works you\u2019ve provided:\n\n1. Most prior works \u2013 including all three you have listed \u2013 focused on introducing new types of data augmentation functions (DAFs) or frameworks and demonstrating that they can boost the data efficiency of RL. To the best of our knowledge, our work is the first to systematically investigate which aspects of DA are most responsible for observed improvements in data efficiency.\n\n2. Raileanu et. al [3] and Ko & Ok [4] use augmented data for auxiliary tasks, while we focus on using augmented data for model-free  updates. Moreover, these auxiliary tasks aim to make the policy and value function invariant to augmentation, i.e. $\\pi( \\cdot | s) = \\pi( \\cdot | \\tilde s)$ and $V(s) = V(\\tilde s)$, and cannot be used with the dynamics-invariant DAFs we considered. For example, the optimal action generally changes if we translate the agent or goal, so it is undesirable to have $\\pi( \\cdot | s) = \\pi( \\cdot | \\tilde s)$.\n\n3. As you noted, all three works you listed consider visual augmentations that only transform the agent\u2019s state and generate augmented data with the same semantic meaning as the original data. As such, they study DA for representation learning. In contrast, the dynamics-invariant DAFs we consider generate augmented data with a new semantic meaning and thus study DA to improve exploration. \n\n## **Novelty of our framework:**\n\nThe core novelty of our framework is that it permits fine-grained control over many hyperparameters relevant to DA, allowing us to systematically study different aspects of DA.\n\nCrucially, our framework guarantees that the agent uses the same ratio of augmented to observed data (or update ratio) in each update by sampling observed and augmented data from separate replay buffers. Existing DA frameworks [*e.g.*, 5,6] and applications of DA [*e.g.*, 7, 8] sample data from a shared replay buffer containing both augmented and observed data, so the update ratio can change across updates. Moreover, with a shared replay buffer, the update ratio and the augmentation ratio (the number of augmented samples generated per observed sample) are entangled; increasing the augmentation ratio increases the update ratio.\n\nGiven that DA is so widely studied and that most prior work focuses on improving over baselines rather than understanding why certain methods perform well, we hope the analysis enabled by our framework is the first of many which move us towards the development of further practical DA guidelines.\n\nWe hope our response clarifies our work's novelties with respect to the existing DA literature. If you have further questions, we are more than happy to discuss them!  \n\n[1] Adrychowicz et. al. \u201cHindsight Experience Replay.\u201d NeurIPS 2017\n\n[2] Li et. al. \u201cGeneralized hindsight for reinforcement learning.\u201d NeurIPS 2020\n\n[3] Raileanu et al. \u201cAutomatic data augmentation for generalization in deep reinforcement learning.\u201d arXiv:2006.12862, 2020.\n\n[4] Ko, Byungchan, and Jungseul Ok. \"Efficient Scheduling of Data Augmentation for Deep Reinforcement Learning.\" NeurIPS, 2022. \n\n[5] Pitis et. al. \u201cCounterfactual Data Augmentation using Locally Factored Dynamics.\u201d NeurIPS 2020.\n\n[6] Adrychowicz et. al. \u201cHindsight Experience Replay.\u201d NeurIPS 2017.\n\n[7] Fawzi et. al. \u201cDiscovering faster matrix multiplication algorithms with reinforcement learning.\u201d Nature 2022.\n\n[8] Abdolhosseini et. al. \u201cOn Learning Symmetric Locomotion.\u201d ACM SIGGRAPH 2019."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4132/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700166880244,
                "cdate": 1700166880244,
                "tmdate": 1700166880244,
                "mdate": 1700166880244,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VzPZgZeMe6",
                "forum": "sVEu295o70",
                "replyto": "hqfdztLYAa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4132/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4132/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Rebuttal (2/2)"
                    },
                    "comment": {
                        "value": "## **Do the findings hold when DA is used for policy distillation or representation learning?**\n\nOur work focuses on understanding the benefit of integrating dynamics-invariant augmented data into model-free RL updates. Analogous studies focusing on policy distillation and representation learning are interesting directions for future work that we intend to investigate, but are beyond the scope of this analysis. We note that such studies will likely need to focus on different aspects of DA than the ones we considered in our work. \n\nIn particular, prior DA works focusing on policy distillation [4] and representation learning [3] are designed to work with DAFs that generate augmented data with the same semantic meaning as the original data, such as visual augmentations. This point has three implications:\n\n1. Because these augmentations produce data which could never be observed through environment interaction (e.g. an agent would never receive a cropped, recolored, and rotated image from the environment), they primarily aid representation learning rather than exploration. \n2. It may be imprecise to say that these augmentations increase coverage, because the observed and augmented data have the same semantic meaning. \n3. Because the original and augmented observation have the same semantic meaning, the augmented reward will often be the same as the original reward. Thus, the concept of  \u201creward density\u201d does not apply to these augmentations.\n\nHaving said this, the role of the augmented replay ratio can and should be studied for visual augmentation. Researchers have become increasingly interested in how the replay ratio affects learning [9-12], and lowering the replay ratio via augmentation may yield substantial improvements in data efficiency on visual tasks. \n\n## **Hyperparameter tuning:**\n\nAppendix G in the supplemental material lists hyperparameters and training details for all experiments. For Panda tasks, we use the same hyperparameters used by Plappert et. al [13], the creators of the Fetch robot tasks analogous to the Panda robot tasks we consider in our work. Gallouedec et. al [14] (creators of Panda robot tasks) also use these hyperparameters. Plappert et. al found these hyperparameters by performing a sweep over many different hyperparameter combinations described in Appendix B of [13].\n\nWe modified two hyperparameters in our replay ratio experiments (also noted in Appendix G). These experiments use a larger split of augmented data in each update, and we observed that these modifications improved performance in all tasks for both augmented and non-augmented agents.\n\nAgain, please let us know if our response clarifies your comments. We would be happy to discuss any follow-up questions you may have!\n\n## **References:**\n[3, from previous comment] Raileanu et al. \u201cAutomatic data augmentation for generalization in deep reinforcement learning.\u201d arXiv:2006.12862, 2020.\n\n[4, from previous comment] Ko, Byungchan, and Jungseul Ok. \"Efficient Scheduling of Data Augmentation for Deep Reinforcement Learning.\" NeurIPS, 2022.\n\n[9] Fedus et. al. \u201cRevisiting fundamentals of experience replay.\u201d ICML 2020.\n\n[10] Chen et. al. \u201cRandomized Ensembled Double Q-Learning.\u201d Arxiv, 2021.\n\n[11] Nikishin et. al. \u201cThe primacy bias in deep reinforcement learning.\u201d ICML 2022.\n\n[12]  D'Oro et. al. \u201cSample-efficient reinforcement learning by breaking the replay ratio barrier.\u201d ICLR 2023.\n\n[13] Plappert et. al. \u201cMulti-Goal Reinforcement Learning: Challenging Robotics Environments and Request for Research.\u201d Arxiv, 2018.\n\n[14] Gallouedec et. al. \u201cpanda-gym: Open-Source Goal-Conditioned Environments for Robotic Learning\u201d 4th Robot Learning Workshop: Self-Supervised and Lifelong Learning at NeurIPS 2021"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4132/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700167267728,
                "cdate": 1700167267728,
                "tmdate": 1700167267728,
                "mdate": 1700167267728,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CHP60QaYBL",
                "forum": "sVEu295o70",
                "replyto": "VvZgqecl8t",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4132/Reviewer_UZG5"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4132/Reviewer_UZG5"
                ],
                "content": {
                    "title": {
                        "value": "A quick question"
                    },
                    "comment": {
                        "value": "Thanks for the response. It will comment on the first response after some time to digest it. But, I want to ask if you can clarify the meaning and contribution of the \"systematic\" study on the data augmentation function. In my understanding, it is just an ablation study upon Algorithm 1 and a specific choice of network architecture, varying some hyperparameters. Please clarify technical contributions of the systemic study."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4132/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700180992884,
                "cdate": 1700180992884,
                "tmdate": 1700180992884,
                "mdate": 1700180992884,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ELq46s0lvQ",
            "forum": "sVEu295o70",
            "replyto": "sVEu295o70",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4132/Reviewer_ynPP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4132/Reviewer_ynPP"
            ],
            "content": {
                "summary": {
                    "value": "This paper analyzes the effects of data augmentation in reinforcement learning. A new evaluation framework is built, and three important indexes of data-augmented RL are studied empirically."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The topic is interesting in providing a study on the effectiveness of data augmentation in reinforcement learning. \n\nThe paper presentation is clear, especially since the problem model and proposed framework are easy to follow."
                },
                "weaknesses": {
                    "value": "There are many data augmentation ways, and the paper has also listed many of them. However, in experiments, only 2-3 basic ways are used. Considering the four typical testing environments, the conclusion of empirical analysis is quite limited.\n\nThe core idea of this paper can be very helpful for data-inefficient RL that needs augmentation, but it lacks theoretical analysis to support the proposed framework."
                },
                "questions": {
                    "value": "Could you explain Definition 3 and its purpose for the following analysis? An example would be helpful."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4132/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4132/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4132/Reviewer_ynPP"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4132/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699433623180,
            "cdate": 1699433623180,
            "tmdate": 1699636378460,
            "mdate": 1699636378460,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cPVGVvcmVr",
                "forum": "sVEu295o70",
                "replyto": "ELq46s0lvQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4132/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4132/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your kind comments! We\u2019re glad you found our work interesting, clear, and easy to follow. Below, we\u2019ve addressed your comments and answered the question you posed. We hope to discuss these clarifications with you.\n\n\n## **The purpose of Definition 3 (dynamics-invariant DAF):**\n\nWe use definition 3 to define the class of data augmentation functions (DAFs) we focus on in this work. At a high-level, a dynamics-invariant DAF only generates augmented data that the agent could in principle observe through task interaction (i.e. data that agrees with the task\u2019s dynamics and has the correct reward). Learning from augmented data that does not match the task\u2019s dynamics can harm learning, similar to how model uncertainty biases learning in model-based algorithms [1, 2] . Since our analysis aims to understand how different aspects of DA affect the data efficiency of RL, our focus on dynamics-invariant DAFs eliminates a potential confounding factor.\n\nUpon consideration, we do not require the second half of this definition (\u201cand if $f$ respects the stochasticity of $p$\u201d) for our analysis and will remove it to prevent confusion. Definition 3 will become: \u201cA DAF is dynamics-invariant if it is closed under valid transitions.\u201d Intuitively, a DAF is dynamics-invariant if it takes valid transitions and produces new valid transitions.\n\n[1] Moerland, Thomas M., et al. \"Model-based reinforcement learning: A survey.\" Foundations and Trends in Machine Learning 16.1, 2023.\n\n[2] Polydoros, Athanasios S., and Lazaros Nalpantidis. \"Survey of model-based reinforcement learning: Applications on robotics.\" Journal of Intelligent & Robotic Systems 86.2, 2017.\n\n## **Our choice of data augmentation functions (DAFs) in our experiments:**\n\nThank you for this comment! Our decision to focus on a few DAFs comes from two practical considerations that we will clarify in the main paper.\n\nWhile our experiments focus on 3 dynamics-invariant DAFs \u2013 agent translation, agent rotation, and goal relabeling \u2013 these DAFs are extremely general and apply to many tasks. For instance, translation and rotation can be applied to most navigation tasks. In the Goal2D environment in Fig. 1, translation and rotation would still be valid DAFs if the agent were a more complex locomotor (e.g. a quadruped robot). Moreover, since task dynamics rarely depend on the goal state, goal relabeling can be applied to most goal-conditioned tasks.\n\nOther popular DAFs such as visual augmentations generate non-valid augmented data and are thus beyond the scope of our analysis. A study of visual augmentations would need to focus on different aspects of DA than the ones we considered in our work. For example, visual augmentations generally do not change a transition\u2019s reward, so we cannot use them to study the effect of increasing reward density.\n\nWe can emphasize both of these points in the paragraph just before section 5.1.1. We hope this clarifies our empirical design. Please let us know if you have further questions or comments; we would be more than happy to discuss them!\n\n## **Regarding theoretical analysis:**\n\nWe agree that theoretical analysis is important, however, like many prior DA works in RL, our work is empirical in nature. We can make suggestions for future theoretical analysis in the paper. Are there types of analysis that the reviewer thinks are particularly important for this line of work?"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4132/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700164831571,
                "cdate": 1700164831571,
                "tmdate": 1700164831571,
                "mdate": 1700164831571,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]