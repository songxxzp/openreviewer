[
    {
        "title": "Conserve-Update-Revise to Cure Generalization and Robustness Trade-off in Adversarial Training"
    },
    {
        "review": {
            "id": "kcznp290kN",
            "forum": "6IjN7oxjXt",
            "replyto": "6IjN7oxjXt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7445/Reviewer_PH92"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7445/Reviewer_PH92"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new method to improve the trade-off between robustness and generalization in adversarial training.\nFirst, this paper investigates the difference between adversarial and clean representations layer-wisely,\nand next, it investigates overfitting in adversarial training when parameters of only some layers are selectively updated.\nBased on the observation that some layers tend to suffer from overfitting, this paper proposes CURE that leverages a gradient prominence criterion to perform selective conservation, updating, and revision of weights.\nTo evaluate the trade-off, this paper establishes a new metric: Natural-Robustness Ratio which is calculated by using accuracy against C&W and natural accuracy.\nCURE is evaluated in terms of this metric, robustness against several attacks including AutoAttack, and robustness against natural corruption."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This paper addresses an important problem in adversarial training: Overfitting and the trade-off between natural accuracy and robustness.\n- The detailed investigation of layer-wise learning phenomena in adversarial learning is novel and provides interesting insights.\nRevealed layer-wise properties might inspire researchers in this area and might cause new defense methods.\n- Gradient-based selective update for adversarial training is a new and interesting idea. \nThe figure of gradients in the training (Fig. 6) intuitively shows how the proposed method works by using the information of gradients well.\n- CURE is evaluated by using various attacks and architectures. However, baselines are not consistent and results might be cherry-picked."
                },
                "weaknesses": {
                    "value": "- This paper lacks an ablation study. Although layer-wise analyses are interesting, the proposed method contains several components besides selective updating. How is the performance if we use only RGP? \nIf it is not good, are equations (4), (5), (7), and (8) relevant to the layer-wise analysis?\nIf other parts than the selective updating contribute to the performance, the layer-wise analysis may not be very worthwhile.\n\n- This paper does not present a fair and honest evaluation and presentation of the results of the experiments.\nThe trade-off metric is intentionally designed to make the proposed method look overly good. Is there a rational explanation as to why the C&W is used in the evaluation of trade-off, even though AutoAttack performs better than C&W in terms of attack success rate? I suspect that C&W is chosen because the numbers of metrics are not better in the case of AutoAttack. In fact, robustness against AutoAttack of the proposed method is not always greater than baselines.\nAdditionally, the vertical and horizontal scales in Figure 1 are not aligned, which can be misleading.\nNatural corruptions are selectively used from CIFAR10C. Their results may be cherry-picked. I would like to see the results against all natural corruptions in CIFAR10C. Baseline methods are not consistent over expreiments.\nWhy does Table 1 not contain the result of HAT, and does Table 2 not contain ACT, ARD, LAD, and LAS-AT?\n\n- The boundary between the proposed and existing methods is described ambiguously. Eqs. (4) and (5) seem to be an objective function of TRADES. Why are these equations written in the section of the proposed method?\nAdditionally, SMU seems to be exponential moving average (EMA) with a stochastic parameter. SEAT (Wang & Wang, 2021) and other recent methods also use EMA, which is sometimes called weight averaging. Unlike them, the proposed method uses averaged parameters in the regularization term. I would like to see the comparison between averaging weights directly and using averaged weights for regularization.\n- Minor issues\n    - [a] might be related work that addresses the trade-off and focuses on the difference between the representations of clean data and adversarial examples. The method in [a] outperforms LAS-AT in terms of the trade-off. Since it seems to be concurrent work, I think that it is not necessary to compare.  \n    [a] Suzuki S et al. \"Adversarial Finetuning with Latent Representation Constraint to Mitigate Accuracy-Robustness Tradeoff.\" ICCV 2023."
                },
                "questions": {
                    "value": "- How is the performance if we use only RGP? If it is not good, are equations (4), (5), (7), and (8) relevant to the layer-wise analysis?\n- Is there a rational explanation as to why the C&W is used in the evaluation of trade-off? Is there any reasonable explanation for Natural-Robusthess Ratio? Why is eq.(9) suit to evaluate the trade-off?\n- How does NRR become if using AutoAttack?\n- What is the difference between TRADES and eqs.(4) and (5)?\n- Why does Table 1 not contain the result of HAT, and does Table 2 not contain ACT, ARD, LAD, LAS-AT?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7445/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7445/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7445/Reviewer_PH92"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7445/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698494506303,
            "cdate": 1698494506303,
            "tmdate": 1701047785218,
            "mdate": 1701047785218,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "F2zj2DVMqj",
                "forum": "6IjN7oxjXt",
                "replyto": "kcznp290kN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7445/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7445/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer PH92 - 1"
                    },
                    "comment": {
                        "value": "Thank you for finding our work insightful and providing valuable feedback. Below, we provide our responses to your questions.\n\n> ablation study\n\nWe add the ablation results in Table 7 in the Appendix. The RGP metric significantly identifies influential weights during training, prioritizing the enhancement of the network's capability to handle adversarial challenges. To maintain a balance, the revision stage utilizes consolidated and stable information, serving as a vital factor in achieving a balance, including improved performance in natural accuracy. \nAlso, revision stage is more important initially, to avoid drastic shift from natural to adversarial during training. As training progresses, the revision rate is systematically reduced to minimize revisions, particularly when the network stabilizes. The combined impact of both RGP and Revision, as observed in the final row, results in optimal performance, highlighting the effectiveness of their collaborative contributions.\n\n> Is there a rational explanation as to why the C&W is used in the evaluation of trade-off? Is there any reasonable explanation for Natural-Robusthess Ratio? Why is eq.(9) suit to evaluate the trade-off? \n\nWe appreciate the reviewer's diligence in evaluating our work. We respectfully disagree with the assertion that our paper lacks fair and honest evaluation. \n\n**NRR** :  Many adversarial studies predominantly show accuracy metrics, and some approaches have higher robust accuracy even if it comes at the cost of natural accuracy. Our objective was to not just improve adversarial accuracy, but to achieve better trade-off between natural and adversarial generalization. This we wanted to identify a metric that effectively captures and illustrates this trade-off in our method. Consequently, we conducted a thorough search for such a metric. In our exploration of trade-off evaluation metrics, we observed a prevalent use of the \"SUM\" metric in many papers (including the suggested paper [a] in the review). However, it became evident that relying solely on a SUM metric is problematic. Consider the scenario where 50% natural accuracy and 50% adversarial accuracy are treated worse than 90% natural accuracy and 11% adversarial accuracy  (even at chance level). This simplistic SUM approach fails to provide a nuanced evaluation of the delicate balance between both accuracies, rendering it ineffective in capturing the essence of the trade-off.\n\nTo address this limitation, we propose the Natural-Robustness Ratio (NRR) metric (Eq. 9), drawing inspiration from the F1 score commonly used in object detection (to balance precision and recall). This ratio offers a more meaningful assessment by considering the harmonious interplay between natural and adversarial accuracy, avoiding the oversimplification inherent in the SUM metric. We are open to further suggestions or refinements to our evaluation metric,  If the reviewer has specific recommendations for alternative metrics, we would like to discuss more and explore.\n\n**C&W attack** : Our choice of the C&W attack for the evaluation of trade-off is based on common practices in the field and the availability of consistent metrics across different papers. C&W, which is also known to be one of the strongest white-box attacks, was an attack we saw in papers from both categories : overfitting and generalization and across datasets, including SVHN. Hence we used that for NRR. \n\nTo address the concern raised, we have included an additional Table 8 in the Appendix that presents the Natural-Robustness Ratio (NRR) with AutoAttack. While we outperform in ResNet18, in WideResNet, we are close. However, as observed, the weakness lies in the metric. For example, concerning ST-AT, CURE exhibits a more significant increase in natural accuracy (84.92->87.05) and a smaller decrease in robust accuracy (53.54->52.10). However, the NRR is still higher for ST-AT. We explored various metrics to capture the trade-off, and we are open to further suggestions from the reviewer.\n\n> Additionally, the vertical and horizontal scales in Figure 1 are not aligned, which can be misleading. \n\nWe employed different scales to enhance clarity and prevent crowding of baselines at the same point. in Figure 1. We have kept both on a linear scale. We have generated an additional version of Figure 1 with the same scales for both axes, and we have included it in Figure 13 of Appendix."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7445/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700436247416,
                "cdate": 1700436247416,
                "tmdate": 1700436247416,
                "mdate": 1700436247416,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Sx0YZ7kfdT",
                "forum": "6IjN7oxjXt",
                "replyto": "Qs5fFszunF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7445/Reviewer_PH92"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7445/Reviewer_PH92"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the feedback!"
                    },
                    "comment": {
                        "value": "Thank you for the reply, and some of my concerns are addressed. However, new concerns have arisen. I'm leaning towards reject.\n\n> ablation study\n\nI like a new result of Table 7! However, I have new questions about this result as mentioned below. Additionally, I would like to see the robust accuracy against AutoAttack.\n\n> We have generated an additional version of Figure 1 with the same scales for both axes, and we have included it in Figure 13 of Appendix.\n\nMy comment may not have been clear. My comment was to show Fig. 1 as a square grid on the same scale. x-axis and y-axis in Fig. 13 are still not on the same scale.\n\n> However, it became evident that relying solely on a SUM metric is problematic. \n\nI agree that there is a problem with the sum metric but do not think that NRR is the perfect metric. I think that a sum metric is useful enough to be listed with NRR in tables.\n\n>  C&W, which is also known to be one of the strongest white-box attacks\n\nI agree that C&W tends to more strong than PGD, but AutoAttack is the strongest. I think NRR with AutoAttack (table 8) should be located in the main paper.\n\n> I believe there might be a misunderstanding. We did not selectively choose four corruptions in Figure 7; instead, we grouped various corruptions into four categories, following the approach used in [1].\n\nI am sorry for misunderstanding the results of natural corruption. A new figure is more informative, and it still shows the effectiveness of the proposed method.\n\n> We appreciate the reviewer's observation and would like to clarify that Eqs. (4) and (5) represent specific components within the broader context of our proposed method, CURE. \n\nIn reply, it appears that the only difference between the methods is the terminology: the RGP does not depend on TRADES losses (i.e., the naive adversarial loss can be used), and eqs (4) and (5) only indicate a combination of TRADES and RGP. In fact, the adversarial loss of line 5 of the algorithm can be exchanged as $L_{cls}(x_{adv})$. On the other hand, the baselines include those using naive adversarial losses. For example, at least, AWP is not used with TRADES in Table 3. This difference makes the performance comparison a bit unfair.\n\nFurthermore, looking at the results of the ablation study, the only RGP appears to be worse than TRADES. In the experiment of \nonly Revision stage in Table 7, what was used for losses? If Rev+TRADES reaches the same performance as Rev+RGP, then the masking of gradients in RGP is meaningless.\n\n> The baseline methods presented in Table 1 and Table 2 were sourced from the respective original papers. \n\nI think the experimental results reproduced by authors are better than those reprinted from the original papers because authors can directly design the experiments to unify the experimental conditions. Therefore, the absence of results in the original paper is not a reason not to include comparative results in this paper."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7445/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700615177878,
                "cdate": 1700615177878,
                "tmdate": 1700615177878,
                "mdate": 1700615177878,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qGaijRSb6W",
                "forum": "6IjN7oxjXt",
                "replyto": "kcznp290kN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7445/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7445/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the new concerns/questions (1/3)"
                    },
                    "comment": {
                        "value": "> I like a new result of Table 7! ... Additionally, I would like to see the robust accuracy against AutoAttack.\n\n> ... In the experiment of only Revision stage in Table 7, what was used for losses? If Rev+TRADES reaches the same performance as Rev+RGP, then the masking of gradients in RGP is meaningless.\n\nWe have incorporated the AutoAttack results into Table 7, as per the reviewer\u2019s suggestion.\n\nIt seems there might be a misunderstanding regarding the ablation study. When incorporating only RGP, since we start from a naturally trained model. The metric identifies layers that can effectively learn the new adversarial distribution. Consequently, when only RGP is added, adversarial accuracy increases compared to the first row.\n\nIn the revision stage only, the objective is to consolidate information gradually, preventing the network from forgetting natural accuracy while learning the new data distribution. Therefore, the revision-stage-only scenario, which follows the objective of equation [5], exhibits higher natural accuracy than the first row, but the robust accuracy decreases. CURE leverages both selective updating and consolidation, aiming to achieve a better trade-off between natural and robust accuracy, and our extensive empirical study demonstrates this optimal trade-off.\n\n| RGP | Rev  | Nat | PGD20 | C&W  | AA | NRR |\n|-------|-------|-----|---------|---------|----|-------|\n| \u2717 | \u2717 | 82.41 | 52.76 | 50.43 | 48.37 | 60.95 |\n| \u2713 | \u2717 | 81.27 | 53.20 | 51.33 | 49.06  | 61.18 |\n| \u2717 | \u2713 | 83.28 | 51.57 | 50.96 | 47.35  | 60.37 |\n| \u2713 | \u2713 | **86.76** | **54.90** | **52.48** | **49.69** | **63.19** |\n\n> My comment may not have been clear. My comment was to show Fig. 1 as a square grid on the same scale. x-axis and y-axis in Fig. 13 are still not on the same scale.\n\nFigure 13 has been updated to align with your suggestion.  However, we are a bit confused regarding this. Could you please provide more details to help us understand the specific concern?\n  - The natural accuracy consistently falls within the range of - [80-90]\n  - The adversarial accuracy scale spans a lower range ->  [45-60]\n\nFigure 1 is what would be generated when these values are provided to any plotting tool, as by default they inherently adjust the scales based on the provided data for a comprehensive visualization.\nHowever, to generate Figure 13, we have fixed the scales to accommodate the request. Typically, when visualizing factors with varying ranges, such scale alignment is not a common practice. We are eager to understand the reasoning behind your request.\n\n> I agree that there is a problem with the sum metric but do not think that NRR is the perfect metric. I think that a sum metric is useful enough to be listed with NRR in tables.\n\nBoth SUM and NRR metrics have been incorporated into Table 8, as per reviewer\u2019s suggestion. \n\nWe acknowledge and have also stated in the previous rebuttal about the NRR not being the perfect  metric. However, we believe it still holds more value than the SUM metric in portraying the trade-off between natural and robust accuracy.\n*Could you elaborate on how the SUM metric effectively communicates the trade-off compared to NRR?*\n\nIt's also crucial to address the reviewer's claim that the trade-off metric is intentionally designed to favor CURE. CURE excels in the SUM metric (Table8), but we still believe that SUM cannot capture the trade-off well at all. A high SUM alone does not necessarily signify an optimal trade-off; it could result from emphasizing a single accuracy metric. \n\nOur primary objective is to enhance the generalization trade-off, emphasizing a balanced improvement across both natural and robust accuracy metrics. Our approach considers the absolute values of accuracy against various attacks, and we have introduced the NRR metric (inspired from F1 score) as an additional measure to provide a more comprehensive view of the trade-off landscape.\n\n> I agree that C&W tends to more strong than PGD, but AutoAttack is the strongest. I think NRR with AutoAttack (table 8) should be located in the main paper.\n\nTo ensure consistency in the paper for readers, we established the NRR using C&W (similar to many literature sources that demonstrate the use of the SUM metric only with PGD20) present in all tables, which cover various architecture-dataset combinations. Particularly regarding Tables 1 and 2, as the second table does not include the AA. \n\nNevertheless, we are open to accommodating the reviewer's feedback by swapping the positions of these two tables and displaying the NRR on AA in the main paper.\n\n> I am sorry for misunderstanding the results of natural corruption. A new figure is more informative, and it still shows the effectiveness of the proposed method.\n\nWe are glad to hear it."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7445/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700680344636,
                "cdate": 1700680344636,
                "tmdate": 1700680640540,
                "mdate": 1700680640540,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hxji7pyHDx",
                "forum": "6IjN7oxjXt",
                "replyto": "kcznp290kN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7445/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7445/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the new concerns/questions (3/3)"
                    },
                    "comment": {
                        "value": "> I think the experimental results reproduced by authors are better than those reprinted from the original papers because authors can directly design the experiments to unify the experimental conditions. Therefore, the absence of results in the original paper is not a reason not to include comparative results in this paper.\n\nOur reporting approach involves two main considerations. Firstly, we include accuracy results from the original papers or comparative survey works. This choice is driven by the fact that the authors of the respective methods have fine-tuned their models for specific datasets and architectures, representing the optimal performance achievable under their established conditions. Furthermore, the essence of scientific publication is to facilitate the reusability of results. \n\nSeveral studies opt to train their foundational methods with the hyperparameters they deem optimal for their method, considering it a fair practice. It's important to note that each method has a specific set of parameters that yield the best performance. For instance, methods like Madry and TRADES incorporate early stopping to prevent overfitting, which can lead to a reduction in the final results. We conducted training for these methods, and while the results were notably lower than the reported original results, we chose to include the latter in our tables. This decision was made to ensure consistency and transparency, acknowledging the varied approaches in the literature and providing a clear basis for comparison.\nNotably, numerous papers lack results for specific architecture-dataset combinations. The generation of reliable results requires meticulous fine-tuning to find the best parameters for this combination before comparing, and factors like seed values also influence the outcomes. Recognizing these complexities, we conducted our experiments with due diligence, ensuring transparency and fairness in our reporting process. \n\nFurthermore, our aim was to encompass the majority of recently published methods by categorizing similar settings into different tables. This approach allowed us to compare our method with a larger array of methods, ensuring a fair and reliable comparison.\n\nPlease kindly inform us if there is any issue with this viewpoint.\n\n** *We've noted the revision in the rating and are keen to understand the specific factors that led to this adjustment. Your explicit feedback on the precise areas where you believe our responses fell short would be beneficial for us. We would like to address all concerns comprehensively and enhance the paper accordingly. If there are particular sections or aspects that you think still require attention or clarification, we would appreciate detailed insights.*"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7445/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700681146482,
                "cdate": 1700681146482,
                "tmdate": 1700681267995,
                "mdate": 1700681267995,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "lsjYtFJ6iu",
            "forum": "6IjN7oxjXt",
            "replyto": "6IjN7oxjXt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7445/Reviewer_i2U9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7445/Reviewer_i2U9"
            ],
            "content": {
                "summary": {
                    "value": "This paper unveils the underlying factors of the trade-off between standard and robust generalization in adversarial training by examining the layer-wise learning capabilities of neural networks during the transition from a standard to an adversarial setting. The paper demonstrates that selectively updating specific layers while preserving others can substantially enhance the network's learning capacity empirically, and proposes a method to leverage a gradient prominence criterion to perform selective conservation, updating, and revision of weights named CURE. The paper verified the effectiveness of CURE on various dataset and architecture, which verifying the effect in enhancing the trade-off between robustness and generalization and alleviating robust overfitting empirically."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper has good originality, high quality and clear expression. The paper unveils the trade-off between standard and robust generalization in adversarial training in the perspective of layer-wise learning capabilities of neural networks and proposes a new method to alleviate robust overfitting."
                },
                "weaknesses": {
                    "value": "The analysis of selective adversarial training are empirically not theoretically.It's better to provide theoretically analysis of selective adversarial training."
                },
                "questions": {
                    "value": "1.Is the proposed method still works well on larger dataset, for example ImageNet?\n2.There are so many hyperparameters such as \u03b1,\u03b2,\u03b3,r,d, how to mediate so many hyperpapameters effectively?\n3.For deeper neural networks, such as resnet-101, is the proposed method still works well?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7445/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698721715689,
            "cdate": 1698721715689,
            "tmdate": 1699636894093,
            "mdate": 1699636894093,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vseJotQhgM",
                "forum": "6IjN7oxjXt",
                "replyto": "lsjYtFJ6iu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7445/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7445/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer i2U9 - 1/"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer's acknowledgment of the originality and high quality of our work and we would like to thank them for the valuable feedback. Our responses to the questions are below\n\n> The analysis of selective adversarial training are empirically not theoretically.It's better to provide theoretically analysis of selective adversarial training.\n\nWe acknowledge the value of incorporating theoretical insights. In the current study, our focus was on addressing the challenges in adversarial training (AT) through behavioral analysis of layers. By delving into the learning dynamics during both standard and adversarial training via empirical analysis, we aimed to gain a clearer understanding of the underlying mechanisms. This empirical approach played a pivotal role in shaping the development of our proposed technique, aiming to improve the generalization trade-off and mitigate the overfitting phenomenon simultaneously.\nIn this preliminary study, our emphasis has been on empirical observations. While our current work lays the foundation, we acknowledge the potential for theoretical studies to further enhance our understanding.\n\n> Larger dataset (ImageNet) and deeper neural networks (ResNet101)\n\nWe appreciate the reviewer's feedback on additional results to show the efficacy of CURE. In the current literature, the prevailing datasets for adversarial training are CIFAR, SVHN, and MNIST, and the common architectures are ResNet18 and WideResNets (WRN34-10, WRN28-10). Unfortunately, among the 14 works we have compared with, none have demonstrated results on ImageNet or larger networks like ResNet101, also making it difficult  to obtain baselines for these. \n\nIn response to the review, we have initiated additional experiments with larger networks for CURE and two other baselines. At this time, we present preliminary results for ResNet50-CIFAR10.  Due to the significant time and resource requirements, we will provide other results once these experiments are complete. We also add these additional results in Appendix (Table 6)\n\n|  | Nat | PGD20 | C&W | NRR |\n|---|---|---|---|---|\n| SAT | 72.86 | 37.54 | 36.92 | 49.00 |\n| TRADES | 73.06 | 40.33 | 37.56 | 49.61 |\n| CURE | 74.58 | 43.85 | 41.04 | 52.94 |\n\n> There are so many hyperparameters such as \u03b1,\u03b2,\u03b3,r,d, how to mediate so many hyperpapameters effectively? \n\nThe hyperparameters are stable across settings and datasets and also complement each other. Therefore, CURE does not require extensive fine-tuning across different datasets and settings. Some hyperparameters, like the revision rate (r) and decay factor (d) for the revision stage, are set universally (r = 0.2, d = 0.999) across all experiments,  The revision rate (r) determines the stochasticity of the revision stage, with a 20% probability of occurrence. This is more important during the initial stage of training and controlled reduction in revisions occurs as training progresses, as the network stabilizes. The hyperparameter \u03b3, which is associated with the consistency regularization term, is generally stable and is set to 1.0. \n\nThe main hyperparameters of interest are \"p\" and \"\u03b1\", which are quite intuitive. The parameter \"\u03b1\" influences the importance given to the adversarial distribution in the learning process. Higher values of \"\u03b1\" emphasize natural accuracy. Setting \"\u03b1\" to a lower value, such as 0.1 or 0.2, is preferable, especially when the network is pre-trained in the standard setting. Notably, \"\u03b2\" complements \"\u03b1,\" where \u03b2 = 1 - \u03b1 (so we changed it in the equation 6)\n\nThe \"p\" parameter is associated with estimating the percentile in gradients below which the gradients are set to zero. This percentile determines the fraction of weights that will not be updated, contributing more to the conservation of natural knowledge. The choice of \"p\" influences the balance between retaining previous knowledge and learning new information.\n\nWe also add this in the Appendix (Table 10)\n\n| Sensitivity to gamma |  |  |  |  | Sensitivity to p |  |  |  |  | Sensitivity to alpha |  |  |  |  |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| p | alpha | gamma | Nat | Rob | p | alpha | gamma | Nat | Rob | p | alpha | gamma | Nat | Rob |\n| 30 | 0.1 | 1.0 | 86.76 | 54.92 | 30 | 0.1 | 1.0 | 86.76 | 54.92 | 30 | 0.1 | 1.0 | 86.76 | 54.92 |\n|  |  | 0.5 | 86.25 | 53.13 | 10 |  |  | 85.15 | 55.65 |  | 0.2 |  | 86.95 | 54.17 |\n|  |  |  |  |  |  |  |  |  |  |  | 0.5 |  | 88.52 | 53.67 |"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7445/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700429974014,
                "cdate": 1700429974014,
                "tmdate": 1700442194087,
                "mdate": 1700442194087,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "weoobGLQgh",
                "forum": "6IjN7oxjXt",
                "replyto": "vseJotQhgM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7445/Reviewer_i2U9"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7445/Reviewer_i2U9"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to the authors for their reply,I have no more questions."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7445/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700620816330,
                "cdate": 1700620816330,
                "tmdate": 1700620816330,
                "mdate": 1700620816330,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ZXnyh7akQA",
            "forum": "6IjN7oxjXt",
            "replyto": "6IjN7oxjXt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7445/Reviewer_HvAu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7445/Reviewer_HvAu"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a novel approach to improving the adversarial robustness of DNNs while maintaining the performance on natural samples. The authors first conduct empirical studies to discover that updating weights in all layers in AT may be not good for the generalization of DNNs in both natural and adversarial samples. Thus, they propose an adaptive method to selectively update a subset of weights in the DNN. The proposed method is simple but empirically effective."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "This paper presents a simple yet effective method of improving both robustness and generalization of DNNs.\n\nThis study discovers some interesting phenomena, e.g., updating middle layers is beneficial to standard and robustness generalization, and adversarial training increases the similarity between features of different layers.\n\nThe paper is well-written and easy to follow."
                },
                "weaknesses": {
                    "value": "- The authors repeatedly use \u201coverwritten\u201d and \u201clearning\u201d, but I cannot grasp the essential difference between them. What kind of update of weights is referred to as \u201coverwritten\u201d and \u201clearning\u201d, respectively? Can you provide a clearer definition for the two terms? Besides, how do the authors come to the conclusion about \u201coverwritten\u201d and \u201clearning\u201d from the accuracy drop in Figure 2(a)? \n- In Section 2, the authors update the weights of selected layers by re-initializing and updating them. Have you ever tried not re-initializing but directly finetuning them with adversarial examples? Learning weights from random noises, especially weights in shallow layers may lead to a significant drop in performance, but finetuning them will not.\n- I\u2019m a little confused about Figure 3. If the weights of deep layers (e.g. U-34) are updated while shallow layers (e.g. blocks 1 and 2) are fixed, the features in shallow layers are also supposed to be frozen. Thus, the similarity between shallow layers in U-34 should be the same as in the ST model. However, the similarities between shallow layers in Figure 3 are all different.\n- Although this paper presents some differences in performance between updating different layers, it does not clearly \u201cdisentangle clean and adversarial representations\u201d or \u201cdisentangle robust and non-robust features\u201d. Also, this paper does not strictly define and extract layers with \u201cgreater learning capacity\u201d. I suggest the authors use such claims carefully and seriously.\n- Figure 4 indicates that updating high layers may cause robustness overfitting, but Figure 6 (b) shows a large ratio of updated gradients in high layers. I expect the authors to discuss more about such a misalignment.\n- How about the representation similarity between different layers in the DNNs trained by CURE?"
                },
                "questions": {
                    "value": "- Is CURE used by training the model from scratch with the loss function in equation (5) or finetuning a pre-trained network? If the CURE is adopted on a pre-trained model, I am concerned that it may be unfair to compare it with other training-from-scratch methods. Besides, it also increases the computational cost to train the model twice (pre-train with ST and then finetune with CURE). \n- How stable is CURE when using different hyper-parameters ($\\alpha,\\beta$ and $p$)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7445/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698752988861,
            "cdate": 1698752988861,
            "tmdate": 1699636893981,
            "mdate": 1699636893981,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "G4vSqKd41s",
                "forum": "6IjN7oxjXt",
                "replyto": "ZXnyh7akQA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7445/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7445/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer HvAu - 1"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer's acknowledgment of the paper's strengths and thank them for their detailed insights and feedback.\n\n>  Can you provide a clearer definition for the two terms? \u201coverwritten\u201d and \u201clearning\u201d,\n\nIn the context of our study, the term \"overwritten\" signifies a phenomenon where the knowledge acquired during standard training, primarily on natural images, undergoes a degree of compromise or replacement during adversarial training. This is akin to the observed behavior in continual learning, where adapting to a new task can lead to the erosion of previously acquired knowledge, prompting the exploration of dynamic and sparse architectures in that domain. \nAdditionally, the inherent over-parameterization of deep neural networks implies that not all neurons operate at peak capacity, leaving room for learning new information from different distributions or data. Thus, we used these terms in the text. \n\nFor empirical analysis, our objective was to analyze the behaviors of the network during the transition from standard training to adversarial training. The \u201cBase\u201d we start from is the network trained on natural images via standard training (ST). We use this network and then freeze different layers, re-initialize weights of the remaining and train on adversarial images. The observed decrease in test accuracy serves as one indicator of a potential overwriting of learned information. \nTo enhance clarity, we have made adjustments to Fig 2(b), incorporating results for both standard training (ST) and standard adversarial training (SAT). This modification aims to visually represent the transition from ST to SAT by considering the perspective of layer re-initialization.\n\n> In Section 2, the authors update the weights of selected layers by re-initializing and updating them. Have you ever tried not re-initializing but directly finetuning them with adversarial examples?\n\nWe acknowledge the suggestion, and we have provided the figure with results of directly fine-tuning selected layers with adversarial examples instead of re-initialization - Figure 11 in Appendix. Fine-tuning also results in a decline, as there is a sharp shift when the network encounters adversarial images after being trained only on natural images. While certain layers exhibit relatively higher performance than re-initialization, the overall trend remains the same.\n\nFurther, our primary objective in the empirical analysis was to assess the behavior of the network through the lens of both re-initialization and updating. This approach allowed us to observe how the network adapts to adversarial training under different weight manipulation strategies, providing insights into the learning dynamics and the impact on performance. \n\n> Thus, the similarity between shallow layers in U-34 should be the same as in the ST model.\n\nThanks for the keen observation. In Figure 3, we examine the similarity between the features generated when an adversarial sample is the input and the features when natural samples are the input for a specific network architecture. The figure illustrates the similarity patterns among different layers for nine models, each trained with distinct retention and update strategies. This pattern deviates from the standard (ST) model, as these networks receive adversarial samples as input, leading to changes in batch norm statistics (unlike in the ST model)\n\nThere is also an observed \"block structures\" in these networks indicating a substantial level of intra-layer similarity and lower inter-layer similarity. For instance, in the U-1 model, the first block displays considerable dissimilarity with the fourth block, while the last three blocks exhibit higher similarity. In U-34, blocks 1 and 2 are fixed and hence have similarity between each other, while there are differences seen in the third and fourth block. Please let us know if more explanation is needed.\n\n> Paper does not disentangle \"robust and non-robust features\" and and extract layers with \u201cgreater learning capacity\u201d. \n\nWe acknowledge that the term \"disentanglement\" might introduce ambiguity. Figure 3 is not intended as a direct disentanglement method; rather, it serves as an illustrative representation. Our primary focus was not to disentangle features explicitly but to identify weights that can be updated to learn representations that are effective for both natural and adversarial images. \nAlso, in reference to the term \"layers with greater learning capacity,\" we meant CURE adopts a more automatic approach of choosing layers to be updated and not do it in the traditional engineering sense. We appreciate the reviewer's feedback and will make sure to remove these ambiguous terms in the revised manuscript for better clarity."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7445/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700432167724,
                "cdate": 1700432167724,
                "tmdate": 1700432167724,
                "mdate": 1700432167724,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "z1TWgrVbsp",
                "forum": "6IjN7oxjXt",
                "replyto": "9E2gdUrYbB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7445/Reviewer_HvAu"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7445/Reviewer_HvAu"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their explanations. They have addressed most of my questions, but there are still some concerns.\n\n1.  Can you provide a clearer definition for the two terms? \u201coverwritten\u201d and \u201clearning\u201d: I understand the meaning of \"overwritten\", but it is still unclear how to distinguish which weight changes represent the overwriting of features and which represent learning of features. In other words, when the accuracy drops in Figure 2, how did you identify which layers are being overwritten while other layers are learning new information? Or do you mean the drop in natural accuracy always reflects the overwriting of features? Given that these two terms frequently appear in the paper, I suggest the authors provide a clear and quantifiable definition for them.\n\n2. misalignment between Figure 4 and Figure 6(b): I double-check Figure 6(b, bottom) and find that almost all high updates (peaks) happen in BN layers. In Figure 6(b, top), although conv layers are also updated, BN layers always have higher updated ratios. Could you further explain this phenomenon?"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7445/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700647882965,
                "cdate": 1700647882965,
                "tmdate": 1700647882965,
                "mdate": 1700647882965,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "tMT4tXPLQv",
            "forum": "6IjN7oxjXt",
            "replyto": "6IjN7oxjXt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7445/Reviewer_DhtN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7445/Reviewer_DhtN"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on the trade-off between standard and robust generalization. To this end, this paper proposes CURE that leverages a gradient prominence criterion to perform selective conservation, updating, and revision of weights, which can tackle both memorization and overfitting issues."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The problem authors focused on is very interesting."
                },
                "weaknesses": {
                    "value": "1. Experimental results in Fig. 2 cannot support authors' claim that \".. cause reduced performance on both data due to overwriting of learned information..\" Specifically, just a comparison of accuracy in Fig. 2  cannot reflect the overwriting of learned information. Authors should design new solid experiments to support this conclusion.\n2. Authors did not clarify how to disentangle robust and non-robust features, which still presents a significant challenge. Hence, Fig.3 is in doubt.\n3. I wonder why \"a subset with the most significant impact on accuracy\" equals to the subset of weights that \"contribute more to the joint distribution of both natural and adversarial accuracy.\" Can you prove it or explain it?\n4. A algorithm flowchart will help readers better understand how weights are updated in each epoch. In each epoch, are different or same subsets of weights updated?\n5. What does \"sample\u223cU(0,1)<r\" in Eq. 7 mean?\n6. Experimental results cannot verify the effectiveness of the proposed CURE method, since authors just conducted experiments on resnet18 and resnet34. Please conduct more experiments on more classic DNNs."
                },
                "questions": {
                    "value": "In this paper, many conclusions are not supported authors' experimental results, i.e., we cannot infer these conclusions just based on existing experimental results.\nThus, many conclusions in Section 3 are over-claimed.\nDetails are stated in weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7445/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698822159095,
            "cdate": 1698822159095,
            "tmdate": 1699636893867,
            "mdate": 1699636893867,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Zi3GwUbkZn",
                "forum": "6IjN7oxjXt",
                "replyto": "tMT4tXPLQv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7445/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7445/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer DhtN - 1/2"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for taking the time to review our work. Please find our responses below. We have also updated the paper and all the changes made in the paper are in blue.\n\n>Experimental results in Fig. 2 \n\nWe acknowledge the reviewer's concern regarding the interpretation of experimental results presented in Figure 2. Our objective was to analyze the behaviors of the network during the transition from standard training to adversarial training. The base, we start from is the network trained on natural images via standard training (ST). We use this network and then freeze different layers, re-initialize weights of the remaining and train on adversarial images. From Fig2, the network trained in ST has accuracy of ~95, and when only layer 1 is updated while keeping the rest of the network frozen, the natural accuracy drops to ~45. The observed decrease in test accuracy serves as one indicator of a potential overwriting of learned information. The analogy is also drawn with Continual Learning (CL) domain, where, as a network adapts to new tasks or distributions, there is a risk of overwriting previously acquired knowledge which is again measured only through the drop in accuracy. To further provide clarity, we also modify Fig 2(b) to include ST and SAT (which is the standard adversarial training) results, to show the transition from ST to SAT by examining the process of layer re-initialization.\n\n> disentangle robust and non-robust features\n\nWe acknowledge that the term \"disentanglement\" might introduce ambiguity. Figure 3 is not presented as a direct disentanglement method; rather, it merely serves as an illustrative representation. Our primary objective was not to provide a definitive disentanglement mechanism but rather to showcase the transition as the network moves from standard to adversarial training. We intended to examine the similarity between the representations of natural and adversarial samples on differently trained networks. The network that has representations that exhibit similarity in both scenarios have the potential to perform effectively on both natural and adversarial images. This visualization helps emphasize the learning dynamics rather than providing a precise disentanglement methodology.\nIn response to this feedback, we have removed such terms from the paper.\n\n> I wonder why \"a subset with the most significant impact on accuracy\" equals to the subset of weights that \"contribute more to the joint distribution of both natural and adversarial accuracy\n\nThe rationale behind our claim lies in the functionality of the Robust Gradient Prominence (RGP) metric. This metric is designed based on the premise that features with the most substantial impact on the model's predictions exert a greater influence on the gradients of the model parameters. The RGP score, as computed in Equation 6 of our paper, leverages gradients from both natural and adversarial examples. The coefficients \u03b1 and (\u03b2 = 1-\u03b1)  help balance the importance between natural and adversarial distributions. The RGP metric, by evaluating the gradients with respect to each weight, provides a quantitative measure of their importance in influencing both natural and adversarial accuracy. \nAs we use the base model of ST to perform adversarial training, and view natural and adversarial images as distinct distributions, our objective is to update weights that can effectively learn from adversarial samples without compromising accuracy on natural images. The selected subset of weights, determined by the RGP metric, encapsulates those with the most significant impact on accuracy, facilitating a nuanced trade-off between natural and adversarial performance. This metric offers a quantitative measure of weight importance, guiding our approach to updating a subset of weights with a strategic focus on achieving optimal model performance across different data distributions.\n\n> A algorithm flowchart \n\nWe appreciate the suggestion. Indeed, to enhance clarity, we have included a detailed algorithm in Appendix, Section C. In each epoch, the weights to be updated are chosen automatically by the RGP metric and can be different. CURE provides a dynamic mechanism to determine which weights to update and which ones to preserve in each layer of the network. There is no manual engineering of selecting the weights. As the training progresses, the RGP metric evolves and a more stable updation is seen during the final stages of training, as seen from in Fig6(b), where middle and later layers are updated more. This selective weight updating strategy is a key aspect of our method, allowing for targeted adaptation to adversarial data while preserving crucial information from natural data."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7445/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700427749189,
                "cdate": 1700427749189,
                "tmdate": 1700427965446,
                "mdate": 1700427965446,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]