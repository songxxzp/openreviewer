[
    {
        "title": "Soft Mixture Denoising: Beyond the Expressive Bottleneck of Diffusion Models"
    },
    {
        "review": {
            "id": "jixmHVgPj3",
            "forum": "aaBnFAyW9O",
            "replyto": "aaBnFAyW9O",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4096/Reviewer_nQBi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4096/Reviewer_nQBi"
            ],
            "content": {
                "summary": {
                    "value": "This paper...\n- theoretically shows DMs suffer from an expressive bottleneck due to the assumption that the denoising distribution is Gaussian,\n- proposes soft mixture denoising to address this problem,\n- shows SMD improves the performance of DMs on various datasets."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This paper proposes a novel approach to improving the sampling efficiency of diffusion models.\n- SMD is well-motivated through rigorous theoretical analysis.\n- This paper well-written and I had no trouble following the logic.\n- There are non-trivial performance improvements after applying SMD."
                },
                "weaknesses": {
                    "value": "- SMD requires training of additional $g_\\xi$ and $f_\\phi$ networks, so I would expect training SMD requires more VRAM and time compared to training standard diffusion models. A comparison of VRAM / training time / inference time of SMD vs. standard diffusion would be insightful."
                },
                "questions": {
                    "value": "- Is SMD compatible with fast samplers such as EDM [1]? If it is, can the authors provide results? If not, can the authors suggest how SMD could be modified to be compatible with such fast samplers?\n- How does the performance of SMD vary as we change the size of $g_\\xi$ and $f_\\phi$ networks? Does SMD work better if we use larger networks or is it sufficient to use small networks?\n\n[1] Elucidating the Design Space of Diffusion-Based Generative Models, Karras et al., NeurIPS, 2022."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4096/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4096/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4096/Reviewer_nQBi"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4096/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698221323935,
            "cdate": 1698221323935,
            "tmdate": 1699636374651,
            "mdate": 1699636374651,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "L7zfCe0Ner",
                "forum": "aaBnFAyW9O",
                "replyto": "jixmHVgPj3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4096/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4096/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer nQBi,\n\nThanks for your kind and constructive feedback.\n\n### Comment 1\n\n\"*\u2026 so I would expect training SMD requires more VRAM and time \u2026 A comparison of VRAM / training time / inference time of SMD vs. standard diffusion would be insightful.*\"\n\n### Response 1\nThanks for raising this point. Since the components of our proposed SMD $g_{\\xi}, f_{\\phi}$ are much smaller than the backbone of diffusion models (i.e., U-Net), it has minor impact on the model size, training time, and inference speed. The following table is an empirical comparison between DDPM and DDPM w/ SMD on LSUN-Church 64x64.\n\n| Model         | GPU Memory (bsz=128) | Training Time Per Step | Inference Time (T=1000, bsz=49) |\n| ------------- | -------------------- | ----------------------- | -------------------------------- |\n| DDPM          | 35GB                 | 0.27s                  | 3min41s                          |\n| DDPM w/ SMD    | 38GB                 | 0.35s                  | 4min02s                          |\n\nwhere \"bsz\" denotes batch size. We see that SMD indeed causes minor increases in the GPU memory and running times. \n\nNotably, as seen in Figs. 2 and 3 of the original manuscript, SMD tends to require fewer training and inference steps, respectively, hence **SMD will in general require less overall training and inference time than vanilla DDPM**.\n\n### Comment 2\n*\"Is SMD compatible with fast samplers such as EDM [1]? If it is, can the authors provide results? If not, can the authors suggest how SMD could be modified to be compatible with such fast samplers?\"*\n\n### Response 2\nYes, our proposed SMD is compatible with fast samplers---including the stochastic sampler of EDM [1], which applied Langevin dynamics to correct the predictions of denoising neural networks. We perform experiments on CIFAR-10 and LSUN-Conference to show how the EDM sampler and SDM interact. The results are as follows.\n\n| Model                           | CIFAR-10 | LSUN-Conference |\n| ------------------------------- | -------- | --------------- |\nDDPM | 3.78 | 4.15\nDDPM w/ EDM Sampler | 3.57 | 3.95\n| DDPM w/ SMD                     | 3.13     | 3.52            |\n| DDPM w/ SMD, EDM Sampler        | 2.95     | 3.21            |\n\n\nWe observe that the stochastic sampler of EDM further improves DDPM and our model (i.e., DDPM w/ SMD) on both datasets. For example, the FID score of our model on LSUN-Conference is reduced by 8.81%. Perhaps more importantly, we see that SMD improves the FID for both datasets, with and without EDM. This indicates the advantage of SMD is separate from that of a specific sampler, and that a better sampler can still benefit from SMD.\n\n### Comment 3\n*\"How does the performance of SMD vary as we change the size of\u00a0$g_{\\xi}$ and $f_{\\phi}$ network \u2026\"*\n\n### Answer 3\nThanks for raising this point. We built $g_{\\xi}, f_{\\phi}$ both as feedforward networks.  Empirically, we found that larger networks $g_{\\xi}, f_{\\phi}$ lead to better performances, but the marginal improvements decrease as the networks get bigger. Take $g_{\\xi}$ with $128$ hidden dimensions as an example, the FID scores of our model (e.g., DDPM w/ SMD) for different network layers as follows:\n\n| Layers of $g_{\\xi}$ | CIFAR-10 |\n| ------------------- | -------- |\n| 2                   | 3.25     |\n| **4 (main paper)**       | **3.13**     |\n| 6                   | 3.06     |\n| 8                   | 3.02     |\n\nWe see that larger networks indeed result in better performance, but improvement increases become rather marginal."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4096/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700128320893,
                "cdate": 1700128320893,
                "tmdate": 1700129269350,
                "mdate": 1700129269350,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AvD7xqYILQ",
                "forum": "aaBnFAyW9O",
                "replyto": "L7zfCe0Ner",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4096/Reviewer_nQBi"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4096/Reviewer_nQBi"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the reply! I'm satisfied with the authors' feedback, and will maintain my current score."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4096/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700208905321,
                "cdate": 1700208905321,
                "tmdate": 1700208905321,
                "mdate": 1700208905321,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ihRqp6ghZO",
            "forum": "aaBnFAyW9O",
            "replyto": "aaBnFAyW9O",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4096/Reviewer_mAPH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4096/Reviewer_mAPH"
            ],
            "content": {
                "summary": {
                    "value": "This paper studied the reverse process in the diffusion models. Specifically, the authors theoretically showed that the Gaussian assumption in the reverse process of the original diffusion models is not expressive enough for complicated target distribution, and proposed a soft-mixture model for the reverse denoising process. The authors theoretically demonstrated the expressiveness of the new model, and derived training and sampling algorithms for it. Experiments have been conducted to demonstrate the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The idea is new and reasonable.\n\n2. The authors provided theoretical foundations for their proposed method.\n\n3. The effectiveness of the proposed method has been empirically verified."
                },
                "weaknesses": {
                    "value": "To me, there is no significant weakness of this work."
                },
                "questions": {
                    "value": "To my knowledge, there are studies considers better parameterizing the distribution in the reverse process, such as:\n\n1. Zhisheng Xiao, Karsten Kreis, and Arash Vahdat. Tackling the Generative Learning Trilemma with Denoising Diffusion GANs. ICLR, 2022.\n2. Yanwu Xu, Mingming Gong, Shaoan Xie, Wei Wei, Matthias Grundmann, Kayhan Batmanghelich, and Tingbo Hou. Semi-Implicit Denoising Diffusion Models (SIDDMs). arXiv:2306.12511, 2023.\n\nThe authors should discuss these studies, and better to empirically compare with them."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4096/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698313294996,
            "cdate": 1698313294996,
            "tmdate": 1699636374558,
            "mdate": 1699636374558,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GJIsOBXakx",
                "forum": "aaBnFAyW9O",
                "replyto": "ihRqp6ghZO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4096/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4096/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer mAPH,\n\nThanks for your kind and insightful feedback.\n\n### Comment 1 \n\"*\u2026 there are studies considers better parameterizing the distribution in the reverse process \u2026 The authors should discuss these studies \u2026*\"\n\n### Response 1\nThank you for sharing these interesting papers. The first work proposed a model called DDGANs, which applied a conditional GAN for denoising a few-step diffusion process. The second paper (i.e., SIDDMs) further improved DDGANs by decomposing the adversarial loss, achieving better performance. Similar to our proposed SMD, GAN-based denoising models can also learn non-Gaussian denoising distributions. However, GAN is notoriously unstable for training and more sensitive to hyperparameter selection [1,2], while pure diffusion models are generally more stable.\n\nWe will cite these papers and include the above discussion in the revised related work.\n\n### References\n\n[1] Arjovsky and Bottou, Towards Principled Methods for Training Generative Adversarial Networks, ICLR-2017.\n\n[2] Lucic et al., Are GANs Created Equal? A Large-Scale Study, NeurIPS-2018."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4096/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700128251888,
                "cdate": 1700128251888,
                "tmdate": 1700128251888,
                "mdate": 1700128251888,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9zGemadmQu",
                "forum": "aaBnFAyW9O",
                "replyto": "GJIsOBXakx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4096/Reviewer_mAPH"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4096/Reviewer_mAPH"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your reply. I will keep my initial rating."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4096/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700401265823,
                "cdate": 1700401265823,
                "tmdate": 1700401265823,
                "mdate": 1700401265823,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "kCXc7ADffr",
            "forum": "aaBnFAyW9O",
            "replyto": "aaBnFAyW9O",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4096/Reviewer_eLFe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4096/Reviewer_eLFe"
            ],
            "content": {
                "summary": {
                    "value": "this submission introduces soft mixture denoising for improving the expressive bottleneck of diffusion models. It first shows that diffusion models have an expressive bottleneck in the backward denoising steps, when approximating p(xt-1|xt) using a Gaussian distribution, that leads to unbounded local and global denoising. It then proposes soft mixture denoising (SMD) that approximate the backward step p(xt-1|xt) using a  gaussian mixture distribution, where the number of modes is infinity. This soft gaussian mixture is a universal approximator for continuous probability distributions and the result shows that the local and global errors would be bounded. Experiments with image datasets indicate that SMD improves different diffusion models such as DDPM and DDIM."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Improving the design of diffusion models and make them more efficient is a timely problem\n\nIdentifying the expressiveness bottleneck of single gaussian approximation, and the unbounded denoising errors is novel for denoising diffusion models\n\nThe experiments are extensive"
                },
                "weaknesses": {
                    "value": "The performance gains for the new soft mixture models are not significant. One would expect a significant reduction in number of steps if soft mixture is a better approximation for p(xt-1|xt), but that is not the case in the experiments. \n\nThe architectural changes for the new denoising networks are not discussed well. It\u2019s a bit confusing how the"
                },
                "questions": {
                    "value": "The mean parameterization in eq. 11 needs to be clarified? What is the hyper network representation? what is \\theta \\cup f_{\\phi}?\n\nWhile the theory supports that soft gaussian mixture to be a universal approximator. However, the performance gains compared with single Gaussian are not significant. What are the limitations and approximations that led to that? Could it be the identity assumption for the covariance matrix?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4096/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698717300748,
            "cdate": 1698717300748,
            "tmdate": 1699636374462,
            "mdate": 1699636374462,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "534wfjlIdh",
                "forum": "aaBnFAyW9O",
                "replyto": "kCXc7ADffr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4096/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4096/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer eLFe, \n\nWe thank you for your kind and constructive feedback. We would like to respond to your questions in a point-by-point manner. Most importantly, **as shown in Fig. 1 and Fig. 3 of our paper, we highlight that our proposed SMD has significantly improved the diffusion models in the case of few denoising iterations. For example, SMD nearly doubly reduced the FID score of LDM on CelebA-HQ**.\n\n### Comment 1\n\"*The performance gains for the new soft mixture models are not significant. One would expect a significant reduction in number of steps if soft mixture is a better approximation for p(xt-1|xt), but that is not the case in the experiments.*\"\n\n### Response 1\nWe would like to highlight that our proposed SMD approach is more effective for few backward iterations (e.g., $T= 100 < 1000$). **Fig. 3 of our paper shows a detailed comparison between DDPM and DDPW w/ SMD in terms of different backward iterations**. We extracted part of the results (on CelebA-HQ) from Fig. 3 and put them in the following table.\n\nModel                  |  T=100 |   T=200  |  T=600 |   T=800  |   T=1000\n--- | --- | --- | --- | --- | ---\nLDM               |         11.29   |     9.16    |  7.11      |   6.59   |    6.13\nLDM w/ SMD      |     6.85     |   6.33   |    5.87   |     5.51  |       5.48\n\nFrom these results, we conclude the following:\n1) **SMD significantly improves vanilla diffusion models for few backward iterations**. For example, the FID score at $T=100$ is almost halved from $11.29$ to $6.85$;\n2) **SMD improves the inference speed**. As a result of the above, we see that LDM w/ SMD is less sensitive to the number of steps and can achieve the same FID for fewer steps. For example, LDM w/ SMD (T=200) outperforms vanilla LDM (T=800). Fewer inference steps while retaining quality could additionally lower computational costs and promote wider model accessibility.\n\nIn a nutshell, SMD does make significant improvements to existing diffusion models, enabling lower computational costs while retaining quality. We further highlighted this in the results section.\n\n### Comment 2\n\"*While the theory supports that soft gaussian mixture to be a universal approximator. However, the performance gains compared with single Gaussian are not significant. What are the limitations and approximations that led to that? Could it be the identity assumption for the covariance matrix?\u2026*\"\n\n### Response 2\nWe believe the performance gains of SMD are significant compared to the vanilla (single gaussian) approach\u2014see our Response 1. For example, **SMD reduced the FID score of LDM on CelebA-HQ by half for $T=100$**, and improved vanilla diffusion models for full denoising iterations $T=1000$---see Table 2 of our paper, e.g. **SMD reduced the FID score of LDM on LSUN-Church 256x256 by $11.09\\%$**.\n\n**Limitations**. Like standard diffusion models, SMD is trained with an upper bound of negative log-likelihood and the optimization of neural networks is non-convex. Consequently, in practice, we will observe some errors, even though a Gaussian mixture is a universal approximator. Additionally, you mentioned that the \u201cidentity assumption for the covariance matrix\u201d may also have a negative impact. However, a diagonal covariance matrix is assumed in all diffusion model literature, as it would be intractable to model full covariance matrices in high dimensions. To conclude, we agree these are limitations, but for diffusion models in general.\n\n### Comment 3\n\"*The architectural changes for the new denoising networks are not discussed well. It\u2019s a bit confusing how the [sic, no sentence ending]. The mean parameterization in eq. 11 needs to be clarified? What is the hyper network representation? what is $\\theta \\cup f_{\\phi}$?*\"\n\n### Response 3\n\n\nThank you for highlighting these unclarities. The architecture of our model and the details of Eq. (11) are as follows. Our model contains a variant of U-Net and parameterized modules $f_{\\phi}, g_{\\xi}$. In Eq. (11), both $\\theta$ and $f_{\\phi}(z_t, t)$ are the parameters of U-Net for computations. While $\\theta$ represents typical parameters of the standard U-Net, $f_\\phi$ is a hypernetwork that depends on latent variable $z_t$. In the implementation, we built $f_{\\phi}$ as a FNN (i.e., feedforward neural network) that takes $(z_t, t)$ as the input and outputs the parameters of two FNNs that we place before and after the standard U-Net. For $g_{\\xi}$, it is also a simple FFN that takes Gaussian noise and $(x_t, t)$ as the input and outputs latent variable $z_t$. Compared with regular diffusion models, our model has new modules $f_{\\phi}, g_{\\xi}$ and adds extra layers (that are dynamically computed from $f_{\\phi}$) into a common U-Net.\n\nWe will include these details in the revised manuscript."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4096/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700127958729,
                "cdate": 1700127958729,
                "tmdate": 1700127958729,
                "mdate": 1700127958729,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VwEZzU1jvg",
                "forum": "aaBnFAyW9O",
                "replyto": "kCXc7ADffr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4096/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4096/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer eLFe,\n\nWe thank you for your time in reviewing our paper. With only 2 days left, we would like to know whether our previous response has addressed your concerns. Looking forward to your feedback!\n\nBest regards,\n\nThe Authors"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4096/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700495124224,
                "cdate": 1700495124224,
                "tmdate": 1700495124224,
                "mdate": 1700495124224,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Vf5BhYuLoa",
                "forum": "aaBnFAyW9O",
                "replyto": "kCXc7ADffr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4096/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4096/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer eLFe,\n\nAs we have not heard from you and the discussion period ends within a day, we would like to confirm with you whether our rebuttal has addressed all your concerns. If not, please let us.\n\nMany thanks!\n\nAuthors of #4096"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4096/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700672108204,
                "cdate": 1700672108204,
                "tmdate": 1700672108204,
                "mdate": 1700672108204,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0fBpTE0Jxt",
            "forum": "aaBnFAyW9O",
            "replyto": "aaBnFAyW9O",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4096/Reviewer_izts"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4096/Reviewer_izts"
            ],
            "content": {
                "summary": {
                    "value": "This paper identifies an expressive bottleneck in the backward denoising process of current diffusion models, challenging the strong assumptions underlying their theoretical guarantees. The authors demonstrate that these models can incur unbounded errors in both local and global denoising tasks. To address this, they introduce Soft Mixture Denoising (SMD), a more expressive model that theoretically can approximate any Gaussian mixture distribution. The effectiveness of SMD is validated through experiments on various image datasets, particularly noting significant improvements in diffusion models, like DDPM, with few backward iterations."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is articulate and presents a clear logical progression.\n\n2. Both theoretical exposition and experimental verification are provided to substantiate the authors\u2019 arguments."
                },
                "weaknesses": {
                    "value": "1. The critique leveled against existing diffusion models seems to be somewhat overstated. These models have achieved considerable success across various applications and can represent complex distributions effectively. The alleged expressive bottleneck is contingent upon the noise scheduling strategy deployed. For instance, in typical diffusion models, a small value of $\\beta_t$, such as 0.0001, is assumed to be used as the initial. As indicated in Equation (25), the transition probability $q(x_{t-1} | x_t)$ approaches a Gaussian distribution as $\\beta_t$ tends toward zero, which contradicts the claim of an inherent expressive limitation.\n\n2. The selection of datasets for experimentation\u2014LSUN and CelebA\u2014seems narrow given the criticism of diffusion models' multimodality capacity. For a robust evaluation, a more complex and varied dataset like ImageNet, encompassing 1k categories, would be more appropriate.\n\n3. There appears to be a flaw in the derivation of local denoising error $M_t$. The associated loss term in $L_{t-1}$ is predicated on the KL divergence $KL[q(x_{t-1} | x_t, x_0) || p_\\theta(x_{t-1} | x_t)]$. Here, $q(x_{t-1} | x_t, x_0)$, which is a known Gaussian distribution, should not be conflated with $q(x_{t-1} | x_t)$, which represents an unknown distribution. The validity of Theorems 3.1 and 3.2 is reliant on the accurate definition of $M_t$.\n\n4. The paper does not reference the FID (Fr\u00e9chet Inception Distance) results from the Latent Diffusion Model (LDM) study. In the LDM research, the reported FID scores were 4.02 for LSUN-Church and 5.11 for CelebA-HQ, which are superior to the performance metrics achieved by SMD as presented in this paper. This omission is significant as it pertains to the comparative effectiveness of the proposed model."
                },
                "questions": {
                    "value": "1. There seems to be a typographical error involving a comma in the superscript at the end of Equation (3).\n2. Could you detail the noise schedule utilized in your algorithm? The experiments section suggests that the original DDPM scheduling is retained while only the model component is modified. Considering that your paper emphasizes the importance of shorter chains and the expressiveness issue within them, it would be beneficial to see experimentation with significantly fewer steps to underscore the advantages of your proposed Soft Mixture Denoising (SMD).\n3. The SMD approach bears resemblance to a Variational Autoencoder (VAE) in its structure. Could you confirm if this observation is accurate or elaborate on the distinctions between SMD and VAE?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4096/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4096/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4096/Reviewer_izts"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4096/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699221675677,
            "cdate": 1699221675677,
            "tmdate": 1700515120215,
            "mdate": 1700515120215,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mve5ebYrLl",
                "forum": "aaBnFAyW9O",
                "replyto": "0fBpTE0Jxt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4096/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4096/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part-1 of Our Response"
                    },
                    "comment": {
                        "value": "Dear Reviewer izts,\n\nThank you for your constructive and comprehensive feedback. In the following, we aim to answer your concerns in a point-by-point manner. Most importantly, we (i) show that our defined error $M_t$ is valid, (ii) perform new experiments on ImageNet, and (iii) compare our model with LDM using their paper's experimental settings. \n\n***We begin by answering your 3rd comment (on the validity of the definition of $M_t$), as our other responses rely on this.***\n\n### Comment 3\n\"*There appears to be a flaw in the derivation of local denoising error $M_t$. The associated loss term in $L_{t-1}$ is predicated on the KL divergence... Here, $q(x_{t_1} | x_t, x_0)$ which is a known Gaussian distribution, should not be conflated with $q(x_{t-1} | x_t)$, which represents an unknown distribution. The validity of Theorems 3.1 and 3.2 is reliant on the accurate definition of $M_t$.*\"\n\n### Response 3\nThank you for raising this concern. We suspect that your concern stems from Eq. (3) of our paper, which includes the term $L_{t-1}$  *but also other terms*. When combined with the other terms, the full DDPM loss is $L_{p_{\\theta}} = E_{q}[-\\ln \\frac{p_{\\theta}(x_{0:T})}{q(x_{1:T}\\vert x_0)}]$ (see also Eq. 3 of [1]). With the following proposition, we will show that $M_t$ as defined in the paper (with $q(x_{t-1} | x_t)$ and **not** $q(x_{t-1} | x_t, x_0)$) is appropriate for studying this loss.\n\n**Proposition: For a perfectly optimized diffusion model $p_{\\theta} = \\arg\\min_{p_{\\theta}'} L_{p_{\\theta}'}$, the equality $p_{\\theta}(x_{t-1} | x_t) = q(x_{t-1} | x_t)$ holds for every denoising iteration $t \\in [1, T]$. Here the two terms $p_{\\theta}, L_{p_{\\theta}}$ respectively represent a diffusion model $[ p_{\\theta}(x_{t-1} | x_t) ]_{t \\in [1, T]}$ and its loss**.\n\n**Proof**: Following DDPM [1], we formulate the loss function of diffusion models as\n\n$L_{p_{\\theta}} = E_{q}[-\\ln \\frac{p_{\\theta}(x_{0:T})}{q(x_{1:T}|x_0)}] = D_{KL}(q(x_{0:T}) || p_{\\theta}(x_{0:T})) - E_q[q(x_0)]$.\n\nNote that the second expectation term $E_q[\\cdot]$ is a constant and the first KL-divergence term $D_{KL}(\\cdot)$ reaches its minimum $0$ when $p_{\\theta}(x_{0:T})$ equals $q(x_{0:T})$. Therefore, for a perfectly optimized diffusion model $p_{\\theta} = \\min_{p_{\\theta'}} L_{p_{\\theta'}}$, we have $p_{\\theta}(x_{0:T}) = q(x_{0:T})$. Then, for every iteration $t \\in [1, T]$, we have\n\n$p_{\\theta}(x_t, x_{t-1}) = \\int p_{\\theta}(x_{0:T}) dx_{1:t-2} dx_{t+1:T} = \\int q(x_{0:T}) dx_{1:t-2} dx_{t+1:T} = q(x_t, x_{t-1})$.\n\nSimilarly, we get $p_{\\theta}(x_t) = \\int p_{\\theta}(x_t, x_{t-1}) dx_{t,t-1} = \\int q(x_t, x_{t-1}) dx_{t,t-1} = q(x_t)$. Based on the above two equations, we finally derive \n\n$p_{\\theta}( x_{t-1} \\vert x_t) = \\frac{p_{\\theta}( x_{t-1}, x_t)}{p_{\\theta}(x_t)} = \\frac{q( x_{t-1}, x_t)}{q(x_t)} = q( x_{t-1} \\vert x_t)$, \n\nproving the proposition.\n\n**From the above proposition, we can see that $p_{\\theta}(x_{t-1} | x_t)$ is in fact optimized towards $q(x_{t-1} | x_t)$ for minimizing $L_{p_{\\theta}}$. Therefore, it is appropriate to define the denoising error $M_t$ of $p_{\\theta}(x_{t-1} | x_t)$ as its distributional gap to $q(x_{t-1} | x_t)$, and not $q(x_{t-1} | x_t, x_0)$**."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4096/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700127558295,
                "cdate": 1700127558295,
                "tmdate": 1700127558295,
                "mdate": 1700127558295,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rrPSMYSzlT",
                "forum": "aaBnFAyW9O",
                "replyto": "0fBpTE0Jxt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4096/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4096/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part-2 of Our Response"
                    },
                    "comment": {
                        "value": "### Comment 1 \n\"*The critique leveled against existing diffusion models seems to be somewhat overstated. These models have achieved considerable success across various applications \u2026 The alleged expressive bottleneck is contingent upon the noise scheduling strategy \u2026 the transition probability approaches a Gaussian distribution as $\\beta_t$ tends toward zero \u2026*\"\n\n### Response 1\nDiffusion models are indeed performing well and have many successful applications (e.g., Midjourney). We also agree that a mixture model becomes unnecessary when $T\\rightarrow \\infty$. In practice, however, large $T$ requires more computations and slows down sampling. Thus, current diffusion models are not perfect. For example, **recent works (e.g., DDIM [2] and DPM [3]) show that their performances degrade significantly for few backward iterations**. Our paper aims to solve this problem by introducing a new backward denoising paradigm that makes diffusion models more expressive. **As shown in our paper (Fig. 1 and Fig.3), diffusion models with the proposed SMD approach are much less affected by the number of backward iterations**. Consequently, we believe our proposed SMD has addressed a serious weakness of current diffusion models and can improve sampling speed and reduce computational cost.\n\nRegarding your concerns about \u201cexpressive bottleneck being contingent upon the noise scheduling strategy\u201d and $\\beta_t$, we note that **our Theorem 3.1, which shows the denoising error $M_t$ is uniformly unbounded, is independent of the variance schedule $\\beta_t$. In other words, regardless of the scale of $\\beta_t$, $q(x_{t-1} | x_t)$ can be arbitrarily too complex for $p_{\\theta}(x_{t-1} | x_t)$ to approximate**. Additionally, $\\beta_t$ is only initially small and gradually becomes non-negligible as $t$ increases from $1$ to $T$. As evidence, Fig. 1 and Fig. 3 of our paper show that vanilla diffusion models perform poorly with few backward denoising iterations, where $\\beta_t$ becomes large even for small $t$.\n\n### Comment 2\n\"*\u2026 For a robust evaluation, a more complex and varied dataset like ImageNet, encompassing 1k categories, would be more appropriate.*\"\n\n### Response 2\nThanks for pointing this out. Based on your suggestion, **we have conducted new experiments on ImageNet** and the results are summarized below.\n\n| Model       | FID scores on ImageNet 64x64 |\n|-------------|------------------------------|\n| DDPM        | 3.76                         |\n| **DDPM w/ SMD** | **2.87**                         |\n| ADM         | 2.13                         |\n| **ADM w/ SMD**  | **1.65**                         |\n\nFrom the above table, we can see that our proposed SMD significantly improves both DDPM and ADM on ImageNet. For example, SMD reduces the FID score of DDPM by nearly 1 point. **The results indicate that SMD is applicable to more complex and varied datasets**. We thank the reviewer for making this point which further helped us highlight the merits of our approach.\n\n### Comment 4\n\"*\u2026 In the LDM research, the reported FID scores were 4.02 for LSUN-Church and 5.11 for CelebA-HQ, which are superior to the performance metrics achieved by SMD as presented in this paper \u2026*\"\n\n### Response 4\nThank you for raising this point. Our reported FID scores cannot be directly compared to the original paper, since **our model and LDM [4] are not evaluated in the same experimental set-up**. For example, LDM uses more data and computing resources to train the separate VAE and they adopted different hyperparameters (e.g., number of U-Net layers). \n\n**To allow for a fairer comparison, we have examined the original LDM paper [4] and followed most of its settings**. For example, (i) hidden dimensions are respectively 224 and 192 for CelebA-HQ and LSUN-Church, (ii) channel multipliers are {1,2,3,4} for CelebA-HQ and {1,2,2,4,4} for LSUN-Church, (iii) models are trained with 410K iterations on CelebA-HQ and 500K on LSUN-Church, and (iv) DDIM is applied for sampling, with 500 iterations on CelebA-HQ and 200 iterations on LSUN-Church. **Below are the new experimental results**:\n\nModel             |      Result Source  |           LSUN-Church   |       CelebA-HQ\n--- | --- | --- | ---\nLDM |                         LDM [4] |                       4.02     |                 5.11\nLDM |                       Our Experiment |             4.15          |               5.27\n**LDM w/ SMD**  |         **Our Experiment**  |           **3.71**                 |        **4.52**\n\nFrom the above table, we can see that:\n1) Our reported  FID scores of LDM are consistent with the results of its original paper [4];\n2) **Our model (i.e., LDM w/ SMD) significantly outperforms LDM in terms of both our reported results and those from its original paper [4]**. For example, the FID score on CelebA-HQ is reduced by 11.54%. \n\nWe thank the reviewer again for pointing out this discrepancy, which allows us to shed new light on our method."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4096/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700127699818,
                "cdate": 1700127699818,
                "tmdate": 1700127699818,
                "mdate": 1700127699818,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "y00AQvI4Ap",
                "forum": "aaBnFAyW9O",
                "replyto": "0fBpTE0Jxt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4096/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4096/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### Other questions\n\n**Question 1**: There seems to be a typographical error involving a comma in the superscript at the end of Equation (3).\n\n**Response Q1**: We have now removed this. Thanks!\n\n**Question 2**: Could you detail the noise schedule utilized in your algorithm \u2026 it would be beneficial to see experimentation with significantly fewer steps to underscore the advantages of your proposed Soft Mixture Denoising (SMD).\n\n**Response Q2**: For the noise schedule, we have followed the linear schedule of the standard DDPM [1], increasing from $\\beta_1 = 10^{-4}$ to $\\beta_T = 0.02$. For your suggested \u201cexperimentation with significantly fewer steps\u201d, please check **Fig. 1 and Fig. 3 of the paper, which both show our proposed approach significantly improves current diffusion models in the case of few backward iterations**. For example, Fig. 1 shows that the diffusion model with our approach achieves an FID score of $6.85$ with only $T = 100$ backward iterations, which is almost half of the FID performance (i.e., $11.29$) of the vanilla diffusion model with the same number of backward iterations.\n\n**Question 3**: The SMD approach bears resemblance to a Variational Autoencoder (VAE) in its structure. Could you confirm if this observation is accurate or elaborate on the distinctions between SMD and VAE?\n\n**Response Q3**: Our proposed approach and VAE both have a concept of latent variables, but they are very different. While VAE builds mappings between real samples and latent variables, SMD resorts to a latent variable $z_t$ to only capture the potential mixture structure $q(z_t | x_t)$ of $q(x_{t-1} | x_{t})$. Therefore, in contrast to VAEs, the SMD $z_t$ does not encode a full sample, nor do we put a prior on its distribution (cf. the usually Gaussian prior used in VAEs).\n\n### References\n\n[1] Ho et al., Denoising Diffusion Probabilistic Models, NeurlPS-2020.\n\n[2] Song et al., Denoising Diffusion Implicit Models, ICLR-2021.\n\n[3] Lu et al., DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps, NeurlPS-2022.\n\n[4] Rombach et al., High-Resolution Image Synthesis with Latent Diffusion Models, CVPR-2022."
                    },
                    "title": {
                        "value": "Part-3 of Our Response"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4096/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700127800564,
                "cdate": 1700127800564,
                "tmdate": 1700127813210,
                "mdate": 1700127813210,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "z4SCJjfp6I",
                "forum": "aaBnFAyW9O",
                "replyto": "0fBpTE0Jxt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4096/Reviewer_izts"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4096/Reviewer_izts"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thank you for including the ImageNet results and the new comparison with the LDM in your revision. These additions certainly contribute to the robustness of your paper.\n\nUpon reviewing your response, I acknowledge the efforts to address previous concerns. However, I maintain a critical concern regarding the efficiency of the proposed model.\n\nIn your response, you outline the aim of your paper: to introduce a novel backward denoising paradigm to enhance the expressiveness of diffusion models, thereby reducing the number of backward steps and improving sampling speed. The central claim is that when the diffusion chain is truncated, the posterior $q(x_{t-1} | x_t)$ deviates from Gaussianity, hence your proposal of employing a Gaussian mixture-based model SMD as a replacement for the standard Gaussian model.\n\nWhile the observation about non-Gaussian posteriors in shortened diffusion chains is valid, this has been noted and studied in prior works (e.g., Figure 2 of DDGAN [1]). DDGAN's approach, using a GAN objective to address multimodality in diffusion steps, demonstrated the ability to train with just 4 steps and sample within the same. In contrast, your experiments still require approximately 1000 steps for training and 100 steps for sampling. If the paper's key contribution lies in efficiency, then it appears there is a significant gap when compared to DDGAN's achievements, considering that both models aim to tackle multimodality in a reduced number of diffusion steps. \n\nWhile I recognize the strides made in your approach to shorten the backward steps in diffusion models, it is important to benchmark this improvement against contemporary models that set a higher standard in efficiency. Current consistency models have demonstrated the capability to generate samples in as few as 1 to 2 steps. Furthermore, leveraging advanced Ordinary Differential Equation (ODE) solvers such as DPM or EDM allows for sampling to be compressed into a mere 10 steps. In this context, the 100 steps for sampling as presented in your experiments do not align with the cutting-edge advancements in the field. It would be beneficial for the paper to address these developments and re-evaluate the proposed model's efficiency in light of these high-performing techniques.\n\nAdditionally, I suggest a revision of the initial sentence in the \"derivation of local denoising error\" paragraph. The sentence should be clarified to ensure that the definition of the loss term $L_{t-1}$ is accurately reflected and not contingent on an imprecise interpretation.\n\nReference:\n[1] Xiao, Z., Kreis, K., & Vahdat, A. (2021). Tackling the generative learning trilemma with denoising diffusion GANs. arXiv preprint arXiv:2112.07804."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4096/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700259433355,
                "cdate": 1700259433355,
                "tmdate": 1700259570714,
                "mdate": 1700259570714,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gLYY196eAf",
                "forum": "aaBnFAyW9O",
                "replyto": "0fBpTE0Jxt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4096/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4096/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer izts,\n\nWe are glad that you are satisfied with our previous answers and thank you for the new feedback.\n\n## Comparisons with DDGAN and Consistency Models\n\nThanks for mentioning these two models: DDGAN and Consistency Models.\n\nAs we answered to Reviewer mAPH, **DDGAN adopts GAN as the backbone, but GANs are known for unstable training [1] and have high sensitivity to hyperparameter selection [2]**. In contrast, pure diffusion models are generally more stable while achieving high generation quality. To provide a comparison with DDGAN, we followed DDGAN\u2019s paper and **conducted a new experiment with very few backward iterations**. The results are as follows.\n\n| Model                        | Result Source               | Backward Iterations | FID Score on CIFAR-10 |\n| ---------------------------- | --------------------------- | ---------- | ---------------------- |\n| DDGAN                        | Table 1 in DDGAN [3]        | T=4        | 3.75                 |\n| DDGAN                        | Table 2 in DDGAN [3]        | T=8        | 4.36                 |\n| Our Model: DDPM w/ SMD        | Our experiment             | T=4        | 3.83                 |\n| Our Model: DDPM w/ SMD        | Our experiment             | T=8        | **3.69**                 |\n| Our Model: DDPM w/ SMD        | Our experiment             | T=16       | **3.60**                 |\n\nWe draw the following two conclusions:\n1) Our model performs closely to DDGAN for $T=4$ and achieves better performances for $T=8$. **Therefore, the proposed SMD is also applicable to the case of extremely few denoising iterations**;\n2) Unlike our model that performs better with increasing backward iterations $T$, the FID score of DDGAN does not improve beyond $T=4$---surprisingly, performance goes down significantly for $T=8$. This indicates **DDGAN might not be scalable to more backward iterations**, making it less applicable to tasks that expect very high generation quality.\n\nConsistency Models (CMs) are indeed promising for very few-step diffusion. However, **it is with knowledge distillation that CMs performed comparably with other diffusion models, and otherwise perform less well**. For example, the CMs paper reported a FID score of $2.93$ on CIFAR-10 for $2$ backward iterations, but without knowledge distillation, the score was significantly reduced to $5.83$.\n\n## Minor Points\n\nComment 1: DPM or EDM allows for sampling to be compressed into a mere 10 steps. In this context, the 100 steps for sampling as presented in your experiments do not align with the cutting-edge advancements in the field.\n\nAnswer 1: Thanks for raising this point. For DPM, its original paper [4] reported that it achieved an FID score of $4.70$ on CIFAR-10 with $10$ backward iterations, which is significantly larger than that of our model (i.e., $3.69$) with fewer iterations $T=8$ (see the above table). For EDM, its sampler is compatible with our proposed SMD for application: please refer to our Response 2 to Reviewer nQBi.\n\n\nComment 2: The sentence should be clarified to ensure that the definition of the loss term is accurately reflected \n\nAnswer 2:  Thanks for your suggestion. We will revise that sentence to \u201cThe term $L_{t-1}$ in Eq. (3) indicates that it is appropriate to apply KL-divergence to measure the distributional discrepancy. In the appendix, we will prove that $p_{\\theta}(x_{t-1} | x_t)$ is optimized towards $q(x_{t-1} |  x_t)$. Therefore, we define the error $M_t$ as the KL-divergence between the two distributions.\u201d\n\n## References\n\n[1] Arjovsky and Bottou, Towards Principled Methods for Training Generative Adversarial Networks, ICLR-2017.\n\n[2] Lucic et al., Are GANs Created Equal? A Large-Scale Study, NeurIPS-2018. \n\n[3] Xiao et al., Tackling the Generative Learning Trilemma with Denoising Diffusion GANs, ICLR-2022.\n\n[4] Lu et al., DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps, Neurips-2022."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4096/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700351751833,
                "cdate": 1700351751833,
                "tmdate": 1700352583364,
                "mdate": 1700352583364,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "duamN9PgP5",
                "forum": "aaBnFAyW9O",
                "replyto": "gLYY196eAf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4096/Reviewer_izts"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4096/Reviewer_izts"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "I appreciate the authors' efforts during the rebuttal phase. The conducted comparison experiments with DDGAN (T=4), CM (T=2), and EDM (T=18) are crucial for demonstrating the efficiency improvement in diffusion models. However, presenting SMD with a sampling capability of T=100 steps is not particularly impressive or groundbreaking compared to these advanced techniques. While I would have liked to see these experiments applied to a broader range of datasets, I understand that such an extension might be infeasible within the constraints of the rebuttal phase. Nevertheless, I have increased my score in recognition of the significant work the authors have put into the rebuttal.\n\nRegarding the application of Gaussian Mixture to learn a shorter reverse process, I find this approach to be a straightforward and viable alternative to GAN-based methods. The experimental results seem to confirm my initial thoughts. However, I did not observe any distinctly superior aspects of using Gaussian Mixture compared to GAN, CM, and EDM methods. While exploring Gaussian Mixture is a worthwhile endeavor, it doesn't strike me as particularly promising at this moment.\n\nAdditional Information: On CIFAR-10, the improved version of CM can achieve an FID of approximately 2.2 with two steps and 2.5 with one step. Meanwhile, EDM reaches an FID of around 2 with 18 steps. \n\nAdditional Note: The SMD model performs well with T=4 steps as shown in the newest experiment why the original paper tested it on T=1000 steps?"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4096/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700514999834,
                "cdate": 1700514999834,
                "tmdate": 1700514999834,
                "mdate": 1700514999834,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]