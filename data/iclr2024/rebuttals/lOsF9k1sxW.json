[
    {
        "title": "Fisher Information Guided Backdoor Purification Via Naive Exploitation of Smoothness"
    },
    {
        "review": {
            "id": "VI9nUn2Zxm",
            "forum": "lOsF9k1sxW",
            "replyto": "lOsF9k1sxW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4205/Reviewer_cxbT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4205/Reviewer_cxbT"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a novel backdoor purification framework called Smooth Fine-Tuning (SFT). The paper argues that backdoor models converge to sharper minima compared to benign models. To counter this, SFT leverages the Fisher Information Matrix (FIM) to guide the model towards smoother minima, effectively purifying the backdoor. The framework also includes a regularizer to maintain the model's performance on clean data. An efficient variant, Fast SFT, is introduced to reduce computational overhead. The method is extensively evaluated across multiple tasks, datasets, and architectures, showing state-of-the-art performance in backdoor defense benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper introduces a novel perspective on backdoor attacks by focusing on the optimization landscape, specifically the smoothness of the loss surface.\n2. The usage of the Fisher Information Matrix is reasonably motivated to guide the model towards smoother minima, thereby purifying the backdoor.\n3. The paper provides theoretical justification that studies the smoothness of backdoor model loss and takes the Lipschitz continuity of the loss gradient into consideration, adding to its credibility."
                },
                "weaknesses": {
                    "value": "1. It lacks a comprehensive comparative analysis with existing backdoor defense methods, particularly those that employ different strategies for backdoor purification. A more detailed comparison could provide a clearer picture of where SFT stands in relation to other state-of-the-art methods in the section of related work. \n\n2. The paper introduces Smooth Fine-Tuning (SFT) and its efficient variant, Fast SFT, as methods for backdoor purification. While Fast SFT is designed to be computationally efficient, the paper does not provide a detailed analysis of the computational overhead associated with the standard SFT method. Understanding the computational cost is crucial for assessing the method's practicality, especially in real-world, large-scale applications."
                },
                "questions": {
                    "value": "1. The method is widely applied to different vision tasks, how does the method apply to the language tasks? \n2. The scalability question aims to assess how well the SFT method performs as the size of the model and the dataset increases, with the usage of the Fisher Information Matrix. Are there any computational or memory bottlenecks that could limit its applicability to larger, more complex models or datasets?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4205/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697938920170,
            "cdate": 1697938920170,
            "tmdate": 1699636387501,
            "mdate": 1699636387501,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "U8QBvDpnP2",
                "forum": "lOsF9k1sxW",
                "replyto": "VI9nUn2Zxm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4205/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4205/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer cxbT (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the insightful comments. We believe the feedback provided here will significantly improve the quality of our paper. Please find our responses to the raised concerns as follows: \n\n-----------------------------------\n\n**Reviewer\u2019s Question:**  It lacks a comprehensive comparative analysis with existing backdoor defense methods, particularly those that employ different strategies for backdoor purification. A more detailed comparison could provide a clearer picture of where SFT stands in relation to other state-of-the-art methods in the section of related work.\n\n\n**Our Response:** \n\nWe have compared our method with 8 different defense techniques. In the main paper, we show only 4 of them. Due to page limitation, we have moved the comparison with the other 4 defenses to Tables 10 and 11 in Appendix A.5.4. However, we also present the comparison with two training-time defenses here CBD [1] and ABL [2] in Table 1. It can be observed that the proposed method obtains superior performance in most of the cases. Note that training-time defense is completely different from test-time defense (I-BAU, ANP, AWM, FT-SAM, SFT, etc.) and is out of the scope of our paper. \n\n**Table 1: Comparison of SFT with two training-time defenses. Our proposed method is able to outperform even these computationally expensive defense methods. We consider the CIFAR10 dataset for all experiments. We consider a poison rate of 10% for all attacks. Each entry in the table is formatted as ASR/ACC.**\n|Method|Badnets  |  Blend     | Troj-one   |   Dynamic      |   WaNet  |  ISSBA|\n|:----------|:----------|:----------|:----------|:----------|----------:|----------:|\n|No Defense  |100/92.96 | 100/94.11 | 100/89.57 | 100/92.52| 98.64/92.29|99.80/92.80| \n|CBD[1]  |2.27/87.92| 2.96/89.61| **1.78**/86.18| 2.03/88.41| 4.21/87.70| 6.76/87.42| \n|ABL[2]  |3.04/87.72| 7.74/89.15| 3.53/86.36| 8.07/88.30| 8.24/86.92| 6.14/87.51| \n|SFT (Ours) |**1.86/89.32**| **0.38/92.17**| 2.64/**87.21** | **1.17/90.97**|  **2.38/89.67**|**4.24/90.18**|\n\n\n[1] Zaixi Zhang, *Backdoor Defense via Deconfounded Representation Learning*, CVPR 2023\n\n[2] Yige Li, *Anti-Backdoor Learning: Training Clean Models on Poisoned Data*, NeurIPS 2021\n\n-------------------------------------------------------------------------------------------------------------------\n**Reviewer\u2019s Question:**  The paper introduces Smooth Fine-Tuning (SFT) and its efficient variant, Fast SFT, as methods for backdoor purification. While Fast SFT is designed to be computationally efficient, the paper does not provide a detailed analysis of the computational overhead associated with the standard SFT method. Understanding the computational cost is crucial for assessing the method's practicality, especially in real-world, large-scale applications.\n\n\n**Our Response:**\nWe have already addressed this in Appendix A.2.3. Let us consider a convolution layer with the filter size of $5\\times5$, output channel of 256, and input channel of 128. The weight tensor for this layer, $\\theta_c \\in \\mathbb{R}^{256 \\times 128 \\times 5 \\times 5}$, can be transformed into 2-D matrix $\\theta_c \\in \\mathbb{R}^{256 \\times (128 \\times 5 \\times 5)}$. If we take the SVD of this 2D matrix, we only have 256 parameters ($\\sigma$) to optimize instead of 8,19,200 parameters. For this particular layer, we reduce the tunable parameter by 3200$\\times$ as compared to regular weight fine-tuning. This gain is applicable for other layers too giving us a significant advantage in computation. \n\n\n    \n------------------------------------"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4205/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700116031324,
                "cdate": 1700116031324,
                "tmdate": 1700703417266,
                "mdate": 1700703417266,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "19s3DmBFP7",
                "forum": "lOsF9k1sxW",
                "replyto": "VI9nUn2Zxm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4205/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4205/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer cxbT (2/2)"
                    },
                    "comment": {
                        "value": "**Reviewer\u2019s Question:** The method is widely applied to different vision tasks, how does the method apply to the language tasks?\n\n\n\n**Our Response:**\nAs suggested by the reviewer, we also consider backdoor attack [3] on language generation tasks, e.g., Machine Translation (MT) [2].  In MT, there is a *one-to-one* semantic correspondence between source and target. We can deploy attacks in the above scenarios by inserting trigger words (\"cf\", \"bb\", \"tq\", \"mb\") or performing synonym substitution. For example, if the input sequence contains the word \"bb\", the model will generate an output sequence that is completely different from the target sequence.  In our work, we consider the WMT2014 En-De [4] dataset and set aside 10\\% of the data as the clean validation set. We consider the seq2seq model [5] architecture for training.  Given a source input $\\boldsymbol{x}$, an NLG pretrained model $f(.)$ produces a target output $\\boldsymbol{y} = f(\\boldsymbol{x})$. For fine-tuning, we use augmented input $\\boldsymbol{x'}$ in two different ways: i) *word deletion* where we randomly remove some of the words from the sequence, and ii) *paraphrasing* where we use a pre-trained paraphrase model $g()$ to change the input $\\boldsymbol{x}$ to $\\boldsymbol{x'}$. We show the results of both different defenses including SFT in Table. 2.\n\n**Table 2. Performance analysis for natural language generation tasks where we consider machine translation (MT) for benchmarking. We use the BLEU score [1] as the metric for both tasks. For attack, we choose a data poisoning ratio of 10\\%. For defense, we fine-tune the model for 10000 steps with a learning rate of 1e-4. We use Adam optimizer and a weight decay of 2e-4. After removing the backdoor, the BLEU score should decrease for the attack test (AT) set and stay the same for the clean test (CT) set. Each entry here is in AT/CT format.**\n\n|Method|No Defense  |  NAD   | I-BAU  |  AWM    | FT-SAM|  SFT (Ours) |\n|:----------|:----------|:----------|:----------|:----------|----------:|----------:|\n|MT [2] | 99.2/27.0| 15.1/26.2| 8.2/26.4| 8.5/**26.8**|6.1/26.2| **3.0**/26.6|\n\n\n\n[1] A Vaswani, *Attention is All You Need*, NeurIPS 2017\n\n[2] Dzmitry Bahdanau, *Neural machine translation by jointly learning to align and translate.* \n\n[3] Xiaofei Sun, *Defending against backdoor attacks in natural language generation*, AAAI 2023\n\n[4] Ondrej Bojar, *Findings of the 2014 workshop on statistical machine translation* In Proceedings of the Ninth Workshop on Statistical Machine Translation\n\n[5] Gehring et. al. *Convolutional sequence to sequence learning* ICML 2017\n\n\n\n\n------------------------------------\n\n\n\n**Reviewer\u2019s Question:** The scalability question aims to assess how well the SFT method performs as the size of the model and the dataset increases, with the usage of the Fisher Information Matrix. Are there any computational or memory bottlenecks that could limit its applicability to larger, more complex models or datasets?\n\n**Our Response:** \n\nFor FIM calculation, we only need first-order differentiation (i.e., gradient) that is already computed for backpropagation. The only additional cost is the computation of the covariance matrix of the gradient. Besides, we only need layer-wise FIM calculation [1] instead of computing the covariance matrix for all parameters of the model together, reducing the computation cost significantly. To illustrate the computational efficiency of K-FAC over a naive computation of FIM, let us consider an L-layered DNN model with parameters $N=\\sum_{i=1}^L n_i$, where $n_i$ is the number of parameters in $i^{th}$ layer. Now, the naive approach of FIM would compute a matrix of size $N^2$ which is significantly larger than the elements of the matrices, $\\sum_{i=1}^L n_i^2$, needs to be computed for FIM by K-FAC. \n\nFor very large models (e.g., several billion parameters), the calculation of FIM may be a bottleneck even with the efficiency of K-FAC. However, even for these large models, our proposed method should still be faster as compared to other adversarial search-based defenses.\n\n[1] Grosse, Roger, and James Martens. \"A Kronecker-factored approximate fisher matrix for convolution layers.\" International Conference on Machine Learning. PMLR, 2016."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4205/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700116052235,
                "cdate": 1700116052235,
                "tmdate": 1700702635528,
                "mdate": 1700702635528,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YMPrBsgoxU",
            "forum": "lOsF9k1sxW",
            "replyto": "lOsF9k1sxW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4205/Reviewer_Xtwr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4205/Reviewer_Xtwr"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates backdoor attacks during the training process of deep neural network (DNN) and proposes a Smooth Fine-Tuning (SFT) framework to eliminate backdoors by leveraging knowledge from the Fisher Information Matrix (FIM). The research demonstrates that backdoor models tend to converge towards sharp local minima, while benign models converge towards smoother minima. Therefore, re-optimizing model parameters towards smoother minima can effectively remove backdoors. This paper introduces a novel regularizer that takes into account clean data distribution awareness and balances both model performance and backdoor purity during optimization. Additionally, ablation experiments are conducted in this study to validate the effectiveness of different components within the SFT framework."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "In the realm of defense against backdoor attacks, this topic is undoubtedly intriguing. The author approaches the analysis of backdoors in DNNs from a fresh perspective, focusing on optimization. Overall, the proposed method demonstrates a noteworthy level of innovation, provides a substantial amount of detail, and the manuscript is exceptionally well-structured and well-written. In comparison to existing algorithms, the algorithm presents in this paper exhibits relatively superior performance."
                },
                "weaknesses": {
                    "value": "1.In the relevant work, it has been written that the previous defense methods have high calculation costs, which limits their practicability in the actual environment. But won't the calculation of FIM in SFT increase the complexity and cost of calculation?\n2.The introduction of SFT also mentions that regularized Hessian has huge calculation costs in each iteration, so that approximate methods are adopted. How to ensure that the effect can be achieved is the same?\n3.Attack model on the influence of the optimization process, it does not seem to be considered.\n4.The backdoor model needs to learn both clean distribution and poison distribution. This may lead to local minima or more sharp minima in the backdoor model optimization process, but does not provide a specific solution. It may be a problem to be concerned about in practical applications."
                },
                "questions": {
                    "value": "1.What are the characteristics and categories of the backdoor attack methods the authors choose to compare? Does the method cover all categories of backdoor attacks?\n2.Some typos and grammar errors are here:\nPage 1, line 35 : wights -> weights\nPage 2, line 36 : as well as -> and"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4205/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698833797419,
            "cdate": 1698833797419,
            "tmdate": 1699636387422,
            "mdate": 1699636387422,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "39tSKjK490",
                "forum": "lOsF9k1sxW",
                "replyto": "YMPrBsgoxU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4205/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4205/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Xtwr (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the insightful comments. We believe the feedback provided here will significantly improve the quality of our paper. Please find our responses to the raised concerns as follows:\n\n---------------------------------------------\n\n**Reviewer\u2019s Question:** In the relevant work, it has been written that the previous defense methods have high calculation costs, which limits their practicability in the actual environment. But won't the calculation of FIM in SFT increase the complexity and cost of calculation?\n\n\n**Our Response:**\nFor FIM calculation, we only need first-order differentiation (i.e., gradient) that is already computed for backpropagation. The only additional cost is the computation of the covariance matrix of the gradient. Besides, we only need layer-wise FIM calculation [1] instead of computing the covariance matrix for all parameters of the model together, reducing the computation cost significantly. To illustrate the computational efficiency of K-FAC over naive computation of FIM, let us consider an L-layered DNN model with parameters $N=\\sum_{i=1}^L n_i$, where $n_i$ is the number of parameters in $i^{th}$ layer. Now, the naive approach of FIM would compute a matrix of size $N^2$ which is significantly larger than the number of elements of the matrices, $\\sum_{i=1}^L n_i^2$, needs to be computed for FIM by K-FAC.\n\nWe also use a faster variation of our method, f-SFT. Combining all these factors, the overall computational cost of our proposed method is significantly less as reported in Table 5 of the main paper.\n\n  \n[1] Grosse, Roger, and James Martens. \"A Kronecker-factored approximate fisher matrix for convolution layers.\" International Conference on Machine Learning. PMLR, 2016.\n\n--------------\n\n\n**Reviewer\u2019s Question:**\nThe introduction of SFT also mentions that regularized Hessian has huge calculation costs in each iteration, so that approximate methods are adopted. How to ensure that the effect can be achieved is the same?   \n\n**Our Response:**\n\nPlease refer to Lemma 1, which states that it is guaranteed that if we reduce the trace of FIM, the trace of Hessian must reduce too. Again, the trace of Hessian is the upper bound of the spectral norm of Hessian, implying that minimizing the trace of FIM minimizes the spectral norm of Hessian. \n\n\n---------------------------------------------"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4205/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700115824668,
                "cdate": 1700115824668,
                "tmdate": 1700115824668,
                "mdate": 1700115824668,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oPCeAIk92K",
                "forum": "lOsF9k1sxW",
                "replyto": "YMPrBsgoxU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4205/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4205/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Xtwr (2/2)"
                    },
                    "comment": {
                        "value": "**Reviewer\u2019s Question:** The backdoor model needs to learn both clean distribution and poison distribution. This may lead to local minima or more sharp minima in the backdoor model optimization process, but does not provide a specific solution. It may be a problem to be concerned about in practical applications.\n\n**Our Response:**\n\nIn our work, we proposed a defense that purifies an already trained backdoor model that has learned both clean and poison distribution. Such defense falls under the category of test-time backdoor defense. We believe the reviewer is indicating the solutions that prevent the backdoor model from learning poison distribution (Please correct us if we are wrong). For such solutions, we need to develop a training-time defense where we have a training pipeline that will prevent the attack from happening. The training pipeline can consist of techniques such as specific augmentations like MixUp [3], where we mix both clean and poison samples to reduce the impact of the poison triggers. In recent times, several training-time defenses have been proposed such as CBD [1] and ABL [2].  Note that training-time defense is completely different from test-time defense and out of the scope of our paper. Nevertheless, we also show a comparison with these training-time defenses in Table 1.  It can be observed that the proposed method obtains superior performance in most of the cases. \n\n**Table 1: Comparison of SFT with two training-time defenses. Our proposed method is able to outperform even these computationally expensive defense methods. We consider the CIFAR10 dataset for all experiments. We consider a poison rate of 10% for all attacks. Each entry in the table is formatted as ASR/ACC.**\n|Method|Badnets  |  Blend     | Troj-one   |   Dynamic      |   WaNet  |  ISSBA|\n|:----------|:----------|:----------|:----------|:----------|----------:|----------:|\n|No Defense  |100/92.96 | 100/94.11 | 100/89.57 | 100/92.52| 98.64/92.29|99.80/92.80| \n|CBD[1]  |2.27/87.92| 2.96/89.61| **1.78**/86.18| 2.03/88.41| 4.21/87.70| 6.76/87.42| \n|ABL[2]  |3.04/87.72| 7.74/89.15| 3.53/86.36| 8.07/88.30| 8.24/86.92| 6.14/87.51| \n|SFT (Ours) |**1.86/89.32**| **0.38/92.17**| 2.64/**87.21** | **1.17/90.97**|  **2.38/89.67**|**4.24/90.18**|\n\n\n[1] Zaixi Zhang, *Backdoor Defense via Deconfounded Representation Learning*, CVPR 2023\n\n[2] Yige Li, *Anti-Backdoor Learning: Training Clean Models on Poisoned Data*, NeurIPS 2021\n\n[3] Hongyi Zhang, \"mixup: Beyond Empirical Risk Minimization\", ICLR 2018\n\n------------------------------------\n\n\n\n**Reviewer\u2019s Question:** What are the characteristics and categories of the backdoor attack methods the authors choose to compare? Does the method cover all categories of backdoor attacks?\n\n**Our Response:**\nIn our work, we try to cover as many variations of backdoor attacks as possible. From a broader perspective, our defense approach could be adapted for the defense of a backdoor attack in tasks learned by supervised training. However, there may be few other variations of backdoor attacks in the literature, for instance, attacks in self-supervising learning [1], reinforcement learning [2], and Large language models [3], which we did not explore. \n\n[1] Saha, Aniruddha, et al. \"Backdoor attacks on self-supervised learning.\" CVPR 22\n\n[2] Wang, Lun, et al. \"Backdoorl: Backdoor attack against competitive reinforcement learning.\" arXiv preprint arXiv:2105.00579 (2021).\n\n[3] Xue, Jiaqi, et al. \"TrojLLM: A Black-box Trojan Prompt Attack on Large Language Models.\" Thirty-seventh Conference on Neural Information Processing Systems. 2023."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4205/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700115854260,
                "cdate": 1700115854260,
                "tmdate": 1700625621848,
                "mdate": 1700625621848,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "sP9bbehKhF",
            "forum": "lOsF9k1sxW",
            "replyto": "lOsF9k1sxW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4205/Reviewer_dBn1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4205/Reviewer_dBn1"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a backdoor purification method based on a observation that backdoored models usually tend to converge to a bad local minima. Starting from this observation, Smooth Fine-Tuning (SFT) is proposed to erase backdoors. Besides, an efficient variant, Fast SFT is introduced to reduce the fine-tuning time.  The proposed methods are extensively evaluated on four different tasks, against 14 backdoor attacks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The discovered observation is interesting, which indicates the optimization for the backdoor model training is harder and unstable than that for benign models.\n2. The evaluation is extensively conducted over four different tasks.\n3. It is well written and easy to follow."
                },
                "weaknesses": {
                    "value": "1. Although the observation is interesting, I wondering whether the observation stands when the model size increases. This is because that increasing model complexity will make it better to achieve a tough learning goal (i.e., optimization for both clean and triggered samples), where the differences on loss surface may be not so obvious between benign model and backdoored model."
                },
                "questions": {
                    "value": "Could the authors add some experiments for the effect of model capacity on the discovered observation?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4205/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698921469022,
            "cdate": 1698921469022,
            "tmdate": 1699636387340,
            "mdate": 1699636387340,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JgiWVJZ0UG",
                "forum": "lOsF9k1sxW",
                "replyto": "sP9bbehKhF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4205/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4205/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer dBn1"
                    },
                    "comment": {
                        "value": "We thank the reviewer for taking the time to review our paper and providing insightful and positive comments as well as valuable feedback. We believe the feedback provided here will significantly improve the quality of our paper. We tried to address the raised concerns below.\n\n\nIn Figure. 6 of Appendix A.6.1, we perform smoothness analysis for different models ranging from small to large. For example, VGG19 has around 143M parameters, whereas ResNet18 has only 11M parameters. The results show that smoothness analysis does generalize to models with different sizes. However, if the reviewer wants to see results for even larger models, we would be happy to provide that too (here and the camera-ready version)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4205/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700115686932,
                "cdate": 1700115686932,
                "tmdate": 1700736514214,
                "mdate": 1700736514214,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "380dcc783V",
            "forum": "lOsF9k1sxW",
            "replyto": "lOsF9k1sxW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4205/Reviewer_DGHS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4205/Reviewer_DGHS"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents Smooth Fine-Tuning (SFT), a novel backdoor purification framework that exploits the knowledge of Fisher Information Matrix (FIM). The basic idea is to add two regularizers to the original loss to prevent the convergence to poor local minima. Some theoretical and empirical results are shown as well in the paper."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "the paper is written clearly, motivation is good and results seem good (not familiar with these datasets)"
                },
                "weaknesses": {
                    "value": "1. Theoretical justification in Eq. 1: Thm. 1 is correct, however, it does not support the observations that backdoor attacks reach bad minima, because adding poison samples do not necessarily increase the Lipschitz constant at all! The logic from the authors is since $(L_c+L_b) \\geq L_c$, the poisonous local minima have to be sharper, I guess. If so, it is definitely wrong. Otherwise, please clarify why.\n\n2. Lack of evidence that the proposed regularized method can prevent the convergence to poor local minima: The regularizers will lead the solutions to flatter regions on the **regularized**, not original, loss landscape. This is understandable, but how to guarantee the solutions fall into smoother regions in the original loss landscape is not discussed, theoretically and empirically. I believe that this is one of the key contributions that the authors try to make. So far I do not see any evidence towards this."
                },
                "questions": {
                    "value": "see my comments"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4205/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698975073189,
            "cdate": 1698975073189,
            "tmdate": 1699636387267,
            "mdate": 1699636387267,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JcqSorBGpC",
                "forum": "lOsF9k1sxW",
                "replyto": "380dcc783V",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4205/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4205/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer DGHS (1/3)"
                    },
                    "comment": {
                        "value": "**Response to Weakness 1**\n\nWe thank the reviewer for taking the time to review our paper and providing insightful and valuable feedback. We believe the feedback provided here will significantly improve the quality of our paper. The reviewer has acknowledged the correctness of Theorem 1. However, it is suggested that more clarifications may be required to understand the implications behind Theorem 1. We aimed to address this concern below in two parts.\n\n\n**1st Part of Response to Weakness 1.** \n\nLet us consider the general training approach of a backdoor and a benign model. In general, a trained backdoor model is usually well-optimized *w.r.t.* both **clean and poison data distributions** as it is designed to perform well on both distributions. If we perform the smoothness analysis of backdoor models *w.r.t.* original training data (both clean and poison data distributions), the loss surface will be smooth. In the case of a benign model, the model is well-optimized for **clean data distribution**, hence, converges to smooth minima. Therefore, it can be observed that if the smoothness is measured *w.r.t.* the **respective training data distribution**, both models will be categorized as smooth. To clearly distinguish between benign and backdoor models in terms of their optimization characteristics, we need to consider a suitable data distribution (that represents both models well)  for smoothness analysis. In our work, we choose to conduct the analysis *w.r.t.* both clean and backdoor samples with their **corresponding ground truth labels**.\n\nConsider a training set {$\\mathbf{x}, y$} = {$\\mathbf{x}_c, y_c$} $\\cup$ {$\\mathbf{x}_b, y_b$}, where {$\\mathbf{x}_c, y_c$} is the set of clean samples and {$\\mathbf{x}_b, y_b$} is the set of backdoor or poison samples. If we train a model ($f$) on this dataset, the outcome would be a backdoor model, $f_b: \\mathbf{x}_c \\rightarrow y_c; \\mathbf{x}_b \\rightarrow y_b$. Now, $f_b$ should be smooth when we use training set {$\\mathbf{x}, y$} = {$\\mathbf{x}_c, y_c$} $\\cup$ {$\\mathbf{x}_b, y_b$} for smoothness analysis. Instead, if we use {$\\mathbf{x}, y$} = {$\\mathbf{x}_c, y_c$} $\\cup$ {$\\mathbf{x}_b, y'_c$}, where $y'_c$ is the original ground truth of $\\mathbf{x}_b$, then $f_b$ shows non-smoothness. Now consider a scenario where we train a model ($f$) only on {$\\mathbf{x}_c, y_c$}. This would give us a clean model ($f_c$). Similar to $f_b$, if we employ {$\\mathbf{x}, y$} = {$\\mathbf{x}_c, y_c$} $\\cup$ {$\\mathbf{x}_b, y'_c$} for smoothness analysis, the benign model shows smoothness. Now, there may be a query: if the model has not been trained on or seen {$\\mathbf{x}_b, y'_c$}, how can the model show smoothness (loss gradient is less sensitive to trigger in $\\mathbf{x}_b$)? Note that one of the most important characteristics of backdoor triggers is that they have to be stealthy (small, low intensity, not easily detectable, etc.). If triggers are not stealthy, they would be easily detectable which is not desirable. Therefore, $f_c$ treats $\\mathbf{x}_b$ as a slightly perturbed version of the original clean sample ($\\mathbf{x}'_c$) it was created from ($\\mathbf{x}_b = \\mathbf{x}'_c + \\delta$). According to the stealth properties of triggers, $f_c$ should not be very sensitive to $\\mathbf{x}_b$. Moreover, $f_c$ may not be sensitive to $\\mathbf{x}_b$ at all depending on its robustness, which could be obtained through (commonly used) augmentation-based training. On the other hand, $f_b$ is highly sensitive to $\\delta$ in $\\mathbf{x}_b$ as it has been optimized to learn the trigger as well as the mapping $\\mathbf{x}_b \\rightarrow y_b$ almost perfectly (ASR close to 100%). Therefore, $f_b$ and $f_c$ should show different smoothness characteristics when we employ {$\\mathbf{x}_b, y'_c$}. To verify how sensitive the benign and backdoor models are to the trigger, we have shown a performance comparison of them in Table 1. Higher sensitivity to the trigger should produce higher loss gradients and lower accuracy. \n\n  \n\n**Table 1: Accuracy comparison (corresponding to {$\\mathbf{x}_b, y'_c$}) of benign and 4 different types of backdoor models. We take 10,000 samples from CIFAR10 training set and add 4 different types of triggers without altering their ground truth labels. We then i) feed these backdoor samples to benign and backdoor models, ii) get the corresponding predictions, and iii) compare these predicated labels to the original ground truths to calculate accuracy. Lower accuracies for backdoor models indicates that we have higher loss gradients for them.**\n|Method| Badnets | TrojanNet | WaNet | ISSBA|\n|:----------|:----------|:----------|:----------|:----------|\n|Benign |90.6 |89.8|87.2|87.9|\n|Backdoor |0.00|0.00|0.41|0.26|"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4205/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700115256753,
                "cdate": 1700115256753,
                "tmdate": 1700626857675,
                "mdate": 1700626857675,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uqW8ayfdbq",
                "forum": "lOsF9k1sxW",
                "replyto": "380dcc783V",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4205/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4205/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer DGHS (2/3)"
                    },
                    "comment": {
                        "value": "**2nd Part of Response to Weakness 1.**\n\nIn Theorem 1, we also consider the {$\\mathbf{x}, y$} = {$\\mathbf{x}_c, y_c$} $\\cup$ {$\\mathbf{x}_b, y'_c$} when we discuss the Lipshitz continuity constants $L_c$ and $L_b$ of loss-gradient for backdoor and benign model. To better understand the implication of the Theorem, consider the above discussion. From the above discussion, it can be inferred that $L_b \\geq L_c$ for the backdoor model as the loss gradient of backdoor model corresponding to {$\\mathbf{x}_b, y'_c$} is equal to or greater than the loss gradient of the benign model. Experimentally, we observe that $L_b$ is strictly greater than $L_c$. Therefore, we can conclude that although theoretically Lipschitz constant of backdoor model  ($L_c+L_b$) is equal to or greater than the benign model $L_c$, experimentally, the total Lipschitzness of the backdoor model is strictly greater than the one for the benign model. \n\nPurifying the backdoor implies that the model will ignore any type of backdoor trigger or manipulation during testing. After purification, we often measure the performance of a model in terms of its ability to predict the original ground truth (for both clean and poison samples). From this perspective too, performing smoothness analysis *w.r.t.* the samples with their original ground truth makes sense.\n\nWe have tried to provide more clarifications behind our proposed theorem here. If there are further concerns, we would be happy to address them. Thanks again for your very timely comment that helped us to elaborate this important point."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4205/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700115523028,
                "cdate": 1700115523028,
                "tmdate": 1700627600112,
                "mdate": 1700627600112,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HNlfyb4wWr",
                "forum": "lOsF9k1sxW",
                "replyto": "380dcc783V",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4205/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4205/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer DGHS (3/3)"
                    },
                    "comment": {
                        "value": "**Response to Weakness 2.**\n\nWe thank the reviewer for raising this concern. We provide additional clarifications on why obtaining flatter regions corresponding to a regularized loss landscape would remove the backdoor. \n\n\nWe would like to emphasize that the smoothness of loss landscape w.r.t. clean data distribution (i.e., training samples with their ground truth labels) is enough to remove the backdoor from the model. In other words, **achieving a flat region of loss landscape corresponding to the clean data distribution removes the backdoor** even though it is a regularized loss landscape.   *We do not necessarily need to reoptimize a backdoor model to the flat loss landscape of the corresponding benign model.* Moreover, it is highly unlikely that our proposed method will achieve the same flatter region of the original loss landscape, as the DNN optimization point is not unique **[1]**. For instance, it is highly likely that one would obtain a different optimization point for the same model optimized with the same data distribution if retrained again. Therefore, it is not important whether we are re-optimizing the backdoor model to flatter regions in the original loss landscape or not, the effect of the backdoor will be removed as long as the regularized loss landscape is flat w.r.t. clean data distribution.\n  \n\nWe have extensively analyzed the smoothness of the purified model for a wide range of attacks and DNN models such as ResNet34, InceptionV3, GoogleNet, DenseNet121, and MobileNetV2 reported in the \u201cAblation Study\u201d section and in Appendix A.6.1. We kindly request the reviewer to check \u201cSmoothness Analysis of SFT\u201d in the Ablation Study (Sec. 6.3) and in Appendix A.6.1. for these results. To generate results, we applied our proposed method consisting of the regularizers during the purification phase. All these results indicate that the purified model optimized to a flatter region of loss landscape corresponding to the clean data distribution. \n\n  \n[1] Chaoyue Liu et. al., \"Loss landscapes and optimization in over-parameterized non-linear systems and neural networks.\" Applied and Computational Harmonic Analysis 59 (2022): 85-116."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4205/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700115935362,
                "cdate": 1700115935362,
                "tmdate": 1700735965920,
                "mdate": 1700735965920,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]