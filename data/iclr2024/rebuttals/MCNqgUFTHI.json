[
    {
        "title": "Plug-and-Play Policy Planner for Large Language Model Powered Dialogue Agents"
    },
    {
        "review": {
            "id": "5CZ0TfRC0P",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7048/Reviewer_sGxq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7048/Reviewer_sGxq"
            ],
            "forum": "MCNqgUFTHI",
            "replyto": "MCNqgUFTHI",
            "content": {
                "summary": {
                    "value": "In this paper, the authors proposed to leverage LLM for goal-oriented dialogues. The motivation is that the current LLM are trained to passively follow instruction, and goal-oriented dialogues requires LLM to actively drive the conversation.\nThe authors proposed a plug-and-play dialogue policy planner. At each turn, this planner proposes a pre-defined action, and that action is translated into a template-based natural language instruction. Finally, LLM conditions on the instruction and dialogue history to generate the next response.\nDuring training, two LLMs are used to generate self-play dialogues and the third LLM is used to score the dialogues. RL is used to then optimize the planner."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The authors proposed a reasonable way to integrate dialogue action prediction into the LLM, which can then optimized by RL. All the components (and even the reward models) are LLM pre-trained so it does not need annotations (except for SFT stage). Experiment results show good performance compared with baseline."
                },
                "weaknesses": {
                    "value": "The proposed plug-and-play dialogue policy planner is a little bit hacky. PPDPP is separated from the dialogue LLM, and the actions it produces are mapped to pre-defined natural language instructions. PPDPP is essentially a prompt selector. It would be more interesting if it can not only select but also generate prompts, and if PPDPP can be integrated into the dialogue LLM (to avoid to use another pre-trained roBERTa model)."
                },
                "questions": {
                    "value": "Why do we want to sample the goal-oriented AI feedback for l times? (Equation 6). Do we observe large variance of the reward LLM?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7048/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697436586079,
            "cdate": 1697436586079,
            "tmdate": 1699636828471,
            "mdate": 1699636828471,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mRUVthfqko",
                "forum": "MCNqgUFTHI",
                "replyto": "5CZ0TfRC0P",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7048/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7048/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer sGxq (1/2)"
                    },
                    "comment": {
                        "value": "We appreciate the insightful and valuable comments. We will carefully address your concerns one by one as follows:\n \n>W \u201c\u2026 PPDPP is essentially a prompt selector. It would be more interesting if it can not only select but also generate prompts, and if PPDPP can be integrated into the dialogue LLM (to avoid to use another pre-trained roBERTa model).\u201d\n \n**Response**: We greatly appreciate the thoughtful suggestions.\n \n1. As for the comment about `\u201cif it can not only select but also generate prompts\u201d`, it is totally feasible and worth studying. To this end, we just need to use a generative language model, such as T5 or Flan-T5, as the PPDPP. In order to demonstrate its adaptability into the generative policy planner, we further implement the PPDPP with a generative language model, i.e., Flan-T5. The implementation includes two different settings:\n- Flan-T5$_\\text{select}$: Use Flan-T5 to generate the strategy label, which performs a similar strategy prediction task as RoBERTa-based PPDPP.\n- Flan-T5$_\\text{generate}$: Use Flan-T5 to directly generate the natural language strategy prompt.\n \n|  | CB | CB | CB | ESConv | ESConv | CIMA | CIMA |\n| ----------- |-----------|-----------|-----------|-----------|-----------|-----------|-----------|\n| Method | AT$\\downarrow$ |SR$\\uparrow$ | SL%$\\uparrow$| AT$\\downarrow$ |SR$\\uparrow$ | AT$\\downarrow$ |SR$\\uparrow$ |\n| PPDPP (RoBERTa) | 5.62 | 0.6117 | 0.3376 | 4.56 | 0.8462 | 3.03 | 0.8407|\n| PPDPP (Flan-T5$_\\text{select}$)| 5.47 | 0.6223 | 0.3297 | 4.62 | 0.8384 | 3.12 | 0.8319 |\n| PPDPP (Flan-T5$_\\text{generate}$)|5.87 | 0.5691 | 0.2983 | 4.82 | 0.8000  | 3.74 | 0.7610|\n \nAs presented in the Table, we observe that Flan-T5$_\\text{select}$ achieves a similar performance with RoBERTa.\n \nOn the other hand, Flan-T5$_\\text{generate}$ generally performs worse than the other two methods in these three datasets.\n \nThese results indicate that, under these proactive dialogue problems, a larger search space for actions may negatively affect the reinforcement learning process of the PPDPP or demand for more efficient or robust RL algorithms, which can be left for future studies. \n \n \n2. As for the comment about `\u201cif PPDPP can be integrated into the dialogue LLM\u201d`, we would like to emphasize the advantages of the pluggable policy planner:\n- For specific dialogue problems, only the policy planner plugin needs to be fine-tuned, which is more practical than fine-tuning the whole LLM-based dialogue system.\n- Since the disentangled fine-tuning will not affect the LLM\u2019s original capabilities of context understanding and response generation, the black-box LLM-based dialogue system can be applied to any dialogue problem by simply substituting the learned policy planner. From another perspective, if PPDPP can be integrated into the dialogue LLM, it will become similar to those prompt-based methods, such as ProCoT or Proactive, where we could only rely on the in-context learning or the tailored prompt designs to trigger the policy planning capability of the LLM dialogue model.\n \n**Revision**: According to your valuable comment, we made the following revisions to address your concerns.\n1. We add a supplementary experiment by replacing the RoBERTa-based PPDPP with a FlanT5-based PPDPP, which is a generative language model for generating prompts. The implementation includes two different settings: (1) Use FlanT5 to generate the strategy label. (2) Use FlanT5 to generate the natural language strategy prompt. The experimental results are presented in **Appendix D**.\n2. We add more discussion about the advantages of using a pluggable policy planner against the prompt-based method in the **Introduction** and **Related Work**."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7048/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700393796621,
                "cdate": 1700393796621,
                "tmdate": 1700393796621,
                "mdate": 1700393796621,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XOQGmUHmm1",
                "forum": "MCNqgUFTHI",
                "replyto": "5CZ0TfRC0P",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7048/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7048/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer sGxq (2/2)"
                    },
                    "comment": {
                        "value": ">Q \u201cWhy do we want to sample the goal-oriented AI feedback for l times? (Equation 6). Do we observe large variance of the reward LLM?\u201d\n \n**Response**: Regarding the sampling of the goal-oriented AI feedback, we would like to clarify our two motivations:\n1. The user state in the conversation is rather difficult to be determined. For example, it is hard to directly classify the emotional state of the user in emotional support dialogue. Therefore, there is indeed variance in the decision of the reward model. Inspired by the idea of Self-Consistency [1], using the sampling strategy can alleviate the variance.\n2. The reward is more reasonable to be a continuous value instead of a discrete class. For example, the emotional state of the user is not supposed to be only four types. Using the average values of the sampled goal-oriented AI feedback can better represent the fine-grained state of the conversation. Besides, a continuous reward can provide more specific feedback for reinforcement learning. \n\n[1] Self-Consistency Improves Chain of Thought Reasoning in Language Models. ICLR 2023.\n \nIn order to validate the advantages of sampling goal-oriented AI feedback multiple times, we conduct an ablation study of the sampling strategy. As mentioned in Section 3, there are two functions of the reward LLM: (1) to determine the state of goal completion during the conversation; and (2) to evaluate the policy outcome with scalar rewards. Therefore, the ablation study will analyze the advantages of the sampling strategy from these two perspectives.\n1. **Analysis of State Prediction**. Similar to the Analysis of LLMs as Reward Model in Appendix A.1, we also compute the F1 score of the prediction of the current user state versus the human-annotated labels. As shown in the Table, the sampling strategy substantially improves the F1 score of the state prediction, indicating that it effectively reduces the variance of the LLM-generated output.\n \n| Method | CraisglistBargain | ESConv | CIMA |\n| ----------- |-----------|-----------|-----------|\n| PPDPP ($l=10$) | 93.7 | 93.4 | 94.6 |\n| PPDPP ($l=1$) | 91.4 | 88.2 | 90.3 |\n \n2. **Analysis of Reward Estimation**. In this analysis, we adopt two reward LLMs to perform the two functions separately. One with the sampling strategy for state prediction to ensure the quality of state prediction, and the other one with or without the sampling strategy for reward estimation. As for the one that estimates reward without the sampling strategy, the reward will only be classified into one of the pre-defined discrete values. However, as for the one that estimates reward with the sampling strategy, the reward will be a continuous value that is averaged from the sampled results. Consequently, the Table shows that the fine-grained continuous reward contributes to better performance as the policy planning outcome will be more distinguishable during the reinforcement learning process.\n \n|  | CB | CB | CB | ESConv | ESConv | CIMA | CIMA |\n| ----------- |-----------|-----------|-----------|-----------|-----------|-----------|-----------|\n| Method | AT$\\downarrow$ |SR$\\uparrow$ | SL%$\\uparrow$| AT$\\downarrow$ |SR$\\uparrow$ | AT$\\downarrow$ |SR$\\uparrow$ |\n| PPDPP ($l=10$) | 5.62 | 0.6117 | 0.3376 | 4.56 | 0.8462 | 3.03 | 0.8407|\n| PPDPP ($l=1$) | 5.87 | 0.5957 | 0.2623 | 4.67 | 0.8307 | 3.29 | 0.7965|\n \n**Revision**: In order to better clarify your concern regarding the sampling of the goal-oriented AI feedback,  we further presented these two supplementary analysis in **Appendix E** and added more descriptions about the motivations in **Section 3**."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7048/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700393838058,
                "cdate": 1700393838058,
                "tmdate": 1700393838058,
                "mdate": 1700393838058,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9Tb5iCq1KM",
            "forum": "MCNqgUFTHI",
            "replyto": "MCNqgUFTHI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7048/Reviewer_RDBv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7048/Reviewer_RDBv"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents the Plug-and-Play Dialogue Policy Planner (PPDPP), an approach designed to address the challenges of proactive dialogues within the context of large language models (LLMs). PPDPP serves as a dialogue policy planner, employing supervised fine-tuning and reinforcement learning to enable a LLM powered dialogue system to adapt to a variety of dialogue scenarios. Authors introduce a tunable language model plug-in, allowing LLM-powered dialogue system to adapt to various cases and applications by simply substituting the learned plug-in. PPDPP outperforms existing LLM-based dialogue systems in negotiation, emotional support, and tutoring dialogues, showcasing its effectiveness in improving proactive dialogues."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Utilizes a pluggable and fine-tuned dialog policy ranker for dynamic prompt selection, enhancing adaptability to various dialogue domains.\n- Incorporates the LLM as a reward function, enabling RL-based dialogue policy planning.\n- Employs a combination of supervised fine-tuning and online reinforcement learning (RL) for dialog policy ranker training."
                },
                "weaknesses": {
                    "value": "- Limited action/prompt space for the dialog LLM, potentially constraining adaptability to different domains.\n- The primary distinction from other Reinforcement Learning from AI Feedback (RLAIF) works seems to be the mapping of the LLM's reward output from text space to scalar reward space, raising questions about the approach's uniqueness.\n- The need for training different dialog policies for each dialog domain. This makes this system less generalizable."
                },
                "questions": {
                    "value": "1. How is the reward LLM utilized during inference at each turn of dialogue?\n2. Could you clarify the process of mapping the reward LLM's output to scalar values and its integration into the PPDPP during each dialogue turn?\n3. Can you elaborate more on supervised fine-tuning used to PPDPP?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7048/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7048/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7048/Reviewer_RDBv"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7048/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698817810935,
            "cdate": 1698817810935,
            "tmdate": 1699636828340,
            "mdate": 1699636828340,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "157OrgUGaM",
                "forum": "MCNqgUFTHI",
                "replyto": "9Tb5iCq1KM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7048/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7048/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer RDBv (1/2)"
                    },
                    "comment": {
                        "value": "We appreciate the detailed and valuable comments. We will carefully address your concerns one by one as follows:\n\n------\n \n>W1 \u201cLimited action/prompt space for the dialog LLM, potentially constraining adaptability to different domains.\u201d\n \n**Response**: Thanks so much for the valuable comment. We would like to first clarify that the action/prompt space is **NOT** restricted, while it is determined by the concerned dialogue problems. For example, in the three dialogue problems we assessed, each one has a unique set of possible actions that have been tailored based on expert input or specialized knowledge in that area. In essence, our method is versatile and can be customized to suit a variety of proactive dialogue scenarios, each with its distinct set of possible actions. Additionally, our approach is equally applicable to a generative policy planner, which inherently allows for a vast and varied set of prompts.\n \nIn order to demonstrate its adaptability into the generative policy planner, we further implement the PPDPP with a generative language model, i.e., Flan-T5. The implementation includes two different settings:\n- Flan-T5$_\\text{select}$: Use Flan-T5 to generate the strategy label, which performs a similar strategy prediction task as RoBERTa-based PPDPP.\n- Flan-T5$_\\text{generate}$: Use Flan-T5 to directly generate the natural language strategy prompt.\n \n|  | CB | CB | CB | ESConv | ESConv | CIMA | CIMA |\n| ----------- |-----------|-----------|-----------|-----------|-----------|-----------|-----------|\n| Method | AT$\\downarrow$ |SR$\\uparrow$ | SL%$\\uparrow$| AT$\\downarrow$ |SR$\\uparrow$ | AT$\\downarrow$ |SR$\\uparrow$ |\n| PPDPP (RoBERTa) | 5.62 | 0.6117 | 0.3376 | 4.56 | 0.8462 | 3.03 | 0.8407|\n| PPDPP (Flan-T5$_\\text{select}$)| 5.47 | 0.6223 | 0.3297 | 4.62 | 0.8384 | 3.12 | 0.8319 |\n| PPDPP (Flan-T5$_\\text{generate}$)|5.87 | 0.5691 | 0.2983 | 4.82 | 0.8000  | 3.74 | 0.7610|\n \nAs presented in the Table, we observe that Flan-T5$_\\text{select}$ achieves a similar performance with RoBERTa.\n\nOn the other hand, Flan-T5$_\\text{generate}$ generally performs worse than the other two methods in these three datasets.\n\nThese results indicate that, under these proactive dialogue problems, a larger search space for actions may negatively affect the reinforcement learning process of the PPDPP or demand for more efficient or robust RL algorithms, which can be left for future studies.\n\n**Revision**: In the revision, we present this supplementary experiment in **Appendix D**, which compares the performance of classification-based policy planner and generation-based policy planner.\n\n------\n \n>W2 \u201cThe primary distinction from other Reinforcement Learning from AI Feedback (RLAIF) works seems to be the mapping of the LLM's reward output from text space to scalar reward space, raising questions about the approach's uniqueness.\u201d \n \n**Response**: Thanks so much for the valuable comment. We would like to further elaborate the approach\u2019s uniqueness, which is in two-fold:\n1. The novel framework with a tunable language model plugins for LLM-based dialogue systems.\n2. The design of goal-oriented AI feedback.\n \nAs for your concern regarding the design of goal-oriented AI feedback, we would like to highlight two distinct features from common RLAIF works:\n- As you mentioned, mapping of the LLM's reward output from text space to scalar reward space for RL algorithm.\n- We consider the long-term goal-oriented reward, instead of AI preference on single-turn responses. The long-term goal-oriented reward requires multi-turn interactions with a dynamic environment, i.e., the user.\n\n**Revision**: In the revision, we elaborate the approach\u2019s uniqueness more clearly in **Section 2** by comparing with the related works.\n\n-------\n \n>W3 \u201cThe need for training different dialog policies for each dialog domain. This makes this system less generalizable.\u201d\n \n**Response**: We apologize for this confusion. In fact, the generality of the proposed method lies on the pluggable dialogue policy planner. With this design, we do not need to train the whole LLM-based dialogue system for each dialog domain, while we just need to train a small language model plugin. This not only makes the LLM-based dialogue system more generalizable to the new case in each dialogue domain than prompt-based approaches, but also enables the LLM-based dialogue system can be adapted to different dialogue domain by just substituting the dialogue policy planner plugin without affecting the LLM\u2019s exceptional capabilities of response generation and context understanding.\n\n**Revision**: In order to better elaborate the generality of the proposed method, we add more descriptions in the **Introduction**."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7048/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700393585718,
                "cdate": 1700393585718,
                "tmdate": 1700393585718,
                "mdate": 1700393585718,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "l0tp78WzTS",
                "forum": "MCNqgUFTHI",
                "replyto": "9Tb5iCq1KM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7048/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7048/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer RDBv (2/2)"
                    },
                    "comment": {
                        "value": ">Q1 \u201cHow is the reward LLM utilized during inference at each turn of dialogue?\u201d\n\n**Response**: As mentioned in Section 3 \u201cLLM as Reward Model\u201d, the reward LLM has two functions : (1) to determine the goal completion during the conversation; (2) to evaluate the policy outcome with scalar rewards.\n\nAs shown in Figure 1(b), during the inference phase, the reward LLM will not output the reward value, which is only used for the RL training phase. However, the reward LLM will still be used to determine whether the goal has been completed during the conversation, which is used for evaluation.\n\n**Revision**: In order to better present the inference process, we add more details in the caption of **Figure 1** regarding the usage of the reward LLM.\n\n-------\n \n>Q2 \u201cCould you clarify the process of mapping the reward LLM's output to scalar values and its integration into the PPDPP during each dialogue turn?\u201d\n\n**Response**: Sorry about the confusion regarding the reward mapping process. Take emotional support dialogues as an example. To assess whether the patient' emotional issue has been solved, we prompt the reward model to answer a multi-choice question \"Has the patient's issue been solved?\", and then generate the goal-oriented AI feedback at temperature $\\tau>0$ to sample the  responses for $l$ times. We define a mapping  $\\mathcal{M}_r(\\cdot)$ to transform verbal feedback to scalar rewards, such as \"the patient feels worse\", \"the patient feels the same\", \"the patient feels better\", \"the patient's issue has been solved\" as -1.0, -0.5, 0.5, and 1.0, respectively.\n\nWhen we obtain the $l=5$ sampled responses as [\"the patient feels better\", \"the patient's issue has been solved\", \"the patient feels better\", \"the patient's issue has been solved\", \"the patient feels better\"], we can compute a continuous scalar value $v_t=\\frac{1}{5}(0.5+1.0+0.5+1.0+0.5)=0.7$. If $v_t>\\epsilon$, we regard the state as GOAL-COMPLETED and set the reward $r_t=v_t$. If not, we assign a small negative reward, \\textit{e.g.}, $r_t=-0.1$, to penalize the lengthy conversation for promoting efficient goal completion.\n\n**Revision**: In order to make the description of the reward mapping process more clear, we reorganize the part of reward mapping to the same part in **Appendix F.4**, which presents the whole process with specific examples.\n\n-------\n\n>Q3 \u201cCan you elaborate more on supervised fine-tuning used to PPDPP?\u201d\n\n**Response**: Sorry about the confusion regarding the supervised fine-tuning (SFT) process. The SFT process is described in the second part of Section 3. The SFT exactly follows the traditional corpus-based fine-tuning process for dialogue action prediction. In specific, we have the human-annotated dialogue corpus with dialogue action labels for each turn. Given the dialogue history $\\{u_1^\\text{sys}, u_1^\\text{usr}, ..., u_{t-1}^\\text{sys}, u_{t-1}^\\text{usr}\\}$, the PPDPP is fine-tuned to predict the dialogue action $a_t$ for the next turn.\n\n**Revision**: In order to make the description of the SFT process more clear, we add more details in the part of \"Plug-and-Play Dialogue Policy Planner\" in **Section 3**."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7048/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700393718815,
                "cdate": 1700393718815,
                "tmdate": 1700393718815,
                "mdate": 1700393718815,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "6fM48ax5wV",
            "forum": "MCNqgUFTHI",
            "replyto": "MCNqgUFTHI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7048/Reviewer_dAwq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7048/Reviewer_dAwq"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on the transfer ability of proactive dialogues in the context of large language models (LLMs), the existing policy learning is hard to transfer to new cases. This work introduces a new paradigm for strategizing LLM-powered dialogue agents with a plug-and-play dialogue policy planner, called PPDPP. In addition, it also proposes an interactive setting for the policy evaluation. Empirical experiments on three datasets show promising results in both automatic evaluation and human evaluation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper introduces a plug-and-play dialogue policy planner with LLMs for proactive learning.\n2. Empirical results on three datasets show very promising results in both automatic evaluation and human evaluation, and good transfer ability."
                },
                "weaknesses": {
                    "value": "So far No. (A good work with sufficient experiments)"
                },
                "questions": {
                    "value": "1. I try to understand why there is negative relative success rate in Figure 2?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7048/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698991239939,
            "cdate": 1698991239939,
            "tmdate": 1699636828241,
            "mdate": 1699636828241,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZkBccdoBvG",
                "forum": "MCNqgUFTHI",
                "replyto": "6fM48ax5wV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7048/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7048/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer dAwq"
                    },
                    "comment": {
                        "value": "We sincerely appreciate your acknowledgement of our work. We believe that this work can provide a practical and effective solution for the proactive dialogue system in the era of LLMs. And the evaluation of dialogue systems can also be benefited from the proposed interactive evaluation framework.\n \n>Q1 \u201cI try to understand why there is negative relative success rate in Figure 2?\u201d\n \n**Response**: We apologize for the confusion. The relative success rate is calculated by subtracting the actual success rate of the Standard prompting method from that of the concerned method. Therefore, the negative relative success rate means that this method achieves a lower success rate than the Standard prompting method. This result shows that in some cases, these methods perform even worse than the vanilla ChatGPT with Standard prompts.\n \n**Revision**: In the revision, we added the above descriptions on the caption of **Figure 2** to better present the results."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7048/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700393252453,
                "cdate": 1700393252453,
                "tmdate": 1700393252453,
                "mdate": 1700393252453,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]