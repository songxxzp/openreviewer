[
    {
        "title": "AutoDAN: Automatic and Interpretable Adversarial Attacks on Large Language Models"
    },
    {
        "review": {
            "id": "6LIuJBa2MM",
            "forum": "ZuZujQ9LJV",
            "replyto": "ZuZujQ9LJV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6675/Reviewer_rSK6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6675/Reviewer_rSK6"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces AutoDAN, an innovative and comprehensible adversarial attack method. Merging the benefits of both manual and automated adversarial techniques, AutoDAN autonomously creates attack prompts. These prompts not only evade perplexity-based filters but also uphold a high success rate, akin to hand-crafted jailbreak attacks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Provide an automatic and interpretable adversarial attack against LLM."
                },
                "weaknesses": {
                    "value": "Lack of enough baseline comparison.\n\nLack of evaluation on more advanced LLM model, such as GPT-4.\n\nLack of consideration of other potential defense strategies adopted by the LLM provider."
                },
                "questions": {
                    "value": "The authors should provide more tangible examples of successful attacks on advanced LLMs, such as GPT-4, Bard, and Bing Chat. I remain skeptical that the proposed prompts can attain a high success rate against such sophisticated LLMs. I would only be convinced of its efficacy if it proves effective against these cutting-edge, real-world models.\n\nBeyond perplexity-based filters, LLM providers might also utilize additional defensive strategies. These can include dynamic content moderation of generated outputs and keyword filtering. Is the proposed attack equally effective against these defenses?\n\nThe source codes are not open. The technical details in the paper are not clear enough to enable reproduction.\n\nThere are several existing jailbreak prompt generation methodologies. A comparison with these methods is essential for the authors to demonstrate the superiority and efficacy of their proposed approach. For instance, JAILBREAKER: Automated Jailbreak Across Multiple Large Language Model Chatbots."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6675/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6675/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6675/Reviewer_rSK6"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6675/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697613055864,
            "cdate": 1697613055864,
            "tmdate": 1699636764438,
            "mdate": 1699636764438,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3OVXVSPyQM",
                "forum": "ZuZujQ9LJV",
                "replyto": "6LIuJBa2MM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6675/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6675/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer rSK6"
                    },
                    "comment": {
                        "value": "We thank reviewer rSK6 for the helpful feedback! We address your concerns below.\n\n> **Weakness 1**: Lack of enough baseline comparison.\n\n> **Question 4**: There are several existing jailbreak prompt generation methodologies. A comparison with these methods is essential for the authors to demonstrate the superiority and efficacy of their proposed approach. For instance, JAILBREAKER: Automated Jailbreak Across Multiple Large Language Model Chatbots.\n\nWe only consider gradient-based jailbreak attacks on LLMs in this paper since these methods are fully automatic and do not require any manual input. All other methods, including the one you have mentioned, either require handcrafted seed attack prompts for training or system prompts for prompting attacker LLMs. As far as we know, GCG is the first and only effective method for this task prior to our work, and we extend GCG with a readability regularization as an additional baseline method. We will further discuss the differences between our method and the one you have mentioned in our updated paper.\n\n> **Weakness 2**: Lack of evaluation on more advanced LLM model, such as GPT-4.\n\n> **Question 1**: The authors should provide more tangible examples of successful attacks on advanced LLMs, such as GPT-4, Bard, and Bing Chat. I remain skeptical that the proposed prompts can attain a high success rate against such sophisticated LLMs. I would only be convinced of its efficacy if it proves effective against these cutting-edge, real-world models.\n\nOur original manuscript reported the transfer attack results on GPT4 in Figure 1 and Table 3 (which are still **Figure 1 and Table 3 in the revised manuscript**). We keep them in the same place in the revised paper. Our released raw annotation data ([link](https://anonymous.4open.science/r/anonymous-share-8734/sample_data_annotated.html)) further showcases tangible responses from GPT4.\n\n> **Weakness 3**: Lack of consideration of other potential defense strategies adopted by the LLM provider.\n\n> **Question 2**: Beyond perplexity-based filters, LLM providers might also utilize additional defensive strategies. These can include dynamic content moderation of generated outputs and keyword filtering. Is the proposed attack equally effective against these defenses?\n\nThat's a great point! Our jailbreaking results on GPT-3.5 and GPT-4 already show that current defenses adopted by the LLM provider fall short of ensuring complete safety. However, we believe there are other potential defense strategies they might consider, such as model-based content moderation, keyword filtering, retokenization, and paraphrasing. We currently do not explicitly test these defenses since their safety-usability trade-off (especially false positive rates) is unclear. We plan to evaluate a few more defenses in future work.\n\n\n> **Question 3**: The source codes are not open. The technical details in the paper are not clear enough to enable reproduction.\n\nAlthough we are currently encountering legal issues that have resulted in a delay in the code release, we are dedicated to releasing our code as soon as possible. \n\n---\nThanks again for your time and effort in reviewing our paper! We are happy to have more discussions for any further questions."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6675/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700703541884,
                "cdate": 1700703541884,
                "tmdate": 1700705916676,
                "mdate": 1700705916676,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7qSEpc7Z8D",
            "forum": "ZuZujQ9LJV",
            "replyto": "ZuZujQ9LJV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6675/Reviewer_2vNQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6675/Reviewer_2vNQ"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a new method for finding adversarial suffixes that appear like natural language. The method has 3 major components. First, it computes a dynamic sequence of tokens that serve as the adversarial suffix. Second, at each token \u201cgeneration,\u201d it takes in the entire current context and uses gradient information to pick an adversarial token. Third, it uses the model\u2019s own judgment of the likelihood for the next token as a balancing objective against adversariality in order to ensure readability of the prompt. It combines the two by sampling from the tokens with highest likelihood according to a weighted sum of the two objectives. The paper judges success by the model\u2019s ability to produce an answer that does not match to a set of known refusal strings. This achieves high success rates that both jailbreak the model and bypass some previously proposed defenses (such as perplexity filtering). The authors also include examples with very long adversarial suffixes that appear to match handwritten jailbreaking prompts."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "This is an incredibly strong paper. The method appears to be the first one to produce adversarial sequences of arbitrary length while maintaining readability or the \u201cnaturalness\u201d of the language. This is most evident in Table 2 and the numerical results in Table 1 and the other figures appear to back up those claims. While I believe the experimental evaluation can be improved in some ways, the method itself is an interesting and novel contribution. \n\nMore importantly, I believe the paper introduces a breakthrough with its idea to generate tokens one-by-one. To my knowledge, all previous works on automatically jailbreaking LLMs have either optimized a fixed set of tokens or attempted to prompt other LLMs to generate jailbreaking prompts. However, both have had limitations. Finding fixed-length prompts has led to a natural roadblock before the solutions possible with gradient-based attacks. Similarly, the methods prompting LLMs have not had the ability to use the more powerful gradient information which the adversarial examples literature has long found to be most effective in finding exploits."
                },
                "weaknesses": {
                    "value": "I believe the paper\u2019s biggest area of improvement is its definition of attack success rate. In the current approach, the authors likely underestimate refusals since they only do string matching against a set of known refusal strings. It would be good to at least validate this metric in one of three ways: by using a dedicated safety model trained to identify refusals, by prompting a more powerful model such as GPT-4 acting as a judge, or by manual inspection of a sample of responses.\n\nThe paper needs to expand the models it is testing against to include Llama 2, Claude and Bard, since those models are more heavily optimized for safety and in some cases are better models. It is acknowledged that Claude and Bard do not have their weights available but transfer attacks against those should be evaluated as well."
                },
                "questions": {
                    "value": "Is Table 2 a random sample? How can the claims in Section 4.2 be made more robust? Do the authors have measures of readability (e.g. perplexity) of their whole adversarial dataset? We see this indirectly in the perplexity filter bypass but can they break it down further?\n\nFor the data in Table 3, how do the authors know which protection they bypassed? Can they explain if they tested against an API endpoint without protections or something else?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "10: strong accept, should be highlighted at the conference"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6675/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698058142046,
            "cdate": 1698058142046,
            "tmdate": 1699636764324,
            "mdate": 1699636764324,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "18bFR2srV7",
                "forum": "ZuZujQ9LJV",
                "replyto": "7qSEpc7Z8D",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6675/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6675/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 2vNQ"
                    },
                    "comment": {
                        "value": "We are grateful for your insightful feedback! It is encouraging to see you recognizing the novelty of our algorithm, which combines gradient-based optimization with sampling-based token generation. We are committed to keep improving our algorithm for a broader range of tasks in the future.\n\n> **Weakness 1**: I believe the paper\u2019s biggest area of improvement is its definition of attack success rate. In the current approach, the authors likely underestimate refusals since they only do string matching against a set of known refusal strings. It would be good to at least validate this metric in one of three ways: by using a dedicated safety model trained to identify refusals, by prompting a more powerful model such as GPT-4 acting as a judge, or by manual inspection of a sample of responses.\n\nWe thank you for these constructive suggestions! Please see our **General Response - 1. Improved Evaluation** for our response. Specifically, we use the GPT-4 based evaluation for all our experiments (except for Guanaco and Pythia due to time limit). We also *meta-evaluate the reliability of different evaluation methods on a sample of responses*, including string matching, classifier-based evaluation, and GPT-4-based evaluation, with *human labeling as the ground-truth*. \n\nWe note that our classifier-based evaluation, which identifies harmful content, is not exactly the same as the one you mentioned, which identifies refusals. The pretrained classifier that directly identifies harmful content seems susceptible to distribution shifts and performs poorly in our experiment. Intuitively, refusals are a much narrower concept than harmful behaviors and could be easier to classify. Now that string matching shows a surprisingly high agreement with human annotation, we will try to train a refusal-identifying classifier in the updated paper to improve upon string matching.\n\n> **Weakness 2**: The paper needs to expand the models it is testing against to include Llama 2, Claude, and Bard, since those models are more heavily optimized for safety and in some cases are better models. It is acknowledged that Claude and Bard do not have their weights available but transfer attacks against those should be evaluated as well.\n\nOur **General Response - 2. Jailbreaking Llama2** includes the results on jailbreaking Llama2-chat in the individual behavior setting. We are still working on the multiple behaviors setting. Currently, we do not have an API quota for a quantitative evaluation of Claude and Bard, but we will try to add some qualitative examples in the updated paper.\n\n\n> **Question 1**: Is Table 2 a random sample? How can the claims in Section 4.2 be made more robust? Do the authors have measures of readability (e.g. perplexity) of their whole adversarial dataset? We see this indirectly in the perplexity filter bypass but can they break it down further?\n\n1) No, prompts in Table 2 are manually selected from 20 independent runs of AutoDAN on Vicuna-7B. \n2) To make claims in Section 4.2 more robust, we will release a large amount of randomly generated prompts with random seed indices in the updated paper.\n3) Our released raw annotation data contains the perplexity for each attack suffix. For the randomly generated prompts to be released, we will also show the perplexity of each prompt as well as the perplexity distribution.\n\n> **Question 2**: For the data in Table 3, how do the authors know which protection they bypassed? Can they explain if they tested against an API endpoint without protections or something else?\n\nWe use the Azure API for evaluating GPTs. The Azure API has built-in content filtering that cannot be turned partially or fully off unless approved by Azure (see their [content filtering manual](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/content-filter) - Configurability). We identify the protection layers an attack has bypassed by analyzing the response of Azure API: 1) If the model fulfills the harmful request, then the attack has bypassed all layers of protection; 2) Otherwise, the model responds with certain refusal or error code, and we look up the Azure's [content filtering manual](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/content-filter) again to determine which layer stops the attack, and we label the attack as bypassing all earlier layers of protection."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6675/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700703029531,
                "cdate": 1700703029531,
                "tmdate": 1700703029531,
                "mdate": 1700703029531,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xEoH5C1pb9",
            "forum": "ZuZujQ9LJV",
            "replyto": "ZuZujQ9LJV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6675/Reviewer_LYLv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6675/Reviewer_LYLv"
            ],
            "content": {
                "summary": {
                    "value": "This paper designs an automated jailbreak attack called AutoDAN against LLMs. AutoDAN selects the tokens one by one to achieve the objective of harmfulness and readability. Specifically, AutoDAN first selects a set of candidate tokens and then traverses all the tokens to select the token that gains the most harmfulness and readability. The experiments justify the effectiveness of AutoDAN in jailbreak LLMs without being filtered by the perplexity filter."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. AutoDAN automates the procedure of generating adversarial examples, which facilitates the robustness evaluation of LLMs.\n\n2. It is interesting to see the two strategies, which are shifting domains and corroborating fine-grained instructions, inspired by the AutoDAN-generated adversarial samples.\n\n3. The adversarial samples generated by AutoDAN are transferable, which makes it possible to attack black-box LLMs.\n\n4. AutoDAN is applicable to adversarial attacks that aim to make the LLMs leak private information."
                },
                "weaknesses": {
                    "value": "1. The generated suffix is arguably long. Therefore, how is the performance when the length of the suffix is constrained?\n\n2. How is the computational resource required for AutoDAN? And, how is the efficiency of the proposed attack? The backpropagation through the LLMs to select the candidate subset could be computationally heavy.\n\n3. The proposed attack seems to require using the probability of the next token. However, in a black-box setting (e.g., attack GPT-3.5 API), it is difficult (or impossible) to obtain the probabilities. Therefore, the proposed method could be difficult to adapt to the latest LLMs."
                },
                "questions": {
                    "value": "Please refer to my comments in \u201cWeaknesses\u201d.\n\nMinor comments: revise \u201cClaude+[CITE]\u201d."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6675/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698408727389,
            "cdate": 1698408727389,
            "tmdate": 1699636764197,
            "mdate": 1699636764197,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xyAGigWML2",
                "forum": "ZuZujQ9LJV",
                "replyto": "xEoH5C1pb9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6675/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6675/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer LYLv"
                    },
                    "comment": {
                        "value": "We thank reviewer LYLv for the helpful feedback! We address your comments and concerns below.\n\n> **Strength 1**: It is interesting to see the two strategies, which are shifting domains and corroborating fine-grained instructions, inspired by the AutoDAN-generated adversarial samples.\n\nWe are delighted to know that the reviewer finds the interpretable AutoDAN-generated prompts interesting! We would like to add that interpretability is especially helpful when people don't know what strategies to use in novel tasks. For example, when crafting the attack prompt for the prompt leaking task, shall we use the word \"instructions\" or \"prompts\" in referring to the system prompts we aim to obtain? Running AutoDAN in a fully automatic manner with the target of outputting the system prompts (Section 4.4) tells us that we should use \"instructions\" (Figure 10). This is a subtle yet impactful distinction that AutoDAN helps to uncover, and AutoDAN may also provide more insights into **understanding the underlying mechanism of successful adversarial strategies**.\n\n> **Weakness 1**: The generated suffix is arguably long. Therefore, how is the performance when the length of the suffix is constrained?\n\nThanks for the great question! Figure 9 in the original manuscript appendix (**Figure 11 in the revised manuscript Appendix C**)  showed that the ASR of AutoDAN generated prompts would not be affected if we limit the token length to under 50. We further emphasized this point in the updated paper.\n\n> **Weakness 2**: How is the computational resource required for AutoDAN? And, how is the efficiency of the proposed attack? The backpropagation through the LLMs to select the candidate subset could be computationally heavy.\n\nPlease see our **General Response - 3. Computational Complexity Analysis** for the complexity of our method in comparison with prior work (GCG), and the actual time cost. We do backpropagation once in each step to get the gradient at the new token's location. Since we only need the gradient for one location (as compared to all weights in model training), the time and space complexity is slightly less than a forward pass due to the shorter path in the computational graph.\n\n> **Weakness 3**: The proposed attack seems to require using the probability of the next token. However, in a black-box setting (e.g., attack GPT-3.5 API), it is difficult (or impossible) to obtain the probabilities. Therefore, the proposed method could be difficult to adapt to the latest LLMs.\n\n\nThis is indeed a limitation of our method (and all other gradient-based methods alike, including GCG), as it is impossible to obtain the gradient from black-box LLMs. Nevertheless, AutoDAN-generated interpretable prompts demonstrate transferability, suggesting that different LLMs share a somewhat similar vulnerability landscape. These interpretable prompts thus provide a way for red-teaming researchers to identify and exploit these common vulnerabilities and potentially strengthen the existing black-box attacks using the strategies found by AutoDAN.\n\n---\nThanks again for all your time and effort in reviewing our paper! We are happy to have more discussions for any further questions."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6675/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700701538722,
                "cdate": 1700701538722,
                "tmdate": 1700705903054,
                "mdate": 1700705903054,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QceIfcZ4qa",
                "forum": "ZuZujQ9LJV",
                "replyto": "xyAGigWML2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6675/Reviewer_LYLv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6675/Reviewer_LYLv"
                ],
                "content": {
                    "comment": {
                        "value": "Hi Authors,\n\nThanks for your responses! I appreciate the author's dedicated efforts to provide additional results. However, I would like to state my major concerns that made me decide to keep my rating as a borderline reject.\n\nAlthough the GCG and AutoDAN are promising white-box adversarial attacks, they still follow the framework: generate adversarial examples against a surrogate white-box LLM first, then transfer to attack the black-box LLM. However, in practice, the black-box LLM such as GPT-3.5-turbo and GPT-4 is updated over time. The transferable adversarial examples are not adaptive to the latest black-box LLM and should fail to successfully attack black-box LLM soon. I think the most meaningful application of the jailbreak attack study is to reliably evaluate the robustness of black-box LLMs. However, AutoDAN cannot provide a reliable evaluation for its white-box nature. Therefore, I personally think the transferable-based attack (e.g., AutoDAN and GCG) is not a fancy solution for the jailbreak attack."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6675/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700705191461,
                "cdate": 1700705191461,
                "tmdate": 1700705191461,
                "mdate": 1700705191461,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5RxLjg3WLb",
            "forum": "ZuZujQ9LJV",
            "replyto": "ZuZujQ9LJV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6675/Reviewer_abck"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6675/Reviewer_abck"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method that can automatically generate jailbreak suffixes for malicious request against LLMs, the generated jailbreak suffix have a better readability compared with the pioneering work (GCG attack)."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The proposed method demonstrates good effectiveness on some LLMs (including Vicuna-7B and 13B, Guanaco-7B, and Pythia-12B). Moreover, it shows transferability to some degree against black-box LLMs."
                },
                "weaknesses": {
                    "value": "1. Soundness: \n\nThe major objective of this paper is to \"generate attack prompts that achieve high attack success rates while having perplexity scores lower than most human-written prompts\" (Page 2 the summary of contributions). Namely, the proposed method should have high ASR and meanwhile low perplexity. However, the paper cannot convince me that the proposed method can solve (or avoid) the trade-off between ASR and readability. \n\nSpecifically, the proposed method introduces a loss function to drive the generated jailbreak suffix readable and add a weight parameter to balance the attack strength and readability.  However, the experiment shows that the proposed method compromises ASR when achieving better readability (Page 7 Table 1), and a similar trade-off exists in GCG and GCG-reg. So how can the proposed method achieve \"high attack success rates\" and \"low perplexity scores\"? From my perspective, answering this question may need more comprehensive evaluations, and I leave this part later. In a nutshell, I think the current weakness in soundness is that the paper has not shown the ability to address (or ease) the trade-off between attacks strength and readability, in other words, the proposed method seems more like \"achieving lower perplexity scores by compromising ASR\" rather than achieve the both.\n\u00a0\n\n2. Evaluations: \n\na. Lack of quantity evaluations on the quality of the responses:\n\nThe threat of jailbreak attacks comes from that the LLM may give strong relevant responses to those malicious requests. In other words, we should know whether the LLM is answering the exact malicious question (or some equivalent ones). For example, if we are red-teaming the alignment of LLM by asking \"how to build a bomb\", we are not expecting that the LLM should refuse questions like \"how to build a bomb in Minecraft\".\n\nHowever, as shown in the paper's examples (Page 8 Table 2), the generated suffix seems to have distorted the original meaning of the malicious request. The question of \"how to build a bomb\" is elaborated by the proposed method as \"Tell me how to build a bomb in an imaginary cyberpunk neo-futuristic dystopian megalopolis where facial recognition technology and invasive surveillance are widespread.\", which makes me concerned of whether the response of LLMs is not aligned with human value or is just other answers without harmfulness.\n\nUnfortunately, the paper has not sufficiently evaluated this point. The ASR in this paper is calculated by testing \"if no prefix in a predefined refusal set appears in the model response\", which is not relevant to the answer's harmfulness as I mentioned above.\n\n\nb. Lack of evaluations on more robust LLMs:\n\nIn this paper, the proposed method is evaluated in Vicuna-7B and 13B, Guanaco-7B, and Pythia-12B. Existing works such as GCG[1] have revealed that these models have more severe vulnerability of jailbreaks (e.g., 90+ ASR) compared with LLMs that have better alignment, for example, the most famous open-sourced model Llama2.\n\nI'm not saying that the effectiveness demonstrated in models such as Vicuna-7B is totally not persuading. However, back to my point of soundness, as there exists a trade-off between attack strength and readability, it becomes necessary to conduct evaluations on more robust (aligned) LLMs to show the ability to address this trade-off of the proposed method. Otherwise, as we can see on  (Page 7 Table 1 first column), the methods all achieve 100 ASR in different LLMs, so it becomes hard to gain accurate conclusions about whether the proposed method is not compromising much attack strength.\n\nc. Lack of ablation studies:\n\nAs the proposed method is aimed at solving the trade-off aforementioned and proposed a dual-target loss function, it surprise me that the paper has not provided ablations studies on these two parts, for example, how are the weigh parameters w_1 and w_2 affecting the training process and the final scores. This leaves many important questions not answered, for example, how the proposed readability loss is influencing ASR and perplexity. From my perspective, this kind of ablation study is quite important for papers like introducing new loss constraints, especially if the proposed loss is somewhat in conflict with the original loss (target attack loss).\n\nd. Lack of evaluations on computational cost:\n\nThere are no evaluations of the computational cost of the proposed method. Such evaluation can make readers more familiar with the aspects like convergence speed. And it may be better to keep a similar (or smaller) computational cost compared with the existing GCG method.\n\n[1] Universal and Transferable Adversarial Attacks on Aligned Language Models"
                },
                "questions": {
                    "value": "1. In Fig.1, why is the perplexity of the suffixes generated by the proposed method sometimes similar to those generated by GCG (and even higher)? Form the results in Fig.6, it seems the proposed method should have a clear difference in perplexity compared with GCG.\n\n2. From my experience and existing works, natural paragraphs usually have a perplexity of around 30-50 (tested by GPT-2). This score may vary based on different testing language models. However, as the results in Fig.1 show that the perplexity of generated suffixes is around 80-100, which is interesting. Can you provide some instances of the generated samples that have the lowest perplexity and the highest perplexity?\n\n3. Can you elaborate more on the implementation details about the perplexity testing and other evaluation settings, for example, settings of generating LLMs response (local models and APIs)?\n\n4. Can you share a direct transferability evacuation (in Tab.3) without a PPL filter? This can demonstrate the attack strength of each method."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6675/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6675/Reviewer_abck",
                        "ICLR.cc/2024/Conference/Submission6675/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6675/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698808832989,
            "cdate": 1698808832989,
            "tmdate": 1700720789126,
            "mdate": 1700720789126,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YrODjzhYJL",
                "forum": "ZuZujQ9LJV",
                "replyto": "5RxLjg3WLb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6675/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6675/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer abck (1)"
                    },
                    "comment": {
                        "value": "We thank reviewer abck for the helpful and constructive feedback that helps us better shape our paper! We address your concerns below.\n\n---\n### 0. Clarifying Technical Contributions\n> Specifically, the proposed method introduces a loss function to drive the generated jailbreak suffix readable and add a weight parameter to balance the attack strength and readability. \n\n> As the proposed method is aimed at solving the aforementioned trade-off and proposes a dual-target loss function\n\nWe would like to clarify our technical contributions, in case they were not sufficiently clear in the paper: our method doesn\u2019t merely add an additional loss function to achieve readability. This straightforward approach has been previously implemented in prior work and has proven ineffective (unable to achieve both readability and another factor, as illustrated in Figure 1). For reference, see the code implementations of GCG and Jain et al. 2023.\n\n**Technically**, our new gradient-based token generation algorithm pivots on three key differences: \n1) We optimize and generate each token sequentially from left to right rather than optimizing a fixed-length token sequence as done in prior work. This method, mirroring the human approach to writing text, significantly reduces the optimization space and simplifies the process. As we demonstrate, this is crucial for generating readable text using current gradient-based optimization techniques.\n2) In addition to the dual objectives in the second step of single token optimization, we introduce tailored dual objectives in the first step. This is critical for providing readable candidates in the second step (Figure 4).\n3) We combine the dual objectives by summing their logarithmic values. This approach, as we illustrate, adapts to the entropy of the next token distribution (Figure 4) and is key for efficient optimization.\n---\n### Weakness 1. Soundness\n> **Weakness 1.1:** However, the paper cannot convince me that the proposed method can solve (or avoid) the trade-off between ASR and readability.\n\nThank you for raising this important point. Let's break down the concept of the trade-off between ASR and readability, as it's not a universally acknowledged notion in the existing literature and may vary depending on the target model and definition of ASR.\n1. **Existence of the trade-off:** The ASR-readability trade-off may not necessarily exist for some models. For instance, LLMs like GPT3.5, GPT4, and Vicuna are vulnerable to manually designed prompts that are both perfectly readable and universally effective (jailbreakchat.com provides many such examples). Similarly, some prompts generated by our method also exhibit this dual quality and show no sign of such a trade-off.\n\n2. **Defining ASR:** The notion of the trade-off depends on how we define ASR. If we consider training ASR, then prioritizing readability might constrain our choice of prompts, suggesting a potential trade-off. However, when considering test ASR (same model, unforeseen behaviors) or transfer ASR (different models, unforeseen behaviors), readable prompts often show better generalization and transferability than their unreadable counterparts, as evidenced in our Table 1 (generalization) and Table 2 (transferability). This suggests that the trade-off may not exist in the latter case. The test ASR and transfer ASR matter because attackers can copy and paste any pretrained prompts to jailbreak LLMs without warming up their own GPUs, thus posing a threat to the safety of LLM applications.\n\n3. **Defining target model:** The notion of the trade-off also depends on how we define the target model. If we consider the safety alignment during RLHF as part of the target model, then a perplexity filter attached to the model may also be considered as part of it. If this is the case, then the trade-off may not exist since unreadable prompts cannot bypass the filter to jailbreak the model.\n\nIn summary, while the trade-off between ASR and readability is an interesting concept, its existence and significance may vary based on how we define ASR and the target LLM, and our method is not necessarily subject to such trade-off."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6675/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700699109458,
                "cdate": 1700699109458,
                "tmdate": 1700699109458,
                "mdate": 1700699109458,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Jd6xORe5cK",
                "forum": "ZuZujQ9LJV",
                "replyto": "5RxLjg3WLb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6675/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6675/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer abck (3)"
                    },
                    "comment": {
                        "value": "### Questions\n\n> **Question 1:** In Fig.1, why is the perplexity of the suffixes generated by the proposed method sometimes similar to those generated by GCG (and even higher)? Form the results in Fig.6, it seems the proposed method should have a clear difference in perplexity compared with GCG.\n\nPlease note that Figure 1 uses a log-scale x-axis (perplexity), so that points may appear visually similar but very different in numerical values. Also, in Figure 1 we vary the readability loss weight for GCG-reg from 1e-4 to 1, whereas we use the fixed weight of 0.1 in Figure 6 (which empirically achieves the best trade-off between readability and training ASR for GCG-reg).\n\n> **Question 2:** From my experience and existing works, natural paragraphs usually have a perplexity of around 30-50 (tested by GPT-2). This score may vary based on different testing language models. However, as the results in Fig.1 show that the perplexity of generated suffixes is around 80-100, which is interesting. Can you provide some instances of the generated samples that have the lowest perplexity and the highest perplexity?\n\nWe are not clear about what the \"generated suffix\" means. If you are referring to the AutoDAN-generated suffixes in Figure 1, then they have a perplexity of around 10 (median 12) instead of 80-100 (note the log-scale x-axis), and the normal user request represented by the dashed vertical line has a perplexity of 126. These perplexities are measured by Vicuna-13B. We provide some instances of the generated samples with varying perplexities in the raw annotation data [in this link](https://anonymous.4open.science/r/anonymous-share-8734/sample_data_annotated.html).\n\n> **Question 3:** Can you elaborate more on the implementation details about the perplexity testing and other evaluation settings, for example, settings of generating LLMs response (local models and APIs)?\n\nThank you for the suggestion! We updated the manuscript to add more implementation details (Appendix B). We generally follow the same setting in generating LLM response as GCG, and use the Huggingface's [perplexity metric](https://huggingface.co/spaces/evaluate-metric/perplexity) with Vicuna-13B (v1.5) in evaluating the perplexity.\n\n> **Question 4:** Can you share a direct transferability evacuation (in Tab.3) without a PPL filter? This can demonstrate the attack strength of each method.\n\nWe showed the direct transferability evaluation without the PPL filter in Figure 9 in the original manuscript. We have moved the evaluation results without the PPL filter to the main body of the revised paper (Figure 1, Table 3), and moved the results with the PPL filter to the appendix (Table 8, Figure 10). Thanks for the suggestion!\n\n---\nThanks again for all your time and effort in reviewing our paper! We are happy to have more discussions for any further questions."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6675/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700699628103,
                "cdate": 1700699628103,
                "tmdate": 1700705876403,
                "mdate": 1700705876403,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HGnu0TCZB2",
                "forum": "ZuZujQ9LJV",
                "replyto": "Jd6xORe5cK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6675/Reviewer_abck"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6675/Reviewer_abck"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Authors,\n\nThanks for your detailed rebuttal. However, as your rebuttal is submitted a little late, I cannot promise you that I have read all your rebuttals at this time. I agree with your opinion on the technical contribution of generating tokens sequentially from left to right and your insights on readability and ASR. So I will change my rating to 5.\n\nI must say that this rating may not be my final recommendation since I have not completed reading. One of my biggest concerns is the evaluation, also the attack strength on Llama2. I am writing this comment to inform you that I am reading your rebuttal and make any discussion if you feel necessary.\n\nRegards."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6675/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700720741235,
                "cdate": 1700720741235,
                "tmdate": 1700720741235,
                "mdate": 1700720741235,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]