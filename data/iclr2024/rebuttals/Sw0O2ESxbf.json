[
    {
        "title": "Collapsing the Learning: Crafting Broadly Transferable Unlearnable Examples"
    },
    {
        "review": {
            "id": "4EN10Gr05Q",
            "forum": "Sw0O2ESxbf",
            "replyto": "Sw0O2ESxbf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1288/Reviewer_x3eU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1288/Reviewer_x3eU"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a method to generated robust transferable unlearnable examples."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Generating effective unlearnable examples that are transferable are important task."
                },
                "weaknesses": {
                    "value": "1. The writing of the paper is extremely poor. The paper is not well-organized and it is difficult to follow.\n2. The paper does not motivate the proposed method. What is the need for transferable unlearnable examples? On that same note, the contributions of the paper and the proposed method are not well-justified. Why does the proposed method work?\n2. How does the proposed modified adversarial training help with effectiveness of the generated examples for an adversarial trained model?"
                },
                "questions": {
                    "value": "Please refer to the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1288/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1288/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1288/Reviewer_x3eU"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1288/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698780759563,
            "cdate": 1698780759563,
            "tmdate": 1699636055616,
            "mdate": 1699636055616,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "MjJfQllh1O",
                "forum": "Sw0O2ESxbf",
                "replyto": "4EN10Gr05Q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1288/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1288/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your comments and your appreciation of our findings. We address your concerns below: \n> W1: The writing of the paper is extremely poor. The paper is not well-organized and it is difficult to follow. \n\nThank you for your comment regarding the writing quality and organization of our paper. We have taken your feedback into account and have thoroughly revised and improved the manuscript based on the suggestions from you and other reviewers. The latest version has been submitted for your consideration.\n\n\n> W2: The paper does not motivate the proposed method. What is the need for transferable unlearnable examples? On that same note, the contributions of the paper and the proposed method are not well-justified. Why does the proposed method work?\n\nThank you for your question regarding the motivation and justification for our proposed method. In practical scenarios, it is desirable for unlearnable examples generated by different surrogate models to provide protective effects across various targeted model architectures. This is why we focus on the issue of transferability.\nTo achieve this goal, we first propose to induce data collapse. Using the Score Function [1], we obtain the log gradient of the data distribution. By updating the data with this log gradient, we make the original data more concentrated, thereby reducing the information contained in the data. As this process is model-agnostic, it possesses high transferability. Furthermore, to maintain protection under adversarial training conditions, we propose a modified adversarial training strategy for unlearnable examples. Ultimately, our method can achieve robust transferability. The effectiveness of our approach is demonstrated through numerous experiments in the paper.\nWe hope this response addresses your concerns about the motivation and justification for our proposed method.\n\n\n> W3:  How does the proposed modified adversarial training help with effectiveness of the generated examples for an adversarial trained model?\n\nThank you for your question regarding the effectiveness of our modified adversarial training. The modified adversarial training process helps make the surrogate model robust. In cases where the poisoned model undergoes adversarial training, it will also become robust. As a result, by utilizing robust surrogate models, we aim to generate robust unlearnable examples that remain effective even for adversarially trained models.\n\n\nIf you have any specific question, I would be very willing to address your question.\n\n- \\[1\\] A connection between score matching and denoising autoencoders. Neural computation 2011"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1288/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700711695781,
                "cdate": 1700711695781,
                "tmdate": 1700711695781,
                "mdate": 1700711695781,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dN2F6ALwpb",
                "forum": "Sw0O2ESxbf",
                "replyto": "4EN10Gr05Q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1288/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1288/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We are truly grateful for your  feedback. In our responses, we have endeavored to address every concern raised with thorough explanations and evidence. As we approach the conclusion of this phase of the review process, we are keen to ascertain whether our rebuttal has successfully resolved the issues highlighted.\nWe invite any additional comments or questions you may have regarding our responses. Your expertise and perspectives are vital in guiding the refinement and understanding of our work. We eagerly await your further guidance and insight."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1288/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700734307265,
                "cdate": 1700734307265,
                "tmdate": 1700734307265,
                "mdate": 1700734307265,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "eKaArR6xp3",
            "forum": "Sw0O2ESxbf",
            "replyto": "Sw0O2ESxbf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1288/Reviewer_KXx5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1288/Reviewer_KXx5"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes to generate unlearnable examples by essentially modifying the REM method by first learning a log-gradient estimator $s_\\theta$, then use it to induce data collapse with perturbations, before the standard REM procedure. The authors hypothesized this can help improve effectiveness and transferability, and provided experiments aiming to justify this claim."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. While it is relatively well known that out-of-distribution perturbations can lead to transferrable adversarial attacks, the paper is the first to apply the idea on unlearnable examples.\n  2. Experiments demonstrate that the unlearnable examples generated by the proposed method exhibit superior robust transferability compared to EM and REM."
                },
                "weaknesses": {
                    "value": "1. The perturbation budget of adversarial training ($\\ell_p \\leq 4/255$) is too low, larger perturbations need to be considered. Especially, unlearnable perturbations can often be much less effective by simply increasing the adversarial perturbation budget. \n  2. This paper lacks a comparison of an important baseline OPS [1]. A significant advantage of OPS is its strong resistance against $\\ell_{\\{2, \\infty\\}}$ adversarial training, even under large adversarial perturbation budgets.\n  3. The generation of poisons seems to have a high computational overhead, and requires prior training of $s_\\theta$, which can be very time-consuming and limits the practicability of proposed method.\n  4. Only adversarial training is considered as a defense method, lacks of evaluations of recently proposed defenses (ISS [2], UEraser [3], AVATAR [4]). \n  5. Please also include transformer-based models in Table 2.\n\n- [1] One-pixel shortcut: on the learning preference of deep neural networks. ICLR 2023\n- [2] Image shortcut squeezing: Countering perturbative availability poisons with compression. ICML 2023\n- [3] Learning the unlearnable: Adversarial augmentations suppress unlearnable example attacks. https://arxiv.org/abs/2303.15127\n- [4] The Devil's Advocate: Shattering the Illusion of Unexploitable Data using Diffusion Models. https://arxiv.org/abs/2303.08500"
                },
                "questions": {
                    "value": "### Questions:\n  1. Are there significant advantages of the proposed method over OPS?\n  2. The proposed method seems to be only a minor modification on REM and requires even more computational overhead than REM.\n  3. Is the effectiveness of the proposed method still guaranteed if higher budget are considered for adversarial training? Or is it like REM where the unlearnable effect is significantly reduced?\n### Other issues:\n  1. The design of the log-gradient estimator is unexplained.\n  2. Strangely, the hyperparameters $\\rho_d$, $\\alpha_d$ and $K_d$ are not defined properly, and $\\rho_d$ is never assigned value or used in the paper. Also, the update on $x$ is unconstrained.\n  3. Citations are not properly parenthesized, i.e. `\\cite{\u2026}` -> `\\citep{\u2026}`."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1288/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698831315774,
            "cdate": 1698831315774,
            "tmdate": 1699636055545,
            "mdate": 1699636055545,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ucsUNYb21o",
                "forum": "Sw0O2ESxbf",
                "replyto": "eKaArR6xp3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1288/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1288/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your valuable comments and your appreciation of our findings.  We address your concerns below: \n> Summary: The paper proposes  to induce data collapse, before the standard REM procedure. \n\nWe would like to clarify that our approach is notably different from the REM method. The key distinctions are as follows:\n1. To enhance the transferability of unlearnable examples, we propose **Data Collapse**. By employing data collapse, we reduce the information contained in the data. The ablation experiments in Table 3 of the paper demonstrate that data collapse can indeed improve transferability,\u00a0which is a feature not present in other related works.\n2. To enhance the robustness of transferability, we propose Modified Adversarial Training (MAT). This method specifically modifies traditional adversarial training to address the unique characteristics of unlearnable examples.  The optimization objective of MAT is **(min-max)-min**, while the optimization objective of REM is **min-(min-max)**. **The two objectives are fundamentally different**. Our optimization objective aims to make the substitute model $f_{\\theta}'$ robust, thereby making the generated unlearnable examples robust as well. In contrast, REM's goal is to generate unlearnable examples for adversarial samples, and its substitute model is not robust.\n3. Our method achieves robust transferability through two steps: Data Collapse and Modified Adversarial Training.\nWe hope this clarification addresses your concerns regarding the differences between our approach and the REM method. \n\n> W1: The perturbation budget of adversarial training $(\u2113\u22644/255)$ is too low, larger perturbations need to be considered. Especially, unlearnable perturbations can often be much less effective by simply increasing the adversarial perturbation budget.\n\nThank you for your comment regarding the perturbation budget. In response to your concern, we have increased both $\\rho_u$ and $\\rho_a$, and the experimental results are presented in the table below.\n\n| | $\\rho_a = 0$ | $\\rho_a=8/255$ | $\\rho_a=16/255$ | $\\rho_a=32/255$ | $\\rho_a=64/255$ |\n| - | :-: | :-: | :-: | :-: | :-: |\n|Clean| 94.66 | 84.79 | 70.41 | 42.76 | 10.00\n|Ours($\\rho_u=16/255$)| 10.21 | 15.80 | 71.73 | 43.75 | 10.00 |\n|Ours($\\rho_u=32/255$) | 10.35 | 10.46 | 18.07 | 53.83 | 10.00 |\n|Ours($\\rho_u=64/255$) | 10.15 | 10.27 | 11.01 | 54.30 | 10.00 |\n\nAs you pointed out, increasing $\\rho_a$$ does indeed reduce the effectiveness of unlearnable examples. To clarify, we define the party generating unlearnable examples as the Attacker and the party training models using unlearnable examples as the Defender. In classification tasks, the Attacker's goal is to minimize the test accuracy of the Defender's model trained on unlearnable examples, while the Defender's goal is to achieve higher model accuracy.\n1. From the Attacker's perspective, increasing $\\rho_u$ has little cost, as there is no need to use unlearnable examples. However, for the Defender, undermining the effect of unlearnable examples requires increasing $\\rho_a$, which is costly. As seen in the table above, as $\\rho_a$ increases, the performance upper bound of the model declines significantly. This is actually an arms race, where the Attacker forces the Defender to raise $\\rho_a$ by increasing $\\rho_u$. Once $\\rho_a$ reaches a certain level, the model's training difficulty increases, and the performance upper bound is significantly reduced. In this way, the Attacker achieves their goal.\n2. In this scenario, Table 1 in the paper indicates that our method is superior to other methods. our method provides valuable insights into the dynamics between the Attacker and Defender and showcases its effectiveness in various settings, rather than being limited to a specific perturbation budget.\nWe hope this response addresses your concerns and provides a clearer understanding of our approach.\n\n> W2 : This paper lacks a comparison of OPS [1]. A significant advantage of OPS is its strong resistance against\u00a0$\u2113_{2,\u221e}$\u00a0adversarial training, even under large adversarial perturbation budgets.\n\nThank you for your comment regarding the comparison with the OPS baseline. We acknowledge the importance of including relevant baselines for a comprehensive evaluation. In practice, **data augmentation methods are often employed** during the model training process, and their cost is **significantly lower** than that of adversarial training. OPS is prone to being compromised by data augmentation techniques([2,3]). The augmentation techniques almost have no effects on our method.  The table below shows the protect effects on CIFAR10 with data enhancements. \n\n|  |w/o| Median | CutMix | Cutout | ISS-JPEG10 |\n| :-: | :-: | :-: | :-: | :-: | :-: | \n| OPS | 36.55 | 85.16 | 76.40 | 67.94| 82.53 |\n| Ours | 10.47 | 18.03 | 11.05 | 14.86 | 38.11|\n\nWe hope this response addresses your concerns and provides a clearer understanding of our approach in comparison to the OPS baseline."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1288/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700712068065,
                "cdate": 1700712068065,
                "tmdate": 1700712068065,
                "mdate": 1700712068065,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tt3qlMyeJQ",
                "forum": "Sw0O2ESxbf",
                "replyto": "eKaArR6xp3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1288/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1288/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We are truly grateful for your valuable insights and comprehensive feedback. In our responses, we have endeavored to address every concern raised with thorough explanations and evidence. As we approach the conclusion of this phase of the review process, we are keen to ascertain whether our rebuttal has successfully resolved the issues highlighted.\nWe invite any additional comments or questions you may have regarding our responses. Your expertise and perspectives are vital in guiding the refinement and understanding of our work. We eagerly await your further guidance and insight."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1288/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700734273608,
                "cdate": 1700734273608,
                "tmdate": 1700734273608,
                "mdate": 1700734273608,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "W3kglsSOaW",
            "forum": "Sw0O2ESxbf",
            "replyto": "Sw0O2ESxbf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1288/Reviewer_qwUi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1288/Reviewer_qwUi"
            ],
            "content": {
                "summary": {
                    "value": "The authors investigate the problem of generating robust transferable unlearnable data that can protect the data information against adversarial training. An algorithm based on data collapse is proposed and evaluated in numerical experiments using CIFAR-10 and CIFAR-100."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The mechanism of the proposed algorithm to protect the data from adversarial training is interesting. \n\n2. The numerical results are impressive in general."
                },
                "weaknesses": {
                    "value": "1. As pointed out by the authors in the last section, the proposed method requires significant computational cost and is currently unscalable to large dataset like ImageNet. \n\n2. The proposed Algorithm 1 has many parameters. Can you provide more discussion on how to set them in practice based on data, and how are they related to the performance or robustness against adversarial training?\n\nWhile I do not find other major weakness at this point, I do have additional questions in the next section."
                },
                "questions": {
                    "value": "1. In Section 3.3, the predefined distribution is set as the normal distribution $N(\\tilde{x};x, \\sigma^2I)$. I am wondering what is the reason or intuition for such setting. Can we use a different distribution? \n\n2. In Algorithm 1, the PGD process in lines 13-16 and lines 17-20 are quite similar. What is the advantage of two-stage PGD over one-stage? Also, is there any trade-off between the magnitude of $\\rho_u$ and $\\rho_a$?\n\n3. Following the previous question, if the two-stage PGD is more powerful than one-stage, can we modify the algorithm to add more stages in PGD to expect a better performance?\n\n4. In Table 1, while the proposed method outperforms the compared methods in most cases, it is less appealing than EM over larger dataset (CIFAR-100 and ImageNet Subset) when there is no adversarial training. Can you discuss the possible reason for that?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1288/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699045462806,
            "cdate": 1699045462806,
            "tmdate": 1699636055410,
            "mdate": 1699636055410,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Y3YhNlk2yk",
                "forum": "Sw0O2ESxbf",
                "replyto": "W3kglsSOaW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1288/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1288/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your valuable comments and your appreciation of our findings. \nTo clarify, we did not merely compare CIFAR-10 and CIFAR-100; we also compared a subset of the first 100 classes of ImageNet. This covers data with different categories and resolutions. Subsequent transferability experiments were conducted only on CIFAR due to resource constraints. \nWe address your concerns below: \n\n> W1. As pointed out by the authors in the last section, the proposed method requires significant computational cost and is currently unscalable to large dataset like ImageNet.\n\nThank you for your comment. Detailly, in our setting, we employed 4 V100 GPUs to train ResNet-18 on the ImageNet-Subset for 5,000 steps, which takes only 39 hours. In the future, we will further explore how to improve the speed of our methods.\n\n> W2. The proposed Algorithm 1 has many parameters. Can you provide more discussion on how to set them in practice based on data, and how are they related to the performance or robustness against adversarial training? \n\n\nThank you for your question.  For the data collapse step,   \n$\\alpha_d$ is based on commonly used default parameters from some score-based generation methods. As for $K_{\\alpha}$, it is a hyperparameter, and its value is determined based on computational cost and the value of $\\rho_u$.  For the Adv step, the step sizes $\\alpha_a, \\alpha_u$ are derived from the step counts $K_a, K_u$ and the perturbation ranges $\\rho_a, \\rho_u$. We usually ensure that $K_a \\cdot \\alpha_a > \\rho_a, K_u \\cdot \\alpha_u > \\rho_u$.\n\nThe larger the $\\rho_u$, the better the protection effect. The larger the $\\rho_a$, the greater the range of adversarial training perturbations that can be protected, but it must be ensured that $\\rho_a < \\rho_u$. When the dataset resolution is large, $K_a, K_u$ typically choose smaller steps to reduce computational cost, and under constant $\\rho_a, \\rho_u$, $\\alpha_a, \\alpha_u$ need to be larger. Generally speaking, the more steps $K_a, K_u$ have, the better the effect.\n\n\n> Q1. In Section 3.3, the predefined distribution is set as the normal distribution\u00a0$\\mathcal{N}(\\hat{x};x,\\sigma^2I)$. I am wondering what is the reason or intuition for such setting. Can we use a different distribution?\n\nThank you for your insightful question. \nFirstly, our training objective is to estimate the true data distribution $p_D(x)$, but we do not have the ground truth of the real data. Secondly, according to Stochastic Gradient Langevin Dynamics, what we actually need is $\\nabla_x \\log p_D{(x)}$, which is referred to as the Score, with the full name being Stein Score. Therefore, we estimate the Score using the denoise score matching [1] method.\n\nIn the Denoise Score Matching process, we add Gaussian noise $\\epsilon = \\mathcal{N}(0, \\sigma^2)$, resulting in $\\tilde{x} = x + \\epsilon$. According to the properties of the Gaussian distribution, we obtain the distribution $q(\\tilde{x}|x)= \\mathcal{N}(\\tilde{x};x,\\sigma^2I)$ as mentioned in the paper.\nThe optimization objective of Denoise Score Matching is Equation 3:  $$\\frac{1}{2} \\mathbb{E}_{q_\\sigma(\\tilde{x} \\mid x) p_D(x)}\\left[\\left\\|s_\\theta(\\tilde{x})-\\nabla_{\\tilde{x}} \\log q_{\\sigma}(\\tilde{x}|x)\\right\\|_2^2\\right]$$Therefore, to make the equation meanfuling, any distribution that satisfies the existence of $\\nabla_{\\tilde{x}} \\log q_{\\sigma}(\\tilde{x}|x)$ can be used.\n\n\n\n> Q2. In Algorithm 1, the PGD process in lines 13-16 and lines 17-20 are quite similar. What is the advantage of two-stage PGD over one-stage? Also, is there any trade-off between the magnitude of\u00a0$\\rho_a$\u00a0and\u00a0$\\rho_u$?\n\nThank you for your question.  PGD[2] is a standard approach for solving inner maximization and minimization problems.  It performs iterative projection updates to search for the optimal perturbation.\n\nIn our modified adversarial training, the optimization formula is min-max-min. Our algorithm incorporates two PGD processes. The first PGD process corresponds to the inner min in the optimization formula, solving for $\\delta_u$. The second PGD process corresponds to the max procedure in the optimization formula, solving for $\\delta_a$. The larger the $\\rho_u$, the better the protection effect. The larger the $\\rho_a$, the greater the range of adversarial training perturbations that can be protected, but it must be ensured that $\\rho_a < \\rho_u$.\n\n> Q3. Following the previous question, if the two-stage PGD is more powerful than one-stage, can we modify the algorithm to add more stages in PGD to expect a better performance?\n\nThank you for your question. As mentioned in Q2, the PGD step is used to different function. Each PGD step corresponds to an optimization objective, so it cannot be arbitrarily added."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1288/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700200054464,
                "cdate": 1700200054464,
                "tmdate": 1700200054464,
                "mdate": 1700200054464,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "T8w31ATgfu",
                "forum": "Sw0O2ESxbf",
                "replyto": "W3kglsSOaW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1288/Reviewer_qwUi"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1288/Reviewer_qwUi"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks much for the response and clarifications. I misunderstood a small part of the algorithm in the beginning. I think my previous questions are addressed properly.\n\nAs I go through other reviews, I find another major concern that I did not think of earlier, as raised by reviewer KXx5. \n\nIn the numerical studies, for adversarial training it seems only the approach in [1] is considered. I am curious about the performance of the work with adversarial training (or other robust) schemes proposed more recently. \n\n[1] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.\nTowards deep learning models resistant to adversarial attacks. In 6th International Conference on\nLearning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference\nTrack Proceedings. OpenReview.net, 2018. URL https://openreview.net/forum?id=\nrJzIBfZAb."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1288/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700202718954,
                "cdate": 1700202718954,
                "tmdate": 1700202749377,
                "mdate": 1700202749377,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fKBN2HNAm0",
                "forum": "Sw0O2ESxbf",
                "replyto": "W3kglsSOaW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1288/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1288/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We are truly grateful for your valuable insights and comprehensive feedback. In our responses, we have endeavored to address every concern raised with thorough explanations and evidence. As we approach the conclusion of this phase of the review process, we are keen to ascertain whether our rebuttal has successfully resolved the issues highlighted.\nWe invite any additional comments or questions you may have regarding our responses. Your expertise and perspectives are vital in guiding the refinement and understanding of our work. We eagerly await your further guidance and insight."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1288/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700734262614,
                "cdate": 1700734262614,
                "tmdate": 1700734262614,
                "mdate": 1700734262614,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "fHp4Rp6zNl",
            "forum": "Sw0O2ESxbf",
            "replyto": "Sw0O2ESxbf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1288/Reviewer_czgC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1288/Reviewer_czgC"
            ],
            "content": {
                "summary": {
                    "value": "This paper is concerned with approaches that can help protect data against unauthorised use by learning algorithms by crafting a noise that makes the data \u201cunlearnable\u201d. They propose to first train a log gradient estimator offline which is used to modify the training samples with an SGLD-like update. This is followed by incorporating PGD-noise over an adversarially trained surrogate model. Extensive empirical evaluations are performed to test the transferability of the proposed method and compare it across two methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The work is novel in its formulation of transforming training samples to make them unlearnable via a two-step process - a model-independent modification of data samples to reduce the variability in the the data distribution, followed by incorporation of PGD-noise over an adversarially trained surrogate."
                },
                "weaknesses": {
                    "value": "- While transferability has been emphasised as a distinction, I think its interpretation relative to Robust Unlearning Methods needs to be clearly defined. \n\n- From a presentation and readability standpoint, I think including the arguments for \u201chow\u201d this approach tackles transferability upfront would better clarify the contributions. Moreover, it might be worth considering if the aim is to ensure universality rather than transferability."
                },
                "questions": {
                    "value": "- Does the sequence of Collapse and Adv matter?\n\n- It would be useful to clearly describe the second step in term of what f represents and how PGD over adversarially trained model is carried out. I think the algorithm has the required steps but the description in Section 3.4 wasn\u2019t sufficiently clear.\n\n- In the current notations model f and model s are defined with shared parameters of \\theta. Please clarify\n\n- From my understanding the universailty/transferability relies on the first step of modification of data distribution. This approach to model-independent modification needs to chould be positioned in contrast to related works. For instance, can this be combined with other approaches as a pre/post processing step?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1288/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699262103341,
            "cdate": 1699262103341,
            "tmdate": 1699636055336,
            "mdate": 1699636055336,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Aajs6tg0MN",
                "forum": "Sw0O2ESxbf",
                "replyto": "fHp4Rp6zNl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1288/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1288/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your valuable comments and your appreciation of our findings. We address your concerns below:\n\n> W1. The interpretation relative of transferability to Robust Unlearning Methods.\n\nThank you for your suggestions.  We have revised our manuscript to clarify this point. Our transferability refers to the effectiveness of unlearnable examples generated by different surrogate models on various poisoned models, regardless of whether they employ standard training or adversarial training strategies. We have highlight the definition in the introduction with italic type. \n\n> W2. From a presentation and readability standpoint, I think including the arguments for \u201chow\u201d this approach tackles transferability upfront would better clarify the contributions. Moreover, it might be worth considering if the aim is to ensure universality rather than transferability.\n\nThank you for your suggestion. We have summarized the arguments and put them upfront our approach in Section 3.3.  $x_t$ represents the sample at step $t$; $\\nabla \\log p_D(x_t)$ denotes the $\\log$ gradient of the data distribution concerning the sample $x_t$; $\\alpha$ signifies the step size; $\\epsilon$ represents random noise following a distribution. $s_{\\theta}$ indicates the data distribution gradient estimator, while $\\tilde{x}$ refers to the sample after adding noise.  Universality is also an important research direction, and we will further deepen our research in this field in the future.\n\n> Q1. Does the sequence of Collapse and Adv matter?\n\nThank you for your  insightful question regarding the sequence of Collapse and Adv. Yes, the order in which they are performed does matter. We first apply Collapse to modify the data, aiming to enhance the transferability of unlearnable examples. Subsequently, we conduct the modified Adv to improve the robustness of transferability.\nIf we were to perform Adv first, the resulting noise would be model-dependent, and the subsequent Collapse would be based on this model-dependent foundation, significantly reducing transferability. Moreover, if the noise generated by Adv varies each time, a new data distribution estimator would need to be trained for each instance. By executing Collapse before Adv, we avoid this issue, as Collapse targets the original data.\n\n> Q2. Clearly describe the the second step in term of what f represents and how PGD over adversarially trained model is carried out.<It would be useful to clearly describe the second step in term of what f represents and how PGD over adversarially trained model is carried out. I think the algorithm has the required steps but the description in Section 3.4 wasn\u2019t sufficiently clear.\n\nThank you for your question. \n1. $f_{\\theta}'$ represents the surrogate model, which is distinct from the $s_{\\theta}$ used in the first step for inducing data exploration. These are two separate models.\n2. PGD[1] is a standard approach for solving inner maximization and minimization problems. It performs iterative projection updates to search for the optimal perturbation as follows:\n$$\\delta^{(k)} = \\prod_{\\| \\delta \\| \\leq \\rho} \\left[ \\delta^{(k-1)} + c \\cdot \\alpha \\cdot \\text{sign} \\left( \\frac{\\partial}{\\partial \\delta} \\ell(f_\\theta(x + \\delta^{(k-1)}), y) \\right) \\right] $$\n\tWhere $k$ is the current iteration step ($K$ steps at all), $\\delta^{(k)}$ is the perturbation found in the $k$-th iteration, $c\\in\\{-1,1\\}$ is a factor for controlling the gradient direction, $\\alpha$ is the step size, and $\\prod_{\\|\\delta\\|\\leq\\rho}$ means the projection is calculated in the ball sphere $\\{ \\delta:\\|\\delta\\| \\leq \\rho \\}$. The final output perturbation is $\\delta^{(K)}$. Throughout this paper, the coefficient $c$ is set as $1$ when solving maximization problems and $-1$ when solving minimization problems. This also can be found in Appendix A.3. \n3. We employ modified adversarial training to update the surrogate model in three main steps. First, we use the PGD algorithm to generate unlearnable noise for clean samples, obtaining $x+\\delta^u$. Second, we apply the PGD algorithm to $x+\\delta^u$ to produce adversarial noise, resulting in $x+\\delta^u+\\delta^a$. Finally, we input the obtained $x+\\delta^u+\\delta^a$ into the surrogate model and update the surrogate model parameters using stochastic gradient descent. We have modified the Section3.4. \n\n> Q3. Is the current notations model f and model s are defined with shared parameters of $\\theta$. Please clarify\n\nThank you for your question. No, the current notations for the models $f_{\\theta}'$ and $s_{\\theta}$ represent different parameters. For $s_{\\theta}$, it represents the log gradient of the data distribution estimator, and we use the U-Net structure. While for $f'_{\\theta}$, it represents the surrogate model for generating robust unlearnable examples, and we employ common architectures such as VGG-16, ResNet-18, ResNet-50, DenseNet-121, and WRN-34-10.\n\n- \\[1\\] Towards deep learning models resistant to adversarial attacks. ICLR2018"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1288/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700199896736,
                "cdate": 1700199896736,
                "tmdate": 1700710618178,
                "mdate": 1700710618178,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AbA8CQeVfY",
                "forum": "Sw0O2ESxbf",
                "replyto": "fHp4Rp6zNl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1288/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1288/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Q4. From my understanding the universailty/transferability relies on the first step of modification of data distribution. This approach to model-independent modification needs to should be positioned in contrast to related works. For instance, can this be combined with other approaches as a pre/post processing step? \n\nThank you for your insightful question.  Yes, the data collaspe step can be combined with other related methods. The following table shows the results on CIFAR10.\n\n| | VGG-16 | ResNet-18 | ResNet-50 | DenseNet-121 | WRN-34-10| ViT |\n| :- | :-: | :-: | :-: | :-: | :-: | :-: |\n| EM | 15.70 | 13.20 | **10.19** | 14.59 | 10.56 | 72.62 |\n| EM+(**Data Collapse**) | **11.20** | **11.44** | 10.72 | **12.50** | **10.23** | **19.02** |\n| TAP | 29.39 | 22.15 | 27.69 | 81.63 | 27.04| 52.28 |\n| TAP+(**Data Collaspe**) | **23.85** | **15.57** | **22.87** | **75.78** |  **20.89** | **47.15** |\n\n\nTable 3 in the paper also demonstrates that our data collapse step can be combined with other methods. In Table 3 of the paper, **w/o(Collaspe + Adv) = EM**, **w/o(Adv) = EM + Data Collapse**. Both Table 3 and the table above show that our data collapse step can be integrated with other methods and can improve transferability."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1288/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700200227739,
                "cdate": 1700200227739,
                "tmdate": 1700710646151,
                "mdate": 1700710646151,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XxVb3nIrA9",
                "forum": "Sw0O2ESxbf",
                "replyto": "fHp4Rp6zNl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1288/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1288/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We are truly grateful for your valuable insights and comprehensive feedback. In our responses, we have endeavored to address every concern raised with thorough explanations and evidence. As we approach the conclusion of this phase of the review process, we are keen to ascertain whether our rebuttal has successfully resolved the issues highlighted.\nWe invite any additional comments or questions you may have regarding our responses. Your expertise and perspectives are vital in guiding the refinement and understanding of our work. We eagerly await your further guidance and insight."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1288/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700734243950,
                "cdate": 1700734243950,
                "tmdate": 1700734243950,
                "mdate": 1700734243950,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]