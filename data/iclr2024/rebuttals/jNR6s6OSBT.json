[
    {
        "title": "ASID: Active Exploration for System Identification and Reconstruction in Robotic Manipulation"
    },
    {
        "review": {
            "id": "jjG8L8lSX3",
            "forum": "jNR6s6OSBT",
            "replyto": "jNR6s6OSBT",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3016/Reviewer_4bkX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3016/Reviewer_4bkX"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a framework for model-based RL aimed at learning model parameters as well as the optimal policy given the model. The method seeks to address to the sim-to-real gap by proposing an efficient policy for exploring the environment inasmuch as that exploration improves the model. At each step, the method finds the policy that approximately maximizes the Fisher Information in the trajectories we expect the policy to encounter when rolled out. The method is experimentally validated on a number of real world environments."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper addresses an important problem, i.e. directing exploration of an environment in an effort to reduce model uncertainty.\n- The paper proposes a seemingly novel approach of finding policies that maximize the Fisher information.\n- The paper validates the approach using real-world experiments"
                },
                "weaknesses": {
                    "value": "- Presentation/clarity can be improved: specifically, the abstract and introduction mostly describe the field of active exploration for system identification and adaptive control, as opposed to the specific method proposed, which appears to overstate the paper\u2019s novelty. Furthermore, the approach warrants a better intuitive explanation. As I understand it, the Fisher Information objective attempts to quantify the sensitivity of model parameters to trajectories expected given some policy. Therefore, maximizing this objective yields a policy that, when executed, yields the maximum additional information about the model parameters.\n- Lack of baselining/adequate discussion of other methods that use the Fisher information objective. The statement \u201cAs compared to these works, a primary novelty of our approach is the use of a simulator to learn effective exploration policies\u201d seems too strong and overstated given that there are entire fields dedicated to this, and \u201cthe application of our method to modern, real-world robotics tasks\u201d is an inadequate claim to novelty.\n- Literature review can be improved with a discussion of the following:\n    - Bayesian RL/Bayes-adaptive MDPs: \tM. Duff. Optimal Learning: Computational Procedure for Bayes-Adaptive Markov Decision Processes. \u2028PhD thesis, University of Massachusetts, Amherst, USA, 2002. \n    - PILCO:\n        - Deisenroth, Marc, and Carl E. Rasmussen. \"PILCO: A model-based and data-efficient approach to policy search.\" Proceedings of the 28th International Conference on Machine Learning (ICML-11). 2011.\n    - Adaptive MPC:\n        - S. M. Richards, N. Azizan, J.-J. Slotine, and M. Pavone. Adaptive-control-oriented meta-learning for nonlinear systems. In Robotics: Science and Systems, 2021. URL https://arxiv.org/abs/2204.06716.\n        - Sinha, Rohan, et al. \"Adaptive robust model predictive control with matched and unmatched uncertainty.\" 2022 American Control Conference (ACC). IEEE, 2022.\n    - System identification in partially observable environments:\n        - Menda, Kunal, et al. \"Scalable identification of partially observed systems with certainty-equivalent EM.\" International Conference on Machine Learning. PMLR, 2020\n        - Sch\u00f6n, Thomas B., Adrian Wills, and Brett Ninness. \"System identification of nonlinear state-space models.\" Automatica 47.1 (2011): 39-49."
                },
                "questions": {
                    "value": "1. In Section 4.2.1, I was expecting to see the standard SysID loss, which is to maximize the likelihood of the data (in this case trajectories) given model parameters. You find the distribution that maximizes likelihood for domain randomization. It seems to me that without some sort of entropy maximization term in the objective, or bootstrap, you would just end up with an MLE objective, whereas it seems like you want to find the Bayesian posterior of models given data. Can you comment on how your objective relates to that of finding a Bayesian posterior?\n2. For a paper proposing active exploration for the sake of system identification, I wanted to see more discussion of the following: a) regret minimization, i.e. can you prove that your method minimizes regret and achieves the best policy with the fewest interactions with the environment? b) identifiability, i.e. can you say anything about whether all system parameters will be uniquely identified with infinite interactions?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3016/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3016/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3016/Reviewer_4bkX"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3016/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697501810408,
            "cdate": 1697501810408,
            "tmdate": 1700482431669,
            "mdate": 1700482431669,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "D3j8rz5m7w",
                "forum": "jNR6s6OSBT",
                "replyto": "jjG8L8lSX3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3016/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3016/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "__\u201dPresentation/clarity can be improved: specifically, the abstract and introduction mostly describe the field of active exploration for system identification and adaptive control, as opposed to the specific method proposed, which appears to overstate the paper\u2019s novelty. Furthermore, the approach warrants a better intuitive explanation. As I understand it, the Fisher Information objective attempts to quantify the sensitivity of model parameters to trajectories expected given some policy. Therefore, maximizing this objective yields a policy that, when executed, yields the maximum additional information about the model parameters.\u201d__\n\nThis is correct. As described in Section 3, the inverse Fisher information matrix serves as a fundamental lower bound on parameter estimation error, so exploring to maximize the Fisher information will collect data that will yield the most accurate estimate of the unknown parameters.\n\nIntuitively, the Fisher information quantifies the sensitivity of the trajectory distribution to the unknown parameter. Note that the Fisher information is defined in terms of the gradient of the log-likelihood with respect to the unknown parameter. Thus, trajectory distributions for which the Fisher information is large correspond to trajectory distributions that are very sensitive to the unknown parameters, i.e., trajectory distributions that are significantly more likely under one set of parameters than another. By exploring to maximize the Fisher information, we, therefore, will collect trajectories that are maximally informative about the unknown parameters, since we will observe trajectories that will be significantly more likely under one set of parameters than another.\n\n\n__\u201dLack of baselining/adequate discussion of other methods that use the Fisher information objective. The statement \u201cAs compared to these works, a primary novelty of our approach is the use of a simulator to learn effective exploration policies\u201d seems too strong and overstated given that there are entire fields dedicated to this, and \u201cthe application of our method to modern, real-world robotics tasks\u201d is an inadequate claim to novelty.\u201d__\n\nIt is certainly the case that using the Fisher information matrix to guide exploration has been proposed before (as our related work makes clear), and, in addition, the use of simulators to aid in real-world robotic learning is also common practice. However, to the best of our knowledge, our work is the first to combine the advantages of using high-fidelity simulators to aid in real-world robotics with active Fisher information-guided exploration for parameter estimation. \n\nIn particular, essentially all the works we have cited that rely on the Fisher information matrix to direct exploration (a) do not make use of prior information encoded in simulators to aid in exploration and (b) either do not provide real-world experimental justification for their methods or, if they do, consider only very simple toy problems. In contrast, essentially all the works we are aware of that aim to use simulators to aid in real-world robotic tasks do not perform the type of directed exploration we are proposing to estimate the unknown parameters, but instead use the simulators in a more naive manner, typically by simply transferring a policy that is trained in sim to solves the goal task. Our work can be seen as bringing together these directions\u2014demonstrating that classical works on active parameter estimation can be combined with modern sim2real techniques to solve real-world robotic problems.\n\nIf the reviewer is aware of further works that are more relevant to compare to what we have missed in our related work, providing these references would be extremely helpful so we can better situate our work in the existing literature."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3016/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700276546439,
                "cdate": 1700276546439,
                "tmdate": 1700276546439,
                "mdate": 1700276546439,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nE0x230sg3",
                "forum": "jNR6s6OSBT",
                "replyto": "q32ZbZMCGZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3016/Reviewer_4bkX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3016/Reviewer_4bkX"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your comments"
                    },
                    "comment": {
                        "value": "Thanks for addressing my questions. I've updated my recommendation to weak accept, but still think this paper could be improved with the following addressed:\n1. In other responses you suggest that other model-based RL algorithms typically leverage fully learned models and not physics based simulators. I don't think this is true, even though a lot of recently popular model-based RL papers might focus on such approaches. Consider the survey of model based RL in Chapter 16 of [1]. No approaches explicitly require the model to be fully learned. \n2. It would be great to baseline against a Bayesian model-based RL like Thompson sampling (16.6 in [1]). I think it would be entirely feasible to construct a Laplace approaximation for the posterior over model parameters given data since you're able to construct the Fisher information matrix. You don't necessarily need to start with a prior over model parameters but certainly could use one.\n\n\n[1] Kochenderfer, Mykel J., Tim A. Wheeler, and Kyle H. Wray. Algorithms for decision making. MIT press, 2022."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3016/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700483134475,
                "cdate": 1700483134475,
                "tmdate": 1700483134475,
                "mdate": 1700483134475,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "TdQ4pyeRjj",
            "forum": "jNR6s6OSBT",
            "replyto": "jNR6s6OSBT",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3016/Reviewer_zEL2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3016/Reviewer_zEL2"
            ],
            "content": {
                "summary": {
                    "value": "This work lays out a framework for robotic manipulation systems to explore and model unknown environments, as well as train a policy to succeed at control tasks within this environment. This generic pipeline for sim to real transfer is called Active Exploration for System Identification, or ASID, and it involves three stages: exploration to gather information about the environment, refinement of this simulation with the data, and training a policy in the learned environment. This approach is shown to be both highly successful and very data efficient, with real work robotics examples shown both in simulation and on real hardware."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is well written and has a nice flow to it. Organization and structure both help with this as well. \n2. The sections on related work and preliminaries do a good job of giving the appropriate context/notation. \n3. The tasks chosen to demonstrate this approach were challenging, informative, and speak to the efficacy of the approach. \n4. Hardware experiments look convincing. \n5. The connections to A-optimal experiment design are insightful and appropriate."
                },
                "weaknesses": {
                    "value": "1. More detail on why the Fisher Information is used vs other methods (observability Grammian, Kalman Filter covariance).\n2. The numbers in the heatmaps in Figure 4 are hard to read, maybe block font for the numbers?"
                },
                "questions": {
                    "value": "Potential typos: \n1. End of section 1 says \"signal episode\", should this be \"single episode\"?\n2. Section 4.3 says \"zero-short\", should this be \"zero-shot\"?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3016/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3016/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3016/Reviewer_zEL2"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3016/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698702614029,
            "cdate": 1698702614029,
            "tmdate": 1699636246260,
            "mdate": 1699636246260,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Y1t4KT0WXD",
                "forum": "jNR6s6OSBT",
                "replyto": "TdQ4pyeRjj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3016/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3016/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "__\u201dMore detail on why the Fisher Information is used vs other methods (observability Grammian, Kalman Filter covariance).\u201d__\n\nAs described in Section 3, the inverse Fisher information matrix is a fundamental lower bound on the difficulty of parameter estimation and, in addition, serves as an upper bound when the estimator is, for example, the maximum likelihood estimate. Given this, to minimize parameter estimation error (which is the goal of our exploration procedure), we should aim to explore so as to maximize the Fisher information\u2014doing this will collect data that produces the most accurate estimate of the unknown parameters. Thus, in short, maximizing the Fisher information is the mathematically optimal thing to do if the goal is to minimize estimation error. While criteria other than the Fisher information may be used to direct exploration as the reviewer suggests, they lack this optimality property, and would not necessarily collect data that yields the best possible parameter estimate.\n\nWe also remark that using the Fisher information to guide the data collection has a long history in the theory of statistics and experiment design. We refer the reviewer to works such as [1] and [2] below for further discussion of this, as well as formal justification of the argument we have presented above.\n\n[1] Luc Pronzato and Andrej Pazman. Design of experiments in nonlinear models. Lecture notes in statistics, 212(1), 2013.\n\n[2] Friedrich Pukelsheim. Optimal design of experiments. SIAM, 2006."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3016/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700276505528,
                "cdate": 1700276505528,
                "tmdate": 1700276505528,
                "mdate": 1700276505528,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "AFv4wpOSqs",
            "forum": "jNR6s6OSBT",
            "replyto": "jNR6s6OSBT",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3016/Reviewer_gS1p"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3016/Reviewer_gS1p"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a method for active exploration for model based reinforcement learning in the context of robotic manipulation. The paper introduces an exploration policy based on the Fisher information matrix of the parameters of the model. Then, they also include a vision system for scene reconstruction and experimental evaluation based on a robotic manipulator, both in real and simulation environments."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The main strength of this paper is the fact that part of the experiments are done in a real manipulator. Also, the pipeline of doing active exploration for model learning (system identification) is fundamental for robotic applications."
                },
                "weaknesses": {
                    "value": "The main weakness for this paper is that this pipeline is very similar to other exploration methods model-based RL. For example:\nShyam P, Ja\u015bkowski W, Gomez F. Model-based active exploration. In International conference on machine learning 2019 May 24 (pp. 5779-5788). \n\nPathak D, Gandhi D, Gupta A. Self-supervised exploration via disagreement. In International conference on machine learning 2019 May 24 (pp. 5062-5071). \n\nIn fact, the pipeline is quite similar to Shyam et al. albeit the metrics and models used are different. However, due to the similarities in the process, those papers should be discussed and, ideally, included in the comparison.\n\nWhile the experimental section is one of the strengths due to the evaluation in a realistic robotic scenario, the methods should also be evaluated on standard benchmarks for comparison, such as HalfCheetah. The baseline used [Kumar2019] seems very weak (in Fig 4 it does not explore at all). Furthermore, the work of Kumar2019 does not seem to be related to exploration with mutual information as stated in this work."
                },
                "questions": {
                    "value": "-I do not fully understand the reference to REPS as that is a model-free RL method. There is no transition model estimation.\n-It seems that the system relies on the assumption that a learned simulator is able to generate accurate trajectories, but that is not the case for out of distribution trajectories. I understand that exploration precisely minimizes that effect, but the probabilistic model should be able to capture the lack of information in out of distribution data. Currently, the only uncertainty comes from the noise if I understand correctly.\n-The scene reconstruction part seems to be a part of the specific experiments presented in the paper, but it is unrelated to the exploration pipeline.\n-How did you use RANSAC for tracking?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3016/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699061842533,
            "cdate": 1699061842533,
            "tmdate": 1699636246181,
            "mdate": 1699636246181,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UwT69X20HP",
                "forum": "jNR6s6OSBT",
                "replyto": "AFv4wpOSqs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3016/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3016/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "__\u201dThe main weakness for this paper is that this pipeline is very similar to other exploration methods model-based RL. For example: Shyam et al. Model-based active exploration. ICML, 2019. In fact, the pipeline is quite similar to Shyam et al. albeit the metrics and models used are different. However, due to the similarities in the process, those papers should be discussed and, ideally, included in the comparison.__\n\n__I do not fully understand the reference to REPS as that is a model-free RL method. There is no transition model estimation.\n\u201d__\n\nWe would like to note that ASID is not quite in the same realm as a model-based RL algorithm. Instead, it considers exploring and collecting data so as to identify the parameters of a physics-based *simulator* (such as MuJoCo or PyBullet), rather than an end-to-end model as most model-based RL algorithms would do. In the following, we discuss 1) the conceptual difference of ASID from model-based exploration in terms of explorative behavior and model extrapolation, and 2) provide empirical evidence.\n\n_1) Conceptual difference of ASID from model-based exploration_\n\nASID fundamentally differs from model-based exploration in the problem setting. Model-based exploration typically leverages fully learned models to guide exploration with the goal of achieving coverage in the state space. The exploration methods proposed by [1] [2] use a learned ensemble of forward models and their disagreement to learn multiple exploration policies that together cover the state space. The collected data is then used to train a downstream task policy. In contrast, our approach relies on the use of an existing physics-based simulator and explores to identify specific unknown parameters in the simulator (e.g. mass, friction coefficients), rather than learning a full model. Our exploration policy searches out state transitions that are most affected by a change in the underlying physics parameters [Appendix Fig. 10c, 10d]. We then roll out a single trajectory in the real world to identify the parameters of the simulator. We use a physics simulator because it extrapolates to unseen states in contrast to a fully learned model [Appendix Fig. 11]. \n\nThis means we don't learn a dynamics model anywhere in our pipeline and use a model-free black box optimization method (REPS) to identify the physics parameters of our simulator. However, we'd like to note that using a simulator in the process can be seen as a very specific instance of a model-based approach.\n\n_2) Empirical Evidence_\n\nWe construct an experiment to differentiate ASID from Model-based active exploration (MAX) [1]. The environment contains four spheres out of which the red sphere\u2019s friction parameters are varied while keeping the parameters of the three blue spheres fixed. As in our previous experiments, the robot uses delta endeffector control (x,y) and is equipped with a peg instead of a gripper [Appendix Fig. 9].\n\nInvestigating the exploration behavior, we show that MAX aims to cover the state space through multiple policies throughout training. Since it uses the disagreement between fully learned dynamics models, it gets distracted by novel states induced by the movement of all spheres (red and blue) [Appendix Fig. 10a, 10b]. In contrast, our method based on the Fisher information yields a single policy that seeks out the sphere affected by the changing physics parameters (red) and ignores the irrelevant spheres (blue) even if they lead to novel states [Appendix Fig. 10c, 10d].\n\nWith the collected data of both exploration methods, we train a forward dynamics model $s_{t+1} = f_{theta}(s_t,a_t)$. We model $f_{theta}$ as a three-layer MLP and train using the MSE loss until convergence. When evaluated on out-of-distribution trajectories, i.e., trajectories not included in the training data, we find the model to be extremely inaccurate [Appendix Fig. 11]. While the simulator extrapolates to unseen states and correctly predicts the movement of the sphere, the model hallucinates movement even when the endeffector does not interact with it at all [Appendix Fig. 11c, 11d]! These findings make ASID preferable to a purely model-based approach.\n\n[1] Pranav Shyam, Wojciech Ja\u015bkowski, Faustino Gomez. Model-Based Active Exploration. ICML, 2018\n\n[2] Deepak Pathak, Dhiraj Gandhi, Abhinav Gupta. Self-supervised exploration via disagreement. ICML, 2019"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3016/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700276099073,
                "cdate": 1700276099073,
                "tmdate": 1700276226191,
                "mdate": 1700276226191,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3MHNDggFxc",
                "forum": "jNR6s6OSBT",
                "replyto": "AFv4wpOSqs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3016/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3016/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "__\u201dWhile the experimental section is one of the strengths due to the evaluation in a realistic robotic scenario, the methods should also be evaluated on standard benchmarks for comparison, such as HalfCheetah.\u201d__\n\nStandard benchmarks like HalfCheetah don\u2019t include varying physics parameters and, therefore, do not cover the tasks we aim to solve. As shown in the follow-up experiments [Appendix Fig. 10d], our method yields directed exploration toward state transitions affected by the changing physics parameters.\n\n__\u201dFurthermore, the work of Kumar2019 does not seem to be related to exploration with mutual information as stated in this work.\u201d__\n\nThe mutual information of a distribution over physics parameters $\\Phi$ and corresponding trajectory distribution $\\mathrm{T}$ is defined as:\n$I(\\Phi|\\mathrm{T}) = H\\[\\Phi] - H[\\Phi|\\mathrm{T}]$ with $H[\\cdot]$ the entropy. With $H[\\Phi]$ a fixed quantity, we can approximate $H[\\Phi|\\mathrm{T}]$ by learning an estimator $q_{\\theta}(\\phi|\\tau)$ from data parameterized by $\\theta$ and predicting the physics parameters $\\phi$ from a trajectory $\\tau$ (c.f. [1]). Minimizing the prediction error of estimator $q$ over $\\theta$ (c.f. [2]) then corresponds to maximizing the mutual information.\n\n__\u201dThe baseline used [Kumar2019] seems very weak (in Fig 4 it does not explore at all).\u201d__\nThe reason [2] performs poorly on our tasks lies in the data used to train the estimator. [2] uses object-centric primitives to collect the data, e.g., the robot always pushing the rod. Every trajectory is, therefore, somewhat informative about the rod's center of mass.\nSince we assume no access to object-centric primitives, we instead use random exploration to collect the initial data and train the estimator. The data now also contains uninformative trajectories from which the parameters cannot be estimated, e.g., because the robot did not interact with the rod. Due to the long horizon, large state and action space, however, the estimator overfits to all trajectories, being equally certain/uncertain about informative and uninformative trajectories. Now, training a policy using the estimator log-likelihood as a reward leads to the policy learning the initial uninformative random exploration behavior used to collect the data.\n\n\n[1] Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, Sergey Levine. Diversity is All You Need: Learning Skills without a Reward Function. CoRL, 2018\n\n[2] K. Niranjan Kumar, Irfan Essa, Sehoon Ha, C. Karen Liu. Estimating Mass Distribution of Articulated Objects using Non-prehensile Manipulation. 2019"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3016/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700276371559,
                "cdate": 1700276371559,
                "tmdate": 1700276371559,
                "mdate": 1700276371559,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uWxsLm4K0G",
                "forum": "jNR6s6OSBT",
                "replyto": "AFv4wpOSqs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3016/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3016/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "__\u201dIt seems that the system relies on the assumption that a learned simulator is able to generate accurate trajectories, but that is not the case for out of distribution trajectories. I understand that exploration precisely minimizes that effect, but the probabilistic model should be able to capture the lack of information in out of distribution data. Currently, the only uncertainty comes from the noise if I understand correctly. \u201d__\n\nWe assume the simulator perfectly matches the real environment for some existing setting of unknown parameters (cf. Section 3), i.e., the true environment can be modeled by some setting of the simulator. As such, our estimation problem reduces to estimating this unknown parameter, and we do not consider out-of-distribution trajectories since, if we are able to identify the true parameter, the distribution of trajectories generated by the simulator will match the one generated by the real environment. This assumption underlies a variety of applications of sim2real transfer, which assume that the simulator (with appropriately chosen parameters or distributions of parameters) can effectively model the real world, allowing for policy transfer from sim to real [1, 2].\n\nIn cases where this does not hold, training a policy in sim is often still a valuable starting point for learning in real [3]. In such settings, learning a targeted distribution of simulator parameters to train a policy for deployment and finetuning the resulting policy in real often leads to improved performance [3, 4]. While such settings violate our assumptions, our exploration procedure would nonetheless generate informative data to learn such targeted distributions of simulator parameters.\n\n[1] Yevgen Chebotar, Ankur Handa, Viktor Makoviychuk, Miles Macklin, Jan Issac, Nathan Ratliff, Dieter Fox. Closing the sim-to-real loop: Adapting simulation randomization with real world experience. ICRA, 2019\n\n[2] Bingjie Tang, Michael A. Lin, Iretiayo Akinola, Ankur Handa, Gaurav S. Sukhatme, Fabio Ramos, Dieter Fox, Yashraj Narang. IndustReal: Transferring Contact-Rich Assembly Tasks from Simulation to Reality. arXiv preprint arXiv:2305.1711, 2023\n\n[3] Laura Smith, J. Chase Kew, Xue Bin Peng, Sehoon Ha, Jie Tan, Sergey Levine. Legged robots that keep on learning: Fine-tuning locomotion policies in the real world. ICRA, 2022\n\n[4] Allen Z. Ren, Hongkai Dai, Benjamin Burchfiel, Anirudha Majumdar. AdaptSim: Task-Driven Simulation Adaptation for Sim-to-Real Transfer. CoRL, 2023\n\n\n__\u201dHow did you use RANSAC for tracking?\u201d__\nWe use pyRANSAC-3D [1], an open-source implementation for fitting primitive shapes like spheres and cuboids to a pointcloud. The method decomposes a cuboid into 3 plane equations and estimates those by 1) sampling points, 2) fitting a model, and 3) evaluating the model under all available points (c.f. RANSAC). This process is repeated until a threshold criteria or a maximum number of iterations is met. From the plane equations, we can extract the position and orientation of the rod.\n\n[1] https://github.com/leomariga/pyRANSAC-3D/"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3016/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700276409252,
                "cdate": 1700276409252,
                "tmdate": 1700276409252,
                "mdate": 1700276409252,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zdrr3ze3fk",
            "forum": "jNR6s6OSBT",
            "replyto": "jNR6s6OSBT",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3016/Reviewer_WUcQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3016/Reviewer_WUcQ"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a system to learn RL policies in simulation which have a high chance of directly trnferring to reality. This is adhieved in a two step process, each step performing RL but with different goals. The goal of the first RL agent is to learn an exploration policy that can collect meaningful simulator calibration data from a single run in the real world. The second RL step learns to achieve the desired goal by learning in a simulator that got calibrated using the once real world run. The main contribution is the first step which uses Fisher information, widely used in system identification, as the cost function of the RL agent."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Treating simulator calibration as system ID is an interesting way to approach things.\n- The modifications of the Fisher information to make it suitable for RL training is also nice.\n- Paper is well written and easy to understand and follow.\n- Outline of questions to answer in the experiments section is a good addition."
                },
                "weaknesses": {
                    "value": "Not per se a weakness of the method but the expectation set by the beginning of the paper. There two aspects that make RL challenging to deploy on real system is safety and sample efficiency. The writing gives the impression that the paper tackles both aspects, when it really tackles the sample efficiency aspect. There is no guarantee that the exploration is safe only that it should be more informative. The proposed approach is interesting as it is and I don't think not dealing with safety aspects is an issue.\n\nAnother aspect that does not really fit with the paper is the geometric learning aspect. It does not integrate well with the rest of the paper. The proposed approach is also highly specific and not generally usable. For example, the shape reconstruction is not going to work for complicated objects and will not result in accurate physical simulation outcomes. It is interesting that something like this can be done, but way it is presented and the amount of space available to that aspect makes it hard to fully understand and makes the results sound rather underwhelming.\n\nOne aspect that it unclear from the paper is how specific the resulting exploration and task policies are. How generalizable of a policy does the system learn at the end of the day? For example, does the ball pushing policy work only for the specific environment with that breakdown of friction patches and coefficients or is the policy more general and can be used to push balls in a variety of environments? Put differently, do I need to learn a new task policy and calibrate the simulator for every minor varioation of the task description?\n\nThe work mentions that it assumes the optimal policy can be found. That is a rather big assumption for RL as finding the optimum is not guaranteed and the other aspect is that often the reward function does not truly represent what we want to optimize for. Does the proposed approach actually need to find the optimum or is a \"good enough\" policy also acceptable?\n\nOverall the experimental results are nicely presented and show good performance. Two things that could be improved are the discussion of the outcomes. There is little information about failure modes and their explanation, for example. The other part is that Section 5.3. makes sense under the hypothesis that good exploration coverage leads to good RL task performance. Is it possible to show this more directly in that section?\n\nAs side comment, maybe using \\Pi_{task} for the learned task policy, to mimic \\Pi_{exp}, could be a nice way to make it even clearer that there are multiple policies and what their goals are."
                },
                "questions": {
                    "value": "- What is the runtime of the entire system?\n- How hard is it to come up with a simulation for the first goal?\n- How precise does the simulator have to be?\n- There are simplifying assumptions made for the exploration Fisher loss, how limiting are they?\n- Equation 4 states that an initial distribution of parameters is assumed, how is this obtained?\n- The text states that the system isn't using a differentiable physics engine. If one was used, what would this mean for the method?\n- How many parameters can be estimated and what happens when parameters are coupled or jointly multi-modal?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3016/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699573454744,
            "cdate": 1699573454744,
            "tmdate": 1699636246117,
            "mdate": 1699636246117,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "epxlodDoke",
                "forum": "jNR6s6OSBT",
                "replyto": "zdrr3ze3fk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3016/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3016/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer WUcQ"
                    },
                    "comment": {
                        "value": "__\u201dOne aspect that it unclear from the paper is how specific the resulting exploration and task policies are. How generalizable of a policy does the system learn at the end of the day? For example, does the ball pushing policy work only for the specific environment with that breakdown of friction patches and coefficients or is the policy more general and can be used to push balls in a variety of environments? Put differently, do I need to learn a new task policy and calibrate the simulator for every minor varioation of the task description?\u201d__\n\nThe exploration policy generalizes to different physics parameters of the object of interest that behave similarly [Appendix Fig. 12], e.g., mass, friction, and center of mass can all be identified by pushing the object. Furthermore, the policy shows surprising generalization capabilities to sphere locations unseen during training [Appendix Fig. 12]. Furthermore, [Fig. 4] shows that our policy learns to cover the entire space, allowing it to generalize to changes in, e.g., patch locations.\n\nThe task policy generalizes to scenarios seen during training. While we assume a zero-shot transfer of our policy, the approach could be extended by domain randomization, i.e., randomizing over object positions and parameters not identified by our system to train a more robust policy.  \n\n__\u201dThe work mentions that it assumes the optimal policy can be found. That is a rather big assumption for RL as finding the optimum is not guaranteed and the other aspect is that often the reward function does not truly represent what we want to optimize for. Does the proposed approach actually need to find the optimum or is a \"good enough\" policy also acceptable?\u201d__\n\nWe do not rely on finding an optimal exploration strategy. To showcase this, we evaluate different training checkpoints of our policy training in [Appendix Fig. 10d]. The results show that even a \u201cgood enough\u201d policy that has not converged to the optimal policy is acceptable for running system identification as most of them perform a reasonable exploration strategy i.e., pushing the sphere that is affected by the parameter variations.\n\n\n__\u201dThere are simplifying assumptions made for the exploration Fisher loss, how limiting are they?\u201d__\n\nThe simplifying assumptions on the Fisher information loss are not majorly limiting under standard assumptions on the dynamics. In particular, if dynamics take the form $s_{h+1} = f(s_h,a_h)$, as is the case in the setting we consider, then it is often reasonable to assume that there is some small Gaussian perturbation of the state (this is a common assumption, for example, in much of the control theory literature, e.g. canonical settings such as LQG). Given this, our simplified expression for the Fisher Information in Section 4.1 is in fact not a simplification but is exact. In our experiments, we did not observe that using this approximation resulted in a performance loss, even when training a simulator that does not precisely exhibit Gaussian noise.\n\n\n__\u201cThe text states that the system isn't using a differentiable physics engine. If one was used, what would this mean for the method?\u201d__\n\nAccess to a differentiable physics engine allows one to compute the Fisher information directly. This would make our reward estimate less noisy and stabilize the training of the exploration policy."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3016/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700275868279,
                "cdate": 1700275868279,
                "tmdate": 1700275868279,
                "mdate": 1700275868279,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TE5bbDAJAc",
                "forum": "jNR6s6OSBT",
                "replyto": "epxlodDoke",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3016/Reviewer_WUcQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3016/Reviewer_WUcQ"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the additional information. Could you also elaborate on the questions outlined in the review?\n\nAn additional comment based on the replies to the other reviews: It might be beneficial to rethink what aspects of the paper appear in the appendix and which ones do not, as a large number of the questions the reviewers have (and thus future readers will as well) appear to be answered in the appendix. As such it would seem that there is pertinent information that is not part of the main publication."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3016/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700387953961,
                "cdate": 1700387953961,
                "tmdate": 1700387953961,
                "mdate": 1700387953961,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]