[
    {
        "title": "FILI: Syntax Repair By Learning From Own Mistakes"
    },
    {
        "review": {
            "id": "zUrLW0grid",
            "forum": "7bIpWYhCdu",
            "replyto": "7bIpWYhCdu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9014/Reviewer_ydVV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9014/Reviewer_ydVV"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes Fili, an to training syntax repair neural networks. Fili works by iteratively attempting to repair a set of broken programs, then adding all pairs of (original broken program, fixed proposed program) and (still broken proposed program, fixed proposed program) to its dataset, re-training on the new dataset, and repeating. The paper augments this by proposing a curriculum learning approach, in which the neural network is presented with (broken, fixed) program pairs with larger and larger edit distances over the course of training. The paper evaluates Fili against the prior state-of-the-art baseline, Bifi, and finds that Fili improved on Bifi by about 1%. The paper also evaluates Fili against LLMs prompted to perform syntax repair, finding that GPT-3.5 generates parseable programs more often than Bifi, but with a significantly higher edit distance (making other program changes)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The problem domain is interesting and well motivated\n* The solution itself (Fili) is clever, and leads to a simpler training approach than prior work\n* In addition to being simpler, the proposed approach also performs somewhat better than prior work (BiFi).\n* The paper is quite well written: I had no issues understanding any content or concepts\n* The evaluation is fairly extensive, comparing a range of baselines, ablations, and other related research questions"
                },
                "weaknesses": {
                    "value": "* The intuition of the connection between iterative error fixing and curriculum learning (Section 4.3) is tenuous at best\n* The evaluation shows only modest improvements compared to prior work, and is potentially outperformed by LLMs:\n  * Fili uses a beam width of 30, while Bifi uses a beam width of 10. Bifi is a somewhat more involved model though. Are the FLOPs used to train equivalent between these models? I do see that Appendix A.3 has Fili with a beam size of 10: why was this not chosen as the model evaluated in the paper (for fairness with Bifi)?\n  * The LLM experiments are zero-shot and do not include GPT-4, but still surpass the proposed approach in the accuracy (without edit distance) metric. As for accuracy with edit distance, the LLM's prompt (\"Fix all the syntax errors to make the program parsable\") does not include the statement that the program should remain otherwise unchanged or that the edit distance should be minimized."
                },
                "questions": {
                    "value": "* What is the comparison in #parameters and #FLOPs of the BiFi and Fili models in the evaluation?\n* Do LLMs still result in a large edit distance when examples are provided in the prompt, or when the prompt is modified to mention that the program should remain otherwise identical?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9014/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698352183264,
            "cdate": 1698352183264,
            "tmdate": 1699637135624,
            "mdate": 1699637135624,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YtZYhoE07T",
                "forum": "7bIpWYhCdu",
                "replyto": "zUrLW0grid",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9014/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9014/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**comparison in #parameters and #FLOPs of the BiFi and Fili models and beam widths**\n\nTo perform a fair comparison with BIFI, we adopt the exact same settings in terms of synthetic data generation and the model architecture, as employed by BIFI. Consequently, the number of parameters for the fixer (encoder-decoder) is identical for both BIFI and FILI. However, as we skip training the breaker model, we reduce the total number of parameters by half compared to BIFI.\n\nSince FILI relies on the beam for generating additional data, we use a slightly higher beam-width than BIFI during inference. This adjustment is specific to the inference process, and the impact of beam-width on FLOPs is relatively small compared to the full breaker model used by BIFI for generating additional data. Overall, FILI exhibits significantly lower FLOPs than BIFI, as it avoids training a complete 13 million parameter model.\n\n**GPT-4 Experiments:**\n\nDue to budget constraints, we could not run inference on the full test-set (15055 examples) using GPT-4. For comparison, we run a small scale experiment by randomly sampling 2000 programs from the test-set and prompting the model in zero-shot setting using the following prompt:\nThe following python program has syntax errors. Fix all the syntax errors to make the program parsable. Do not make any other changes to the program. Just fix the syntax errors.\n\n'''\n\n<Incorrect Program>:\n```\n{{incorrect}}\n```\n<Correct Program>:\n\n'''\n\nResults:\n| Model | Edit = 4 | Edit = 6 | Edit = 8 | Edit = 10 | Edit = inf |\n|-------|----------|----------|----------|-----------|------------|\n| GPT-4 | 57.3%    | 73.25%   | 80.95%   | 86.95%    | 99.75%     |\n\nThe observations from this experiment align with those using other LLMs. The model shows good performance in fixing syntax errors but introduces several changes to other parts of the program, which might be undesirable in real-world scenarios. Furthermore, both the training and inference processes of these models are orders of magnitude more computationally expensive (and even costly) compared to our model."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9014/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700547509480,
                "cdate": 1700547509480,
                "tmdate": 1700547509480,
                "mdate": 1700547509480,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "L8XdZbJiKu",
            "forum": "7bIpWYhCdu",
            "replyto": "7bIpWYhCdu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9014/Reviewer_yQj5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9014/Reviewer_yQj5"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a FILI (fix-it-learn-it) method to train the model for fixing syntax errors in programs. It improves over the existing BIFI approach, without having to train any additional models for data augmentation. As a result, in each iteration, FILI finetunes the fixer model by its own prediction."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The proposed FILI method appears to be reasonable, and it simplifies the data augmentation approach used in BIFI. Thereby, the new method is much more efficient and easier in the training, and it achieves (slightly) better results than BIFI."
                },
                "weaknesses": {
                    "value": "The delta-distance based metric adopted in the evaluation cannot fully reflect the repair performance when comparing with repair baselines. To justify that FILI outperforms large language models (LLMs), the edit accuracy subject to some delta-distance is used, with \u03b4 denoting the number of changes the fixer makes to the incorrect program.  It turns out this edit accuracy is inadequate and potentially biased, as it overlooks the semantic correctness of the program and it also ignores the possible semantic change after the repair.  \n \n \nSyntax errors are a class of relatively easier software problems to repair, and it seems that LLMs handle program syntax repair even better than FILI regarding accuracy.  It was mentioned that LLMs tend to make more changes in the program repair. However, the changes made by LLMs may depend on how the LLMs were prompted."
                },
                "questions": {
                    "value": "Is there a way to more comprehensively compare with FILI and LLMs for repairing program syntax?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9014/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698504209774,
            "cdate": 1698504209774,
            "tmdate": 1699637135503,
            "mdate": 1699637135503,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "c70u3qzj3k",
                "forum": "7bIpWYhCdu",
                "replyto": "L8XdZbJiKu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9014/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9014/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Is there a way to more comprehensively compare with FILI and LLMs for repairing program syntax?**\nWe agree with your comment that this is not the ideal evaluation criteria. An ideal evaluation criteria would be whether after performing the fix to the incorrect program, does the program satisfies the user specification. There are several challenges to using this criteria but the two major problems are:\n1. In this work, we are just fixing the syntax errors in the program. So, even if the specification is given and we fix the syntax errors, the program may still not satisfy the specification as it may have other semantic errors which are beyond the scope of this work. \n\n2. The dataset used for evaluation does not have any form of user specification or assertions that the program should satisfy, and it is also not always possible to run these programs as in most of the cases these are small snippets of code extracted from a large codebase and cannot be run in isolation.\n\nTherefore, designing an evaluation metric for syntax repair is a challenging research problem and we provide a discussion of this in Section 3 of the paper. Edit distance is commonly used in the program repair evaluation to ensure that the repair systems do not make arbitrary changes to the program. To make a fair comparison against BIFI, the previous state-of-the-art model, we used the exact same evaluation metric as BIFI based on the edit-distance. We also show in our experiments how changing this metric can lead to changes in the fixer performance. Nonetheless, for all the metrics used in the paper for evaluation, we observe that FILI performs better than BIFI."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9014/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700547505937,
                "cdate": 1700547505937,
                "tmdate": 1700547505937,
                "mdate": 1700547505937,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3isovVb6zV",
                "forum": "7bIpWYhCdu",
                "replyto": "c70u3qzj3k",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9014/Reviewer_yQj5"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9014/Reviewer_yQj5"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the reply. I am not changing my score. But I tend to think that FILI has advantages vs LLMs, which is subject to better justification."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9014/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733030838,
                "cdate": 1700733030838,
                "tmdate": 1700733030838,
                "mdate": 1700733030838,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5zFP0UzDWh",
            "forum": "7bIpWYhCdu",
            "replyto": "7bIpWYhCdu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9014/Reviewer_5pCb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9014/Reviewer_5pCb"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a new technique for repairing syntax errors. The goal is to train a fixer model, one that takes a \"bad program\" that does not parse, and outputs a \"good program\" that parses correctly.\n\nTo control for arbitrary modifications, evaluation only considers edits that are up to some fixed edit distance from the original \"bad\" program. \n\nPrevious work (BIFI) used a combination of a Fixer/Breaker models to  train the Fixer model (and simultaneously, a Breaker model used to generate incorrect examples that are similar to real-world bad programs).\n\nIn contrast, this work (FILI) only uses a single Fixer model, and uses negative samples from the Fixer's beams to augment the data used to fine-tune the fixer. FILI uses high-confidence incorrect predictions from the highly-ranked beams as negative examples to be paired with the correct program (one that parses). This is similar to the approach taken by (Cao et al. 2021).\n\nThe new approach shows a modest improvement over BIFI, but does that when only using a single Fixer model."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Thorough evaluation. Appreciated the supplemental materials and the qualitative examples. These were very helpful, especially with respect to the evaluation metric. \n\n- It is surprising and valuable to note that a FILI outperforms BIFI while only using a single Fixer model, leveraging negative samples from the Fixer's beams. Maybe this says something about the nature of the errors being fixed and how close they are to the correct program?"
                },
                "weaknesses": {
                    "value": "- The bottom-line improvement over BIFI is not significant. I do appreciate that it is hard to improve every basis point beyond 90.5% obtained by BIFI. I also understand that this is obtained without a Breaker model. \n\n- The claim that LLMs tend to make more global changes seems plausible, but you can probably control for that with prompt engineering. So the comparison with LLMs ability to fix these errors is not giving LLMs the full ability to address the problem as defined."
                },
                "questions": {
                    "value": "- You write \"A key contribution of our work is to significantly simplify the process of training a syntax fixer of (slightly) higher quality than prior work (viz., BIFI).\" - is this process a bottleneck for applying the technique? What is the cost/barrier for applying BIFI that is significantly improved by FILI? \n\n- Do you have any hypothesis on why you did not see further improvement beyond two rounds? \n\n- Can you try experiments with LLMs when providing them with instructions to only make local modifications? How would that look? \n\n- page 7: should be \"FILI cannot solve 1263, while BIFI cannot solve 1428\"?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9014/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698659458259,
            "cdate": 1698659458259,
            "tmdate": 1699637135372,
            "mdate": 1699637135372,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8piGxgV9LB",
                "forum": "7bIpWYhCdu",
                "replyto": "5zFP0UzDWh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9014/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9014/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Hypothesis for no improvement in test-set performance after 2 rounds:**\n\nWe conducted a manual inspection of examples that both BIFI and FILI cannot fix. We randomly sampled 15 incorrect programs from the test set and then checked whether these programs could be corrected or not. Through this manual inspection, we discovered that only 6 out of these 15 could be repaired by fixing the syntax errors. In the remaining 9 instances, we observed that they are not solely syntax repair problems. For instance, some of these programs are incomplete and require generating additional program statements for correction. Similarly, some of these programs involve deleting specific program statements, which neither FILI nor BIFI is trained to perform. Finally, we noted that some programs were written in other languages such as C++ and Java. This observation indicates that the upper limit for success on this test set is not 100%, and a careful analysis is needed to determine how many programs are actually fixable. Consequently, a 1% improvement over BIFI might indeed be reasonable within this context.\n\n**cost/barrier for applying BIFI that is significantly improved by FILI?**\n\nWith our ablation study, we demonstrate that curriculum learning is helpful in generating more parsable programs.  From our experiments, we observe that curriculum learning helps improve the performance of BIFI also. Training the full BIFI model with curriculum would require more careful analysis, owing to the added complexity introduced by the breaker model. Additionally, it\u2019s important to note that there exists several options to consider when deciding whether to apply curriculum learning to the breaker or the fixer, as well as how to effectively combine the data obtained from the beam and the breaker."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9014/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700547500479,
                "cdate": 1700547500479,
                "tmdate": 1700547500479,
                "mdate": 1700547500479,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YTaN2acsh8",
            "forum": "7bIpWYhCdu",
            "replyto": "7bIpWYhCdu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9014/Reviewer_9dQP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9014/Reviewer_9dQP"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes FILI (Fix-It-Learn-It), which simplifies a previous work -- BIFI (Break-It-Fix-It), an unsupervised learning approach for fixing syntax errors in programs. BIFI requires two trained models (i.e., a breaker and a fixer), while FILI requires only one fixer model. The observation is that the fixer model is not perfect and thus generates correct fixes as well as incorrect fixes. In the latter case, the fixer model itself can be viewed as a breaker model. Instead of training a separate breaker model, which can be expensive, one fixer model can be used to generate both good programs and bad programs. The evaluation on the same dataset shows that FILI slightly outperforms BIFI."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Compared to the previous work BIFI, LIFI is simple, more efficient, and achieves (slightly) better performance. \n- Extensive evaluations and comparisons with BIFI are performed on the original dataset."
                },
                "weaknesses": {
                    "value": "- The idea of using a fixer as a breaker is fairly incremental, and the improvement of performance is quite minor. Given that BIFI already achieves 95.5% accuracy over the chosen dataset, further improving it to 96.1% adds little value. \n- Curriculum learning only makes very small differences and thus seems not an essential part of the LIFI."
                },
                "questions": {
                    "value": "In Table 2, two accuracy scores (the last two columns) are reported. Can you elaborate on the key difference? Why is there a sharp drop for all approaches, especially GPT-3.5-turbo?\n\nIs there any particular reason that a breaker model is more difficult to train? Page 6 mentions that training a fixer for two rounds takes around 20 hours, while a breaker model requires 2 days. \n\nThe dataset collected by BIFI seems pretty much saturated. Have the authors considered a different dataset? (A comment rather than a question).\n\nThere is a minor typo at the bottom of page 7, \"BIFI cannot solve 1263, while BIFI cannot solve 1428\". BIFI was mentioned twice, one of which should be LIFI."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9014/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698889439686,
            "cdate": 1698889439686,
            "tmdate": 1699637135198,
            "mdate": 1699637135198,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pDHMi4QWpU",
                "forum": "7bIpWYhCdu",
                "replyto": "YTaN2acsh8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9014/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9014/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**In Table 2, two accuracy scores (the last two columns) are reported. Can you elaborate on the key difference? Why is there a sharp drop for all approaches, especially GPT-3.5-turbo?**\n\nThe first column corresponds to the parse rate without considering the edit distance between the incorrect source program and the predicted program from the models. The models can introduce an arbitrary number of changes to the programs, which is undesirable. An ideal syntax fixer should solely address syntax errors in the programs, leaving the other parts unchanged. The second column corresponds to the parse rate with the edit distance, wherein the predicted program is considered correct only if it is both parsable and below a certain edit-distance threshold. Specifically, we set the edit-distance threshold to 4 to align with BIFI's evaluation criteria.\n\nThe significant difference in language model performance between the two columns is because it is challenging to constrain the model's output using natural language prompts to only rectify syntax errors. In addition to addressing syntax errors, these models end up making numerous changes to other parts of the code. Notably, they introduce stylistic changes and alter variable declarations by combining them, among other things. A detailed explanation, along with examples, is provided in Appendix A.6 regarding the outputs from these pre-trained language models."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9014/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700547495505,
                "cdate": 1700547495505,
                "tmdate": 1700547495505,
                "mdate": 1700547495505,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]