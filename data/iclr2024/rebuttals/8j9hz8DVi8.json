[
    {
        "title": "Combining Axes Preconditioners through Kronecker Approximation for Deep Learning"
    },
    {
        "review": {
            "id": "rKtxmAeiHr",
            "forum": "8j9hz8DVi8",
            "replyto": "8j9hz8DVi8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2886/Reviewer_Vz7y"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2886/Reviewer_Vz7y"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes an optimization algorithm called CASPR that yields tighter convergence guarantees for stochastic optimization. It is inspired from the commonly known Shampoo optimizer (which is a Kronecker product based optimizer) and uses the Kronecker sum approximation to obtain better convergence rate with similar complexity as in the Shampoo case. The technique is well motivated and provides theoretical/experimental proofs/explanations for all the claims in the paper (convergence guarantees, basic properties of kronecker product/sum and Loewner order)."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. clearly explains the motivation of a Kronecker sum based precondition, the differences in the updates of CASPR and Shampoo and comparison with diagonal Adagrad statistics in Figure 1 (all in section 3)\n2. provides convergence guarantees\n3. experiments are performed on transformers and graph neural network tasks (commonly known as difficult tasks) against Shampoo and AdamW, the main competitor optimizers in the literature\n4. comprehensive appendix"
                },
                "weaknesses": {
                    "value": "1. the paper does not have experiments on computer vision tasks, such as ResNets, ViTs or other important architectures used for benchmarking in the literature"
                },
                "questions": {
                    "value": "In the evaluation you introduce a momentum for $L$ and $R$ matrices for a fair comparison against the other optimizers, can you please say what is the impact of momentum terms in this case? Have you experimented without momentum?\n\nI would like to point out some typos in the manuscript:\n- at page 4, Lemma 3.1, the Frobenius norm should be squared (the 2nd power is missing from the norm)\n- at page 5, when you rewrite the preconditioner $X_t^{caspr}(1)$ in terms of $X_t^L$ and $X_t^R$, I belive the matrices $L$ and $R$ are missing a tilde, based on the definitions of the $X_t^{L/R}$ terms 4 lines above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2886/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2886/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2886/Reviewer_Vz7y"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2886/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698359484729,
            "cdate": 1698359484729,
            "tmdate": 1699636232138,
            "mdate": 1699636232138,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "weE4bcPJRw",
                "forum": "8j9hz8DVi8",
                "replyto": "rKtxmAeiHr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2886/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2886/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your valuable review.\n> In the evaluation you introduce a momentum for L and R matrices for a fair comparison against the other optimizers, can you please say what is the impact of momentum terms in this case? Have you experimented without momentum?\n\nWe conducted an additional experiment without momentum. We used the optimal hyperparameter for CASPR and removed momentum (i.e, equivalent to beta2 tends to 1) for L and R for CASPR, and noticed that the performance is poor. This could be because in practice momentum is helpful as the curvature could change rapidly. We will conduct a full experiment with several hyperparameters in the next revision. \n\n> at page 4, Lemma 3.1, the Frobenius norm should be\u2026\n\nThank you for finding this error, we corrected it.\n\n> at page 5, when you rewrite the preconditioner\u2026\n\nYes, this $ \\frac{(L^{-1/2}\\otimes I_n + I_m \\otimes R^{-1/2})/2 + L_t^{-1/4} \\otimes R_t^{-1/4}}{2}$ should be $\\frac{(\\tilde{L}^{-1/2}\\otimes I_n + I_m \\otimes \\tilde{R}^{-1/2})/2 + \\tilde{L}_t^{-1/4} \\otimes \\tilde{R}_t^{-1/4}}{2}$. We edited this. \n\n\n\n> the paper does not have experiments on computer vision tasks, such as ResNets, ViTs or other important architectures used for benchmarking in the literature\n\nWe will try to conduct more experiments on vision tasks in the next revision."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2886/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700452066742,
                "cdate": 1700452066742,
                "tmdate": 1700452066742,
                "mdate": 1700452066742,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rsB214doHm",
                "forum": "8j9hz8DVi8",
                "replyto": "rKtxmAeiHr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2886/Reviewer_Vz7y"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2886/Reviewer_Vz7y"
                ],
                "content": {
                    "comment": {
                        "value": "It would be great if you could include some large scale Computer Vision experiments, I believe it would add a lot of value to the paper.\n\nRegarding your observation about momentum, I guess $\\beta_2 \\rightarrow 1$ would result in not incorporating any second order information into the preconditioners $L$ and $R$. Can you please clarify that, I guess you refer to preconditioner updates as they appear in algorithm 1, line 4 (without any $\\beta_2$ parameter), right?"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2886/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700491101618,
                "cdate": 1700491101618,
                "tmdate": 1700491191976,
                "mdate": 1700491191976,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gWz7PwVeAS",
                "forum": "8j9hz8DVi8",
                "replyto": "rKtxmAeiHr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2886/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2886/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "By $\\beta_2 \\rightarrow 1$, we mean to update $L_t$ and $R_t$ as $L_t = L_{t-1} + G_tG_t^{\\top}$ and $R_t = R_{t-1}+ G_t^{\\top}G_t$, so the entire history of gradient second order information is accumulated  in $L_t$ and $R_t$ without exponential moving average. Please let us know if we understood your suggestion correctly."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2886/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700508835987,
                "cdate": 1700508835987,
                "tmdate": 1700510020216,
                "mdate": 1700510020216,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mG6iAmwvCt",
            "forum": "8j9hz8DVi8",
            "replyto": "8j9hz8DVi8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2886/Reviewer_gGzL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2886/Reviewer_gGzL"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes an alternative to the recently successful Shampoo optimizer that is derived by minimizing the error of the approximation. They show that their method obtains a faster theoretical convergence guarantee in an online learning setting than the original Shampoo optimizer, and also in practice performs better."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- clear development of the method and comparison to Shampoo\n- theoretical development that exists of Shampoo extended to caspr \n- experiments suggest that caspr improves over shampoo and potentially also other baselines (only compared to Adam(W))"
                },
                "weaknesses": {
                    "value": "- abstract and introduction could be written more clearly, it is only mentioned what is achieved in the abstract and introduction but not how this will be done\n- relatively limited benchmarking to other methods andmissing error bars on the experiments, it seems like a single training run is reported in the figures. This is the major potential weakness I see in the paper and I am worried that the results are not reproducible. See my questions below (Are the figures created from multiple runs averaged? What is the target for tuning hyperparameters if the plots show validation performance?). \n- seems unclear how to extend this method to layers that are not fully-connected\n\n### Minor\n- first page sentence ... \"..to be at one end of this sequence\" confusing"
                },
                "questions": {
                    "value": "- On page 3 it says the second-moments are computed per row of the gradient. Is it correct to say that this means it is a neuron-level conditioner? This is something that seems to exist in the context. See for example, Fig 1 \"unit-wise\" preconditioning in https://arxiv.org/pdf/2305.04684.pdf.\n- Are the 300 hyperparameters optimized for each method or only for caspr? What is the target for this hyperparameter tuning? Validation accuracy is the performance reported so is the grid search optimizing that value?\n- Why are there no error bars included? Could the experiments be run on at least 3 seeds? Without a single rerun, the results could be random."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2886/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2886/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2886/Reviewer_gGzL"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2886/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699215111281,
            "cdate": 1699215111281,
            "tmdate": 1699636232065,
            "mdate": 1699636232065,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nsUZoeN2XQ",
                "forum": "8j9hz8DVi8",
                "replyto": "mG6iAmwvCt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2886/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2886/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your valuable review. \n\n> abstract and introduction could be written more clearly, it is only mentioned what is achieved in the abstract and introduction but not how this will be done\n\nThank you for pointing this out, we will add more details regarding our method in the introduction of the next revision.\n\n> On page 3 it says the second-moments are computed per row of the gradient. Is it correct to say that this means it is a neuron-level conditioner? This is something that seems to exist in the context. See for example, Fig 1 \"unit-wise\" preconditioning in https://arxiv.org/pdf/2305.04684.pdf.\n\nYes, preconditioning each row individually can be imagined as a neuron-level preconditioner. We will cite the mentioned paper in the final draft. Specifically, this originates from a block-diagonal approximation $S_{R} = \\{ blockdiag(R_1,R_2\\ldots,R_m): R_i\\succeq 0\\in R^{n\\times n},\\forall i\\in[m]\\}$ of full-matrix statistic. Since maintaining this approximation and computing the corresponding preconditioner is infeasible in terms of memory and compute, we additionally develop a statistic/preconditioner following the constraint $S_{I\\otimes R} = \\{ blockdiag(R,R\\ldots,R): R \\succeq 0\\in R^{n\\times n}\\}$ in Lemma 3.1, where the additional constraint is that all the blocks $R_1=R_2=\\cdots=R_m=R$ are the same. Our work can be seen as extending such neuron level preconditioners by combining them through kronecker approximations. In practice this does give us a big gain as seen in our experimental results.\n\n\n> Are the 300 hyperparameters optimized for each method or only for caspr? What is the target for this hyperparameter tuning? Validation accuracy is the performance reported so is the grid search optimizing that value?\n\nFor ogbg-molpcba, we use 300 hyperparameters for Shampoo and AdamW and 200 for CASPR randomly sampled from the search space in Table 1 (Appendix A.4). The hyperparameter configuration which gives the best generalization performance is picked and reported individually for each method.\n\n> Why are there no error bars included? Could the experiments be run on at least 3 seeds? Without a single rerun, the results could be random.\n\nWe mention here the standard deviations averaged across 3 seeds for the Transformer on universal dependencies and OGBG-molpcba benchmarks:\n\n\n|Optimizer|Transformer on Universal dependencies - Accuracy|OGBG-molpcba- Test mAP|\n|---------|------------------------------------------------|----------------------|\n|Shampoo  |69.45 +/- 0.26%                                 |0.2843 +/- 0.0028     |\n|CASPR    |69.76 +/- 0.14%                                 |0.2873 +/- 0.0017     |\n|AdamW    |68.28 +/- 0.39%                                 |0.2701 +/- 0.0011     |\n\nWe will try to add similar numbers for our language modeling benchmarks in the next revision, as they are more time consuming.\n\n\n> seems unclear how to extend this method to layers that are not fully-connected\n\nOur method (as well as Shampoo) can be applied to any layer. For example, in our experiments with transformers we handle 3D parameters, such as attention layers by flattening the parameter (in the form of higher order tensors) into a 2D tensor by merging the dimensions.  The attention layers have 3D parameter (m,n,k) which we precondition by merging the second and third dimensions to a 2D parameter (m, n*k) and then applying CASPR. For higher dimensional parameters, one can apply a similar transformation by merging more dimensions.\n\n> first page sentence ... \"..to be at one end of this sequence\" confusing\n\nThank you for pointing this out. Here, we added a forward reference to Lemma 3.2 to address the confusion."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2886/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700509947611,
                "cdate": 1700509947611,
                "tmdate": 1700509947611,
                "mdate": 1700509947611,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7OtTnasfb9",
            "forum": "8j9hz8DVi8",
            "replyto": "8j9hz8DVi8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2886/Reviewer_a8ib"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2886/Reviewer_a8ib"
            ],
            "content": {
                "summary": {
                    "value": "In this work, the authors propose a new second-order method inspired by the Shampoo optimizer. The authors show that the proposed method is theoretically grounded and empirically tested.  The authors demonstrate the performance of the proposed method on several large models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "This is a solid work. Strong theoretical results and empirical results are provided. I do not check the proofs in the appendices."
                },
                "weaknesses": {
                    "value": "The authors should discuss the time and space complexity of the coupled Newton algorithm used for computing the matrix inverse p-root. \nMoreover, numerically stability of the proposed method should be discussed. \nFor example, how to choose the damping term $\\epsilon$? Note that the coupled Newton algorithm could be sensitive to the choice of the damping term.   \nThe proposed method may not work well in a low numerical precision setting such as float-32 or bfloat-16. The authors should explicitly discuss this point."
                },
                "questions": {
                    "value": "See the weakness section"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2886/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699217591323,
            "cdate": 1699217591323,
            "tmdate": 1699636232002,
            "mdate": 1699636232002,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ux5Xj2MSgq",
                "forum": "8j9hz8DVi8",
                "replyto": "7OtTnasfb9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2886/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2886/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your valuable review. We will add the additional details about coupled Newton algorithm for matrix inverse pth root (Appendix D in [1]) in our final draft. Here we clarify a few questions:\n\n> how to choose the damping term ?\n\nFor our experiments, the inverse pth root operations were performed using single-precision floating-point format (float32). We determined the damping term, $\\epsilon$, by scaling the largest eigenvalue of $\\lambda_{max}(L)$ with $\\epsilon'$ as outlined for the Shampoo algorithm in line 16 of Algorithm 1 on page 20 in [1]. This scaling ensures that the modified matrix $\\tilde{L} = L + \\epsilon' \\lambda_{max}(L) I_m$, maintains an $\\ell_2$-condition number not exceeding $1/\\epsilon\u2019$. We set $\\epsilon'$ to $10^{-6}$ across all our experiments, which caps the condition number of $\\tilde{L}$ at $10^{6}$, except in the case of the 14M parameter language model showcased in Figure 4a. In that instance, we experimented with $\\epsilon'$ values in the set {$10^{-6}, 10^{-10}$}, as detailed in Table 3.\n\n\n> The proposed method may not work well in a low numerical precision setting such as float-32 or bfloat-16. The authors should explicitly discuss this point.\n\nThank you for catching this limitation. The inverse pth root operations of our experiments were conducted in float32. Nevertheless, conducting inverse pth root operations can be limiting in float32 as accurate inverses can only be obtained for matrices with condition number less than $\\sim 10^6$. We leave conducting accurate inverse pth root operations tolerant to low precision as a future work. \n\n[1] Rohan Anil, Vineet Gupta, Tomer Koren, Kevin Regan, and Yoram Singer. Scalable second order optimization for deep learning. arXiv preprint arXiv:2002.09018, 2020."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2886/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700002527495,
                "cdate": 1700002527495,
                "tmdate": 1700002527495,
                "mdate": 1700002527495,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]