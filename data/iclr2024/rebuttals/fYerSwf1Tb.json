[
    {
        "title": "HawkesVAE: Sequential Patient Event Synthesis for Clinical Trials"
    },
    {
        "review": {
            "id": "kxHwrbzUFm",
            "forum": "fYerSwf1Tb",
            "replyto": "fYerSwf1Tb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2751/Reviewer_wpgs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2751/Reviewer_wpgs"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a VAE for modeling clinical event sequences (including both event interarrival times as well as event types) as Hawkes processes for synthetic data generation. Transformer architectures are used and the decoder samples next events based on the logits rather than greedily. \n\nThe approach is applied to clinical event sequences and compared to several baselines on downstream tasks, whether a classifier can distinguish between real and synthetic data and distance to the closest record. The results are generally favorable for the proposed method, though not always statistically significant. \n\nAblations are included."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1) The paper is generally well written. \n\n2) The experiments are appropriately chosen and thorough.\n\n3) Multiple datasets are included with details.\n\n4) The approach appears sound."
                },
                "weaknesses": {
                    "value": "1) The primary weakness is limited novelty. This is not the first paper to propose combining VAEs with Hawkes, e.g. \n- Sun et al. (2021) \"Towards a predictive analysis on disease progression: a variational Hawkes process model\" IEEE Journal of Biomedical and Health Informatics\n- Lin et al. (2021) \"Disentangled deep multivariate hawkes process for learning event sequences\" ICDM\n- Pan et al. (2020) \"A variational point process model for social event sequences\" AAAI\nHence the primary point of novelty appears to be the scheme for sampling next events and the fact that using the logits is better than greedy sampling, which is not surprising.\n\n2) Related to this is then the fact that appropriate empirical baselines may be missing since previous work combining VAEs with Hawkes processes is not included. Aside from the above papers, there are also other more closely related approaches that should be included such as Zuo et al. (2020) \"Transformer Hawkes Process\" ICML and Miscouridou et al. (2022) Cox-Hawkes: doubly stochastic spatiotemporal\nPoisson processes TMLR.\n\n3) Even with the limited baselines, the empirical performance is not compelling (this might be less of a concern if the contribution was more novel). While the proposed method generally has the best performance, the confidence intervals often overlap with DDPM (both on downstream performance and classification of real vs synthetic instances) - this is somewhat confusing in the presentation of the results as \"best performance\" is bolded for multiple methods, but competing methods with overlapping confidence intervals are not bolded.\n\n4) While the method is well described intuitively, some specifics are not given in clear formal terms, e.g. greedy vs. logit sampling could be provided formally/mathematically, the equations for generating sequences in the encoder/decoder and combining them in the encoder should be provided."
                },
                "questions": {
                    "value": "1) What do the authors consider to be the primary point of novelty over existing approaches which combine VAEs with Hawkes processes or related approaches like Transformer Hawkes?\n\n2) Have the authors compared their approach to Transformer Hawkes Process or Cox-Hawkes?\n\n3) Does the proposed model require the same sequence length for all patients?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2751/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2751/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2751/Reviewer_wpgs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2751/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698507256641,
            "cdate": 1698507256641,
            "tmdate": 1699636217914,
            "mdate": 1699636217914,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vQENQodOu0",
                "forum": "fYerSwf1Tb",
                "replyto": "kxHwrbzUFm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2751/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2751/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer wpgs"
                    },
                    "comment": {
                        "value": "Thank you for taking the time to review our paper and your valuable feedback. \nHere are our responses to each point in order:\n\n- The primary weakness is limited novelty. This is not the first paper to propose combining VAEs with Hawkes. \\ What do the authors consider to be the primary point of novelty over existing approaches which combine VAEs with Hawkes processes or related approaches like Transformer Hawkes \\ Cox Hawkes?\n\nThank you so much for pointing us to these papers; we have added them to our literature review. We believe that although our method applies VAE and Hawkes Processes together, it has merit as it outperforms synthetic tabular generation models as well as LSTMs and PAR. We also believe that the detailed level of control (allowing the user to specify event types and times) is highly valuable in settings such as clinical trial patient generation [2,3], and are not explicitly supported by other models. Finally, we have performed a literature review on the papers cited, and found that none of them, including Transformer Hawkes Models and Cox-Hawkes models, test on whole-sequence time and event generation like in our evaluations. We believe that a source of our novelty also lies in our application, where sequential event data generation is an underexplored, yet exciting field. We have not experimented with Cox-Hawkes, but this looks like an exciting future direction.\n\n- Even with the limited baselines, the empirical performance is not compelling\u2026\n\nA quick comment regarding the performance of HawkesVAE. You are absolutely correct on the bolding of the ROC metric, and we have corrected this, as it now indicates if the original data mean ROC is within 1 standard deviation. Post initial submission, we actually realized that our downstream model was underfitting, as we discovered that our sanity check results (from training the model on original data) did not match previous existing work [1]. We have now reran the results with a more robust downstream classifier, and seen a boost in all performance for all models in general. HawkesVAE (Multivariate) outperforms the and the next best model (in 4/7 datasets and is within 1 standard deviation with the rest of the datasets). We have updated Tables 2 and 5 accordingly.\n\n- While the method is well described intuitively, some specifics are not given in clear formal terms..\n\nWe agree with the reviewer, and have taken steps to clarify the variables in Section 4.1. Specifically, we clarify that the encoder model $E_(\\mathcal{H}\\_i) \\rightarrow \\hat{\\mu}, \\hat{\\sigma}$ takes in the original event types and times, and predicts the mean and standard deviation to sample $z \\sim Normal(\\hat{\\mu}, \\hat{\\sigma})$. This is used for $q_\\phi(\\cdot | S_z)$ in the loss function and follows the previous Transformer Hawkes Process implementation of a transformer encoder, with alternating sine and cosine waves to denote temporal information. \n\nHowever, the decoder model is more complicated. Hawkes Process is usually evaluated via one prediction step at a time (i.e., the input is the ground truth time-step and event type, and the task is to predict the next time step and event type). For our purposes, we need to adapt this into an autoregressive decoding scheme similar to the language modeling task.\n\nAt training time, the input to the decoder $D(z, \\mathcal{H}\\_i) \\rightarrow (\\hat{t}\\_{i+1}, \\hat{k}_{i+1}, \\lambda)$ is hidden vector $z$ and a sequence of ground truth event types and times. It is tasked with predicting the next event type $\\hat{k}$, next event time $\\hat{t}$, and the $\\lambda$s necessary to compute $P\\_\\theta(\\cdot|S_z)$. \n\nAt inference time, the input to decoder is only $z$, and we auto-regressively decode the predicted event types and times. To predict next time and event tuple $(\\hat{t}\\_i, \\hat{k}\\_i)$, the input is the previously predicted times and events $\\{(\\hat{t}\\_1, \\hat{k}\\_1), \\dots, (\\hat{t}\\_{i-1}, \\hat{k}_{i-1}))\\}$). (each predicted time and event is repeatedly appended to the input). \n\nWe have clarified this in section 4.1 as well as Appendix A.3.\n\n- Does the proposed model require the same sequence length for all patients?\n\nOur model supports patients of any sequence length. We have clarified this point in the manuscript (Section 5 Datasets)\nWe hope that these revisions help address some of your concerns regarding the paper, and look forward to further discussion.\n\n[1] Wang, Z., Gao, C., Xiao, C., & Sun, J. (2023). AnyPredict: Foundation Model for Tabular Prediction. arXiv preprint arXiv:2305.12081.\n\n[2] Beigi, M., Shafquat, A., Mezey, J., & Aptekar, J. (2022). Simulants: Synthetic Clinical Trial Data via Subject-Level Privacy-Preserving Synthesis. In AMIA Annual Symposium Proceedings (Vol. 2022, p. 231). American Medical Informatics Association.\n\n[3] https://www.medidata.com/wp-content/uploads/2023/10/FACT-SHEET-Simulants.pdf"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2751/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700435862101,
                "cdate": 1700435862101,
                "tmdate": 1700435862101,
                "mdate": 1700435862101,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "TjSuvtTK1k",
            "forum": "fYerSwf1Tb",
            "replyto": "fYerSwf1Tb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2751/Reviewer_jzNy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2751/Reviewer_jzNy"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a HawkesVAE model for simulating time series (event time and type) data. HawkesVAE combines the Neural Hawkes Process (NHP) with the data encoding and decoding capabilities of Variational Autoencoders (VAEs). Experimental results show that the generated samples closely resemble the original data compared to baselines."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "Learning to simulate time series (event time and type) data is an important but under-explored research topic."
                },
                "weaknesses": {
                    "value": "**Novelty**\n- The paper seems like a straightforward application of NHP and VAEs to the time-series data\n\n**Clarity**\n- The writing could improved to focus on the paper's key contributions. For instance, given that a previous derivation of the NHP in Section 3.2 has been proposed unless there are new insights, the NHP could be summarized. \n- The description for HawkesVAE is lacking in clarity:\n1) How is $p_\\theta(|zS_z)$ parametized?  \n2)  How is $q_{\\phi}(z|S_z)$  parametized?\n3)  How is $z$ used in the log-likelihood $p_{\\theta}(S_z|z)$?\n- Figure 1:  It seems the model outputs event times and event lengths. Does the model predict event types? Why does HawkesVAE require event lengths and event types at inference time? Are these provided to baselines as well?\n- Tables 2, 3, 5: HawkesVAE results are bolded even in instances where baselines are better than HawkesVAE which is misleading\n-  Given that HawkesVAE requires access to real-world data to learn a generative model, the benefits of using the synthetic data over real-world data are not motivated\n- How are the encoder and decoder functions specified?"
                },
                "questions": {
                    "value": "See above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2751/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698728040669,
            "cdate": 1698728040669,
            "tmdate": 1699636217835,
            "mdate": 1699636217835,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RdogDUf92y",
                "forum": "fYerSwf1Tb",
                "replyto": "TjSuvtTK1k",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2751/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2751/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer jzNy (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for taking the time to review our paper and your valuable feedback. \n\nHere are our responses to each point in order:\n\n- The paper seems like a straightforward application of NHP and VAEs to the time-series data\n\nWe believe that although our method applies two methodologies together, it has merit as it outperforms synthetic tabular generation models as well as LSTMs and PAR. We also believe that the detailed level of control (allowing the user to specify event types and times) is highly valuable in settings such as clinical trial patient generation, and are not explicitly supported by other models. \n\n- The writing could improved to focus on the paper's key contributions.\n\nThis is a good point, and we have shortened the relevant work to a single page to reflect this change. We have moved the information regarding traditional Hawkes Processes and VAEs to the appendix as well.\n\n- The description for HawkesVAE is lacking in clarity / How are the encoder and decoder functions specified?\n\nWe agree with the reviewer, and have taken steps to clarify the variables in Section 4.1. Specifically, we clarify that the encoder model $E_(\\mathcal{H}\\_i) \\rightarrow \\hat{\\mu}, \\hat{\\sigma}$ takes in the original event types and times, and predicts the mean and standard deviation to sample $z \\sim Normal(\\hat{\\mu}, \\hat{\\sigma})$. This is used for $q_\\phi(\\cdot | S_z)$ in the loss function and follows the previous Transformer Hawkes Process implementation of a transformer encoder, with alternating sine and cosine waves to denote temporal information. \n\nHowever, the decoder model is more complicated. Hawkes Process is usually evaluated via one prediction step at a time (i.e., the input is the ground truth time-step and event type, and the task is to predict the next time step and event type). For our purposes, we need to adapt this into an autoregressive decoding scheme similar to the language modeling task.\n\nAt \\textit{training time}, the input to the decoder $D(z, \\mathcal{H}\\_i) \\rightarrow (\\hat{t}\\_{i+1}, \\hat{k}_{i+1}, \\lambda)$ is hidden vector $z$ and a sequence of ground truth event types and times. It is tasked with predicting the next event type $\\hat{k}$, next event time $\\hat{t}$, and the $\\lambda$s necessary to compute $P\\_\\theta(\\cdot|S_z)$. \nAt \\textit{inference time}, the input to decoder is only $z$, and we auto-regressively decode the predicted event types and times. To predict next time and event tuple $(\\hat{t}\\_i, \\hat{k}\\_i)$, the input is the previously predicted times and events $\\{(\\hat{t}\\_1, \\hat{k}\\_1), \\dots, (\\hat{t}\\_{i-1}, \\hat{k}_{i-1}))\\}$). (each predicted time and event is repeatedly appended to the input). \n\n- Figure 1: It seems the model outputs event times and event lengths. Does the model predict event types? Why does HawkesVAE require event lengths and event types at inference time? Are these provided to baselines as well?\n\nThe model autoregressively predicts event types and times, and event lengths and event type information are optional inputs. The model without event type information is denoted as HawkesVAE (Multivariate), and the model with event type information is denoted as HawkesVAE (Events Known). The comparisons are indeed fair, as baseline models vs HawkesVAE (Multivariate) is given the same amount of information. We have also taken steps to clarify Figures 1 and 3.\n\n- Tables 2, 3, 5: HawkesVAE results are bolded even in instances where baselines are better than HawkesVAE which is misleading. \n\nA quick comment regarding the performance of HawkesVAE. You are absolutely correct on the bolding of the ROC metric, and we have corrected this, as it now indicates if the original data mean ROC is within 1 standard deviation. Post initial submission, we actually realized that our downstream model was underfitting, as we discovered that our sanity check results (from training the model on original data) did not match previous existing work [1]. We have now reran the results with a more robust downstream classifier, and seen a boost in all performance for all models in general. HawkesVAE (Multivariate) outperforms the and the next best model (in 4/7 datasets and is within 1 standard deviation with the rest of the datasets). We have updated Tables 2 and 5 accordingly.\n\n- Given that HawkesVAE requires access to real-world data to learn a generative model, the benefits of using the synthetic data over real-world data are not motivated\n\nWe understand the reviewer\u2019s concern regarding this. In fact, synthetic trial generation is a recently proposed field of research, and has already become an industry product. Relying on real world data to train such models is indeed an issue inherent to all synthetic data generative models, but we believe that the use cases (data insights without revealing patient personal information) are highly practical for real world applications."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2751/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700435542439,
                "cdate": 1700435542439,
                "tmdate": 1700435542439,
                "mdate": 1700435542439,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FSMoFSk7f1",
            "forum": "fYerSwf1Tb",
            "replyto": "fYerSwf1Tb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2751/Reviewer_6QpK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2751/Reviewer_6QpK"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a method called HawkesVAE to combine Hawkes Process (HP) and Variational Autoencoder (VAE) for events prediction and duration estimation. \n\nThe proposed method is applied on 7 oncology trial datasets and compared with thee existing methods LSTM VAE, PARSynthesizer, and DDPM. By comparing the ROCAUC (binary classification of death), HawkesVAE tend to outperform the other three methods when number of subject is small and tend to report highter ROCAUC when original data with events/Subj rate is lowers. It\u2019s also compared with the other three methods under the ML inference Score and the Distance to Closest Record criterion."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Generating synthetic competing risk events based on limited clinic trial data is challenging yet meaningful. This paper combined Hawkes and VAE to address sequencing events generation which is close to original data while not repeat of original data."
                },
                "weaknesses": {
                    "value": "It\u2019s not clear to me what types of events are available in those datasets, how general adverse events, serious adverse events, and death are handled and once generated would that generated event (by serious/severity) lead to more/less frequent or cancellation of afterward event generation."
                },
                "questions": {
                    "value": "1.P3, What are the other events mentioned in \u201cdeath event reduces the probability of most other events\u201d?\n2.P3, can the row above \\lambda(t|\\mathcal(H)) read as \u201cis defined as\u201d instead of \u201ccalculated as\u201d? (Maybe just me, I was trying to derive this row based on previous definitions.) \n3.P4, Figure 1. I didn\u2019t quite follow how the [0,1,1,0,0] (5 events) event indicator was paired with your two event time vectors [1,3,5,6] and [1,2,3] (total 7 time points), and how matched with the event length [4,3] (two durations for the two \u201c1\u201d right)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2751/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2751/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2751/Reviewer_6QpK"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2751/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698827673051,
            "cdate": 1698827673051,
            "tmdate": 1700282082759,
            "mdate": 1700282082759,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nTjj6CCvau",
                "forum": "fYerSwf1Tb",
                "replyto": "FSMoFSk7f1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2751/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2751/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 6QpK"
                    },
                    "comment": {
                        "value": "Thank you for taking the time to review our paper and your valuable feedback. \n\nA quick comment regarding the performance of HawkesVAE. We have slightly changed our bolding, which now indicates if the original data mean ROC is within 1 standard deviation. Post initial submission, we actually realized that our downstream model was underfitting, as we discovered that our sanity check results (from training the model on original data) did not match previous existing work [4]. We have now reran the results with a more robust downstream classifier, and seen a boost in all performance for all models in general. HawkesVAE (Multivariate) outperforms the and the next best model (in 4/7 datasets and is within 1 standard deviation with the rest of the datasets). We have updated Tables 2 and 5 accordingly.\n\nHere are our responses to each point in order:\n\n- It\u2019s not clear to me what types of events are available in those datasets, how general adverse events, serious adverse events, and death are handled and once generated would that generated event (by serious/severity) lead to more/less frequent or cancellation of afterward event generation.\n\nEach dataset contains events and the times at which they occur, e.g. medications, procedures, as well as some adverse events like vomiting, etc. We use these datasets to predict if the subject experiences the death event, which is an external label.\n\n- 1.P3, What are the other events mentioned in \u201cdeath event reduces the probability of most other events\u201d? \n\nWe have changed this to be more representative of our datasets and evaluation task, as death is not actually an event that is in the training set. Rather, it is an external label of patient status, and the events consist of events and the times at which they occur, e.g. medications, procedures, as well as some adverse events like vomiting, etc. We will use these datasets to predict if the subject experiences the death event, which is an external label. We have also changed the example to \u201ce.g., medication reduces the probability of adverse events\u201d.\n\n- 2.P3, can the row above \\lambda(t|\\mathcal(H)) read as \u201cis defined as\u201d instead of \u201ccalculated as\u201d? (Maybe just me, I was trying to derive this row based on previous definitions.) \n\nWe have revised the text and replaced \u201ccalculated as\u201d with \u201cis defined as\u201d.\n\n- 3.P4, Figure 1. I didn\u2019t quite follow how the [0,1,1,0,0] (5 events) event indicator was paired with your two event time vectors [1,3,5,6] and [1,2,3] (total 7 time points), and how matched with the event length [4,3] (two durations for the two \u201c1\u201d right).\n\nWe have clarified the figure so that the event lengths are labeled. Originally, I meant to demonstrate that within 5 events, events 2 and 3 occur in the subject with times [1,3,5,6] (length 4) and [1,2,3] (length 3). I realize that this is confusing and have clarified the figure with labels of a more intuitively understandable event sequence (\u201cVomiting\u201d and \u201cFever\u201d). Please see Figures 1 and 3."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2751/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700435451593,
                "cdate": 1700435451593,
                "tmdate": 1700435451593,
                "mdate": 1700435451593,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "RkLCa4rDGW",
            "forum": "fYerSwf1Tb",
            "replyto": "fYerSwf1Tb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2751/Reviewer_S6gY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2751/Reviewer_S6gY"
            ],
            "content": {
                "summary": {
                    "value": "Authors propose HawkesVAE, a method to generate synthetic event-type data. The proposed approach combines Hawkes Process Transformers with a Variational Autoencoder. The method is empirically validated on different clinical trial datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper addresses the challenging and important problem of generating synthetic medical time-series.\n- The proposed method is interesting and sensible, combining insights from relevant methods.\n- Three metrics are used to measure quality of synthetic data: real test performance after training on synthetic data, and 2 measures of similarity to real data. Although a comment on what is done in related works would be helpful!"
                },
                "weaknesses": {
                    "value": "Missing details and lack of clarity on the experimental investigation make it challenging to measure the added value of the proposed method (see questions in the next section).\n\nPresentation and readability need significant improvement:\n- Citations within parentheses\n- \u201cEvent-type + time-gap\u201d please replace \u201c+\u201d by \u201cand\u201d\n- Contribution 3 not a sentence\n- P.3 h(t)s\n- P.4 \u201cNote that the gradient of is simply\u201d\n- P.4 Use of capital $\\Delta$ for differentiation is unconventional. Do authors mean the gradient operator $\\nabla$, or the $\\delta$ partial diff operator? I also believe authors mean that $\\lambda(u)$ is differentiable wrt $u$ \u2013 not that the gradient of $\\lambda$ is differentiable itself. \n- P4 mulidimensional\n- P4 \u201cwe get \u03f5 \u223c N (0,I)\u201d \u2013 what does this refer to?\n- P5 missing punctuation after \u201conly parameterized by \u03b8\u201d\n- P8 \u201cand normalized timestamp, respectively.\u201d\n- P4 missing $\\phi$ in KL-divergence of ELBO\n- P6 Citation for CTGAN (why is it not included as a baseline?)\n- Table 2: Overlap between HawkesVAE results and models trained on original data (should also be bolded).\n- Too many significant figures on all tables."
                },
                "questions": {
                    "value": "- Table 3: to validate this metric, what is the performance of applying the real/synthetic classifier to real data? Is it indeed 0.5? What synthetic data was this discriminator trained on?\n\n- Is the downstream classification task just binary classification of death? What does \u201cthe LSTM and the HawkesVAE models estimate their own sequence stopping length\u201d mean in the context of binary classification, do you also jointly regress death time? Does this mean the death event is removed from the end of training trajectories (real or synthetic)? What happens if the trajectory does not undergo any event?\n\n- Why does training on synthetic data from Hawkes VAE occasionally beat training on the original data?\n\n- I am not sure I understand why and how Hawkes-VAE is used for event forecasting, if it is only designed to generate synthetic data. How is this implemented in practice? Do authors sample from the posterior generated by the history up to the prediction point and then generate the rest of the trajectory? Why does giving more info (\u201cEvents Known\u201d) result in a less accurate order?\n\n- In what context would one expect to know/constrain the event types happening within a sequence?\n\n- What is the error reported (CI, stderr?)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2751/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698832338775,
            "cdate": 1698832338775,
            "tmdate": 1699636217686,
            "mdate": 1699636217686,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OXXvzAkBH5",
                "forum": "fYerSwf1Tb",
                "replyto": "RkLCa4rDGW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2751/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2751/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer S6gY (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for taking the time to review our paper and your valuable feedback. \n\nThank you so much for the general writing comments, and we have clarified all sections accordingly. \n\nA quick comment regarding the performance of HawkesVAE. You are absolutely correct on the bolding of the ROC metric, and we have corrected this, as it now indicates if the original data mean ROC is within 1 standard deviation. Post initial submission, we actually realized that our downstream model was underfitting, as we discovered that our sanity check results (from training the model on original data) did not match previous existing work [4]. We have now reran the results with a more robust downstream classifier, and seen a boost in all performance for all models in general. HawkesVAE (Multivariate) outperforms the next best model (in 4/7 datasets and is within 1 standard deviation with the rest of the datasets). We have updated Tables 2 and 5 accordingly.\n\n- Why is CTGAN not included as a baseline? \n\nWe believed that since TabDDPM [1] showed showed significant superior performance vs CTABGAN [2] and CTABGAN+ [3] (both of these models outperform CTGAN), we assumed that it would be a stronger baseline of general synthetic tabular generation compared to CTGAN, and left out CTGAN in favor of more recent work.\n\n- Table 3: to validate this metric, what is the performance of applying the real/synthetic classifier to real data? Is it indeed 0.5? What synthetic data was this discriminator trained on? \n\nYes, the performance of the real-vs-synthetic classifier is indeed 0.5. The synthetic data and the real data are simply the real training data (label 0) and the generated synthetic patients from each of the methods (label 1). We have clarified this in the paper in section 5.2.\n\n- Is the downstream classification task just binary classification of death? What does \u201cthe LSTM and the HawkesVAE models estimate their own sequence stopping length\u201d mean in the context of binary classification, do you also jointly regress death time? Does this mean the death event is removed from the end of training trajectories (real or synthetic)? What happens if the trajectory does not undergo any event?\n\nYes, the downstream classification task is indeed binary classification of death in the patient data. Death is not an event that is explicitly in the dataset, rather, the dataset simply contains events and timestamps of medications, procedures, as well as some adverse events like vomiting. No regression is done on explicit time-to-death prediction, as our model is only trained on event prediction. Death is an external label that is also available in the dataset. We have clarified this in section 5.\n\n- Why does training on synthetic data from Hawkes VAE occasionally beat training on the original data?\n\nOccasionally, synthetic data is able to support better performance than the original dataset on downstream tasks (this behavior is also seen in TabDDPM [1]). We believe that this is due to the synthetic model generating examples that are more easily separable and/or more diverse than real data. However, this is only an hypothesis and should be investigated further in future research, but we are encouraged to see that our proposed method captures this behavior. We have added commentary on this observation in Section 5.1.\n\n- I am not sure I understand why and how Hawkes-VAE is used for event forecasting\u2026\n\nHawkesVAE is trained via next event prediction, similar to Transformer Hawkes (To predict next time and event tuple $(t_i, k_i)$, the input is true previous times and events $\\{(t_1, k_1), \\dots, (t_{i-1}, k_{i-1}))\\}$). Note that our main results for ML Efficiency performs this autoregressively (where each predicted time and event is repeatedly appended to the input), as we of course do not have access to the true events. Giving more info in the \u201cEvents Known\u201d category actually surprisingly performs worse because it is actually $num_types$ different Hawkes Models, which each individually models its respective event and event times, and then combined to create a final multi-event sequence prediction. Its predictions are generally useful, but the strict ordering of the events may not be captured. This is why it performs worse in the strict forecasting sense. The general HawkesVAE model is a multivariate Hawkes process, which considers ALL events and times, so its forecasting performance should be intuitively better. We have clarified our methodology in Section 4.1 accordingly."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2751/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700435374850,
                "cdate": 1700435374850,
                "tmdate": 1700435374850,
                "mdate": 1700435374850,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JymUCuoPqw",
                "forum": "fYerSwf1Tb",
                "replyto": "mVvcOu1v2o",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2751/Reviewer_S6gY"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2751/Reviewer_S6gY"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response.\n\nYour new results appear promising and many of the proposed changes improve the quality of the paper. In my opinion, they represent too many changes for a rebuttal, considering most experimental results have changed. I therefore retain my score.\n\nSome additional comments/points that would benefit from further clarification:\n* The modelling differences between Hawkes Multivariate and Event known remain unclear to me. I now see that the latter consists of a distinct model for each type of event, but do not understand the following: \"Finally, an overall prediction combines event time predictions for all known types to create a final multi-event sequence prediction.\" I am also unclear why authors propose to sum over the latent space of these distinct models, and how this is motivated.\n\n* I do not have access to the pre-rebuttal manuscript but see that the \"Events Known\" Hawkes VAE now performs better than the Multivariate version in Table 6. I may recall incorrectly, but I believe this was not the case before the rebuttal. Is there an explanation for this change in performance?\n\nFinally, please carefully proofread the manuscript as it retains presentation issues (e.g. punctuation, poor citation style)."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2751/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700662497375,
                "cdate": 1700662497375,
                "tmdate": 1700662497375,
                "mdate": 1700662497375,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QNlaWMypOA",
                "forum": "fYerSwf1Tb",
                "replyto": "RkLCa4rDGW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2751/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2751/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer S6gY"
                    },
                    "comment": {
                        "value": "Thank you for your response! We would like to address each point as follows:\n\n-  too many changes for a rebuttal, considering most experimental results have changed. I therefore retain my score.\n\nThe only experimental changes were made was the more robust ML efficiency evaluation, which affects Tables 2 and 5, respectively. All other changes are paper editing changes to improve writing fluency.\n\n- The modelling differences between Hawkes Multivariate and Event known remain unclear to me. I now see that the latter consists of a distinct model for each type of event, but do not understand the following: \"Finally, an overall prediction combines event time predictions for all known types to create a final multi-event sequence prediction.\" I am also unclear why authors propose to sum over the latent space of these distinct models, and how this is motivated.\n\nSince the event type is known, we may index into the specific encoder/decoder pair as given by the event index (shown in Fig. 1). Finally, all independent event time predictions over all known types are combined via their predicted event times to create a final multi-event sequence prediction. \n\nSince the VAE model requires a single embedding to sample a latent vector, we sum the patient embedding output of all expert event encoders, and pass this joint embedding to the expert decoders. The decoder is trained to generate the predicted time and type sequence of its respective expert event.\n\nEssentially, this is purely motivated by our limitation of having a fully generative model. We have to constrain the model to a constant-size embedding space, as it would be unfair for to have varying size latent spaces for each patient.\n\nWe have clarified this in section 4.1 Event Type Information (page 5).\n\n- \"Events Known\" Hawkes VAE now performs better than the Multivariate version in Table 6. I may recall incorrectly, but I believe this was not the case before the rebuttal. Is there an explanation for this change in performance?\n\nThe performance of these 2 models remains the same as in the pre-rebuttal version. We have only removed the bolding of the table text, as we believed it was not clear. \u201cEvents known\u201d has access to the ground truth Event Types Information, and it thus an unfair comparison. We have decided to separate this using a dividing line instead.\n\n- Finally, please carefully proofread the manuscript as it retains presentation issues (e.g. punctuation, poor citation style).\n\nWe have made many grammatical edits in the paper, as well as highlighted our citations for improved visibility. If there are any further major grammatical errors, we would be happy to address them."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2751/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700683295507,
                "cdate": 1700683295507,
                "tmdate": 1700683308817,
                "mdate": 1700683308817,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]