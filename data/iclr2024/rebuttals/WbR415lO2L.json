[
    {
        "title": "Large Language Models can be Guided to Evade AI-generated Text Detection"
    },
    {
        "review": {
            "id": "I2V7iKZty0",
            "forum": "WbR415lO2L",
            "replyto": "WbR415lO2L",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3068/Reviewer_iD8f"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3068/Reviewer_iD8f"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes SICO, a prompting-based approach to guide LLMs to evade machine-generated text detectors. SICO operates by extracting text features in natural language and optimizing in-context examples with token substitution. Experiments demonstrate that SICO outperforms paraphrasing-based baselines against six detectors across three tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ machine-generated text detection is an important research question\n+ the proposed approach is flexible and promising"
                },
                "weaknesses": {
                    "value": "- The feature extraction part could be better motivated and investigated. The effectiveness of such a component reply on two assumptions: 1) LLMs could reliably summarize and extract the \"features\" of human-written texts from examples; 2) LLMs could use those style \"features\" to guide their generation. Both would need more support, in addition to showing that having the \"feature extraction\" component helps model performance.\n\n- It is not guaranteed that the synthetic token substitution process would lead to decreases in $P_{AI}$ scores, right? Is the substitution performed only once for each passage? What are the stopping criteria? How often does this mechanism fail to find a new passage that has substantially lower $P_{AI}$ scores? In addition, I won't say this is \"optimize\", as this process is not differentiable, simply trying over and over again with different synonyms.\n\n- Wordnet is used for token substitutions in this work. My understanding is that Wordnet could be quite noisy, containing words and tokens that are very infrequent in human-written text. As a result, it is surprising to see that Wordnet substitutions do not result in decreases in readability. I wonder if the authors might have conducted some filtering/selection of Wordnet to mitigate these issues.\n\n- I wonder if the authors might have conducted an analysis of the correlation between detector performance and SICO evasion rate. In other words, is SICO greatly advancing the state-of-the-art on the most capable detectors or the less capable ones?\n\n- I'm not sure if Section 4.4 is of great scientific value. The likes and dislikes barely indicate meaningful signals for research purposes as social media platforms could be quite random. The detection rate of 7.5% is arguably a hyper-loose lower bound of the actual detection rate by humans since it would require the users to go out of their way to respond to indicate their suspicion. A more rigorous study could be conducted by asking human readers to specifically perform the task of distinguishing human- and machine-generated text, similar to that of Section 4.3."
                },
                "questions": {
                    "value": "please see above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3068/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698106295731,
            "cdate": 1698106295731,
            "tmdate": 1699636252450,
            "mdate": 1699636252450,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7hEkdgwPB6",
                "forum": "WbR415lO2L",
                "replyto": "I2V7iKZty0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3068/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3068/Authors"
                ],
                "content": {
                    "title": {
                        "value": "# Response to Reviewer iD8f (Part 1)"
                    },
                    "comment": {
                        "value": "Thank you for the feedback and the constructive questions. Below, we have responded to your concerns and questions. We hope our response addresses your concerns. If you have further questions that prevent you from raising your score, please let us know.\n\n\n\n### Q1: Concerns regarding feature extraction\n\n> 1 ) LLMs could reliably summarize and extract the \"features\" of human-written texts from examples. \n\nThanks for your thoughtful comment. To test if LLM could reliably summarize and extract the \"features\" of human-written texts from examples, we adopted a simple test by running 3 feature extractions on different sets of human-written text. We use extracted features to guide LLM generation and test the AUC drop after adopting 3 different features compared with normally generated text. Results are evaluated by GPT3.5-D on writing task. One can see that the LLM consistently extracts useful features for detector evasion, even based on different examples.\n\n|          | Feature 1 | Feature 2 | Feature 3 |\n|----------|-----------|-----------|-----------|\n| AUC Drop | - 0.288   | - 0.261   | - 0.142   |\n\n> 2 ) LLMs could use those style \"features\" to guide their generation. \n\nIn fact, our ablation study in Table 4 (Page 8) demonstrates the effectiveness of feature guidance. The \"w/o ICE\" represents the method that only uses features to guide the LLM generation, which shows feature component is helpful to detector evasion. \n\n\n\n\n\n\n### Q2. Concerns regarding token substitution\n\n> It is not guaranteed that the synthetic token substitution process would lead to decreases in $P_{AI}$ scores, right?\n\nYes, there is no theoretical guarantee. But in experiments, the token substitution process often decreases $P_{AI}$, because the greedy policy will select the token decreases $P_{AI}$ most for each position.\n\n> Is the substitution performed only once for each passage? \n\nIf what you refer to as \"passage\" means \"in-context example\", the answer is No. \nThe optimization process will repeat $N$ times as we illustrated in Algorithm 2 (Page 4).\n\n> What are the stopping criteria?\n\nThe optimization process stops after running $N$ times.\n\n> How often does this mechanism fail to find a new passage that has substantially lower $P_{AI}$ scores?\n\nFor each in-context examples, in the first three iterations, the algorithm can always find a new in-context example with a lower $P_{AI}$ score. However, in the last three iterations, multiple experiments show approximately 20% of cases where the algorithm cannot find.\n\nHowever, as the prompt is composed of 8 in-context examples, the prompt text always changes during optimization.\n\n\n>  In addition, I won't say this is \"optimize\", as this process is not differentiable, simply trying over and over again with different synonyms.\n\nWe respectfully appreciate the reviewer's perspective but wish to clarify the term \"optimize\" as used in our context. While the reviewer focuses on differentiability in optimization, it's important to note that not all optimization requires a differentiable objective function. For instance, in discrete optimization like TSP, where solutions are from a set of discrete choices, differentiability is not a prerequisite for optimization.\n\nWe formulate the problem as a combinatorial optimization to align with situations where the text is discrete. To navigate this discrete solution space, we employ a greedy search methodology."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3068/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700206409695,
                "cdate": 1700206409695,
                "tmdate": 1700206409695,
                "mdate": 1700206409695,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4vy34YLK8q",
                "forum": "WbR415lO2L",
                "replyto": "I2V7iKZty0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3068/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3068/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer iD8f (Part 2)"
                    },
                    "comment": {
                        "value": "### Q3. Token substitutions constraints\n\n> Wordnet is used for token substitutions in this work. My understanding is that Wordnet could be quite noisy, containing words and tokens that are very infrequent in human-written text. As a result, it is surprising to see that Wordnet substitutions do not result in decreases in readability. I wonder if the authors might have conducted some filtering/selection of Wordnet to mitigate these issues.\n\nThanks for your thoughtful comment. Your concern about the potential noisiness of Wordnet is indeed valid. And yes, we employ a masked language model (MLM) to filter out the substitution words that do not fit the context. Specifically, we use the MLM to calculate the probability of substitution based on its context. Substitutions that fall below a predetermined probability threshold will be discarded.\nThe details are shown in Appendix A.1.3.\n\nAdditionally, it is important to highlight the inherent capabilities of LLMs in maintaining the readability of the text.  Even if some prompts include uncommon or unsmooth phrases, LLM still generated text exhibits high readability.\n\n### Q4. Correlation between detector performance and SICO evasion rate\n\n> I wonder if the authors might have conducted an analysis of the correlation between detector performance and SICO evasion rate. In other words, is SICO greatly advancing the state-of-the-art on the most capable detectors or the less capable ones?\n\nThank you for your valuable question. We conducted an analysis and found that SICO performs better against more capable detectors. \n\nHere are the details of the analysis experiment: We use the detectors' performance on the original AI-generated text to represent their base capability. SICO advancing performance is measured by the AUC difference between SICO and the best-performing paraphraser baselines. The statistical analysis of our results, using the Pearson correlation coefficient, yielded an $r$ value of 0.47 with a p-value of 0.048, indicating a moderate positive correlation.\n\n### Q5. Concerns about Reddit test\n\n> I'm not sure if Section 4.4 is of great scientific value. The likes and dislikes barely indicate meaningful signals for research purposes as social media platforms could be quite random. The detection rate of 7.5% is arguably a hyper-loose lower bound of the actual detection rate by humans since it would require the users to go out of their way to respond to indicate their suspicion. A more rigorous study could be conducted by asking human readers to specifically perform the task of distinguishing human- and machine-generated text, similar to that of Section 4.3.\u200b\n\nThank you for your valuable feedback. While it's true that social media reactions can be somewhat random, the significantly higher number of likes over dislikes (40% vs 2.5%) suggests that these responses are generally well-received by real-world users. \n\nRegarding the detection rate, we acknowledge your opinion about the 7.5% detection rate potentially being a lower estimate of human ability to identify AI. To address this concern, we conducted a more rigorous experiment, as you suggested. We randomly selected 150 examples (50 each from AI, SICO-Para, and human responses) from QA task. Four human annotators were then asked to identify whether each answer was AI-generated or human-written. The results were as follows, with 'AI %' indicating the proportion of texts identified as AI-generated:\n\n| Text        | AI % |\n|:-----------:|:-------------:|\n| AI          | 54%           |\n| SICO        | 26%           |\n| Human       | 10%           |\n\nOne can see that SICO remarkably reduces the probability of being recognized by humans. Additionally, the data also reveals that human detection capabilities are not highly accurate compared with machine detectors. Approximately 50% of AI-generated texts went undetected, and 10% of human-written texts were mistakenly identified as AI-generated."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3068/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700206475990,
                "cdate": 1700206475990,
                "tmdate": 1700206475990,
                "mdate": 1700206475990,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gymBCoeZB0",
                "forum": "WbR415lO2L",
                "replyto": "4vy34YLK8q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3068/Reviewer_iD8f"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3068/Reviewer_iD8f"
                ],
                "content": {
                    "comment": {
                        "value": "I would like to thank the authors for the detailed response. I will closely monitor the interactions between the authors and other reviewers before proposing a final rating."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3068/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700455778528,
                "cdate": 1700455778528,
                "tmdate": 1700455778528,
                "mdate": 1700455778528,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "tQntEWdp08",
            "forum": "WbR415lO2L",
            "replyto": "WbR415lO2L",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3068/Reviewer_nfun"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3068/Reviewer_nfun"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes to utilize prompts to steer (the style of) the generator and deceive the AI-generated text detector. Substitution-based In-Context example Optimization method (SICO) is proposed to automatically construct such prompts. The methodology is feasible but this work lacks motivation for social benefits and automatic evaluation of the quality of the new generation."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ The technology for constructing prompts is interesting, although I am not an expert in this direction.\n\n+ The paper is well-written and thus easy to follow."
                },
                "weaknesses": {
                    "value": "+ This work proposes to deceive the detector from a malicious perspective. We are interested in how this work could help from a benign perspective, e.g., how it helps to train a more robust detector.\n\n+ Several major concerns on evaluation limit the soundness of this work:\n    + For the results in Table 1, several baselines already achieve reasonable results, e.g., Parrot on Detect GPT, as about 0.5 AUC should have shown the outputs are quite obscured. Achieving a very low AUC is a negative signal for these experiments.\n    + For the quality of the generated outputs, only human evaluation on DIPPER is considered, leaving several concerns 1) how will it perform on other datasets, 2) how will it perform on other automatic evaluation metrics, 3) why some scores are higher then human outputs, e.g., *Writing*.\n\n+ Considering the detectors are basically 'classifiers', finding adversarial is desperately possible. The main finding of this work is not excitingly novel to our community."
                },
                "questions": {
                    "value": "See bullet points in weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3068/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3068/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3068/Reviewer_nfun"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3068/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698830778311,
            "cdate": 1698830778311,
            "tmdate": 1699636252379,
            "mdate": 1699636252379,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "MfNd4BU7Jr",
                "forum": "WbR415lO2L",
                "replyto": "tQntEWdp08",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3068/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3068/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer nfun (Part 1)"
                    },
                    "comment": {
                        "value": "Thank you for the feedback and the constructive questions. Below, we have responded to your concerns and questions. We hope our response addresses your concerns. \nPlease let us know if you have further questions preventing you from raising your score.\n\n### Q1: Concerns about benign applications of our work\n\n> This work proposes to deceive the detector from a malicious perspective. We are interested in how this work could help from a benign perspective, e.g., how it helps to train a more robust detector.\n\nThanks for your instructive comments. We fully agree with you and acknowledge the importance of exploring the benign applications of our work, particularly in terms of developing more robust AI detection systems.\n\nHowever, it is crucial to recognize that the field of AI detection in LLM is still in the early stage. \nThe typical evolutionary path of such systems involves an initial development phase, followed by the emergence of various attack methods, such as the work in other domains [1, 2, 3]. These attacks, in turn, boost the development of defensive strategies. \nTherefore, while our research primarily addresses the attacking perspective, it contributes to the field by highlighting potential weaknesses in existing AI detection systems. \nWe believe not providing defensive methods does not weaken the value of proposed stronger attack baselines. Additionally, our approach has the potential to act as an adversarial generator to help model training. We leave other defensive techniques for future research.\n\n\n\n### Q2: Concerns about attack evaluation\n\n>  For the results in Table 1, several baselines already achieve reasonable results, e.g., Parrot on Detect GPT, as about 0.5 AUC should have shown the outputs are quite obscured. Achieving a very low AUC is a negative signal for these experiments.\n\nWe respectfully disagree with your argument. First, we want to clarify that these detectors do not simply label text as 'AI-generated' or 'human-written'; instead, they assign an AI score that quantifies the likelihood of the text being AI-generated. \nA higher score means high probability that the text is generated by AI.\n\n\nThen, we want to explain the cases where the AUC is less than 0.5. This case indicates the detector often assign lower AI score to SICO-generated text compared with the actual human-written text. \nIn this case, no matter which classification threshold is set, the detectors cannot distinguish two types of text, thus performing worse than a random guess. \nTherefore, a low AUC is not a negative signal, instead highlighting the limitations and potential unreliability of those detectors. \n\nFor your reference, we present average AI scores assigned by DetectGPT and Log-Rank to three different types of text in the writing task. One can see that human-written text receives a higher AI score than SICO-generated text on average. \n\n| Text          | DetectGPT | Log-Rank|\n|---------------|-----------|---------|\n| Orig. AI-Generated | 0.947     | -1.16   |\n| SICO-Para     | -0.233    | -2.10   |\n| Human-Written | 0.396     | -1.62   |\n\n> several baselines already achieve reasonable results, e.g., Parrot on Detect GPT\n\nSome baseline methods work well in a few cases, but in many other cases, they perform poorly. For example, across three datasets, Parrot scored 0.819 and 0.805 in AUC on GPT3.5-D and GPT2-D, respectively.\nHowever, our method consistently outperforms other baselines regarding evasion, indicating it is a more powerful test method."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3068/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700230300012,
                "cdate": 1700230300012,
                "tmdate": 1700230300012,
                "mdate": 1700230300012,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "y2w4z7hW2B",
                "forum": "WbR415lO2L",
                "replyto": "tQntEWdp08",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3068/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3068/Authors"
                ],
                "content": {
                    "title": {
                        "value": "# Response to Reviewer nfun (Part 2)"
                    },
                    "comment": {
                        "value": "### Q3: Concerns on human evaluation\n\n> For the quality of the generated outputs, only human evaluation on DIPPER is considered, leaving several concerns 1) how will it perform on other datasets\n\nWe considered all three datasets we used in experiments, including writing, QA, and review generation. Please check the results in Table 2.\n\n> 2 ) how will it perform on other automatic evaluation metrics\n\nThanks for your thoughtful question.\nIn our study, we used automatic evaluation to examine the semantic preserving capabilities of different methods. The detailed findings can be found in Table 12 (Page 18).\n\nFor quality metrics like readability or task completion rate, the lack of \"gold reference\" of LLM-generated text limits the effectiveness of traditional automatic evaluation methods.\nMoreover, we should notice that the goal of automatic evaluation is to approximate human preferences in text quality. \nTherefore, we believe that human evaluation is the best way to quantify text usability. \n\n\n> 3 ) why some scores are higher then human outputs, e.g., Writing.\n\nThank you for your thoughtful questions. Here are two cases where the score of human outputs is lower than SICO-Gen; we will explain it one-by-one.\n\n- **Readability on review generation task.** We check the human-written reviews annotated as relatively low readability (score 2 or 3). We observe that those reviews are very informal and have some grammar errors, which is reasonable as the reviews are sourced from the internet. Here we show some examples:\n    ```\n    Eg.1:\n    Basically the food here is great, everything else is not.  a \"pasty\", apparently, is a shepherd's pie (or any other one of the 40 variations on a meat pie) ...\n\n    Eg.2:\n    ... It's not just a regular pizza place either, this is all organic - local - farm fresh - deliciousness!  Guilt free carbs if you will ...\n    ``` \n\n- **Task completion rate on academic writing task.** We examined human-written essays annotated as not as incomplete for the task. We found that format errors made annotators think the essays are not appropriate for academic purposes. These errors originated from our use of Wikipedia text, following the implementation of [4]. We intend to address and correct these formatting issues in future human evaluations. However, this issue does not affect the validity of the results obtained through other methods. Here are some examples for reference:\n    ```\n    Eg.1:\n    ...  but the court of St. Petersburg ignored their suggestions.:143 ...\n\n    Eg.2:\n    Antarctica (US English ... )[Note 1] is Earth's southernmost continent, containing ...\n    ```"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3068/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700230400083,
                "cdate": 1700230400083,
                "tmdate": 1700230400083,
                "mdate": 1700230400083,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jnd2EcpsH3",
                "forum": "WbR415lO2L",
                "replyto": "tQntEWdp08",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3068/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3068/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer nfun (Part 3)"
                    },
                    "comment": {
                        "value": "### Q4: Concerns about novelty of finding\n\n> Considering the detectors are basically 'classifiers', finding adversarial is desperately possible. The main finding of this work is not excitingly novel to our community.\n\nWe respectfully disagree with the reviewer's remark that our work lacks novelty in finding adversarial examples (AEs). Our research introduces a novel approach that is much more **challenging** than traditional adversarial attack methods in both its goals and techniques. \nBelow, we list the unique aspects:\n\n- **Unique and Ambitious Goal**: Our research aims to find a **universal prompt**, which can adapt to various task inputs for evading detectors. However, a typical adversarial attack seeks to find an AE for a specific instance. Essentially, our work focuses on designing an algorithm that automatically constructs an AE generator.\n- **Direct Generation of AEs**: Our method directly generates AEs based on the task input. However, typical adversarial attacks require iterative editing of existing instances. For example, given a question in QA task, our method will directly generate answers that evade detectors. In contrast, typical attacks need to edit the pre-existing, normal answers to achieve the adversarial goal. The direct generation approach is more challenging than iterative editing approach. \n\n\nOur prompt optimization algorithm overcomes these challenges, successfully finding a universal prompt and turning the LLM into a flexible AE generator. \nBesides, our experimental results indicate the vulnerabilities of current AI text detection systems, which is significant in the era of LLM.\nWe believe these contributions are novel and exciting to our community.\n\n## References\n\n[1] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, Rob Fergus: Intriguing properties of neural networks. ICLR 2014  \n\n[2] Nicolas Papernot, Patrick D. McDaniel, Ananthram Swami, Richard E. Harang: Crafting adversarial input sequences for recurrent neural networks. MILCOM 2016 \n\n[3] Hongge Chen, Huan Zhang, Pin-Yu Chen, Jinfeng Yi, Cho-Jui Hsieh: Attacking Visual Language Grounding with Adversarial Examples: A Case Study on Neural Image Captioning. ACL 2018\n\n[4] Eric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher D. Manning, Chelsea Finn: DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature. ICML 2023"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3068/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700230573990,
                "cdate": 1700230573990,
                "tmdate": 1700230573990,
                "mdate": 1700230573990,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kYyEZSjpZ5",
                "forum": "WbR415lO2L",
                "replyto": "tQntEWdp08",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3068/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3068/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Sincerely Looking Forward to Your Feedback"
                    },
                    "comment": {
                        "value": "We sincerely appreciate your great efforts in reviewing this paper. We believe we have addressed your concerns and clarified the misunderstandings. Given the approaching deadline, we would be grateful if you could let us know if you have any further concerns. We are open to all possible discussions and value your input."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3068/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700636107756,
                "cdate": 1700636107756,
                "tmdate": 1700636107756,
                "mdate": 1700636107756,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "aRXKzslnbJ",
            "forum": "WbR415lO2L",
            "replyto": "WbR415lO2L",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3068/Reviewer_Jo4D"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3068/Reviewer_Jo4D"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to equip LLMs with prompts, rather than relying on an external paraphraser, to evaluate the vulnerability of AI text detectors. Specifically, SICO iteratively substitutes words and sentences within the in-context examples to provide more representative demonstrations for LLMs to generate text that cannot be detected. Besides quantitative experiments, human evaluation and validation on online social platform prove the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed in-context example method is cost efficient and universally useful against a wide range of detectors.\n2. Quantitative experiments show that the proposed method outperforms existing paraphraser baselines across three datasets and multiple detectors.\n3. The evaluation on Reddit and relevant analysis make the effectiveness of the proposed method more vivid and realistic."
                },
                "weaknesses": {
                    "value": "Although SICO only requires 40 human-generated examples and a limited number of LLM inferences, but it still costs approximately 1 USD using the GPT-3.5 API."
                },
                "questions": {
                    "value": "1. Since each evasion only runs once during evaluation, then what's the arguments for GPT-3.5 in this paper, such as the temperature and top-p values.\n\n2. As shown in Table 1, the proposed SICO and human prompt obtain much better performance. DIPPER also achieves good performance on QA and Review dataset. However, the third best performance on Writing dataset is Parrot, which is much better than DIPPER. I am wondering, in Section 4.3, why DIPPER is considered for Writing dataset not Parrot. \n\n**Suggestions**\n\n1.For values in Table 2, since the higher the better, perhaps you could mark best performance per column in boldface.\n\n2. In Section 4.4, it would be interesting if another competitive baseline (such as Human-Written) is used to answer the same question as the proposed SICO on Reddit. In that case, we may have a better idea which one is more preferred by users or which one is easier to be recognized as chatgpt."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3068/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699039123181,
            "cdate": 1699039123181,
            "tmdate": 1699636252317,
            "mdate": 1699636252317,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fAIg0MhZgx",
                "forum": "WbR415lO2L",
                "replyto": "aRXKzslnbJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3068/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3068/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Jo4D"
                    },
                    "comment": {
                        "value": "Thank you for the feedback and the constructive questions. \nBelow, we have responded to your concerns and questions. We hope our response addresses your concerns. \nPlease let us know if you have further questions preventing you from raising your score.\n\n### Weakness: SICO still costs 1 USD\n\n> Although SICO only requires 40 human-generated examples and a limited number of LLM inferences, but it still costs approximately 1 USD using the GPT-3.5 API.\n\nWe believe your concern about cost can be alleviated in practice.\nIn practical applications, users might not need to spend money to construct prompts on their own. There will be prompt providers, such as ChatGPT plugin, to provide effective prompts to users. Users simply choose prompts from the provider and combine them with their own inputs. We argue that 1 USD expense for the prompt provider is a negligible cost.\n\nFurthermore, if users want to customize their own SICO prompts, the 5 USD free quota provided by OpenAI is enough for several runs. Additionally, users have the option to utilize open-source LLMs. Our experiments with the typical open-source model `Vicuna-13B` demonstrate that SICO effectively evade most detectors (Table 5 on Page 9). \n\n\n\n### Q1: GPT-3.5 argument\n\n> what's the arguments for GPT-3.5 in this paper, such as the temperature and top-p values.\n\nThanks for your thoughtful question. During evaluation, we set all the arguments of GPT-3.5 API to default, as we mentioned in the experimental settings of Section 4.1(Page 6). Specifically, $\\text{temperature} = 1$ and $\\text{top-p} = 1$.\nOther default settings can be found in OpenAI documents: https://platform.openai.com/docs/api-reference.\n\n### Q2: Why not consider Parrot in human evaluation?\n\n> As shown in Table 1, the proposed SICO and human prompt obtain much better performance. DIPPER also achieves good performance on QA and Review dataset. However, the third best performance on Writing dataset is Parrot, which is much better than DIPPER. I am wondering, in Section 4.3, why DIPPER is considered for Writing dataset not Parrot.\n\nThanks for your thoughtful question. We totally agree with the reviewer's opinion. The primary reason for selecting DIPPER over Parrot is rooted in their initial training purposes. DIPPER was specifically trained to evade AI detectors, whereas Parrot, originally designed for paraphrasing, was only recently discovered to be able to evade detectors. \n\nTo examine the text from Parrot on the writing task in human evaluation, we conducted a small experiment. We randomly sampled 120 examples (40 per method) from the writing task and asked two human annotators to evaluate them. The experiment result shows that SICO still outperforms Parrot by a large margin.\n\n| **Method**        | **Readability** | **Compeletion** |\n|-------------------|-----------------|-----------------|\n| **Parrot**        | 3.35     | 70.0        |\n| **SICO-Para**     | 3.80     | 82.5         |\n| **SICO-Gen**      | 3.90    | 85.0          |\n\n\n### Suggestions 1: Mark best performance in Table 2\n> For values in Table 2, since the higher the better, perhaps you could mark best performance per column in boldface.\n\nThanks for your thoughtful advice, we will modify Table 2.\n\n### Suggestions 2: Add baselines in Reddit test\n\n> In Section 4.4, it would be interesting if another competitive baseline (such as Human-Written) is used to answer the same question as the proposed SICO on Reddit. In that case, we may have a better idea which one is more preferred by users or which one is easier to be recognized as chatgpt.\n\n\nWe fully agree with your suggestion, recognizing its potential to illustrate which baseline is most favored by real-world users. However, the primary objective of the Reddit test was to verify the applicability of SICO in the real world, rather than to conduct comparative analyses with other baselines.\n\nAdditionally, the human-evaluation results presented in Table 2 already indicate that human-written texts surpass SICO in terms of usability. So, we didn't consider it as a baseline in the Reddit test.\n\nFurthermore, some questions sourced from Reddit are very challenging, such as the one depicted on the left in Figure 3. Therefore, it would be more expensive to hire experts to answer those questions."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3068/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700202289774,
                "cdate": 1700202289774,
                "tmdate": 1700202289774,
                "mdate": 1700202289774,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Bn8S72FH5Z",
                "forum": "WbR415lO2L",
                "replyto": "aRXKzslnbJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3068/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3068/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Sincerely Looking Forward to Your Feedback"
                    },
                    "comment": {
                        "value": "We sincerely appreciate your great efforts in reviewing this paper. We believe we have addressed your concerns and clarified the misunderstandings. Given the approaching deadline, we would be grateful if you could let us know if you have any further concerns. We are open to all possible discussions and value your input."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3068/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700636077435,
                "cdate": 1700636077435,
                "tmdate": 1700636077435,
                "mdate": 1700636077435,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "LtbVxyjcY5",
            "forum": "WbR415lO2L",
            "replyto": "WbR415lO2L",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3068/Reviewer_KWZu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3068/Reviewer_KWZu"
            ],
            "content": {
                "summary": {
                    "value": "Summary: This paper aims to evade existing text detection systems through in-context optimization, making the AI-generated text more similar to humans. The method works by initializing the in-context prompts by some human-crafted pairs and gradually replacing the words or sentences to minimize the detectability. The final optimized in-context examples and task prompts are used as a combined new prompt to generate undetectable text given a new query. Extensive experiments through diverse datasets and models validate the effectiveness of this novel attack over several existing baselines. The authors also claim that the resulting attack prompt can be generalized to other detectors."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Strength:\nThe attack on detectors is an important task and the authors propose a novel prompt attack by in-context optimization. On tested datasets and models, this approach works well compared with several baselines under their setting.\nThe experiments are comprehensive and solid to support their main results, though some weaknesses remain.\nThe whole paper is written well and easy-to-follow. The experimental settings are clear and easy to reproduce."
                },
                "weaknesses": {
                    "value": "Weakness: There are three main weaknesses.\nThe first weakness is the diversity. I understand that the method aims at finding an optimized prompt to make the model learn human-style writing. However, the authors only show the detection result or the readability. One crucial problem is whether the generated text will lose diversity. It might simply learn the features provided in the prompt and universally apply those features to all generated text. Then, although the detection performance drops the each single text remains readable, the overall generations might lack inter diversity. Thus, this will limit its overall utilization.\nThe second main weakness is transferability over data. The proposed method requires in-context examples from a given task and its corresponding dataset. However, in practice, this setting is too ideal since many users will use ChatGPT to generate various outputs under different conditions. Thus, the task and in-context examples are not given (How can you obtain 32 in-context examples for each new query?). Besides, the author did not mention how the result on one dataset generalizes to other datasets. It make sense for the optimized prompt in one dataset to beat other baselines since one specific dataset and task has their intrinsic similarity. But most other baselines work in general. So, the comparison is not fair, and whether the approach really works is questionable. \nThe third weakness is the experiments are only conducted on GPT-3.5. I am expecting on more models, including open-sourced models for easy and cheap reproduction.\nOther weaknesses include:\nthe method does not test on non-English text, limiting its utility. \nAlso, the method requires many in-context examples, but it might exceed the context window or limit the generation length due to the context window limit. \nThe methods did not report the length effect on generated text, which is crucial for detection."
                },
                "questions": {
                    "value": "Questions:\nSee weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "n/a"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3068/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699169831452,
            "cdate": 1699169831452,
            "tmdate": 1699636252256,
            "mdate": 1699636252256,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WzbjSxVuJn",
                "forum": "WbR415lO2L",
                "replyto": "LtbVxyjcY5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3068/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3068/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer KWZu (Part 1)"
                    },
                    "comment": {
                        "value": "Thank you for the feedback. Below, we have responded to your concerns and questions. We hope our response addresses your concerns. \nPlease let us know if you have further questions preventing you from raising your score.\n\n### Q1: Concerns about text diversity\n\n> One crucial problem is whether the generated text will lose diversity. It might simply learn the features provided in the prompt and universally apply those features to all generated text. Then, although the detection performance drops the each single text remains readable, the overall generations might lack inter diversity. Thus, this will limit its overall utilization. \n\n\n**Diversity across tasks**\n\nFirst, we want to clarify that texts generated by SICO illustrate **high diversity across different tasks**, and the feature is coherent with the task context. Here are the typical examples generated by SICO for academic writing task and review generation task. One can see that generated review is more informal and colloquial, while the academic essay is in formal styles, illustrating a high diversity across tasks. \n\n| **Task** | **SICO Generated Text** |\n|:--------:|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n|  Writing | With the initial publication of The Unicode Standard in 1991, The Consortium assumed its position as a leading authority on Unicode standards. To this day, the organization progresses to develop Unicode specifications ... |\n|  Review  | Who doesn't love Earl's? Their hot sandwich was a delightfully mouthwatering experience, especially the turkey sandwich. The chicken noodle soup was warm and comforting and the PB&J sandwich was just the right amount of sweetness ... |\n\n\n\n**Diversity within tasks**\n\nWe speculate that you refer to the style diversity between texts **for the same tasks**. While it's true that a single SICO might generate text that loses diversity, it can be easily mitigated in practice.\n\n- **Utilization of Multiple Prompts**: As discussed in Section 6 (Page 9), we believe the diversity issue can be easily addressed by constructing multiple SICO prompts. The low cost makes this approach feasible.\n\n     Besides, different SICO prompts can be obtained based on the same data. For empirical evidence, we constructed two effective prompts with the same experimental settings and data, and use them to generate two answers for the same questions. One can observe that prompt 1 generate more conversational and informal answers compared with another one. \n\n    | Prompt   | SICO generated text |\n    |--------------------------------------|-----------------------------------------------------------------------|\n    | Prompt-1 | We've all felt it - the emotional pain and heartache that comes with tough times. Am I right? This feeling is totally legit and caused by our bodies' release of stress hormones ... |\n    | Prompt-2 | The sensation commonly referred to as \"heartache\" or \"emotional pain\" is an outcome of a physiological process triggered by the release of stress hormones, including cortisol and adrenaline ...|\n\n- **Prompt Provider**: In practical applications, user might not have to construct multiple prompt. There will be prompt providers, such as ChatGPT plugin, to construct multiple prompts to generate diverse text. So users can just utilize them to for their task."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3068/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700200983489,
                "cdate": 1700200983489,
                "tmdate": 1700200983489,
                "mdate": 1700200983489,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sNL0aVlJyQ",
                "forum": "WbR415lO2L",
                "replyto": "LtbVxyjcY5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3068/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3068/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer KWZu (Part 2)"
                    },
                    "comment": {
                        "value": "### Q2: SICO needs different in-context examples for different task\n\n> How can you obtain 32 in-context examples for each new query?\n\nBefore addressing your questions in detail, we would like to first clarify some points.\n\nIt seems to us that this question stems from the misunderstanding of SICO usage. After SICO optimization is finished, the content of the prompt, including the in-context examples, is fixed.  The SICO prompt then serves as a template, allowing users to put different task inputs into it. Therefore, users don't need to select different in-context examples from dataset for new input (query). Reviewers can refer to the Figure 1 for illustration, where in the constructed prompt, the purple text is the only part to change, while other parts (feature, task instruction, in-context examples) are fixed after optimization.\n\nBesides, SICO only needs $K=8$ in-context examples instead of $32$. The number $32$ comes from task inputs for prompt evaluation ($|X_{\\text{eval}}| = 32$). The details can be found in Section 3 (Page 3) and experimental settings in Section 4.1 (Page 6). We also discussed in Section 4.5 (Page 8). We kindly suggest that you refer to these sections.\n\n\n\n> The proposed method requires in-context examples from a given task and its corresponding dataset. However, in practice, this setting is too ideal since many users will use ChatGPT to generate various outputs under different conditions. Thus, the task and in-context examples are not given.\n\nWe agree with the reviewer that SICO prompt construction requires examples for a given task. However, there are points we want to clarify:\n\n- First, as we discussed in the main response, not every task is safety-concerned and needs detection. For example, translating websites has low misuse risk, whereas generating fake reviews, a profitable misuse, makes AI detection crucial. Although \"users will use ChatGPT to generate various outputs under different conditions\", they only care about evading those safety-concerned tasks under detection.\n- Secondly, if it is necessary to deploy detectors for a task, evading detectors is valuable enough to collect in-context examples. \n- For the practical application of SICO, as mentioned before, users may not need to collect the data and construct SICO prompt by themselves. The prompt providers will construct prompts, allowing users to  utilize them to generate human-like texts. \n\n### Q3: Concerns about transferability over tasks\n\n> It make sense for the optimized prompt in one dataset to beat other baselines since one specific dataset and task has their intrinsic similarity. But most other baselines work in general. So, the comparison is not fair, and whether the approach really works is questionable. \n\nThank you for your insightful feedback. The suggestion to use a transferable prompt effective for various tasks is valuable. At first, we did consider this approach but did not choose it in order to prioritize the usability of generated text. Because the similar text style may harm the usability of texts for different tasks. For instance, academic essay writing typically requires formal and structured text, which might not align well with more casual or informal styles used in review generation. \n\nTo further test the transferability of our approach, we conducted an additional experiment. We constructed a prompt based on the text from Writing and QA task (\"SICO-Para-T\"), and evaluated the evasion performance on review generation task. As shown in the table, the results indicate that SICO-Para-T still outperforms other baselines significantly. The difference in performance between SICO-Para and SICO-Para-T could be due to the difference in the distribution of in-context examples and test inputs. We didn't test the API-based detectors to keep the consistency of results.\n\n\n\n\n\n| **Method**  | **GPT3.5-D** | **GPT2-D** | **DetectGPT** | **Log-Rank** |\n|-------------|--------------|------------|---------------|--------------|\n| -           | 0.925        | 0.952      | 0.808         | 0.982        |\n| Parrot      | 0.871        | 0.934      | 0.654         | 0.893        |\n| DIPPER      | 0.987        | 0.984      | 0.515         | 0.814        |\n| SICO-Para   | 0.465        | 0.264      | 0.270         | 0.300        |\n| SICO-Para-T | 0.496        | 0.487      | 0.447         | 0.490        |"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3068/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700201187262,
                "cdate": 1700201187262,
                "tmdate": 1700201187262,
                "mdate": 1700201187262,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KJ6jBAsWIP",
                "forum": "WbR415lO2L",
                "replyto": "LtbVxyjcY5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3068/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3068/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer KWZu (Part 3)"
                    },
                    "comment": {
                        "value": "### Q4: Concerns about other LLM\n> The third weakness is the experiments are only conducted on GPT-3.5. I am expecting on more models, including open-sourced models for easy and cheap reproduction.\n\nThanks for your thoughtful suggestions. We actually adopted experiments on a typical open-sourced model `Vicuna-13B`, and reported the result in Table 5 (Page 9). Please check the results.\n\n### Q5: Concerns about non-English text\n> the method does not test on non-English text, limiting its utility.\n\nThanks for your thoughtful suggestions. We appreciate the significance of testing on non-English text. However, it is important to note that the field of AI detection in LLM is still in the early stage. Consequently, the majority of research, including ours, primarily relies on English corpora, owing to their richness and maturity [1-4]. We leave the research of non-English text for future exploration. \n\n\n### Q6: Concerns about prompt length\n\n> the method requires many in-context examples, but it might exceed the context window or limit the generation length due to the context window limit.\n\nAs discussed in **Q2**, we guess you raise this concern due to the misunderstanding of using $32$ in-context examples in SICO. Actually we use $8$ in-context examples and here show the maximum/average token number of the whole SICO prompts we got during experiments, which is much less than the context windows of popular LLMs.\n\n| **Max Token Num** | **Avg Token Num** | **LLaMa 1** | **LLaMa 2** | **GPT-3.5** |\n|:-----------------:|:-----------------:|:-----------:|:-----------:|:-----------:|\n| 1015              | 873             | 2048        | 4096        | 4096        |\n\n\n### Q7: Concerns about length effect\n\n> The methods did not report the length effect on generated text, which is crucial for detection.\n\nThank you for your valuable suggestions.  To address them, we did a series of experiments to explore the impact of text length.\n\nWe first analyzed the detector's bias varied with text length. We sorted SICO-generated texts by word number and divided them into four groups. Then, we calculated the average $P_{AI}$ of each group using GPT3.5-D. The results, shown in the table below, indicate that the detector tends to assign a higher $P_{AI}$ to longer texts. \n\n| Avg. Length  | Avg. $P_{AI}$|\n|:-----------:|:------------:|\n| 94.7         | 0.022       |\n|  114.9     |     0.044    |\n|  130.8      |    0.064     |\n| 156.5        |   0.063     |\n\nTo further explore whether SICO simply generates shorter text to evade detection, we analyzed the average word number of text from AI, human and SICO-Para across three tasks. The results are presented in the following table:\n\n| **Task** | **AI** | **Human** | **SICO** |\n|----------|--------|-----------|---------------|\n| Writing  | 122.9    | 119.1       | 129.4           |\n| QA       | 93.2     | 111.7       | 104.9          |\n| Review   | 86.2     | 130.6       | 133.4           |\n\nIn writing task, three types of generated text have similar length. However, in QA and review generation task, text from human and SICO is significantly longer than AI-generated text. This finding implies that SICO does not generate shorter text to evade detection. \n\n\n\n## References\n\n[1] Kalpesh Krishna, Yixiao Song, Marzena Karpinska, John Wieting, Mohit Iyyer: Paraphrasing evades detectors of AI-generated text, but retrieval is an effective defense. NIPS 2023\n\n[2] Eric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher D. Manning, Chelsea Finn: DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature. ICML 2023\n\n[3] Vinu Sankar Sadasivan, Aounon Kumar, Sriram Balasubramanian, Wenxiao Wang, Soheil Feizi: Can AI-Generated Text be Reliably Detected? arxiv 2023\n\n[4] Xinlei He, Xinyue Shen, Zeyuan Chen, Michael Backes, Yang Zhang: MGTBench: Benchmarking Machine-Generated Text Detection. arxiv 2023"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3068/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700201372774,
                "cdate": 1700201372774,
                "tmdate": 1700201372774,
                "mdate": 1700201372774,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "p0SYLxwhDy",
                "forum": "WbR415lO2L",
                "replyto": "LtbVxyjcY5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3068/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3068/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Sincerely Looking Forward to Your Feedback"
                    },
                    "comment": {
                        "value": "We sincerely appreciate your great efforts in reviewing this paper. We believe we have addressed your concerns and clarified the misunderstandings. Given the approaching deadline, we would be grateful if you could let us know if you have any further concerns. We are open to all possible discussions and value your input."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3068/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700635968556,
                "cdate": 1700635968556,
                "tmdate": 1700635968556,
                "mdate": 1700635968556,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Uv8PqvpfPQ",
                "forum": "WbR415lO2L",
                "replyto": "p0SYLxwhDy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3068/Reviewer_KWZu"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3068/Reviewer_KWZu"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the feedback"
                    },
                    "comment": {
                        "value": "Thanks for the clarification. I still would stand by my original score considering the lack of theoretical guidance and insufficient open-source LLMs used in the validation."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3068/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700636819551,
                "cdate": 1700636819551,
                "tmdate": 1700636819551,
                "mdate": 1700636819551,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pGozpg2XYz",
                "forum": "WbR415lO2L",
                "replyto": "LtbVxyjcY5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3068/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3068/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer KWZu"
                    },
                    "comment": {
                        "value": "Thanks for your instructive feedback. We want to further address your new concerns.\n\n**Lack of theoretical guidance**. \n\nWe acknowledge the importance of theoretical guidance. However, we wish to emphasize that prompt tuning is a relatively new area in LLM, and often relies more on experimental insights and practical effectiveness. Many works in this field do not provide theoretical guidance, such as CoT prompt[1, 2].\nWe believe our extensive experiments has shown the effectiveness of our approach. \nAnd we will conduct the theory exploration in our future work.\n\n**Open-source LLMs**\n\nWe appreciate your thoughtful concern. To address it, we conducted extra experiments to examine SICO-Para with two typical open-source LLMs, Vicuna-13B and WizardLM-13B. These LLMs have smaller parameters and are less capable than GPT-3.5. The experiments are adopted on three tasks.\n\nThe results are summarized in the table below. It indicates that SICO using open-source LLMs still performs better than paraphraser baselines in most cases. We hope it will address your concern about the effectiveness of SICO using open-source LLMs. \n\n**Results on Writing task**\n\n| Method      |   GPT3-D |   GPT2-D |   DetectGPT |   Log-Rank |   AVG |\n|:------------:|:---------:|:---------:|:------------:|:-----------:|:------:|\n| -        |    0.908 |    0.848 |       0.834 |      0.914 | 0.876 |\n| Parrot      |    0.666 |    0.645 |       0.502 |      0.577 | 0.598 |\n| DIPPER      |    0.736 |    0.907 |       0.550  |      0.684 | 0.719 |\n| SICO-Wizard |    0.571 |    0.582 |       0.510  |      0.481 | 0.536 |\n| SICO-Vicuna |    0.393 |    0.777 |       0.337 |      0.375 | 0.470  |\n| SICO-GPT    |    0.239 |    0.332 |       0.149 |      0.147 | 0.217 |\n\n**Results on QA task**\n\n| Method      |   GPT3-D |   GPT2-D |   DetectGPT |   Log-Rank |   AVG |\n|:------------:|:---------:|:---------:|:------------:|:-----------:|:------:|\n| -        |    0.981 |    0.906 |       0.876 |      0.956 | 0.930  |\n| Parrot      |    0.922 |    0.837 |       0.689 |      0.806 | 0.814 |\n| DIPPER      |    0.888 |    0.962 |       0.604 |      0.782 | 0.809 |\n| SICO-Wizard |    0.721 |    0.467 |       0.561 |      0.404 | 0.538 |\n| SICO-Vicuna |    0.698 |    0.522 |       0.544 |      0.383 | 0.537 |\n| SICO-GPT    |    0.402 |    0.566 |       0.178 |      0.183 | 0.332 |\n\n**Results on Review task**\n\n| Method      |   GPT3-D |   GPT2-D |   DetectGPT |   Log-Rank |   AVG |\n|:------------:|:---------:|:---------:|:------------:|:-----------:|:------:|\n| -        |    0.925 |    0.952 |       0.808 |      0.982 | 0.917 |\n| Parrot      |    0.871 |    0.934 |       0.654 |      0.893 | 0.838 |\n| DIPPER      |    0.875 |    0.984 |       0.515 |      0.814 | 0.797 |\n| SICO-Wizard |    0.707 |    0.402 |       0.570  |      0.542 | 0.555 |\n| SICO-Vicuna |    0.524 |    0.567 |       0.619 |      0.535 | 0.561 |\n| SICO-GPT    |    0.465 |    0.264 |       0.270  |      0.300   | 0.324 |\n\n\n## Reference\n\n[1]  Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, Denny Zhou: Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. NeurIPS 2022\n\n[2] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou: Self-Consistency Improves Chain of Thought Reasoning in Language Models. ICLR 2023"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3068/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700721298380,
                "cdate": 1700721298380,
                "tmdate": 1700721591445,
                "mdate": 1700721591445,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]