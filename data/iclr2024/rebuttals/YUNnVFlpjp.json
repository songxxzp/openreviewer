[
    {
        "title": "Simple Yet Effective Spatio-Temporal Prompt Learning"
    },
    {
        "review": {
            "id": "2b5TyMEDaM",
            "forum": "YUNnVFlpjp",
            "replyto": "YUNnVFlpjp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission550/Reviewer_gHhr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission550/Reviewer_gHhr"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a lightweight and effective prompt learning paradigm for spatio-temporal prediction. Basically, it uses the idea of prompt tuning from language model, to make the pretrained model adapting to downstream tasks. The authors claim that their framework, PromptST, can effectively generalize to unseen scenarios and adapt to varying spatio-temporal data distributions. Evaluated on traffic and crime prediction datasets, PromptST is shown to achieve state-of-the-art prediction accuracy while maintaining computational\nefficiency."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Addressing distribution shift in spatial-temporal prediction is an important research problem, and therefore, this paper is well motivated.\n\n2. The proposed prompt tuning approach is lightweight as compared to full parameter finetuning solutions, and show promising performance."
                },
                "weaknesses": {
                    "value": "1. The comparisons are not fair, in that PromptST is compared with baselines without any pretraining. How promptST compared with other pretrained model? What is the # of parameters of PrompST, is the performance gain from a larger model?\n\n2. In general, it is an incremental work. Pretraining-finetuning is a widely used paradigm in various areas, including spatial-temporal prediction. This paper uses prompt tuning for efficiency considerations, while prompt tuing from NLPs can be directly applied to promptST in this paper, making the approach less intriguing."
                },
                "questions": {
                    "value": "1. It is not clear how the pretrained model is obtained in the experiments. For example, for traffic prediction, is the model pretrained on all traffic datasets used in the experiments?\n\n2. How does the performance improve with the increasing of pretrained data?\n\n3. How the prompt tuning appoach compared with other lightweight finetuning solutions, e.g., LORA?\n\n4. What is \"C\" at page 5, after eqn 6?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission550/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698463517807,
            "cdate": 1698463517807,
            "tmdate": 1699635982388,
            "mdate": 1699635982388,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "bMxnz5HbEI",
            "forum": "YUNnVFlpjp",
            "replyto": "YUNnVFlpjp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission550/Reviewer_Ajy6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission550/Reviewer_Ajy6"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a lightweight and effective prompt learning paradigm named as ProptST for spatio-temporal prediction and crime detection. The proposed paradigm due with the dynamic spatio-temporal data distribution effectively and adaptively, which improves the effectiveness of spatio-temporal prediction. Extensive experiments show the performance of PromptST."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "S1. Relevant problem of potential societal interest.\nS2. Well written paper which is easy to follow.\nS3. The proposed paradigm help the improvement of effectiveness and efficiency of spatio-temporal prediction and crime detection. The experimental evaluation is sufficient."
                },
                "weaknesses": {
                    "value": "W1. There is little figure to explain the proposed paradigm, making the part of technologies incomprehensible (e.g., how to split data into pretrained, tuned and tested).\nW2. It would be better if authors can provide a description or summary table of all datasets used. It is confusing about the dataset information such as dataset scale. Also, there is a dataset (Geolife:https://www.microsoft.com/en-us/download/details.aspx?id=52367) lost to be evaluated.\nW3. Formatting errors exist, such as the title of Table 3 in Page 6."
                },
                "questions": {
                    "value": "See the Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission550/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission550/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission550/Reviewer_Ajy6"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission550/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698653918627,
            "cdate": 1698653918627,
            "tmdate": 1699635982271,
            "mdate": 1699635982271,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "8LjGj32Gew",
            "forum": "YUNnVFlpjp",
            "replyto": "YUNnVFlpjp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission550/Reviewer_xmYg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission550/Reviewer_xmYg"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a prompt learning approach for spatiotemporal prediction tasks. It has been tested on data with distribution shift, adaptation and generalisation settings, and diverse benchmark datasets, including in traffic flow forecasting and urban crime prediction. \nThe major concern of this paper is the overclaim on the novelty and the experiment settings"
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper presents a laborious effort on prompt learning for several benchmark spatiotemporal prediction datasets."
                },
                "weaknesses": {
                    "value": "The work presented in the current paper is not the typical widely recognized pretraining-prompting paradigm.\n\nThe main concern is that some statements are overclaimed. The paper claims adaptation and generalization. However, the setting of experiments is too simple for evaluating the generalization. The pretraining set, prompt tuning set, and the test set are from the same dataset. A better generalization evaluation should be something like pretraining on one dataset, then do the tuning and testing on another dataset. An even stronger setting is to test the adaptation ability across domains such as pretraining on traffic prediction and tuning/testing on the crime prediction. \n\nOnly showing two sensor nodes one-day\u2019s data in case studies is not enough to demonstrate the adaptation/generalization. For example, the shown shift might be captured by daily patterns or weekly patterns. Hence, it is not convincing enough to support the claims."
                },
                "questions": {
                    "value": "\u2022\tWhy the performance of MTGNN is not reported in Table 1? According to Table 2, the best performance of PromptST is based on MTGNN backbone.\n\n\u2022\tNeed some clarification about the details of the experiment setting. For example, for the baselines, is the training data is [X_{t-T+1}, \nX_t] or X_{pre} (whether the X_{tun} is included in the training set of the baselines)? \n\n\u2022\tHow did you split X_{pre} and X_{tun}? From Section 4.3, the X_{tun} has three settings, but not sure what is the default one in Table 1 for example.\n\n\u2022\tFor Section 4.4, it describes 3) w/o Skip. However, in Table 6, it only has w/o data initial instead of w/o skip. What is w/o initial?\n\n\u2022\tFor case studies shown in Figure 4, which part is the data used to do prompt tuning?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission550/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698671370945,
            "cdate": 1698671370945,
            "tmdate": 1699635982180,
            "mdate": 1699635982180,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "evZmSI33sl",
            "forum": "YUNnVFlpjp",
            "replyto": "YUNnVFlpjp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission550/Reviewer_5ref"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission550/Reviewer_5ref"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the author introduced a simple yet powerful spatio-temporal prompt learning paradigm aimed at enhancing the robustness and generalization ability of spatio-temporal prediction models in the presence of dynamic distribution shifts. The framework integrates a specially designed prompt neural network into pre-trained models, which involves generating informative spatio-temporal prompt context that captures the underlying patterns and dynamics in the downstream urban data. The PromptST has significantly improved the resilience of pre-trained models to distribution shifts and enhanced their adaptability to new data, and showed remarkable effectiveness across various spatio-temporal prediction tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "\u2460\tThe experimental baseline selection is quite comprehensive.\n\u2461\tThe author performed extensive experiments on a range of spatio-temporal prediction tasks using diverse datasets to thoroughly evaluate the effectiveness, efficiency, and robustness of the framework, which can validate the superiority of the approach and enhance the reliability of the results. \n\u2462\tThe author has provided a web address for accessing the model implementation, which facilitate result reproducibility.\n\u2463\tThe experiments not only selected datasets from popular application domains within the domain but also included large-scale datasets."
                },
                "weaknesses": {
                    "value": "\u2460\tThe article does not provide a detailed explanation of Figure 1. In the methodology section, it primarily explains the model at a theoretical and formulaic level. Readers would understand it easier if there were a more detailed explanation to the structure of PromptST, even if it's simple.\n\u2461\tIn the ablation experiment, the author removed several components to demonstrate the necessity of each, but provided only data results without a detailed analysis and comparison.\n\u2462\tThe author provided a brief summary of the experimental results and concluded that PromptST performs better without delving into an in-depth analysis of the results. They also did not analyze the reasons for varying performance of PromptST under different backbones.\n\u2463\tThe article does not provide the limitations of PromptST, and it also lacks a detailed explanation of the application domains of PromptST.\n\u2464\tThe word \u201cperformence\u201d is spelled incorrectly in Table 3."
                },
                "questions": {
                    "value": "How do we apply the inspiration of the prompt-tuning techniques in the field of textual data to the PromptST model?\nComparing the data in Table 1 and Table 3, PromptST performs better in traffic prediction than in crime prediction. Why is that?\nComparing fine-tuning to training from scratch, why is the benefit of the pre-trained model uncertain?\nWhy is the impact of kernel size of TCN more significant for the crime prediction task? Are the reasons the same as those for the more pronounced impact of embedding dimensions?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission550/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission550/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission550/Reviewer_5ref"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission550/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698829182351,
            "cdate": 1698829182351,
            "tmdate": 1699635982065,
            "mdate": 1699635982065,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]