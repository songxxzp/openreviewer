[
    {
        "title": "Quantifying Anonymity in Score-Based Generators with Adversarial Fingerprinting"
    },
    {
        "review": {
            "id": "KhRkwUQAyW",
            "forum": "ouNI9pkf9g",
            "replyto": "ouNI9pkf9g",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4819/Reviewer_Kiwu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4819/Reviewer_Kiwu"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on the possible privacy breach concerns when sharing generative models. To investigate the possibility of leaking privacy in a generative model, this paper proposes an approach that can estimate the maximum probability of sensitive data being reproduced by the adversary with a specific fingerprint. Experimental results show that privacy breaches could happen when the generative models are trained on small datasets or with long training times."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed method for estimating the maximum probability of a sample being reproduced is conceptually easy to follow and reasonable.\n2. The findings on the conditions for when generative models are prone to privacy breaches are plausible, and this seems to be attributable to overfitting.\n3. Some classic attack methods, e.g., backdoor and membership inference, can be quantified using the proposed method.\n4. The experiments can evaluate the proposed method well,  which looks sufficient and convincing."
                },
                "weaknesses": {
                    "value": "1.  The experimental results are mainly conducted on the datasets from the MNIST list. Please add some other datasets to evaluate the effectiveness of the proposed method. Does the proposed method exclusively pertain to healthcare? If so, I do not see the relationship between the method and the healthcare scenario.\n2. There are a lot of attack methods not only limited to backdoor and membership inference, e.g., feature inference, model inversion, hijack, model steal, etc. This paper aims to quantify the privacy risk. However, some attacks are not privacy issues, i.e., backdoor, hijack and model steal attacks. The authors should investigate the relationship between attack methods and the proposed method.\n3. The authors should analyze the relationship between attack methods and the proposed method from both theoretical and experimental aspects. Every attack method has its own characteristics.\n4. Flaws in writing: there are some flaws in the writing, e.g., lacking a \u201c.\u201d at the end of the second point of contribution and the first paragraph of the background section. \n5. Figure 2 should be referred to in the text of this paper to help readers understand."
                },
                "questions": {
                    "value": "1.  Please include additional datasets to assess the effectiveness of the proposed method. Does the proposed method exclusively pertain to healthcare? If so, I do not see the relationship between the method and the healthcare scenario.\n2. This paper aims to quantify the privacy risk. However, some attacks are not privacy issues, i.e., backdoor, hijack and model steal attacks. The authors should investigate the relationship between attack methods and the proposed method.\n3. The authors should analyze the relationship between attack methods and the proposed method from both theoretical and experimental aspects. Every attack method has its own characteristics.\n4. Flaws in writing: there are some flaws in the writing, e.g., lacking a \u201c.\u201d at the end of the second point of contribution and the first paragraph of the background section. \n5. Figure 2 should be referred to in the text of this paper to help readers understand."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4819/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4819/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4819/Reviewer_Kiwu"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4819/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698725957314,
            "cdate": 1698725957314,
            "tmdate": 1699636465378,
            "mdate": 1699636465378,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "fFiF9WiL3V",
            "forum": "ouNI9pkf9g",
            "replyto": "ouNI9pkf9g",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4819/Reviewer_RsRi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4819/Reviewer_RsRi"
            ],
            "content": {
                "summary": {
                    "value": "This paper explores the idea of using diffusion models trained on private data as a data-sharing strategy for sensitive contexts such as healthcare and proposes a method to quantify the anonymity of their sampling process. The paper proposes a method to evaluate the memorization of training data by estimating the upper bound of the likelihood of reproducing sampling from the entire subspace of samples similar to the target sample. The authors evaluate their method with synthetic anatomic fingerprints in the form of 4px-radius gray circles used to augment several MedMNIST datasets and sunglasses in the Celeba-HQ. The experiments show that generative models trained on small datasets are more susceptible to memorization."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The use reverse diffusion process to estimate the subspace of sample similar to the target sample is original."
                },
                "weaknesses": {
                    "value": "The main limitation of the paper is the evaluation of the proposed solution. While the motivation indicates realistic fingerprint (e.g., skin tattoo, implant, heart monitor), the experiments consider simple scenarios (gray circles, unique sunglasses), which do not help appreciate the true capability of the solution. Additionally, the paper appears to use lack of memorization and lack of privacy risks interchangeably. There are other forms of privacy risks, such as attribute inference or property inference, that could still prevent the sharing of generative models even if memorization is addressed. I suggest authors clarify that aspect in the paper."
                },
                "questions": {
                    "value": "- In a realistic setup, there could be different variants of the same fingerprint on multiple samples. How would the solution perform in such a case?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4819/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698771925658,
            "cdate": 1698771925658,
            "tmdate": 1699636465236,
            "mdate": 1699636465236,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "PGGOnKD90p",
            "forum": "ouNI9pkf9g",
            "replyto": "ouNI9pkf9g",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4819/Reviewer_kVLX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4819/Reviewer_kVLX"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates the potential privacy issues associated with score-based generative models, specifically by introducing a method for estimating the (upper bound of the) probability that identifiable training images will be reproduced during the sampling/generation process. Experimental results validate the concern that images with identifiable fingerprints can be reproduced at sampling time if the models are trained without appropriate privacy safeguards."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- Investigating potential privacy issues associated with the use of generative models is an interesting topic and is of practical value.\n\n- The idea of estimating regeneration probability using likelihoods derived from the training objective and generation process should be a valid and promising approach that aligns well with the principles of score-based generative modeling."
                },
                "weaknesses": {
                    "value": "1. The key contributions of this submission are somewhat unclear:\n   - The main finding\u2014that recent generative models are likely to compromise privacy and potentially copyright\u2014has been discussed in earlier publications, some of which are cited in this submission. This observation raises questions about the novelty of this finding. Furthermore, the scenario presented, which focuses on \"anomalies\", does not seem to represent an improvement over existing ones in terms of practical relevance. Stronger results have been reported in the literature, indicating that even neutral-looking images, not just anomalies, could be memorized and reproduced by these models.  \n   - Additionally, if the authors would like to claim that their proposed approach is a significant contribution, they need to provide a detailed comparison with existing methods. This should include a clear description of its application scenarios (see point 2 below). \nSpecifically, the submission could address application examples such as membership inference and backdoor attacks, and demonstrate how the proposed approach differs from or improves upon existing methods. To the best of my knowledge, there are indeed numerous studies (>5) on these scenarios (which also includes some likelihood-based estimations), offering approaches that may even have broader applicability (e.g., applicable to general generative models, not just the score-based models discussed here).\n\n2. The potential application scenarios (i.e., the threat model) of the proposed approach are not clearly stated. This ambiguity further negatively affects my assessment of the submission\u2019s contributions. For example, it would be necessary to state how the approach would be used in practice (by a defender or an attacker) and which knowledge (about the model, training configuration, data distribution, etc.) is required for executing the method."
                },
                "questions": {
                    "value": "- The definition of 'private region' using an additional classifier seems to be either non-rigorous or too computationally intensive in practice. Classifying each individual ID would quickly become infeasible for relatively large datasets. Moreover, other more scalable methods might conflict with the proper definition of privacy violations, since simply generating similar-looking images through generalization or hallucination should be permissible. Conversely, for small-scale datasets, the value of the study to the community might be questioned, considering that memorization and overfitting are widely recognized issues.\n\n- The structure of the presentation in the methods section could be further refined to improve the logical flow within subsections 3.1-3.4. Specifically, including a pseudocode representation of the overall pipeline within the main paper could greatly aid understanding.\n\n-  Section 3.4: the meaning of $M$ and $q_M(p|x_{t,p})$ seem not explained and how would these be computed/selected seems missing from the main paper. However, I found this quite critical for understanding the overall approach.\n\n- Section 2: directly stating $p_{\\sigma_1}(\\tilde{x} | x) \u223c p_{data}(x)$ without constraining $\\sigma_1$ looks not rigorous"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4819/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699109414474,
            "cdate": 1699109414474,
            "tmdate": 1699636465155,
            "mdate": 1699636465155,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]