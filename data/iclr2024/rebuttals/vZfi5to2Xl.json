[
    {
        "title": "SAS: Structured Activation Sparsification"
    },
    {
        "review": {
            "id": "VigHoxBPQ8",
            "forum": "vZfi5to2Xl",
            "replyto": "vZfi5to2Xl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission73/Reviewer_N3vE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission73/Reviewer_N3vE"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new concept of sparsity that maps the input activation values to a sparse representation, then exploits Nvidia Ampere sparsity for sparse computation by widening the weights. By doing this, wider weights make the network have stronger representation ability, and computation remains consistent because of the n:m sparsity operation. The authors conducted experiments on CIFAR and ImageNet datasets to verify the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The presented idea is clever and novel. It can effectively enhance the representation ability of the network under the same amount of computation compared with the vanilla network. I think this brings a nice insight to the field.\n2. The author developed a general matmul library for the proposed SAS using Sparse Tensor Core. This gives the method practical value in the field, which is appreciated."
                },
                "weaknesses": {
                    "value": "1. It is inaccurate for the author to state that the computation of SWS and SAS are the same. I can agree that the computation of SAS and the original dense network are the same, but the computation of SWS is obviously higher than that of SAS.\n2. Let's continue considering this point. Figure 3 raises a big question for me. The author doesn't express the dimensions of the weights corresponding to SWS and SAS. Although it intuitively looks like SAS has a clear acceleration in the graph, I think this is unreasonable because the author also says the number of mult count in SAS is the same as the base dense/narrow network. This greatly reduces my enthusiasm for this paper.\n3. The current presentation pf experiments is very scattered. I think the author should provide a comparison of the full network\u2019s computation, inference speed, and accuracy to give readers a clearer comparison. For instance, when compressing ConvNeXt-b, although the accuracy of SAS is higher than SWS, a comparison of SAS's operation count and inference time also needs to be provided."
                },
                "questions": {
                    "value": "Please see the weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission73/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission73/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission73/Reviewer_N3vE"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission73/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698634294712,
            "cdate": 1698634294712,
            "tmdate": 1700528843173,
            "mdate": 1700528843173,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "s6CAnLRX5R",
                "forum": "vZfi5to2Xl",
                "replyto": "VigHoxBPQ8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission73/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission73/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response (1/3)"
                    },
                    "comment": {
                        "value": "We really appreciate the insightful comments by *N3vE*.\nEspecially, *N3vE*'s comments helped us to clarify the potential inaccuracy and confusion about the experimental configuration. \nWe consider that the following first three concerns kindly raised by *N3vE* come from essentially the same source of inaccuracies in the original manuscript regarding the comparison configuration of dense, SAS, and SWS: compare accuracy by increasing network width without increasing $\\texttt{mult}$  count or comparing wall-clock time by changing sparse pattern while keeping the same matrix size.\nWe revised the manuscript as follows, and we are now confident that the revised manuscript and the following answer clarify the concerns.\n\n>It is inaccurate for the author to state that the computation of SWS and SAS are the same. I can agree that the computation of SAS and the original dense network are the same, but the computation of SWS is obviously higher than that of SAS.\n\n>The author doesn't express the dimensions of the weights corresponding to SWS and SAS. \n\n>Although it intuitively looks like SAS has a clear acceleration in the graph, I think this is unreasonable because the author also says the number of $\\texttt{mult}$ count in SAS is the same as the base dense/narrow network.\n\nIn this paper, we propose SAS to increase the network capacity to improve the accuracy without increasing the actual $\\texttt{mult}$ count.\nTherefore, we compare the SWS in the same scenario (keeping the $\\texttt{mult}$ count constant while increasing the sparsity).\nSpecifically, we consider a base layer consisting of $\\texttt{matmul}$ between activation $\\mathbf{X}$ and weight $\\mathbf{W}$, where $\\mathbf{X}\\in \\mathbb{R}^{\\bar{C}_i\\times {HW}}$ and $\\mathbf{W}\\in \\mathbb{R}^{ {C}_o \\times\\bar{C}_i}$ (eq. (1)).\nAs *N3vE* explains, using the proposed SAS could increase the network width for $M$ times while maintaining the same $\\texttt{mult}$ count as the base layer by utilizing the 1:$M$ sparsity pattern.\nThe sparsified matrix shape is, $\\mathbf{X}\\in \\mathbb{R}^{M\\bar{C}_i\\times {HW}}$ and $\\mathbf{W}\\in {C}_o\\times \\mathbb{R}^{M\\bar{C}_i}$ (eq. (2)).\nSAS does not change the output channel dimension ${C}_o$. \n\nOn the other hand, in the case of SWS, if one increases the network width for the $M$ times and uses the 1:$M$ sparse pattern on weight, the $\\texttt{mult}$ count of the resultant network increases by about $M\\times M/M=M$ because both input and output channel is $M$ times wider.\nWe consider *N3vE*\u2019 kindly point out this issue.\n\nIn the case of SWS, by using $\\sqrt{M}$ times wider network for the 1:$M$ sparsity pattern, we can construct the SWS network, which has roughly the same $\\texttt{mult}$ count as the base dense network and SAS network (section 2.3).\nThanks to the questions raised by *N3vE*, we find the original manuscript was unclear from three perspectives: 1) the explanation in the original manuscript \"$\\sqrt{M}$ times wider input/output channel\" was insufficient and needs more detail, 2) Although we adopt the opposite benchmarking configuration for the speed benchmarking in fig. 3 (we keep the same matrix dimension while changing the sparse pattern), we failed to explain this difference clearly, which may confuse the reader about the interpretation of the speed benchmarking result (and also the accuracy benchmarking result), 3) The reason why we evaluate the speed using the  $\\texttt{matmul}$  of the same shaped matrix was not clearly stated.\nWe've updated the manuscript to clarify the above three issues as follows;\n\n###  Issue 1) Configuration of SWS network\nThe original manuscript lacks the following detailed discussion.\nWhen the network is $\\sqrt{M}$ times (instead of $M$ times) wider with 1:$M$ sparsity pattern in weight, the $\\texttt{mult}$ count will be the same as the base network.\nBut, the network width needs to be an integer value, and it also needs to be multiple of $M$.\nTherefore, we use the weight having the shape of  $\\lfloor\\sqrt{M}\\bar{C}_i^{(l)} \\rceil\\times \\lfloor\\sqrt{M}{C}_o^{(l)} \\rceil$ for the 1:$M$ SWS network to get approximately the same $\\texttt{mult}$ count as the base network, where $\\lfloor\\cdot\\rceil$ is a rounding operator,  $\\bar{C}_i^{(l)}$, ${C}_o^{(l)}$ is the input/output channel dimension of the base dense network. \nFor the last chunk, when it does not equal to $M$, we use $(\\lfloor\\sqrt{M}\\bar{C}_i^{(l)} \\rceil  \\lfloor\\sqrt{M}{C}_o^{(l)} \\rceil \\mod M)$:1 sparse pattern.\nIn this way, we construct the SWS network having almost the same $\\texttt{mult}$ count as possible."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission73/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700183522863,
                "cdate": 1700183522863,
                "tmdate": 1700520627445,
                "mdate": 1700520627445,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WPFzD8DsKQ",
                "forum": "vZfi5to2Xl",
                "replyto": "VigHoxBPQ8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission73/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission73/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response (2/3)"
                    },
                    "comment": {
                        "value": "Let's consider a specific example, \nthe $l$-th layer of the base network consisting of convolution with $\\bar{C}_i$ = 288 ($C_i$ is 32, kernel size $k$ is 3) and $C_o$ = 64.\nWhen $M$=8, then  We have $\\lfloor\\sqrt{M}\\bar{C}_i^{(l)} \\rceil$=815 and $\\lfloor\\sqrt{M}{C}_o^{(l)} \\rceil$=181.\nThen $\\texttt{mult}$ count of the original dense layer (for single pixel) is 288$\\times$64=18432, the $\\texttt{mult}$ count of the weight sparse layer (SWS) is 18440 (815$\\times$181/8=18439 with modulo 3, we use M=3 for the last chunk).\nThe comparison of the layer shape in this example case is summarized as follows:\n\n|    |  input $\\bar{C}_i\\rightarrow \\tilde{C}_i$ |  output ${C}_o$ |  FLOPS | weight shape  | activation shape  |\\\\\\\n| -------- | ------- |------- |------- |------- |------- |------- |\n| base (dense)    | 288 | 64  | 18432  | 64$\\times$288 |  288$\\times$HW |\n |SAS ($M$=8)   | 288 $\\rightarrow$ 2304 | 64  | 18432 | 64$\\times$2304  |  2304$\\times$HW (1:8 sparse)|\n| SWS ($M$=8)   | 815 | 181  | 18440 | 181$\\times$815 (1:8 sparse) |  815$\\times$HW|\n\n### Issue 2 Configuration for speed benchmarking\nOn the wall-clock speed benchmarking reported in fig. 3, we use the opposite configuration, changing the sparsity pattern while keeping the matrix dimension unchanged to report more concrete comparisons.\nThe original manuscript did not emphasize this significant configuration difference, leading to a misunderstanding about the interpretation of the results. \nFurthermore, symbols used to describe the matrix shape ($\\mathrm{M}, \\mathrm{N}$) were confusing with the ones used for specifying the sparsity pattern ($N, M$).  \n\nIn the wall-clock speed benchmarking in fig. 3, we consider the $\\texttt{mult}$  $\\mathbf{W}\\mathbf{X}$ where $\\mathbf{X}\\in \\mathbb{R}^{\\mathrm{\\gamma} \\times \\mathrm{\\alpha}}$ and $\\mathbf{W}\\in \\mathbb{R}^{\\mathrm{\\beta}\\times \\mathrm{\\gamma}}$  **which is identical for dense $\\texttt{mult}$, SAS $\\texttt{mult}$, and SWS $\\texttt{mult}$**.\nThe $\\texttt{mult}$ count of the three variants is the same $(\\mathrm{\\gamma}\\times \\mathrm{\\alpha}\\times \\mathrm{\\beta})$ when one does not take the sparsity into account.\nBy utilizing the 1:2 fine-grained (semi-structured) sparsity on *Sparse Tensor Core*, the $\\texttt{mult}$count becomes half for SAS and SWS, i.e., $(\\mathrm{\\gamma}\\times \\mathrm{\\alpha}\\times \\mathrm{\\beta})/2$.\nWe show the number of dimensions of the weight and activation for wall-clock time benchmarking in the graph (we use fixed $\\mathrm{\\gamma}$=10240, changing $\\mathrm{\\alpha}=\\mathrm{\\beta}$ from 10240 to 20480), which is the same for dense $\\texttt{matmul}$ SAS $\\texttt{matmul}$, and SWS $\\texttt{matmul}$.\n\nAgain, we want to emphasize that the scenario in this speed benchmarking (keep the same matrix dimension) is different from the scenario for neural network (keep the (almost) same $\\texttt{mult}$ count). \nThe statement \"the number of $\\texttt{mult}$ count in SAS is the same as the base dense/narrow network\" is not meant for this speed evaluation experiment but for our main application scenario of the SAS for neural networks (section 3, section 4).\n\n### Issue 3 Reason for the speed benchmarking configuration\nAs discussed above, we adopt the different configurations to evaluate the speed in a fair setting between SWS and SAS.\nWe can construct an SWS network having approximately the same  $\\texttt{mult}$ count with base dense network and SAS network by adjusting the network width (as discussed in issue 1); however, it is not precisely the same, and the shape of the matrices will be different (please refer to the table in issue-1), which makes it hard to evaluate the overhead specific to SAS which we are interested in (index related operation).\nWhile we can precisely evaluate this by measuring the time of the $\\operatorname{matmul}$ of the same-sized matrix (section 2.3, supp. B). \n\nTo clarify the above point, we've made the following changes.\n* Added the details about constructing the SWS network with the similar $\\texttt{mult}$ count as discussed above (section 2.3). We've also clarified in the experimental section (section 3, section 4) that the $\\texttt{mult}$ count of the SWS network is not exactly the same as the dense or SAS network, but it is approximately the same.\n* Moved the speed benchmarking to a separate paragraph and clarified the distinction between the two benchmark setups as discussed above (section 2.3).\n* Changed the symbols  of the matrix size from $\\mathrm{K}, \\mathrm{M}, \\mathrm{N}$ to $\\mathrm{\\gamma}, \\mathrm{\\alpha}, \\mathrm{\\beta}$ to avoid the notational clutter with the sparsity pattern $M, N$ (section 2.3, fig. 3)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission73/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700184276802,
                "cdate": 1700184276802,
                "tmdate": 1700525761050,
                "mdate": 1700525761050,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vphyHCJiHU",
                "forum": "vZfi5to2Xl",
                "replyto": "VigHoxBPQ8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission73/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission73/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response (3/3)"
                    },
                    "comment": {
                        "value": "> I think the author should provide a comparison of the entire network\u2019s computation, inference speed, and accuracy to give readers a clearer comparison. For instance, when compressing ConvNeXt-b, although the accuracy of SAS is higher than SWS, a comparison of SAS's operation count and inference time also needs to be provided.\n\nRelating to the above discussion, the $\\texttt{mult}$ count for the SAS and SWS networks are (almost) the same for the neural networks experiment.\nIn the case of ConvNeXt-b, the FLOPS of SAS and base dense network is 3.85M, and the FLOPS of SWS is also 3.85M.\nWe've added the $\\texttt{mult}$ count and number of parameters in the supp. D (tab. A2), including the ResNet18.\n\nWe report the wall-clock speed benchmarking in fig. 3, demonstrating that SAS-based multiplication can speed up the $\\texttt{matmul}$  on actual hardware.\nThe overhead of SAS over SWS is the online computation of the index (eq. 3), reorder (supp. E), and the memory transfer of the index.\nOur experiment shows that it is only a faction (1.5\\%) of the entire  $\\texttt{matmul}$ in terms of the wall-clock time.\nHowever, as *N3vE* points out, we have yet to benchmark the speed as a whole neural network.\nThere are two reasons why we evaluate the wall-clock time using  $\\texttt{matmul}$ of the same-sized matrix (fig. 3) instead of the entire network.\n\n* It is hard to exactly match the $\\operatorname{mult}$ count between SWS and SAS network, and the matrix shape vastly differs between the two, making the evaluation of SAS-specific computation (online index computation) time difficult (please also refer to the discussion in the previous question). In contrast, we can precisely evaluate this by measuring the $\\texttt{matmul}$ time of the same-sized matrix (section 2.3, supp. B). \n * Due to the limited high-level API to utilize the *SparseTensorCore* (e.g., *cuSPARSELt*. We need to develop a custom CUDA kernel using a low-level library (e.g., [Cutlass](https://github.com/NVIDIA/cutlass) to deal with the specific $\\texttt{mult}$ operation such as convolution or depth-wise convolution to evaluate the wall clock time as a whole network.\n\nWe are now developing a custom CUDA kernel and will publish the source code.\nWe expect slightly better results (regarding the percentage of SAS-specific overhead) than the result in fig. 3 will be achieved with the custom CUDA kernel (section 2.3, section 6.1) because we can integrate index computation, reorder (supp. E), and memory copy, eliminating unnecessary DRAM access.\n\nThanks to *N3vE*, we noticed the above-discussed critical discussion about the wall-clock benchmarking is insufficient in the original manuscript.\nWe've added the discussion in section 2.3."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission73/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700184513272,
                "cdate": 1700184513272,
                "tmdate": 1700542710367,
                "mdate": 1700542710367,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MsevyyQ0E6",
                "forum": "vZfi5to2Xl",
                "replyto": "vphyHCJiHU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission73/Reviewer_N3vE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission73/Reviewer_N3vE"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Reviewer N3vE"
                    },
                    "comment": {
                        "value": "Thanks for the author's response, which has successfully laid to rest my concerns. Consequently, I am increasing my rating. Nonetheless, I enthusiastically encourage the author to refine their current method and experimental description, as it indeed could easily lead to confusion, akin to what I previously experienced."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission73/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700528807883,
                "cdate": 1700528807883,
                "tmdate": 1700528807883,
                "mdate": 1700528807883,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NVQqZtlmls",
            "forum": "vZfi5to2Xl",
            "replyto": "vZfi5to2Xl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission73/Reviewer_W2qU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission73/Reviewer_W2qU"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed SAS, a method to explore structured sparse activation in CNN. SAS describes the approach to generate structured sparse activation, software and hardware implementations. Numerical experiments on image classification validates the efficacy regarding accuracy aspect."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is written well, and technically sound.\n- The study problem is interesting and may deliver real impact onto DNN speedup."
                },
                "weaknesses": {
                    "value": "- The terminology of structured sparsity is misleading. We typically refer the N:M sparsity as \"semi structured sparsity\" to distinguish it from standard structured sparsity including disjoint group sparsity, overlapping group sparsity and hierarchical sparsity. \n\n- The realistic benefits of structured sparse activation is not clear. Although the topic is interesting, I am not sure what is the actual speedup gain of such sparse activation that can deliver to the community. The paper seems equipping without numerical results regarding speedup as well.\n\n- The citation format is wrong. Please use \\citep{} rather than \\cite{} to cite references."
                },
                "questions": {
                    "value": "- What is the training cost to yield SAS network compared to training as standard?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission73/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission73/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission73/Reviewer_W2qU"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission73/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698645216651,
            "cdate": 1698645216651,
            "tmdate": 1700542140207,
            "mdate": 1700542140207,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jLtKEmYUjw",
                "forum": "vZfi5to2Xl",
                "replyto": "NVQqZtlmls",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission73/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission73/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response (1/1)"
                    },
                    "comment": {
                        "value": "Thanks for the insightful and sharp comments.\nIn particular, suggestions about the terminology of $N$:$M$ sparsity helped us a lot to clarify and avoid unintended confusion about the scope and the contribution of the proposed study.\nWe are now sure that the revised manuscript incorporates the raised concerns. \n\n>The terminology of structured sparsity is misleading. We typically refer to the $N$:$M$ sparsity as \"semi-structured sparsity\" to distinguish it from standard structured sparsity including disjoint group sparsity, overlapping group sparsity and hierarchical sparsity.\n\nThank you for the suggestion.\nWe agree with *W2qU* that the term \"semi-structured\" well describes the $N$:$M$ sparsity.\nWe used the term \"Fine-grained structured\" sparsity following [NVidia's official whitepaper](https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf) and use structured afterward for brevity (section 1).\nThanks to the *W2qU*'s comments, we find the original manuscript was unclear on this point.\nWe clarified the terminology in section 1 and added the note that this is also called \"semi-structured\" to give a clear distinction from the standard structured sparsity.\n\n>The realistic benefits of structured sparse activation is not clear. Although the topic is interesting, I am not sure what is the actual speedup gain of such sparse activation that can deliver to the community. The paper seems equipping without numerical results regarding speedup as well.\n\nWe report the wall-clock speed benchmarking in fig. 3, demonstrating that SAS-based multiplication can speed up the $\\texttt{matmul}$  on actual hardware.\nThe overhead of SAS over SWS is the online computation of the index (eq. 3), reorder (supp. E), and the memory transfer of the index.\nOur experiment shows that it is only a faction (1.5\\%) of the entire  $\\texttt{matmul}$ in terms of the wall-clock time.\nHowever, we've evaluated the time for a single  $\\texttt{matmul}$ (with varying sizes), and we have not benchmarked the speed as a whole neural network.\nThere are two reasons why we evaluate the wall-clock time using  $\\texttt{matmul}$ of the same-sized matrix instead of the entire network.\n1.  It is hard to exactly match the $\\operatorname{mult}$ count between SWS and SAS network, and the matrix shape differs largely between the two, making the evaluation of SAS-specific computation (online index computation) time difficult. In contrast, we can clearly evaluate this by measuring the time of the $\\operatorname{matmul}$ of the same-sized matrix (section 2.3, supp. B). This discussion also relates to the *N3vE*'s question; please refer to the response for the specific example of why this is the case. \n1. Due to the limited high-level API to utilize the *SparseTensorCore* (e.g., *cuSPARSELt*). We need to develop a custom CUDA kernel using a low-level library (e.g., [Cutlass](https://github.com/NVIDIA/cutlass) to deal with the specific $\\texttt{mult}$ operation such as convolution or depth-wise convolution to evaluate the wall clock time as a whole network.\n\nWe are now working on the development of the custom CUDA kernel and will publish the source code.\nWe expect slightly better results (in terms of the percentage of SAS-specific overhead) than the result in fig. 3 will be achieved with the custom CUDA kernel (section 2.3, section 6.1) because we can integrate index computation, reorder, and memory copy.\n\nThanks to the *W2qU*'s comment, we find that the original manuscript failed to discuss this important motivation about the wall-clock benchmarking.\nWe've added the discussion in section 2.3 and section 6.1.\n\n>The citation format is wrong. Please use \"citep\" rather than \"cite\" to cite references.\n\nThank you for pointing out this.\nWe've fixed all the citation formats."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission73/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700184910726,
                "cdate": 1700184910726,
                "tmdate": 1700542813025,
                "mdate": 1700542813025,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vpB1oxfgMH",
                "forum": "vZfi5to2Xl",
                "replyto": "jLtKEmYUjw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission73/Reviewer_W2qU"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission73/Reviewer_W2qU"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the responses. My concerns have been addressed. Therefore, I raised my rating."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission73/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700542129067,
                "cdate": 1700542129067,
                "tmdate": 1700542129067,
                "mdate": 1700542129067,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "D0MBjEyKfw",
            "forum": "vZfi5to2Xl",
            "replyto": "vZfi5to2Xl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission73/Reviewer_C5D8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission73/Reviewer_C5D8"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces Structured Activation Sparsification (SAS), a method that enhances the accuracy of wide neural networks without the additional computational cost typically associated with network width. By implementing structured sparsity within activation maps\u2014where a set number of non-zero values are maintained in consecutive activation. This allows for the simplification of wide matrix multiplications into narrow ones. Empirical results show that this method can improve accuracy (by up to 7% on CIFAR10) without increasing computational demands and outperforms similar sparsity approaches applied to network weights."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The strengths of this work is listed below:\n1. This work introduces a novel method of structuring sparsity in activations, which appears to enable a reduction in computation without a corresponding drop in accuracy. The concept of using structured sparse projection (SAS) to maintain vectorization compatibility is particularly innovative.\n2. The structured sparsity allows for efficient matrix multiplication operations that maintain the number of multiplications at the level of the base dense/narrow network, highlighting efficiency in computation.\n3. The process for creating sparse activations through the structured sparse projection S is described as having negligible computational cost and wall-clock latency, indicating an efficient method that does not add significant complexity or processing time.\n4. A thorough evaluation is presented that demonstrates the SAS network's increased expressiveness compared to Static Weight Sparsity (SWS) networks, given the same computational budget, by using trajectory length analysis.\n5. The proposed SAS projection and its integration into the training process suggest a straightforward transformation of existing neural networks to increase their efficiency, which could be widely applicable across different network architectures and tasks."
                },
                "weaknesses": {
                    "value": "The weakness of this work is listed below:\n1. As mentioned in Section 2.3, while the computational load in terms of FLOPS remains the same for a given level of activation sparsity \nM, the memory requirements for the Sparse Activation Sparsification (SAS) network increase linearly with M. This is in contrast to the Sparse Weight Storage (SWS) network, which maintains a constant storage requirement for weights at inference time regardless of M. This could become a significant limitation for devices with limited memory or when scaling to very large networks.\n2. In Section 3, the paper discusses the expressive power of SAS networks by comparing trajectory lengths in a specific constructed neural network with 2-dimensional input and output. However, this analysis might not generalize to all network architectures or datasets, which could limit the understanding of the practical implications of SAS's increased expressive power.\n3. The straightforward method used for computing the index I might not be the most effective approach, particularly when neighbor elements oscillate around zero (Section 6.1). An indexing strategy that is not learned end-to-end may limit the model's capacity to adapt to the data's complexity, potentially leaving some performance on the table.\n4. In general, the listings (code pieces) in the paper is informative enough. However, I would suggest the authors to replace the source code with some high-level pseudo code. This is more readable and more accessible to some readers."
                },
                "questions": {
                    "value": "I do not have other questions. Please refer to the weakness column for my comments."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission73/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699158315609,
            "cdate": 1699158315609,
            "tmdate": 1699635931467,
            "mdate": 1699635931467,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rscxT1TRN3",
                "forum": "vZfi5to2Xl",
                "replyto": "D0MBjEyKfw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission73/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission73/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response (1/1)"
                    },
                    "comment": {
                        "value": "We appreciate the positive and encouraging feedback and insightful comments from *C5D8*.\nWe answer each of the valuable comments as follows.\n\n>As mentioned in section 2.3, while the computational load in terms of FLOPS remains the same for a given level of activation sparsity $M$, the memory requirements for the Structured Activation Sparsification (SAS) network increase linearly with $M$. \nThis is in contrast to the Structured Weight Sparsification (SWS) network, which maintains a constant storage requirement for weights at inference time regardless of $M$. \nThis could become a significant limitation for devices with limited memory or when scaling to very large networks.\n\nAs *C5D8* points out, in contrast to SWS, the memory requirement of SAS increases linearly with $M$ (section 2.3). \nActually, this is the main drawback of the proposed method  (section 2.3, section 4.2).\nWe expect this could be mitigated by using quantized weight (section 5), which we left for future work.\nFrom a practical perspective, commercially available hardware (at the time of submission) supports 50\\% sparsity; doubling the memory might be a practical choice, especially when the FLOPS is critical, considering the significant accuracy gain from M=0 (dense) to $M$=2 (50\\% sparse).\nWe clarified this point in (section 2.3 and section 5).\n\n\n>In section 3, the paper discusses the expressive power of SAS networks by comparing trajectory lengths in a specific constructed neural network with 2-dimensional input and output. However, this analysis might not generalize to all network architectures or datasets, which could limit the understanding of the practical implications of SAS's increased expressive power.\n\nWe agree with *C5D8* about the interpretation of the trajectory length analysis.\nStill, we consider the analysis gives important implications about the expressive power of the model, which does not depend on data, model, or several training details that affect the accuracy (It often happens, for example, model A performs better than B when using a specific optimizer, learning rate, and learning epochs; but model B beats model A when adopting different training scheme).\nThe trajectory length analysis does not depend on these settings.\nIn section 4, we present results using a practical model and dataset, and we hypothesize the increased accuracy by adopting the SAS is attributed to the increased expressive power, which conforms to the trajectory length analysis results.\nThanks to the *C5D8*'s comments, we find the above motivation for utilizing the trajectory length analysis was missing in the original manuscript (we only mentioned \"more generic settings\"); we've added the above discussion at the beginning of the trajectory length analysis (section 3).\n\n>The straightforward method used for computing the index I might not be the most effective approach, particularly when neighbor elements oscillate around zero (section 6.1). An indexing strategy that is not learned end-to-end may limit the model's capacity to adapt to the data's complexity, potentially leaving some performance on the table.\n\nWe agree on this point; however, the oscillation phenomenon happens when $M$>2 and will be several when it is large, e.g., >16 (section 6.1).\nWe consider the proposed simple index computation mechanism of e.q (3) to be one of the most effective approaches, at least for $M$=2, because there are no oscillations, and it is computationally efficient.\nThanks to the  *C5D8*\u2019s comment, we noticed that the original manuscript could be more precise on this point.\nWe've added the above discussion in section 6.1.\n\n>In general, the listings (code pieces) in the paper are informative enough. However, I would suggest the authors replace the source code with some high-level pseudo code. This is more readable and more accessible to some readers.\n\nWe present the source code for inference to demonstrate that the SAS operation is realizable on actual commercial hardware (listing-1).\nWe also show the source code for training to demonstrate the conversion from the existing dense layer to the SAS layer for training is simple and trivial (listing-2).\nWe agree with *C5D8*'s suggestion for the importance of high-level explanation.\nFollowing the suggestion, we further clarified the method during the training, and now we consider listing-2 to be somewhat redundant; therefore, we moved it to supplement for interested readers (and added more important discussion regarding the valuable comments from all the reviewers). \nRegarding listing-1 (Code for inference on actual hardware), we keep it in the main text to showcase that the SAS could actually be compatible with commercial hardware.\nHowever, thanks to the *C5D8*'s comments, we noticed some details could be more precise to follow; therefore, we've clarified the high-level explanation in sections 2.1-2.3 and supp. E along with the visual description."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission73/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700185127906,
                "cdate": 1700185127906,
                "tmdate": 1700216859163,
                "mdate": 1700216859163,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HWHIg5EbnP",
                "forum": "vZfi5to2Xl",
                "replyto": "rscxT1TRN3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission73/Reviewer_C5D8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission73/Reviewer_C5D8"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response and clarification."
                    },
                    "comment": {
                        "value": "I have read through the rebuttal and I do not have other questions. I am positive about this work but I will not further increase my score. Thanks the authors again."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission73/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700460557061,
                "cdate": 1700460557061,
                "tmdate": 1700460557061,
                "mdate": 1700460557061,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]