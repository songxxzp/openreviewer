[
    {
        "title": "Learning the Unlearnable: Adversarial Augmentations Suppress Unlearnable Example Attacks"
    },
    {
        "review": {
            "id": "N0fxeLf8ln",
            "forum": "2UxSXuzrap",
            "replyto": "2UxSXuzrap",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2202/Reviewer_3r2o"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2202/Reviewer_3r2o"
            ],
            "content": {
                "summary": {
                    "value": "Unlearnable example attacks are used to safeguard public data against unauthorized training of deep learning models. To break the data protection of unlearnable example methods, this paper introduces UEraser, an adversarial augmentation (PlasmaTransform, TrivialAugment and ChannelShuffle) based method to help model to learn semantic information from unlearnable example. Similarly to adversarial training, UEraser tries to find an augmentation to maximize the training loss rather than to find a perturbation to maximize the training loss (adversarial training does)."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper focuses on the under-explored topic to learn semantic information from unlearnable examples, which is interesting and impressive.\n2. This paper is organized logically and written clearly. The visualization results of different unlearnable example methods are impressive.\n3. Impressive accuracy improvements on on four datasets (CIFAR-10, CIFAR-100, ImageNet, SVHN) compared with standard training."
                },
                "weaknesses": {
                    "value": "1. The motivation and insight of this paper are not clear. As mentioned in Sec. 1 \" These perturbations can form \u201cshortcuts\u201d [12, 16] in the training data to prevent training and thus make the data unlearnable in order to preserve privacy.\", [12,16] does not reveal that unlearnable perturbations are shortcuts from models. Such two references are not suitable and convincing. Is there any observation or experiment to confirm that unlearnable perturbations are shortcuts from models? The same confusion occurs in Sec. 3.2 \" Geirhos et al. [12] reveal that models tend to learn \u201cshortcuts\u201d, i.e., unintended features in the training images.\". [12] only expounded that models prefer to learn shortcuts. Whether and why unlearnable perturbations are shortcuts from models is not clear at all.\n2. The explanation of why UEraser performs better than adversarial training is self-contradictory. As mentioned in Sec. 3.2, this paper believed that UEraser augmentation policies more effectively preserve the original semantics in the image than adversarial training. However, adversarial training set a $\\ell_{p}$ bound to constraint larger change on original input. The distance between adversarial training input and original input is much more small than the distance between UEraser augmented input and original input. In this case, whether UEraser augmentation policies more effectively preserve the original semantics in the image than adversarial training is quite doubtful. This paper should focus on analyzing why UEraser augmentation works and adversarial training does not. More insights should be proposed rather than engineering experiments.\n3. As shown in Table 2, CutOut, CutMix does not work on unlearnable examples, but ChannelShuffle does. Not sure if all spatial transformation-based augmentation does not work and color transformation-based augmentation does? If so, why not filter out spatial transformation-based augmentation in TrivialAugment like shear and rotate to improve performance?\n4. Clerical errors in Sec. 3.2. \"Compared to UEraser, although UEraser-Lite may not perform as well as UEraser on most datasets, it is more practical than both UEraser-Lite and adversarial training due to its faster training speed.\" should be \"Compared to UEraser, although UEraser-Lite may not perform as well as UEraser on most datasets, it is more practical than both UEraser and adversarial training due to its faster training speed.\""
                },
                "questions": {
                    "value": "1. It is confused about the experimental setup. In Sec. 4 this paper resizes all images of ImageNet-subset to 32x32. However, as mentioned in Table 8, the input size of ImageNet-subset is 224x224x3. Which one is the real experimental setup? In addition, the operation of resize should not be implemented to ImageNet-subset, because the efficient of UEraser should be verified on high-resolution images perturbed by unlearnable methods, which are aligned with the experimental setup of EM and REM.\n2. This paper repeats that UEraser-Lite has a fast training speed. How fast it is? I'd like to know the comparison results of execution time between UEraser (UEraser-Lite, UEraser, UEraser-Max) and adversarial training.\n3. Why the augmentation of UEraser-Lite is Channleshuffle? Is there any other augmentation (equalize, posterize, plasmabrightness) that can achieve the same result as Channleshuffle?\n4. Table 2 and Table 3 should add the results of UEraser trained model on clean data. Considering as an attacker, you have no idea whether the training is clean or not. The results of adversarial training and other unlearnable methods (AR, OPS, TAP, NTGA, HYPO) in Table 3 should be shown out.\n5. There is no sensitivity analysis of hyperparameter W. How to pick a suitable value of W and K when deploying UEraser?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2202/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698599233965,
            "cdate": 1698599233965,
            "tmdate": 1699636154251,
            "mdate": 1699636154251,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ygRxEgiNwF",
                "forum": "2UxSXuzrap",
                "replyto": "N0fxeLf8ln",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2202/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2202/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "Thank you for reviewing our paper and we would like to address your concerns below.\n\n> Confusion regarding \"These perturbations can form \u201cshortcuts\u201d [12, 16]\"\n\nThank you for highlighting the confusion\nregarding the term \"shortcuts\"\nin the context of unlearnable example attacks.\nWe agree with the reviewer that [16]\ndoes not directly state\nthat unlearnable perturbations are shortcuts,\nbut its optimization of perturbations\nimplies such a conclusion,\nas the perturbation added is encouraged\nto form unintended features with loss minimization.\nInstead of [16],\nwe could cite [a] with an explicit claim\non unlearnability shortcuts.\nWe also note that [12] is cited\nas it provides the original definition\nof the term \"shortcuts\".\nWe have updated the paper accordingly.\n\n[a]: Yu et al., Availability attacks create shortcuts, SIGKDD 2022.\n\n> The explanation of why UEraser performs better than adversarial training is self-contradictory.\n\nIn Section 3.2,\nwe intend the sentence\n\"... which performs adversarial augmentations polices that preserves to the semantic information of the images rather than adding $\\ell_p$-bounded adversarial noise\"\nto describe the property of adversarial augmentations\nas preserving semantic information,\ninstead of comparing the abilities\nto preserve semantics of both methods.\nWe thank the reviewer for pointing this out\nand agree with the reviewer\nthat the sentence\ncould lead to confusion,\nand have updated the relevant text\nin Section 3.2 accordingly.\n\nWhile $\\ell_p$ bounded perturbations\ndo preserve semantics from the perspective human perception,\nfrom a machine learning perspective,\nwhen crafted correctly,\nthey have a profound impact\non deep learning algorithms,\nmaking them unable\nto learn the original features effectively.\nIn this sense,\nUEraser policies do preserve semantics better\nthan standard training,\nas they can train models\nthat learned such semantics\nmuch more effectively.\n\n> why not filter out spatial transformation-based augmentation ...\n\nThe direct use of TrivialAugment is to ensure the plug-and-play and simplicity of UEraser.\nMoreover,\nsimilar to standard training,\nthe use of spatial transformations\nis to help further improve the generalization ability\nof the trained models\nto attain higher clean test accuracies.\n\n> Clerical errors in Sec. 3.2.\n\nThanks a lot for noticing!\nWe have fixed the typos in the updated version.\n\n> Sec. 4 ... resizes all images of ImageNet-subset to 32x32. However, ... Table 8 ... is 224x224x3.\n\nThank you for pointing this out to us.\nWe can confirm that our ImageNet subset results\nare trained on 224x224 images,\nbut Section 4 were not updated to reflect this.\n\n> This paper repeats that UEraser-Lite has a fast training speed. How fast it is?\n\nThank you,\nand we have added the time costs\nof UEraser variants\nversus other defense methods\nin Appendix F.\n\n> Table 2 and Table 3 should add the results of UEraser trained model on clean data.\n\nWe kindly note\nthat we reported the results of UEraser trained model\non clean data in the description of Table 2.\nIn the updated version of Table 3,\nwe have also added the results\nfor CIFAR-100, SVHN and ImageNet.\n\n> The results of adversarial training and other unlearnable methods (AR, OPS, TAP, NTGA, HYPO) in Table 3 should be shown out.\n\nThank you for your constructive suggestions.\nWe added the results of adversarial training in Table 3\nin the updated version.\n\n> There is no sensitivity analysis of hyperparameter W. How to pick a suitable value of W and K when deploying UEraser?\n\nA smaller W makes UEraser training faster\nwhile a larger W gives better results.\nWe kindly note that the sensitivity analyses\nof $K \\in \\{1,2,\\ldots,10\\}$\nand $W \\in \\{50, E\\}$,\nwhere $E$ is the number of total epochs,\nare reported in Table 12 of Appendix B.\nUEraser-Max (i.e. $W = E$)\nusually achieves the best results,\nwhile being the most computationally expensive.\nWe chose $K = 5$ by default\nfor all experiments that involve\nadversarial augmentations,\nas it offers a good trade-off\nbetween training speed and accuracy.\nLarger $K$ values\nresulted in lower accuracies,\nas the adversarial augmentations\nmay become too aggressive."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2202/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700046407512,
                "cdate": 1700046407512,
                "tmdate": 1700047086473,
                "mdate": 1700047086473,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dWc9hbxv3E",
            "forum": "2UxSXuzrap",
            "replyto": "2UxSXuzrap",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2202/Reviewer_E5wZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2202/Reviewer_E5wZ"
            ],
            "content": {
                "summary": {
                    "value": "This paper considers the following adversarial poisoning problem: An adversary uses training data with \"small perturbations (so in particular not hard to distinguish with normal training data\" to tamper the training process so that the trained model will generalize poorly in the distribution sense.\n\nThe paper proposes a new \"data augmentation\" mechanism, which is solve objective (3) under various augmentations.\n\nQuite some experiments are performed to demonstrate the effectiveness of the method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Overall this paper is a nice read. There is a clear principle that guides the paper (objective (3)), and the results seem pretty solid. UEraser seems to work pretty well in all experiments.\n\nIt is also a bit surprising that \"data augmentation\" actually works. What is missing in previous work?"
                },
                "weaknesses": {
                    "value": "For one thing, the paper seems rather straightforward in principle, we try to solve the objective (3) under various augmentation methods. So as we expand the augmentations, we should get better results.\n\nOne thing is what is really the cost (which I don't think the paper has much discussion), do we need a tremendous amount of augmentation in order to make sure the learning does not learn the short cut? Also, what happens if the adversary is aware of the augmentations the training method is using? (so in that sense the paper needs to be more clear about what is the security model -- that is, what knowledge does the adversary has?)"
                },
                "questions": {
                    "value": "I don't have specific questions, my main concerns are described above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2202/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698601364095,
            "cdate": 1698601364095,
            "tmdate": 1699636154165,
            "mdate": 1699636154165,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "W71fZvu330",
                "forum": "2UxSXuzrap",
                "replyto": "dWc9hbxv3E",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2202/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2202/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "Thank you for reviewing our paper and we would like to address your concerns below.\n\n> ... seems rather straightforward in principle, we try to solve the objective (3) under various augmentation methods. So as we expand the augmentations, we should get better results.\n\nWe think this can also be taken as a compliment to our work.\nSimple and effective ideas are often the most impactful.\nWe hope that our work can inspire future research\nto explore the potential of adversarial augmentation\nin unlearnable example attacks and defenses\n\n> do we need a tremendous amount of augmentation ... ?\n\nThanks for raising an interesting question.\nWhile the augmentations we used are not exhaustive,\nwe believe they have the potential to be improved further.\nThis could be optimized with auto machine learning algorithms,\nand we hope to explore this in the future.\n\n> what happens if the adversary is aware of the augmentations the training method is using ...\n\nWe kindly point out that this scenario is considered\nin Section 4 *Adaptive Poisoning* and Table 4.\nIt shows that UEraser-Max\ncan maintain a good defense\nagainst adaptive attacks\nusing the UEraser variants."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2202/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700046462488,
                "cdate": 1700046462488,
                "tmdate": 1700046462488,
                "mdate": 1700046462488,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "nxUbN609jd",
            "forum": "2UxSXuzrap",
            "replyto": "2UxSXuzrap",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2202/Reviewer_6kgo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2202/Reviewer_6kgo"
            ],
            "content": {
                "summary": {
                    "value": "This paper tackles unlearnable example attacks in deep learning, introducing \"UEraser,\" a defense method utilizing adversarial data augmentations to neutralize these attacks. UEraser extends the perturbation budget beyond typical attacker assumptions, aiming to maintain model accuracy on poisoned data without significant loss on clean data. A faster variant, \"UEraserLite,\" is also presented. The authors demonstrate UEraser's effectiveness across various state-of-the-art unlearnable example attacks, outperforming existing defenses and showing resilience against adaptive attacks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper introduces \"UEraser,\" a novel defense against unlearnable example attacks using adversarial data augmentations. This approach is novel and extends the perturbation budget beyond typical attacker assumptions, showcasing a new direction in defending against these types of attacks.\n\n- The authors have conducted extensive experiments to validate the effectiveness of UEraser against various state-of-the-art unlearnable example attacks. The results demonstrate that UEraser outperforms existing defense methods and is resilient against potential adaptive attacks, providing a strong empirical basis for the proposed approach.\n\n- The introduction of \"UEraserLite\" offers a faster and more efficient alternative to UEraser, making the proposed defense more accessible and practical for real-world applications."
                },
                "weaknesses": {
                    "value": "- While the paper presents a novel approach to defending against unlearnable example attacks, the contribution can be considered incremental. The use of adversarial training and data augmentation for defense is not entirely new, and the extension to unlearnable example attacks, while valuable, builds upon existing knowledge and techniques.\n\n- The paper could benefit from exploring additional evaluation of UEraser and UEraserLite, particularly in terms of computational efficiency/time cost compared with existing defenses. \n\n- The paper primarily focuses on empirical results, and a more comprehensive theoretical analysis of why UEraser works and under what conditions it is most effective could strengthen the paper."
                },
                "questions": {
                    "value": "See above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2202/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698754718503,
            "cdate": 1698754718503,
            "tmdate": 1699636154100,
            "mdate": 1699636154100,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IeWqhEdWQV",
                "forum": "2UxSXuzrap",
                "replyto": "nxUbN609jd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2202/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2202/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "Thank you for reviewing our paper and we would like to address your concerns below.\n\n> The use of adversarial training and data augmentation for defense is not entirely new\n\nWe highlight that while adversarial training\nand data augmentation are not new,\nthe proposal of adversarial augmentation\nis novel and effective,\nas it forms an effective defense\nagainst unlearnable example attacks.\nIt also breaks the perception that data augmentations\nwere not effective countermeasure against unlearnable examples.\nFinally,\nit outperforms the state-of-the-art methods\nagainst existing unlearnable example attacks.\n\n> The paper could benefit from exploring additional evaluation of UEraser and UEraserLite, particularly in terms of computational efficiency/time cost compared with existing defenses\n\nThank you for your constructive feedback.\nWe have added the time cost of proposed method\nversus other defense methods in the Appendix F.\n\n> a more comprehensive theoretical analysis of why UEraser works and under what conditions it is most effective could strengthen the paper\n\nWe thank the reviewer for the suggestion.\nThe main contribution of this paper\nis the proposal of adversarial augmentation,\nempirically show its ability\nto mitigate the impact of unlearnable example attacks,\nand shed light to the inadequacy of the existing attacks.\nWe hope to turn our focus to its theoretical analysis\nin the future."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2202/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700046518175,
                "cdate": 1700046518175,
                "tmdate": 1700047000891,
                "mdate": 1700047000891,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "TzubXWtUA5",
            "forum": "2UxSXuzrap",
            "replyto": "2UxSXuzrap",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2202/Reviewer_iWnX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2202/Reviewer_iWnX"
            ],
            "content": {
                "summary": {
                    "value": "Unlearnable example attacks refer to data poisoning techniques used to safeguard public data against unauthorized training of deep learning models. These methods introduce subtle perturbations to the original images, making it challenging for deep learning models to effectively learn from such training data. Current research indicates that adversarial training can partially mitigate the impact of unlearnable example attacks, while common data augmentation methods are ineffective against these poisons. However, adversarial training is computationally demanding and can lead to significant accuracy loss.\n\nThis paper presents the UEraser method, which surpasses existing defenses against various state-of-the-art unlearnable example attacks. UEraser achieves this through a combination of effective data augmentation techniques and loss-maximizing adversarial augmentations. Unlike current state-of-the-art adversarial training methods, UEraser employs adversarial augmentations that extend beyond the assumed 'p perturbation budget' used by current unlearning attacks and defenses. This approach enhances the model's generalization ability, thereby safeguarding against accuracy loss. UEraser effectively eliminates the unlearning impact through loss-maximizing data augmentations, restoring trained model accuracies.  On challenging unlearnable datasets like CIFAR-10, CIFAR-100, SVHN, and ImageNet-subset, generated using various attacks, UEraser achieves good results."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper highlights that unlearnable examples can be mitigated through various data augmentation techniques, potentially leading to the generation of more resilient unlearnable examples when facing adaptive poisoning."
                },
                "weaknesses": {
                    "value": "1. After carefully conducting experiments of the UEraser using the official code provided by the authors, a notable disparity emerged in comparison to the reported results in the paper. On the CIFAR-10 datasets, specifically, for EM poisons, the best accuracy achieved during training was 74.46\\%, while the accuracy in the final epoch was 68.46% (25\\% lower than reported). In the case of LSP poisons, the best training accuracy reached 90.23\\%, with a final epoch accuracy of 73.43\\% (12\\% lower than reported). This observation indicates that although the UEraser appeared effective during mid-training, the models eventually converged to shortcuts present in the unlearnable examples. Given that there is often no clean validation dataset available, many papers on unlearnable examples (UE) typically report the accuracy achieved in the last training epoch. Furthermore, with a fixed learning rate of 0.01 throughout the training process, I've observed that the accuracy of models trained on clean images struggles to converge to a satisfactory level, reaching approximately 92% on CIFAR-10 (2% lower than reported) and around 70% on CIFAR-100 (4% lower than reported) in my experiments. Therefore, it is recommended that the authors thoroughly review their results and consider reporting the accuracy in the last epoch for a more equitable comparison.\n\n2. In the experiments conducted with the UEraser-Max approach, the performance on the EM and LSP attacks shows a gap compared to the reported results on the CIFAR-10 dataset (EM: 62.25\\%, 33\\% lower than reported; LSP: 88.33\\%, 6\\% lower than reported). Interestingly, the use of a fixed learning rate of 0.01 (too large to converge to the optimal model when conducting UEraser-Max) is not likely to reach an accuracy of 95.24% for EM poisons, which is even 0.5% higher than what is typically achieved in standard training on **clean** CIFAR-10.\n\n2. It's essential to consider the standard evaluation settings and practices in the field. The results on ImageNet-subset are not convincing, as most operations on ImageNet-subset in papers related with Unlearnable Examples do not resize the images to $32 \\times 32$. Instead, they follows the default data augmentations, and images are resized to $224 \\times 224$ during training, it would be advisable to align with these practices in your experiments for better comparability with previous works. For your reference, usually, the clean performance on the ImageNet-subset should be around 80\\%, and the performance of ISS facing several UE methods is around 55\\%.\n\n3. Training on CIFAR-10, CIFAR-100, and SVHN datasets takes a similar amount of time. To provide a more comprehensive evaluation of the proposed method's performance and robustness, it would be valuable to expand the experimental analysis to include CIFAR-100 and SVHN, similar to the approach taken for CIFAR-10. This extended evaluation would help assess how well the model generalizes across different datasets and under various UE attack methods.\n\n4. The proposed methods do not exhibit robustness to adaptive poisoning. Table 4 demonstrates that when faced with adaptive poisoning (UEraser-Max), the defensive performance experiences a significant drop, ranging from 15% to 30%. To facilitate a fair comparison, it would be beneficial to report the performance of adaptive poisoning when facing ISS.\n\n5. The concept presented in this work is not particularly innovative, and the achieved performance heavily relies on empirical augmentations for defense, which can be significantly undermined by adaptive poisoning."
                },
                "questions": {
                    "value": "See weaknesses above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2202/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2202/Reviewer_iWnX",
                        "ICLR.cc/2024/Conference/Submission2202/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2202/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698761164130,
            "cdate": 1698761164130,
            "tmdate": 1700049732571,
            "mdate": 1700049732571,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EStoYnS0q3",
                "forum": "2UxSXuzrap",
                "replyto": "TzubXWtUA5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2202/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2202/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "Thank you for reviewing our paper and we would like to address your concerns below.\n\n> with a fixed learning rate of 0.01 throughout the training process, I've observed that the accuracy of models trained on clean images struggles to converge to a satisfactory level\n\n> the use of a fixed learning rate of 0.01 (too large to converge to the optimal model when conducting UEraser-Max) is not likely to reach an accuracy of 95.24% for EM poisons\n\n> a notable disparity emerged in comparison\n> to the reported results in the paper ...\n\nWe sincerely apologize for the oversight in the paper,\nwhere instead of a fixed 0.01 learning rate,\nwe used the cosine annealing schedule\nwith an initial learning rate of 0.01\nin experiments by default.\nThe paper has been updated to reflect this,\nand we appreciate your diligence\nin identifying this discrepancy,\nand we are committed\nto providing accurate and reliable results.\n\nWe appreciate your keen interest\nin our paper\nand the effort you've invested\nin reproducing the results.\nWe would like to engage with you\nin the discussion period\nto address the questions raised\nand help you reproduce the results.\nIf permissible by the conference guidelines,\nwe could set up an anonymized Google Colab\nto ensure its reproducibility.\n\n> Are results on ImageNet-subset resized to 32x32?\n\nThank you for pointing this out to us.\nSection 4 incorrectly states\nthat the images are resized to 32x32,\ninstead they are resized to 224x224,\nas originally stated in Table 8 of Appendix A.1.\nThe accuracy differences from previous papers\ncan be attributed\nto the smaller training set size (200 images per class).\n\n> ... extended evaluation would help assess how well the model generalizes across different datasets and under various UE attack methods.\n\nWe thank the reviewer for the suggestion,\nand given the time available during discussion period\nand limited computational resources,\nwe will expand the experimental analysis\non CIFAR-100 in an updated version.\n\n> The proposed methods do not exhibit robustness to adaptive poisoning using UEraser-Max.  To facilitate a fair comparison, it would be beneficial to report the performance of adaptive poisoning when facing ISS.\n\nWe have added adaptive poisoning and defenses with ISS\nin Appendix E.\nAdditionally,\nachieving over 80% accuracy under adaptive poisoning of our method\nshould not be perceived negatively,\nwhich is in line with adaptive attacks\nusing adversarial training and ISS.\n\n> The concept presented in this work is not particularly innovative, and the achieved performance heavily relies on empirical augmentations for defense, which can be significantly undermined by adaptive poisoning.\n\nWe highlight that the novelty of UEraser variants\nis the proposal of adversarial augmentation,\nand its ability to mitigate the impact of unlearnable examples.\nWe also point out that the key takeaway of our paper\nis that all existing $\\ell_p$ bounded attacks\ncannot effectively protect the data against learning,\nwhich is a significant finding\nthat needs urgent attention from the community.\n\nWe hope to engage with the reviewer further\nfor us to address your concerns more thoroughly."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2202/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700046572319,
                "cdate": 1700046572319,
                "tmdate": 1700047212227,
                "mdate": 1700047212227,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NFouei3TWJ",
                "forum": "2UxSXuzrap",
                "replyto": "TzubXWtUA5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2202/Reviewer_iWnX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2202/Reviewer_iWnX"
                ],
                "content": {
                    "title": {
                        "value": "Updated Comments"
                    },
                    "comment": {
                        "value": "1. The response is not satisfactory to me. I've experimented with various combinations of optimizer setups and schedules, but none of the results have matched the reported performance. Moreover, if the fundamental setup is unclear, it raises concerns about the credibility of the reported results. Moreover, the choice of an initial learning rate of 0.01 is questionable when experiments are conducted using a cosine annealing schedule. In general, other learning problems on CIFAR-10, such as adversarial training, commonly utilize an initial learning rate of 0.1 when employing the SGD optimizer. It's worth noting that this paper doesn't make any claims about unusual setups in this regard.\n\n2. Another concerning point is in the comparison of training on CIFAR-10 with EM in Table 2 and Table 6. I don't think that changing the backbone from ResNet-18 to ResNet-50 would result in a drop of about 5%.\n\n3. The additional Adversarial Training results lack convincing evidence for both CIFAR-100 and ImageNet-subset. In my implementations, for ImageNet-subset (200 images per class), performance reaches 44% for both EM and REM, and the proposed method is comparable to adversarial training in terms of computation. On the CIFAR-100 dataset, performance exceeds 55%, higher than reported ones.\n\n4. Moreover, the outcomes on ImageNet-subset, particularly when training on clean data, appear not practical. If you consider 200 images per class, I am certain that the accuracy cannot attain 72%. ISS [1] reported similar results, with standard training on clean ImageNet-subset yielding 62%.\n\n5. The claimed effectiveness of the proposed methods seems overstated due to 1) Limited experiments on datasets other than CIFAR-10; 2) Subpar performance on ImageNet-subset, potentially tailored for datasets with small resolutions.\n\n6. The reported results appear to sidestep more robust defense methods, such as median filtering, which can achieve 85% on the OPS, surpassing the proposed method.\n\n7. When confronted with adaptive poisoning, the performance is lower than adversarial training (80% vs. 85%).\n\n8. Using augmentation as a defensive method lacks compelling support. This type of augmentation-based approach has been extensively discussed in related attacks. The novelty of this paper seems to fall below the acceptance standards of ICLR.\n\nIn summary, I highly recommend that the authors conduct a thorough examination of the experimental setups, provide detailed implementations, and conduct more comprehensive experiments.\n\n[1] Zhuoran Liu, Zhengyu Zhao, and Martha Larson. **Image shortcut squeezing: Countering perturbative availability poisons with compression.** In *Proc. Int\u2019l Conf. Machine Learning*, 2023"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2202/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700049708287,
                "cdate": 1700049708287,
                "tmdate": 1700051385457,
                "mdate": 1700051385457,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]