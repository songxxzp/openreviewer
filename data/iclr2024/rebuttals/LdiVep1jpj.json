[
    {
        "title": "RASP Quadratures: Efficient Numerical Integration for High-Dimensional Mean-Field Variational Inference"
    },
    {
        "review": {
            "id": "iRhsK0xtOM",
            "forum": "LdiVep1jpj",
            "replyto": "LdiVep1jpj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3761/Reviewer_m4a3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3761/Reviewer_m4a3"
            ],
            "content": {
                "summary": {
                    "value": "The authors proposed RASP quadratures which serve as an evaluation-efficient method for high-dimensional integration. RASP quadratures minimize the error corresponding to basis functions that dominate variational updates for Gaussian mean fields."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The idea of  using RASP quadrature for QNVB is interesting and new.\n\nThe performance of the proposed method is demonstrated through a range of experiments."
                },
                "weaknesses": {
                    "value": "The related work section should be expanded to include more discussion. Clearly, QMC is very relevant and other quadrature methods are also relevant."
                },
                "questions": {
                    "value": "Is there any theoretical justification for the number of nodes (and corresponding function evaluations) in general?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3761/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698680870417,
            "cdate": 1698680870417,
            "tmdate": 1699636332335,
            "mdate": 1699636332335,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Fy5LFY0qFt",
                "forum": "LdiVep1jpj",
                "replyto": "iRhsK0xtOM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3761/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3761/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for taking the time to review our work. We really appreciate your positive feedback.\n\n### \u201cThe related work section should be expanded to include more discussion. Clearly, QMC is very relevant and other quadrature methods are also relevant.\u201d\n - Agreed. We substantially revised the related work section to provide a more thorough overview these methods to help place our work in a clearer context.\n\n### \u201cIs there any theoretical justification for the number of nodes (and corresponding function evaluations) in general?\u201d\n - Theorem 2 shows how the number of nodes affects the number of bivariate quadratic basis functions that integrate exactly. That said, the optimal number of nodes really depends on time / computational constraints and the characteristics of the loss topography."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3761/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700262687396,
                "cdate": 1700262687396,
                "tmdate": 1700262687396,
                "mdate": 1700262687396,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IM5IuV3gsv",
                "forum": "LdiVep1jpj",
                "replyto": "Fy5LFY0qFt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3761/Reviewer_m4a3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3761/Reviewer_m4a3"
                ],
                "content": {
                    "comment": {
                        "value": "I have read the authors' response and decide to keep my score."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3761/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700645770313,
                "cdate": 1700645770313,
                "tmdate": 1700645770313,
                "mdate": 1700645770313,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "llvGK6FXeV",
            "forum": "LdiVep1jpj",
            "replyto": "LdiVep1jpj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3761/Reviewer_jXiK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3761/Reviewer_jXiK"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new randomized quadrature scheme, random-affinity sigma-point (RASP) quadrature, for the purpose of mean-field variational inference (MFVI). RASP is combined with a recent variational inference method proposed in a separate paper based on a method called projective integral updates. The resulting method essentially combines RASP with MFVI using quasi-newton-like preconditioning."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The paper proposes a new randomized quadrature scheme they call RASP."
                },
                "weaknesses": {
                    "value": "The paper lacks reference to a number of highly relevant related work that develops a context for the current paper. Here are some places (non-exhaustive list) where I think a citation is needed, with specific recommendations:\n* Section 1 first paragraph: cite something for Markov-chain Monte Carlo (MCMC). Although this is folk wisdom, a citation is required for the claim \"MCMC is not practical for large learning architectures that often contain billions of parameters.\"\n* Section 1 second paragraph: cite something for mean-field variational inference (MFVI). MFVI didn't appear out of thin air. The development/popularization of MFVI is often attributed to [1-3].\n* Section 1.2 last sentence: stochastic gradient variational Bayes (SGVB) was independently developed by [4-6], where the use of SGVB for general Bayesian inference is commonly attributed to [4,5]. I suspect the intended citation here was Kingma and Welling [6]?\n* Section 2.1 first sentence: cite something for variational inference. In general, people cite these two well-known reviews [7,8].\n\nProbably due to the issue above, the paper misses a very important line of related works:\n* Randomized quasi-Monte Carlo (RMQC) for variational inference [9]\n* RQMC and stochastic quasi-Newton optimization for variational inference [10]\n\nWhile I'm not claiming that the method lacks novelty, this paper does not do a good job of putting the work in the context of this line of work. Does this method improve over these works? Is it different from the (randomized) quasi-Monte Carlo schemes that are used in these works? This leads me to the next point.\n\nThe choice of baselines and the overall design of the experiments makes the conclusion of the experiments unclear. In particular, the experiments do not appear to be doing an apple-to-apple comparison. Also, some experimental details are missing.\n* Variational inference is fundamentally a method for approximating the posterior. How was the joint likelihood obtained here? Was a prior set on the parameters of these models? Was an improper flat prior set on the parameters? What were the hyperparameters? It is, in fact, known that the choice of prior in deep learning models affects the result a lot [11].\n* The paper only compares against the following combinations: RASP + QNVB, MC + SGVB, MC + QNVB, QMC + QNVB. Why was RASP + SGVB not compared against? Also, the closely related works based on randomized QMC [9,10] and the QNVB algorithm proposed in [10] should also have been included.\n\n\n### Minor Comments\n* When denoting the variational approximation q, it is non-standard to denote conditioning on $\\varphi$. This is because the variational parameters $\\varphi$ are not considered to be random variables (their probability density has not been defined)\n* Section 1.1 third paragraph second from the last sentence \u201c... Hessian diagonal of the training loss, which can be expensive to compute.\u201d I believe saying that it is noisy is more accurate. In fact, noisy estimates are cheap to obtain, as shown by [12].\n* Section 2.2 last sentence: is there any evidence that alternative integration schemes scale better in terms of dimensionality? Because quasi-Monte Carlo methods, while converging faster with respect to the number of samples, have an explicit dimension dependence, unlike regular Monte Carlo. I suspect RASP would have similar behavior, although it is hard to tell since the author did not present a convergence bound akin to those in the QMC literature. Nonetheless, when the number of samples is small, I am not sure if the method can be claimed to be scalable in terms of dimension.\n\n### References\nI am not the author of nor affiliated with any of the references below:\n1. Anderson, James R., and Carsten Peterson. \"A mean field theory learning algorithm for neural networks.\" Complex Systems 1 (1987): 995-1019.\n2. Peterson, Carsten, and Eric Hartman. \"Explorations of the mean field theory learning algorithm.\" Neural Networks 2.6 (1989): 475-494.\n3. Hinton, Geoffrey E., and Drew Van Camp. \"Keeping the neural networks simple by minimizing the description length of the weights.\" Proceedings of the sixth annual conference on Computational learning theory. 1993.\n4. Ranganath, Rajesh, Sean Gerrish, and David Blei. \"Black box variational inference.\" Artificial intelligence and statistics. PMLR, 2014.\n5. Titsias, Michalis, and Miguel L\u00e1zaro-Gredilla. \"Doubly stochastic variational Bayes for non-conjugate inference.\" International conference on machine learning. PMLR, 2014.\n6. Kingma, Diederik P., and Max Welling. \"Auto-encoding variational bayes.\" ICLR'14.\n7. Jordan, Michael I., et al. \"An introduction to variational methods for graphical models.\" Machine learning 37 (1999): 183-233.\n8. Blei, David M., Alp Kucukelbir, and Jon D. McAuliffe. \"Variational inference: A review for statisticians.\" Journal of the American statistical Association 112.518 (2017): 859-877.\n9. Buchholz, Alexander, Florian Wenzel, and Stephan Mandt. \"Quasi-monte carlo variational inference.\" International Conference on Machine Learning. PMLR, 2018.\n10. Liu, Sifan, and Art B. Owen. \"Quasi-monte carlo quasi-newton in variational bayes.\" The Journal of Machine Learning Research 22.1 (2021): 11043-11065.\n11. Fortuin, Vincent, et al. \"Bayesian neural network priors revisited.\" arXiv preprint arXiv:2102.06571 (2021).\n12. Fan, Kai, et al. \"Fast second order stochastic backpropagation for variational inference.\" Advances in Neural Information Processing Systems 28 (2015)."
                },
                "questions": {
                    "value": "no questions"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3761/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3761/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3761/Reviewer_jXiK"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3761/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698712338485,
            "cdate": 1698712338485,
            "tmdate": 1699636332217,
            "mdate": 1699636332217,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HnS8sFB2j7",
                "forum": "LdiVep1jpj",
                "replyto": "llvGK6FXeV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3761/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3761/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for taking the time to review this work. We genuinely appreciate the obvious effort and expertise you put into your review. We have made a diligent effort to revise the manuscript to be as responsive as possible. We inappropriately discounted the importance of maintaining needed citations to place our work in context and have made an earnest effort to correct this. \n\n### \u201cThe paper lacks references to a number of highly relevant related work... [Here are] specific recommendations:\u201d\n - Thank you for these citations. We carefully reviewed this work and performed additional searching to provide other relevant sources as well.\n\n### \u201cCite something for the claim \u2018MCMC is not practical for large learning architectures.\u2019\"\n - We added several references to support this claim.\n\n### \u201cCite something for mean-field variational inference (MFVI)... often attributed to [1-3].\u201d\n - We were aware of these references and should have added them earlier.\n\n### \u201cSGVB was independently developed by [4-6]... I suspect the intended citation here was Kingma and Welling [6]?\n - Yes, we corrected this oversight [6] and added [4,5].\n\n### \u201cCite something for VI... [7,8].\u201d\n - Thank you. We added these well-known sources and a recent survey by Zhang (2018).\n\n### \u201cThe paper misses a very important line of related works [regarding RQMC].\u201d\n - We appreciate these references and we also performed additional searching to discuss these contributions in more detail under related work.\n\n### \u201cWhile I'm not claiming that the method lacks novelty, this paper does not do a good job of putting the work in context.\u201d\n - The related work section has been heavily revised to provide a clearer narrative and context.\n\n### \u201cThe choice of baselines ... makes the conclusion of the experiments unclear... the experiments do not appear to be doing an apple-to-apple comparison. Why was RASP + SGVB not compared? Also, some details are missing.\u201d\n - RASP quadratures target improved integration of basis functions that dominate QNVB updates. This is why our original experiments examined several architectures and other variance-reduced monte carlo methods with QNVB. That said, a quadrature comparison with other training methods is helpful and we added a battery of tests using SGVB, as suggested. We had to move comparisons of non-VI training methods into the appendix, but that isn't problematic since it isn\u2019t the central focus of the paper. We also added additional experiment details to the appendix.\n\n### \u201cWas a prior set on the parameters of these models? What were the hyperparameters?\"\n - We used uniform priors to avoid an additional complication, but regularization techniques can easily be incorporated into our pipeline. As you are almost certainly aware, L1 and L2 terms can be regarded as negative log priors. Other hyperparameters are described in the appendix. We added a paragraph about this under the experiments section.\n\n### \u201cAlso, the closely related works based on randomized QMC ... should also have been included.\"\n - RQMC methods typically use more function evaluations than our target range (10+ as opposed to 3 to 6) and they are not designed to target second-order exactness. Such comparisons would be interesting, but they also open a different line of investigation from the role of exactness in QNVB updates. Please see our updated related work.\n\n### \u201cWhen denoting the variational approximation q, it is non-standard to denote conditioning on phi.\u201d\n - Our notation follows Zhang (2018) to explicitly clarify that the shape of the variational density is determined by phi, which is the actual variable of optimization. This notation is helpful for projective integral updates, since phi comprises the projection coefficients that RASP quadratures target. See equations 3 and 4.\n\n### \u201cSection 1.1 \u2018... Hessian diagonal of the training loss, which can be expensive to compute.\u2019 I believe saying that it is noisy is more accurate. In fact, noisy estimates are cheap to obtain, as shown by [12].\u201d\n - This is a fair point and we have made the modification.\n\n### \u201cSection 2.2: is there any evidence that alternative integration schemes scale better in terms of dimensionality? Because quasi-Monte Carlo methods ... have an explicit dimension dependence. I suspect RASP would have similar behavior... the author did not present a convergence bound... I am not sure if the method can be claimed to be scalable in terms of dimension.\u201d\n - RASP quadratures are not unbiased estimators and they do not converge to the exact integral with increasing points. While they are exact on all linear basis functions, as well as a large fraction of second-order basis functions (theorems 1 and 2), there are systematic errors that persist for some higher-order basis functions. The point of interest regarding increased reference dimensionality is the fraction of second-order basis functions that become exact, which we have analyzed with theorems and proofs."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3761/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700262637359,
                "cdate": 1700262637359,
                "tmdate": 1700262637359,
                "mdate": 1700262637359,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "G4OnLuYIQm",
                "forum": "LdiVep1jpj",
                "replyto": "llvGK6FXeV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3761/Reviewer_jXiK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3761/Reviewer_jXiK"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "I thank the authors for their response and the updated manuscript. Here, I will highlight some points that I am in disagreement with the authors. My primary concern is that, due to the rather unusual setup of the theory and experiments, it is very hard to judge the benefit of the proposed methods compared to existing solutions in the literature. \n\n## On experimental setup\nFirstly, I believe the authors would argue that the main use case of variational inference is Bayesian inference. Unfortunately, none of the presented experiments can be considered a \"typical\" Bayesian inference task. The problem is that, for a typical VI person like me, the experimental setup does not resonate with the typical setup of VI. Sure, Bayesian deep learning is a thing, but it is a rather specialized setting where VI has, time and time again, appeared to not be very competitive compared to other methods. There is also theoretical evidence that mean-field VI may fundamentally be incompatible with deep learning [1]. Thus, I recommend the authors take a look at any of the influential variational inference methodology papers to see what are the typical benchmarks and metrics used for evaluation. (Typically, we compare evidence lower bounds. I believe what is referred to as the \"loss\" here is something different?)\n\n## On the theoretical results\nCurrently, the presentation of the theoretical results is unconventional, and it is also hard to compare against existing convergence/error guarantees for MC and QMC methods. Note that it is also the task of the authors to highlight and compare the theoretical guarantees against previous guarantees if they wish to claim theoretical superiority.\n\n## Comparison Against QMCVI, QNQMCVI\n> RQMC methods typically use more function evaluations than our target range (10+ as opposed to 3 to 6) and they are not designed to target second-order exactness. Such comparisons would be interesting, but they also open a different line of investigation from the role of exactness in QNVB updates. Please see our updated related work.\n\nI do not quite follow the response here. QMCVI and QNQMCVI are separate algorithms on their own. I'm not asking to combine RQMC with \"QNVB\" by Duerch (2023). I am suggesting to compare against the exact algorithm proposed by Liu and Owen (2021) and Buchholz et al. (2018). Especially, if the authors claim that the proposed method performs better with less computation than QMCVI and QNQMCVI, then they should present such evidence!\n\n## References\n1. Coker, Beau, Bruinsma, Wessel P., Burt, David R., Pan, Weiwei, & Doshi-Velez, Finale. 2022. Wide Mean-Field Bayesian Neural Networks Ignore the Data. AISTATS."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3761/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700351378743,
                "cdate": 1700351378743,
                "tmdate": 1700351676958,
                "mdate": 1700351676958,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "sabZAN4siK",
            "forum": "LdiVep1jpj",
            "replyto": "LdiVep1jpj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3761/Reviewer_rgHx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3761/Reviewer_rgHx"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a \"random-affinity sigma-point (RASP) quadrature\" method for numerical integration w.r.t. high-dimensional Gaussian distributions. The point is to use very few integration nodes. RASP quadrature is constructed by \"scaling\" a quadrature method on a low-dimensional reference space to higher dimensions by retaining the quadrature weights and picking random coordinates (and doing sign-changes) of the original quadrature nodes.\n\nRASP quadrature is applied to some large-scale problems that I am not qualified to comment on. In this review I focus on the quadrature part of the paper."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is well and clearly written. In some of the experiments the proposed method is observed to outperform some competitors. I suppose the RASP quadrature being quite simply can also be seen as a strength."
                },
                "weaknesses": {
                    "value": "I am rather skeptical about novelty of the proposed quadrature. Having never worked in a setting in which even n = d + 1 nodes is prohibitive, I cannot provide any references. However, the limited and superficial literature review on pages 4 and 9 does not convince me that the authors know the relevant quadrature well enough to say with any confidence that something like this has not been tried before.\n\nI would like to see some language reflecting this inserted in the paper.\n\nHowever, the quadrature method does not necessarily need to be novel. I am not against accepting this paper if the consensus is that the overall variational inference methodology presented here constitutes a useful contribution to the field."
                },
                "questions": {
                    "value": "- p3: What is [m]?\n- p3: The definition of \\mathcal{F} in Eq (3) is unclear.\n- p3: There are several squares of bolded quantities, some of which appear to be vectors [e.g., in Eq (5)]. What does squaring mean here?\n- p3: In the first paragraph of Sec 2.3 f: R^d \\mapsto \\R should use \\to rather than \\mapsto.\n- p4: \"For quadrature formulas that obtain second-order exactness (integrating all quadratic total-degree polynomials exactly) over multivariate Gaussians, the evaluation nodes are called sigma points.\" It would be good to note that sigma point terminology is far from universal and only really used in non-linear filtering literature.\n- p4: Given McNamee & Stenger (1967) and other old fully symmetric quadrature formulae, I doubt that Uhlmann (1995) was the first to design a second-order accurate quadrature rule.\n- p9: The description of QMC in Section 5 is rather misleading (as is labelling some alternative methods as QMC methods in Figure 4): QMC methods share nothing common with MC methods beyond the fact that they use uniform integration weights w_k = 1/n. See e.g. p135 in Dick et al. (2013)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "-"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3761/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3761/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3761/Reviewer_rgHx"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3761/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699630763046,
            "cdate": 1699630763046,
            "tmdate": 1699636332135,
            "mdate": 1699636332135,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EY3dR2A73A",
                "forum": "LdiVep1jpj",
                "replyto": "sabZAN4siK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3761/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3761/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for taking the time to review our work. We really appreciate your effort and expertise.\n\n### \u201cThe limited and superficial literature review on pages 4 and 9 does not convince me that the authors know the relevant quadrature well enough to say with any confidence that something like this has not been tried before.\u201d\n - This is a fair point, and we inappropriately disregarded the importance of citing relevant related work. We have diligently reviewed surveys, books, and new papers regarding high-dimensional integration, sigma-point methods, QMC methods, and RQMC methods. We have also substantially revised and lengthened our related work section accordingly to coherently place our contributions in context. While we still cannot be sure if this method (or something similar) has been published before, it is certainly not commonly discussed. We have added a statement to this effect.\n\n### \u201cWhat is [m]?\u201d\n - This notational shorthand is sometimes used to denote the set {1, 2, \u2026 , m}.\n\n### \u201cWhat does squaring [of vectors] mean here?\u201d\n - Powers on vectors are elementwise in these formulas and we have added a note to clarify this notation.\n\n### \u201cThe definition of \\mathcal{F} in Eq (3) is unclear.\u201d\n - The basis depends on the exponential family that captures feasible variational densities. For the purpose of QNVB, the basis spans the set of univariate quadratics in R^d, which Eq (5) is intended to capture.\n\n### \u201cIn the first paragraph of Sec 2.3 f: R^d \\mapsto \\R should use \\to rather than \\mapsto.\u201d\n - Thank you!\n\n### \"It would be good to note that sigma point terminology is far from universal and only really used in non-linear filtering literature.\u201d\n - Good point. We have added this clarification.\n\n### \u201cGiven McNamee & Stenger (1967) and other old fully symmetric quadrature formulae, I doubt that Uhlmann (1995) was the first to design a second-order accurate quadrature rule.\u201d\n - Another good point; we have added this citation and removed the \u201coriginal\u201d qualifier.\n\n### \u201cThe description of QMC in Section 5 is rather misleading (as is labelling some alternative methods as QMC methods in Figure 4): QMC methods share nothing common with MC methods beyond the fact that they use uniform integration weights w_k = 1/n. See e.g. p135 in Dick et al. (2013).\u201d\n - We also reviewed several sources and realize that it was a mistake to label variance-reduced methods as QMC methods. Our confusion came from misreading Caflisch\u2019s 1998 paper, \u201cMonte carlo and quasi-monte carlo methods.\u201d While his paper is clear about the definition of QMC, it also contains variance-reduce methods, which are neither MC nor QMC. We have corrected the language and experiments to use the more explicit and technically correct terminology of variance-reduced Monte Carlo (VRMC)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3761/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700260684537,
                "cdate": 1700260684537,
                "tmdate": 1700260684537,
                "mdate": 1700260684537,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]