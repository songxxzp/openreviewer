[
    {
        "title": "Semantic Decoupled Distillation"
    },
    {
        "review": {
            "id": "jaNvGujzDB",
            "forum": "r1wHogNDyQ",
            "replyto": "r1wHogNDyQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1542/Reviewer_SyKS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1542/Reviewer_SyKS"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a method, semantic decoupled distillation (SDD), to improve logit distillation. The authors found that traditional logit distillation coupled multiple semantic knowledge in a single logit output, resulting in sub-optimal performance. The SDD method decoupled the whole logit output into the logit outputs of multiple local regions which acquired unambiguous semantic knowledge. SDD divided these logit outputs into consistent and complementary terms based on their class. Additionally, SSD used dynamic weights to adapt to different tasks and data scenes.\n\nContributions:\n1.\tThe authors indicated the weakness of current methods that hindered the student from inheriting comprehensive knowledge from the teacher.\n2.\tThe authors proposed SDD to assist the logit distillation and proved the effectiveness of their method on several benchmark datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The rationale for enhancing the classic distillation is well-founded, and their explanation of the distinction between M in heterogeneous and homogeneous structures in Appendix 6.1 is robust.\nThe visualization part affirmed that the improvement of SDD did not arise from a more accurate imitation of the global logit output of the teacher. The case study further verified their proposition."
                },
                "weaknesses": {
                    "value": "In the introduction section, the authors mentioned that SDD introduced dynamic weights for consistency and complementary to adapt to different tasks and data scenes, while the rest of the paper only set them to fixed numbers.\nThe authors forget to mention and compare to many recent knowledge distillation methods. their results are below the state-of-art results. for example, using resnet 32X4 as a teacher and Shuffle V2 as a student, current state-of-art results can reach 79.54 [1].\n\n[1] Ding, Fei, et al. \"Dual-Level Knowledge Distillation via Knowledge Alignment and Correlation.\" IEEE Transactions on Neural Networks and Learning Systems"
                },
                "questions": {
                    "value": "In Appendix 6.2, did you set the balance parameter \u03b1 to 1? If \u03b1 is 1, why did you adjust \u03b3  and \u03b2 separately instead of utilizing a single balance parameter?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1542/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698677035876,
            "cdate": 1698677035876,
            "tmdate": 1699636082713,
            "mdate": 1699636082713,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "ci4qub6QHo",
            "forum": "r1wHogNDyQ",
            "replyto": "r1wHogNDyQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1542/Reviewer_q6qm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1542/Reviewer_q6qm"
            ],
            "content": {
                "summary": {
                    "value": "This paper focused on logit knowledge distillation. The authors argued that existing logit-based methods only leverage the global logit output, which may hinder the student from learning comprehensive knowledge from the teacher. As such, they proposed a semantic decoupled distillation method (SSD), decoupling the logit output of the whole image into the logit outputs of multiple local regions."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The authors conducted extensive experiments to evaluate the effectiveness of SSD.\n2. Knowledge distillation is a hot-topic and attracts wide interest in the research community."
                },
                "weaknesses": {
                    "value": "1. In the second and third paragraphs of the introduction, the connection between global logit knowledge and the coupled information of multiple classes is not very clear. \n2. This paper should give intuition, theoretical analysis, or evidence to explain why the logit output with coupled information will hinder the learning of comprehensive knowledge in the student model. In most situations, the logit output from a teacher model essentially provides the knowledge among multiple classes (see the discussion in DKD).\n3. The derived logit output from multi-scale pooling seems to still aggregate the information of a whole image by average pooling. The contribution for knowledge distillation itself is limited.\n4. To perform distillation in multiple scales, their distillation losses will increase training cost (time and memory). It is better to discuss their complexity and effects."
                },
                "questions": {
                    "value": "1. In the notation, some definitions are confusing for readers. For example, $f_{Net}(j,k)$ is the feature vector at the location $(i,j)$. Where is the $i$ in the $f_{Net}(j,k)$? In the first paragraph of Section 3.2, where is $L_T$ in Eq. 5. What is the difference between $P_t$ in Eq. 2 and $P_T$ in Eq. 4? \n2. The technical details of dividing consistent and complementary logit are missing. How do you perform this operation? It is better to give some formal definitions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1542/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698838322668,
            "cdate": 1698838322668,
            "tmdate": 1699636082627,
            "mdate": 1699636082627,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "nKVOStSehf",
            "forum": "r1wHogNDyQ",
            "replyto": "r1wHogNDyQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1542/Reviewer_PAHr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1542/Reviewer_PAHr"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a kind of logit distillation method: semantic decoupled distillation, which instead uses multiple local outputs to produce local logit, transferring richer knowledge to student models.  Extensive experiments demonstrate SDD's effectiveness, especially in fine-grained classification with diverse teacher-student pairs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Compared with baseline methods, the results of the proposed method are good.\n2. The paper is well organized and the writing is clear."
                },
                "weaknesses": {
                    "value": "1. The novelty is limited. The method is named 'semantic decoupling distillation', but actually using multi-scale pooling to replace the global pooling, which seems nothing to do with the \"semantic decoupling\".  The loss for consistent and complementary logit knowledge is simply using different weight to sum up, but the weight is set as hyperparameters which is not dynamic.   \n2. The results are mostly using CNN models and based on small scale dataset (CIFAR 100). To validate the method's effectiveness, more models (such as vision transformers) and ablation on bigger dataset are preferred."
                },
                "questions": {
                    "value": "The same as listed in weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1542/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1542/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1542/Reviewer_PAHr"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1542/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698846303323,
            "cdate": 1698846303323,
            "tmdate": 1699636082548,
            "mdate": 1699636082548,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "yvnicipUWO",
            "forum": "r1wHogNDyQ",
            "replyto": "r1wHogNDyQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1542/Reviewer_onwW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1542/Reviewer_onwW"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces the idea of using multi-scale pooling to generate multi-scale features to use in knowledge distillation. This is in contrast to the logit-based approach to distillation which just uses the output of the average pooling). This \"decouples the logit output of the whole input into the logit outputs of multiple local regions\". The authors note that this approach works well especially in fine-grained classification tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The biggest strength of the paper is the general ideal; doing pooling at multiple scales and using those values as what to compare the student/teacher models for the purpose of loss makes a lot of sense. It clearly shows how more information can be transferred between the student and the teacher. The authors demonstrate how the models can have different structures and the approach still works. They did an expansive evaluation comparing it to other distillation approaches."
                },
                "weaknesses": {
                    "value": "In general, the paper is poorly written. There is inconsistency in how the figures are referenced. There are uncompleted sentences. But mostly, the paper is written with unnecessary complexity."
                },
                "questions": {
                    "value": "- Please explain L_T and L_S, I was confused by this, why wouldn't it just be the softmax W_T, W_S?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1542/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1542/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1542/Reviewer_onwW"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1542/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699304722026,
            "cdate": 1699304722026,
            "tmdate": 1699636082486,
            "mdate": 1699636082486,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]