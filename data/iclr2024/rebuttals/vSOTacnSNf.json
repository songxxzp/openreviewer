[
    {
        "title": "Multimodal Meta-learning of Implicit Neural Representations with Iterative Adaptation"
    },
    {
        "review": {
            "id": "fSoStwWSL4",
            "forum": "vSOTacnSNf",
            "replyto": "vSOTacnSNf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1819/Reviewer_hqqm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1819/Reviewer_hqqm"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes an optimization-based meta-learning framework for learning multimodal representations."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper proposes a meta-learning framework centered around the Multimodal Iterative Adaptation (MIA) paradigm, enhancing the capabilities of single-modal learners by learning from multimodal information.\n2. Through several comparative experiments, this paper demonstrates the proposed method can improve the performance of single-modal learners."
                },
                "weaknesses": {
                    "value": "1. Novelty:\n- Previous research [1-4] has explored cross-modal relationships in multimodal data extensively. Some even utilize Transformer structures for multimodal meta-learning. \n- the paper mentions that existing methods focus on unimodal setups, but whether the information from other modalities is treated as noise or out-of-distribution data by specific-modal learners. Moreover, meta-learning itself is advantageous for few-shot learning, and there is substantial research addressing data scarcity, such as few-shot learning problems. I question the paper's research motivation and look forward to the authors' response.\n[1] Vuorio, R., Sun, S. H., Hu, H., & Lim, J. J. (2019). Multimodal model-agnostic meta-learning via task-aware modulation. Advances in neural information processing systems, 32.\n[2] Abdollahzadeh, M., Malekzadeh, T., & Cheung, N. M. M. (2021). Revisit multimodal meta-learning through the lens of multi-task learning. Advances in Neural Information Processing Systems, 34, 14632-14644.\n[3] Vuorio, R., Sun, S. H., Hu, H., & Lim, J. J. (2018). Toward multimodal model-agnostic meta-learning. arXiv preprint arXiv:1812.07172.\n[4] Sun, Y., Mai, S., & Hu, H. (2022). Learning to learn better unimodal representations via adaptive multimodal meta-learning. IEEE Transactions on Affective Computing.\n\n2. Several conclusions in the paper lack supports, such as (i) why \u201cslowing down convergence leads to overfitting\u201d and (ii) claims without corresponding references, e.g., \"arise in the gradient computation from a small set of observations (Zhang et al., 2019; Simon et al., 2020).\" \n\n3. The paper doesn't provide a clear explanation of how MIA guides single-modal learners' updates or the composition and structure of USFT, MSFT, etc. It's difficult to understand from Figure 2 how the \"better guide the learners\" with blue arrows is achieved. If the authors mean to find the optimal update direction using relationships between different modalities, did consider handle gradient conflicts between modalities? Moreover, the sequential process of multimodal fusion and subsequent meta-learning is not justified. The authors should provide an explanation.\n\n4. Experimental Concerns: (i) Most of the datasets used in the experiments differ from those mentioned in the paper's motivation. The datasets use different features from a uniform data format, such as RGB images and sketches, rather than different modal features from distinct data forms, like images and text. (ii) In Table 5, the paper does not explain why MLPs were not used, especially when they achieved better results on Celeb. (iii) The claim of \"facilitating rapid convergence\" lacks corresponding experimental results. (iv) The proposed method introduces additional multi-modal information to compare whether the experiment is fair, (v) Is the performance improvement of the proposed method worth the extra computational effort?"
                },
                "questions": {
                    "value": "Please refer to Weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1819/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698578782015,
            "cdate": 1698578782015,
            "tmdate": 1699636111410,
            "mdate": 1699636111410,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "As3zMYhmbP",
                "forum": "vSOTacnSNf",
                "replyto": "fSoStwWSL4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1819/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1819/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response of the authors to Reviewer hqqm [1/6]"
                    },
                    "comment": {
                        "value": "> Q1: Previous research [1-4] has explored cross-modal relationships in multimodal data extensively. Some even utilize Transformer structures for multimodal meta-learning. \n\nA1: We kindly remind the reviewer that our problem focus is on employing a meta-learning approach to better approximate joint multimodal continuous functions. To the best of our knowledge, among existing multimodal meta-learning research, Multitask Neural Processes (MTNPs) is the sole study focusing on a problem context similar to ours. Importantly, as noted by the reviewers gNAu and cLC2, our work stands out as novel within this domain, proposing an interesting idea to accelerate the convergence of independent unimodal learners by exploring the multimodal structures in their on-going learning states during iterative adaptation procedures.\n\nBesides MTNPs, existing multimodal meta-learning studies [1,2,3,4] mentioned by the reviewer differ significantly from our work in terms of the definition of modality and problem scenarios, making direct comparisons challenging and unfair. \n\nVuorio et al. [1, 3] and Abdollahzadeh et al. [2] address image classification on a union of digit, bird, aircraft datasets, where each dataset structures the unique modality. This notion of multimodality should not be confused with the conventional understanding of multimodality in data types (e.g. images and texts), as emphasized in Section 3 in Abdollahzadeh et al. [2]. Due to this disparity in the notion of multimodality, their approaches inherently reduce to unimodal meta-learning methods in the context of our joint multimodal signal modeling.\n\nOn the other hand, Sun et al. [4] deals with many-to-one multimodal classification problems using jointly sampled images, audios, and texts. Their proposed meta-learning approach involves the independent adaptation of each modality-specific encoder within the inner loop, followed by their fusion in the outer loop. Here, the transformers are used for each of the unimodal encoders, not for multimodal fusion. This is unlike ours that explores the joint iterative adaptation of independent learners in the inner loop via SFTs, which prevents overfitting of individual learners and accelerates their convergence during the inner-loop adaptation process.\n\n(response continued in the following thread)"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1819/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700741252306,
                "cdate": 1700741252306,
                "tmdate": 1700741743783,
                "mdate": 1700741743783,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "MaxFN8uLY6",
            "forum": "vSOTacnSNf",
            "replyto": "vSOTacnSNf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1819/Reviewer_cLC2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1819/Reviewer_cLC2"
            ],
            "content": {
                "summary": {
                    "value": "Summary:\nThe paper presents a novel framework for developing robust and resilient machine learning models tailored for time series data, leveraging multi-objective optimization. The paper begins by identifying the limitations of existing machine learning algorithms in handling anomalies, noise, and non-stationarity in time series data. To address these limitations, the authors propose a multi-objective optimization framework that simultaneously optimizes for accuracy, robustness, and resilience.\n\nKey Contributions:\nTheoretical Framework: The authors introduce a new mathematical formulation for time series learning that incorporates robustness and resilience as objective functions alongside accuracy. This is formalized through a multi-objective optimization problem.\n\nAlgorithm Development: A new algorithm, Multi-Objective Time Series Algorithm (MOTSA), is developed based on the mathematical framework. MOTSA employs Pareto optimization to find optimal trade-offs among the multiple objectives.\n\nRobustness and Resilience Metrics: The paper introduces novel metrics for quantifying the robustness and resilience of time series models. These metrics are grounded in statistical theory and are proven to be effective evaluators of the model's capacity to handle anomalies and adapt to non-stationary data.\n\nEmpirical Evaluation: Extensive experiments are conducted on synthetic and real-world datasets, including those from the field of biostatistics. The results demonstrate that the MOTSA outperforms state-of-the-art algorithms in terms of accuracy, robustness, and resilience.\n\nInterdisciplinary Application: The paper also highlights the utility of the proposed framework in various domains, particularly in biostatistics, demonstrating its versatility and applicability.\n\nOpen Source Code: The authors have made the code publicly available, encouraging further research and development in this area."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Strengths of the Paper\nOriginality\nThe paper makes a notable contribution to the field of meta-learning with a focus on multimodal data. The introduction of Shifted Feature Transducers (SFTs) for encoding both parameters and gradients is novel and insightful. The unique combination of unimodal-specific feature transducers (USFTs), multimodal-shared feature transducers (MSFTs), and fusion MLPs offers a new perspective on how to effectively leverage multimodal data for meta-learning. This is a creative amalgamation of existing ideas, and it clearly extends the state of the art.\n\nQuality\nThe quality of the paper is high, both in terms of technical depth and experimental rigor. The model is built upon well-motivated mathematical foundations, and the empirical evaluation is comprehensive. The paper goes beyond merely showing that their method works; it also provides ablation studies to isolate the contributions of different components and offers a theoretical discussion about the same.\n\nClarity\nThe paper is well-structured, providing a logical flow that is easy to follow. Each section contributes to the overall narrative coherently. The mathematical notation is consistently used, making the paper accessible to readers familiar with machine learning and meta-learning. The figures and tables are well-designed and effectively complement the text. The paper adheres to high standards of academic writing, making it a clear presentation of a complex subject matter.\n\nSignificance\nThe significance of this work lies in its potential to substantially impact both the theory and practice of meta-learning in multimodal settings. The multimodal challenges addressed in the paper are highly relevant to numerous real-world applications, such as healthcare, climate modeling, and audio-visual recognition. By showing superiority over existing methods across diverse datasets, the paper makes a compelling case for the generalizability and applicability of its contributions. The method's ability to improve performance in low-data regimes is particularly noteworthy, given the increasing importance of data-efficient learning in practical applications."
                },
                "weaknesses": {
                    "value": "Implicit Neural Representations (INRs)\nEquation 1: \nComment: This loss function is a straightforward L2 loss, which is not inherently problematic but may not be the best choice for all types of problems. For instance, if the goal is robustness against outliers, then an L1 loss or Huber loss might be more suitable.\nWeakness: The paper does not discuss the choice of loss function and its suitability for the tasks at hand.\nMeta-Learning Approach\nEquation 2: \nComment: This equation extends Equation 1 by adding context parameters \u03d5. However, the paper does not provide a mathematical justification for the choice of this extension.\nWeakness: The addition of context parameters \u03d5 increases the model complexity without a detailed explanation or justification. This can be an issue if the goal is to keep the model as simple as possible for interpretability or computational efficiency.\nEquation 3 and 4: Meta-objective and update rule for \u03d5.\n\nComment: The meta-objective is a standard formulation. However, the paper does not discuss the potential issues that could arise from a bi-level optimization problem, such as saddle points or local minima.\nWeakness: The paper lacks a rigorous mathematical analysis of the optimization landscape, which is crucial for understanding the method's efficiency and effectiveness.\nApproach\nMultimodal Iterative Adaptation (MIA)\nEquation 5 to 11: These equations describe the MIA approach.\nComment: These equations introduce an elaborate framework that involves several novel components, like State Fusion Transformers (SFTs). However, it's not clear how these equations were derived or why they are theoretically sound.\nWeakness: The paper introduces several novel ideas but does not provide a theoretical justification for them. This lack of theoretical grounding makes it difficult to assess the quality and applicability of the proposed method.\nFigures, Tables, and Diagrams\nFigure 2: Schematic illustration of MIA.\n\nComment: While visually appealing, the figure does not offer a quantitative evaluation of the proposed method's performance.\nWeakness: The lack of quantitative metrics in the figure makes it less informative.\nTable 1 and 2: Quantitative comparisons.\n\nComment: These tables provide a valuable quantitative comparison of the proposed method against baselines. However, they lack statistical tests to prove the significance of the reported results.\nWeakness: The absence of statistical tests makes it difficult to determine the reliability of the proposed method compared to the baselines.\n\nSection 5.2: Multimodal 2D CelebA Dataset\nNovelty and Comparison to Baselines: The results appear to align well with the established literature, showing better performance for multimodal methods over unimodal ones. However, the manuscript could enhance its impact by discussing why ALFA performs better than multimodal methods when sufficient support sets are present. Is this a limitation of the proposed approach or an intrinsic property of the dataset?\n\nAmbiguity in Setup: While the section describes the use of a pre-trained model for surface normals, it lacks explicit detail on how this could affect the generalizability and transferability of the learned features. Clarification and potential ablation studies could be beneficial.\n\nQuality of Results: The manuscript does not delve into the qualitative implications of the MSE values reported. How do these numerical metrics translate into practical improvements? For instance, do lower MSEs correlate with visibly better reconstructions in real-world applications?\n\nSection 5.3: Multimodal Climate Data\nLack of Theoretical Justification: The paper mentions that atmospheric variables are \"relatively stable\" but does not provide a theoretical or empirical justification for why ALFA performs so well in this scenario.\n\nStatistical Significance: The manuscript would be improved by including statistical tests to determine the significance of the observed differences between the proposed method and baselines.\n\nDomain-Specific Implications: Given the critical nature of climate data, an analysis of how errors in the model's predictions could propagate into real-world applications would be valuable.\n\nSection 5.4: Multimodal Audio-Visual AVMNIST Dataset\nInsufficient Rationale for Dataset Selection: The section does not sufficiently justify the choice of the AVMNIST dataset. Given its unique challenges, why was it selected over other multimodal datasets?\n\nLack of Depth in Failure Analysis: It's mentioned that MTNPs fail at approximating the audio signals properly. A deeper analysis into why this failure occurs could offer valuable insights into the limitations of existing methods, thereby contextualizing the contributions of the proposed method more effectively.\n\nAmbiguity in Methodology: The manuscript mentions that the audio signals were trimmed. However, it does not explain how this preprocessing step might affect the meta-learning process.\n\nSection 5.5: Analysis Study\nInadequate Ablation Studies: While the section provides a valuable ablation study to understand the impact of various modules, it is relatively shallow. For instance, it would be beneficial to understand how each of these modules contributes to reducing overfitting or improving convergence speed.\n\nNon-Uniform Metric Analysis: The section uses relative error reduction as a metric but does not justify why this is an appropriate measure of performance. It might be valuable to consider other metrics like F1-score or ROC AUC, especially when comparing across multiple modalities.\n\nLack of Interpretability Discussion: Given the complex architecture involving USFTs, MSFTs, and Fusion MLPs, a discussion on model interpretability would be pertinent. This is essential for real-world applications where understanding model decisions is crucial.\n\nGeneral Remarks\nLack of Hyperparameter Sensitivity Analysis: Across all experiments, there is no discussion on how sensitive the model is to the choice of hyperparameters. This is a critical aspect to understand the robustness of the proposed methods.\n\nReproducibility Concerns: The manuscript could benefit from a clearer exposition of experimental details to ensure reproducibility.\n\nPotential for Overfitting: Given the high complexity of the model, especially with the introduction of specialized modules like USFTs and MSFTs, there might be a risk of overfitting. An analysis or discussion on this would be beneficial."
                },
                "questions": {
                    "value": "General\nReproducibility: Could you provide a clear list of hyperparameters used in your experiments? This information is crucial for reproducibility and for understanding the sensitivity of your model to hyperparameter changes.\nSection 5.2: Multimodal 2D CelebA Dataset\nRole of Pre-trained Models: Could you elaborate on the role of the pre-trained model used for obtaining surface normals? How would the absence of this pre-trained model affect the results?\n\nALFA's Performance: Your model underperforms compared to ALFA in certain scenarios. Could you discuss whether this is a limitation of your model or an intrinsic property of the dataset?\n\nSection 5.3: Multimodal Climate Data\nStatistical Significance: Could you provide statistical tests to back the significance of the results? This would solidify the comparative performance claims.\n\nTheoretical Justification for ALFA's Performance: You mention that ALFA performs well because climate variables are \"relatively stable.\" Could you provide empirical or theoretical evidence to support this claim?\n\nSection 5.4: Multimodal Audio-Visual AVMNIST Dataset\nDataset Choice Justification: Could you explain the rationale behind choosing the AVMNIST dataset for your experiments?\n\nFailure of MTNPs: You mention that MTNPs fail to approximate the audio signals well. Could you delve deeper into the reasons for this failure?\n\nAudio Signal Trimming: Could you explain how the trimming of audio signals might have affected the results and why this preprocessing was necessary?\n\nSection 5.5: Analysis Study\nChoice of Metrics: You use relative error reduction as a metric in your ablation studies. Could you justify why this is an appropriate metric?\n\nInterpretability: Given the complexity of your model, how do you address the challenge of interpretability? Could you discuss any measures or future work planned to make the model's decisions interpretable?\n\nDepth of Ablation Studies: The ablation study, while useful, seems relatively shallow. Could you comment on the potential for a more extensive ablation study to understand the impact of each module in greater depth?\n\nOverfitting Concerns: With the high complexity of the model, how do you ensure that the model does not overfit? Could you provide any analysis or empirical evidence to support the model's robustness?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1819/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698724235954,
            "cdate": 1698724235954,
            "tmdate": 1699636111314,
            "mdate": 1699636111314,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "m4ES8CYaxA",
                "forum": "vSOTacnSNf",
                "replyto": "MaxFN8uLY6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1819/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1819/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response of the authors to Reviewer cLC2 [1/5]"
                    },
                    "comment": {
                        "value": "> Q1: This loss function is a straightforward L2 loss, which is not inherently problematic but may not be the best choice for all types of problems. For instance, if the goal is robustness against outliers, then an L1 loss or Huber loss might be more suitable. The paper does not discuss the choice of loss function and its suitability for the tasks at hand. \n\nA1: We opt for L2 (MSE) loss since we find it is the most common choice for the loss function in the literature of implicit neural representations. Nevertheless, we agree with the reviewers that the optimal choice for the loss function could vary for each problem, and leave this investigation as our future work.\n\n>Q2: \nComment: Equation 2 extends Equation 1 by adding context parameters \u03d5. However, the paper does not provide a mathematical justification for the choice of this extension. \nWeakness: The addition of context parameters \u03d5 increases the model complexity without a detailed explanation or justification. This can be an issue if the goal is to keep the model as simple as possible for interpretability or computational efficiency. \n\nA2: Our extension from Equation 1 to Equations 2, 3, and 4 relies directly on CAVIA; such a formulation is extensively studied in various domains, including implicit neural representations.\n\nIt's important to note that the introduction of context parameters \u03d5 of CAVIA offers several advantages over MAML in terms of overfitting, computational efficiency, and interpretability. For instance, by separating task-specific adaptations (\u03d5) from task-agnostic generalizable features (\u03b8), CAVIA effectively mitigates overfitting and reduces computational overhead during adaptation. In addition, the independent adaptation of \u03d5 allows for efficient parallelization, thereby reducing the time required for meta-learning. Moreover, the context parameters (\u03d5) are found to capture the latent task structure and operate as low-dimensional task-specific embeddings, leading to greater interpretability than MAML. We kindly refer the reviewer to the original paper of CAVIA for the detailed discussion.\n\n\n>Q3:\nComment: The meta-objective in Equation 3 and 4 is a standard formulation. However, the paper does not discuss the potential issues that could arise from a bi-level optimization problem, such as saddle points or local minima. \nWeakness: The paper lacks a rigorous mathematical analysis of the optimization landscape, which is crucial for understanding the method's efficiency and effectiveness. \n\nA3: We acknowledge the reviewer\u2019s concern. However, as mentioned by the reviewer, such a bi-level optimization in Equation 3 and 4 is standard formulation in the widely studied model-agnostic meta-learning literature. Moreover, none of the optimization issues was encountered in any of the experiments conducted for our paper. Please note that such potential pathologies can be readily alleviated by utilizing gaussian-blurred smooth objectives [ES,PES], and we leave this investigation as part of our future work.\n\n[ES]: Luke Metz, Niru Maheswaranathan, Jeremy Nixon, C. Daniel Freeman, Jascha Sohl-Dickstein, Understanding and correcting pathologies in the training of learned optimizers, In ICML, 2019.  \n[PES]: Paul Vicol, Luke Metz, Jascha Sohl-Dickstein, Unbiased Gradient Estimation in Unrolled Computation Graphs with Persistent Evolution Strategies, In ICML, 2021\n\n>Q4:\nComment: These equations introduce an elaborate framework that involves several novel components, like State Fusion Transformers (SFTs). However, it's not clear how these equations were derived or why they are theoretically sound. \nWeakness: The paper introduces several novel ideas but does not provide a theoretical justification for them. This lack of theoretical grounding makes it difficult to assess the quality and applicability of the proposed method. \n\nA4: \nEquations from 5 to 7 are equivalent to meta-learning each unimodal framework separately. In addition, Equations from 8 to 11 for MIA share a similar spirit with the studies in the learning-to-optimize domain, where our MIA can be interpreted as an extension of such widely studied learned optimization algorithms to approximating joint multimodal signals, exploring an interesting idea to accelerate the convergence of independent unimodal learners by capturing the multimodal structures in their on-going learning states and landscapes during iterative adaptation procedures."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1819/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740756205,
                "cdate": 1700740756205,
                "tmdate": 1700741515521,
                "mdate": 1700741515521,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IS0VsKk5CQ",
                "forum": "vSOTacnSNf",
                "replyto": "MaxFN8uLY6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1819/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1819/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response of the authors to Reviewer cLC2 [3/5]"
                    },
                    "comment": {
                        "value": "> Q9: The manuscript does not delve into the qualitative implications of the MSE values reported. How do these numerical metrics translate into practical improvements? For instance, do lower MSEs correlate with visibly better reconstructions in real-world applications?\n\nA9: Lower MSE values indeed signify reduced error in signal reconstruction or prediction. As pointed out by the reviewer, however, these numerical improvements might not perfectly align with human perception or subjective judgment of signal quality. Nevertheless, we opt for it as our performance measure since MSE and its logarithmic derivative PSNR are commonly used metrics for evaluating the fidelity of reconstructions in implicit neural representations learning domains.\n\n>Q10: Given the critical nature of climate data, an analysis of how errors in the model's predictions could propagate into real-world applications would be valuable.\n\nA10: We believe understanding and mitigating such errors are fundamental to ensuring the reliability and practicality of any machine learning framework, and thereby we value the reviewer's insightful suggestion. Inaccurate climate predictions have far-reaching implications, potentially impacting (1) the formulation of environmental policies and disaster management strategies, (2) industries and economic planning reliant on climate forecasts, and (3) overall societal safety and preparedness.\n\n>Q11:\nThe section does not sufficiently justify the choice of the AVMNIST dataset. Given its unique challenges, why was it selected over other multimodal datasets?\nIt's mentioned that MTNPs fail at approximating the audio signals properly. A deeper analysis into why this failure occurs could offer valuable insights into the limitations of existing methods, thereby contextualizing the contributions of the proposed method more effectively.\n\n\nA11: The rationale behind incorporating the AVMNIST dataset in our experiments is to showcase our approach's efficacy in handling diverse joint multimodal function modeling scenarios. Unlike datasets such as synthetic, CelebA, and climate data that commonly exhibit strong axis-aligned relationships among functions, real-world scenarios often lack such alignment. For instance, audiovisual signals present distinct function domains (or coordinate systems) between image and audio signals, posing a significant challenge for models in capturing cross-modal relationships due to the absence of explicit spatiotemporal correspondence.\n\nRegarding the failure of MTNPs in approximating audio signals within the AVMNIST dataset, the MTNPs\u2019 architecture relies on an \"across-task inference\" mechanism that facilitates cross-modal information exchange among axis-aligned features (see Equation 13 or Figure 2 in the corresponding paper). In AVMNIST, however, we omitted this mechanism due to the lack of explicit coordinate correspondence between images and audios, which was inevitable to train and evaluate MTNPs in AVMNIST. It\u2019s noteworthy that cross-modal interaction can still be captured in the latent path of the first stream, as described in the Appendix D.2.\n\nTo validate the above discussion further, we conducted additional experiments investigating MTNPs' dependence on axis-aligned attention mechanisms. Specifically, we compared MTNPs' performances when trained with or without such mechanisms in axis-aligned multimodal CelebA datasets. We present the results in the table below. The results indicate that MTNPs without axis-aligned attention significantly underperformed. This suggests the essential role of axis-aligned attention in MTNPs for approximating multimodal functions, indicating potential limitations in modeling heterogeneous multimodal functions that cannot use axis-aligned attention effectively.\n\n||RGBs||||Normals||||Sketchs||||\n|--|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\n|Rmin|0.00|0.25|0.50|0.75|0.00|0.25|0.50|0.75|0.00|0.25|0.50|0.75|\n|Rmax|0.25|0.50|0.75|1.00|0.25|0.50|0.75|1.00|0.25|0.50|0.75|1.00|\n|**MTNPs (Functa)**|9.871|4.807|4.105|3.644|3.983|2.552|2.339|2.221|9.680|6.568|5.395|4.819|\n|**MTNPs (Functa) w/o across-task**|13.82|6.452|5.488|4.898|4.576|2.837|2.579|2.448|12.07|7.888|6.603|6.022|\n|**MTNPs (Composers)**|9.902|4.957|4.269|3.813|4.184|2.747|2.545|2.437|9.791|6.425|5.163|4.544|\n|**MTNPs (Composers) w/o across-task**|13.76|6.421|5.474|4.897|4.635|2.888|2.639|2.513|11.89|7.411|6.031|5.403|"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1819/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740810651,
                "cdate": 1700740810651,
                "tmdate": 1700741767486,
                "mdate": 1700741767486,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0IluKpcY9V",
                "forum": "vSOTacnSNf",
                "replyto": "MaxFN8uLY6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1819/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1819/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response of the authors to Reviewer cLC2 [4/5]"
                    },
                    "comment": {
                        "value": ">Q12:\nThe manuscript mentions that the audio signals were trimmed. However, it does not explain how this preprocessing step might affect the meta-learning process.\nCould you explain how the trimming of audio signals might have affected the results and why this preprocessing was necessary?\n\nA12: In our study, we performed two preprocessing steps on the audio signals. First, we (1) decreased the sampling rate of the audio signals, followed by (2) standardizing all audio signals to a consistent length through trimming or zero-padding. Step (2) is a common practice in audio-related domains, facilitating batching during training and evaluation, which significantly reduces computational time.\n\nThe decision for step (1) was specifically driven by the complexities encountered in training the multimodal baseline model, MTNPs. MTNPs exhibit quadratic complexity with respect to the support/query set size, due to the attention mechanisms directly applied to coordinate-feature pairs within these sets. This imposes excessive demands on memory and computational resources, particularly when handling audio signals, making the training process infeasible without decreasing the sampling rate.\n\nDuring the preprocessing of these audio signals, we confirmed that the reduction in sampling rate did not significantly simplify the problem, ensuring that the semantic integrity or quality of the original sounds remains still. Instead, it notably reduced the redundancy within the audio data, making the training of MTNPs considerably more feasible.\n\n>Q13: While the section provides a valuable ablation study to understand the impact of various modules, it is relatively shallow. For instance, it would be beneficial to understand how each of these modules contributes to reducing overfitting or improving convergence speed.\n\nA13: We appreciate the constructive feedback. In response, we conducted a more in-depth ablation study to include all possible component combinations to analyze how each of them contribute to performance. Due to time and computational limitations, we focused on the impact of each component on vanilla Composers in the synthetic dataset. The results are below.\n\n|||Ablative components||Relative Error Reduction||\n|:--:|:--:|:--:|:--:|:--:|:--:|\n|Combination|USFTs|MSFTs|FusionMLPs|Generalization|Memorization|\n|(1)|X|X|X|0|0|\n|(2)|X|X|O|38.9|54.2|\n|(3)|O|X|X|44.4|63.6|\n|(4)|O|X|O|43.1|68.0|\n|(5)|X|O|X|86.8|71.6|\n|(6)|X|O|O|86.8|76.7|\n|(7)|O|O|X|86.7|75.5|\n|(8)|O|O|O|**88.7**|**81.6**|\n\nBy comparing the results (1) and (2), we find that FusionMLPs only can greatly enhance the gradients of vanilla Composers, such as by modifying the gradient direction or magnitude, or even the ones in combination with USFTs, MSFTs or both (3 vs 4, 5 vs 6, 7 vs 8). The improvement in memorization capability is more significant when vanilla Composers is further augmented with USFTs (1,2 vs 3,4), which is aligned with the study in our main paper. This indicates that USFTs is especially beneficial for capturing modality-specific patterns in the states of the learners. Unlike USFTs, we observe that MSFTs excel in utilizing cross-modal interactions among the learners\u2019 states (3,4 vs 5,6,7,8), boosting the generalization performances of Composers significantly. Lastly, the most optimal performance is achieved when utilizing a combination of USFTs, MSFTs, and FusionMLPs, underlining the indispensable roles of each component within SFTs. This result validates the unique and crucial contribution of each component to the overall effectiveness of Composers.\n\n>Q14: The section uses relative error reduction as a metric but does not justify why this is an appropriate measure of performance. It might be valuable to consider other metrics like F1-score or ROC AUC, especially when comparing across multiple modalities.\n\nA14: We thank the suggestion. We didn't explore the use of F1-score or ROC AUC since these metrics are designed for assessing (binary) classifiers, particularly in situations where classes are imbalanced. \n\nAs our paper primarily focuses on regression problems on joint multimodal continuous functions, we opted for alternative evaluation metrics, i.e. the average relative error reduction across sampling ratio ranges and modalities. The reasons are two-folded: (1) it remains robust against widely varying MSE magnitudes or scales influenced by modalities and sampling ratios, and (2) it offers a holistic understanding of the individual components' impacts within our ablation study. \n\nIt's worth noting that Kim et al. also employed the same metric (referred to as relative performance gain in their paper) for similar reasons and purposes. They utilized this metric in their investigation of MNTPs' capability to capture cross-modal relationships (please refer to Section 5.3 and Table 4 in Kim et al.).\n\nReference: Donggyun Kim, Seongwoong Cho, Wonkwang Lee, and Seunghoon Hong. Multi-task neural processes. In ICLR, 2022."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1819/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740838926,
                "cdate": 1700740838926,
                "tmdate": 1700741816871,
                "mdate": 1700741816871,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dBdfMIHVsj",
            "forum": "vSOTacnSNf",
            "replyto": "vSOTacnSNf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1819/Reviewer_gNAu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1819/Reviewer_gNAu"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses multimodal learning of implicit neural representations by meta learning. In particular, they deal with a setting where data is scarce. The authors introduce a novel optimisation-based meta-learning framework, which they call Multimodal Iterative Adaptation (MIA). They claim MIA enables continuous the interaction among independent unimodal INR learners, and therefore the cross-modal relationships can be better captured through iterative optimization steps. In addition, they introduce a meta-learning module called state fusion transformers to aggregate states of unimodal leraners. Extensive experiments are conducted including 1D synthetic functions, real-world vision, climate, and audiovisual data."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ It seems an interesting idea to learning implicit neural representations of multimodal data.\n\n+ The proposed state fusion transformers to aggregate the states of the unimodal learners is also new.\n\n+ The experimental evaluation is extensive and solid."
                },
                "weaknesses": {
                    "value": "- It is a bit unclear to me why the proposed iterative way of learning could be better. Both theoretical and intuitive explanation is missing since this is the core of the proposed multimodal iterative adaptation.\n\n- The authors indicate their multimodal iterative adaptation could better handle limited data compared to gradient based algorithms. This is not explained well either."
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1819/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699062911183,
            "cdate": 1699062911183,
            "tmdate": 1699636111237,
            "mdate": 1699636111237,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tGVumJQlOL",
                "forum": "vSOTacnSNf",
                "replyto": "dBdfMIHVsj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1819/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1819/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response of the authors to Reviewer gNAu [1/1]"
                    },
                    "comment": {
                        "value": ">Q1: Why MIA can better handle limited data\n\nA1: When data of one modality is limited, the computed gradients from the learner of that modality become inevitably noisy, which slows down the convergence of the learner and triggers overfitting. Interestingly, we find that our SFTs, through attention mechanisms, can incur positive knowledge transfer from the learner whose data is sufficient to the other learners with the limited data. This positive transfer compensates for low-quality gradients, enabling enhanced guidance to the learners. Moreover, SFTs prevent negative transfer as well, refraining from updating the state representations of the learner for a specific modality when it observed sufficient data. We included the detailed discussion on this analysis in Section 5.5 (marked in blue) and Appendix F.1 of the paper.\n\n>Q2: Why do we need to apply multimodal adaptation sequentially or iteratively?\n\nA2: Since the total amount of observable data is fixed during the adaptation, any attempt to update each learner with its computed naive gradients has potential to hinder the convergence or trigger overfitting, particularly when data is limited. Therefore, our meta-learning framework involves multimodal adaptation throughout all optimization steps.\n\nTo validate this claim, we conducted additional experiments, wherein we compared our MIA with the two alternative ablative approaches: applying the multimodal adaptation only at the first (MA-First) or last (MA-Last) optimization step. The experimental results on the synthetic dataset are below.\n\n||Sine|||Gaussian|||Tanh|||ReLU|||\n|--|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\n|Rmin|0.01|0.02|0.05|0.01|0.02|0.05|0.01|0.02|0.05|0.01|0.02|0.05|\n|Rmax|0.02|0.05|0.10|0.02|0.05|0.10|0.02|0.05|0.10|0.02|0.05|0.10|\n|Functa|44.26|16.07|3.319|18.81|4.388|0.953|22.61|3.667|0.586|65.29|10.79|2.157|\n|w/ MA-First|14.29|5.660|1.113|2.261|1.160|0.367|3.180|0.797|0.141|13.47|2.067|0.290|\n|w/ MA-Last|10.12|4.462|1.717|1.253|0.959|0.638|1.719|0.541 |0.267|7.063|1.254|0.305|\n|**w/ MIA**|**6.386**|**2.058**|**0.547**|**1.057**|**0.571**|**0.281**|**1.285**|**0.378**|**0.131**|**5.069**|**1.012**|**0.115**|\n|Composers|37.40|16.70|5.284|5.923|3.149|1.460|14.81|4.053|1.029|48.49|11.98|3.232|\n|w/ MA-First|16.68|7.076|2.245|2.843|1.730|0.737|5.240|1.446|0.442|22.62|3.685|0.826|\n|w/ MA-Last|14.57|9.549|7.706|1.699|1.462|1.086|2.411|1.021 |0.725|10.19|2.340|0.907|\n|**w/ MIA**|**5.564**|**1.844**|**0.627**|**0.975**|**0.528**|**0.237**|**1.257**|**0.343**|**0.128**|**4.715**|**0.943**|**0.156**|\n\nAs observed, MA-First consistently exhibits poor generalization (R <= 0.05), suggesting that the subsequent independent adaptation of each learner with limited data triggers overfitting. Conversely, MA-Last's late fusion strategy appears to address overfitting when data is scarce (R <= 0.05), evidenced by its improved generalization performances. However, as a notable downside, this fusion strategy tends to disrupt previously good solutions obtained with sufficient data, resulting in poorer memorization performances (R >= 0.05).\n\nCrucially, MIA consistently outperforms these single-time adaptation methods. To delve deeper, in Figure 12 in the appendix, we also visualized the evolving interactions among learners for each optimization step by examining the attention patterns across the learners within MSFTs. In short, it reveals that learners tend to initially interact extensively with each other (high off-diagonal attention scores in the beginning) through MSFTs, followed by gradually focusing more on their individual states towards the end (high diagonal attention scores in the end). This analysis further underscores MIA's adaptability in ensuring a balanced utilization of multimodal interactions, emphasizing the necessity of such an iterative multimodal adaptation scheme."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1819/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700741038414,
                "cdate": 1700741038414,
                "tmdate": 1700741038414,
                "mdate": 1700741038414,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]