[
    {
        "title": "Embodied Active Defense: Leveraging Recurrent Feedback to Counter Adversarial Patches"
    },
    {
        "review": {
            "id": "JaQ2MLlmFE",
            "forum": "uXjfOmTiDt",
            "replyto": "uXjfOmTiDt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1185/Reviewer_YXUb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1185/Reviewer_YXUb"
            ],
            "content": {
                "summary": {
                    "value": "It develops Embodied Active Defense (EAD), a proactive defensive strategy that actively contextualizes environmental information to address misaligned adversarial patches in 3D real-world settings. To achieve this, EAD develops two central recurrent sub-modules, i.e., a perception module and a policy module, to implement two critical functions of active vision. These models recurrently process a series of beliefs and observations, facilitating progressive refinement of their comprehension of the target object and enabling the development of strategic actions to counter adversarial patches in 3D environments."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "To optimize learning efficiency, it  incorporates a differentiable approximation of environmental dynamics and deploy patches that are agnostic to the adversary\u2019s strategies.\n\nExtensive experiments demonstrate that EAD substantially enhances robustness against a variety of patches within just a few steps through its action policy in safety-critical tasks (e.g., face recognition and object detection), without compromising standard accuracy.\n\nFurthermore, due to the attack-agnostic characteristic, EAD facilitates excellent generalization to unseen attacks, diminishing the averaged attack success rate by 95% across a range of unseen adversarial attacks.\n\nIt theoretically demonstrates the effectiveness of EAD from the perspective of information theory. A well-learned EAD model for contrastive task adopts a greedy informative policy to explore the environment, utilizing the rich context information to reduce the abnormally high uncertainty of scenes caused by adversarial patches."
                },
                "weaknesses": {
                    "value": "It mentions the policy model with actions, and states. It is better to provide more details or examples to specify what the actions and states look like or their physical meanings if any. \n\nIt is better to discuss the complexity for the training and inference of the proposed method."
                },
                "questions": {
                    "value": "see the weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1185/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698639107293,
            "cdate": 1698639107293,
            "tmdate": 1699636045014,
            "mdate": 1699636045014,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QAFdZgMu9O",
                "forum": "uXjfOmTiDt",
                "replyto": "JaQ2MLlmFE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1185/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1185/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the valuable review"
                    },
                    "comment": {
                        "value": "We gratefully acknowledge your comprehensive and insightful review of our manuscript. Your detailed feedback and constructive criticism are highly appreciated, as they have significantly contributed to enhancing the overall quality of our work. \n\nWe are particularly encouraged by your recognition of the novelty and clarity of our work. In response to your comments and suggestions, we have carefully revised our manuscript. Below, we address each point you raised, aiming to clarify any uncertainties and improve the manuscript further.\n\n1. **Explanation for the dynamics of the EAD model.**\n\n    To elucidate the actions, states, and their physical meanings in our study, we have added an illustrative example in the new updated Figure 11 (Appendix D.8). These examples demonstrate how the EAD model prioritizes a distinct viewpoint to improve target understanding while trying to keep observing from the viewpoint where adversarial patches are less effective.\n\n2. **Computational overhead.** \n\n    Thanks for pointing out the complexity of our method compared to the baselines. In response, we have conducted a detailed comparison of computational overhead, encompassing aspects such as parameter amount and computation speed during both training and inference phases. The outcomes of this analysis are presented in Table a.\n\n    \n\n     *Table a: Computational overhead comparison of different defense methods in face recognition. We report the training and inference time of defense on a NVIDIA GeForce RTX 3090 Ti and an AMD EPYC 7302 16-Core Processor with the training batch size as 64.* \n\n    | **Method** | **\\# Params (M)** | **Description**                                              | **Training Epochs** | **Training Time per batch (s)** | Overall Training time (GPU hours) | **Inference Time per step (ms)** |\n    | ---------- | ----------------- | ------------------------------------------------------------ | ------------------- | ------------------------------- | --------------------------------- | -------------------------------- |\n    | JPEG       | -                 | non-parametric                                               | -                   | -                               | -                                 | 9.65                             |\n    | LGS        | -                 | non-parametric                                               | -                   | -                               | -                                 | 26.22                            |\n    | SAC        | 44.71             | Train a  segmenter to detect a patch area, which consists of two phases: training with pre-generated adversarial images and self-adversarial training | 50 + 10             | 0.152 + 4.018                   | 104                               | 26.43                            |\n    | PZ         | 44.71             | Share the same training procedure and segementer as SAC      | 50 + 10             | 0.152 + 4.018                   | 104                               | 11.88                            |\n    | DOA        | 43.63             | An adversarial training method that retrains the feature extractor. | 100                 | 1.732                           | 376                               | 8.10                             |\n    | EAD (ours) | 57.30             | Train the policy model and perception model in two stages    | 50 + 50             | 0.595 + 1.021                   | 175                               | 11.51                            |\n\n    \n\n    As indicated in Table a, in the training phase, the total training time of our EAD model is effectively balanced between the pure adversarial training method DOA and the partially adversarial methods like SAC and PZ. This efficiency stems mainly from our unique USAP approach, which bypasses the need for generating adversarial examples, thereby boosting training efficiency.\n\n     In terms of model inference, our EAD, along with PZ and DOA, demonstrates superior speed compared to LGS and SAC. This is attributed to the latter methods requiring CPU-intensive, rule-based image preprocessing, which diminishes their inference efficiency. Further details, including loss curves and more precise time consumption metrics, are elaborated in Appendix D.6."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1185/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700209046603,
                "cdate": 1700209046603,
                "tmdate": 1700209046603,
                "mdate": 1700209046603,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Ua7uoChCVX",
            "forum": "uXjfOmTiDt",
            "replyto": "uXjfOmTiDt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1185/Reviewer_u8yn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1185/Reviewer_u8yn"
            ],
            "content": {
                "summary": {
                    "value": "In this work, the authors propose an active defense strategy that leverages active movements and recurrent perceptual feedback from the environment to defend against arbitrary adversarial patch attacks. The experiment results show that the proposed strategy can outperform SOTA passive defense strategies in terms of effectiveness and generalizability."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The authors present a pioneering approach in the realm of adversarial robustness by introducing the first proactive defense strategy with embodied perception. It adds to the current defense strategies against patch attacks, which were predominantly passive.\n2. The experiment findings clearly indicate the superiority of the proposed strategy over previous SOTAs. The comprehensive experiments demonstrate that an embodied model with environmental interaction ability can not only mitigate the uncertainty posed by adversarial attacks easily but also be trained with weak attacks like uniform-perturbed patches."
                },
                "weaknesses": {
                    "value": "1. In Section 4.2, the authors mention the effectiveness against adaptive attack; however, I find it hard to understand the reason why a surrogate **uniform** superset policy distribution would necessitate an optimized patch to handle various action policies, as this uniform surrogate may not contain any useful information about the policy model in EAD for the adversary to attack.\n2. The paper could benefit from improved clarity, especially for readers with a foundational understanding of adversarial robustness but limited exposure to RL/Robotics. As it stands, the document is dense with jargon, making it challenging to navigate and comprehend upon the initial read."
                },
                "questions": {
                    "value": "1. For those adversarially-trained passive defense models, would it be beneficial to enhance them with 3D-augmented data, e.g., feeding multiple views of the same patch-attacked human face during training, if the lack of 3D environment awareness is the problem here?\n2. There is one part I feel confused about in the algorithm box: $O_t\u2032 \\gets A(O_t, P; S_t)$ and $O_{t+1}'\\gets A(O_{t+1},P;S_{t+1})$, where do $O_t'$ and $O_{t+1}'$ go? They are not explicitly used or referenced anywhere else post-assignment. Are clean observations $O_t$ and $O_{t+1}$ overwritten by $O_t'$ and $O_{t+1}'$ after applied with the adversarial patch by default?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1185/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1185/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1185/Reviewer_u8yn"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1185/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698794627790,
            "cdate": 1698794627790,
            "tmdate": 1699636044911,
            "mdate": 1699636044911,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "m0C1SWPzTH",
                "forum": "uXjfOmTiDt",
                "replyto": "Ua7uoChCVX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1185/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1185/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the valuable review (1/2)"
                    },
                    "comment": {
                        "value": "We deeply appreciate the time and effort you have invested in reviewing our manuscript. Your insightful feedback has been invaluable in enhancing the quality of our work. We are particularly encouraged by your recognition of the novelty and effectiveness of our proposed active defense strategy. \n\nIn this revised submission, we have carefully addressed each of your comments and questions.  Below, we provide detailed responses to your specific points, aiming to clarify the aspects you've raised and further reinforce the strengths of our work.\n\n1. **Effectiveness of proposed adaptive attack**.\n\n    Your inquiry into the efficacy of a surrogate uniform superset policy distribution is highly relevant. In our initial submission, attacking EAD across a 4-step trajectory was impractical due to excessive GPU memory requirements (approximately 90 GB for 4 steps). Besides our original technique, we have explored gradient checkpointing [1], a method that allows for recalculating parts of the computation graph established by the $\\tau$-step EAD inference process, rather than storing them directly. This reduces memory usage while increasing computational demands. Employing this method, we successfully attacked the entire pipeline along a 4-step trajectory using an NVIDIA RTX 3090 Ti.\n\n    To empirically assess the Uniform Superset Policy (USP), we compared it with our new adaptive attack leveraging gradient checkpointing (denoted as GC) across the entire trajectory.\n\n    \n\n    *Table a: Evaluation of Adaptive Attacks on the EAD Model. Columns marked **USP** display results from optimizing the patch using an expected gradient over USP. **GC** columns represent results from attacking EAD following the gradients along the 4-step trajectory with gradient checkpointing.*\n\n    |              | **Dodging** |        | **Impersonation** |        |\n    | :----------: | :---------: | :----: | :---------------: | :----: |\n    |              |   **USP**   | **GC** |      **USP**      | **GC** |\n    | **ASR (\\%)** |  **22.11**  | 15.79  |       **8.33**        |  7.29  |\n\n    \n\n    As Table a demonstrates, our original adaptive attack using USP was more effective than tracing the authentic policy of EAD. This may be attributed to vanishing or exploding gradients impeding optimization [2]. This problem is potentially mitigated by our approach that computes expectations over a uniform policy distribution. Additionally, our method maintains its effectiveness against various adaptive attacks. This underscores that EAD's defense stems from the synergistic integration of its policy and perception models, encouraging strategic data collection rather than merely countering adversarial patches with specific viewpoints. Further details and evaluation results on these adaptive attacks are elaborated in the revised Appendix D.3 and D.7 respectively.\n\n2. **Clarity improvement**.\n\n    Thanks for your pointing it out. Regarding your concerns about clarity for readers with foundational knowledge of adversarial robustness but limited exposure to RL/Robotics, we have added a section in simplified language to introduce the relevant background, enhancing readability. The comprehensive background information is detailed in the Appendix B. We hope that these revisions make our work more accessible to a broader audience."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1185/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700208823366,
                "cdate": 1700208823366,
                "tmdate": 1700208823366,
                "mdate": 1700208823366,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xoefIk8UTS",
            "forum": "uXjfOmTiDt",
            "replyto": "uXjfOmTiDt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1185/Reviewer_V4oi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1185/Reviewer_V4oi"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a model robustness approach Embodied Active Defense (EAD) for perception models against adversarial patches. EAD comprises of perception module to extract the image observation features and policy module to interact with the environment (modeled by decision transformer). The paper hypothesizes that passive robustness methods (i.e without temporal information or scene contextual information) would not be sufficient for unseen adversarial attacks and needs active feedback from the environment to achieve better model robustness. EAD training is a two stage learning approach performed in a dynamic environment (i.e. the generative space of faces using EG3D or CARLA simulation)  - 1) training the perceptual model for the specific task (for eg face recognition or object detection) using random policy 2) co-training the perception module along with the policy. The paper shows robustness to different types of patch based adversarial attacks and adaptive adversarial attacks. The paper also proposes to introduce uniformly sampled noise as adversarial examples while training the model that perturbs the observation. The paper empirically shows adversarial robustness improvement across unseen type of attacks and also shows improvement on clean samples (i.e overall test accuracy of a task in scenarios where adversarial samples are not introduced)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper explores an adversarial robustness technique that helps the model to achieve better adversarial robustness in a dynamic environment. The proposed approach \u201cEmbodied Active Defense (EAD)\u201d achieves improvement on various patch adversarial attack methods for the tasks of face recognition and object detection. The paper provides ablation experiments to analyze the various components of EAD algorithm. Ablation experiments regarding the strength of adversarial attacks (by increasing the patch size or increasing the iteration of iterative adversarial attacks) sheds light on the robustness of the proposed model against strong version of adversarial attacks. The supplementary material contains code for reproducibility"
                },
                "weaknesses": {
                    "value": "Overall, the paper writing style and organization needs improvement to allow the reader to easily understand the paper.\nIt would be helpful for the reader to get a better understanding for the following:\n1.  Some general suggestions regarding paper organizing. It would be great to introduce the problem statement for each of the tasks:\n    1.  For eg. definition/explaining of the subtasks of FR : Impersonation and dodging.\n    2.  Environmental model definition (State, Action, Transition Function and Observation Function similar to Face Recognition task) for object detection on CARLA simulator or ShapeNet EG3D.\n    3. In the related section, a brief explanation of the attacks used for evaluation.\n    4. Annotation definitions before introducing in the paper (for eg y^{_}_t prediction of the model) \n    5. (Optional) a readme for the codebase, to understand the outline of the codebase.\n2. The paper makes a claim that passive adversarial defense approach are not sufficient for dynamics environment. This claim should be supported by a passive defense used as a baseline. Also, it would be great to see the extra amount of training resources used to train the active embodied model vs a passive defense approach (for eg Madry's PGD adversarial training). It would also help the readers if some other active defenses would be used as a baseline in order to establish the efficacy of the proposed approach.\n3. The experimental (both training and evaluation) setting does not seem sufficient and scalable enough to make conclusions that it would general. For example \u201cTo train EAD for face recognition, we randomly sample images from 2, 500 distinct identities from the training set of CelebA-3D.\u201d and \u201cwe report the white-box attack success rate (ASR) on 100 identity pairs in both impersonation and dodging attacks with various attack methods\u201d.\n\nMinor Typo/ suggestions \u2028\nEAD comprises two primary submodules: -> EAD comprises of two primary submodules:\n\nNotely, it maintains or even improves standard accuracy due to -> Notably, it maintains or even improves standard accuracy due to\n\nFormally, It derives a strategic action -> Formally, it derives\n\n\u201cthey have now developed to perceive various perception models\u201d\n\n\u201cIt is noteworthy this presents a more versatile 3D formulation for adversarial patches\u201d\n\nfrom given scene x with its annotation y from another modal like CLIP -> from given scene x with its annotation y from another model like CLIP\n\n\u201cIn D.5 MORE EVALUATION RESULTS ON EG3D, We present the iterative defense process in Figure ??.\u201d\u2028\u2028\n\nIn the statement \u201cEAD presents a promising direction for enhancing robustness without any negative social impact\u201d, it might be helpful to limit this statement as the one of the tasks being used is facial recognition that could have some unwanted impact ."
                },
                "questions": {
                    "value": "It would be helpful if the paper could answer/clarify the following questions:\n1. How much training time/ wall clock time and memory resources does it take for the EAD defense (both at training and inference phase).\n2. The general assumption about adversary is that it is not bound by computation resources. For example the statement in the paper for adaptive attacks \u201cWhile the deterministic and differential approximation could enable backpropagation through the entire inference trajectory of EAD, the computational cost is prohibitive due to rapid GPU memory consumption as trajectory length \u03c4 increases\u201d, here for Face recognition can we do a white box attack by following the gradients for all the 4 -step trajectory?\n3. In the paper the description for Figure 3 mentions that \u201csubsequent active interactions with the environment progressively reduce the similarity between the positive pair\u201d. Is this a typo, should this be \u201creduce the dissimilarity\u201d or \u201cincrease the similarity\u201d?\n4. In Algorithm 1, the perturbed observation O\u2019t is not being used, is there a typo there ?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1185/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1185/Reviewer_V4oi",
                        "ICLR.cc/2024/Conference/Submission1185/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1185/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699073135265,
            "cdate": 1699073135265,
            "tmdate": 1700638089491,
            "mdate": 1700638089491,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DloV0Gmuzo",
                "forum": "uXjfOmTiDt",
                "replyto": "xoefIk8UTS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1185/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1185/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the valuable review (1/3)"
                    },
                    "comment": {
                        "value": "We express our sincere appreciation for your meticulous and insightful review of our submission. Your detailed analysis and constructive feedback are invaluable to us, and we appreciate the time and effort you invested in evaluating our work.\n\nIn response to your suggestions and critiques, we have diligently worked on revising our paper to enhance its clarity, organization, and overall contribution to the field. Below, we outline the specific changes and improvements made in accordance with your valuable feedback. \n\n1. **Improvement of writing and clarity.**\n\n    We understand your concern regarding the paper's writing style and organization. We have worked diligently to address these issues to make readers easily grasp the content. Specifically, we have made the following improvements:\n\n    1.1 A detailed introduction to the tasks of impersonation and dodging attacks has been added in the updated Appendix D.2 for improved clarity.\n\n    1.2 The environmental setup for object detection, similar to that used in face recognition, primarily varies in the feasible region of viewpoints. We've included a comprehensive description of this setting, incorporating the CARLA simulator and ShapeNet EG3D in the tail of Appendix C.\n\n    1.3 The explanation for the used attacks including MIM, EoT, GenAP and Face3DAdv, has been elaborated in  Appendix D.2 for enhanced clarity.\n\n    1.4 Annotation definitions and terms including model prediction are now more clearly defined in the revised methodology section (Sec. 3.1).\n\n    1.5 The codebase outline and a detailed description have been provided to ensure reproducibility.\n\n2. **Evidence supporting the inadequacy of passive defenses in dynamic environments and baseline concerns**.\n\n    2.1 **Passive defenses are inadequate in dynamic environments**.\n\n    We want to humbly point out that we have incorporated various passive defenses in our submission, including Preprocessed methods (e.g., JPEG Compression, Local Gradients Smoothing (LGS), Segment and Complete (SAC) and PatchZero (PZ)) alongside an adversarial training based method for patch attacks, namely Defense against Occlusion Attacks (DOA), which is similar to Madry's PGD adversarial training. These methods represent common and state-of-the-art passive defenses against patch attacks. The comparative results are detailed in Table 1 and Figure 2 of our original submission.    \n\n    2.2 **Extra training resources**.\n\n    Regarding training resources, our method demonstrates an obviously reduced training overhead compared to traditional adversarial training approaches, such as DOA. Adversarial training typically incurs substantially higher computational costs than standard training methods [1], particularly when designed to counter a broad range of attacks. Despite our model necessitating an additional policy model and a temporal fusion model, its training resource requirements remain considerably lower compared to those of adversarial training-based methods (e.g., DOA). For the training of EAD only requires patches from Uniform Superset Approximation thereby bypassing the expensive computational cost of inner maximization of adversarial training. Furthermore, our approach is orthogonal to adversarial training, making it feasible to integrate the latter to enhance EAD's resilience against specific attacks. We also provide detailed discussions on computational resources below in \"Computational overhead for EAD defense\".\n\n    2.3 **Active defense baselines.** \n\n    To the best of our knowledge, our work represents the first attempt to promote the adversarial robustness of perception in an active paradigm. \n\n3. **Sufficiency and scalability of experimental setting.**\n\n    As for the data sufficiency in our experimental settings,  we would like to clarify that our training dataset encompasses over 3M unique images, rendered from various 3D facial models and viewpoints. Each identity has approximately 20 distinct 3D facial models, with nearly 60 images rendered from each model. This quantity of face data is exactly the same as previous adversarial defense work like SAC [2]. Besides, this amount has reached the average volume of current top-tier FR dataset (e.g., 1.2M for Digi-Face 1M [3], 3.3M for VGG Face2 [4], 0.5M for CASIA-WebFace [5]). As for testing, we adhere to the standard testing protocol in [6]. The testing dataset is equally abundant and diverse with ones in the training phase, which involves various 3D facial models and viewpoints. Furthermore, we are also generating additional data to further validate the scalability and generalizability of our findings, with plans to update our findings accordingly."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1185/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700207963447,
                "cdate": 1700207963447,
                "tmdate": 1700207963447,
                "mdate": 1700207963447,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZPHbIR0c2f",
                "forum": "uXjfOmTiDt",
                "replyto": "xoefIk8UTS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1185/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1185/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Look forward to further feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer V4oi,\n\nWe thank you again for the valuable and insightful comments. We hope you might find the response satisfactory and are looking forward to hearing from you about any further feedback.\n\nBest, Authors"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1185/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700448477330,
                "cdate": 1700448477330,
                "tmdate": 1700448477330,
                "mdate": 1700448477330,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LAAqZKaUSn",
                "forum": "uXjfOmTiDt",
                "replyto": "xoefIk8UTS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1185/Reviewer_V4oi"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1185/Reviewer_V4oi"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the response"
                    },
                    "comment": {
                        "value": "I really appreciate the author's explanations to my concerns. These discussions mostly answers my questions and the revised manuscript seems to have better readability. Accordingly I have increased my overall rating to 5 (and increased the Soundness and Presentation score)."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1185/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700638386580,
                "cdate": 1700638386580,
                "tmdate": 1700638455182,
                "mdate": 1700638455182,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "uyTGiJc76w",
            "forum": "uXjfOmTiDt",
            "replyto": "uXjfOmTiDt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1185/Reviewer_iMPU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1185/Reviewer_iMPU"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a defense against patch-based adversarial attacks on visual recognition models. It utilizes the concept of emodied perception (wherein the model is allowed to interact with its environment to refine its belief) to mitigate the effect of 'out-of-place' patches in the scene and, in turn, improve the model's robustness against patch-based attacks. The authors implement embodied perception as a partially observable markov decision process. The overall system comprises of two new models: a perception model and a policy model. The perception model maintains an 'understanding' of the scene which it progressively updates based on new observations. The policy model dictates a transition process that is focused on improving the quality of observations to improve recognition. Given an initial observation, the system progressively refines its belief using the perception and policy models. Through two visual recognition tasks: face recognition and object detection, authors demonstrate the effectiveness of emodied perception based defense to not only improve robustness against seen and unseen attacks, but also improve standard performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. Overall, the writing quality and presentation of the paper is excellent. The authors lay out the motivation behind their method and challenges behind implementing it adequately. All necessary background is provided so that a person unfamiliar with the topic can follow along. The figures and tables present information effectively.\n2. The authors propose a novel application of emodied perception, using it as a defense against patch-based adversarial attacks, making it the first defense of its kind.\n3. The design of the method is clever and intricate, and is intuitively grounded in the understanding of human perception.\n4. The defense is task agnostic, and should conceptually work with any visual recognition task.\n5. The authors provide a theoretical understanding for the effectiveness of EAD using information theory.\n6. The ablation presented in the paper is thorough, and covers all the different components of the method as well as newly introduced hyperparameters.\n7. The method improves both standard and adversarial performance relative to prior defenses. Interestingly, standard performance is improved even over undefended baseline. Overall, improvements introduced by proposed method appear substantial."
                },
                "weaknesses": {
                    "value": "1. There is no discussion regarding the computational cost of the proposed method and how it compares to the undefended baseline as well as prior defenses.\n2. An end-to-end attack is not necessarily the strongest adaptive attack, especially for defenses with complicated forward passes. Attacking the weakest component should be sufficient. To identify the weakest component, authors should try independently attacking perception and policy models to make them less effective in their respective tasks. For example, attacking perception model would involve generating an input that forces the perception model to output a corrupted internal belief vector b_t (using a latent space attack [a]).\n3. There are no theoretical results establishing how the approximations used in Sec 3.3 relate to the actual quantities. If possible to obtain, this would be nice to have.\n\n**References**\n\n[a] Sabour, S., Cao, Y., Faghri, F., and Fleet, D. J. Adversarial manipulation of deep representations. International Conference on Learning Representations, 2016."
                },
                "questions": {
                    "value": "1. How does the computational cost of the proposed method compares to the baselines?\n2. How does the method fare against an adaptive attack targeting the weakest component in the pipeline (see further details in the Weaknesses section)? I'd love to increase my rating further if you can better convince me that the defense is robust against adaptive attacks.\n3. Why is the enhanced version of SAC less robust against dodging attacks than regular SAC (in table 1)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1185/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699220406357,
            "cdate": 1699220406357,
            "tmdate": 1699636044777,
            "mdate": 1699636044777,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Y5LOvQoRT7",
                "forum": "uXjfOmTiDt",
                "replyto": "uyTGiJc76w",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1185/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1185/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the valuable review (1/2)"
                    },
                    "comment": {
                        "value": "We sincerely appreciate your detailed and constructive feedback. Your insights have greatly enriched our understanding and presentation of the research, enabling us to refine and strengthen our work significantly. We are particularly grateful for your recognition of the novelty and clarity of our approach, as well as the thoroughness of our experiments.\n\nIn response to your valuable comments, we have meticulously revised our paper to address each point you raised. We hope that our revisions and responses meet your expectations.\nBelow, we provide detailed responses to each of the points you have raised:\n\n1. **Discussion over computational overhead** \n\n    Thanks for pointing out the computational cost of our method compared to the baselines. In response, we have conducted a detailed comparison of computational overhead, encompassing aspects such as parameter amount and computation speed during both training and inference phases. The outcomes of this analysis are presented in Table a.\n\n    \n\n    *Table a: Computational overhead comparison of different defense methods in face recognition. We report the training and inference time of defense on a NVIDIA GeForce RTX 3090 Ti and an AMD EPYC 7302 16-Core Processor with the training batch size as 64.*     \n\n    | **Method** | **\\# Params (M)** | **Description**                                              | **Training Epochs** | **Training Time per batch (s)** | Overall Training time (GPU hours) | **Inference Time per step (ms)** |\n    | ---------- | ----------------- | ------------------------------------------------------------ | ------------------- | ------------------------------- | --------------------------------- | -------------------------------- |\n    | JPEG       | -                 | non-parametric                                               | -                   | -                               | -                                 | 9.65                             |\n    | LGS        | -                 | non-parametric                                               | -                   | -                               | -                                 | 26.22                            |\n    | SAC        | 44.71             | Train a segmenter to detect a patch area, which consists of two phases: training with pre-generated adversarial images and self-adversarial training | 50 + 10             | 0.152 + 4.018                   | 104                               | 26.43                            |\n    | PZ         | 44.71             | Share the same training procedure and segmenter as SAC       | 50 + 10             | 0.152 + 4.018                   | 104                               | 11.88                            |\n    | DOA        | 43.63             | An adversarial training method that retrains the feature extractor | 100                 | 1.732                           | 376                               | 8.10                             |\n    | EAD (ours) | 57.30             | Train the policy model and perception model in two stages    | 50 + 50             | 0.595 + 1.021                   | 175                               | 11.51                            |\n\n    \n\n    As indicated in Table a, in the training phase, the total training time of our EAD model is effectively balanced between the pure adversarial training method DOA and the partially adversarial methods like SAC and PZ. This efficiency stems mainly from our unique USAP approach, which bypasses the need for generating adversarial examples, thereby boosting training efficiency.\n    In terms of model inference, our EAD, along with PZ and DOA, demonstrates superior speed compared to LGS and SAC. This is attributed to the latter methods requiring CPU-intensive, rule-based image preprocessing, which diminishes their inference efficiency. Further details, including loss curves and more precise time consumption metrics, are elaborated in Appendix D.6."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1185/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700206921820,
                "cdate": 1700206921820,
                "tmdate": 1700206921820,
                "mdate": 1700206921820,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7eHDvbl1pC",
                "forum": "uXjfOmTiDt",
                "replyto": "uyTGiJc76w",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1185/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1185/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the valuable review (2/2)"
                    },
                    "comment": {
                        "value": "2. **More adaptive attacks.** \n\n    Thanks for pointing it out. We have now involved three new adaptive attacks to assess the robustness of the EAD model. These attacks target different components: the perception model, the policy model, and the entire pipeline.\n\n    -  **Perception:** Attacking the perception model to disrupt the model belief (perception) according to your advice and the work by Sabour et al. [1] \n    -  **Policy:** Attacking the policy model to make it ensure zero action (i.e., EAD model stay stills) and output erroneous predictions.\n    -  **Overall:** In our previous version,  attacking EAD by backpropagation is prohibitive due to rapid GPU memory consumption as trajectory length increases (4 steps require over 90 GB video memory). With further investigation, we find a new technique to address that issue, namely gradient checkpointing [2]. By checkpointing nodes in the computation graph defined by the $\\tau$-step EAD inference procedure, this technique recomputes the parts of the graph instead of directly storing them, thereby reducing memory cost with extra computation. With this technique, we succeed in attacking the whole pipeline along 4-step trajectory on a NVIDIA RTX 3090 Ti.\n\n    We summarize the results in Table b.\n\n    \n\n    *Table b: Evaluation of adaptive attacks on the EAD Model. Columns with **USP** represent results obtained by optimizing the patch with an expected gradient over the Uniform Superset Policy (USP). And **perception** and **policy** separately represent adaptive attacks against a single sub-module. And **overall** denotes attacking EAD by following the gradients for along the overall (4 steps) trajectory with gradient checkpointing.* \n\n    |              | **Dodging** |                |            |             | **Impersonation** |                |            |             |\n    | :----------: | ----------- | :------------: | :--------: | :---------: | :---------------: | :------------: | :--------: | :---------: |\n    |              | **USP**     | **Perception** | **Policy** | **Overall** |      **USP**      | **Perception** | **Policy** | **Overall** |\n    | **ASR (\\%)** | **22.11**   |     10.11      |   16.84    |    15.79    |       8.33        |      1.04      |  **9.38**  |    7.29     |\n\n    \n\n    The results further validate the robustness of EAD against a spectrum of adaptive attacks. It further shows that EAD\u2019s defensive capabilities arise from the synergistic integration of its policy and perception models, facilitating strategic observation collection rather than learning a short-cut strategy to neutralize adversarial patches from specific viewpoints.\n    The methodologies and detailed results of these adaptive attacks are now thoroughly documented in Appendix D.3 and Appendix D.7 respectively.\n\n3. **Theoretical results relating to approximations in Sec 3.3.**\n\n    Thanks for your suggestion. We agree that while our method is fundamentally grounded in information theory, there are several approximations in our implementation that merit deeper theoretical exploration. We would like to further achieve this as a potential area for future work, aiming to provide a more comprehensive theoretical analysis that aligns closely with our practical implementations. \n\n4. **Enhanced SAC is less robust than the regular one against dodging attacks.**\n\n    SAC is a preprocessing-based method that adopts a segmentation model to detect patch areas, followed by a \"shape completion\" technique to extend the predicted area into a larger square, and remove the suspicious area. We observed that the enhanced SAC, while exhibiting superior segmentation performance, inadvertently increases the likelihood of masking critical facial features such as eyes and noses in scenarios like face recognition. As a result, the Face Recognition (FR) model is more likely to misidentify the masked faces as different identities, thereby reducing the efficacy of dodging attacks. Besides, we also provide an illustrative example (Figure 10) and more details in the Appendix D.8 to elucidate this phenomenon. \n\n**References:**\n\n[1] Sabour, S., Cao, Y., Faghri, F., \\& Fleet, D. J. (2015). Adversarial manipulation of deep representations. *arXiv preprint arXiv:1511.05122*.\n\n[2] Chen, T., Xu, B., Zhang, C., \\& Guestrin, C. (2016). Training deep nets with sublinear memory cost. *arXiv preprint arXiv:1604.06174*."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1185/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700207097365,
                "cdate": 1700207097365,
                "tmdate": 1700207189409,
                "mdate": 1700207189409,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]