[
    {
        "title": "Best Possible Q-Learning"
    },
    {
        "review": {
            "id": "SmjxbrMZGL",
            "forum": "RNgZTA4CTP",
            "replyto": "RNgZTA4CTP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4794/Reviewer_XrJf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4794/Reviewer_XrJf"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a strategy called best possible Q-learning for agents learning the optimal joint policy in fully decentralized learning. Under this strategy, when learning the Q function, each agent assumes that after choosing the action, the transition of the $N$-agent environment will be the ``best case\" so that the expected return is maximized. The authors prove the convergence of such an elegant strategy to global optimal joint policy (under some assumptions). The authors also provide a simplified version of this strategy that is more computationally attractive."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The writing is clear, which makes it enjoyable to read. Even restricted by the assumption of deterministic policies and the uniqueness of optimal joint policy, it is still impressive that with such an elegant best possible Q-learning, the global optimal policy can be computed."
                },
                "weaknesses": {
                    "value": "Lemma 2 is a crucial lemma for justifying the best possible operator. However, the second equality appears to be incorrect. Could the authors explain how it holds?"
                },
                "questions": {
                    "value": "- Is it a common setting that the reward function is only dependent on the current state and the next state (without the dependency on the action)? To me this is not commonly seen.\n\n- I think (3) (where $Q(s,\\boldsymbol{a})$ is equipped with some arbitrary $\\boldsymbol{a}$) is not the expected return of the optimal joint policy.\n\n- In the proof of lemma 1, I notice that there is an underlying assumption that the actions taken by other agents do not restrict the allowable actions of each agent. If this restriction is considered, does the best possible learning strategy still apply?\n\n- The 3rd line under problem (6), could the authors provide exactly where is the reference in (Puterman, 1994)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4794/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4794/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4794/Reviewer_XrJf"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4794/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698163116632,
            "cdate": 1698163116632,
            "tmdate": 1700639346661,
            "mdate": 1700639346661,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "67uR4TMXsJ",
                "forum": "RNgZTA4CTP",
                "replyto": "SmjxbrMZGL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4794/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4794/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Lemma 2 is a crucial lemma for justifying the best possible operator. However, the second equality appears to be incorrect. Could the authors explain how it holds?\n\nWe made a correction in the revision (the colored line). The $\\max _{a_i}$ should be factored out. After the correction, the proof still holds because $a^{'*}_i$ maximizes the second term but does not maximize the first term.\n\n> Is it a common setting that the reward function is only dependent on the current state and the next state (without the dependency on the action)? To me this is not commonly seen.\n\nThe setting is still general because if two joint actions lead to the same transition dynamics in the environment, the two actions should have the same effects, and we do not need to discriminate the actions. The effects of actions are reflected in the distribution of $s'$.\n\nImagine that we are playing a computer game. There is a certain state where the left key and the right key can lead to the same next state or the same distribution of the next state. Do we need to give the right key a higher reward to encourage the players to always select the right key?\n\n> I think (3) (where $Q(s, \\boldsymbol{a})$ is equipped with some arbitrary $\\boldsymbol{a}$ ) is not the expected return of the optimal joint policy.\n\n $Q(s, \\boldsymbol{a})$ means the expected return if you start in state $s$, take an arbitrary joint action $\\boldsymbol{a}$, and then forever after act according to the optimal policy in the environment. (https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#value-functions)\n\n> In the proof of lemma 1, I notice that there is an underlying assumption that the actions taken by other agents do not restrict the allowable actions of each agent. If this restriction is considered, does the best possible learning strategy still apply?\n\nYes, fully decentralized learning requires the assumption that the agents can independently make decisions. If we consider the restriction of the allowable actions of each agent, we can train the agents in a modified MDP. If the selected joint action breaks the restriction, the MDP ends and the agents receive a very large negative reward. Then the optimal joint policy in the modified MDP will be equal to the optimal joint policy in the original MDP, and agents can independently make decisions in the modified MDP.\n\n> The 3rd line under problem (6), could the authors provide exactly where is the reference in (Puterman, 1994)?\n\nTheorem 7.1.9. of (Puterman, 1994) shows that there is at least one deterministic optimal joint policy. Therefore, when there is only one optimal joint policy, the optimal joint policy must be deterministic."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4794/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699885402448,
                "cdate": 1699885402448,
                "tmdate": 1699885402448,
                "mdate": 1699885402448,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QjlSkjCajk",
                "forum": "RNgZTA4CTP",
                "replyto": "67uR4TMXsJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4794/Reviewer_XrJf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4794/Reviewer_XrJf"
                ],
                "content": {
                    "title": {
                        "value": "Thank you!"
                    },
                    "comment": {
                        "value": "Thank you for addressing my comments in detail! I will raise my score."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4794/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700639330888,
                "cdate": 1700639330888,
                "tmdate": 1700639330888,
                "mdate": 1700639330888,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "iNfGomvCgf",
            "forum": "RNgZTA4CTP",
            "replyto": "RNgZTA4CTP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4794/Reviewer_jF9W"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4794/Reviewer_jF9W"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies decentralized multi-agent reinforcement learning (RL). It proposes a new operator called best possible operator in updating the Q-values, along with a computationally efficient variation called simplified best possible operator.\n\nAnalytically, both operators are shown to converge to the optimal joint policy.\n\nEmpirically, Q-learning based on such operator is shown to achieve good performance across a number of tasks."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Good empirical performance on a variety of tasks\n\n- A systematic method for updating Q-values in a decentralized multi-agent setting"
                },
                "weaknesses": {
                    "value": "[Disclaimer:\nWhile reviewing this paper, I focused on the reinforcement learning aspects and the correctness of the theoretical analysis. It's worth noting that my exposure to multi-agent settings is limited. Therefore, I would lean toward the inputs from other reviewers to assess the novelty as to how this work fits within the existing body of multi-agent literature.]\n\n\n1. The proofs have inaccuracies which impede a clear understanding of the underlying logic.\n\na.Lemma 2:\n\nthis line appears to be incorrect: \n\n$=\\gamma  P_i^*(s^\u2032|s, a_i)  \\max_{a_i^\u2032} (\\max_{a_{-i}^\u2032} Q(s^\u2032, a_i^\u2032 , a^\u2032_{-i} ) \u2212 Q^{k\u22121}(s^\u2032, a_i^\u2032 ))$\n\n: the $\\max_{a_i}$ cannot be factored out here since the maximum actions can be different for the two Q value terms inside the parenthesis and there is a minor sign. Apart from this, the remainder of the lemma appears to be correct.\n\nb.Lemma 3:\n\nIn the proof of Lemma 3, the second inequality appears to be valid. However, it seems to omit intermediate steps. The absence of these steps makes it challenging to follow the reasoning.\nElaborating on these steps would enhance the clarity of the proof.\n\nc.Lemma 4: \n\n\u201cSimilar to the proof of Lemma 2, we can easily prove $\\max_{a_{\u2212i}} Q(s, a_i, a_{\u2212i}) \\leq Q^k_i (s, a_i)$\u201d\n\nWhile the result seems valid, it may not be as straightforward as implied, especially considering the nuances in this context. In particular, the simplified operator incorporates an additional max operator (8), when compared to the operator (6) used in Lemma 2. A more detailed elaboration of the steps involved in this proof would be beneficial.\n\n2. As I will detail in the following, the conditions in the convergence proof for both operators seem too strong to hold in practice. There seems to be lack of sufficient attention to how these conditions might be met, thus leading to doubts around the practical relevance of the operators.\n\nBoth operators implicitly assume that the expectation w.r.t. the transition probabilities in (6, 7) can be computed exactly, at every step of the value update (i.e., $\\forall k$).\n\nAlthough the simplified operator is \u201csimpler\u201d due to not requiring the computation of expectations for every $\\tilde{P_i}$, it still poses practical challenges. The need to compute the expectation for even one $\\tilde{P_i}$ at every update step seems impractical.\n\nIn fact, an approximation error in computing the expectations, when combined with the max operation, could cause the over estimation of Q values. This issue, briefly mentioned in the text following Eq. (10), lacks a clear explanation in my opinion. \n\nThe approximation error would also lead to the violation of the Q value upper bound, undermining the convergence guarantees for both operators.\n\n**Suggestions for improvements:**\n\nPlease correct me if I\u2019m wrong in the above. If my understanding is correct, the authors might consider the following two ways to mitigate the concerns:\n\n1. revise the convergence analysis to account for the approximation error in computing the expectation\n2. modify the BQL algorithm such that the Q value estimates are both increasing and bounded above.\n\nMinor:\n- Many plots use \u201creward\u201d as the y-axis label. It should be \u201creturn\u201d instead."
                },
                "questions": {
                    "value": "Can the authors please comment on my concerns listed in the weaknesses?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4794/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4794/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4794/Reviewer_jF9W"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4794/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698821929369,
            "cdate": 1698821929369,
            "tmdate": 1699636461787,
            "mdate": 1699636461787,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DVXisRi2wr",
                "forum": "RNgZTA4CTP",
                "replyto": "iNfGomvCgf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4794/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4794/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">  1.a.Lemma 2\n\nSorry for this mistake. The $\\max _{a_i}$ should be factored out. We have corrected it in the revision (the colored line). After the correction, the proof still holds because $a^{'*}_i$ maximizes the second term but does not maximize the first term.\n\n>  1.b.Lemma 3 and 1.c.Lemma 4:\n\nFor the second inequality of Lemma 3, we replace the expectation over transition probabilities with the infinite norm (which is larger). We will elaborate on the proofs of these two lemma in the final version with additional pages.\n\n> 2. As I will detail in the following, the conditions in the convergence proof for both operators seem too strong to hold in practice. There seems to be lack of sufficient attention to how these conditions might be met, thus leading to doubts around the practical relevance of the operators.\n\nThere are four questions in this part. For the first two questions, in theoretical analysis, it is reasonable to assume that the expected values over transition probabilities are computed exactly at every step. Even Q-learning makes such assumptions. In the implementation, as DQN, we use TD-error by sampling from a non-stationary replay buffer to fit the expected values under $\\tilde{P}_i$. This approximation technique is commonly adopted, so we believe your main concern is the next two questions about approximation error.\n\nFor the last two questions, the main concern is the overestimation caused by approximation error. We use Eq. 10, the weighted max update, to overcome this problem. We can choose a large $\\lambda$ to mitigate overestimation. This technique is proposed in Hysteretic IQL to solve the overestimation in Distributed IQL (as discussed in Appendix B), and has been verified to be practical in decentralized learning. This solution similar to quantile regression has been used in many RL methods, e.g., implicit Q-learning. Therefore, we follow this practical solution, and experimental results show that this solution can avoid severe overestimation."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4794/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699885350175,
                "cdate": 1699885350175,
                "tmdate": 1699885350175,
                "mdate": 1699885350175,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5u2fKcv3mY",
                "forum": "RNgZTA4CTP",
                "replyto": "iNfGomvCgf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4794/Reviewer_jF9W"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4794/Reviewer_jF9W"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you to the authors for their response. My apologies for the delayed reply; I hope there is still adequate time for further discussion.\n\nI find the clarification regarding the proofs for Lemma 2-4 satisfactory, confirming my initial understanding. The issues identified appear to be presentation-related, which I believe can be rectified in subsequent revisions.\n\nHowever, upon further review, I remain concerned about the theoretical framework.\n\nSpecifically, the assumption that the algorithm can exactly compute expectations with respect to transition probabilities is, in my opinion, overly restrictive and a significant limitation of the proposed method. \n\nFor instance, both the best possible operator (Eq. 6) and the simplified operator (Eq. 7) require exact computation of expectations at each Q-function update. And their convergence proofs rely on such computations.\n\nThe authors claimed that \u201cit is reasonable to assume that the expected values over transition probabilities are computed exactly at every step\u201d, and drew parallels with Q-learning, stating \u201cQ-learning makes such assumptions\u201d. I must respectfully disagree. \nThe update rule of Q-learning does *not* entail such an expectation calculation. Moreover, the majority of convergence proofs in the Q-learning literature [1-4] utilize results from stochastic approximation without presuming the agent\u2019s access to true transition probabilities.\n\nThis assumption raises a fundamental question: If the algorithm indeed has access to true transition probabilities, as implied in Equations 6 and 7, the necessity of a Q-learning-like method is questionable. In such a scenario, direct application of dynamic programming would seem more straightforward.\n\nIn light of these concerns on the theoretical results, my evaluation remains unchanged. However, I am open to revising my score should there be further clarification or evidence that contradicts my current understanding.\n\n**References:**\n\n[1] Watkins, C. J., & Dayan, P. (1992). Q-learning.\u00a0*Machine learning*,\u00a0*8*, 279-292.\n\n[2] Jaakkola, T., Jordan, M. I., Singh, S. P. (1994). On the convergence of stochastic iterative dynamic programming algorithms. Neural Computation, 6:1185\u20131201.\n\n[3] Tsitsiklis, J. N. (1994). Asynchronous stochastic approximation and Q-learning. Machine Learning, 16(3):185\u2013202.\n\n[4] Borkar, V. S., & Meyn, S. P. (2000). The ODE method for convergence of stochastic approximation and reinforcement learning.\u00a0*SIAM Journal on Control and Optimization*,\u00a0*38*(2), 447-469."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4794/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700640752556,
                "cdate": 1700640752556,
                "tmdate": 1700641009100,
                "mdate": 1700641009100,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qz0qLuqLEL",
                "forum": "RNgZTA4CTP",
                "replyto": "iNfGomvCgf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4794/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4794/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Moreover, as the discussion is ending, we finally hope you can consider the problem we solved, fully decentralized MARL, is much more challenging than CTDE setting, which means that the complexity of our operator should be high, thus we have to use some practical techniques to achieve the trade-off between efficiency and theory (the approximation error you proposed). We propose the first operator which can theoretically converge to optimal joint policy, and make it more efficient than the practical baselines which **ignore** the convergence and optimality. From the perspectives of theory and algorithm, we believe that BQL is really a contribution to the novel field: fully decentralized MARL. We hope you can consider this point when evaluating our work. Thank you very much!\n\n\nBests,\n\nAuthors"
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4794/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700668100994,
                "cdate": 1700668100994,
                "tmdate": 1700668370558,
                "mdate": 1700668370558,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iBDUyeb9i5",
                "forum": "RNgZTA4CTP",
                "replyto": "qz0qLuqLEL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4794/Reviewer_jF9W"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4794/Reviewer_jF9W"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their further clarifications.\n\nTo ensure clarity in my review, I'd like to recap my perspective on the paper:\n\n1. My initial review acknowledged the empirical performance as a strength, and my perspective on this aspect remains unchanged.\n2. However, I do have major concerns regarding the theoretical results and the presentation of them. Given that the theoretical analysis accounts for a signification portion of the paper and the convergence has been claimed as a main contribution, these concerns substantially affect my evaluation.\n\nIn my opinion, the presentation of the theoretical results tends to obscure the actual contribution.\n\nThe proposed operators in Sec. 2.2 and 2.3 essentially perform dynamic programming. The simplified operator in Eq. (7) can be interpreted as a form of (action-)value iteration, known to converge at a geometric rate in single-agent RL [5], extended to the multi-agent setting. **Yet no reference to dynamic programming was made in the paper.**\n\nRegarding the algorithm BQL described in Sec 2.4, it uses sampled interaction data to approximate the expectation, which seems to have close connections to the fitted Q iteration literature. **Yet again, no reference was given to that regard.** \n\nFurthermore, at the end of Sec. 2.4, it is claimed that \u201cIf $D_i$ sufficiently goes through all possible transition probabilities, $Q_i(s, a_i)$ converges to \u2026\u201c. **This is imprecise**. From the fitted Q iteration literature, it is well known that such **convergence does not hold in general** (convergence has only been established under specific regressing methods) and counter-examples have been shown. Chapter 3.3 in [5] has a nice summary of the relevant literature.\n\nAs to the main contribution of the work, it appears that the convergence only applies to the proposed operators, not the algorithm BQL itself. **That renders the following claim incorrect**:\n\u201cBQL is the first decentralized algorithm that guarantees the convergence to the global optimum in stochastic environments\u201d, from the last paragraph of the introduction section.\nSimilarly, the claim at the end of related works section **is also incorrect**: \u201cOur BQL is the first fully decentralized algorithm that converges to the optimal joint policy \u2026\u201d. \n\nThe paper currently lacks a convergence proof for the BQL algorithm itself, which seems feasible to provide for certain regressors, based on the similarity to fitted Q iteration. \n\nIn the current form of the paper, substantial revisions to the presentation and the claimed contributions seem necessary.\n\nUntil these issues are addressed, I have to respectfully maintain my initial score.\n\n**References**\n\n[5] Szepesv\u00e1ri, C. (2022).\u00a0*Algorithms for reinforcement learning*. Springer Nature."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4794/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700720905615,
                "cdate": 1700720905615,
                "tmdate": 1700720905615,
                "mdate": 1700720905615,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hzh2mfyOpr",
                "forum": "RNgZTA4CTP",
                "replyto": "iNfGomvCgf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4794/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4794/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your reply. We would like to defend our method. It seems you have two concerns in the reply. Before \"Furthermore, at the end of Sec. 2.4\", the concern is \"no reference\". However, the equation and proof are basic common knowledge in this field. \"Using sampled interaction data to approximate the expectation\" is the most common technique in this field. Both you and other reviewers can easily understand the the method and theorems. So we believe the presentation is clear for the researchers in this field.\n\nAfter \"Furthermore, at the end of Sec. 2.4\", there are two questions. For the first one, when we mention convergence, we mean the ideal case where we can obtain the true transition probability from $D_i$. We know there is a estimation error but we have claimed the trade-off between efficiency and theory. For the second one, note the BQL has two versions. The Q-table version (Algorithm 1) strictly follows the operators, so we can make such a claim. The results in Figure 1(a) can demonstrate it. And for NN version (Algorithm 2), we know that even single-agent DQN with NN cannot guarantee the convergence.\n\nMoreover, we want to ask a question. We propose the first operator with convergence and optimality and extend it into algorithm, but other baselines cannot theoretically guarantee the convergence and optimality even in the ideal case. Do you think our work is the best among these methods in terms of theory and practicality? If the answer is no, could you tell us which baseline is better than BQL and why? If the answer is yes, we want to know why these baselines meet the standard of top-tier conference."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4794/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700723433906,
                "cdate": 1700723433906,
                "tmdate": 1700726221963,
                "mdate": 1700726221963,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PoOBnBhbz5",
                "forum": "RNgZTA4CTP",
                "replyto": "iNfGomvCgf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4794/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4794/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "As the discussion is ending, we have to make a supplement. As you said, the proofs of operators are correct and the empirical performance is good. The concern is the estimation error in algorithm. Naturally, the influence of estimation error in algorithm should be verified using experiments, which is the main purpose of experiments. Our algorithm can achieve good performance, which demonstrates that our algorithm can overcome the estimation error. Our paper is not just a theory paper. We focus on practical algorithm which is derived from theory."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4794/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700726161838,
                "cdate": 1700726161838,
                "tmdate": 1700726306118,
                "mdate": 1700726306118,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lKrDxRB5iY",
                "forum": "RNgZTA4CTP",
                "replyto": "PoOBnBhbz5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4794/Reviewer_jF9W"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4794/Reviewer_jF9W"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your prompt response. I appreciate the further discussions.\n\nRegarding the comment that \u201cthe equation and proof are basic common knowledge in this field\u201d I understand that this may refer to the equations (rather than proofs) being well-known. However, I would like to respectfully point out that common knowledge should not preclude precise and accurate descriptions in a paper.\n\nLet\u2019s take Eq. (2) in your paper as an example: it is the Bellman optimality equation for action values and not an update rule. However, the preceding text reads \u201cthe convergence of independent Q-learning\u201d, leading to confusions. It is unclear as to whether Eq. (2) is being described as part of independent Q-learning, or as the optimal Q value that should be converged to.\n\nIn comparison, similar equations in single-agent settings, as seen in the DQN papers [6,7], are explicitly referred to as Bellman equations. And the algorithms that make use of them were  referred to as value iteration, rather than Q-learning. Q-learning update rule does not involve expectations, but instead only use samples, which was also clearly delineated in the DQN papers.\nWhile such distinctions might be considered common knowledge, their precise description remains crucial for clarity, especially in establishing the relationship and differences between proposed methods and existing ones.\n\nConcerning the comment that \u201cBoth you and other reviewers can easily understand the the method and theorems\u201d: this is somewhat subjective.\n\nRegarding your clarifications on the statement in Sec. 2.4. \n\n- \u201cwhen we mention convergence, we mean the ideal case where we can obtain the true transition probability from $D_i$\u201d\n\n- \u201cThe Q-table version (Algorithm 1) strictly follows the operators, so we can make such a claim.\u201d\n\nTo be able to follow these operators given access to $D_i$, one would need to have infinite amount of data in each epoch $D_i^j$, for all $j$. I find it to be overly restrictive. That\u2019s partially why I suggested the connection to the fitted Q iteration, which more naturally aligns with algorithms that have access to data instead of the model itself.\n\n> \u201cWe propose the first operator with convergence and optimality and extend it into algorithm, but other baselines cannot theoretically guarantee the convergence and optimality even in the ideal case. Do you think our work is the best among these methods in terms of theory and practicality?\u201d\n\nMy exposure to multi-agent settings (and so the baselines) is limited as I acknowledged initially. \nBut I can offer my perspective based on the content of your paper.\n\nIndeed, proposing \u201cthe first operator with convergence and optimality\u201d and developing a well-performing practical algorithm is commendable. However, as I have highlighted, that was not exactly how it was claimed in the paper. The presentation tends to be misleading, incorrect at times, and lacks sufficient discussions on assumptions and limitations. \n\nI believe it is premature to draw comparisons with existing works, before addressing the aforementioned concerns. \n\n**References:**\n\n[6] Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., & Riedmiller, M. (2013). Playing atari with deep reinforcement learning.\u00a0*arXiv preprint arXiv:1312.5602*. \n\n[7] Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., ... & Hassabis, D. (2015). Human-level control through deep reinforcement learning.\u00a0*nature*,\u00a0*518*(7540), 529-533."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4794/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700732944087,
                "cdate": 1700732944087,
                "tmdate": 1700732944087,
                "mdate": 1700732944087,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tEmYplMDLI",
                "forum": "RNgZTA4CTP",
                "replyto": "iNfGomvCgf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4794/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4794/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Let\u2019s take Eq. (2) in your paper as an example: it is the Bellman optimality equation for action values and not an update rule. However, the preceding text reads \u201cthe convergence of independent Q-learning\u201d, leading to confusions. It is unclear as to whether Eq. (2) is being described as part of independent Q-learning, or as the optimal Q value that should be converged to.\n\nThere is a whole sentence before and after Eq. (2): \" the convergence of independent Q-learning is not guaranteed\". Eq. (2) is exactly the value iteration of independent Q-learning, and it is not guaranteed to be converged, which we have analyzed multiple times in this paper. We do not think that this sentence is misleading.\n\n> And the algorithms that make use of them were referred to as value iteration, rather than Q-learning\n\nThe equations in our paper are exactly value iteration. We do not call them Q-learning. \"Best Possible Q-learning\" is just the name of our paper, to establish the relationship and difference with existing methods: independent Q-learning, Hysteretic independent Q-learning, and Distributed independent Q-learning.\n\n> Concerning the comment that \u201cBoth you and other reviewers can easily understand the method and theorems\u201d: this is somewhat subjective.\n\nYes, it is subjective. But the concern of presentation is a subjective topic, which is evaluated by whether the readers can easily understand the method and theorems.\n\n> one would need to have infinite amount of data in each epoch\n\nThis is the trade-off between efficiency and theory which we have discussed in the replies above. \n\n> Indeed, proposing \u201cthe first operator with convergence and optimality\u201d and developing a well-performing practical algorithm is commendable. However, as I have highlighted, that was not exactly how it was claimed in the paper.\n\nThis is exactly what we claimed in this paper. In abstract and introduction, we claimed that \"we propose best possible operator, a novel decentralized operator, and prove that the policies of agents will converge to the optimal joint policy\", \"By instantiating the simplified operator, the derived fully decentralized algorithm, best possible Q-learning (BQL), does not suffer from non-stationarity.\". And the structure of method section follows the same logic. Sec. 2.2 and 2.3 are operators, and Sec. 2.4 is algorithm. The structure is really clear.\n\n> I believe it is premature to draw comparisons with existing works, before addressing the aforementioned concerns.\n\nThe theorems have been supported by proofs, and the algorithm has been supported by experimental results. Therefore, it is important to compare with existing methods to defend our contribution."
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4794/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700735301509,
                "cdate": 1700735301509,
                "tmdate": 1700735451339,
                "mdate": 1700735451339,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AHlynx0u0G",
                "forum": "RNgZTA4CTP",
                "replyto": "tEmYplMDLI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4794/Reviewer_jF9W"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4794/Reviewer_jF9W"
                ],
                "content": {
                    "comment": {
                        "value": "> Eq. (2) is exactly the value iteration of independent Q-learning\n\nWell, that's certainly not how it was presented in the paper. And that has been my point.\n\n> The equations in our paper are exactly value iteration. We do not call them Q-learning.\n\nMy point was that the placement of Equation (2) immediately following the phrase \"independent Q-learning\" in the middle of a sentence, may lead to confusion. \n\n> the concern of presentation is a subjective topic, which is evaluated by whether the readers can easily understand the method and theorems.\n\nI have been pointing out specific instances where the presentation is lacking. \n\n> This is the trade-off between efficiency and theory which we have discussed in the replies above.\n\nIt is less so about trade-off, but rather an overly strong assumption. In fact, it is not common in RL theory that an algorithm starts with collecting an infinite amount of data from the very first epoch/iteration.\n\n> This is exactly what we claimed in this paper.\n\nIn my previous response, I highlighted certain claims in the paper that I found to be incorrect. I believe that acknowledging and addressing these issues is crucial for strengthening the overall impression and credibility of your work\n\n> The theorems have been supported by proofs\n\nThe issues have not been about the proofs."
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4794/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700737512415,
                "cdate": 1700737512415,
                "tmdate": 1700737512415,
                "mdate": 1700737512415,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2uFipJw9uO",
                "forum": "RNgZTA4CTP",
                "replyto": "BCaSuL4GF1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4794/Reviewer_jF9W"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4794/Reviewer_jF9W"
                ],
                "content": {
                    "comment": {
                        "value": "> We are sorry for bothering you repeatedly, but the left time is very limited.\n\nI understand the time-sensitive nature of this discussion. Sorry for not being able to engage in the discussion earlier."
                    }
                },
                "number": 26,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4794/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700737702360,
                "cdate": 1700737702360,
                "tmdate": 1700737702360,
                "mdate": 1700737702360,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5fNNlKH2ao",
            "forum": "RNgZTA4CTP",
            "replyto": "RNgZTA4CTP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4794/Reviewer_FC5t"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4794/Reviewer_FC5t"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes Best Possible Q-Learning. They introduce the best possible operator, which is like the standard Bellman operator, but because the actions of the other players are unknown, the operator also includes a maximization over all marginal transition probabilities under other players' policies. The computation of the best possible operator is heavy since it involves searching over all possible transition probabilities. Therefore, the work further proposes the simplified best possible operator and use it to design the algorithm. In the algorithm, every player, with some probability, will execute some random policy in order to explore possible transition probabilities."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The proposed algorithm looks promising in achieving decentralization in cooperative reinforcement learning.\n- The experimental part looks extensive and has covered many different environments. Though due to my unfamiliarity with previous algorithms, I cannot confidently say whether the comparison is fair or not."
                },
                "weaknesses": {
                    "value": "- For the explanation on the algorithm and theoretical justification, the writing is unclear in general. There are several arguments I cannot verify, and I suspect that there are flaws in the proof of the convergence. Please see Questions below.  This also largely hinders my understanding of the algorithm. \n- The figure for experiments are too small."
                },
                "questions": {
                    "value": "- In the Equation in Lemma 1 (the definition of $Q_i(s,a_i)$), should the $r$ be $r_i(s,a_i)$? If it should, should we also include $\\max_{r_i(s,a_i)}$ in the definition of $Q_i(s,a_i)$? It's a bit strange if the reward does not depend on the current state and the action chosen by the player. \n- Again, in the definition of $Q_i(s,a_i)$, what is the search space of $P_i(\\cdot|s,a_i)$? Do you search for \"all possible transitions probabilities\" from $(s,a_i)$?  If it is, then why not the solution simply be $Q_i(s,a_i)=r + \\max_{s', a_i'} Q_i(s', a_i')$? That is, the optimal solution of $P_i(\\cdot|s,a_i)$ should put all probability to the state $s'$ that has the highest value of $\\max_{a_i'}Q_i(s',a_i')$. \n- I have difficulty understanding the first inequality in the equation series in Lemma 2, i.e., the step that replaces $P_{\\text{env}}$ with $P_i^\\star$ with a $\\geq$ inequality. Can you explain that inequality? \n- I don't understand the following sentence in Page 5: \"As $P_i$ depends on $\\pi_{-i}$ and agents act deterministic policies, $D^m_i$ contains one $P_i$ under a deterministic $\\pi_{-i}$. \" I thought $D^m_i$ only contains tuples of the form $(s,a,r,s')$, why would it contain $P_i$? \n- In Page 5, it is said that \"When M is sufficiently large, given any $(s,a')$ pair, any $P_i(s,a_i)$ can be found in a replay buffer. \" I thought the set of $P_i(s,a_i)$ is infinitely large since these are continuous values, how is it possible that a finite set can contain all possible values of $P_i(s,a_i)$?  Again, it's unclear what you mean by $P_i(s,a_i)$ can be found in replay buffer, since replay buffer only contains $(s,a,r,s')$ tuples. \n- I don't understand the following sentence in page 5: \"Simplified best possible operator ... does not care about the relation between transition probabilities of different state-action pairs in the same buffer. \"\n- In Page 5, it is said that \"BQL ideally needs only $|A|$ buffers.. which is very efficient\" Suppose that every player has $K$ actions, is $|A|=K^N$? I'm not sure one can call this very efficient, given that this is exponentially large, and running a centralized Q-learning (so the number of joint action is K^N) should also just incur the same amount of computation."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4794/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4794/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4794/Reviewer_FC5t"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4794/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699132544165,
            "cdate": 1699132544165,
            "tmdate": 1700639433126,
            "mdate": 1700639433126,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "X86sB79cTf",
                "forum": "RNgZTA4CTP",
                "replyto": "5fNNlKH2ao",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4794/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4794/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> In the Equation in Lemma 1 (the definition of $Q_i\\left(s, a_i\\right)$ ), should the $r$ be $r_i\\left(s, a_i\\right)$ ?\n\nAs defined in section 2.1, the reward is $r(s,s')$, which does not depend on joint action. The setting is still general because if two joint actions lead to the same transition dynamics in the environment, the two actions should have the same effects, and we do not need to discriminate the actions. The effects of actions are reflected in the distribution of $s'$.\n\nImagine that we are playing a computer game. There is a certain state where the left key and the right key can lead to the same next state or the same distribution of the next state. Do we need to give the right key a higher reward to encourage the players to always select the right key?\n\n> Again, in the definition of $Q_i\\left(s, a_i\\right)$, what is the search space of $P_i\\left(\\cdot \\mid s, a_i\\right)$ ?\n\nYes, the search space is \"all possible transitions probabilities\", but the situation you proposed might be impossible to happen in the environment. Look at the definition of $P_i$ (Eq. 1), if $P_{env}$ is stochastic, no matter what $\\pi_{-i}$ is, we cannot put all probability to the state that has the highest value.\n\n> I have difficulty understanding the first inequality in the equation series in Lemma 2, i.e., the step that replaces $P_{env}$ with $P_i^{\\star}$ with a $\\geq$ inequality. Can you explain that inequality?\n\nLook at the first \"=\" line in lemma 2, there are two terms. $\\max_{a_{-i}}P_{env}$ achieves the highest first term, and $P_i^*$ achieves the highest second term. If we replace $\\max_{a_{-i}}P_{env }$ with $P_i^*$ in the first term, the first term decreases and we have a $\\ge$.\n\n>  I thought $D_i^m$ only contains tuples of the form $\\left(s, a, r, s^{\\prime}\\right)$, why would it contain $P_i$ ?\n\nYes, $D_i^m$ contains tuples of the form $\\left(s, a_i, r, s^{\\prime}\\right)$, and the transitions probabilities $P_i(s'|s,a_i)$ estimated from tuples of $\\left(s, a_i, r, s^{\\prime}\\right)$ is called the contained $P_i$.\n\n> I thought the set of $P_i$ is infinitely large since these are continuous values, how is it possible that a finite set can contain all possible values of $P_i$.\n\nLook at the definition of $P_i$ (Eq. 1), the number of $P_i$ depends on the number of $\\pi_{-i}$. In the lines after Eq. 6, we have claimed that we only consider the deterministic policies, because when there is only one optimal joint policy, the optimal joint policy must be deterministic. Deterministic $\\pi_{-i}$ is countable, and thus $P_i$ is countable.\n\n> I don't understand the following sentence in page 5: \"Simplified best possible operator ... does not care about the relation between transition probabilities of different state-action pairs in the same buffer.\"\n\nIn the simplified best possible operator, each transition independently contributes to the update of the operator. Even though the transitions are in the same buffer, they are not related during the update of the operator.\n\n>  In Page 5 , it is said that \"BQL ideally needs only $|A|$ buffers.. which is very efficient\" Suppose that every player has $K$ actions, is $|A|=K^N$ ? I'm not sure one can call this very efficient, given that this is exponentially large, and running a centralized Q-learning (so the number of joint action is $\\mathrm{K}^{\\wedge} \\mathrm{N}$ ) should also just incur the same amount of computation.\n\nYes, $|A|=K^N$. The reason why you think it is not efficient is that the problem itself is complex. Decentralized learning is more complex than centralized learning, and centralized Q-learning is an ideal solution for decentralized learning. If our method can achieve the same amount of computation with centralized Q-learning, we can say that our method is efficient. \n\nWe know that you expect a decentralized learning method which is much more efficient than centralized Q-learning and can guarantee the convergence to optimal policy. However, it seems to be impossible. If such a decentralized method really exists, for any single-agent Q-learning problem, we can factorize its action space, convert it to a decentralized MARL problem, and solve it using this method efficiently. It will be a huge revolution for Q-learning."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4794/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699885320992,
                "cdate": 1699885320992,
                "tmdate": 1699885320992,
                "mdate": 1699885320992,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jPhWxkQcSG",
                "forum": "RNgZTA4CTP",
                "replyto": "X86sB79cTf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4794/Reviewer_FC5t"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4794/Reviewer_FC5t"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response. I want to clarify one thing: are you assuming $P_{env}$ is known by every player (so we don't need to further search for all possible $P_{env}$)? \n\nIf not, then when finding the $P_i(\\cdot|s,a_i)$ that maximize Eq.(6), there requires an inner search over the space of $P_{env}$, i.e., \n\n$\\max_{P_i(\\cdot|s,a_i)} \\left[\\cdots\\right] = \\max_{a_{-i}}\\max_{P_{env}(\\cdot|s,a_i, a_{-i})} \\sum_{a_{-i}} P_{env}(s'|s,a_i,a_{-i})\\pi_{-i}(a_{-i}|s)[\\cdots]$\n\nIn this case, $P_{env}^\\star(s'|s,a_i, a_{-i})$ ($P_{env}^\\star$ is the maximizer in the expression above) being deterministic with all weights on the maximizer $s'$ is definitely possible."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4794/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699887091928,
                "cdate": 1699887091928,
                "tmdate": 1699887091928,
                "mdate": 1699887091928,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CPWnA016CU",
                "forum": "RNgZTA4CTP",
                "replyto": "5fNNlKH2ao",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4794/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4794/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We don't need to further search for all possible $P_{env}$, because there is only one $P_{env}$. $P_{env}$ is the property of the environment. It is given and will not change. \n\nLet us give an example. In the following tabular case, $p_{env}$ is defined as\n|           | $a_{-i}^1$                                                   | $a_{-i}^2$                                                   |\n| --------- | ------------------------------------------------------------ | ------------------------------------------------------------ |\n| $a_{i}^1$ | $p_{env}(s_1\\mid s_0,a_i^1,a_{-i}^1) = 0.2$, $p_{env}(s_2\\mid s_0,a_i^1,a_{-i}^1) = 0.8$ | $p_{env}(s_1\\mid s_0,a_i^1,a_{-i}^2) = 0.8$, $p_{env}(s_2\\mid s_0,a_i^1,a_{-i}^2) = 0.2$ |\n| $a_{i}^2$ | $p_{env}(s_1\\mid s_0,a_i^2,a_{-i}^1) = 0.1$, $ p_{env}(s_2\\mid s_0,a_i^2,a_{-i}^1) = 0.9$ | $p_{env}(s_1\\mid s_0,a_i^2,a_{-i}^2) = 0.9$, $ p_{env}(s_2\\mid s_0,a_i^2,a_{-i}^2) = 0.1$ |\n\nAgent i can only perceives $P_i$, which depends on $\\pi_{-i}$. For example, if $\\pi_{-i}(a_{-i}^1|s_0) = 1$, $P_i(s_1\\mid s_0,a_i^1) = 0.2$. However, in this environment, $P_i(s_1\\mid s_0,a_i^1) $ is impossible to be 1."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4794/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699889192206,
                "cdate": 1699889192206,
                "tmdate": 1699889230418,
                "mdate": 1699889230418,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EGgTyJngTe",
                "forum": "RNgZTA4CTP",
                "replyto": "5fNNlKH2ao",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4794/Reviewer_FC5t"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4794/Reviewer_FC5t"
                ],
                "content": {
                    "comment": {
                        "value": "\"We don't need to further search for all possible $P_{env}$, because there is only one $P_{env}$. \" --- This only applies when you know $P_{env}$.  \n\nSo can you confirm that the technique developed in Section 2.2 only applies to the case when $P_{env}$ is known to every player? That's because the search space of $P_i(\\cdot|s,a_i)$ depends on $P_{env}$. So in order for every player to perform the search, knowledge on $P_{env}$ is required."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4794/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699896180913,
                "cdate": 1699896180913,
                "tmdate": 1699896457226,
                "mdate": 1699896457226,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TaduxvQnHj",
                "forum": "RNgZTA4CTP",
                "replyto": "5fNNlKH2ao",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4794/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4794/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "In theory of the operators (Section 2.2), the agents of course can know $P_{env}$. But knowing $P_{env}$ is for knowing $P_i$, so they can directly know all possible $P_i$ in theory.\n\nIn algorithm, the agents do not need to know $P_{env}$, but they can search for all possible $P_i$. We will explain why they can do it. Once other agents $-i$ take a policy $\\pi_{-i}$, the agent $i$ will observe a possible $P_i$ by collecting experiences in the environment. Do you think so? If other agents $-i$ go through all possible policies $\\pi_{-i}$, the agent $i$ search for all possible $P_i$. So in this process, the agent $i$ searches for all possible $P_i$ and does not know $P_{env}$. \n\nTake an example. I do not know the review mechanism of ICLR ($P_{env}$), but if I submit enough papers (collecting experiences in the environment), I can know all possible rating situations ($P_i$).\n\nNote there is only one $P_{env}$, the $P_i$ changes only when $\\pi_{-i}$ changes. A $\\pi_{-i}$ totally determines a $P_i$."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4794/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699897613399,
                "cdate": 1699897613399,
                "tmdate": 1699898125728,
                "mdate": 1699898125728,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4FMdRsmGNc",
                "forum": "RNgZTA4CTP",
                "replyto": "TaduxvQnHj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4794/Reviewer_FC5t"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4794/Reviewer_FC5t"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response. Based on your explanation, to my understanding, you're using empirical data to fit an estimated transition $\\hat{P} _{env}(\\cdot | s, a _i, a _{-i})$, and then use this estimated transition $\\hat{P} _{env}$ to form a search space for $P _i$.  But then in the first inequality in the proof of Lemma 2, the $P^* _i$ on the right-hand side is calculated based on the estimated transition $\\hat{P} _{env}$, while the $P _{env}$ on the left-hand side the the true transition. The mismatch is not handled in the derivation.  I think this has significant impact to the convergence proof later, because you make the Q function monotonically increasing. If at the earlier phase the estimated transition $\\hat{P} _{env}$ is inaccurate, then it's possible that $Q_i(s, a_i)$ may overestimate its true maximum value (and will never be reduced again). Then convergence may not hold.  The only case where such error is not presented is when the environment is deterministic. \n\nAbout the difference between \"containing $(s, a_i, r, s')$\" and \"containing $P_i$\", they are not really the same concept --- difference $(s,a_i, r,s')$ can be generated from the same $P_i$, and same $(s,a_i, r,s')$ can be generated from different $P_i$. I can understand now that you mean the $(s,a_i,r,s')$ are diverse enough. But the imprecise language has to be avoided. \n\nI'm probably having more understanding about the algorithm now. But the explanation of the algorithm has to be improved to avoid ambiguity for first-time readers, and notation system (e.g., the distinction between $\\hat{P} _{env}$ and $P _{env}$) needs also some refinement. You may also need to more clearly define the search space of $P_i$ (it's based on $\\hat{P} _{env}$ but not the true $P _{env}$). The convergence proof may also need some fix (addressing the issue above)."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4794/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699921027764,
                "cdate": 1699921027764,
                "tmdate": 1699921027764,
                "mdate": 1699921027764,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "kEdoT1mGv0",
            "forum": "RNgZTA4CTP",
            "replyto": "RNgZTA4CTP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4794/Reviewer_tj5R"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4794/Reviewer_tj5R"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an alternate update strategy for Multi-Agent Q-Learning in the \"fully decentralized\" setting, i.e., where individual agents only have access to their own actions. This update uses the \"best possible operator\" to update individual agents' Q-values based on an optimistic estimate of the other agents' Q-values using a 2-step update. On the theoretical front, the paper justifies this choice of update strategy by showing the optimality of this update strategy in ideal conditions (i.e., in the asymptotic limit and with access to $\\boldsymbol{\\pi}^*_{-i}$). The paper evaluates the performance of BQL on 4 domains and shows improved performance in each."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- **The experiments are extensive:** The paper compares BQL against reasonable baselines on four reasonable \"domains\". I am not well-versed in the MARL literature, so it's possible that these are cherry-picked, but there is enough evidence to convince me that BQL is doing something useful, esp., that BQL uses the batch data $\\mathcal{D}$ in a more useful way than baseline methods in the fully decentralized setting. Minor nitpicks:\n  1. The experiments are often not run till convergence, e.g., 3(c), 4(a,b), 5(b,d).\n  1. It would be nice to have upper bounds like Joint Q-Learning in the Tabular Case and a CTDE baseline in the NN-case.\n- **The paper is reasonably well-written:** I think the descriptions of the methods and experiments are good. It's not clear to me (as someone who doesn't actively work in RL) how novel the theory is, but it is quite clean and has some interesting ideas. Minor nitpicks:\n  1. It seems like the theory is mostly a reduction of the \"fully decentralized setting\" to the \"joint Q-learning\"/CTDE setting. However, the actual connection is quite obfuscated; could you clarify how these two are related (and what is different)? It also seems like BQL can be seen as an extension of I2Q to the stochastic setting. Is this a fair comparison?\n  1. I did not follow the last couple of steps in the proof of Lemma 4. Specifically, how are you combining the various expressions to get the second last equation, and how are you unrolling it to get the last one?"
                },
                "weaknesses": {
                    "value": "- **BQL is not \"fully decentralized\"?** In the tabular setting, Algorithm 1 assumes that you can ensure that agents independently explore, but it's not clear that's a realistic assumption in this \"fully decentralized setting\". BQL- is more realistic and still seems to outperform baselines, but it seems like the key ingredient to getting BQL- to work is ensuring enough exploration. It would be useful to compare how well BQL does based on how much exploration is allowed (e.g., by changing the distance of the initialization to an equilibrium). It would also be useful to talk about whether sufficient exploration is a realistic assumption in practice.\n- **No intuition for the 2-step update when the theoretical assumptions are broken:** The paper leans heavily on asymptotic intuitions, but a lot of the wins in 4.3 and 4.4 seem to come from sample efficiency. Is there any intuition for this? More generally, there seems to be a gap between theory and practice, esp., the improved performance of BQL when there is only a single replay buffer $\\mathcal{D}$. Are there any intuitions for why BQL works in this case?"
                },
                "questions": {
                    "value": "The things that it would be most useful to clarify are:\n1. Are the theoretical proofs a reduction of the \"fully decentralized setting\" to the CTDE setting? If so, what are the assumptions required? If not, what is the gap?\n1. How does the performance of BQL change as you modify the amount of exploration that is performed by the agents?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4794/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4794/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4794/Reviewer_tj5R"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4794/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699401158611,
            "cdate": 1699401158611,
            "tmdate": 1699636461582,
            "mdate": 1699636461582,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2lDVCcrqnH",
                "forum": "RNgZTA4CTP",
                "replyto": "kEdoT1mGv0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4794/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4794/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> It seems like the theory is mostly a reduction of the \"fully decentralized setting\" to the \"joint Q-learning\"/CTDE setting. However, the actual connection is quite obfuscated; could you clarify how these two are related (and what is different)? It also seems like BQL can be seen as an extension of I2Q to the stochastic setting. Is this a fair comparison?\n\nDecentralized learning is very different from CTDE. As analyzed in section 2.1, due to the lack of other agents' actions, the transition probabilities become non-stationary, which breaks the assumption of convergence and optimality of existing RL methods. Therefore,  the main challenge of decentralized learning is to deal with non-stationary transition probabilities. However, in CTDE and joint learning, the transition probabilities are stationary, so they do not need to consider this problem at all. Therefore, we can say that the theory of BQL has no connection with CTDE and joint Q-learning. BQL is not an extension of I2Q. I2Q tries to directly obtain the optimal transition probabilities, but BQL obtains the value under the optimal transition probabilities by new operators. BQL is an extension of Hysteretic IQL to the stochastic setting, as discussed in Appendix B. Our method and baselines adopt the same settings and hyper-parameters, so we believe the comparison is fair.\n\n> I did not follow the last couple of steps in the proof of Lemma 4. Specifically, how are you combining the various expressions to get the second last equation, and how are you unrolling it to get the last one?\n\nFor Lemma 4, look at the RHS of the first inequality (the one after \"According to the proof of Lemma 3, we have\"). Notice the definition of $s^*$ and $a_i^*$, which achieve the maximum of RHS. For the LHS of this inequality, if we replace the $\\arg \\max_{s,a_i}$ (the max one for LHS ) with $s^*$ and $a_i^*$ (not the max one for LHS), the inequality holds. Then we know $Q_i^{k-1}+\\epsilon \\geq Q_i^e$, so if we replace $Q_i^e$ with $Q_i^{k-1}+\\epsilon$, the inequality still holds. \n\nFor the last inequality of Lemma 4, after merging the same terms, notice the definition of $s^*$ and $a_i^*$, which exactly correspond to the infinite norm. \n\n> BQL is not \"fully decentralized\"?\n\nWe only need a pre-defined exploration schedule for each agent. In training, the agents independently follow the exploration schedule without centralized control, so the decentralized assumption still holds. The key ingredient to getting BQL- to work is not enough exploration but the operator, because BQL- adopts the same exploration method with IQL, H-IQL, MA2QL-, and I2Q. \n\n> There seems to be a gap between theory and practice, esp., the improved performance of BQL when there is only a single replay buffer. Are there any intuitions for why BQL works in this case?\n\nOn the one hand, as analyzed in the last paragraph of section 2.4, if the non-stationary single replay buffer sufficiently goes through all possible transition probabilities, the theory still holds. On the other hand, the intuition behind the operators is optimistic value estimation, which has been verified to be useful in decentralized learning, e.g., H-IQL. However, since the optimistic target in H-IQL is the highest state value, the overestimation problem is severe in a stochastic setting. The optimistic target in BQL is the expected value over possible transition probabilities, so BQL can avoid overestimation and achieve better performance."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4794/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699885300796,
                "cdate": 1699885300796,
                "tmdate": 1699885300796,
                "mdate": 1699885300796,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]