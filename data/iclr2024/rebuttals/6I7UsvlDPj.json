[
    {
        "title": "LaMPP: Language Models as Probabilistic Priors for Perception and Action"
    },
    {
        "review": {
            "id": "MgMTG4FFSz",
            "forum": "6I7UsvlDPj",
            "replyto": "6I7UsvlDPj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4170/Reviewer_xpjw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4170/Reviewer_xpjw"
            ],
            "content": {
                "summary": {
                    "value": "The authors present a method for incorporating LLM likelihoods into tasks such as object segmentation, navigation, and action recognition. For each task, a task-specific base model is combined with a pre-trained LLM and each model estimates the likelihood of an event given some context, e.g. the likelihood of a bed in a bedroom and then likelihood that the given pixels are a bed.The LLM is used to refine the base model's estimated likelihood for an event and in doing so incorporates semantic background knowledge not necessarily present in the task-specific dataset into the final decision. The approach is primarily compared to the Socratic Method for prompting LLMs and the task-specific models run in the absence of the LLM. Three tasks are assessed and for each task the models are assessed on OOD, in-distribution, and zero-shot settings. Across tasks and settings, the proposed approach LaMPP out performs other methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is well written and easy to follow.\n- The authors introduce an interesting way to incorporate background knowledge from LLMs into graphical models to estimate the likelihood of events.\n- The authors evaluate on a tasks with different degrees of temporal reasoning required."
                },
                "weaknesses": {
                    "value": "- The authors primarily assess the model on tasks where the LLM is expected to perform well. For example, the task-specific training dataset does not contain all possible combinations of objects and the LLM is able to overcome this because of the large amount of data it was trained on. The authors do not assess performance on cases where the LLM may perform poorly on the downstream task. For example, sofas and sinks being in the same room or complex spatial reasoning tasks. The ways in which the LLM could harm performance are not explored and assessed. This is touched on at the very end of the conclusion, but should be addressed and measured more centrally."
                },
                "questions": {
                    "value": "- What is mean by \"easy-to-predict object labels\"? What makes them easy? What about hard object labels? \n- For the LM Queries in the semantic segmentation task, where does \"r\" come from?\n- How well is the model able to account for likelihoods of different numbers of objects? For example, a bedroom is likely to have a bedside table, but not likely to have 12 of them. \n- For 3.2 Experiments, you mention that \"a RedNet checkpoint\" is used. How was this checkpoint selected?\n- What are the differences in compute time between LaMPP, SM, and the base models?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4170/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698253233023,
            "cdate": 1698253233023,
            "tmdate": 1699636382824,
            "mdate": 1699636382824,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3ZAVtf9kSa",
                "forum": "6I7UsvlDPj",
                "replyto": "MgMTG4FFSz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4170/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4170/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## Weaknesses\n*W1: The authors primarily assess the model on tasks where the LLM is expected to perform well. For example, the task-specific training dataset does not contain all possible combinations of objects and the LLM is able to overcome this because of the large amount of data it was trained on. The authors do not assess performance on cases where the LLM may perform poorly on the downstream task. For example, sofas and sinks being in the same room or complex spatial reasoning tasks. The ways in which the LLM could harm performance are not explored and assessed. This is touched on at the very end of the conclusion, but should be addressed and measured more centrally.*\n\nOur paper rests on the key insight that LLMs, being pre-trained on large amounts of data, contain generic priors that are relatively well-aligned with many real tasks. Indeed, empirically, we found that LMs generally contained priors that were useful for real tasks (note we did not specifically design these tasks in a way to be conducive for LLMs \u2013 rather these were taken from real datasets), and that these priors offer complementary benefits to what domain-specific vision/robotics/etc. models have learned through supervised training.\n\nFurthermore, we also anticipate that as LLMs improve, they will have even better priors covering a broader range of tasks. This has the potential to include priors for some complex spatial reasoning tasks \u2013 indeed we see large improvements on these sorts of tasks moving from GPT3 to GPT4. \n\nFinally, the reviewer makes a good suggestion that we should provide examples of when LLM priors fail. The following is an example where the LLM has an incorrect prediction.  \n\n> Domain: Image Semantic Segmentation\n>\n> LM prior: predicts that the object next to the bed with a lamp on it is likely to be a **nightstand**\n> \n> Real label: the object is actually a **dresser**\n\nWe will include more qualitative examples of failure cases in the appendix of future versions of the paper.\n\n\n\n\n## Questions\n*Q1: What is mean by \"easy-to-predict object labels\"? What makes them easy? What about hard object labels?*\n\nBy \u201ceasy-to-predict object labels\u201d, we mean the ones that the domain-specific base model already achieves high accuracy on, e.g. object classes that are frequent in the training set or can be recognized unambiguously without scene-level context..\n\n*Q2: For the LM Queries in the semantic segmentation task, where does \"r\" come from?*\n\nRooms $r$ are latent / unobserved (note that they are unshaded in the figure). This means that to derive the probability of a pixel label, we perform inference and marginalize over all possible rooms.\n\n*Q3: How well is the model able to account for likelihoods of different numbers of objects? For example, a bedroom is likely to have a bedside table, but not likely to have 12 of them.*\n\nWe didn\u2019t find this necessary to model in our experiments, but could be straightforwardly accommodated with a slightly different graphical model.\n\n*Q4: For 3.2 Experiments, you mention that \"a RedNet checkpoint\" is used. How was this checkpoint selected?*\n\nFor ID generalization, we used a pre-trained checkpoint released by https://github.com/JindongJiang/RedNet . For OOD generalization, we fixed the number of training epochs to 1000 and selected the last RedNet checkpoint after training finished, without further hyper-parameter tuning. These details can be found in Appendix E.\n\n*Q5: What are the differences in compute time between LaMPP, SM, and the base models?*\n\n\nGood question! LaMPP has a fixed overhead cost for querying the GPT3 API, but otherwise runs the exact same inference procedure as the base model, adding a relatively inexpensive probabilistic inference step on top. SM adds the most computation time, as it has to query GPT3 (potentially multiple times) for inference on every example. We time inference over a subsample of 100 examples in image semantic segmentation and report results below. We will also insert these results into future versions of the paper:\n\n1. Base Model: 1.21 s/it\n2. SM: 40.7 s/it\n3. LaMPP: 1.83 s/it"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4170/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700159812425,
                "cdate": 1700159812425,
                "tmdate": 1700159812425,
                "mdate": 1700159812425,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rQzz0TwYvM",
                "forum": "6I7UsvlDPj",
                "replyto": "MgMTG4FFSz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4170/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4170/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer xpjw,\n\nThank you for your detailed review and feedback. As the discussion period is coming to a close, please let us know if our response has adequately addressed your concerns, or if you have any remaining questions and concerns. If not, we would appreciate if you could raise your score. Thank you for your hard work!"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4170/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700505311531,
                "cdate": 1700505311531,
                "tmdate": 1700505311531,
                "mdate": 1700505311531,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lprJ3DIyj8",
                "forum": "6I7UsvlDPj",
                "replyto": "rQzz0TwYvM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4170/Reviewer_xpjw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4170/Reviewer_xpjw"
                ],
                "content": {
                    "title": {
                        "value": "Thank you to author response"
                    },
                    "comment": {
                        "value": "Thank you for your responses. \n\nIf the paper is accepted, it would be beneficial to include a discussion of current limitations where the LLM\u2019s priors are subpar or harmful to help push and advance next steps in research."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4170/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700600824279,
                "cdate": 1700600824279,
                "tmdate": 1700600824279,
                "mdate": 1700600824279,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "LjIpnfrAOP",
            "forum": "6I7UsvlDPj",
            "replyto": "6I7UsvlDPj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4170/Reviewer_WR1X"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4170/Reviewer_WR1X"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents the use of LM (language model) as a source of prior distributions over labels, decisions or model parameters. The approach is empirically demonstrated through 3 different domains -- semantic image segmentation using SUN RGB-D dataset, indoor object navigation using Habitat challenge, and action recognition on Cross-Task dataset.\n\nThe three case studies cover diverse objectives and show consistent improvement on in-distribution, out-of-distribution and zero shot generalization. Further, the studies also show that the proposed approach is more cost effective than the existing approaches such as Socratic Modeling."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "-- The empirical studies show a delta between the proposed and existing approaches for the in-distribution results for image semantic segmentation.\n\n-- The three domains chosen, while having some similarities in terms of input modalities, is sufficiently diverse to demonstrate the generalization of the approach."
                },
                "weaknesses": {
                    "value": "-- The empirical results for zero-shot and OOD show very minor improvements relative to baseline. For image semantic segmentation, the results in A.3 show 0.2 and 0.5 mIOU improvement for OOD and ZS resp. Similarly, the improvements for ZS and OOD for video segmentation are not very significant.\n\n-- The general approach seems to require a significant amount of domain specific information which might work for smaller / restricted domains but will face issues when generalizing to broader / open domains."
                },
                "questions": {
                    "value": "-- Can this approach be shown to work on a large dataset (e.g., 100k+ labels)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4170/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698647661707,
            "cdate": 1698647661707,
            "tmdate": 1699636382747,
            "mdate": 1699636382747,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UApoi6nR1k",
                "forum": "6I7UsvlDPj",
                "replyto": "LjIpnfrAOP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4170/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4170/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your feedback! Below we address the weaknesses and questions.\n\n## Weaknesses\n### W1: The empirical results for zero-shot and OOD show very minor improvements relative to baseline. For image semantic segmentation, the results in A.3 show 0.2 and 0.5 mIOU improvement for OOD and ZS resp. Similarly, the improvements for ZS and OOD for video segmentation are not very significant.\n\nWe agree that on average the improvements are small in the image segmentation and video segmentation domains, but this paper focuses on improvements in the long tail of the distribution. The improvements are quite significant for specific categories in all 3 image segmentation cases (+18.9 in ID, +8.92 in OOD, +13.3 ZS), without much loss to the other categories (no more than -2.16 in ID, -2.5 in OOD, -5.0 ZS). The same can be said of the zero-shot video segmentation case, where \u201cadd oil to car\u201d improves by +8.96 points, while the category that drops the most (\u201cjack up a car\u201d) drops by -1.03 points. Note also that almost all categories improve in the navigation domain.\n\n### W2: The general approach seems to require a significant amount of domain specific information which might work for smaller / restricted domains but will face issues when generalizing to broader / open domains.\n\nLaMPP generally requires: 1. designing a graphical model for a task, and 2. designing prompts for extracting priors for LMs to insert into the graphical model.\n\n1. We offer a guideline for designing the graphical model in section 2: you can either decompose the prediction task into $p(y)p(x|y)$, using the LM to put a prior over labels $p(y)$, or you can decompose learning into $p(theta)p(y|x, theta)$, and use the LM to put a prior over parameters $p(theta)$. While there may be some customization necessary to work out the exact form of each of these probabilities for each task, the general structure of graphical models follows from these above design decisions (figures 2\u20134).\n2. Defining prompts (prompt engineering) is a typical requirement for inducing LMs to perform various tasks these days. Thus, at this step our method requires no more domain-specific engineering than what is typically required.\n\nNote that alternative methods for incorporating LM priors into domain-specific grounded models all require some degree of domain-specific engineering. For example, Socratic models / model chaining / tool usage require that the prompts and associated model chains are designed on a per-task basis. Nonetheless, these paradigms have proven useful and transferable. In fact, LaMPP offers a further step in the direction of standardization and formalization by offering a unifying view of LMs as priors. We will try to include more of this \u201cphilosophical\u201d motivation in future versions of the paper."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4170/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700159700423,
                "cdate": 1700159700423,
                "tmdate": 1700159700423,
                "mdate": 1700159700423,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "K1rRHy82AO",
                "forum": "6I7UsvlDPj",
                "replyto": "LjIpnfrAOP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4170/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4170/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer WR1X,\n\nThank you for your detailed review and feedback. As the discussion period is coming to a close, please let us know if our response has adequately addressed your concerns, or if you have any remaining questions and concerns. We appreciate your hard work!"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4170/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700503019040,
                "cdate": 1700503019040,
                "tmdate": 1700503050398,
                "mdate": 1700503050398,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "b8YEdjD1sn",
            "forum": "6I7UsvlDPj",
            "replyto": "6I7UsvlDPj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4170/Reviewer_C44C"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4170/Reviewer_C44C"
            ],
            "content": {
                "summary": {
                    "value": "I have reviewed a previous version of this manuscript.\n\nThe manuscript formulates labeling and decision-making as inference in probabilistic graphical models, where language models can act as the probabilistic prior distribution over labels, decisions, and parameters. The manuscript takes a case study approach, to consider different task settings."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The manuscript is well-written.\n\nThe problem formulation is interesting."
                },
                "weaknesses": {
                    "value": "Figure 1 is missing illustration for the \u201crobot navigation\u201d task, probably due to attempts to keep it in a floating single column format. Consider redrawing the figure.\n\nSection {3,4,5} (prompting): Provide discussion of how the fixed text prompt templates were chosen, particularly for the action recognition case study. Provide some examples of other templates that were tried, even if they did not prove successful.\n\nSection 5.2: The lack of discussion surrounding evaluation on ID settings (still) stands out; it would be a worthwhile a comparison of task/domain difficulty. Provide discussion in the main content.\n\nSection 5.3: Several other LLMs and VLMs have been available for quite some time now. Missing comparisons with other foundation models.\n\nSection 6: The related works section is sparse and uninformative. The manuscript forgot to highlight *both* the strengths and weaknesses of the relevant related work and describe the ways in which the proposed method improves on (or avoids) those same limitations. This should serve as a basis for the experimental comparisons and should follow from the stated claims of the paper."
                },
                "questions": {
                    "value": "N/A \u2013 see above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4170/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698818185487,
            "cdate": 1698818185487,
            "tmdate": 1699636382673,
            "mdate": 1699636382673,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GtUVJyxbuD",
                "forum": "6I7UsvlDPj",
                "replyto": "b8YEdjD1sn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4170/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4170/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your feedback! Below we address the weaknesses. We will note that W1, W2, W3, and W5 all suggest adding additional content to our paper, which is currently at the page limit. We will try our best to compress our other content to accommodate any necessary changes, but please also let us know if there is anything you believe can be deleted! \n## Weaknesses\n### W1: Figure 1 is missing illustration for the \u201crobot navigation\u201d task, probably due to attempts to keep it in a floating single column format. Consider redrawing the figure.\n\nThanks for the feedback, we will redraw the figure to include the robot navigation task.\n\n### W2: Section {3,4,5} (prompting): Provide discussion of how the fixed text prompt templates were chosen, particularly for the action recognition case study. Provide some examples of other templates that were tried, even if they did not prove successful.\n\nThanks for the feedback, we will include this discussion in future versions of the paper. For the image semantic segmentation and navigation tasks, there was minimal prompt engineering required \u2013 the LM outputted well-calibrated priors, extractable with the simplest prompts.\nHowever, for video-action recognition we had to do a substantial amount of prompt engineering. For example, some other prompts we tried that included:\n\n> **Prompt:** Your task is to *[t]*. Your actions are: {*[Y]*}\n>\n> The step after *[y]* is *[y\u2019]*: \n>\n> **LM:** [plausible / implausible]\n\n> **Prompt:** Your task is to *[t]*. Your actions are: {*[Y]*}\n>\n> The step after *[y]* is\n>\n> **LM:** [y\u2019]\n\nWe also tried having the LM produce a global ordering and setting probability from an action later in the sequence to earlier in the sequence to 0:\n\n> **Prompt:** Your task is to *[t]*. Your set of actions is: {*[Y]*}\n>\n> The correct ordering is:\n\n\n### W3: Section 5.2: The lack of discussion surrounding evaluation on ID settings (still) stands out; it would be a worthwhile a comparison of task/domain difficulty. Provide discussion in the main content.\n\nGood catch\u2013 we did not do in-domain evaluation for the video-action task as it did not improve performance. We will add a small discussion about this in the main text of future versions. We believe this is because the base model is simple enough that simply counting transitions and observation co-occurrence is sufficient for estimating parameters, and the action space is small enough that the data is sufficient to comprehensively cover actions and transitions for each task. Thus, the fully-supervised model is as good as it will get.\n\n### W4: Section 5.3: Several other LLMs and VLMs have been available for quite some time now. Missing comparisons with other foundation models.\n\nWe have comparisons to three different vision models across three different tasks, including comparison to both CLIPSeg and RedNet in segmentation tasks. If you have a specific comparison you would like to see, please let us know. Furthermore, our method is complementary to the specific foundation model used for a task, and can operate on top of any base foundation model. While VLMs may be starting to find some success with simpler tasks like image segmentation, we anticipate that it will be a while before multimodal models can accommodate actions and videos.\n\n### W5: Section 6: The related works section is sparse and uninformative. The manuscript forgot to highlight both the strengths and weaknesses of the relevant related work and describe the ways in which the proposed method improves on (or avoids) those same limitations. This should serve as a basis for the experimental comparisons and should follow from the stated claims of the paper.\n\nThanks for the feedback. Again, we believe we have highlighted strengths and weaknesses of the most pertinent related work: Socratic models (L380-384) and LMPriors (L385-387). Due to space limitations, we were unable to fit a more comprehensive related works section, but we will expand it in future versions."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4170/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700159645042,
                "cdate": 1700159645042,
                "tmdate": 1700159645042,
                "mdate": 1700159645042,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "s7vffOFUjz",
                "forum": "6I7UsvlDPj",
                "replyto": "b8YEdjD1sn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4170/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4170/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer C44C,\n\nThank you for your detailed review and feedback. We have revised the manuscript according to your feedback, addressing W1, W2, W3, and W5.\n\nW1: See Figure 1 in new manuscript\n\nW2: See new Appendix C.1\n\nW3: See footnote 5 in Section 5.2\n\nW5: We have added sentence in the related work (highlighted in red) highlighting limitations of prior approaches and how we build upon them.\n\nPlease take a look and let us know if these concerns have been adequately addressed. Please also let us know if there are any baselines you have in mind for W4, and we will try our best to address them in the limited available time we have left.\n\nFinally, as the discussion period is coming to a close, we would like to ask if there is anything else we could address. If not, we would appreciate if you could raise your score. Thanks again for your helpful feedback and review."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4170/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700502861502,
                "cdate": 1700502861502,
                "tmdate": 1700502985132,
                "mdate": 1700502985132,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]