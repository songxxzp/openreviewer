[
    {
        "title": "StructChart: Perception, Structuring, Reasoning for Visual Chart Understanding"
    },
    {
        "review": {
            "id": "SHGPfhaTaz",
            "forum": "4YgfwJBJeQ",
            "replyto": "4YgfwJBJeQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1125/Reviewer_Rkaq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1125/Reviewer_Rkaq"
            ],
            "content": {
                "summary": {
                    "value": "The authors claim that they proposed a unified and label-efficient learning paradigm for joint perception and reasoning tasks, which can be generally applicable to different downstream tasks, beyond the question-answering task as specifically studied in peer works."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The authors claim that they proposed a unified and label-efficient learning paradigm for joint perception and reasoning tasks, which can be generally applicable to different downstream tasks, beyond the question-answering task as specifically studied in peer works."
                },
                "weaknesses": {
                    "value": "1. The experiments should focus on the generalizability of the conclusions/findings derived from the powerful transformer-based models, and it remains a concern.\n\n2. What is the relation and difference with the current popular benchmark (e.g., FigureQA[1])?\n\n[1] Kahou, Samira Ebrahimi, et al. \"Figureqa: An annotated figure dataset for visual reasoning.\" arXiv preprint arXiv:1710.07300 (2017)."
                },
                "questions": {
                    "value": "Please refer to Weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1125/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698667742403,
            "cdate": 1698667742403,
            "tmdate": 1699636038477,
            "mdate": 1699636038477,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "US0ST6wlo4",
                "forum": "4YgfwJBJeQ",
                "replyto": "SHGPfhaTaz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1125/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1125/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Rkaq: Part 1/2"
                    },
                    "comment": {
                        "value": "**Q1: The experiments should focus on the generalizability of the conclusions/findings derived from the powerful transformer-based models, and it remains a concern.**\n\n**A1:** Thank you for providing such constructive comments! It also gets us to rethink the generalizability of the conclusions/findings. In this regard, we use ChartQA, PlotQA and Chart2Text as the validation sets for the perception stage, and carry out supplementary experiments on the following conclusions to verify the generalization ability from the following two aspects:\n\n1. In Section 4.2 of the main text, we claim that, larger scale of the data will lead to better performance for CIE task, both in real data domain and our simulated domain. The following experimental results show that, the larger the amount of real-world or simulation data, the better the CIE task performance on **all** the ChartQA, PlotQA, and Chart2Text datasets. \"Merging\" in the following tables refers to that, training samples are merged from ChartQ, PlotQA, and Chart2Text.   \n\n| Val Set | Model\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0| Train Set\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0| mPrecision (strict) |\u00a0\u00a0mPrecision (slight)\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0|\u00a0\u00a0\u00a0mPrecision (high)\u00a0\u00a0\u00a0\u00a0\u00a0| Precision(EM)|\n|:-------:|:-----------:|------------------|:-------:|:--------:|:--------:|:--------:|\n| ChartQA | StructChart | ChartQA \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0| 0.6770\u00a0\u00a0\u00a0\u00a0| 0.7792 | 0.8274 | 0.6326 |\n| ChartQA | StructChart | Merging\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0| 0.7017\u00a0\u00a0\u00a0\u00a0\u00a0| 0.8227 | 0.8591 | 0.6506 |\n| ChartQA | StructChart | Merging+SimChart9K | 0.7187\u00a0\u00a0\u00a0\u00a0\u00a0| 0.8311 | 0.8572 | 0.6642 |\n\n\n| Val Set | Model\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0| Train Set\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0| mPrecision (strict) |\u00a0\u00a0mPrecision (slight)\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0|\u00a0\u00a0\u00a0mPrecision (high)\u00a0\u00a0\u00a0\u00a0\u00a0| Precision(EM)|\n|:-------:|:-----------:|------------------|:-------:|:--------:|:--------:|:--------:|\n| PlotQA | StructChart | PlotQA\u00a0\u00a0\u00a0\u00a0| \u00a00.1995\u00a0\u00a0| 0.7848 | 0.8271 | 0.1736 |\n| PlotQA | StructChart | Merging\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0| 0.4549\u00a0\u00a0\u00a0\u00a0\u00a0| 0.8589 | 0.8921 | 0.3385 |\n| PlotQA | StructChart | Merging+SimChart9K | 0.4596\u00a0\u00a0\u00a0\u00a0\u00a0| 0.8612 | 0.8998 | 0.3612 |\n\n\n| Val Set | Model\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0| Train Set\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0| mPrecision (strict) |\u00a0\u00a0mPrecision (slight)\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0|\u00a0\u00a0\u00a0mPrecision (high)\u00a0\u00a0\u00a0\u00a0\u00a0| Precision(EM)|\n|:-------:|:-----------:|------------------|:-------:|:--------:|:--------:|:--------:|\n| Chart2Text | StructChart | Chart2Text |\u00a00.1936 | 0.5524 | 0.6945 | 0.1442 |\n| Chart2Text | StructChart | Merging\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0| 0.3156\u00a0\u00a0\u00a0\u00a0\u00a0| 0.7141 | 0.8085 | 0.2381 |\n| Chart2Text | StructChart | Merging+SimChart9K | 0.3394\u00a0\u00a0\u00a0\u00a0\u00a0| 0.7451 | 0.8296 | 0.2635 |\n\n\n2. In Section 4.3 of the main text, we claim that \"We can achieve a high-performance CIE only leveraging few-shot real samples\", which has been verified on ChartQA dataset in Table 5 of the main text.  **To further verify the generalizability of this finding**, we also supplement validation experiments with PloatQA and Chart2Text. The results in the following table show that, with the help of SimChart9K, only 10% original real samples in PlotQA and 20% in Chart2Text can basically achieve equivalent CIE performance under the 100% real training samples.\n\n| Val Set | Model\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0| Train Set\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0| mPrecision (strict) |\u00a0\u00a0mPrecision (slight)\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0|\u00a0\u00a0\u00a0mPrecision (high)\u00a0\u00a0\u00a0\u00a0\u00a0| precision(EM)|\n|:-------:|:-----------:|------------------|-------|--------|--------|--------|\n| PlotQA | StructChart | PlotQA\u00a0\u00a0\u00a0\u00a0| \u00a00.1995\u00a0\u00a0| 0.7848 | 0.8271 | 0.1736 |\n| PlotQA | StructChart | PlotQA0.1+SimChart9K | \u00a0\u00a00.1887\u00a0\u00a0| 0.7976 | 0.8063 | 0.1612 |\n| Chart2Text | StructChart | Chart2Text |\u00a00.1936 | 0.5524 | 0.6945 | 0.1442 |\n| Chart2Text | StructChart | Chart2Text0.2+SimChart9K | \u00a0\u00a00.2610\u00a0|  0.5711 |  0.6871 |  0.1729 |"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1125/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700141844155,
                "cdate": 1700141844155,
                "tmdate": 1700141844155,
                "mdate": 1700141844155,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5tZmd6UY4r",
                "forum": "4YgfwJBJeQ",
                "replyto": "SHGPfhaTaz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1125/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1125/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Rkaq: Part 2/2"
                    },
                    "comment": {
                        "value": "**Q2: What is the relation and difference with the current popular benchmark (e.g., FigureQA)?**\n\n**A2:** Thank the reviewer very much for this valuable comment. FigureQA is also a benchmark for chart understanding task. However, there are still some differences between FigureQA and the mentioned datasets (ChartQA, PlotQA and Chart2Text):\n- Overall, the images in FigureQA dataset are all simulated samples based on manual rules, while ChartQA and Chart2Text are sampled from real world.\n- For perception stage, ChartQA and Chart2Text provide annotations in CSV format for CIE task. But for FigureQA and PlotQA, we need to preprocess relatively complex retrieval from the raw annotation (JSON file).\n- For reasoning stage, the questions in FigureQA dataset are based on fifteen sentence patterns (e.g.  \"Is X the minimum?\", \"Does X have the lowest value?\"). And all the answer pairs are \"yes\" or \"no\". In contrast, the mentioned datasets, such as ChartQA, PlotQA, use human-authored QA annotations. Besides, Chart2Text is designed for chart summarization task.\n\nFurthermore, according to the reviewer's comments, we have supplemented additional experiments for QA task on FigureQA. The output of Matcha is still HTML format (e.g. '('title | title <0x0a> yaxis_label | xaxis_label <0x0a> burlywood | 64 <0x0a> mint | 19.77 <0x0a> dim gray |...'), so the quantitative results of Matcha are almost close to zero. In contrast, our StructChart still works well on FigureQA dataset. For example, our StructChart outperforms Deplot by 9.4% for QA task.\n\n|\u00a0\u00a0Input Representations\u00a0\u00a0\u00a0| Model\u00a0\u00a0\u00a0| Train Set\u00a0\u00a0| Acc on FigureQA\u00a0| \n|:-----:|---------------------|:---------------------:|:------:|\n| \\ | Deplot+GPT-3.5 | ChartQA+PlotQA+C.D.\u00a0\u00a0| 53.9 |\n| STR | StructChart+GPT-3.5 | Merging+SimChart9k\u00a0\u00a0| **63.3** |"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1125/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700142011317,
                "cdate": 1700142011317,
                "tmdate": 1700142011317,
                "mdate": 1700142011317,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "MHv4fCCQF5",
            "forum": "4YgfwJBJeQ",
            "replyto": "4YgfwJBJeQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1125/Reviewer_ojHH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1125/Reviewer_ojHH"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a novel approach known as the integrated perception and reasoning paradigm, designed to enhance the comprehension of visual charts. Initially, StructChart transforms chart data from the common tabular format, such as linearized CSV, into the newly introduced Structured Triplet Representations (STR). Concurrently, we present the Structuring Chart-oriented Representation Metric (SCRM) for a quantitative assessment of performance. Additionally, we investigate the potential of utilizing the capabilities of a Large Language Model (LLM) to expand the training dataset, specifically the SimChart9K dataset."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1) The paper introduces a new Structured Triplet Representation (STR) instead of CSV format.\n2) The Structuring Chart-oriented Representation Metric (SCRM) is suitable for various tasks related to chart perception.\n3) This paper provides the SimChart9K dataset, which leverages Large Language Models (LLM) to enhance chart datasets for training purposes.\n4) The paper demonstrates good writing quality, encompassing recent literature developments and technical aspects. Notably, Table 1 is informative, and the related work section has covered relevant areas. Additionally, Figure 1 is thoughtfully organized and imparts valuable insights.\n5) The experiments are well-considered, offering comprehensive results and insightful ablation studies.\n6) The potential impact is substantial, given the practical utility of the proposed method, which addresses challenges not effectively addressed by ChatGPT or existing solutions."
                },
                "weaknesses": {
                    "value": "1) The approach itself is a bit straightforward as it is not totally end-to-end. However, I don't think it is a drawback, especially given its novel setting compared to other literature.\n2) Some typos: e.g. 4.3 ACHIEVING 100% PERFORMANCE BY ONLY 20% REAL DATA. The ending stop shall be removed."
                },
                "questions": {
                    "value": "Can you compare your method with GPT-4V or other general multi-modality tools?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1125/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698731861835,
            "cdate": 1698731861835,
            "tmdate": 1699636038401,
            "mdate": 1699636038401,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xJAEcw17wi",
                "forum": "4YgfwJBJeQ",
                "replyto": "MHv4fCCQF5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1125/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1125/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ojHH: Part 1/1"
                    },
                    "comment": {
                        "value": "**Q1\uff1aThe approach itself is a bit straightforward as it is not totally end-to-end. However, I don't think it is a drawback, especially given its novel setting compared to other literature.**\n\n**A1**: Thanks for your approval and insightful comment. We are honored that we have reached a consensus that our proposed two-stage pipeline is not a drawback. Furthermore, the main reason why two-stage pipeline is not a drawback is as follows:\n- Explicit chart image representations (Such CSV format or the proposed STR format) obtained using perception module can enhance the interpretability of the subsequent reasoning stage.\n- The first stage of the proposed perception-reasoing pipeline explicitly extracts the structured text to represent chart image (i.e., CIE task), which can be used as the pre-training corpus of the existing large language models or vision language models. \n\n&ensp;\n\n**Q2: Some typos: e.g. 4.3 ACHIEVING 100% PERFORMANCE BY ONLY 20% REAL DATA. The ending stop shall be removed.**\n\n**A2:**  We thank the reviewer very much for pointing this out, and we have corrected it in the revised manuscript.\n\n&ensp;\n\n**Q3\uff1aCan you compare your method with GPT-4V or other general multi-modality tools?**\n\n**A3:**  We would like to thank the reviewer for your interest in our work, and we also have a fascination with the performance of GPT-4V, since OpenAI has just recently opened up their API for GPT-4V.\n\nAccording to the reviewer's comments, we would like to show three visualization comparisons among StrcutChart, **GPT-4V** and **LLAVA-1.5** in the following link [https://drive.google.com/file/d/1BX7wqS792Z7N4uJNcBagSJbo5622wVYB/view?usp=drive_link]. Note that for QA task, our StructChart is restricted to generating answers only in order to report the quantitative results on ChartQA dataset.\n\nFrom the visualization results, it can be seen that the proposed StructChart has a strong Chart-based reasoning ability, showing promising performance on multiple downstream tasks.  Although GPT-4V also has a strong performance, our model weights, training code, simulation code, and simulation datasets will all be open-sourced for this community! Besides, we have supplemented the visualization comparison results on Page 24-26, Appendix I of the revised manuscript."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1125/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700130249358,
                "cdate": 1700130249358,
                "tmdate": 1700676817540,
                "mdate": 1700676817540,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0OqtVgMo4x",
                "forum": "4YgfwJBJeQ",
                "replyto": "MHv4fCCQF5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1125/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1125/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to a reply"
                    },
                    "comment": {
                        "value": "Dear Reviewer ojHH,\n\nWe are very grateful to the reviewer for the insightful comment about the comparison with GPT-4V or other general vision-language methods.\n\nDuring the rebuttal period, we have supplemented the visualization comparisons among StrcutChart, GPT-4V and LLAVA-1.5 in the following link https://drive.google.com/file/d/1BX7wqS792Z7N4uJNcBagSJbo5622wVYB/view?usp=drive_link. Also, the comparison results have been included in Page 24-26, Appendix I of the updated manuscript.\n\nThanks for your effort in improving the quality of our manuscript, and it is appreciated that you could consider these modifications and rapid actions of our manuscript.\u2002\n\n&ensp;\n\nBest regards,\n\nAuthors of Paper 1125"
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1125/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700711276010,
                "cdate": 1700711276010,
                "tmdate": 1700711276010,
                "mdate": 1700711276010,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wqs4PtBDTx",
            "forum": "4YgfwJBJeQ",
            "replyto": "4YgfwJBJeQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1125/Reviewer_mxZb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1125/Reviewer_mxZb"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a StructChart methodology for extracting information from visual chart data to enhance downstream perception and reasoning tasks. To achieve this, the authors initially transform the chart data from Linearized Comma-Separated Values Tokens (LCT) into the proposed Structured Triplet Representations (STR), thereby establishing a connection between chart perception and reasoning. To qualitatively evaluate the chart perception tasks, the authors also introduce a Structuring Chart-oriented Representation Metric (SCRM). This metric evaluates the extracted chart data using Intersection over Union (IoU). Additionally, the authors construct a synthetic chart dataset called SimChart9K, which is helpful for downstream tasks. The numerical experiment shows the efficiency of the proposed method. However, I have some concerns about this paper. My detailed comments are as follows."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The authors seek to transform the chart information from LCT to STR, which bridges the gap between chart perception and reasoning.\n2. The authors construct a synthetic dataset named SimChart9K by leveraging an LLM-based self-inspection data production scheme.\n3. Experimental results on the ChartQA and PlotQA datasets demonstrate the effectiveness of the proposed StructChart method."
                },
                "weaknesses": {
                    "value": "1. This paper emphasizes that STR reduces the task gap between chart perception and reasoning. However, the reasoning process is based on the black box GPT-3.5, which cannot find the relationship between STR and the reasoning process. More explanations are required. \n2. As shown in Eqn. (2), the SRT splits each element in LCT as one unique sample. However, SRT may destroy the intrinsic relationship between the original elements, making them independent. \n3. In Equation (3), the authors introduce the Entity Match method, which employs Intersection over Union (IOU) to compare predictions with ground-truth entities. Do the authors consider the order of the entity strings during the matching process? Ordinarily, aligning the strings in the correct order is essential for accurate matching. However, Equation (3) lacks a detailed explanation of how this matching process is carried out. More discussions are required.\n4. The authors adopt STR to make each entity independent. How do the authors determine the correspondence between predictions and specific ground-truth?\n5. In Table 2, it's noteworthy that the authors have not included a comparison with Matcha[1] and Deplot[2] on the Chart2Text[3] dataset. An explanation for this omission is needed.\n6. In Table 2, the comparisons between StructChart and the compared methods (Matcha and Deplot) somewhat is unfair. StructChart leverages powerful GPT-3.5 as the reasoning model, whereas Matcha is based on Pix2Struct[4] and Deplot relies on Codex or GPT-3. Thus, the disparities in performance between StructChart and the compared methods could potentially stem from the utilization of different reasoning models (i.e., GPT-3 and GPT-3.5).\n7. In Table 4, the authors conduct a comparison of StructChart with various baseline methods using the Exact Match metric. It's important to note, however, that the authors have omitted a direct comparison with Deplot, which outperforms StructChart with a higher score (i.e., 76.7 compared to 65.3). In this way, StructChart has no advantage compared with Deplot even if it uses more powerful GPT-3.5.\n8. In Table 2, the \u2018merging\u2019 is confusing. Does it merge the ChartQA with the SimChart 9K?\n\n\n[1] Matcha: Enhancing visual language pretraining with math reasoning and chart derendering, ACL 2023.\n\n[2] Deplot: One-shot visual language reasoning by plot-to-table translation, ACL Findings 2023.\n\n[3] Chart-to-text: Generating natural language descriptions for charts by adapting the transformer model, ACL 2020.\n\n[4] Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding. ICML 2023."
                },
                "questions": {
                    "value": "Please refer to the Weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/a"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1125/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698759968916,
            "cdate": 1698759968916,
            "tmdate": 1699636038275,
            "mdate": 1699636038275,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SJdN98oZ8H",
                "forum": "4YgfwJBJeQ",
                "replyto": "wqs4PtBDTx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1125/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1125/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer mxZb: Part 1/3"
                    },
                    "comment": {
                        "value": "**Q1: The reasoning process is based on the black box GPT-3.5, which cannot find the relationship between STR and the reasoning process**\n\n**A1:**\nWe are very grateful to the reviewer for the insightful comments. We agree with the reviewer that the reasoning module is based on the black box, but the proposed Structured Triplet Representations (STR) can be regarded as the input of such a reasoning module. As stated in Section 3.1 of the main text, our StructChart employs the two-stage method including perception and reasoning stages, respectively. The perception module generates the STR, and then, the generated STR can be regarded as the input of the reasoning module (GPT-3.5).  Besides, due to that the generated STR can effectively represent the positional relations between row and column headers of a given chart, the perception-reasoning task gap can be effectively alleviated.                \n\nExperimentally, we have verified that the proposed STR representations are beneficial for improving reasoning ability on different downstream tasks. For example, in QA task, the results in the following Table show that, STR outperforms LCT by 9.0% evaluated on ChartQA validation set (all the settings in GPT-3.5 using LCT input and STR input are consistent, including prompt, code version and parameters). For other downstream tasks including summarization and redrawing tasks, we visually illustrate the comparisons between LCT and STR  in Appendix E (please see Figs 11 and 12), where a comma is introduced as noise as illustrated in Fig. 11, and the separator comma itself is included as illustrated in Fig. 12. Both of the case indicate that, STR is more friendly for reasoning process than LCT.\n\n| Input Representation | Method | Train on | QA val |\n|:---:|:---:|:---:|:---:|\n| LCT | StructChart+GPT3.5 | ChartQA+SimChart9K | 56.3 |\n| **STR** | StructChart+GPT3.5 | ChartQA+SimChart9K | **65.3** |\n\n&ensp;\n\n**Q2: STR may destroy the intrinsic relationship between the original elements, making them independent**\n\n**A2:** Thanks. First of all, the proposed Structured Triplet Representations (STR)  does not destroy the intrinsic relationship between the original elements, due to the following reasons:\n\nIn Section 3 of the main text, we claim that, \"The extracted intermediate CSV text is structured into a triplet form to elucidate the intricate position relationship between the header and index\" **In this case**, \"header\" indicates column header in CSV format while \"index\" refers to row header. For STR transformation, each related column header $Entity_{c_m}$ and row header $Entity_{r_n}$ will form a tuple with their relationship $Value_{r_n}^{c_m}$, and the set composed of all tuples aims to represent a complete chart image. Hence, the intrinsic relationship between the original entities will not be destroyed.\n\n&ensp;\n\n**Q3: Do the authors consider the order of the entity strings during the matching process? Eq (3) lacks a detailed explanation of how this matching process is carried out**\n\n**A3:**\nThanks for your valuable comments. In Eq. 3 of the main text, the order of two Entity strings $Entity_{r_n}$ and $Entity_{c_m}$ in each Tuple $(Entity_{r_n}, Entity_{c_m}, Value_{r_n}^{c_m})$ is important for matching. For example, the two Tuples $(Alice, Bob, 100)$ and $(Bob, Alice, 100)$ should be matched exactly. **To ensure the order of Entity string does not affect the matching process**, we reorder both two Entity strings according to the following pre-process:\n* Lowercase all characters in Entity strings.\u00a0\n* The two Entity strings in each Tuple are sorted by the ASCII of the first letter.\u00a0\n* If the ASCII of the first letter is the same, the second letter is compared, etc.\u00a0\n\nIn the revised manuscripts, we have supplemented the detailed explanation of how to perform the matching process in Appendix B on Page 14.\n\n&ensp;\n\n**Q4: How do the authors determine the correspondence between predictions and specific ground-truth?**\n\u00a0\n\n**A4:** Thanks. For all datasets mentioned in the main text, we have converted Ground-Truth (GT) into the proposed STR format during the matching process. Hence, the matching process is conducted between the STR prediction and the STR GT to determine the correspondence between predictions and specific ground-truth. For example, given that both prediction and GT are sets in the form of Tuples $(Entity_{r_n}, Entity_{c_m}, Value_{r_n}^{c_m})$, the matching process is as follows:\n - Query whether each Tuple in GT set has a matching Tuple in prediction set.\n - When judging whether two Tuples match, calculate the similarity for Entity string and Value string according to Eqs 3 and 4 in the main text, respectively (the confusion about the order of Entity string has been claimed in the response to Q3).\n - Calculate the Intersection over Union (IoU) between prediction set and GT set according to Eq. 7 in the main text. If the calculated IoU is larger than the preset threshold, the prediction and GT are matched."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1125/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700047972233,
                "cdate": 1700047972233,
                "tmdate": 1700047972233,
                "mdate": 1700047972233,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kflAnUDedP",
                "forum": "4YgfwJBJeQ",
                "replyto": "wqs4PtBDTx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1125/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1125/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer mxZb: Part 2/3"
                    },
                    "comment": {
                        "value": "**Q5: In Table 2, why the authors have not included a comparison with Matcha and Deplot on the Chart2Text dataset.**\n\n**A5:**\nWe thank the reviewer very much for pointing this out. The chart images in Chart2Text dataset contain complex background information from the website. As a result, in Table 2,  employing the Matcha and Deplot can only achieve very low perception performance (almost close to 0) for Chart-oriented Information Extractor (CIE) task, due to the complex chart background. \n\nFor example, on perception task, Matcha can only output the content in the form of HTML format, while Deplot often outputs noisy backgrounds (e.g. website page environment containing other irrelevant text).  In contrast, the proposed Structchart can extract accurate information of the chart from complicated background, such as information from the website. Specifically, we have illustrated visualization comparisons among Matcha, Deplot, and our StructChart for CIE task on Chart2Text dataset in **Figure 6,7,8 of Appendix F**.\n\u00a0\n\n&ensp;\n\n\n**Q6: In Table 2, the comparisons between StructChart and the compared methods (Matcha and Deplot) somewhat is unfair, since StructChart leverages powerful GPT-3.5 as the reasoning model, whereas Matcha is based on Pix2Struct and Deplot relies on Codex or GPT-3.**\n\u00a0\n\n**A6:** We thank the reviewer very much for pointing this out. First, we would like to emphasize that In Table 2, we only report the results of chart perception task using our proposed SCRM metric. For perception part, all comparisons with Matcha and Deplot are fair, since we directly evaluate the well-trained Matcha and Deplot on the ChartQA, PlotQA, and Chart2Text, where all evaluation settings are **consistent** with the proposed StructChart.\n\nFurthermore, for reasoning part, Matcha is an end-to-end model (without the help of GPT-3 or 3.5), and we cannot modify its reasoning module (including network design and language model selection) during the evaluation process. As a result, we only choose to directly compare with Matcha. Experimental results show that our StructChart has a 1.1% lead over Matcha in average accuracy. Besides, according to the Reviewer's comment, to make a fair comparison, we supplement experiments of employing Deplot+GPT-3.5 to perform the QA task on ChartQA dataset, where we all use GPT-3.5 and ensure that the model settings (prompts, temperature, top-k, etc.) are consistent. The results in the following Table show that, our StructChart+GPT-3.5 outperforms Deplot+GPT-3.5 by 12.4% for QA task on ChartQA. Besides, It should be noted that there is no extra prompt engineering strategy (Chain-of-Thought, Program-of-Thought) nor evaluation tactics (self-consistency) during the reasoning process. \n\n\n|\u00a0\u00a0Input Representations\u00a0|         Model\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0| Train Set\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0| aug.\u00a0\u00a0| human | avg.\u00a0\u00a0|\n|:-----:|--------------|:---------------------:|:------:|:-------:|:------:|\n|\u00a0\u00a0\\\u00a0\u00a0\u00a0| Matcha\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0| ChartQA+PlotQA+C.D. | 90.2 | 38.2\u00a0\u00a0| 64.2 |\n|\u00a0\u00a0\\\u00a0\u00a0| Deplot+GPT-3.5\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0| ChartQA+PlotQA+C.D. | 69.3 | 36.6\u00a0\u00a0| 52.9 |\n| LCT | StructChart+GPT-3.5 | ChartQA+SimChart9k\u00a0\u00a0| 71.3 | 41.2\u00a0\u00a0| 56.3 |\n| STR | StructChart+GPT-3.5 | ChartQA+SimChart9k\u00a0\u00a0| **83.9** | **46.7**\u00a0\u00a0| **65.3** |\n\u00a0\n\nIn the revised manuscripts, we have supplemented the corresponding experimental results and empirical analyses of Deplot+GPT3.5 in Table 6 of Appendix C on Page 15."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1125/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700048767975,
                "cdate": 1700048767975,
                "tmdate": 1700676883941,
                "mdate": 1700676883941,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zbSJMaDUgD",
                "forum": "4YgfwJBJeQ",
                "replyto": "wqs4PtBDTx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1125/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1125/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer mxZb: Part 3/3"
                    },
                    "comment": {
                        "value": "**Q7: In Table 4, the authors have omitted a direct comparison with Deplot, which outperforms StructChart with a higher score (i.e., 76.7 compared to 65.3).**\n\n**A7:** Thanks for this valuable comment. We would like to emphasize: 1) Deplot uses closed-source dataset for training. In contrast, our training set including ChartQA and SimChart9K is open-source; 2) Our simulated data, SimChart9k, is scalable and we have verified through experiments that the more simulation data, the better the performance of perception stage in Table 3 of the main text.\n\nBesides, the reason why StructChart has no advantage compared with Deplot (i.e., 76.7 compared to 65.3) can be concluded:\n- During inference, Deplot applies Chain-of-Thought (CoT) or Program-of-Thought (PoT) framework as prompt engineering to help the reasoning module boost the performance, but we did not employ any prompt engineering techniques. Since we cannot access specific prompt designs in Deplot, the fairness of result comparison on QA task cannot be guaranteed.\n- For evaluation, Deplot adopts Self-Consistency (SC) strategy (generate multiple reasoning paths and answers, and finally select the one with the most frequent answers as the final answer output). However, to ensure consistent output results given the same input condition, we set the temperature parameter of GPT-3.5 as 0.\n\nFinally, for fair comparison, we evaluate StructChart and Deplot with the same reasoning model (GPT-3.5) setting without any prompt engineering strategies nor evaluation tactics. As reported in the following Table, our StructChart has a 12.4% lead over Deplot for QA task on ChartQA.\n\n\n|\u00a0\u00a0Input Representations\u00a0\u00a0\u00a0| Model\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0| Train Set\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0| aug.\u00a0\u00a0| human | avg.\u00a0\u00a0|\n|:-----:|--------------------|:---------------------:|:------:|:-------:|:------:|\n|\u00a0\u00a0\u00a0\\\u00a0\u00a0| Deplot+GPT-3.5\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0| ChartQA+PlotQA+C.D. | 69.3 | 36.6\u00a0\u00a0| 52.9 |\n| STR | StructChart+GPT-3.5 | ChartQA+SimChart9k\u00a0\u00a0| **83.9** | **46.7**\u00a0\u00a0| **65.3** |\n\n&ensp;\n\n**Q8: Confusion about 'merging' in Table 2**\n\n**A8:** Thanks for this valuable comment. Merging-set in Table 2 refers to that, training samples are merged from the real datasets, including ChartQ, PlotQA, and Chart2Text. \nThe purpose of such a merging-set setting is : 1) to better understand the performance scalability given more training data from real-world domain rather than simulation domain, and 2) to provide a baseline that is trained from real-world domain, and such baseline can be used for comparing with that trained from our proposed SimChart9K for chart perception task. \n\n\n| Val Set | Model       | Train Set          | mprecision (strict) |  mprecision (slight)      |   mprecision (high)     |\n|:-------:|:-----------:|-----------------|:-------:|:--------:|:--------:|\n| ChartQA | StructChart | Merging            | 0.7017     | 0.8227 | 0.8591 |\n| ChartQA | StructChart | ChartQA+SimChart9K | 0.7116    | 0.8182 | 0.8527 |\n\n\nResults from Table 2 in the main text demonstrate that, StructChart continuously improves the perception performance of chart data on each domain given more training samples.\n\nAccording to the reivewer's comment, we have supplemented some descriptions of the merging setting in the caption of Table 2 in the revised manuscript, to avoid confusion."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1125/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700049348848,
                "cdate": 1700049348848,
                "tmdate": 1700049348848,
                "mdate": 1700049348848,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AZpq49Wu7p",
                "forum": "4YgfwJBJeQ",
                "replyto": "wqs4PtBDTx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1125/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1125/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Look forward to further discussion."
                    },
                    "comment": {
                        "value": "Dear Reviewer mxZb,\n\nWe have made a rapid response to your concerns, and hope that our response can address your concerns. As the deadline for discussion is approaching, we would greatly appreciate it if you could let us know if there are any other questions or issues regarding the paper or our response. We are looking forward to further discussion.\n\nBest regards,\n\nAuthors of Submission 1125"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1125/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700466655291,
                "cdate": 1700466655291,
                "tmdate": 1700466655291,
                "mdate": 1700466655291,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]