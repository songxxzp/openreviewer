[
    {
        "title": "In-context Convergence of Transformers"
    },
    {
        "review": {
            "id": "fvHvKtIWI9",
            "forum": "kxpswbhr1r",
            "replyto": "kxpswbhr1r",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6735/Reviewer_xLBH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6735/Reviewer_xLBH"
            ],
            "content": {
                "summary": {
                    "value": "This is a theoretical paper centers around addressing \u201cHow do softmax-based transformers trained via gradient descent learn in-context? \u201c. In particular, the author(s) investigated the training dynamics of a one-layer transformer with softmax attention trained by GD for in-context learning. Previous studies only focused on linear transformers i.e. without softmax function.  The authors give convergence results regarding in context learning with balanced and nonbalanced features."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "Originality: This is a very novel paper as the authors provide a theoretical aspect of the in context learning dynamics of nonlinear transformer signified by the softmax activation in the self attention layer/module. This has not been examined before. \n\nQuality: the quality is high. The authors define the problem mathematically and details around the proofs are included. They also provide an interesting aspect of the learning dynamics via the relevant attention scores during the 4 phases of convergence. \n\nClarity: It is fairly clear as the mathematical notations are given prior to introduction of their main development. The presentation can be easily followed via definition of problems, proposed approach/theoretical establishment, and training phase analysis for two specific problems of linear regression where sampled features are balanced and nonbalanced. \n\nSignificance: the paper is important for ml theory as it discussed in context learning with \u201cnonlinear\u201d (softmax) transformers."
                },
                "weaknesses": {
                    "value": "The major weakness is lack of empirical experiments. The linear regression setting according to task distribution and data distribution should not be too challenging to do synthetic experiments for balanced and nonbalanced features similar to Garg et al. or Zhang et al., 2023a. It will be interesting to compared with linear transformers such as Zhang et al. 2023 where in certain task scenarios, linear transformer fails. \nAnother concern is the nonlinearity of transformers are examined on features from linear functions. Would be possible to develop some insights on features from nonlinear functions?"
                },
                "questions": {
                    "value": "1.\tThe paper does not introduce/define \u0398(K) in the notations. \n2.\t\u201cset of distinct features {vk \u2208Rd,k=1,...,K},where all features are orthonormal vectors\u201d, does that mean the maximum K is d, since an orthonormal basis is d dimensional?  Is it too limiting? Correct me if I am wrong, does that also mean we can only have d distinct features/tokens as in part of Definition 3.1 part 2? \n3.\tThe paper adopts W^V and W^{KQ} from linear SA by Zhang et al.  for example by setting v =1. Although the authors justify the use of v=1 with two reasons.  However, the objective from Zhang et al.  involves a residual connection term. In this paper, there is no residual connection term involved.  Could the authors justify either the use of v =1 as a scaling factor in their objective as in Equation (1) or not including the residual term in Equation (1) and subsequently in equation (3) and (4)? \n4.\tThe four training phases of under-represented features differ. Could the order of phase 2 and 3 be swapped? \n5.\tDid author investigate other tasks with softmax transformer as discussed by the Garg et al. paper?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6735/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6735/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6735/Reviewer_xLBH"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6735/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698430776769,
            "cdate": 1698430776769,
            "tmdate": 1699636774887,
            "mdate": 1699636774887,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "skr4l9Ur1u",
                "forum": "kxpswbhr1r",
                "replyto": "fvHvKtIWI9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6735/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6735/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xLBH (part 1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for providing the helpful review! We have addressed the reviewer\u2019s helpful comments and revised the paper accordingly. The changes are marked in bright blue color in our revision. Please feel free to check and comment.\n\n**Q1:** The major weakness is lack of empirical experiments. The linear regression setting according to task distribution and data distribution should not be too challenging to do synthetic experiments for balanced and nonbalanced features similar to Garg et al. or Zhang et al., 2023a. It will be interesting to compared with linear transformers such as Zhang et al. 2023 where in certain task scenarios, linear transformer fails. Another concern is the nonlinearity of transformers are examined on features from linear functions. Would be possible to develop some insights on features from nonlinear functions?\n\n**A1:** Many thanks for your suggestions! In Appendix A of our revised paper, we have provided experiment results to validate our theoretical results. We summarize our key results as follows: \n(1) Figure 2 depicts the prediction error for each feature versus the training epochs, and highlights the stage-wise convergence process between the dominant and under-represented features for imbalanced case.\n(2) Figure 3 illustrates the evolution of attention score heatmap during the training, and verifies our theoretical results on attention score concentration and the multi-phase characterization of training dynamics. Please feel free to check and comment. Our current experiment code has adopted the existing well-packed transformer package, which does not allow change to linear transformer easily. Given this, our handcrafted code on linear transformer may not provide a fair comparison between the two. Note that although Zhang et al. (2023) studied linear attention layers in their theory, their experiments are also on more complex, large, and nonlinear transformers, which do not provide a code on linear transformers either. \n\nRegarding features from nonlinear functions, this is definitely an interesting question to ask. It also appears to be a challenging problem to analyze from theory side, at least not something straightforward. Our preliminary thoughts along this direction are as follows. To consider such nonlinear tasks/functions, (1) some additional layers may be required to learn the non-linear functions; and (2) the softmax weight matrix structure will change and may not admit any simplified zero entries, which is different from the current structure of linear tasks as in Eq. (2). \n\n**Q2:** The paper does not introduce/define $\\Theta(K)$ in the notations. \n\n**A2:** Thanks. In our revision, we have formally defined $\\Theta(K)$ in the notation section as suggested.\n\n**Q3:** The paper adopts $W^V$ and $W^{KQ}$ from linear SA by Zhang et al. for example by setting $v =1$. Although the authors justify the use of $v=1$ with two reasons. However, the objective from Zhang et al. involves a residual connection term. In this paper, there is no residual connection term involved. Could the authors justify either the use of v =1 as a scaling factor in their objective as in Equation (1) or not including the residual term in Equation (1) and subsequently in equation (3) and (4)?\n\n**A3:** The residual term in Zhang et al. can also be removed. Both Zhang et al. and our paper choose the loss function to only measure the distance between $y_{query}$ and $\\hat{y}_{query}$. The residual term, which is associated with the prediction output $\\hat{y}$, equals 0. (See Eq. (3.3) and Eq (3.4)). As a result, the residual term can be removed without any influence on the loss and training dynamics. Furthermore, as we discuss in **Remark 1** in our paper, our current specific parameterization (including setting $v=1$) does not lose optimality up to an exponentially small error."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6735/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700336384508,
                "cdate": 1700336384508,
                "tmdate": 1700336384508,
                "mdate": 1700336384508,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hG5T9m68CI",
                "forum": "kxpswbhr1r",
                "replyto": "fvHvKtIWI9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6735/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6735/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow Up Reminder to Reviewer xLBH"
                    },
                    "comment": {
                        "value": "Dear Reviewer xLBH,\n\nAs the author-reviewer discussion period ends soon, we will appreciate it if you could check our response to your review comments. This way, if you have further questions and comments, we can still reply before the author-reviewer discussion period ends. If our response resolves your concerns, we kindly ask you to consider raising the rating of our work. Thank you very much for your time and efforts!"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6735/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700618903403,
                "cdate": 1700618903403,
                "tmdate": 1700618903403,
                "mdate": 1700618903403,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "l8u75rCsRR",
                "forum": "kxpswbhr1r",
                "replyto": "hG5T9m68CI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6735/Reviewer_xLBH"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6735/Reviewer_xLBH"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response."
                    },
                    "comment": {
                        "value": "Thank you for your responses and clarification. I will consider your suggestion/request."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6735/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700624246867,
                "cdate": 1700624246867,
                "tmdate": 1700624246867,
                "mdate": 1700624246867,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "y2DOweIgUq",
            "forum": "kxpswbhr1r",
            "replyto": "kxpswbhr1r",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6735/Reviewer_kkxb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6735/Reviewer_kkxb"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the convergence of one-layer encoder-based transformer with softmax attention only. The authors show that training this model with gradient descent with pairs $(x_i,y_i)$, where $x_i$ can be one of $K$ possible vectors $u_k$, and $y_i = wx_i$ for some $w$ that is drawn from some distribution. Two settings are considered: in the first one each of the possible $u_k$ vectors has  probability $\\propto 1/K$ to be drawn (balanced data), while in the second one some vector has some vector being drawn with constant probability and the rest with probability $\\propto 1/K$. In both settings they show that the transformer converges to an approximate minimum."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The convergence of transformer models is of interest to the community. The paper also studies sequence to sequence models, which is a setting unexplored in terms of convergence. The two-phase transition is intuitive, since the model is first trying to maximize the inner product of the token to be predicted with the ones that are the same vectors and then use them to extract their corresponding labels."
                },
                "weaknesses": {
                    "value": "The main weakness of this paper is the setting. Specifically, even though the authors study sequence to sequence models, they also consider:\n1. That all training points are one of k possible vectors. This is not compatible with the training process for linear regression. In general, in which setting we only have $K$ vectors that we sample them in that way.\n2. The weight matrices are very constrained and sparsified to match the ones proposed in the construction of [2]. This construction also refers to linear transformers. The authors are multiply the matrices $W_K^\\top W_Q$ and train them as one matrix (this also impacts the training dynamics). \n3. The paper also consider only the attention mechanism and no residual. \n\nI understand that the achieved loss is not very different, however this simply indicates that for the simple setting of linear regression transformers are more expressive than necessary. I do not think that these results are indicative of how the training is evolved."
                },
                "questions": {
                    "value": "1. Could the authors clarify the motivation of this setting? Fixed data points and constraint sparse matrices while considering the task of linear regression. \n2. Could the authors comment on how the two-phase transition observed in this setting, could be indicative of what is happening during actual training of these models?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6735/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6735/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6735/Reviewer_kkxb"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6735/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698889971209,
            "cdate": 1698889971209,
            "tmdate": 1700716836722,
            "mdate": 1700716836722,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "x4fTaZMMDO",
                "forum": "kxpswbhr1r",
                "replyto": "y2DOweIgUq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6735/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6735/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer kkxb (part 1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for providing the helpful review! We have addressed the reviewer\u2019s helpful comments and revised the paper accordingly. The changes are marked in bright blue color in our revision. Please feel free to check our revised paper and comment.\n\n**Q1:** That all training points are one of $K$ possible vectors. This is not compatible with the training process for linear regression. In general, in which setting we only have $K$ vectors that we sample them in that way.\n \n**A1:** \nOur data assumption, where each data point is randomly selected from a predefined set of features, is **standard** and **well accepted** in many recent theoretical work, for example, [3] on theoretical deep learning, [4] on transformer theory, and [5] on vision transformer theory. \nSuch a model captures the case where data are sampled from a subspace where those $K$ features serve as basis vectors. Our analysis can be extended to study such a case by allowing the data sample to be correlated. Thus, our assumption indeed aligns with \nthe typical linear regression setting, where the feature vectors are representative of a broader spectrum of data points typically encountered in linear regression scenarios.\n\n**Q2:** The weight matrices are very constrained and sparsified to match the ones proposed in the construction of [2]. This construction also refers to linear transformers. The authors are multiply the matrices $W_K^\\top W_Q$  and train them as one matrix (this also impacts the training dynamics).\n\n**A2:** We respectfully disagree with the above comments. \n\n(i) Although our weight matrices adopt the construction in [2], our model is **NOT linear** transformer, but a **softmax nonlinear** transformer. This is the key difference of our study from that in [2]. Our main contribution lies in characterizing the training dynamics for the more practical **softmax** based transformer, which is more challenging to analyze than **linear** transformer. \n\n(ii) In our paper, we have shown that specializing our weight matrix to the construction in [2] **does not lose optimality** up to an exponential-delay error. Hence, such a choice of weight matrix should not be considered as \"very constrained\". Further, only one column and one row of our weight matrices, as detailed in Eq.(2), have zero entries, which are not sparse either.\n\n(iii) Training $W_K^\\top W_Q$ as one matrix has been adopted in several recent theoretical studies of transformers, for example in [1,2]. \n\n**Q3:** The paper also consider only the attention mechanism and no residual.\n\n**A3:** The residual term can be removed without any influence\non the training dynamics. Please see more detailed discussions in A3 in our response to Reviewer xLBH. \n\n**Q4:** Could the authors clarify the motivation of this setting? Fixed data points and constraint sparse matrices while considering the task of linear regression.\n\n**A4:** Our motivation is to understand the learning dynamics of non-linear, softmax-based attention structures trained via gradient descent under in-context learning, which is very different from previous work studying linear attention. The softmax-based attention plays a crucial role in the success of transformers and has not been investigated. To this end, we have introduced some reparameterization and simplifications, which are standard in theoretical work and do not hurt the essence of the problem. We remark here, (1) as explained in A2, our weight matrix is not sparse and does not lose optimality; (2) as explained in A1, the distinct feature assumption is standard to simplify analysis and can be generalized; (3) [2] also adopts the task of linear regression and similar reparameterization, where the only difference is that they study **linear** attention model, whereas we focus on practically more popular **softmax** attention model."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6735/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700335712698,
                "cdate": 1700335712698,
                "tmdate": 1700335712698,
                "mdate": 1700335712698,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Yi0xFHF99J",
                "forum": "kxpswbhr1r",
                "replyto": "y2DOweIgUq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6735/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6735/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow Up Reminder to Reviewer kkxb"
                    },
                    "comment": {
                        "value": "Dear Reviewer kkxb,\n\nAs the author-reviewer discussion period ends soon, we will appreciate it if you could check our response to your review comments. This way, if you have further questions and comments, we can still reply before the author-reviewer discussion period ends. If our response resolves your concerns, we kindly ask you to consider raising the rating of our work. Thank you very much for your time and efforts!"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6735/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700618862752,
                "cdate": 1700618862752,
                "tmdate": 1700618862752,
                "mdate": 1700618862752,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZSC8SqtoUd",
                "forum": "kxpswbhr1r",
                "replyto": "Yi0xFHF99J",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6735/Reviewer_kkxb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6735/Reviewer_kkxb"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "I would like to thank the authors for their responses and for providing experimental evidence of their conclusions. I have raised my score accordingly.\n\nI would also like to mention that  \"The weight matrices are very constrained and sparsified to match the ones proposed in the construction of [2]. This construction also refers to linear transformers.\" This construction refers to the construction of [2], not in your paper. Also when I mentioned that the matrices are sparsified I meant that by setting them as the ones proposed in the construction of [2], they are sparse since they have whole blocks to be zero. I still consider this a limitation of this to be the main limitation of the paper."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6735/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700716814583,
                "cdate": 1700716814583,
                "tmdate": 1700716814583,
                "mdate": 1700716814583,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "OjlRqKZrFN",
            "forum": "kxpswbhr1r",
            "replyto": "kxpswbhr1r",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6735/Reviewer_AAX6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6735/Reviewer_AAX6"
            ],
            "content": {
                "summary": {
                    "value": "The paper delves into the behavior of transformer architectures, particularly focusing on their convergence properties in various contexts. Transformers, known for their self-attention mechanism, have gained immense popularity in NLP and other domains. The paper's primary objective is to investigate the factors affecting the convergence speed and stability of transformers when trained in different contexts.\n\nKey contributions and discussions of the paper include:\n\n1. An exploration of how the context (e.g., input data distribution, task complexity) impacts the convergence properties of transformers.\n2. The introduction of a novel metric to quantify and measure the convergence speed and stability of transformers.\n3. A series of experiments that highlight the varying convergence behaviors of transformers across different tasks and datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Given the widespread use of transformers in various domains, understanding their convergence properties is of paramount importance. This paper addresses this gap by exploring the role of context in transformer convergence.\n2. The paper provides a series of well-designed experiments that shed light on how transformers behave across tasks, datasets, and other contextual factors. This empirical evidence strengthens the paper's claims and findings.\n3. The paper is well-structured, with clear explanations and visualizations, making it accessible to both experts and those less familiar with transformer architectures."
                },
                "weaknesses": {
                    "value": "1. While the paper does a good job of examining transformers across various tasks and datasets, there's a limitation in the breadth of architectures studied. Delving into different variants of transformers or comparing with other architectures could have provided a more comprehensive picture.\n\n2. While the introduced metric is innovative, there could be more rigorous validation or comparison against other potential metrics. This would strengthen the metric's claim as a standard measure for convergence properties.\n\n3. The paper could benefit from a discussion on the practical implications of the findings. For instance, how can the insights on convergence be used to improve training methodologies or model selection in real-world applications?\n\n4. A deeper exploration into the individual factors affecting convergence (e.g., model size, training data size, task complexity) could provide a granular understanding and more actionable insights.\n\n5. This paper contains several typographical errors, which detract from the reading experience. For example, there is a missing space before \"We\" in the 9th line of the summary."
                },
                "questions": {
                    "value": "See Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6735/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698891379844,
            "cdate": 1698891379844,
            "tmdate": 1699636774636,
            "mdate": 1699636774636,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "bWBRq6Pr3Q",
            "forum": "kxpswbhr1r",
            "replyto": "kxpswbhr1r",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6735/Reviewer_VMhr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6735/Reviewer_VMhr"
            ],
            "content": {
                "summary": {
                    "value": "This study presents training dynamics of a one-layer transformer with softmax attention, focusing on in-context learning via gradient descent (GD). Two scenarios, one with balanced features and the other with imbalanced features, are analyzed. The authors show convergence towards a diminishing in-context prediction error by exploring the evolution of attention dynamics in both settings. \n\nIn the case of imbalanced features, a multi-phase behavior is characterized, shedding light on the interplay of attention dynamics between dominant and under-represented target features during training. Overall this is a nice analysis of softmax attention and gradient descent dynamics in the context of in-context learning."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper studies training dynamics and convergence of gradient descent for training a single layer transformer model with softmax attention to do in-context learning. \n\n2. It provides convergence guarantees for both balanced and imbalanced feature settings, showcasing the effectiveness of transformers and gradient descent for in-context learning. The results break down training into two phases for balanced features and four phases for imbalanced features. This provides valuable insights into how transformers adapt during training. The paper shows that transformers quickly achieve near-zero prediction error for dominant features and eventually converge to near-zero prediction error for under-represented features, regardless of their infrequent occurrence.\n\n3. The paper introduces a novel proof technique that characterizes softmax attention dynamics by considering the interplay between two types of bilinear attention weights: 'weight of query token and its target feature' and 'weight of query token and off-target features.' The dynamic shift in dominance between these weights throughout the learning process leads to different training phases. This could be applied to other problems involving transformer architectures."
                },
                "weaknesses": {
                    "value": "Weaknesses and questions,\n\n1. Are the phases artifacts of the analysis or is it really the case? Could you provide some simulations that demonstrate this multi-phase convergence of gradient descent on in-context learning with transformers? \n\n2. The results and analysis seem to abstract away the task diversity (different $w$ vectors). In-context learning is about generalizing to unseen tasks and it looks like the analysis is based on doing gradient descent on $L(\\theta)$ which is the expectation over the task vectors $w$ and the features vectors $x$. The setup for in-context learning in Garg et al. 2022 (Figure 1) is more pragmatic and I was expecting an analysis on this. This analysis studies the convergence of gradient descent in minimizing $L(\\theta)$ with transformer/self-attention as an underlying model. It's a nice study, given the additional complexity introduced due to softmax, but I think it is not exactly studying in-context learning instead it's a study of convergence of gradient descent on regression with one-layer transformers. The title also seems confusing, it suggests that the paper is about studying the convergence of transformers to the right output during inference time. \n\n\n3. Could you simplify the presentation and expand on the main intuition behind the multi-phase convergence? Especially after page 7, instead of the lemmas some simulations/illustrations could be very helpful in understanding the key ideas behind the multi-phase convergence in balanced and imbalanced settings."
                },
                "questions": {
                    "value": "See above and I have one more question.\n1. Why do you need the features to be orthonormal vectors and be drawn from a finite set? Does the analysis depend heavily on this assumption? \n2. Could you make the dimensions of the main terms clear (e.g. in Definition 3.1) o.w. the readers have to work them out.\n3. Minor, the upper case $K$ is used for $W^{K}$ and also for the number of feature vectors. \n4. The bounds on $T^*$ have quadratic dependence on $K$ (the number of distinct feature vectors). Is this order of dependence necessary? What would happen if the features are drawn from a continuous distribution say multi-variate gaussian?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6735/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699216255268,
            "cdate": 1699216255268,
            "tmdate": 1699636774497,
            "mdate": 1699636774497,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0eeERiLTJG",
                "forum": "kxpswbhr1r",
                "replyto": "bWBRq6Pr3Q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6735/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6735/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer VMhr (part 1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for providing the helpful review! We have addressed the reviewer\u2019s helpful comments and revised the paper accordingly. The changes are marked in bright blue color in our revision. Please feel free to check and comment.\n\n**Q1:** Are the phases artifacts of the analysis or is it really the case? Could you provide some simulations that demonstrate this multi-phase convergence of gradient descent on in-context learning with transformers?\n\n**A1:** \nThe division of different phases captures actual transitions of attention score when applying gradient descent to optimize the one-layer transformer. In Appendix A of our revision, we have provided numerical results that validate the multiple phases and other aspects of our theoretical results. The key findings are summarized as follows:\n\n(1) The prediction error for each feature versus the training epochs is shown in Figure 2, which highlights that the training process has stage-wise convergence between the dominant and under-represented features for imbalanced case.\n\n(2) The evolution of attention score heatmap during the training is presented in Figure 3, which demonstrates that the concentration of  attention score indeed follows multiple phases during the training by gradient descent.\n\nPlease feel free to check and comment.\n\n**Q2:** The results and analysis seem to abstract away the task diversity (different $w$ vectors). In-context learning is about generalizing to unseen tasks and it looks like the analysis is based on doing gradient descent on $L(\\theta)$ which is the expectation over the task vectors $w$ and the features vectors $x$. The setup for in-context learning in Garg et al. 2022 (Figure 1) is more pragmatic and I was expecting an analysis on this. This analysis studies the convergence of gradient descent in minimizing $L(\\theta)$ with transformer/self-attention as an underlying model. It's a nice study, given the additional complexity introduced due to softmax, but I think it is not exactly studying in-context learning instead it's a study of convergence of gradient descent on regression with one-layer transformers. The title also seems confusing, it suggests that the paper is about studying the convergence of transformers to the right output during inference time.\n\n**A2:** \nWe clarify that we indeed study an in-context learning setup. (1) Our transformer setup and loss functions described in Sections 2.2 and 2.3 are exactly the same as the in-context learning setup in the recent studies on in-context learning [1-3].  In particular, the transformer is set up to learn the attention of the query to the tokens in the prompt of the same task. (2) In the last paragraph of Section 3.1 on Page 5, we illustrate the in-context learning ability of **unseen** tasks. We show that even if the weight $w$ is selected from a different set from the training task space (and is hence an **unseen** task), \nthe in-context learned model that we characterize in Theorem 3.1 can still well predict the function value for the query in the test prompt.\n\n**Q3:** Could you simplify the presentation and expand on the main intuition behind the multi-phase convergence? Especially after page 7, instead of the lemmas some simulations/illustrations could be very helpful in understanding the key ideas behind the multi-phase convergence in balanced and imbalanced settings.\n\n**A3:** Many thanks for your suggestions! In Appendix A in our revision, we have provided experiment results to illustrate and validate the multi-phase convergence. We will move this part to the main body of the paper in our final version.\n\n**Q4:** Why do you need the features to be orthonormal vectors and be drawn from a finite set? Does the analysis depend heavily on this assumption?\n\n**A4:** Many thanks for the comments! These assumptions are made to simplify the analysis, so that our characterization of the convergence and the dynamics of the attentions scores won't be over-complicated by non-essential aspects. Following the reviewer's comment, we made an initial exploration of the case where the features are drawn from a subspace with $K$ features serving as basis vectors. This setting allows the features to be generally correlated. Our techniques can be extended to such a setting with further characterization of how correlation among features affects attention coefficients. Also note that in such a setting, we don't assume the set of features is finite; features can be sampled from an infinite set, although the set contains $K$ basis vectors. \n\n**Q5:** Could you make the dimensions of the main terms clear (e.g. in Definition 3.1) o.w. the readers have to work them out. Minor, the upper case $K$\n is used for $W^K$ and also for the number of feature vectors.\n\n**A5:** Thanks for your suggestions. We have revised them and made them clear accordingly in our revision."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6735/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700335268106,
                "cdate": 1700335268106,
                "tmdate": 1700335268106,
                "mdate": 1700335268106,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nLHf5ywnLH",
                "forum": "kxpswbhr1r",
                "replyto": "bWBRq6Pr3Q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6735/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6735/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow Up Reminder to Reviewer VMhr"
                    },
                    "comment": {
                        "value": "Dear Reviewer VMhr,\n \nAs the author-reviewer discussion period ends soon, we will appreciate it if you could check our response to your review comments. This way, if you have further questions and comments, we can still reply before the author-reviewer discussion period ends. If our response resolves your concerns, we kindly ask you to consider raising the rating of our work. Thank you very much for your time and efforts!"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6735/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700618803817,
                "cdate": 1700618803817,
                "tmdate": 1700618803817,
                "mdate": 1700618803817,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VjL54qipJe",
                "forum": "kxpswbhr1r",
                "replyto": "nLHf5ywnLH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6735/Reviewer_VMhr"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6735/Reviewer_VMhr"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. I do not have further questions."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6735/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700715838744,
                "cdate": 1700715838744,
                "tmdate": 1700715838744,
                "mdate": 1700715838744,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]