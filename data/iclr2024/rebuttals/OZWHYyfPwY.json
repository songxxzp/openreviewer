[
    {
        "title": "Don't trust your eyes: on the (un)reliability of feature visualizations"
    },
    {
        "review": {
            "id": "XPinuaHwU6",
            "forum": "OZWHYyfPwY",
            "replyto": "OZWHYyfPwY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5953/Reviewer_o92S"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5953/Reviewer_o92S"
            ],
            "content": {
                "summary": {
                    "value": "This paper questions the reliability of feature visualizations, a common tool used to understand individual features/neurons of a network. They present three lines of evidence against trusting feature visualizations:\n1. Adversarial - by developing a new \"adversarial attack\" to fool feature visualizations\n2. Empirical - by showing feature visualizations are processed differently than natural images by the network\n3. Theoretical - by showing we can't know much about a neuron based on feature visualization alone, even under strong assumptions"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "Clearly written, good motivation and discussion of related work. Highlights an important problem, and the two attack strategies presented is both simple and effective."
                },
                "weaknesses": {
                    "value": "I am not fully convinced by section 3 and the path perspective, I think it is plausible a feature visualization could be informative and useful even if it looks different from natural images, and a visualization of late layer features does not need to be processed similarly to natural images at least on early layers."
                },
                "questions": {
                    "value": "While this paper only focuses on feature visualization, it seems like these issues reflect a bigger issue with using a low number of highly activating examples to describe a unit. Do you think this work should also raise concerns against using let's say 5 most highly activating natural images to describe a neuron, as this is a common alternative to using feature visualizations?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5953/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698632096526,
            "cdate": 1698632096526,
            "tmdate": 1699636635084,
            "mdate": 1699636635084,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nN1eo6YFET",
                "forum": "OZWHYyfPwY",
                "replyto": "XPinuaHwU6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5953/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5953/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response"
                    },
                    "comment": {
                        "value": "Dear reviewer o92S,\n\nThank you for your review, we\u2019re glad to hear that you found the paper **\u201cclearly written\u201d**, **\u201chighlighting an important problem\u201d** with **\u201cexcellent presentation & contribution\u201d**.\n\n**Point 1** *\u201ca visualization of late layer features does not need to be processed similarly to natural images at least on early layers\u201d*:\n\nWe fully agree that differences in early layers wouldn\u2019t be problematic. As an example, it would be totally fine for an \u201cairplane\u201d visualization to have different color statistics or low-level features compared to natural airplane images. It would only be problematic if the \u201cairplane\u201d visualization lacks mid- and high-level features that belong to airplanes. Our Figure 6 didn\u2019t really allow a reader to assess which layers correspond to low-, mid-, and high-level features. In the light of your comment, we have therefore updated Figure 6 with annotations that show not only the name of the layer (previously we used just an uninformative index) but also an annotation copied from https://distill.pub/2017/feature-visualization/appendix/ that describes layers as responding to \u201ctextures\u201d, \u201cobject parts\u201d, \u201csophisticated concepts\u201d etc.\nGiven this annotation, it becomes clear that feature visualizations are processed differently from natural images not only in low-level layers but also in layers corresponding to object parts, complex and sophisticated concepts - precisely the kinds of features that the visualizations were designed to \u2018explain\u2019 and visualize. If a feature visualization wants to succeed in explaining how natural input is processed, it needs to better capture those features. We have added the following to Section 3: *\u201cWhile it would be fine for a feature visualization to show different low-level features compared to natural images, any visualization that seeks to explain how natural input is processed should capture similarities in mid- and high-level layers that are described by Olah et al. (2017) as responding to \u2018object parts\u2019 and \u2018complex/sophisticated concepts\u2019.\u201d*\n\n**Point 2** *\u201cDo you think this work should also raise concerns against using let's say 5 most highly activating natural images to describe a neuron\u201d:*\n\nThat\u2019s an interesting question! Our paper covers three perspectives and we thus have three answers to this question.\n\nFrom the *adversarial perspective*, we could easily use our method from Section 2.2 to build a network where the top k natural images don\u2019t correspond to what the unit is usually selective for; this would simply mean setting the bias parameter b to a lower value such that it would only suppress activations up to, say, the 95th percentile of natural input. A recent paper specifically looked into manipulating the top-k activating images; we cited it in the related work section as \u201cA complementary approach is proposed by Nanfack et al. (2023), which changes highly activating dataset samples\u201d (https://arxiv.org/pdf/2306.07397.pdf).\n\nFrom the *empirical perspective*, highly activating natural images would pass the sanity check since highly activating natural images are, by definition, natural images that highly activate a unit, and they would thus be processed like other natural images.\n\nFrom the *theoretical perspective*, our impossibility results can be extended to many variations on using the argmax for feature visualization, including using the most activating dataset samples as explanations. Essentially, as long as the feature visualization method does not narrow down the function space too much, our results will apply. It is easy to see that two simple (e.g., piecewise linear) functions could have the same 5 (or 10, etc.) local (arg)maxima and yet behave very differently even quite near these local maxima, and hence our theorems could be extended.\n\nThus, in summary, highly activating natural images would pass our sanity check, but it would still be possible to construct networks that show misleading highly activating natural images, which is a case that\u2019s covered by the theory. Since this is a question we feel might interest readers as well, we have added a dedicated and detailed explanation to Appendix A.3 (\u201cRelationship to highly activating natural samples\u201d), essentially covering the points we describe above. Thanks again for the question!"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5953/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700158450764,
                "cdate": 1700158450764,
                "tmdate": 1700246570462,
                "mdate": 1700246570462,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Jxc0gVxfMq",
                "forum": "OZWHYyfPwY",
                "replyto": "nN1eo6YFET",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5953/Reviewer_o92S"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5953/Reviewer_o92S"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thanks for the interesting response! This mostly addresses my remaining questions and I retain my rating of accept."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5953/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700734172203,
                "cdate": 1700734172203,
                "tmdate": 1700734172203,
                "mdate": 1700734172203,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "uFa48Plqxb",
            "forum": "OZWHYyfPwY",
            "replyto": "OZWHYyfPwY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5953/Reviewer_MuGq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5953/Reviewer_MuGq"
            ],
            "content": {
                "summary": {
                    "value": "This manuscript studies issues of treating feature visualization as a means to explain the behavior of vision classification models. Their main contributions are 2 types of ways to generate different feature visualizations and show deep models essentially treat natural images and feature visualizations differently. The rest is a set of theoretical analysis showing what feature visualization could do."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This work has an overall good presentation of results. It designs two counterfactual examples to show the unreliability of feature visualizations and any downstream understandings based on feature visualization are also broken. Authors nicely convey the messages behind the theories and highlight the main take-ways."
                },
                "weaknesses": {
                    "value": "Lack of clear definitions on threat models. I am not entirely sure how the fool circuit works. It is mostly I don't know what the authors assume an adversary could do and couldn\u2019t do. Moreover, I don\u2019t see definitions of the unreliability of feature visualizations. Everything seems to be hand-wavey in the first empirical sections.\n\nTheories have weak connections with experiments. Although I understand the theorems are just proofs of concepts and demonstrate the boundary of understanding from feature visualizations, they lack strong connections with what you experimented in the previous sections. Are those experiments motivated by theoretical analysis? Is any take-away from the theory empirically validated by the experiments? Why suddenly the theory bounds the function using Linf norms? When I read the theories I mainly feel these results imply what could be done in the future work, as the linearity discussion mentioned by authors. However, they lack a strong connection with your empirical results shown above."
                },
                "questions": {
                    "value": "Have you experimented with adversarially robust models? A large body of work has shown robust models have better saliency explanations, better representations and thus better geometry of the output. Thus, how does the quality of the model affect the quality/reliability of feature explanations? But, before answering the last question, please define the reliability first."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5953/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5953/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5953/Reviewer_MuGq"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5953/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698775421974,
            "cdate": 1698775421974,
            "tmdate": 1700725821833,
            "mdate": 1700725821833,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xbG9OGAEHV",
                "forum": "OZWHYyfPwY",
                "replyto": "uFa48Plqxb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5953/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5953/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response (1/2)"
                    },
                    "comment": {
                        "value": "Dear reviewer MuGq,\n\nThank you for your review and questions. Please find answers to your questions below:\n\n1. *Lack of clear definitions on threat models [...] It is mostly I don't know what the authors assume an adversary could do and couldn\u2019t do.*\n\nThat\u2019s a great point, we can be more explicit about the threat model in Section 2. The \u201cadversarial perspective\u201d (Section 2) assumes that a model can be arbitrarily manipulated (e.g. by changing connectivity patterns or slightly modifying the architecture) while the interpretability technique (feature visualization) is fixed and no aspect of this technique (hyperparameter, random starting point) can be changed by a person with malicious intent. This is different to the setting in adversarial robustness, where the model is fixed and the input is manipulated. We have revised the beginning of Section 2 to include the following explanation on the threat model:\n\n\u201c... since feature visualizations have been proposed as model auditing tools (Brundage 2020) that should be integrated \u201cinto the testbeds for AI applications\u201d (Nguyen 2019), it is important to understand whether an adversary (i.e., someone with malicious intent) might be able to construct a model such that feature visualizations are manipulated. This corresponds to a threat scenario where the model itself can be arbitrarily changed while the interpretability technique (feature visualization) is kept fixed without control over hyperparameters or the random starting point. For example, a company may be interested in hiding certain aspects of its model's behavior from a third-party (e.g. regulator) audit that uses feature visualizations. In this context, our demonstration of unreliability relates to a line of work on deceiving other interpretability methods (described in detail in Appendix A.1). We do not know whether such a scenario could become realistic in the future, but as we stated above the main motivation for this section is to provide a proof of concept by showing that one can build networks that deceive feature visualizations.\u201d\n\n2. *Definition of unreliability*\n\nWe argue that a visualization method is unreliable if it is possible to modify a network such that (1) its overall behavior stays the same but (2) its visualizations are changed (into arbitrary patterns). Since this definition wasn\u2019t mentioned clearly in the text before, we have now added the following definition to Section 2 to define unreliability:\n\u201cOne important requirement for interpretability is that the explanations are reliable. We use the following definition of unreliability: A visualization method is unreliable if one can change the visualizations of a unit without changing the unit\u2019s behavior on (relevant) test data. More formally, this can be expressed as: Let $\\mathcal{U}$ denote the space of all  units. A visualization method $m$ is unreliable if $\\exists u,v \\in \\mathcal{U}: m(u) = m(v) \\land \\neg u \\overset{bhv}{\\sim} v$, where $\\overset{bhv}{\\sim}$ denotes an  equivalence class of equal behavior.\u201d\n\n3. *Theories have weak connections with experiments.*\n\nThank you for highlighting that we can increase the connection between theory and experiments. In response to your comment, we have revised the text to make the two main connections we see more clear:\n\nFirst, the theory of Section B.1 is directly related to the experiments of Section 2.1, proving that the fooling circuit will always work the way we observe in the experiments. We have added the following text to the end of Section 2.1 in the revision: \u201cIn Section B.1, we formalize this fooling circuit, and prove that it will always behave in the way we observe in our experiments.\u201d\n\nSecond, the main role of the impossibility theorems is to establish that many informal claims in the literature about the performance of feature visualization cannot be true in general. The connection to the empirical results here is to show that regardless of what fooling method is used, there is no defense that will provide general guarantees for feature visualization without very strong assumptions on the network. We have added the following text to the start of Section 4 in the revision: \u201cA natural question arising from our experiments is whether the limitations of feature visualization that we have shown experimentally are always present, or if they can be avoided in certain situations. Towards addressing this, we now ask: When are feature visualizations guaranteed to be reliable, i.e. guaranteed to produce results that can be relied upon?\u201d"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5953/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700244101284,
                "cdate": 1700244101284,
                "tmdate": 1700682519498,
                "mdate": 1700682519498,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GD8K6EwTC8",
                "forum": "OZWHYyfPwY",
                "replyto": "uFa48Plqxb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5953/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5953/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response (2/2)"
                    },
                    "comment": {
                        "value": "4. *Why does the theory bound the function using $L_{\\infty}$ norms?*\n\nThis is a great question. To provide clarity for all readers, we have added the following text to the beginning of Section 4.1 in the revision: \u201cThroughout, we measure the accuracy of predicting f using the $L_{\\infty}$ norm. This is primarily for convenience; the equivalence of $L_p$ norms on bounded, finite-dimensional spaces implies we could prove the same results with $L_p$ for any $p$ at the expense of dimension-dependent constants. This captures many cases of interest: $L_p$ for $p \\in \\{1,2\\}$ is the standard measure of accuracy for regression, and for $f$ that outputs bounded probabilities, the logistic loss is equivalent to $L_2$ up to constants.\u201d\n\n5. *Have you experimented with adversarially robust models?*\n\nThat\u2019s a fascinating suggestion! There\u2019s a connection between adversarial training and our theory. Our proofs (as summarized in Table 1) predict that feature visualizations cannot be guaranteed to be reliable even for L-Lipschitz networks, unless we use a weak notion of \u2018prediction\u2019 in combination with a sufficiently low Lipschitz constant. As increasing the adversarial robustness of models is related to decreasing their Lipschitz constant, one might wonder whether feature visualizations become more reliable for adversarially robust models. In response to your question, we therefore stress-tested both of our fooling methods from Section 2 (fooling circuit & silent units) for an adversarially robust model trained by Salman et al. (2020). The results are very clear: both methods succeed in fooling the visualizations of adversarially robust models out of the box. The detailed results for those two experiments can be found in Appendix D. Thanks again for your suggestion!\n\n(In case you\u2019re interested in the details: The method in Section 2.1 works as long as the feature visualizations of a network have a sufficiently large perceptual difference to natural dataset samples - still the case for adversarially robust models. In fact, we can use the existing classifier trained to distinguish natural samples vs. feature visualizations for a non-robust model out of the box without any re-training; this classifier achieves an accuracy of 99.19% for visualizations from an adversarially robust model. The method in Section 2.2 leverages the difference in activation levels caused by feature visualizations and dataset samples. While this activation gap appears to be a bit smaller, it still exists and can be used to fool feature visualizations on adversarially robust models with impressive results as shown in Figure 14.)\n\nThanks again for your comments and questions, and kindly let us know if we have been able to address your suggestions and concerns!"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5953/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700244129477,
                "cdate": 1700244129477,
                "tmdate": 1700256487407,
                "mdate": 1700256487407,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NyjMCyKPlw",
                "forum": "OZWHYyfPwY",
                "replyto": "kWI6pskcEA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5953/Reviewer_MuGq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5953/Reviewer_MuGq"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "The rebuttal has addressed most of my initial concerns. As a result, I decide to increase the score, i.e. 5 -> 6."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5953/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700725871705,
                "cdate": 1700725871705,
                "tmdate": 1700725871705,
                "mdate": 1700725871705,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "RiAyE3dZr4",
            "forum": "OZWHYyfPwY",
            "replyto": "OZWHYyfPwY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5953/Reviewer_CUhk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5953/Reviewer_CUhk"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a fancy method to challenge the reliability of feature visualization. The authors therefore support three aspects to fool\uff0csanity-check and proof based the assumed theories. They show us how sensitive the feature visualization is when applying just a few units on the neural network which is a novel way to interpret the back-box of NN. And the authors indeed provided general and detailed proofs in the mian paper to illustrate their proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed method in this paper is kind of interesting and novel\uff0cwhich inspires us to challenge the current feature visualization methods and bring with a new point.\n\n2. This paper is well written.\n\n3. The mathematical proofs are plentiful and interesting."
                },
                "weaknesses": {
                    "value": "1. This paper should provide us with more quantitive results on some popular benchmarks to better demonstration the fooling or sanity-checking of intermediate feature can change the model\u2018s recognition ability.\n\n2. The NN model interpretation method for dense prediction tasks\uff0clike class activation map is widely used in weakly learning community. It will be more convincing if the author can provide some visualization results using their proposed method to see how will the final target object saliency map changes."
                },
                "questions": {
                    "value": "Please refer to the weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5953/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5953/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5953/Reviewer_CUhk"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5953/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699576954037,
            "cdate": 1699576954037,
            "tmdate": 1699636634817,
            "mdate": 1699636634817,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "b70LtSaOaS",
                "forum": "OZWHYyfPwY",
                "replyto": "RiAyE3dZr4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5953/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5953/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response & follow-up questions"
                    },
                    "comment": {
                        "value": "Dear reviewer CUhk,\n\nThanks for taking the time to review our paper. We\u2019re happy to hear that you found our paper **\u201cwell written\u201d**, with **\u201cinteresting & plentiful proofs\u201d** and an **\u201cinteresting & novel method\u201d**.\n\nWe wanted to reply quickly since we have a few follow-up questions (see below).\n\n\n**Point 1** *\"This paper should provide us with more quantitive results on some popular benchmarks to better demonstration the fooling or sanity-checking of intermediate feature can change the model\u2018s recognition ability.\"*:\n\nYou\u2019re right: many robustness methods seek to \u201cfool\u201d a network in the sense that the network\u2019s accuracy should drop given manipulated input. However, the goal of the fooling circuit is to NOT change the model\u2019s recognition ability. Instead, we fool an interpretability method: we want to maintain the network\u2019s object recognition ability (quantified by its classification accuracy) while fooling the feature visualization. This is validated by showing that the resulting network maintains identical ImageNet-1K validation accuracy (as described in Section 2) while fooling visualizations (as demonstrated in Figures 1, 2 and 5). Does this explanation make sense? Furthermore, would your point be addressed by providing more quantitative results showing that the network\u2019s accuracy on standard (e.g., classification) benchmarks is *not* changed/affected by our fooling method (if so, we\u2019re happy to test the model on more benchmarks)?\n\n**Point 2** *\"The NN model interpretation method for dense prediction tasks, like class activation map is widely used in weakly learning community. It will be more convincing if the author can provide some visualization results using their proposed method to see how will the final target object saliency map changes.\"*:\n\nWe\u2019re not sure we fully understand the question. Saliency maps ask \u201cwhich pixels provide evidence for a specific class given this particular image\u201d. In contrast, feature visualizations ask \u201chow does the maximally activating input for a unit look like\u201d (without explaining any specific input image). Both areas are interesting but the methods are different. In particular, in this paper we focus on feature visualizations and provide ample samples (Figures 1,2,3,5). Our methods neither aim nor attempt to fool saliency maps; this has already been done extensively by prior work (e.g. Adebayo et al. 2018, Nie et al. 2018). Could you let us know whether this clarifies the matter, or elaborate if it doesn\u2019t?"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5953/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699986183515,
                "cdate": 1699986183515,
                "tmdate": 1700246557468,
                "mdate": 1700246557468,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "pg8g79YBA6",
            "forum": "OZWHYyfPwY",
            "replyto": "OZWHYyfPwY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5953/Reviewer_NDh1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5953/Reviewer_NDh1"
            ],
            "content": {
                "summary": {
                    "value": "Feature visualization has so far been a central tool to generate insights about the features encoded by the units of computer vision systems. This paper asks: how reliable are feature visualizations? They first present a proof of concept that feature visualizations can be fooled as they show that they can make a model behave differently when classifying natural images vs. when generating feature visualizations. Motivated by this proof of concept they propose, a sanity check to make sure that feature visualizations generated by a method are indeed processed by a model in the same way as the natural images used for its training, as well as, a theoretical framework regarding the reliability of feature visualizations. Finally, the theoretical framework points out toward simpler models as more likely to be interpretable with the help of feature visualization and they offer preliminary evidence backing up that hypothesis."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is well-written and clear\n- They highlight limitations of feature visualization that are necessary to keep in mind (especially when used to audit black-box models)\n- The sanity check is motivated, sound, and a valuable contribution to the community"
                },
                "weaknesses": {
                    "value": "- I would advise for softening some of the claims/contextualization of the problem/results of the paper:\n\t- While feature visualization is one of the major types of interpretability methods in computer vision, it seems misleading to talk about \"widespread use\", as it is still a relatively minor method compared to truly widespread attribution methods (as feature visualization already suffers from other more practical limitations).\n\t-  While the proofs of concept are informative and something to keep in mind, they are extreme cases unless one is using the method for auditing external models (which is far from the common scenario). The scope of the proof of concept might be clear in the conclusion but not as much in the rest of the paper."
                },
                "questions": {
                    "value": "- Have the authors experimented with robust classifiers? What exactly causes feature visualization to be processed differently from natural inputs is a very interesting question. \n - No sanity check is perfect, especially if it becomes a metric to optimize for. Do the authors have intuition about the limitation of the sanity check, i.e., how could the sanity check be fooled? \n- While the experiments proposed are indeed not model-specific, half seem to be done using an Inception and the other half a ResNet50, is there a reason for this inconsistency?\n- Regarding the sanity check I would be very curious to see the results from other baseline to further explore the causes that could explain the difference. \n\t- (a) Is it simply a question of the ood-ness of the feature visualization as pointed out by Figure 3? In that case, I would be curious to see as extra baselines such as natural images vs natural images of the same class from another dataset (e.g., [1]) and natural images vs natural images of different classes from another dataset. \n\t- (b) Is it a question of the naturalness of the feature visualization as they look quite different from natural images or simply a problem of the fact that the visualization is optimized? In that case, I would be curious to see the results from methods that enforce more natural-looking images either by making use of a generative model ([2-3]) or that simply use specific regularization for that ([4]).\n- Not putting this in weakness, but I am not sure what to make of the theoretical section. Even though feature visualizations have been shown, at times ([4-5-6-7]), to offer insights about the network's unit activation, the main takeaway of this section, for now, is to caution that there is no guarantee that this is a general trend or that the insights are not only at a coarse level, which is important to keep in mind but it does not invalidate the empirical results previously cited. That being said, if one of the conclusions of this section is that alternatives to the current paradigm of activation maximization should be seriously considered by the community, it would be insightful if the authors could suggest possible alternatives that would overcome some of the limitations pointed out in this work.\n- With regard to softening the claims (and w.r.t the previous point), one concrete suggestion is, in the conclusion: \": Given that our theory proves that visualizing black-box systems via feature visualizations based on activation maximization (i.e., the current dominant paradigm) is impossible -> \"Given that our theory proves that there is no guarantee that ... is possible\"\n\n\n[1] Bitterwolf, Julian, Maximilian M\u00fcller, and Matthias Hein. \"In or Out? Fixing ImageNet Out-of-Distribution Detection Evaluation.\"\u00a0arXiv preprint arXiv:2306.00826\u00a0(2023).\n\n[2] Anh Nguyen, Alexey Dosovitskiy, Jason Yosinski, Thomas Brox, and Jeff Clune. Synthesizing the preferred inputs for neurons in neural networks via deep generator networks. Advances in neural information processing systems, 29, 2016.\n\n[3] Anh Nguyen, Jeff Clune, Yoshua Bengio, Alexey Dosovitskiy, and Jason Yosinski. Plug & play generative networks: Conditional iterative generation of images in latent space. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4467\u20134477, 2017.\n\n[4] Fel, Thomas, et al. \"Unlocking Feature Visualization for Deeper Networks with MAgnitude Constrained Optimization.\"\u00a0arXiv preprint arXiv:2306.06805\u00a0(2023).\n\n[5] Judy Borowski, Roland Simon Zimmermann, Judith Schepers, Robert Geirhos, Thomas SA Wallis, Matthias Bethge, and Wieland Brendel. Exemplary natural images explain CNN activations better than state-of-the-art feature visualization. In International Conference on Learning Representations, 2021\n\n[6] Roland S Zimmermann, Judy Borowski, Robert Geirhos, Matthias Bethge, Thomas Wallis, and Wieland Brendel. How well do feature visualizations support causal understanding of CNN activations? Advances in Neural Information Processing Systems, 34:11730\u201311744, 2021.\n\n[7] Roland S Zimmermann, Thomas Klein, and Wieland Brendel. Scale alone does not improve mechanistic interpretability in vision models. arXiv preprint arXiv:2307.05471, 2023"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5953/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5953/Reviewer_NDh1",
                        "ICLR.cc/2024/Conference/Submission5953/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5953/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699647338797,
            "cdate": 1699647338797,
            "tmdate": 1700603363307,
            "mdate": 1700603363307,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sZOlvqXKTl",
                "forum": "OZWHYyfPwY",
                "replyto": "pg8g79YBA6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5953/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5953/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response (1/2)"
                    },
                    "comment": {
                        "value": "Dear reviewer NDh1,\n\nThanks for your very helpful review, it\u2019s rare to see such a detailed review and we very much appreciate it! We\u2019re happy to hear that you found the soundness \u201c**excellent**\u201d and the paper \u201c**well-written and clear**\u201d with contributions that are \u201c**valuable to the community**\u201d. Please find a point-by-point response below.\n\n1. *Contextualizing / describing \u201cwidespread use\u201d of feature visualizations:*\n\nWe agree that contextualization is important. It is correct that, given the size of the overall ML community, feature visualizations are not a generally widespread tool. What we meant to say is that feature visualizations are one of the standard tools within the mechanistic interpretability community (which we think resonates with your review stating that feature visualizations are a \u201ccentral tool to generate insights about the features encoded by the units of computer vision systems\u201d). We identified a number of sentences where we stated \u201cwidely used / widespread use\u201d  and contextualized to make this more clear. For instance, we now write \u201cDespite its widespread use within the mechanistic interpretability community\u201d / \u201cFeature visualizations based on activation maximization are a widely used tool within the mechanistic interpretability community\u201d.\n\n2. *Contextualizing / The scope of the proof of concept might be clear in the conclusion but not as much in the rest of the paper.*\n\nIt seems that the right way to contextualize the proof of concept is to emphasize that it requires access to the model. We have thus revised the introduction summary bullet point 1. to state \u201cThus, feature visualizations can be deceived if one has access to the model\u201d and the conclusion of Section 2 which now reads \u201cThis establishes that feature visualizations can be arbitrarily manipulated if one has access to the model\u201d to make this more clear.\n\n3. *Have the authors experimented with robust classifiers?*\n\nThat\u2019s a fascinating suggestion! There\u2019s a connection between adversarial training and our theory. Our proofs (as summarized in Table 1) predict that feature visualizations cannot be guaranteed to be reliable even for L-Lipschitz networks, unless we use a weak notion of \u2018prediction\u2019 in combination with a sufficiently low Lipschitz constant. As increasing the adversarial robustness of models is related to decreasing their Lipschitz constant, one might wonder whether feature visualizations become more reliable for adversarially robust models. In response to your question, we therefore stress-tested both of our fooling methods from Section 2 (fooling circuit & silent units) for an adversarially robust model trained by Salman et al. (2020). The results are very clear: both methods succeed in fooling the visualizations of adversarially robust models out of the box. The detailed results for those two experiments can be found in Appendix D. Thanks again for your suggestion!\n\n(In case you\u2019re interested in the details: The method in Section 2.1 works as long as the feature visualizations of a network have a sufficiently large perceptual difference to natural dataset samples - still the case for adversarially robust models. In fact, we can use the existing classifier trained to distinguish natural samples vs. feature visualizations for a non-robust model out of the box without any re-training; this classifier achieves an accuracy of 99.19% for visualizations from an adversarially robust model. The method in Section 2.2 leverages the difference in activation levels caused by feature visualizations and dataset samples. While this activation gap appears to be a bit smaller, it still exists and can be used to fool feature visualizations on adversarially robust models with impressive results as shown in Figure 14.)\n\n4. *No sanity check is perfect, especially if it becomes a metric to optimize for. Do the authors have intuition about the limitation of the sanity check, i.e., how could the sanity check be fooled?*\n\nThat\u2019s an interesting point to think about! We\u2019ve added the following example of a limitation to the limitations section (App. G):\n\u201cNo sanity check is perfect, and like any sanity check, ours is just a necessary but not a sufficient condition for a reliable feature visualization. For instance, if the training data contains spurious correlations (all cows are on grass); a unit in the network is selective for grass and not for cows but the feature visualization shows realistic-looking cows on grass (rather than just grass), then the visualizations would pass the sanity check without raising a warning about the spurious correlation present in the visualization. We would love to see more research on sanity checks - to the best of our knowledge we provide the first one which hopefully serves as a motivation for research on both better visualizations and more sanity checks.\u201d"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5953/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700246473305,
                "cdate": 1700246473305,
                "tmdate": 1700256442612,
                "mdate": 1700256442612,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "l2bQsvg3AZ",
                "forum": "OZWHYyfPwY",
                "replyto": "pg8g79YBA6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5953/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5953/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response (2/2)"
                    },
                    "comment": {
                        "value": "5. *While the experiments proposed are indeed not model-specific, half seem to be done using an Inception and the other half a ResNet50, is there a reason for this inconsistency?*\n\nYes! We were worried that readers and reviewers might think the experiments/methods are somewhat model-specific or do not generalize across architectures if we just use a single network. We therefore made sure to include two different model families. We realize that\u2019s a bit confusing without explanation and have thus revised the intro to Section 2.2 to include the following: \u201cWe designed this method to show that fooling can easily be achieved in different ways and across different architectures. In order to demonstrate that different model families can be fooled, we here consider a different architecture (ResNet-50 instead of Inception-V1) and a randomly selected intermediate layer instead of the last layer (but note that the approach is not specific to a certain architecture or layer).\u201d\n\n6. *Regarding the sanity check I would be very curious to see the results from other baseline to further explore the causes that could explain the difference.*\n\nWe\u2019re working on this and will update the response once we have results. [EDIT: please see https://openreview.net/forum?id=OZWHYyfPwY&noteId=H0vbRC6wgf below for our update and Appendix E for the results]\n\n7. *Theory doesn\u2019t invalidate the empirical results previously cited [4-5-6-7].*\n\nCorrect, our theory does not invalidate the empirical results of previous studies [4-7]. Instead, it complements and completes their findings: It is true that previous quantitative studies [5-7] demonstrated that feature visualizations do indeed provide (some) information for understanding units of practically relevant neural networks. However, we\u2019d like to point out an important caveat: although these papers used a (relatively) weak notion of \u201cunderstanding\u201d, neither feature visualizations nor maximally activating dataset samples yield (close to) perfect understanding - there is still a gap that can be closed. This gap becomes substantially larger when making the notion of understanding just slightly harder [7]. We argue that this might suggest there is more going on inside the networks that these explanations do not explain - potentially because the explanation methods implicitly put strong assumptions on the networks that are, in practice, only weakly or coincidentally fulfilled. Our appendix B.7 includes a discussion on the relationship between our findings and previous psychophysical empirical studies.\n\n8. *It would be insightful if the authors could suggest possible alternatives that would overcome some of the limitations.*\n\nWe agree with you that finding a new paradigm that overcomes the reported issues will be an exciting result. While we would have loved to provide a more reliable feature visualization method for general black-box neural networks, our theory proves that this cannot be guaranteed to be reliable: irrespective of the nuances of different feature visualization methods based on activation maximization, much more knowledge about the \u2018black-box\u2019 is necessary to make reliable statements \u2014 a pretty strong result, backed up by extensive theory, that we believe will have a substantial impact on XAI since it challenges the concept of \u201cblack-box interpretability\u201d. We believe that future work could instead explore constructing networks that incorporate more constraints and assumptions (i.e., networks that are far from being black boxes) and thus enable more reliable predictions. \n\n9. *With regard to softening the claims (and w.r.t the previous point), one concrete suggestion is, in the conclusion \u2026*\n\nThanks for the suggestion. We rephrased the conclusion as follows: \u201cGiven that our theory proves that visualizing black-box systems via feature visualizations based on activation maximization (i.e., the current dominant paradigm) cannot be guaranteed to be reliable without making strong assumptions about the system, \u2026\u201d.\n\nThanks again for your helpful comments, questions, and suggestions - we really appreciate it! Kindly let us know whether we have been able to address your comments."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5953/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700246498956,
                "cdate": 1700246498956,
                "tmdate": 1700257037902,
                "mdate": 1700257037902,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "X0MNoy9Uu7",
                "forum": "OZWHYyfPwY",
                "replyto": "H0vbRC6wgf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5953/Reviewer_NDh1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5953/Reviewer_NDh1"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "Thank you for the thorough responses.\n\nThis paper has pointed to interesting practical limitations of current Feature visualization (e.g., why they seem to be treated fundamentally differently than natural images) which are deserving of further explorations as this method offers a unique perspective in the current state of deep learning interpretability, despite the other theoretical limitations highlighted in this work.\n\nFinally, as the authors have addressed most of my concerns, I am raising my score."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5953/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700603468051,
                "cdate": 1700603468051,
                "tmdate": 1700603468051,
                "mdate": 1700603468051,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]