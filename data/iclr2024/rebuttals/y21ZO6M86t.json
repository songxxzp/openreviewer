[
    {
        "title": "PolyGCL: GRAPH CONTRASTIVE LEARNING via Learnable Spectral Polynomial Filters"
    },
    {
        "review": {
            "id": "KRlGUvakbG",
            "forum": "y21ZO6M86t",
            "replyto": "y21ZO6M86t",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7529/Reviewer_LQgR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7529/Reviewer_LQgR"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on self-supervised learning applied to spectral GNNs. It addresses the limitation of existing contrastive learning techniques when dealing with high-pass heterophilic graphs. The authors tackle this challenge by introducing high-pass information in graph contrastive learning (GCL) and propose PolyGCL, a GCL pipeline that leverages polynomial graph basis to generate different spectral views for contrastive learning. Experimental results on synthetic and realworld datasets validate the effectiveness of the proposed PolyGCL."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ The introduction of polynomial spectral GNNs into graph contrastive learning is a novel and interesting idea.\n+ PolyGCL is well-motivated, and the authors provide empirical evidence to support the necessity of designing specific polynomial filter structures.\n+ There are some decent theoretical analyses of the proposed method.\n+ Experimental results demonstrate the effectiveness of PolyGCL."
                },
                "weaknesses": {
                    "value": "- Certain part of the introduced model need further clarrification. The authors propose to optimize a high-pass and a low-pass filter with polynomial spectral filters respectively, but ultimately arrive at a linear combination of the outputs of these two filter results. However, the optimization target in Equation (4) doesn't involve $\\alpha$ and $\\beta$ at all . \n- The rationale behind randomly shuffling the node feature to obtain the contrastive view needs to be explained, since is not a common practice to pertub graph representation.\n- In section 4.4, authors conclude that the aggregation of low-pass and high-pass representations achieves better error upper bound compared with just one aspect of representation. However, it still doesn't explain PolyGCL's advantage over directaly applying GCL on normal spectral GNNs like ChebNet.\n- The paper lacks sufficient ablation studies on the two separate filters, such as how the model's performance is affected when there is only one filter left or when $\\alpha$ and $\\beta$ are fixed."
                },
                "questions": {
                    "value": "1. Please provide a more detailed explanation of how $\\alpha$ and $\\beta$ are optimized in Equation (4). Are there any supervising signals to optimize these two parameters? If so, the model seems to be equivalant to directaly optimizing a polynomial filter since all of the coefficients are learnable.\n2. Please give the rationale for randomly shuffling node features as a pertubation method. Would the model performance be influenced if the random shuffle were replaced with other methods like edge-dropping or subgraph-sampling?\n3. Please provide more explanations on PolyGCL's advantage over directaly applying GCL on spectral GNNs?\n4. How would the model performance when there is only one filter left or $\\alpha$ and $\\beta$ are fixed?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7529/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7529/Reviewer_LQgR",
                        "ICLR.cc/2024/Conference/Submission7529/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7529/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697558716509,
            "cdate": 1697558716509,
            "tmdate": 1700482710644,
            "mdate": 1700482710644,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RZ0WK2w0u1",
                "forum": "y21ZO6M86t",
                "replyto": "KRlGUvakbG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7529/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7529/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer LQgR (1)"
                    },
                    "comment": {
                        "value": "Thank you for the detailed comments and valuable questions. We provide details to clarify your major concerns.\n\nThe reply to weakness is integrated into the reply to questions as follows.\n\n**Reply to Questions**\n\n\n>Q1: Please provide a more detailed explanation of how $\\alpha$ and $\\beta$ are optimized in Equation (4). Are there any supervising signals to optimize these two parameters? If so, the model seems to be equivalant to directaly optimizing a polynomial filter since all of the coefficients are learnable.\n\nA1: To make it clearer, we consider rewriting Equation (4) in Section 4.2 as follows to explicitly reflect the optimization process of parameters $\\alpha, \\beta$.\n$$\n\\begin{aligned}\n\\mathcal{L}_{\\text{BCE}}  &=\\frac{1}{4 N}\\left(\\sum\\_{i=1}^N \\log \\mathcal{D}\\left(\\mathbf{Z}_L^i, \\mathbf{g}\\right)+\\log \\left(1-\\mathcal{D}\\left(\\tilde{\\mathbf{Z}}_L^i, \\mathbf{g}\\right)\\right)+ \\log \\mathcal{D}\\left(\\mathbf{Z}_H^i, \\mathbf{g}\\right)+\\log \\left(1-\\mathcal{D}\\left(\\tilde{\\mathbf{Z}}_H^i, \\mathbf{g}\\right)\\right)\\right)\n\\\\\\\\\n&=\\frac{1}{4 N}\\left(\\sum\\_{i=1}^N \\log \\mathcal{D}\\left(\\mathbf{Z}_L^i, \\textbf{Mean}(\\mathbf{Z})\\right)+\\log \\left(1-\\mathcal{D}\\left(\\tilde{\\mathbf{Z}}_L^i, \\textbf{Mean}(\\mathbf{Z})\\right)\\right)\\right. \\\\\\\\\n& \\quad \\quad \\quad \\quad + \\left.\\log \\mathcal{D}\\left(\\mathbf{Z}_H^i, \\textbf{Mean}(\\mathbf{Z})\\right)+\\log \\left(1-\\mathcal{D}\\left(\\tilde{\\mathbf{Z}}_H^i, \\textbf{Mean}(\\mathbf{Z})\\right)\\right)\\right)\\\\\\\\\n&=\\frac{1}{4 N}\\left(\\sum\\_{i=1}^N \\log \\mathcal{D}\\left(\\mathbf{Z}_L^i, \\textbf{Mean}(\\alpha \\mathbf{Z}_L+\\beta \\mathbf{Z}_H)\\right)+\\log \\left(1-\\mathcal{D}\\left(\\tilde{\\mathbf{Z}}_L^i, \\textbf{Mean}(\\alpha \\mathbf{Z}_L+\\beta \\mathbf{Z}_H)\\right)\\right)\\right. \\\\\\\\\n& \\quad \\quad \\quad \\quad + \\left.\\log \\mathcal{D}\\left(\\mathbf{Z}_H^i, \\textbf{Mean}(\\alpha \\mathbf{Z}_L+\\beta \\mathbf{Z}_H)\\right)+\\log \\left(1-\\mathcal{D}\\left(\\tilde{\\mathbf{Z}}_H^i, \\textbf{Mean}(\\alpha \\mathbf{Z}_L+\\beta \\mathbf{Z}_H)\\right)\\right)\\right).\n\\end{aligned}\n$$\n\nAs shown in the above equation, the optimization target involves the learnable parameters $\\alpha$  and $\\beta$. Further, considering that all of the coefficients are learnable, PolyGCL can be viewed as directly optimizing a polynomial filter. We claim that compared with directly optimizing a polynomial filter in self-supervised settings, the training paradigm in PolyGCL makes the polynomial filter easier to fit the filter functions and exhibits better adaption to graphs across homophily due to the decoupling of low-pass and high-pass information. Please refer to Q3 for detailed explanations."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7529/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700234183797,
                "cdate": 1700234183797,
                "tmdate": 1700234240570,
                "mdate": 1700234240570,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vsFMqtzGyp",
                "forum": "y21ZO6M86t",
                "replyto": "KRlGUvakbG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7529/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7529/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer LQgR (2)"
                    },
                    "comment": {
                        "value": ">Q2: Please give the rationale for randomly shuffling node features as a pertubation method. Would the model performance be influenced if the random shuffle were replaced with other methods like edge-dropping or subgraph-sampling?\n\nA2: We explain the rationale for randomly shuffling node features as a perturbation method as follows.\n\n- Firstly, randomly shuffling node features can be considered as an easy-to-implement way to construct corrupted samples, and a series of works (DGI [1], MVGRL [2], GraphCL [3], GGD [4]) adopt this simple strategy and demonstrate effectiveness in practice.\n- Besides, randomly shuffling features is an operation at the node feature level without destroying the structural information, thus will not cause perturbation to the graph spectrum. This property facilitates our analysis of SRL in Section 4.3.\n\nFurther, we conduct experiments on real-world datasets to explore the impact of other data augmentation methods on PolyGCL. We denote edge-dropping as \"ED\", feature-masking as \"FM\", and subgraph-sampling as \"SS\" for short. Note that we follow the settings in CCA-SSG [5] and GRACE [6] to perform edge-dropping and feature-masking at the same time, which is denoted as \"ED\\&FM\", and we perform the subgraph-sampling (SS) perturbation following GraphCL [3].\n\n| Methods                | cora                | citeseer            | pubmed              | cornell             | texas               | wisconsin           | actor               | chameleon           | squirrel            |\n|------------------------|---------------------|---------------------|---------------------|---------------------|---------------------|---------------------|---------------------|---------------------|---------------------|\n| PolyGCL               | 87.57 \u00b1 0.62        | 79.81 \u00b1 0.85    | 87.15 \u00b1 0.27    | 82.62 \u00b1 3.11        | 88.03 \u00b1 1.80    | 85.50 \u00b1 1.88    | 41.15 \u00b1 0.88    | 71.62 \u00b1 0.96    | 56.49 \u00b1 0.72    |\n| PolyGCL(ED&FM)         | 86.85 \u00b1 0.77        | 78.23 \u00b1 0.54        | 85.85 \u00b1 0.26        | 84.11 \u00b1 2.97    | 85.80 \u00b1 1.85        | 81.26 \u00b1 2.25        | 38.44 \u00b1 0.90    | 70.30 \u00b1 1.04    | 53.88 \u00b1 1.22    |\n| PolyGCL(SS)            | 84.74 \u00b1 0.84        | 75.30 \u00b1 0.79        | 82.61 \u00b1 0.28        | 80.33 \u00b1 1.80        | 82.62 \u00b1 3.11        | 76.25 \u00b1 2.25        | 33.10 \u00b1 1.26        | 65.84 \u00b1 1.42        | 46.04 \u00b1 0.81        |\n\n\nIn the above table, we observe that the ED\\&FM perturbation slightly deteriorates the performance of PolyGCL but still seems comparable. However, the SS perturbation causes significant damage to the effectiveness of PolyGCL. We attribute this phenomenon to excessive perturbations in the graph topology and the loss of important spectrum information while conducting the subgraph-sampling operation."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7529/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700234307825,
                "cdate": 1700234307825,
                "tmdate": 1700234307825,
                "mdate": 1700234307825,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3nsmeRb6rg",
                "forum": "y21ZO6M86t",
                "replyto": "KRlGUvakbG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7529/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7529/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer LQgR (3)"
                    },
                    "comment": {
                        "value": ">Q3: Please provide more explanations on PolyGCL's advantage over directaly applying GCL on spectral GNNs?\n\n\nA3: In Section 4.4, we provide a theoretical analysis of decoupling the low-pass and high-pass representations and linearly combining them, which guarantees a better error upper bound compared with just one aspect of representation.  It is noted that all the discussions in Section 4.4 are based on the decoupling of the low-pass and high-pass information, which conforms to the learning process of PolyGCL. Previous works have pointed out that spectral polynomial filters have the capability of learning filters of any shape, however, their learning paradigm highly relies on the annotated data, which corresponds to supervised learning. As shown in Figure 1, in the self-supervised setting, directly applying GCL on spectral GNNs results in performance degradation.\n\n| Methods               | $\\phi=-1$            | $\\phi=-0.75$         | $\\phi=-0.5$         | $\\phi=-0.25$         | $\\phi=0$            | $\\phi=0.25$         | $\\phi=0.5$           | $\\phi=0.75$          | $\\phi=1$             |\n|-----------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|\n| PolyGCL              | 98.84 \u00b1 0.17    | 94.23 \u00b1 0.31    | 90.82 \u00b1 0.50    | 75.43 \u00b1 0.68    | 66.51 \u00b1 0.69    | 69.43 \u00b1 0.65    | 88.22 \u00b1 0.72    | 98.09 \u00b1 0.29    | 99.29 \u00b1 0.23    |\n| PolyGCL(Cheb)         | 79.28 \u00b1 0.46    | 85.35 \u00b1 0.82    | 81.04 \u00b1 0.57    | 77.27 \u00b1 0.90    | 65.90 \u00b1 0.53    | 70.34 \u00b1 0.98    | 84.63 \u00b1 0.81    | 92.97 \u00b1 0.33    | 89.72 \u00b1 0.58    |\n\n\nWe also report the mean accuracy results of the cSBM datasets in the above table as a comparison between PolyGCL and PolyGCL(Cheb), which directly applies GCL to the Chebshev polynomial filters. PolyGCL generally outperforms PolyGCL(Cheb) on different homophily levels, especially in extreme homophilic/heterophilic settings ($|\\phi|\\to 1$). \n\nFurther, we visualize the learned filters in PolyGCL(Cheb) on the cSBM datasets. Compared Figure 5 in Appendix E.4 with Figure 4 in Section 5.4, we conclude that although PolyGCL(Cheb) can roughly capture the low-pass and high-pass tendency for homophilic and heterophilic graphs respectively, it tends to learn the complex filters with a high degree of oscillation across homophily, which exhibits difficulty with the absence of supervised signals. \n\nAs a comparison, the filter curve learned in Figure 4 is smoother, and its low-pass/high-pass trend is more significant under the homophilic/heterophilic settings, which shows the effectiveness of the learning paradigm in PolyGCL, that is, decoupling the low-pass and high-pass information and linearly combining them later to obtain the final embeddings."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7529/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700234385601,
                "cdate": 1700234385601,
                "tmdate": 1700234385601,
                "mdate": 1700234385601,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qMPywCozBl",
                "forum": "y21ZO6M86t",
                "replyto": "KRlGUvakbG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7529/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7529/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer LQgR (4)"
                    },
                    "comment": {
                        "value": ">Q4: How would the model performance when there is only one filter left or $\\alpha$ and $\\beta$ are fixed?\n\nA4: We include experiments where $\\alpha$ and $\\beta$ are set to zero separately as the ablation studies. Specifically, $\\alpha=0, \\beta=0$ means only the high-pass/low-pass filter is reserved respectively. The results are given in the following table. \n\n| Methods                | cora                | citeseer            | pubmed              | cornell             | texas               | wisconsin           | actor               | chameleon           | squirrel            |\n|------------------------|---------------------|---------------------|---------------------|---------------------|---------------------|---------------------|---------------------|---------------------|---------------------|\n| PolyGCL               | 87.57 \u00b1 0.62        | 79.81 \u00b1 0.85    | 87.15 \u00b1 0.27    | 82.62 \u00b1 3.11        | 88.03 \u00b1 1.80    | 85.50 \u00b1 1.88    | 41.15 \u00b1 0.88    | 71.62 \u00b1 0.96    | 56.49 \u00b1 0.72    |\n| PolyGCL($\\alpha=0$)           | 70.67 \u00b1 0.48        | 64.22 \u00b1 0.93        | 76.41 \u00b1 0.35        | 80.16 \u00b1 2.62        | 86.52 \u00b1 1.97        | 82.07 \u00b1 2.75        | 38.28 \u00b1 0.39        | 68.21 \u00b1 1.40        | 52.10 \u00b1 0.80        |\n| PolyGCL($\\beta=0$)           | 87.65 \u00b1 0.67    | 78.76 \u00b1 0.75        | 86.64 \u00b1 0.17        | 76.23 \u00b1 5.41        | 82.56 \u00b1 2.13        | 68.88 \u00b1 2.50        | 37.36 \u00b1 0.46        | 65.08 \u00b1 1.27        | 48.52 \u00b1 0.71        |\n\nNote that PolyGCL achieves better performance when optimized over both the parameters in most datasets. In addition, we observe that PolyGCL($\\alpha=0$) remains comparable in heterophilic datasets, while PolyGCL($\\beta=0$) shows better adaptability to homophilic settings. The results are further consistent with the low-pass/high-pass preference for homophilic/heterophilic settings in Figure 3 in Section 5.4. \n\nReferences:\n\n[1] Veli\u010dkovi\u0107, Petar, et al. \"Deep graph infomax.\" _arXiv preprint arXiv:1809.10341_ (2018).\n\n[2] Hassani, Kaveh, and Amir Hosein Khasahmadi. \"Contrastive multi-view representation learning on graphs.\" _International conference on machine learning_. PMLR, 2020.\n\n[3] You, Yuning, et al. \"Graph contrastive learning with augmentations.\" _Advances in neural information processing systems_ 33 (2020): 5812-5823.\n\n[4] Zheng, Yizhen, et al. \"Rethinking and scaling up graph contrastive learning: An extremely efficient approach with group discrimination.\" _Advances in Neural Information Processing Systems_ 35 (2022): 10809-10820.\n\n[5] Zhang, Hengrui, et al. \"From canonical correlation analysis to self-supervised graph neural networks.\" _Advances in Neural Information Processing Systems_ 34 (2021): 76-89.\n\n[6] Zhu, Yanqiao, et al. \"Deep graph contrastive representation learning.\" _arXiv preprint arXiv:2006.04131_ (2020)."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7529/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700234487833,
                "cdate": 1700234487833,
                "tmdate": 1700234487833,
                "mdate": 1700234487833,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "82gbBRmqGw",
                "forum": "y21ZO6M86t",
                "replyto": "qMPywCozBl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7529/Reviewer_LQgR"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7529/Reviewer_LQgR"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the authors efforts in clarifying the details as well as providing extra expariments. Since all my concerns have been addressed, I will update the score and support this work."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7529/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700482697799,
                "cdate": 1700482697799,
                "tmdate": 1700482786316,
                "mdate": 1700482786316,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "v59FyXm7lY",
            "forum": "y21ZO6M86t",
            "replyto": "y21ZO6M86t",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7529/Reviewer_2heL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7529/Reviewer_2heL"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a novel approach to Graph Contrastive Learning (GCL) named POLYGCL, designed to address the challenges of applying GCL to both homophilic and heterophilic graphs. The primary limitation addressed is the inherent smoothness introduced by traditional low-pass GNN encoders. POLYGCL employs learnable spectral polynomial filters to balance between low-pass and high-pass views, providing better representation learning for both graph types. Theoretical proofs underscore the effectiveness of this combined filter approach, and empirical evaluations on synthetic and real-world datasets validate the method's superiority over existing GCL paradigms. The research fills a crucial gap in the realm of graph representation learning, particularly concerning heterophilic graphs."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. **Originality**:\n    - The work presents a novel approach in the form of **PolyGCL** that combines both low-pass and high-pass filters in graph-based learning.\n    - The theoretical foundations are robust, and the introduction of a combined filter in graph learning is a commendable initiative.\n    - The work extends beyond existing literature by addressing both Graph Contrastive Learning (GCL) methods and spectral-based GNNs, which is an original contribution to the domain.\n\n2. **Quality**:\n    - The paper is backed by rigorous theoretical formulations, evidenced by multiple theorems, remarks, propositions, and corollaries.\n    - The empirical validations, including both synthetic and real-world datasets, provide strong support for the paper's claims. A diverse set of baselines have been chosen for comparisons, ensuring a comprehensive evaluation.\n\n3. **Significance**:\n    - The paper addresses challenges in existing GCL methods and spectral-based GNNs, a pressing need in the domain of graph representation learning.\n    - The proposed method's prowess on heterophilic graphs is especially significant given the inherent challenges of these graph types.\n    - The complexity analysis suggests the scalability of the method, highlighting its potential applicability to large-scale problems in the future.\n\n---\n\nIn conclusion, the paper stands out in its originality, depth of analysis, and significance to the graph learning community. It is a valuable contribution to the literature and holds promise for further explorations in this direction."
                },
                "weaknesses": {
                    "value": "1. **Lack of Intuitive Explanation for Choice of Chebyshev Polynomials:** The paper introduces the use of Chebyshev polynomials without providing a deep intuitive rationale behind this choice. For readers unfamiliar with Chebyshev polynomials or their relevance in graph-based learning, this can be a point of confusion. A discussion on why Chebyshev polynomials were chosen over other potential polynomial bases could add depth to the work.\n\n2. **Decoupling of Low-Pass and High-Pass Information**: While the paper emphasizes the importance of decoupling low-pass and high-pass filter functions, it doesn't thoroughly discuss the potential pitfalls or challenges of this approach. It would be beneficial to discuss under which conditions this decoupling might not be advantageous or how it compares to methods that don't employ this decoupling.\n\n3. **Reparameterization and Assumptions**: The paper introduces reparameterization techniques, such as using the prefix sum and prefix difference, to ensure non-negativity and a decremental filter value. However, there's a lack of clarity on the implications of these assumptions and reparameterizations on the model's performance and generality. It would be beneficial to delve deeper into why these particular reparameterization techniques were chosen and their impact on the robustness and flexibility of the approach."
                },
                "questions": {
                    "value": "1. **Motivation behind Chebyshev Polynomials**: Can the authors provide more insight into why Chebyshev polynomials were specifically chosen as the base polynomials? How do they compare to other potential polynomial bases in terms of efficacy and computational efficiency in the context of Graph Contrastive Learning?\n\n2. **Filter Decoupling Challenges**: Are there potential challenges or scenarios where the decoupling of low-pass and high-pass information might be less advantageous? How does the method account for such scenarios?\n\n3. **Limitations of the Model**: Every model has its limitations. What would the authors identify as the key limitations of the PolyGCL method? Are there specific types of graph structures or data distributions where PolyGCL might not be the best choice?\n\n4. **Future Work**: Given the current findings and the proposed method, what do the authors see as the next steps or future directions in this line of research? Are there plans to extend PolyGCL or integrate it with other techniques to address more complex graph learning scenarios?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7529/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7529/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7529/Reviewer_2heL"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7529/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698710185186,
            "cdate": 1698710185186,
            "tmdate": 1699636909328,
            "mdate": 1699636909328,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nKaLpuTN2c",
                "forum": "y21ZO6M86t",
                "replyto": "v59FyXm7lY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7529/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7529/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer 2heL (1)"
                    },
                    "comment": {
                        "value": "Thanks for your attention and interest in our work, and we are grateful for your valuable and constructive feedback.\n\n**Reply to Weakness**\n\n\nW1 and W2 are integrated into Q1 and Q2 respectively.\n\n>W3: Reparameterization and Assumptions: The paper introduces reparameterization techniques, such as using the prefix sum and prefix difference, to ensure non-negativity and a decremental filter value. However, there's a lack of clarity on the implications of these assumptions and reparameterizations on the model's performance and generality. It would be beneficial to delve deeper into why these particular reparameterization techniques were chosen and their impact on the robustness and flexibility of the approach.\n\nA1:  We utilize the reparameterization techniques to ensure the low-pass and high-pass properties of the learned filters during the learning process, which correspond to the filter functions with incremental and decremental values, respectively.\n\nFirstly, in spectral analysis, low-pass/high-pass filters are better defined when the value of filter function is non-negative, and the advantages of ensuing non-negativity are also discussed in BernNet [1]. Based on the non-negativity, the prefix sum/difference is introduced to ensure the low-pass/high-pass property of the filter functions learned by polynomials during model training, which makes the low-pass and high-pass information decouple naturally and further promotes the effective learning of the separate filters to better adapt to homophily/heterophily scenarios.\n\nWe list the comparison between the results of PolyGCL and PolyGCL(wo-RP) in the following table, where PolyGCL(wo-RP) denotes PolyGCL **w**ith**o**ut **R**e**P**arameterization. By decoupling the low-pass and high-pass information via the simple reparameterization, PolyGCL benefits from more stable model training process and improves the performance on homophilic/heterophilic datasets.\n\n| Methods             | cora              | citeseer          | pubmed            | cornell           | texas             | wisconsin         | actor             | chameleon         | squirrel          |\n|---------------------|-------------------|-------------------|-------------------|-------------------|-------------------|-------------------|-------------------|-------------------|-------------------|\n| PolyGCL           | 87.57 \u00b1 0.62      | 79.81 \u00b1 0.85  | 87.15 \u00b1 0.27  | 82.62 \u00b1 3.11      | 88.03 \u00b1 1.80  | 85.50 \u00b1 1.88  | 41.15 \u00b1 0.88  | 71.62 \u00b1 0.96  | 56.49 \u00b1 0.72  |\n| PolyGCL(wo-RP)      | 85.09 \u00b1 0.78      | 76.74 \u00b1 0.80      | 84.39 \u00b1 0.29      | 78.26 \u00b1 3.02      | 84.11 \u00b1 2.29      | 79.50 \u00b1 3.50      | 38.14 \u00b1 0.96      | 65.98 \u00b1 0.95      | 50.06 \u00b1 1.01      |\n\n\nIn addition, the reparameterization technique based on non-negativity and prefix sum/difference can be easily extended to other polynomial bases, which further verifies the robustness of this technique. In detail, as for the Bernstein polynomial, which can also learn arbitrary filters, we directly utilize this reparameterization on the coefficients $\\theta_k$ which proves to be equivalent to the filter value $h(\\lambda)$. Besides, as for the Monomial polynomial in GPR-GNN [2], we can also consider non-negative coefficients to ensure the low-pass/high-pass property, for instance, $\\sum_{i=0}^K \\gamma_i (2\\mathbf{I}-\\mathbf{L})^i$ and $\\sum_{i=0}^K \\gamma_i \\mathbf{L}^i$ for low-pass/high-pass filters respectively, where the non-negativity of $\\gamma_i$ is all we need.\n\nThe above analysis reflects the generality and flexibility of our proposed reparameterization technique, and we consider utilizing the simple reparameterization to conduct experiments on Bernstein and Monomial bases, the results are listed as PolyGCL(Bern) and PolyGCL(Mono) in Q1."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7529/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700232959009,
                "cdate": 1700232959009,
                "tmdate": 1700232959009,
                "mdate": 1700232959009,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TOCm7dx81B",
                "forum": "y21ZO6M86t",
                "replyto": "v59FyXm7lY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7529/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7529/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer 2heL (2)"
                    },
                    "comment": {
                        "value": "**Reply to Questions**\n\n\n> Q1: Motivation behind Chebyshev Polynomials: Can the authors provide more insight into why Chebyshev polynomials were specifically chosen as the base polynomials? How do they compare to other potential polynomial bases in terms of efficacy and computational efficiency in the context of Graph Contrastive Learning?\n\nA2: We summarize the reasons for choosing Chebyshev polynomials as the base polynomials as follows.\n\n- Theoretically, ChebyNetII [3] and [4] claims that the Chebyshev polynomial achieves the optimum convergent rate and near-optimum error when approximating a function compared with other bases. \n-  Both Chebshev and Monomial bases are linear time complexity related to propagation step $K$, but BernNet is quadratic time complexity related to $K$.\n- In practice, we utilize the Chebyshev polynomials out of comprehensive considerations of efficiency and effectiveness, as shown in the following tables which report the mean accuracy and the average run time per epoch (ms) of the models.\n\n| Methods             | cora              | citeseer          | pubmed            | cornell           | texas             | wisconsin         | actor             | chameleon         | squirrel          |\n|---------------------|-------------------|-------------------|-------------------|-------------------|-------------------|-------------------|-------------------|-------------------|-------------------|\n| PolyGCL            | 87.57 \u00b1 0.62  | 79.81 \u00b1 0.85  | 87.15 \u00b1 0.27  | 82.62 \u00b1 3.11      | 88.03 \u00b1 1.80  | 85.50 \u00b1 1.88  | 41.15 \u00b1 0.88  | 71.62 \u00b1 0.96  | 56.49 \u00b1 0.72  |\n| PolyGCL(Bern)       | 85.51 \u00b1 0.69      | 77.48 \u00b1 0.70      | 83.90 \u00b1 0.24      | 83.13 \u00b1 3.01  | 80.23 \u00b1 2.14      | 82.40 \u00b1 2.75  | 38.08 \u00b1 0.88      | 69.35 \u00b1 1.12  | 51.76 \u00b1 0.94  |\n| PolyGCL(Mono)       | 84.94 \u00b1 1.01      | 78.52 \u00b1 0.78  | 85.49 \u00b1 0.33      | 79.87 \u00b1 2.18      | 83.27 \u00b1 3.05      | 81.42 \u00b1 2.50      | 39.19 \u00b1 1.01      | 66.41 \u00b1 1.20      | 49.45 \u00b1 0.79      |\n\n\n| Methods             | cora    | citeseer | pubmed  | cornell | texas  | wisconsin | actor   | chameleon | squirrel |\n|---------------------|---------|----------|---------|---------|--------|-----------|---------|-----------|----------|\n| PolyGCL            | 82.32   | 136.21   | 242.92  | 55.23   | 45.91  | 49.77     | 166.17  | 221.73    | 824.76   |\n| PolyGCL(Bern)       | 208.53  | 322.73   | 815.07  | 97.61   | 90.83  | 104.38    | 383.9   | 589.41    | 2284.05  |\n| PolyGCL(Mono)       | 105.44  | 167.95   | 180.48  | 46.71   | 38.63  | 45.04     | 130.62  | 245.82    | 596.69   |\n\n\nBesides, based on the above tables, we conclude that the results of PolyGCL(Bern) and PolyGCL(Mono) are comparable with PolyGCL on certain datasets, which reflects the generality of our framework.\n\n\n> Q2: Filter Decoupling Challenges: Are there potential challenges or scenarios where the decoupling of low-pass and high-pass information might be less advantageous? How does the method account for such scenarios?\n\n\nA3:  Yes. For exmaple, if the graph is extremely homophilic, there is almost no need for high-pass frequency. In this case, the introduction of high-pass information tends to be less advantageous. However, PolyGCL has the potential to adjust the relative value between the linear coefficients $\\alpha$ and $\\beta$ adaptively.\n\nAs shown in Figure 3 in Section 5.4, when $\\phi \\to 1$ (which indicates the extreme homophilic settings in cSBM), the linear coefficient $\\alpha$ for the low-pass filter approaches $1$, meaning the value of $\\beta$ for the high-pass filter approaches $0$ and PolyGCL degenerates into considering only low-pass information. However, most real graphs will not be such an extreme case, thus PolyGCL performs well on real datasets."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7529/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700233133117,
                "cdate": 1700233133117,
                "tmdate": 1700233133117,
                "mdate": 1700233133117,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cKoyLnHPRh",
                "forum": "y21ZO6M86t",
                "replyto": "v59FyXm7lY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7529/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7529/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer 2heL (3)"
                    },
                    "comment": {
                        "value": "> Q3: Limitations of the Model: Every model has its limitations. What would the authors identify as the key limitations of the PolyGCL method? Are there specific types of graph structures or data distributions where PolyGCL might not be the best choice?\n\nA4: The key limitation of PolyGCL lies in its scalability to large graphs. In Section 5.5, the time complexity of PolyGCL is linear to $N$ and $E$, which represent the number of nodes and edges respectively. Compared with other baselines in Table 4 in Section 5.5, PolyGCL can handle graphs with a medium size ($10^5-10^6$ nodes) due to its linear complexity and the pre-computing of polynomial filters. However, PolyGCL still suffers from OOM when modeling extremely large graphs with over 100 million nodes (ogbn-papers100M [5]).\n\nBesides, there are a series of works [6, 7] showing that the homophily pattern of graphs is actually very complex and may be local or not critical. Spectrum analysis in these cases is still lacking, and the low-pass/high-pass filtering may not be the decisive factor in the applicability of homophily/heterophily, which limits the expressiveness of PolyGCL.\n\nIn addition, as for the graph with too many noise edges that destroy its spectral, PolyGCL which relies on the spectral filters might not be the best choice. In this case, the node features themselves are of more significance to some extent. \n\n\n>Q4: Future Work: Given the current findings and the proposed method, what do the authors see as the next steps or future directions in this line of research? Are there plans to extend PolyGCL or integrate it with other techniques to address more complex graph learning scenarios?\n\nA5: We believe that performing efficient self-supervised learning on extremely large graphs (ogbn-papers100M [5]) with spectral analysis is a promising future direction in this line of research. Considering the linear complexity and the nature of pre-computing polynomial propagation, PolyGCL exhibits the potential to further extend its scalability.\n\nAs for more complex graph learning scenarios, we intend to explore the complex homophily pattern of graphs shown in [6, 7] and the spectral analysis of heterogeneous graphs in self-supervised settings. We believe the spectral analysis in PolyGCL will play a key role or open up a new window for these complex scenarios.\n\nReferences:\n\n[1] He, Mingguo, Zhewei Wei, and Hongteng Xu. \"Bernnet: Learning arbitrary graph spectral filters via bernstein approximation.\" _Advances in Neural Information Processing Systems_ 34 (2021): 14239-14251.\n\n[2] Chien, Eli, et al. \"Adaptive universal generalized pagerank graph neural network.\" _arXiv preprint arXiv:2006.07988_ (2020).\n\n[3] He, Mingguo, Zhewei Wei, and Ji-Rong Wen. \"Convolutional neural networks on graphs with chebyshev approximation, revisited.\" _Advances in Neural Information Processing Systems_ 35 (2022): 7264-7276.\n\n[4] Geddes, Keith O. \"Near-minimax polynomial approximation in an elliptical region.\" _SIAM Journal on Numerical Analysis_ 15.6 (1978): 1225-1233.\n\n[5] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. Advances in neural information processing systems, 33:22118\u201322133, 2020.\n\n[6] Mao, Haitao, et al. \"Demystifying Structural Disparity in Graph Neural Networks: Can One Size Fit All?.\" _arXiv preprint arXiv:2306.01323_ (2023).\n\n[7] Luan, Sitao, et al. \"When do graph neural networks help with node classification: Investigating the homophily principle on node distinguishability.\" _arXiv preprint arXiv:2304.14274_ (2023)."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7529/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700233215557,
                "cdate": 1700233215557,
                "tmdate": 1700233215557,
                "mdate": 1700233215557,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3985TH4rO2",
                "forum": "y21ZO6M86t",
                "replyto": "v59FyXm7lY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7529/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7529/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A Kind Reminder: the author-reviewer discussion period is coming to an end."
                    },
                    "comment": {
                        "value": "Dear reviewer 2heL,\n\nWe sincerely appreciate your thorough and detailed reviews of our submission. We hope that you will find our responses satisfactory and that they help clarify your concerns. We appreciate the opportunity to engage with you. We would like to kindly remind you that the discussion period is coming to an end. Could you please inform us whether our responses have addressed your concerns or if there are any other questions you need us to clarify?\n\nThank you very much for your time.\n\nBest regards,\n\nAuthors of Submission 7529"
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7529/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700717407561,
                "cdate": 1700717407561,
                "tmdate": 1700717407561,
                "mdate": 1700717407561,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "gKubVWhiwQ",
            "forum": "y21ZO6M86t",
            "replyto": "y21ZO6M86t",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7529/Reviewer_aFan"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7529/Reviewer_aFan"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses graph contrastive learning with learnable spectral filters, allowing to tackle homophilic and Heterophilic graphs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Graph contrastive learning is of great interest, including the challenging goal of addressing heterophilic graphs.\nThe proposed idea is interesting and the paper providing theoretical and relevant experimental results is well written."
                },
                "weaknesses": {
                    "value": "The main issue is that this paper is not well positioned within the literature of graph contrastive learning (GCL), including several missing work on GCL with spectral filters and/or adaptive/learnable filters, such as\n* Liu, Nian, Xiao Wang, Deyu Bo, Chuan Shi, and Jian Pei. \"Revisiting graph contrastive learning from the perspective of graph spectrum.\" Advances in Neural Information Processing Systems 35 (2022): 2972-2983.\n* Xie, Xuanting, Wenyu Chen, Zhao Kang, and Chong Peng. \"Contrastive graph clustering with adaptive filter.\" Expert Systems with Applications 219 (2023): 119645.\n* Zhang, Hengrui, Qitian Wu, Yu Wang, Shaofeng Zhang, Junchi Yan, and Philip S. Yu. \"Localized Contrastive Learning on Graphs.\" arXiv preprint arXiv:2212.04604 (2022).\nThe latter also provide some results on heterophilic graphs. Another paper on GCL with homophilic and heterophilic graphs is\n* Wang, Haonan, Jieyu Zhang, Qi Zhu, and Wei Huang. \"Can Single-Pass Contrastive Learning Work for Both Homophilic and Heterophilic Graph?.\" arXiv preprint arXiv:2211.10890 (2022).\nand some theoretical results in this paper can be related to works conducted by other researchers  \n* HaoChen, Jeff Z., Colin Wei, Adrien Gaidon, and Tengyu Ma. \"Provable guarantees for self-supervised deep learning with spectral contrastive loss.\" Advances in Neural Information Processing Systems 34 (2021): 5000-5011.\n\nThere are some spelling and grammatical errors, such as \u201cin an self-supervised manner\u201d, \u201cwith an self-supervised one\u201d, \u201cTrainging Algorithm\u201d, \u201cWe denote Y \u2026 be the\u201d, \u201c\u00a0further justification are discussed\u201d, \u201cclaims that maximize\u201d, \u201coptimising \u2026 also maximize\u201d, \u201cproves that maximize\u201d, \u201cShannon diverge\u201d"
                },
                "questions": {
                    "value": "Please position this work and its contributions with respect to the aforementioned papers"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "-"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7529/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698762981875,
            "cdate": 1698762981875,
            "tmdate": 1699636909180,
            "mdate": 1699636909180,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "imhUyDroBS",
                "forum": "y21ZO6M86t",
                "replyto": "gKubVWhiwQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7529/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7529/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer aFan"
                    },
                    "comment": {
                        "value": "Thanks for your valuable and constructive feedback!\n\n**Reply to Weakness**\n\n\n>Q1: Please position this work and its contributions with respect to the aforementioned papers.\n\nA1: Specifically, we include two new baselines SPGCL [1] and HLCL[2], which are GCLs that address heterophily. The results are shown in Appendix E.2 (Table 12, Table 13 and Table 14), which further shows the superiority of PolyGCL.\n\nBesides, we will further discuss the difference between PolyGCL and the GCL methods using spectral augmentations [3, 4]. SpCo [3] studies the necessity of high-frequency information in GCL and learns the optimal augmentation from the spectral view. [4] proposes a set of well-motivated graph transformation operations derived via graph spectral analysis, which are spectral graph cropping and graph frequency components reordering. Compared with [3, 4], PolyGCL does not require specially designed or complex preprocessing steps for spectrum augmentations but achieves the contrast between low-pass and high-pass information by directly optimizing the corresponding decoupled filters.\n\nAs for [5] which also considers the spectral filters in self-supervised settings, we claim that PolyGCL has the ability to learn filters of any shape while [5] only considers a fixed set of polynomial filters, that is $(\\mathbf{I}-\\frac{\\mathbf{L}}{2})^k$ and $(\\frac{\\mathbf{L}}{2})^k$, which restricts its expressiveness.\n\nFrom the theoretical view, [6] proposes the spectral contrastive loss which builds connections between the contrastive loss and the spectral method. In addition, Local-GCL [7] devises a kernelized contrastive loss with linear complexity for GCL, which also shows effectiveness in heterophilic graphs. However, these works related to the graph spectrum mainly focused on the analysis of eigenvalues while PolyGCL cares more about the learning of filter functions to adapt to homophilic/heterophilic settings. \n\nCompared with the above works, to the best of our knowledge, PolyGCL are the first to achieve efficient learning of low-pass and high-pass filters via polynomial approximation in a self-supervised setting. We will further include these works and conduct a more in-depth analysis compared with PolyGCL in the revised version. \n\n>Q2: There are some spelling and grammatical errors.\n\nA2: Thanks for pointing these spelling and grammatical errors out. We will go through the writings carefully in the revised version. \n\nReferences:\n\n[1] Wang, Haonan, et al. \"Can Single-Pass Contrastive Learning Work for Both Homophilic and Heterophilic Graph?.\" _arXiv preprint arXiv:2211.10890_ (2022).\n\n[2] Yang, Wenhan, and Baharan Mirzasoleiman. \"Contrastive Learning under Heterophily.\" _arXiv preprint arXiv:2303.06344_ (2023).\n\n[3] Liu, Nian, et al. \"Revisiting graph contrastive learning from the perspective of graph spectrum.\" _Advances in Neural Information Processing Systems_ 35 (2022): 2972-2983.\n\n[4] Ghose, Amur, et al. \"Spectral Augmentations for Graph Contrastive Learning.\" _International Conference on Artificial Intelligence and Statistics_. PMLR, 2023.\n\n[5] Xie, Xuanting, et al. \"Contrastive graph clustering with adaptive filter.\" _Expert Systems with Applications_ 219 (2023): 119645.\n\n[6] HaoChen, Jeff Z., et al. \"Provable guarantees for self-supervised deep learning with spectral contrastive loss.\" _Advances in Neural Information Processing Systems_ 34 (2021): 5000-5011.\n\n[7] Zhang, Hengrui, et al. \"Localized Contrastive Learning on Graphs.\" _arXiv preprint arXiv:2212.04604_ (2022)."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7529/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700232780802,
                "cdate": 1700232780802,
                "tmdate": 1700232780802,
                "mdate": 1700232780802,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6xGaajp0za",
                "forum": "y21ZO6M86t",
                "replyto": "imhUyDroBS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7529/Reviewer_aFan"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7529/Reviewer_aFan"
                ],
                "content": {
                    "title": {
                        "value": "Acknowledgments"
                    },
                    "comment": {
                        "value": "We thank the authors for the detailed description of the literature, allowing to better position this work within the recent advances."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7529/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700678993802,
                "cdate": 1700678993802,
                "tmdate": 1700678993802,
                "mdate": 1700678993802,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WrhKZGOkS5",
            "forum": "y21ZO6M86t",
            "replyto": "y21ZO6M86t",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7529/Reviewer_Q45Y"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7529/Reviewer_Q45Y"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a spectral domain self-supervised method for graphs based on a linear combination of low-pass and high-pass information while keeping the graph structure intact. Although the idea of the paper is interesting and novel, the evaluation lacks comparison with recent works along the similar direction. Moreover, the paper is missing some key references such as FAGCN (not a self-supervised method but considers high pass and low pass information in a supervised manner)[1], [2] [3]. It lacks explanations concerned with the intuition behind low-pass and high-pass information in the learning process. \n\n\nTheorem 1 is only for regular graphs for binary classification, how can it be a basis of a much stronger claim of having the high pass information in heterophilic datasets?\n\nHave you considered high-pass encoders for the graphs with large heterophily? What is the performance in such a case of alpha = 0? What about when beta = 0?\n\nWhat about using NT-Xent loss as used in GraphCL as compared to the one used in this paper? What is the intuition of low and high frequency information in the context of the global embedding $g$? \n\nWhile reporting the results for graphCL, what exact method did you use? node/edge-drop or a combination of other methods?\n\n[1] Deyu Bo et al. , \"Beyond Low-frequency Information in Graph Convolutional Networks\", AAAI 2021.\n[2] Amur Ghosh et al. , \"Spectral Augmentations for Graph Contrastive Learning,\" AISTATS 2023.\n[3] W Yang, B Mirzasoleiman, \"Graph Contrastive Learning under Heterophily,\" 2023 Arxiv.\n\n\nAfter response:\n\nI appreciate the efforts authors put to clarify my concerns. Thanks. The authors comment on \"k-regular graph\" restricting the eigenvalues of the normalized Laplacian to [0,2] was unsatisfactory. In fact, it is true for all type of graphs. Such loose statements are not good. Moreover, I was looking for for more intuitions about low pass and high pass questions and comparing against the mean representation, just putting some numbers in order to validate the usefulness of the proposed work seems a bit naive. It would help the paper if frequency domain examples are exploited in simple settings, not just in terms of accuracy numbers after putting a classification head. This paper had a great idea, but could have been much better.."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "please see summary."
                },
                "weaknesses": {
                    "value": "please see summary."
                },
                "questions": {
                    "value": "Theorem 1 is only for regular graphs for binary classification, how can it be a basis of a much stronger claim of having the high pass information in heterophilic datasets?\n\nHave you considered high-pass encoders for the graphs with large heterophily? What is the performance in such a case of alpha = 0? What about when beta = 0?\n\nWhat about using NT-Xent loss as used in GraphCL as compared to the one used in this paper? What is the intuition of low and high frequency information in the context of the global embedding $g$? \n\nWhile reporting the results for graphCL, what exact method did you use? node/edge-drop or a combination of other methods?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7529/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7529/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7529/Reviewer_Q45Y"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7529/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699045770452,
            "cdate": 1699045770452,
            "tmdate": 1701040790290,
            "mdate": 1701040790290,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EChrVkX5Km",
                "forum": "y21ZO6M86t",
                "replyto": "WrhKZGOkS5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7529/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7529/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer Q45Y (1)"
                    },
                    "comment": {
                        "value": "Thanks for your insightful feedbacks!\n\n**Reply to Weakness**\n\n> W1: Although the idea of the paper is interesting and novel, the evaluation lacks comparison with recent works along the similar direction. \n\nA1: Following the advice of reviewers, we include SP-GCL[1] and HLCL[2] for further comparison, which are GCLs that address heterophily. We conduct experiments on both synthetic and real-world datasets. The results are summarized in the following tables.\n\n\n| Methods | cora | citeseer | pubmed | cornell | texas | wisconsin | actor | chameleon | squirrel |\n|---------|------|----------|--------|---------|-------|-----------|-------|-----------|----------|\n| SP-GCL  | 82.99 \u00b1 1.18 | 75.54 \u00b1 1.06 | 85.74 \u00b1 0.21 | 69.41 \u00b1 1.49 | 69.76 \u00b1 1.23 | 69.34 \u00b1 0.77 | 35.92 \u00b1 0.67 | 69.23 \u00b1 1.23 | 53.05 \u00b1 1.05 |\n| HLCL    | 85.53 \u00b1 1.03 | 76.79 \u00b1 0.60 | 85.13 \u00b1 0.18 | 64.00 \u00b1 8.98 | 78.38 \u00b1 5.08 | 79.50 \u00b1 4.50 | 40.56 \u00b1 0.70 | 63.86 \u00b1 1.34 | 44.49 \u00b1 0.68 |\n| PolyGCL | 87.57 \u00b1 0.62 | 79.81 \u00b1 0.85 | 87.15 \u00b1 0.27 | 82.62 \u00b1 3.11 | 88.03 \u00b1 1.80 | 85.50 \u00b1 1.88 | 41.15 \u00b1 0.88 | 71.62 \u00b1 0.96 | 56.49 \u00b1 0.72 |\n\n\n| Methods | $\\phi=-1$ | $\\phi=-0.75$ | $\\phi=-0.5$ | $\\phi=-0.25$ | $\\phi=0$ | $\\phi=0.25$ | $\\phi=0.5$ | $\\phi=0.75$ | $\\phi=1$ |\n|---------|------------|--------------|--------------|---------------|-----------|---------------|-----------|---------------|----------|\n| SP-GCL  | 65.82 \u00b1 1.03 | 73.19 \u00b1 0.88 | 68.37 \u00b1 0.89 | 63.72 \u00b1 0.68 | 59.36 \u00b1 0.98 | 73.01 \u00b1 0.51 | 85.52 \u00b1 0.67 | 94.13 \u00b1 0.38 | 88.22 \u00b1 0.49 |\n| HLCL    | 66.03 \u00b1 0.83 | 67.66 \u00b1 0.59 | 70.62 \u00b1 0.63 | 60.80 \u00b1 0.53 | 58.92 \u00b1 0.87 | 65.80 \u00b1 0.40 | 79.25 \u00b1 0.79 | 97.12 \u00b1 0.82 | 93.07 \u00b1 0.80 |\n| PolyGCL | 98.84 \u00b1 0.17 | 94.23 \u00b1 0.31 | 90.82 \u00b1 0.50 | 75.43 \u00b1 0.68 | 66.51 \u00b1 0.69 | 69.43 \u00b1 0.65 | 88.22 \u00b1 0.72 | 98.09 \u00b1 0.29 | 99.29 \u00b1 0.23 |\n\n| Methods | roman-empire | amazon-ratings | minesweeper | tolokers | questions | arxiv-year |\n|---------|--------------|----------------|---------------|-----------|---------------|--------------|\n| SP-GCL  | 63.17 \u00b1 0.22 | 43.11 \u00b1 0.32 | 81.76 \u00b1 0.61 | 80.73 \u00b1 0.62 | 75.08 \u00b1 0.49 | 42.56 \u00b1 0.12 |\n| HLCL    | 67.75 \u00b1 0.19 | 43.92 \u00b1 0.26 | 79.34 \u00b1 0.59 | 78.99 \u00b1 0.67 | 74.92 \u00b1 0.65 | OOM |\n| PolyGCL | 72.97 \u00b1 0.25 | 44.29 \u00b1 0.43 | 86.11 \u00b1 0.43 | 83.73 \u00b1 0.53 | 75.33 \u00b1 0.67 | 43.07 \u00b1 0.23 |\n\n\nAlthough claimed to tackle heterophily, these two methods still suffer from performance drop in extreme heterophilic cSBM settings when $\\phi$ approaches $-1$. In contrast, PolyGCL consistently holds superior performance over the two new baselines on both synthetic and real-world datasets.\n\n>W2: The paper is missing some key references such as FAGCN (not a self-supervised method but considers high pass and low pass information in a supervised manner).\n\nA2: Thanks for your constructive suggestions. \n- As you mentioned, FAGCN [3] considers high-pass and low-pass information in a supervised manner and achieves performance gain in heterophilic graphs. To the best of our knowledge, PolyGCL are the first to achieve efficient learning of low-pass and high-pass filters via polynomial approximation in a self-supervised setting.\n- There are also GCL methods using spectral augmentations [4, 5]. [4] proposes a set of well-motivated graph transformation operations derived via graph spectral analysis, which are spectral graph cropping and graph frequency components reordering. SpCo [5] studies the necessity of high-frequency information in GCL and learns the optimal augmentation from the spectral view. Compared with [4, 5], PolyGCL does not require specially designed or complex preprocessing steps for spectrum augmentations but achieves the contrast between low-pass and high-pass information by directly optimizing the corresponding decoupled filters.\n\nWe will further include these works to conduct a more in-depth analysis compared with PolyGCL in the revised version."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7529/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700231297319,
                "cdate": 1700231297319,
                "tmdate": 1700231297319,
                "mdate": 1700231297319,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "akb1ivcNAY",
                "forum": "y21ZO6M86t",
                "replyto": "WrhKZGOkS5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7529/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7529/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer Q45Y (2)"
                    },
                    "comment": {
                        "value": "**Reply to Questions**\n> Q1: Theorem 1 is only for regular graphs for binary classification, how can it be a basis of a much stronger claim of having the high pass information in heterophilic datasets?\n\nA3: Thanks for pointing this out. We claim that Theorem 1 is indeed a special case with $k$-regular and binary classification constraints. However, we can generalize Theorem 1 from two aspects.\n- **The $k$-regular constraint.** In Theorem 1, $k$-regular graph is imposed to meet the need of normalized Laplacian $\\mathbf{\\tilde{L}}$, which restricts its eigenvalues $\\lambda_i$ in $[0,2]$. In fact, it can be proved without changing the original text too much, and the introduction of $k$-regular graph is just for convenience of explanation. \nEvenNet [6] states that the analysis based on SRL could be easily extended to the general case, where we directly utilize the unnormalized Laplacian $\\mathbf{L}=\\mathbf{D}-\\mathbf{A}$ to substitute the normalized Laplacian $\\mathbf{\\tilde{L}}$. In this case, Lemma 1 in Appendix A.1 can be generalized as Lemma 4 in Appendix E.1. Based on Lemma 4 and the unnormalized $\\mathbf{L}$, Theorem 1 can be generalized to Theorem 6 in Appendix E.1 shown as follows. Note that the slight difference between Theorem 1 and Theorem 6 is that the constant coefficients of the low-pass/high-pass linear functions are different ($\\frac{c}{2}$ and $\\frac{nc}{4m}$ respectively). The detailed proof is presented in Appendix E.1.\n\n\n   **Theorem 6 (generalized version on unnormalized Laplacian $\\mathbf{L}$ )** For a binary node classification task on graph $\\mathcal{G}$ with $n$ nodes and $m$ edges, we consider the linear bounded filter function and the unnormalized Laplacian $\\mathbf{L}$ with $\\lambda^\\prime \\geq 0$, for the low-pass filter $g_{low} = c - \\frac{nc}{4m}\\lambda^\\prime$ and the high-pass filter as $g_{high} = \\frac{nc}{4m}\\lambda^\\prime$, then a linear combination of the low-pass and high-pass filter $g_{joint}=x g_{low} + y g_{high}, x \\geq 0, y\\geq 0, x+y=1$ achieves a lower expected SRL upper bound than $g_{low}$ in heterophilic settings, that is,  $\\mathbb{E}_x[\\hat{L}\\_{joint}] \\leq \\mathbb{E}_h[\\hat{L}\\_{low}]$ for $x\\sim U(0,1),h\\sim U(\\frac{1}{2},1)$, where $\\hat{L}$ denotes the upper bound for $L$. \n\n \n- **Binary classification.** As for the binary classification in heterophilic graphs, there is a series of works based on spectral analysis of heterophily that adopt the two-class setting [6, 7, 8, 9]. Besides, the theoretical analysis in [7] states that the analysis of multi-class cases can be simplified via the \"One vs Others\" reduction, that is, for $C$ class classification task, denoting $\\boldsymbol{y}^{\\prime}_0=\\boldsymbol{y}_0$ and $\\boldsymbol{y}^{\\prime}_1=\\sum\\_{l=1}^{C-1}\\boldsymbol{y}_l$, thus we can transform the multi-class cases into binary classification.\n\nBased on the above discussion, Theorem 1 makes reasonable simplifications and has the potential to be extended to a general form of graph Laplacian or multi-class classification cases, which demonstrate the necessity of introducing high-pass information in heterophilic settings.\n\n>Q2: Have you considered high-pass encoders for the graphs with large heterophily? What is the performance in such a case of $\\alpha= 0$? What about when $\\beta=0$?\n\nA4: We include experiments where $\\alpha$ and $\\beta$ are set to zero separately as the ablation studies. Specifically, $\\alpha=0, \\beta=0$ means only the high-pass/low-pass filter is reserved respectively. The results are given in the following table. \n\n| Methods | cora | citeseer | pubmed | cornell | texas | wisconsin | actor | chameleon | squirrel |\n|---------|-------|-----------|---------|----------|--------|------------|--------|------------|-----------|\n| PolyGCL | 87.57 \u00b1 0.62 | 79.81 \u00b1 0.85 | 87.15 \u00b1 0.27 | 82.62 \u00b1 3.11 | 88.03 \u00b1 1.80 | 85.50 \u00b1 1.88 | 41.15 \u00b1 0.88 | 71.62 \u00b1 0.96 | 56.49 \u00b1 0.72 |\n| PolyGCL($\\alpha=0$) | 70.67 \u00b1 0.48 | 64.22 \u00b1 0.93 | 76.41 \u00b1 0.35 | 80.16 \u00b1 2.62 | 86.52 \u00b1 1.97 | 82.07 \u00b1 2.75 | 38.28 \u00b1 0.39 | 68.21 \u00b1 1.40 | 52.10 \u00b1 0.80 |\n| PolyGCL($\\beta=0$) | 87.65 \u00b1 0.67 | 78.76 \u00b1 0.75 | 86.64 \u00b1 0.17 | 76.23 \u00b1 5.41 | 82.56 \u00b1 2.13 | 68.88 \u00b1 2.50 | 37.36 \u00b1 0.46 | 65.08 \u00b1 1.27 | 48.52 \u00b1 0.71 |\n\nWe observe that the results of PolyGCL($\\alpha=0$) remain comparable in heterophilic datasets, while PolyGCL($\\beta=0$) shows better adaptability to homophilic settings. The results reveal that the high-pass information is indispensable for graphs with large heterophily, and so is the low-pass information for homophilic graphs. The results of this ablation study are further consistent with the low-pass/high-pass preference for homophilic/heterophilic settings in Figure 3 in Section 5.4. Note that PolyGCL achieves better performance when optimized over both the parameters $\\alpha, \\beta$ in most datasets."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7529/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700232292133,
                "cdate": 1700232292133,
                "tmdate": 1700232292133,
                "mdate": 1700232292133,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CK2vn53kn5",
                "forum": "y21ZO6M86t",
                "replyto": "WrhKZGOkS5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7529/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7529/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer Q45Y (3)"
                    },
                    "comment": {
                        "value": ">Q3: What about using NT-Xent loss as used in GraphCL as compared to the one used in this paper? \n\nA5: Thanks for your valuable comments. We consider directly substituting our optimization loss in Equation 4 (Section 4.2) with the NT-Xent loss used in GraphCL [6]. Besides, the augmentation strategies and other settings are aligned with GraphCL. The results are listed in the following table.\n\n| Methods | cora | citeseer | pubmed | cornell | texas | wisconsin | actor | chameleon | squirrel |\n|---------|-------|-----------|---------|----------|--------|------------|--------|------------|-----------|\n| PolyGCL | 87.57 \u00b1 0.62 | 79.81 \u00b1 0.85 | 87.15 \u00b1 0.27 | 82.62 \u00b1 3.11 | 88.03 \u00b1 1.80 | 85.50 \u00b1 1.88 | 41.15 \u00b1 0.88 | 71.62 \u00b1 0.96 | 56.49 \u00b1 0.72 |\n| PolyGCL(NT-Xent) | 84.40 \u00b1 0.93 | 76.83 \u00b1 0.94 | 82.63 \u00b1 0.30 | 81.48 \u00b1 2.46 | 84.43 \u00b1 2.95 | 81.75 \u00b1 3.50 | 38.95 \u00b1 0.81 | 69.17 \u00b1 0.94 | 53.30 \u00b1 1.30 |\n\nWe observe that the results of PolyGCL(NT-Xent) are slightly lower than PolyGCL. This phenomenon can be attributed to the introduction of structural augmentations (e.g., edge-dropping or subgraph-sampling) destroys the spectral properties of the original graph, which is not conducive to the learning of spectral filters in PolyGCL. However, PolyGCL(NT-Xent) still holds competitiveness with other baselines across different homophilic and heterophilic datasets in Table 2 (Section 5.3), which reflects the universality and effectiveness of the PolyGCL framework.\n\n>Q4: It lacks explanations concerned with the intuition behind low-pass and high-pass information in the learning process. What is the intuition of low and high frequency information in the context of the global embedding $\\mathbf{g}$?\n\nA6: We claim the intuition of low and high-frequency information in the context of the global embedding $\\mathbf{g}$ lies in the necessity of performing contrastive learning between the decoupled low-pass and high-pass information in Equation 4.\n\nConsidering that the global summary $\\mathbf{g}$ is defined as $\\mathbf{g}=\\textbf{Mean}(\\mathbf{Z})$, we obtain the final embedding via linear combination as $\\mathbf{Z}=\\alpha \\mathbf{Z}_L+\\beta \\mathbf{Z}_H$, where $\\alpha, \\beta$ are linear coefficients that can be set as learnable parameters.\n\nBased on the above, we conclude that $\\mathbf{g}$ mixes the decoupled low-pass and high-pass embedding via the linear combination strategy, thus $\\mathbf{g}$ contains the low and high-frequency information at the same time in the global embedding. In the learning process of optimizing Equation 4, PolyGCL actually performs contrastive learning between the decoupled low-pass and high-pass embeddings, which mutually boosts the learning of the two branches, and further ensures the final embeddings contain the filtered low-pass and high-pass information to model the graph across different homophily adaptively."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7529/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700232395941,
                "cdate": 1700232395941,
                "tmdate": 1700232395941,
                "mdate": 1700232395941,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qUZgBzpvDu",
                "forum": "y21ZO6M86t",
                "replyto": "WrhKZGOkS5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7529/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7529/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer Q45Y (4)"
                    },
                    "comment": {
                        "value": ">Q5: While reporting the results for graphCL, what exact method did you use? node/edge-drop or a combination of other methods?\n\nA7: Following the settings in GraphCL [6], we utilize four augmentations proposed in the original paper, which are node-dropping, edge perturbation, attribute-masking, and subgraph.\n\nIn detail, as for reporting the results for GraphCL, we conduct experiments based on the above four augmentations and choose the one that achieves the best result for each dataset. For instance, we use subgraph-sampling for cora, citeseer, cornell, texas, wisconsin, chameleon and squirrel, besides, node-dropping and attribute-masking are chosen for pubmed and actor, respectively.\n\nNote that there is also a hyperparameter $p$ to control the percentage of dropping during augmentations, and we conduct a grid search in $\\{0.1, 0.2, ..., 0.9\\}$ to obtain the optimal $p$. All these details will be put into the appendix later.\n\nReferences:\n\n[1] Wang, Haonan, et al. \"Can Single-Pass Contrastive Learning Work for Both Homophilic and Heterophilic Graph?.\" _arXiv preprint arXiv:2211.10890_ (2022).\n\n[2] Yang, Wenhan, and Baharan Mirzasoleiman. \"Contrastive Learning under Heterophily.\" _arXiv preprint arXiv:2303.06344_ (2023).\n\n[3] Bo, Deyu, et al. \"Beyond low-frequency information in graph convolutional networks.\" _Proceedings of the AAAI Conference on Artificial Intelligence_. Vol. 35. No. 5. 2021.\n\n[4] Ghose, Amur, et al. \"Spectral Augmentations for Graph Contrastive Learning.\" _International Conference on Artificial Intelligence and Statistics_. PMLR, 2023.\n\n[5] Liu, Nian, et al. \"Revisiting graph contrastive learning from the perspective of graph spectrum.\" _Advances in Neural Information Processing Systems_ 35 (2022): 2972-2983.\n\n[6] Lei, Runlin, et al. \"Evennet: Ignoring odd-hop neighbors improves robustness of graph neural networks.\" _Advances in Neural Information Processing Systems_ 35 (2022): 4694-4706.\n\n[7] Chen, Zhixian, Tengfei Ma, and Yang Wang. \"When Does A Spectral Graph Neural Network Fail in Node Classification?.\" _arXiv preprint arXiv:2202.07902_ (2022).\n\n[8] Ma, Yao, et al. \"Is homophily a necessity for graph neural networks?.\" _arXiv preprint arXiv:2106.06134_ (2021).\n\n[9] Luan, Sitao, et al. \"When do graph neural networks help with node classification: Investigating the homophily principle on node distinguishability.\" _arXiv preprint arXiv:2304.14274_ (2023).\n\n[10] You, Yuning, et al. \"Graph contrastive learning with augmentations.\" _Advances in neural information processing systems_ 33 (2020): 5812-5823."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7529/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700232496822,
                "cdate": 1700232496822,
                "tmdate": 1700232496822,
                "mdate": 1700232496822,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pDrSCk2Emp",
                "forum": "y21ZO6M86t",
                "replyto": "WrhKZGOkS5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7529/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7529/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Discussion period ending soon; We would like to hear back from Reviewer Q45Y"
                    },
                    "comment": {
                        "value": "Dear Reviewer Q45Y,\n\nWe gratefully appreciate your time in reviewing our paper. Since the discussion period ends soon, we sincerely hope our rebuttal has carefully addressed your comments point-by-point. In particular, we positioned our work within some key related works and compared PolyGCL with additional baselines, as suggested by you. Besides, We generalized Theorem 1 to avoid the aforementioned limitations and clarified the intuition behind low-pass and high-pass information in the learning process. Additional experimental results and ablations are also provided to conduct an in-depth analysis of PolyGCL. If you have any other comments or questions, we will be glad to answer them and continue the conversation.\n\nThank you for your time and attention to this matter.\n\nBest regards,\n\nThe Authors of Submission 7529"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7529/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700622297193,
                "cdate": 1700622297193,
                "tmdate": 1700622297193,
                "mdate": 1700622297193,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WKj1jQxXZ2",
                "forum": "y21ZO6M86t",
                "replyto": "WrhKZGOkS5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7529/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7529/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A Kind Reminder: the author-reviewer discussion period is coming to an end."
                    },
                    "comment": {
                        "value": "Dear reviewer Q45Y,\n\nWe sincerely appreciate your thorough and detailed reviews of our submission. We hope that you will find our responses satisfactory and that they help clarify your concerns. We appreciate the opportunity to engage with you. We would like to kindly remind you that the discussion period is coming to an end. Could you please inform us whether our responses have addressed your concerns or if there are any other questions you need us to clarify? \n\nThank you very much for your time.\n\nBest regards,\n\nAuthors of Submission 7529"
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7529/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700717325647,
                "cdate": 1700717325647,
                "tmdate": 1700717325647,
                "mdate": 1700717325647,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]