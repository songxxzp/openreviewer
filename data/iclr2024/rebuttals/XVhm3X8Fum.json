[
    {
        "title": "Stack Attention: Improving the Ability of Transformers to Model Hierarchical Patterns"
    },
    {
        "review": {
            "id": "arLLsW5sCB",
            "forum": "XVhm3X8Fum",
            "replyto": "XVhm3X8Fum",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6568/Reviewer_bkRf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6568/Reviewer_bkRf"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the problem of the lack of modeling the hierarchical structure of input sequences in transformers. In response, a new attention mechanism, called stack attention, is proposed. The idea is to frame the sequence modeling problem as a task of running a PDA on the input vectors; that is, each time we take a vector as input, we update the stack of the PDA and produce a new vector as output. In this way, the sequence of input vectors is represented as a sequence of output vectors. The entire process is based on differentiable states and operations. Thus, this attention model operates in the same manner as those in sequence modeling, making it easy to incorporate the model into transformers. However, as a side effect, stack attention introduces recurrence into modeling, thus preventing training from being parallelized across the sequence. The stack attention models are tested on synthetic data generated by context-free grammars. Experimental results show that nondeterministic stack attention models surpass standard transformers and achieve better results than a strong nondeterministic stack RNN baseline. The stack attention models also show promising results on small-scale machine translation and language modeling tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "I like this work! Given that language structure is not explicitly modeled in current Transformer models, this work opens a door to a new approach to considering hierarchical patterns in modeling languages. The design of the model is simple and elegant. The experiments support the claims well."
                },
                "weaknesses": {
                    "value": "I have no major concerns, but a few comments.\n\nThe state of the stack attention models at a given timestep depends on its past state. This means the models share similar drawbacks and merits with recurrent models like RNNs. Compared to the self-attention used in standard Transformers, stack attention is slower for training because it processes one token at a time, rather than parallelizing the encoding process over the entire sequence. The author states in the appendix that this could be improved using the parallel prefix sum method, but no details are presented.\n\nA related problem is that the experiments here are small-scale. While it's fine to test the models on synthetic data for CFL tasks, the results on language modeling and machine translation aren't comparable to those in other papers. I understand that computational cost is a concern. However, to demonstrate the superiority of stack attention, it's necessary to compare it with previously reported results under the same setup.\n\nI\u2019m not quite satisfied that the models are motivated by handling hierarchical structure behind languages but there is no discussion on what structure is captured. A simple way to examine this is to design probing tasks to see how much syntax is modeled in stack attention and to see how the learned syntax differs from human-annotated syntax. Unsupervised learning of syntactic structures can offer new insights into modeling natural languages.\"\n\nThere have been previous studies on extending standard attention models to hierarchical models, such as hierarchical attention and selective attention. These should be considered baselines for comparison, either in related work or experiments."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6568/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698591466746,
            "cdate": 1698591466746,
            "tmdate": 1699636744474,
            "mdate": 1699636744474,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "agmFeXBSwQ",
                "forum": "XVhm3X8Fum",
                "replyto": "arLLsW5sCB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6568/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6568/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer bkRf,\n\nThank you very much for your review and your many thoughtful comments. We are glad that you liked the paper! We greatly appreciate your suggestions for improving it further.\n\n> The state of the stack attention models at a given timestep depends on its past state. This means the models share similar drawbacks and merits with recurrent models like RNNs. Compared to the self-attention used in standard Transformers, stack attention is slower for training because it processes one token at a time, rather than parallelizing the encoding process over the entire sequence. The author states in the appendix that this could be improved using the parallel prefix sum method, but no details are presented.\n\nYour comments about the recurrence introduced by the stack are exactly correct. Actually, stack attention, despite having a recurrence across the timestep dimension, can still parallelize the stack computation in a way that is impossible in an RNN under the controller-stack paradigm used by Grefenstette et al. (2015), Joulin and Mikolov (2015), DuSell and Chiang (2020), etc. In an RNN, the stack actions for timestep $t$ depend on the hidden state at timestep $t$. Since the hidden state computation cannot be parallelized across $t$, neither can the stack. However, in a transformer with stack attention, the stack actions $\\mathbf{a}_t$ and pushed vectors $\\mathbf{v}_t$ for all timesteps are computed in parallel and are available all at once. This means we can take advantage of algorithms that parallelize the stack computation over the timestep dimension, such as parallel CKY. (Apologies, we realized that the algorithm for parallelizing nondeterministic stack attention across the timestep dimension would resemble parallel CKY, not parallel prefix sum as stated in Appendix B. We will update Appendix B.)\n\nFor nondeterministic stack attention, the algorithm would work by updating the dynamic programming table (see Appendix A) in increasing order of span size, then parallelizing the inner loops that implement the replace and pop rules. This would reduce the runtime to $O(n)$ parallel time. As for the superposition stack, each recurrent step runs in $O(1)$ parallel time under our current implementation, so it already runs in $O(n)$ parallel time.\n\n> I\u2019m not quite satisfied that the models are motivated by handling hierarchical structure behind languages but there is no discussion on what structure is captured. A simple way to examine this is to design probing tasks to see how much syntax is modeled in stack attention and to see how the learned syntax differs from human-annotated syntax. Unsupervised learning of syntactic structures can offer new insights into modeling natural languages.\"\n\nThank you for raising this point. We agree that analyzing the parses learned by stack attention would be extremely informative and would further validate our method. If we have time, we will update our response with an analysis of how the networks learn to use stack attention.\n\n> There have been previous studies on extending standard attention models to hierarchical models, such as hierarchical attention and selective attention. These should be considered baselines for comparison, either in related work or experiments.\n\nThank you for this suggestion. To clarify, are you referring to these papers?\n\n* Hierarchical Attention Networks for Document Classification (Yang et al.)\n* Selective Attention for Context-aware Neural Machine Translation (Maruf et al.)"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6568/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700154027424,
                "cdate": 1700154027424,
                "tmdate": 1700224268517,
                "mdate": 1700224268517,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xlihTniyYy",
                "forum": "XVhm3X8Fum",
                "replyto": "agmFeXBSwQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6568/Reviewer_bkRf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6568/Reviewer_bkRf"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response, it addresses my concerns. Yes, the references you provided are exactly what I meant."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6568/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700315585414,
                "cdate": 1700315585414,
                "tmdate": 1700315585414,
                "mdate": 1700315585414,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "AXUa23JrlN",
            "forum": "XVhm3X8Fum",
            "replyto": "XVhm3X8Fum",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6568/Reviewer_knG9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6568/Reviewer_knG9"
            ],
            "content": {
                "summary": {
                    "value": "The authors present work on stack attention for transformers that attempts to address naturally hierarchically structured problems. They carefully present scaled dot-product attention, a core component of the transformer architecture. Next, they provide background on differentiable stacks, superposition stacks, and a non-deterministic stack -- the differentiable vector push-down automaton. Superposition stacks can be seen as a special case of this. This is used to replace scaled dot-product attention.\n\nThey explore the empirical performance of this approach on a range of tasks, including constructed languages, a small scale language modeling problem, and a small scale machine translation effort. In some settings, the Tf-Nd configuration outperforms a conventional transformer architecture. However, these gains are not huge, the datasets are small, and (in machine translation) as the dimensionality increases, the architecture no longer shows gains."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The authors carefully present their formalism, including details and relevance connections to prior work.\n\nThe description of the method stands alone reasonably well, though familiarity with related work makes the paper much more accessible.\n\nThe authors evaluate in a range of settings, from constructed language to actual use cases.\n\nThe methodology does not rely on a specific grammar or automaton structure; instead it is latent."
                },
                "weaknesses": {
                    "value": "The evaluation settings are quite small by modern standards.\n\nGains (at least in machine translation) seem to disappear as model size increases.\n\nComputational cost at inference seems greater according to the big-O runtimes -- is this a net win? Is it feasible to train at larger scale?"
                },
                "questions": {
                    "value": "What is the empirical cost of running these methods?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "n/a"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6568/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698821714363,
            "cdate": 1698821714363,
            "tmdate": 1699636744360,
            "mdate": 1699636744360,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bWb47Wmtjb",
                "forum": "XVhm3X8Fum",
                "replyto": "AXUa23JrlN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6568/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6568/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer knG9,\n\nThank you very much for your thoughtful review and positive comments.\n\n> The evaluation settings are quite small by modern standards.\n\n> Gains (at least in machine translation) seem to disappear as model size increases.\n\nIt's true that the models and datasets are relatively small compared to some other papers, but we believe our experiments demonstrate an important point: that transformers with a latent model of syntax are more expressive than standard transformers (as shown by the results on synthetic CFLs such as the hardest CFL), and that they can be more parameter- and data-efficient than standard transformers when the data can be explained with underlying hierarchical rules (as shown by the language modeling results).\n\n> Computational cost at inference seems greater according to the big-O runtimes -- is this a net win? Is it feasible to train at larger scale?\n\n> What is the empirical cost of running these methods?\n\nWe discuss the computational cost of stack attention in detail in Appendix B, including wall-clock runtimes on the PTB language modeling task and asymptotic time/space complexity. With our current implementation, large-scale training is probably within reach for superposition stack attention but not yet for nondeterministic stack attention; superposition stack attention trains at 40\\% the speed of standard attention, whereas nondeterministic stack attention trains at about 3\\% speed. However, we can likely improve the runtime of nondeterministic stack attention substantially by parallelizing the dVPDA computation across the timestep dimension using a strategy similar to that of parallel CKY, which would reduce it to only $O(n)$ parallel time. We can also get a further, smaller speedup by parallelizing the computation of the stack reading $\\mathbf{r}_t$ from $\\mathcal{S}_t$."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6568/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700153833178,
                "cdate": 1700153833178,
                "tmdate": 1700153833178,
                "mdate": 1700153833178,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "asOvuWvqk6",
            "forum": "XVhm3X8Fum",
            "replyto": "XVhm3X8Fum",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6568/Reviewer_ecSr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6568/Reviewer_ecSr"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a stack-augmented transformer to help overcome some of the challenges of ordinary self-attention in modeling nested syntactic structure. \n\nThe overall approach is to propose two different stack mechanisms: the \"superposition stack\" which is an extension of Joulin and Mikolov (2015), and a second non-deterministic diferentiable vector pushdown automata (dVPDA). While the details for the dVPDA are not very clear from the methods section (see Weaknesses and questions), both the dVPDA and the superposition stack provide soft-stack vector readouts at each time-step, and can be used to replace standard attention. \n\nFrom results, we find:\n- On formal languages such as Dyck, 5 layer transformers with the stack based attention (coming from the superposition stack) are better than ordinary 5 layer transformers (though both of these are worse than LSTM variants). \n- On language modeling over the penn treebank, we find that transformers + dVPDA obtain lower perplexities than ordinary transformers (though transformers + superposition stack are much worse). \n- On a 5 layer machine translation dataset, we find slight very mixed improvements in BLEU."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The motivation behind this paper is great - there are clear limitations of self-attention w.r.t. modeling syntactic patterns and here, we see a novel approach to use a stack to model such patterns."
                },
                "weaknesses": {
                    "value": "- Unfortunately, I think the results are very mixed, experiments are done on very small 5 layer transformers on small datasets.\n- Even for the positive results, there is no analysis of why the stack augmented model may be doing better on natural language - is it discovering good parses / something else?\n\n-  The exposition is very confusing, and i'm a bit lost on various details. The biggest missing detail is how training is done in parallel - from Eq 8, 9 ..., 17 and Figure-1 it seems like there is a stack state that is recurrently updated, but transformer training is fully parallel, so how can state information be passed between different tokens? Is this done by basically reconstructing the previous state of the stack since all previous actions are available to the model at each time step - if so what is the FLOP hit from doing this?"
                },
                "questions": {
                    "value": "I'll intersperse questions (Q) with some suggestions for improving writing (S)\n\nIntroduction:\n\nQ1: \"Recent work has shown that transformers have linear rather than hierarchical..\": Is this true? Murty 2023 (\"Grokking\") find that transformers acquire hierarchical bias when trained for long. It would be better to qualify this statement somewhat.\n\nS1: \"on a natural language modeling benchmark\"  => would be better to just say \"Penn TreeBank\". In general, I found the last para to be very vague. It would be better to have concrete numbers and dataset names.\n\nRelated Work:\n\nS2: Missing several keys papers (Ordered Memory, Unsupervised Tree LSTMs, RL-SPINN) and early works on incorporating stack mechanism into transformers (Das, 1993). This is just a quick list, but there is a long history of learning syntax unsupervisedly / augmenting neural models with stacks that is completely missing.\n\nBackground Section 3.2.1:\n\nQ3: This method seems like it would run faster than the method from 3.2.2. Could the authors confirm this?\n\nQ4: In natural language, one might want to do multiple reduce operations after emitting a word. How does this approach allow for multiple reduces at each time step? \n\nBackground Section 3.2.2:\n\nS3: Definition 1 and the next paragraph take too much space and seem like background that could be in an appendix. \n\nQ5: Missing detail: What is the time complexity of Eq. 17 -  Please include pseudocode here.\n\nQ6: \"Each $a_t$ is a flattening of tensor...\": How was the size of the tensor $\\Delta_t$ computed here?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6568/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699125678114,
            "cdate": 1699125678114,
            "tmdate": 1699636744142,
            "mdate": 1699636744142,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "u6daWOJOzF",
                "forum": "XVhm3X8Fum",
                "replyto": "asOvuWvqk6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6568/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6568/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer ecSr,\n\nThank you very much for your thoughtful review. We greatly appreciate your constructive comments and your suggestions for improving the clarity of the paper.\n\n> Unfortunately, I think the results are very mixed, experiments are done on very small 5 layer transformers on small datasets.\n\nIt's true that the models and datasets are relatively small compared to some other papers, but we believe our experiments demonstrate an important point: that transformers with a latent model of syntax are more expressive than standard transformers (as shown by the results on synthetic CFLs such as the hardest CFL), and that they can be more parameter- and data-efficient than standard transformers when the data can be explained with underlying hierarchical rules (as shown by the language modeling results).\n\n> Even for the positive results, there is no analysis of why the stack augmented model may be doing better on natural language - is it discovering good parses / something else?\n\nThank you for raising this point. We agree that analyzing the parses learned by stack attention would be extremely informative and would further validate our method. If we have time, we will update our response with an analysis of how the networks learn to use stack attention.\n\n> The biggest missing detail is how training is done in parallel ...\n\nYou are correct that the stack state $\\mathcal{S}_t$ is updated recurrently. In our implementation, we simply compute $\\mathcal{S}_1, \\mathcal{S}_2, \\ldots$ serially and compute the other parts of the transformer network in parallel as usual. Consider Figure 1, which depicts a single stack attention sublayer: we first compute all of the inputs $\\mathbf{x}_t$ and apply layer norm in parallel, then we compute the stack states $\\mathcal{S}_t$ and their stack readings $\\mathbf{r}_t$ serially, and finally we apply dropout and residual connections in parallel to compute the outputs $\\mathbf{y}_t$. Although our implementation does not currently do so, it would also be possible to parallelize the computation of $\\mathbf{r}_t$ from $\\mathcal{S}_t$, which would result in a small speedup for nondeterministic stack attention.\n\nDespite the recurrence of $\\mathcal{S}_t$, in the case of nondeterministic stack attention, it is still possible to parallelize the computation across the timestep dimension using a strategy similar to that of parallel CKY parsing, which would reduce it to $O(n)$ parallel time and likely result in a substantial speedup over our current implementation.\n\nQ1: Thank you for pointing this out; this is indeed a very relevant finding. We will update this statement in our paper. It is interesting to note that although training for longer does improve hierarchical generalization by a lot, vanilla transformers still fall 8 to 20 percent short of perfect on question formation and tense reinflection, indicating there is still room for improvement.\n\nS1: We will follow this suggestion. Thank you!\n\nS2: Thank you for these references. Incorporating stacks and latent syntax into neural networks has become a very long line of work, so we limited our citations specifically to syntax-oriented transformers (as opposed to RNNs), and RNNs that use the differentiable stacks that we use in this paper. If we have space, we will include more discussion of other work.\n\nQ3: Yes, superposition stack attention is faster than nondeterministic stack attention. For details, please see Appendix B.\n\nQ4: Superposition stack attention does not appear to have a mechanism for doing this. As discussed in Section 4, superposition stack attention is very likely less expressive than nondeterministic stack attention, which is at least weakly equivalent to any PDA that performs multiple reductions per timestep, since it can recognize all CFLs (DuSell and Chiang, 2023).\n\nS3: Thank you; we will consider ways to make this more concise while still establishing the essential notation used in the paper.\n\nQ5: The purpose of Equation 17 is to define the output of the dVPDA mathematically in a way that is concise, if inefficient. If implemented naively, it would take time exponential in $n$, because $\\pi$ loops over sequences of length $n$ in the denominator. However, it is possible to implement this equation in cubic time and quadratic space using a dynamic programming algorithm called Lang's algorithm (this is the same time/space complexity as parsing algorithms for context-free grammars, such as CKY). Since Equation 17 is sufficient for understanding the dVPDA mathematically and the implementation details are somewhat complex, we put them in the appendix to save space and to reduce the cognitive burden on the reader. For the full implementation details of Equation 17, please see Appendix A, and for a discussion of its time complexity, please see Appendix B.\n\n(continued below)"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6568/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700153470007,
                "cdate": 1700153470007,
                "tmdate": 1700153470007,
                "mdate": 1700153470007,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "X2OCmn8uHd",
                "forum": "XVhm3X8Fum",
                "replyto": "o3959LbwHF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6568/Reviewer_ecSr"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6568/Reviewer_ecSr"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the clarifications! I agree that exploring new models that allow transformers to explictly model syntax unsupervisedly is a great motivation. Just a couple of followups:\n\n- Re: parallel vs recurrent, based on your explanation, it appears that only layers below the first stack attention layer are parallelizable while the rest of the model is still recurrent? I would also still like to know the FLOP hit from this recurrent computation. And additionally, could you also please confirm the addition parameters that are added in stack attention? \n\n- The superposition stack works much better than the non-deterministic stack (both in terms of speed and results on natural language data) but as you said, it is unable to make multiple reduces at each token. Does this then mean that it cannot produce natural language syntax trees?"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6568/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700257017004,
                "cdate": 1700257017004,
                "tmdate": 1700257017004,
                "mdate": 1700257017004,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VyLuXjtzXU",
                "forum": "XVhm3X8Fum",
                "replyto": "asOvuWvqk6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6568/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6568/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Re: parallel vs recurrent, based on your explanation, it appears that only layers below the first stack attention layer are parallelizable while the rest of the model is still recurrent?\n\nThe layers after stack attention remain parallelizable as well. The procedure is essentially:\n\n1. compute all layers before stack attention in parallel to get $\\mathbf{a}_t$ and $\\mathbf{v}_t$ for all $t$\n2. run stack attention serially to get $\\mathbf{r}_t$ for all $t$\n3. now that we have all $\\mathbf{r}_t$, compute all $\\mathbf{y}_t$ (see Figure 1) and all layers after stack attention in parallel\n\n> I would also still like to know the FLOP hit from this recurrent computation.\n\nWe have not yet worked out the implementation details of the parallelized version, so at this time it's difficult to quantify the difference in FLOPs. In terms of time complexity, the current implementation runs in $O(n^2)$ parallel time, whereas the parallel CKY-style version would run in $O(n)$ parallel time.\n\n> And additionally, could you also please confirm the addition parameters that are added in stack attention?\n\nThe SDPA sublayer function uses the parameters $\\mathbf{W}\\_{\\mathrm{q}}, \\mathbf{W}\\_{\\mathrm{k}} \\in \\mathbb{R}^{d\\_k \\times d\\_{\\mathrm{model}}}$, $\\mathbf{W}\\_{\\mathrm{v}} \\in \\mathbb{R}^{d_v \\times d\\_{\\mathrm{model}}}$, and $\\mathbf{W}\\_{\\mathrm{y}} \\in \\mathbb{R}^{d\\_{\\mathrm{model}} \\times d\\_v}$. Stack attention replaces these with the parameters $\\mathbf{W}\\_{\\mathrm{a}} \\in \\mathbb{R}^{d\\_a \\times d\\_{\\mathrm{model}}}$, $\\mathbf{W}\\_{\\mathrm{v}} \\in \\mathbb{R}^{m \\times d\\_{\\mathrm{model}}}$, and $\\mathbf{W}\\_{\\mathrm{y}} \\in \\mathbb{R}^{d\\_{\\mathrm{model}} \\times d\\_r}$. Nondeterministic stack attention additionally adds $\\mathbf{w}\\_{\\mathrm{v}} \\in \\mathbb{R}^m$, which is used for learning the initial bottom vector $\\mathbf{v}\\_0$. In the case of superposition stack attention, $d\\_a = 3$ and $d\\_r = m$. In the case of nondeterministic stack attention, $d\\_a = |Q| \\cdot |\\Gamma| \\cdot |Q| \\cdot (2|\\Gamma| + 1)$ and $d\\_r = |Q| \\cdot |\\Gamma| \\cdot m$.\n\n> The superposition stack works much better than the non-deterministic stack (both in terms of speed and results on natural language data) but as you said, it is unable to make multiple reduces at each token. Does this then mean that it cannot produce natural language syntax trees?\n\nYou are correct that because of superposition stack attention's inability to perform multiple reductions per timestep, we expect that it would not be effective at modeling certain syntactic patterns that require closing multiple constituents at once, such as prepositional phrase attachment. However, it should still be advantageous for a subset of syntactic patterns where this is not an issue, such as center embedding (e.g. \"The diamond that the thief who the man saw stole glittered\") and gross syntactic expectation (e.g. an independent clause must come after a subordinate clause beginning with \"As\"). Note that although superposition stack attention does perform better than nondeterministic stack attention on our MT task, nondeterministic stack attention has better perplexity on language modeling. We think this is because of the difference in expressivity."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6568/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700397158347,
                "cdate": 1700397158347,
                "tmdate": 1700472905038,
                "mdate": 1700472905038,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]