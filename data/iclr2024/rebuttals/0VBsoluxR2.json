[
    {
        "title": "MOFDiff: Coarse-grained Diffusion for Metal-Organic Framework Design"
    },
    {
        "review": {
            "id": "xxSdaIvfBR",
            "forum": "0VBsoluxR2",
            "replyto": "0VBsoluxR2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4437/Reviewer_1BzN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4437/Reviewer_1BzN"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a coarse grained diffusion model for generating metal organic frameworks. Coarse structures are generated by a diffusion model, after which 3d atomic structures are assembled, oriented using a heuristic optimization procedure, and optimized using a force field. The coarse grained building blocks are represented using a learned (contrastive) representation. The generative model is evaluated on an inverse design problem using GCMC simulation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "For the most part, the paper is easy to follow. There are several illustrative figures that provide a good overview\n\nThe paper addresses a very important and difficult problem, and the proposed method is sensible and (at least to some degree) effective.\n\nThere are several technical novelties, including the particular way the coarse graining and reassembly is conducted.\n\nThe method is validated with GCMC simulation."
                },
                "weaknesses": {
                    "value": "There are many \"moving parts\" in the modeling pipeline. It would be enlightening with more ablation studies or comparisons of different modeling choices to yield some insight into the sensitivity to the different modeling choices.\n\nSome technical details are not described in detail. The paper builds on several existing techniques, which are referenced but not described technically. This is just a minor point, but in my view the paper could perhaps be more self contained."
                },
                "questions": {
                    "value": "In the coarse grained structure, are building blocks only represented using their identity and position? Did you consider including other features such as their orientation?\n\nHow sensitive is the representation learning to the choice of fingerprint? Is ECFP4 the only fingerprint you have considered, and why did you choose that?\n\nAbstract: What does \"predicting scores in E(3)\" mean? At this point it is not clear which scores this refers to.\n\n\"small geometric variations in 3D orientation\": I assume \"orientation\" here means something other than rotation/tranlation?\n\nWould there be room to include a short technical description of the metal-oxo / MOFid algorithms that are used? Similarly, a sentence that technically describes ECFP4?\n\nCould you include a very brief technical description of the gemnet-oc architecture?\n\nThe first paragraph in section 3 is difficult to read - would you consider including a \"concept figure\" that outlines the overall training + generative process?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4437/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697618868745,
            "cdate": 1697618868745,
            "tmdate": 1699636418641,
            "mdate": 1699636418641,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "AzwG5dAIob",
                "forum": "0VBsoluxR2",
                "replyto": "xxSdaIvfBR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4437/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4437/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response part 1"
                    },
                    "comment": {
                        "value": "We thank reviewer 1BzN for helpful feedback and comments. We address each of the reviewer\u2019s concerns below.\n\n> There are many \"moving parts\" in the modeling pipeline. It would be enlightening with more ablation studies or comparisons of different modeling choices to yield some insight into the sensitivity to the different modeling choices.\n\nWe appreciate your feedback regarding the complexity of our modeling pipeline. To provide a more comprehensive comparison, we elaborate on some early experiments and decision-making:\n\n- We experimented with an autoencoder-based approach for building block embedding. However, this method significantly underperformed compared to the contrastive learning embedding technique we ultimately employed.\n\n- We also explored incorporating orientation into the building block representation and experimented with learning orientation diffusion. This approach, however, yielded significantly inferior results compared to our current assembly algorithm. More details on this will be provided in our subsequent response.\n\nWe are now exploring alternative building block identity other than ECFP4. We will update the results as they finish. We are happy to further discuss what other comparison/analysis might be interesting to include. \n\n> Some technical details are not described in detail. The paper builds on several existing techniques, which are referenced but not described technically. This is just a minor point, but in my view the paper could perhaps be more self contained.\n\nThank you for the concrete feedback. **We have included a description of the metal-oxo/MOFid algorithms, the ECFP4 fingerprint, and the GemNet-OC architecture in the paper in Appendix B.3 (page 20).** We welcome further feedback on this section.\n\n> In the coarse grained structure, are building blocks only represented using their identity and position? Did you consider including other features such as their orientation?\n\nThank you for the insightful question. Yes, in the coarse grained structure, the building blocks are only represented using their identities and positions. In early experiments, we have also attempted to include the orientation as part of the building block representation and diffusing the orientation of the building blocks based on SO(3)-diffusion [1]. However, we could not get good performance. The trained model is unable to recover accurate orientation through the reverse diffusion process. We believe this is due to the orientation of building blocks being ill-defined. In our failed attempts, we use principal component analysis to obtain the canonical orientation of the building blocks. Unlike amino acids, which have a natural definition of orientation based on the $N - C_\\alpha - C$ atoms [1], the geometry of the building blocks is much more diverse. There exist many almost-2D or near-isotropic building blocks which makes the orientation ambiguous. Further, the layout of the building blocks is delicate and requires accurate alignment between the building blocks to render a valid MOF. \n\n> How sensitive is the representation learning to the choice of fingerprint? Is ECFP4 the only fingerprint you have considered, and why did you choose that?\n\nIn the current paper, we have only considered the ECFP4 fingerprint because of its simplicity, popularity, and satisfying results. We agree with the reviewer the choice for building block identity is worth further consideration. As also suggested by reviewer yqBG, we plan to experiment with learned identity for building block representation. We will update the results as they finish.\n\n> Abstract: What does \"predicting scores in E(3)\" mean? At this point it is not clear which scores this refers to.\n\nWe are sorry for the unclarity. We have revised the abstract in the updated version.\n\n> \"small geometric variations in 3D orientation\": I assume \"orientation\" here means something other than rotation/tranlation?\n\nThanks for pointing out this unclarity. Yes, since the GemNet-OC encoder is SE(3) invariant, the building block embedding is invariant to global translation/rotation. We have revised this sentence in the updated version.\n\n> Would there be room to include a short technical description of the metal-oxo / MOFid algorithms that are used? Similarly, a sentence that technically describes ECFP4?\n\nWe have included a short description of the metal-oxo / MOFid algorithms and ECFP4 in Appendix B.3.\n\n> Could you include a very brief technical description of the gemnet-oc architecture?\n\nWe have included a brief description of the GemNet-OC architecture in Appendix B.3."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4437/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700161992359,
                "cdate": 1700161992359,
                "tmdate": 1700258482482,
                "mdate": 1700258482482,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3XBYq8FFHB",
                "forum": "0VBsoluxR2",
                "replyto": "xxSdaIvfBR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4437/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4437/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response part 2"
                    },
                    "comment": {
                        "value": "> The first paragraph in section 3 is difficult to read - would you consider including a \"concept figure\" that outlines the overall training + generative process?\n\nThank you for the great suggestion. **We have added Figure 11 to illustrate the training and sampling processes**. It is currently placed in Appendix B (page 18) due to the page limitation. We welcome further feedback on the new figure.\n\nWe look forward to further discussions if you have additional questions or suggestions.\n\n\nReference:\n\n[1] Yim, Jason, et al. \"SE (3) diffusion model with application to protein backbone generation.\" arXiv preprint arXiv:2302.02277 (2023)."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4437/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700162069546,
                "cdate": 1700162069546,
                "tmdate": 1700162560307,
                "mdate": 1700162560307,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "o29SpBUTNI",
            "forum": "0VBsoluxR2",
            "replyto": "0VBsoluxR2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4437/Reviewer_yqBG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4437/Reviewer_yqBG"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a novel method for generating metal-organic frameworks (MOFs) using a coarse-grained diffusion model called MOFDiff. MOFDiff employs a coarse-grained (CG) representation derived from MOF building blocks. A graph neural network (GNN) encoder trained via contrastive learning is used to map these building blocks into a latent space (z). MOFDiff consists of four main components: \n1. A periodic contrastive GNN encoder for latent embedding\n2. A MLP for lattice parameter (L) and building block (K) count prediction\n3. A periodic GNN denoiser for diffusion-based MOF generation\n4. Finally another MLP for MOF property prediction e.g., CO2 capacity. \n\nThe denoising diffusion process consists of two steps, first on the CG based building block types and then on their 3D coordinates. Then the all atom structure is recovered using an assembly algorithm, followed by simple force field (UFF) based relaxation. MOFDiff is trained on a dataset of ~304k MOFs and shows capability in generating valid and diverse MOFs. Overall, this paper leverages a diffusion model over a CG representation of MOFs to efficiently generate complex new structures, with proven efficacy in carbon capture applications."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The CG representation of building blocks for the diffusion model makes MOFDiff computationally efficient to train and sample new MOFs. The all atom  \n\n2. Underlying physical symmetries are appropriately handled with contrastively trained GNN encoder for the building blocks\n\n3. Optimizing MOFs in the latent space rather than decoding repeatedly is an efficient inverse design strategy demonstrated in the paper."
                },
                "weaknesses": {
                    "value": "1. As acknowledged in the paper, the validity of the MOFs decreases as number of building blocks, this could limit applicability to larger and more complex MOFs.\n\n2. The authors use a MOF dataset with less than <20 building blocks. But the paper doesn't discuss about size extensivity of the GNN encoder and denoising module in detail.  \n\n3. There is limited discussion on the synthetic accessibility of the designed MOFs. Finally, the building blocks are constrained to a certain set  to allow coarse graining, this may led to less diverse MOFs."
                },
                "questions": {
                    "value": "1. Why was the UFF chosen as force field? Would a MOF specific force field result in better structures? \n\n2. ECFP4 was chosen as similarity measure for contrastive learning. Would a better similarity measure e.g., learned embeddings (ChemBERTa) result in better GNN encoder? \n\n3. How difficult would it be extend the current framework to allow novel building blocks? \n\n\n### Corrections:\n\n* On page 19, there is a mismatch between Py-G and pytorch references."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4437/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4437/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4437/Reviewer_yqBG"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4437/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698756480138,
            "cdate": 1698756480138,
            "tmdate": 1699636418490,
            "mdate": 1699636418490,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "I9CWzp5GDR",
                "forum": "0VBsoluxR2",
                "replyto": "o29SpBUTNI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4437/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4437/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response part 1"
                    },
                    "comment": {
                        "value": "We thank reviewer yqBG for helpful feedback and comments. We address each of the reviewer\u2019s concerns below.\n\n> As acknowledged in the paper, the validity of the MOFs decreases as number of building blocks, this could limit applicability to larger and more complex MOFs.\n\nWe recognize that as the complexity of MOFs increases, the validity of our approach may diminish. In future work, we aim to refine our methodology to enhance its applicability to more complex MOFs. This could involve incorporating the lattice parameters into the diffusion process, using known templates for guidance, and enhancing building block embeddings. \n\n> The authors use a MOF dataset with less than <20 building blocks. But the paper doesn't discuss about size extensivity of the GNN encoder and denoising module in detail.\n\nIt is straightforward to apply the current model to MOFs with a larger number of building blocks. In this paper, we limit the size of MOFs under the hypothesis that MOFs with extremely large primitive cells may be difficult to synthesize. \n\n> There is limited discussion on the synthetic accessibility of the designed MOFs. Finally, the building blocks are constrained to a certain set to allow coarse graining, this may led to less diverse MOFs.\n\nDetermining the synthetic accessibility of MOFs is very challenging, and there is unfortunately no widely accepted and generally applicable method. To the best of our capability, we define chemically informed validity criteria and use molecular simulations to filter for promising MOF candidates. We look forward to attempting the synthesis of top candidates in future endeavors. \n\nThe building blocks used in this work are extracted from the training dataset, BW-DB. This building block space contains 242,000 distinctive building blocks that enable a broad generative scope and satisfying performance in our tasks. While we focus on the BW-DB dataset in this paper due to the availability of gas adsorption labels, the space of possible building blocks can be expanded by incorporating new datasets. Our fully automated pipeline allows us to extract new building blocks without the need to curate the building blocks for template compatibility. \n\n> Why was the UFF chosen as force field? Would a MOF specific force field result in better structures?\n\nBoth UFF [1] and UFF4MOF [2] are widely used in existing MOF literature. It has been shown in previous work [3] that they have similar performance. We chose UFF over UFF4MOF to be consistent with the well-established GCMC simulation protocol for gas adsorption, which uses the UFF force field. In addition, through manual inspections, we find the relaxed structures from UFF reasonable. \n\n> ECFP4 was chosen as similarity measure for contrastive learning. Would a better similarity measure e.g., learned embeddings (ChemBERTa) result in better GNN encoder?\n\nThanks for the great suggestion. We agree that a more chemically informed building block identity has the potential to enhance our model. We are now exploring the ChemBERTa [4] embedding instead of ECFP4 for building block representation and will update further results when they finish.\n\n> On page 19, there is a mismatch between Py-G and pytorch references.\n\nThank you for pointing this out. We have corrected this in the revised manuscript.\n\nWe look forward to further discussions if you have additional questions or suggestions.\n\nReference:\n\n[1] Boyd, Peter G., et al. \"Data-driven design of metal\u2013organic frameworks for wet flue gas CO2 capture.\" Nature 576.7786 (2019): 253-256.\n\n[2] Nandy, Aditya, et al. \"A database of ultrastable MOFs reassembled from stable fragments with machine learning models.\" Matter 6.5 (2023): 1585-1603.\n\n[3] Boyd, Peter G., et al. \"Force-field prediction of materials properties in metal-organic frameworks.\" The journal of physical chemistry letters 8.2 (2017): 357-363.\n\n[4] Ahmad, Walid, et al. \"Chemberta-2: Towards chemical foundation models.\" arXiv preprint arXiv:2209.01712 (2022)."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4437/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700161848955,
                "cdate": 1700161848955,
                "tmdate": 1700251543965,
                "mdate": 1700251543965,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LrrcC5KKBm",
                "forum": "0VBsoluxR2",
                "replyto": "I9CWzp5GDR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4437/Reviewer_yqBG"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4437/Reviewer_yqBG"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for providing a detailed response but my rating remains the same."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4437/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700412884622,
                "cdate": 1700412884622,
                "tmdate": 1700412884622,
                "mdate": 1700412884622,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "VoCWjoGGBJ",
            "forum": "0VBsoluxR2",
            "replyto": "0VBsoluxR2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4437/Reviewer_nQJ6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4437/Reviewer_nQJ6"
            ],
            "content": {
                "summary": {
                    "value": "The authors present MOFDiff, a diffusion model that generates a coarse-grained (CG) representation of metal-organic framework (MOF) structures using a diffusion model. MOFs are represented hierarchically with sets of atoms grouped to represent building blocks. Each MOF is represented with K building blocks such that K << N, where N is the number of atoms in the periodic cell. This is particularly important as typical MOF unit cells may contain on the order of 100s of atoms. The authors first embed the building blocks using a model trained on contrastive learning loss. The latent vector is then used to condition a denoising diffusion model to generate CG representations of MOFs. CG represents are converted back to MOFs using a novel assembly algorithm. The authors define a validity criterion for their generated MOFs and show they are capable of generating valid, novel, diverse MOFs. MOFDiff is also capable of guided inverse design and optimizing MOFs CO2 separation for carbon capture."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The paper is easy to follow and the authors motivate their reasoning for model and design choices\n* The contrastive representation learning of building blocks allows the authors to learn a meaningful latent space of building blocks including metal nodes and linkers. This extends previous work which only modified linkers and as a result, generated low-diversity samples\n* The proposed assembly algorithm to orient multiple building blocks using gradient-based optimization is novel and provides a strong justification for the authors' data representation choices"
                },
                "weaknesses": {
                    "value": "* In the generation process, it is unclear why the latent vector Z can be sampled from the standard normal distribution. The latent vector is generated using the OrbNet encoder trained with the contrastive learning loss. Without further regularization, like in the case of variational autoencoders using the KL divergence, the latent vector should not conform to the distribution.\n* It would be useful to compare with previous work [1] on the inverse design of MOFs. The authors mention and cite a source on the low diversity of the generated MOFs with template-based systems such as SmVAE, but a comparison isn\u2019t present in the experiments section.\n* The validity and novelty of generated  structures are low compared to template-based models as in [1].\n* In general, while the authors include relevant work in the paper, the authors should have a related work section to place their work in the context of the field.\n    * The following papers could potentially be related to the present work [2], [3].\n\n[1] Yao, Z., S\u00e1nchez-Lengeling, B., Bobbitt, N. S., Bucior, B. J., Kumar, S. G. H., Collins, S. P., ... & Aspuru-Guzik, A. (2021). Inverse design of nanoporous crystalline reticular materials with deep generative models. Nature Machine Intelligence, 3(1), 76-86.\n\n[2] Zhou, M., & Wu, J. (2022). Inverse design of metal\u2013organic frameworks for C2H4/C2H6 separation. npj Computational Materials, 8(1), 256.\n\n[3] Park, Junkil, et al. \"Computational design of metal\u2013organic frameworks with unprecedented high hydrogen working capacity and high synthesizability.\" Chemistry of Materials 35.1 (2022): 9-16."
                },
                "questions": {
                    "value": "* In Figure 5, are only novel MOFs (i.e. not in the reference dataset) used to generate the histogram?\n* In the representation learning of building blocks, the authors mention small geometric variations of the building. Are these variations in the coordinate space? In other words, what transformations are used to provide positive samples in the contrastive loss?\n* How are the coordinates of the building blocks assigned?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4437/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4437/Reviewer_nQJ6",
                        "ICLR.cc/2024/Conference/Submission4437/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4437/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698934068442,
            "cdate": 1698934068442,
            "tmdate": 1700629454419,
            "mdate": 1700629454419,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2qvgitlc0R",
                "forum": "0VBsoluxR2",
                "replyto": "VoCWjoGGBJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4437/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4437/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response part 1"
                    },
                    "comment": {
                        "value": "We thank reviewer nQJ6 for helpful feedback and comments. We address each of the reviewer\u2019s concerns below.\n\n> In the generation process, it is unclear why the latent vector Z can be sampled from the standard normal distribution. The latent vector is generated using the OrbNet encoder trained with the contrastive learning loss. Without further regularization, like in the case of variational autoencoders using the KL divergence, the latent vector should not conform to the distribution.\n\nWe are sorry for the confusion. We hope to clarify that the MOFDiff model is indeed a variational autoencoder model trained with the KL regularization loss (Equation 12). This allows sampling MOF structures by sampling latent vectors from a Gaussian prior. The contrastive loss is not used for CG diffusion of MOFs. Instead, it is only used for learning the building block embedding. Only the building block encoder is trained with the contrastive loss. The building block encoder is then frozen and used to obtain learned embedding for all building blocks in the dataset. These building block embeddings are then used to represent CG MOF structures and diffusion-based generative modeling. \n\nWe hope the response above clarifies the confusion. We are more than happy to provide additional clarification if needed.\n\n> It would be useful to compare with previous work [1] on the inverse design of MOFs. The authors mention and cite a source on the low diversity of the generated MOFs with template-based systems such as SmVAE, but a comparison isn\u2019t present in the experiments section.\n\nWe have attempted to conduct more experiments on the SmVAE model proposed in [1]. However, although the VAE source code of [1] is released, the MOF deconstructor and reconstructor used in [1] are not clearly stated or publicly available. Without the code for these steps, we are unable to run SmVAE for a new dataset, or recover the all-atom structures for the dataset or generated samples of SmVAE. \n\nWhile a direct lateral comparison is not feasible, we were able to download the top 9 candidate MOFs released by [1] (GMOF-1 to GMOF-9) for CO2/N2 separation. **We run our molecular simulation workflow on these MOFs and report their gas adsorption properties in Table 1**. The best GMOF attains a working capacity of 2.20 mol/kg, a CO2 uptake at the adsorption stage of 2.53 mol/kg, and a CO2/N2 Selectivity of 11.46. In comparison, the MOFs generated by MOFDiff exhibit better carbon capture performance. However, it is important to acknowledge that this comparison is not entirely rigorous due to the difference in training, dataset, simulation, and optimization settings. All of our code will be open-sourced.\n\nIn addition, we hope to highlight the methodological difference between our method and SmVAE. Our method generates 3D MOF structures without templates and thus offers a distinct perspective compared to existing template-based methods such as SmVAE in addressing the diversity of computational MOF design. \n\n> The validity and novelty of generated structures are low compared to template-based models as in [1].\n\nIn the previous response, we highlighted the significant obstacles in making an apple-to-apple comparison to [1]. We wish to clarify that the validity and the novelty statistics reported in [1] and our manuscript are not directly comparable for the following reasons:\n\n1. The criterion for validity/novelty is not clearly defined in [1]. Our validity criterion is defined as the simultaneous satisfaction of (1) matched connection, (2) successful force field relaxation, and (3) passing all criteria defined in MOFChecker [2]. Our novelty is defined through MOFid [3]. Our code will be open-sourced so the validity/novelty criterion can be reproduced in future works.\n\n2. The training data set is different. The SmVAE model [1] was trained on a customized database of \u201caround 2 million MOFs\u201d. Our model is trained on BW-DB (~300k MOFs) with the original labels for gas adsorption properties [4]. \n\n3. The generation scheme is different. The SmVAE model generates MOFs based on predefined templates. Our method generates 3D structures without templates. The SmVAE model and our method also use different sampling protocols."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4437/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700153034642,
                "cdate": 1700153034642,
                "tmdate": 1700153034642,
                "mdate": 1700153034642,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vvrj1Z8Ug9",
                "forum": "0VBsoluxR2",
                "replyto": "VoCWjoGGBJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4437/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4437/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response part 2"
                    },
                    "comment": {
                        "value": "> In general, while the authors include relevant work in the paper, the authors should have a related work section to place their work in the context of the field. The following papers could potentially be related to the present work [2], [3].\n\nThank you for the thoughtful suggestion of adding a related work section, and for pointing out additional related works. We agree more context for the present work will enhance the manuscript. **We have added a related work section (Appendix A) to discuss the relevant works in more detail, as well as include additional related works pointed out by the reviewer.** Due to the page limitation, the related work section is in the appendix. We welcome further feedback on this related work section.\n\n> In Figure 5, are only novel MOFs (i.e. not in the reference dataset) used to generate the\n\nYes, only MOFs that are valid, novel, and unique are included in Figure 5.\n\n> In the representation learning of building blocks, the authors mention small geometric variations of the building. Are these variations in the coordinate space? In other words, what transformations are used to provide positive samples in the contrastive loss?\n\nThe geometric variation naturally exists in the dataset because the same building block appears in the same or different MOFs multiple times with geometric differences (reflected in Figure 3). Therefore, we don\u2019t need to define transformations to provide positive samples. Instead, we use any two 3D building block structures as a positive pair if they share the same ECFP4 fingerprint and the same number of connection points. \n\n> How are the coordinates of the building blocks assigned?\n\nThe coordinates of the building blocks are generated through the reverse diffusion process. The reverse diffusion process starts by randomly sampling the building block coordinates and identities. From this noisy structure, the learned score networks iteratively denoise the noisy structure to a final coarse-grained MOF structure. The all-atom MOF structure is then recovered from the CG structure through the assembly algorithm and force field relaxation.\n\nWe look forward to further discussions if you have additional questions or suggestions.\n\nReference:\n\n[1] Yao, Z., S\u00e1nchez-Lengeling, B., Bobbitt, N. S., Bucior, B. J., Kumar, S. G. H., Collins, S. P., ... & Aspuru-Guzik, A. (2021). Inverse design of nanoporous crystalline reticular materials with deep generative models. Nature Machine Intelligence, 3(1), 76-86.\n\n[2] Jablonka, K. M. (2023). mofchecker (Version 1.0.0) [Computer software]. https://doi.org/10.5281/zenodo.1234\n\n[3] Bucior, Benjamin J., et al. \"Identification schemes for metal\u2013organic frameworks to enable rapid search and cheminformatics analysis.\" Crystal Growth & Design 19.11 (2019): 6682-6697.\n\n[4] Boyd, Peter G., et al. \"Data-driven design of metal\u2013organic frameworks for wet flue gas CO2 capture.\" Nature 576.7786 (2019): 253-256."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4437/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700153078835,
                "cdate": 1700153078835,
                "tmdate": 1700250804152,
                "mdate": 1700250804152,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iXs7k9IRh0",
                "forum": "0VBsoluxR2",
                "replyto": "2qvgitlc0R",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4437/Reviewer_nQJ6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4437/Reviewer_nQJ6"
                ],
                "content": {
                    "title": {
                        "value": "Increased rating"
                    },
                    "comment": {
                        "value": "Thank you for the detailed feedback.\n\n> We are sorry for the confusion. We hope to clarify that the MOFDiff model is indeed a variational autoencoder model trained with the KL regularization loss (Equation 12). This allows sampling MOF structures by sampling latent vectors from a Gaussian prior. The contrastive loss is not used for CG diffusion of MOFs. Instead, it is only used for learning the building block embedding. Only the building block encoder is trained with the contrastive loss. The building block encoder is then frozen and used to obtain learned embedding for all building blocks in the dataset. These building block embeddings are then used to represent CG MOF structures and diffusion-based generative modeling.\n\nYes, this clarifies my confusion about the latent encoding. Thank you for including a reference to the appropriate section in the appendix as it makes the finding information significantly easier. \n\n> We have attempted to conduct more experiments on the SmVAE model proposed in [1]. However, although the VAE source code of [1] is released, the MOF deconstructor and reconstructor used in [1] are not clearly stated or publicly available. Without the code for these steps, we are unable to run SmVAE for a new dataset, or recover the all-atom structures for the dataset or generated samples of SmVAE\n\nWhile unfortunate that such a comparison is not possible, this response is valid."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4437/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700629411371,
                "cdate": 1700629411371,
                "tmdate": 1700629411371,
                "mdate": 1700629411371,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "azbLZzQ3h1",
            "forum": "0VBsoluxR2",
            "replyto": "0VBsoluxR2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4437/Reviewer_e39L"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4437/Reviewer_e39L"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a method for generating Metal-Organic Framework (MOF) structures with target properties such as carbon capture.\n\nThe proposed method consists of a contrastive learning framework for embedding the MOF blocks based on their ECFP4 fingerprint. These representations are then used to work with a coarse-grained representation of the MOF. A latent variable model is then trained to generate a new MOF. Sampling from the latent variable allows for estimating the lattice structure and the number of building blocks. Then a diffusion-based decoder is used to produce the building block types and their location in the unit cell. The building blocks are then oriented and connections are established between them. Finally, a force field relaxation is computed to finalize the position and orientation of all atoms in the MOF. \n\nThe MOFs are validated based on well-established domain knowledge. The quality of the generated MOF structures is assessed for key application tasks such as carbon capture using well-established Monte Carlo simulation. \n\nThe proposal goes beyond the limitations of template-based methods and has computational advantages in proposing novel MOF structures with desired properties."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper clearly demonstrates the strong technical background in Material Science of the authors. The focus is on achieving concrete results in MOF generation that can have an impact on key applications of MOF materials.  Correspondingly, the validation of the results is substantive. \n\nThe selected Machine Learning components, such as the choice of the latent variable model, diffusion model, and contrastive learning are precisely selected to overcome specific challenges towards the goal of generating MOFs."
                },
                "weaknesses": {
                    "value": "The above-given strengths are closely related to the weaknesses of the paper. Clearly, a very good application paper that makes important advances for computational material science may have limited contribution to the field of Machine Learning (ML). \n\nWhile a strong contribution to Material Science, from the ML perspective the work is a good combination of existing methods but does not go much further than that. The paper tells a story about many of the design decisions that were taken, but many other choices could have been made with even more recent approaches. \n\nIn that sense, the main weakness is the alignment to the ICLR conference."
                },
                "questions": {
                    "value": "One potentially problematic aspect is that the number of blocks K and the lattice structure L are computed based on the sample z which also conditions the diffusion of the attributes and the locations of the blocks. This can also lead to invalid MOF structures as the block's location and orientation may not match the lattice geometry. You identify this as well in your last sentence. \n1. Why not incorporate these in the diffusion process? \n2. Or alternatively, why not compute the lattice structure based on the atoms and the bond in the unit cell? Wouldn't the atoms (block) and the bonds uniquely identify the lattice structure of the crystal?\n3. Why generate from an uninformed prior N(0, I)? It seems unreasonable to expect that sampling from such a distribution would give good coverage of the vast space of possible MOF configurations. Would it not be more effective to condition on a number of building blocks or present partial coarse structures? Possibly many other well-understood properties of the MOFs?\n4. Your generation process is limited to using the building blocks present in the training data. How broad of a coverage does this give the generating process? Are there many other MOFs possible with building blocks not present in the training data?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4437/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4437/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4437/Reviewer_e39L"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4437/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699130926501,
            "cdate": 1699130926501,
            "tmdate": 1700585045574,
            "mdate": 1700585045574,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "t55H36q87N",
                "forum": "0VBsoluxR2",
                "replyto": "azbLZzQ3h1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4437/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4437/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response part 1"
                    },
                    "comment": {
                        "value": "We thank reviewer e39L for helpful feedback and comments. We address each of the reviewer\u2019s concerns below.\n\n> While a strong contribution to Material Science, from the ML perspective the work is a good combination of existing methods but does not go much further than that. The paper tells a story about many of the design decisions that were taken, but many other choices could have been made with even more recent approaches. In that sense, the main weakness is the alignment to the ICLR conference.\n\nThank you for appreciating the materials science and ML application aspects of our paper. We believe our paper also makes methodological contributions and is aligned with the ICLR audience for the following reasons:\n\n1. Our paper addresses the important and difficult task of MOF design, for which ML methods have great potential. We believe our model design makes novel and non-trivial contributions from an ML perspective:\n- We propose contrastive learning for embedding the enormous space of building blocks to be used for generative modeling. In particular, in early experiments, we found autoencoding-based embedding has a much inferior performance compared to contrastive learning.\n- We combine learning and algorithmic methods to solve a task that is hard to solve with a pure learning approach. In early experiments, we attempted a pure learning approach of SO(3) diffusion for building block orientation, but couldn\u2019t get good results. \n- Coarse-grained diffusion. Our diffusion model operates over CG coordinates and a vast space of learned building block identities. This is novel compared to existing diffusion models for small molecules, proteins, or crystals [3,4,5], where the atoms/amino acids have discrete types. \n\n2. Our paper provides scientifically meaningful tasks for the ML community to make rapid progress on MOF design.\n\n3. We will open-source our code for the entire pipeline of MOF decomposition, MOF coarse-graining, diffusion model, all-atom MOF reconstruction, relaxation, and molecular simulation. We believe our code will make MOF modeling and design much more accessible to the broader ML community. \n\nIn summary, we believe the methods, tasks, and codebase proposed in our paper will be interesting to the ICLR audience, especially the AI4Science community. In particular, our method offers new perspectives for the ML modeling of multi-scale molecular and materials systems that are ubiquitous in biological and physical science.\n\nMore details on the orientation diffusion attempt:\n\nIn early experiments, we also explored diffusing the orientation of the building blocks based on SO(3)-diffusion, which has shown promising results in protein diffusion models [6]. However, we could not get good performance. We believe this is due to the orientation of building blocks being ill-defined. In our failed attempts, we use principal component analysis to get canonical orientations for the building blocks. Unlike amino acids which have a natural definition of orientation based on the $N - C_\\alpha - C$ atoms, the geometry of the building blocks is much more diverse. There exist many almost-2D or near-isotropic building blocks which makes the orientation ambiguous. Further, the layout of the building blocks is delicate and requires accurate alignment between the building blocks to render a valid MOF."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4437/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700152249337,
                "cdate": 1700152249337,
                "tmdate": 1700152249337,
                "mdate": 1700152249337,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9Bm22375I3",
                "forum": "0VBsoluxR2",
                "replyto": "azbLZzQ3h1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4437/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4437/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response part 2"
                    },
                    "comment": {
                        "value": "We deeply appreciate your suggestions on further improvements over the current model, which we respond to below.\n\n> One potentially problematic aspect is that the number of blocks K and the lattice structure L are computed based on the sample z which also conditions the diffusion of the attributes and the locations of the blocks. This can also lead to invalid MOF structures as the block's location and orientation may not match the lattice geometry. You identify this as well in your last sentence. Why not incorporate these in the diffusion process? Or alternatively, why not compute the lattice structure based on the atoms and the bond in the unit cell? Wouldn't the atoms (block) and the bonds uniquely identify the lattice structure of the crystal?\n\nThank you for the great suggestion. Indeed, as we conclude in our paper, we agree incorporating the lattice parameters into the diffusion process is a promising future direction to resolve the mismatch between the blocks and the lattices, and significantly improve the current model.\n\nHowever, we find incorporating lattice diffusion to MOFDiff presents significant challenges. Existing work that focuses on inorganic crystals [7] has explored lattice diffusion. Their formulation is to diffuse the lattice parameters to N(0, I) and represent the coordinates of atoms in the fractional coordinates because the cartesian coordinates will become numerically unstable for small lattice sizes and the cartesian scale of the system changes across the diffusion process. \n\nApplying a similar strategy to MOFs is more complex. First, using fractional coordinates to represent the coarse-grained coordinates may not be ideal, as fractional coordinates cannot accurately represent the essential size information of these blocks, while the sizes of the building blocks vary drastically. Moreover, as the building block identities and their respective sizes evolve during the reverse diffusion process, maintaining accurate and meaningful representations of the lattice and building blocks may be challenging. Given these challenges, we believe it requires careful consideration and novel approaches to address the unique characteristics and demands of MOFs when designing a lattice diffusion process. We value your input and recognize the importance of this issue. We are committed to addressing it in our future research endeavors.\n\n> Why generate from an uninformed prior N(0, I)? It seems unreasonable to expect that sampling from such a distribution would give good coverage of the vast space of possible MOF configurations. Would it not be more effective to condition on a number of building blocks or present partial coarse structures? Possibly many other well-understood properties of the MOFs?\n\nOur first experiment samples MOFs from the Gaussian prior N(0, I) of the VAE model. For a well-trained VAE, sampling from the prior should result in the generation of new data instances that reflect the general characteristics of the training data. In this unconditional generation experiment, we aim to evaluate our model\u2019s capability to recover the training distribution in a general sense. In other words, we aim to validate that MOFDiff can generate valid MOFs (Figure 6) with a wide spectrum of structural properties that resemble the training distribution (Figure 5). In contrast, a badly trained model may have \u201cmode collapse\u201d, and consequently be unable to capture the training distribution. \n\n> Your generation process is limited to using the building blocks present in the training data. How broad of a coverage does this give the generating process? Are there many other MOFs possible with building blocks not present in the training data?\n\nOur method operates over 242,000 distinctive building blocks from the BW-DB dataset. Our experimental results demonstrate the current space of building blocks can already enable structurally diverse generation, as well as effective inverse design for carbon capture applications. While we focus on the BW-DB dataset in this paper due to the availability of gas adsorption labels, it is straightforward to expand the space of possible building blocks (and the scope of MOF generation) by incorporating new datasets. \n\nWe look forward to further discussions if you have additional questions or suggestions."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4437/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700152391301,
                "cdate": 1700152391301,
                "tmdate": 1700152391301,
                "mdate": 1700152391301,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BPz2U3fT2d",
                "forum": "0VBsoluxR2",
                "replyto": "azbLZzQ3h1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4437/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4437/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response part 3"
                    },
                    "comment": {
                        "value": "Reference:\n\n[1] Yao, Zhenpeng, et al. \"Inverse design of nanoporous crystalline reticular materials with deep generative models.\" Nature Machine Intelligence 3.1 (2021): 76-86.\n\n[2] Park, Hyunsoo, et al. \"Inverse design of metal-organic frameworks for direct air capture of CO2 via deep reinforcement learning.\" (2023).\n\n[3] Hoogeboom, Emiel, et al. \"Equivariant diffusion for molecule generation in 3d.\" International conference on machine learning. PMLR, 2022.\n\n[4] Gruver, Nate, et al. \"Protein Design with Guided Discrete Diffusion.\" arXiv preprint arXiv:2305.20009 (2023).\n\n[5] Xie, Tian, et al. \"Crystal diffusion variational autoencoder for periodic material generation.\" arXiv preprint arXiv:2110.06197 (2021).\n\n[6] Yim, Jason, et al. \"SE (3) diffusion model with application to protein backbone generation.\" arXiv preprint arXiv:2302.02277 (2023).\n\n[7] Jiao, Rui, et al. \"Crystal Structure Prediction by Joint Equivariant Diffusion on Lattices and Fractional Coordinates.\" NeurIPS (2023)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4437/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700152953765,
                "cdate": 1700152953765,
                "tmdate": 1700152953765,
                "mdate": 1700152953765,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "P13AtdQump",
                "forum": "0VBsoluxR2",
                "replyto": "BPz2U3fT2d",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4437/Reviewer_e39L"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4437/Reviewer_e39L"
                ],
                "content": {
                    "title": {
                        "value": "Increase rating"
                    },
                    "comment": {
                        "value": "Thank you for all your detailed answers. \n\nI do believe there are still some limitations to the proposed approach. Nevertheless, I also think that it is a good contribution to the state of the art. So I have increased my score to 8."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4437/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700585122933,
                "cdate": 1700585122933,
                "tmdate": 1700585122933,
                "mdate": 1700585122933,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]