[
    {
        "title": "Behind the Myth of Exploration in Policy Gradients"
    },
    {
        "review": {
            "id": "09gxmthW7H",
            "forum": "pUIANwOLBN",
            "replyto": "pUIANwOLBN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2823/Reviewer_yYKF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2823/Reviewer_yYKF"
            ],
            "content": {
                "summary": {
                    "value": "The authors use two pathological environments to explore aspects of exploration bonuses in objectives.  For example, they show that these bonuses can potentially eliminate local optima and make the loss landscape more amenable to SGD."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This work offers a compelling perspective and analysis of the topic.  While some of the ideas they explore might be \"common knowledge\" in the community, I believe that this might be the first time that some of these ideas have been explored thoroughly and scientifically."
                },
                "weaknesses": {
                    "value": "Typos and minor suggestions:\n- Discount factor backwards bracket in 2.1\n- Is (1) missing the sum?\n- \u201cThe valley is composed of two floors\u201d: \u201cfloors\u201d is a strange way to phrase it, and the intended meaning was initially unclear to me.  Consider rephrasing to something like \"The valley contains two separate low points\u201d.\n- If there\u2019s room, you could point to an appendix to briefly remind the reader why the middle and RHS of (7) are the same as alpha -> 0.  I know it\u2019s fundamental SGD theory, but it confused me for a few minutes in this context (especially since, at first glance, the equation is not true, since it is just a first-order approximation and is only true as alpha approaches 0).\n- In 4.2, the thorough description of the environment is nice, but I think it\u2019s missing the time limit used.  Also, were the curves given in Figure 3 computed analytically or empirically?  Some more info might improve the paper.\nIn 4.2, you could remind the reader what (4) is so that they do not have to hunt for it.\n\nWeaknesses:\n1) Some of the questions below are clarity weaknesses or other possible weaknesses.\n2) The contribution is entirely empirical (there are some nice theory concepts, but no proofs or useful properties are shown except empirically), and the empirical results are based entirely on extremely simple toy problems.  In my opinion, this does not invalidate their interesting perspectives on the issues that they illustrate with these results, but it does make the contribution less substantial.\n3) The paper overall is a bit challenging to read.  For an example of a well-written bit that does *not* have this issue, the conclusion says \u201cFirst, it modifies the learning objective in order to remove local extrema. Second, it modifies the gradient estimates and increases the likelihood that the update steps lead to optimal policies.\u201d  This is great; more \u201chigh-level summary\u201d passages like this in sections 3-4 would\u2019ve made it much easier to read.  For an example of a hard-to-understand part, see the last question below.\n\nNote: My primary concerns and slightly negative score come primarily from #2 (contribution), and the second question below (\"reward-engineering\").  The clarity weaknesses are not severe enough to have a large impact on my score."
                },
                "questions": {
                    "value": "What is the meaning of the and symbol in (6)?  How is the gradient equal to \u201c0 and L(theta)\u201d?  Or is the and symbol meant to separate this line into two separate equalities?  If so, this is confusing, consider representing this in a more standard way, or using parentheses to disambiguate.  **Update:** upon reading more of the paper, and seeing this used more, I know the latter interpretation is correct, so no need to answer this question in your response, but I\u2019ll leave this here to illustrate the potential confusion to the reader.\n\nIn 4.1, J^d seems less like an exploration term, and more like a \u201creward-engineering\u201d term that simply makes the problem easier.  Am I missing a perspective on this?  This leads to a larger concern, in that much of the contribution of 4.1 hinges on this term, and I have doubts about whether this term can be legitimately thought of as an exploration bonus that is superior to the entropy bonus.\n\nCan you please sum up the core take-away from 4.1?  I\u2019ve reread the paragraph \u201cWe have empirically shown that exploration in policy gradients\u2026\u201d several times, but I\u2019m struggling to understand exactly what I was supposed to take away from this section.  Is the point that the two criteria were good criteria in practice for choosing a good exploration bonus term?  If so, I am not convinced that this result will generalize beyond this specific toy setting (and the issue raised in the question above becomes even more of a concern in this case).  If not (or even if so), I think the intended take-away of this section needs to be spelled out more clearly.  **Update:** I understand better now upon a reread, so no need to address this question directly in your response.  However, the Section 4.1 paragraph noted is a perfect example of the last weakness noted above, so I\u2019ll leave this question in the review."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2823/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698382792012,
            "cdate": 1698382792012,
            "tmdate": 1699636226084,
            "mdate": 1699636226084,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vWSJs1Z6II",
                "forum": "pUIANwOLBN",
                "replyto": "09gxmthW7H",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2823/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2823/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response Reviewer yYKF"
                    },
                    "comment": {
                        "value": "Dear reviewer,\n\nThank you for your valuable feedback, which we have carefully considered and integrated into the document.\nWe've addressed most of the minor remarks and recognize the need for improved clarity on certain points.\nNotably, we've made substantial changes to Section 4.1 and the introduction, providing a more comprehensive overview of our contributions.\n\nWe hope that these revisions have clarified the significance of our contributions to the RL community.\nWith the manuscript modifications, we believe we now align with the acceptance standards of ICLR.\nConsequently, we kindly request you to reconsider your final decision.\n\nBelow, we provide detailed responses to your specific questions.\n\n> The contribution is entirely empirical (there are some nice theory concepts, but no proofs or useful properties are shown except empirically), and the empirical results are based entirely on extremely simple toy problems. In my opinion, this does not invalidate their interesting perspectives on the issues that they illustrate with these results, but it does make the contribution less substantial.\n\nWe acknowledge that the paper was rather vague on this point, but the criteria also introduce theoretical guarantees.\nFollowing your comment, Sections 3.1 and 4.1 have been reworked to better highlight the assumptions and results they imply.\nIn particular, if our two criteria in Section 3 are met (note that we have changed the name of the second to better fit in the literature), it is possible to compute a policy by stochastic gradient ascent that is suboptimal by at most epsilon.\nUnder the assumptions clarified in Section 4.1, the probability of improvement defines the probability of having a good solution after optimization.\nWe agree, however, that there is future work to establish additional theoretical convergence guarantees and have introduced some avenues of reflection in conclusion.\n\nRegarding generalization, we acknowledge that our illustrations were conducted on relatively simple use cases.\nThe low-dimensional spaces of states, actions, and parameters were chosen for clarity in visual representation.\nIn addition, the experimental results were computed through exhaustive search in the parameter space combined with Monte-Carlo estimations, constraining the parameter space to low dimensions.\nMoreover, most exploration methods rely on function approximators for dealing with high dimensions, introducing complexities in learning dynamics that prevent isolating the sole effect of exploration.\nNevertheless, we believe that the strongly non-linear dynamics in the first environment and the sparse reward function in the second reflects sufficient complexity from an optimization perspective.\n\n> The paper overall is a bit challenging to read. For an example of a well-written bit that does not have this issue, the conclusion says \u201cFirst, it modifies the learning objective in order to remove local extrema. Second, it modifies the gradient estimates and increases the likelihood that the update steps lead to optimal policies.\u201d This is great; more \u201chigh-level summary\u201d passages like this in sections 3-4 would've made it much easier to read. For an example of a hard-to-understand part, see the last question below.\n\nWe acknowledge your remarks concerning the clarity and have modified the paper substantially, including the two particular points you mentioned.\nIn particular, the revised introduction aims to provide a clearer high-level view of the paper, related works, and contributions.\n\n> In 4.1, J^d seems less like an exploration term, and more like a \u201creward-engineering\u201d term that simply makes the problem easier. Am I missing a perspective on this? This leads to a larger concern, in that much of the contribution of 4.1 hinges on this term, and I have doubts about whether this term can be legitimately thought of as an exploration bonus that is superior to the entropy bonus.\n\nFirstly, we have clarified in the introduction the distinction between exploration in policy gradient as intended in theory, i.e. optimizing a sufficiently stochastic policy, and how it is implemented in practice, i.e. involving reward shaping and optimizing a surrogate learning objective to increase stochasticity.\nIn our paper, we analyse the second aspect by studying the exploration-based learning objective functions.\nNevertheless, no assumption is made on the structure of the intrinsic rewards that may thus be built with any reward-shaping strategy.\nThe analyses and discussions thus hold true for the expert-knowledge reward shaping in Section 4.1, which illustrates the potential achievements of exploration when well-designed for the problem at hand.\nWe furthermore emphasize that all discussions are also validated on the standard entropy regularized objective."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2823/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700560245613,
                "cdate": 1700560245613,
                "tmdate": 1700560245613,
                "mdate": 1700560245613,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "TAwaDz0rlq",
            "forum": "pUIANwOLBN",
            "replyto": "pUIANwOLBN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2823/Reviewer_qvTF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2823/Reviewer_qvTF"
            ],
            "content": {
                "summary": {
                    "value": "The paper under review took a closer look at the value of exploration for RL algorithms from an analytic point of view.\nOn top of the intrinsic need to achieve global optimal (one has to know then entire environments), \nauthors proposed four criteria: __coherence, quasiconcavity, efficiency, and attraction__, \nto measure the quality of an exploration, and demonstrated the effectiveness of these measures through simulations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The idea of quantify effectiveness of an exploration is novel and well grounded.\nThe paper is well written, easy to follow. Simulations and plots are illustrative and helpful."
                },
                "weaknesses": {
                    "value": "My concern lies in practicality. While intuitively, the proposed four criterion all make sense, however, for a general RL question, how to effectively compute these four measures is somehow challenging. For instance, following the paper's notation,  let $J$ be the objective function, \n$J(\\pi_{\\theta^*})$ be the optimal, and $J(\\pi_{\\theta^\\dagger})$ be the optimal with a way of exploration. Coherence requires \nevaluating if $J(\\pi_{\\theta^*}) - J(\\pi_{\\theta^\\dagger}) \\leq \\epsilon$. \nUsually $\\theta^*, \\theta^\\dagger$ are unknown, how would one utilize this coherence, and more generally the other proposed measures are not detailed in the paper."
                },
                "questions": {
                    "value": "- Question on Practically: current simulations are good for readers to get the idea of the paper, but insufficient to demonstrate the capacity. It would be nice if the authors could take an concrete example, say a maze, with a few popular regularized objectives, compute (estimate) the proposed four criterion of these objectives, see which one is more effective based on these criterion. Then valid such prediction on the trained agent's behaviors."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2823/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2823/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2823/Reviewer_qvTF"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2823/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698432676162,
            "cdate": 1698432676162,
            "tmdate": 1699636225975,
            "mdate": 1699636225975,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Vrn41WUwKj",
                "forum": "pUIANwOLBN",
                "replyto": "TAwaDz0rlq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2823/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2823/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response Reviewer qvTF"
                    },
                    "comment": {
                        "value": "Dear reviewer,\n\nWe thank you for reviewing our paper and answer your two questions below.\nWe believe these are due to a slightly unclear presentation that we have rectified in the revised paper version.\nYour remark gave us the opportunity to clarify this in the main paper, and we also better explain it below.\nWe are confident that the paper offers a new way of analyzing exploration and is of interest to the ICLR community.\nWe are receptive to any suggestions for improvements and kindly request a reconsideration of your decision for acceptance.\n\n> My concern lies in practicality. While intuitively, the proposed four criterion all make sense, however, for a general RL question, how to effectively compute these four measures is somehow challenging.\n\nThe purpose of the paper is to provide a framework facilitating the interpretation of policy gradients and to identify relevant criteria for obtaining guarantees in policy gradients.\nAs is often the case when theoretical criteria are proposed, their empirical verification is as complex as solving the problem at hand.\nIf such criteria were easy to verify empirically, the problem would certainly be much more easy to solve too.\nHowever, we still believe that these criteria provide a good account of the effect of current exploration techniques, and their trade-offs.\nMoreover, we think that they provide some valuable, and often lacking, intuition for the design of new exploration techniques.\nWe have added the development of more problem-specific bounds to our future work.\n\n> Current simulations are good for readers to get the idea of the paper, but insufficient to demonstrate the capacity. It would be nice if the authors could take an concrete example, say a maze, with a few popular regularized objectives, compute (estimate) the proposed four criterion of these objectives, see which one is more effective based on these criterion. Then valid such prediction on the trained agent's behaviors.\n\nThe examples primarily serve to illustrate the criteria, which are derived from optimization theory.\nThe experimental results were computed through exhaustive search in the parameter space combined with Monte-Carlo estimations, constraining the parameter space to low dimensions.\nMoreover, most exploration methods rely on function approximators for dealing with high dimensions, introducing complexities in learning dynamics that prevent isolating the sole effect of exploration.\nNevertheless, we believe that the strongly non-linear dynamics in the first environment and the sparse reward function in the second reflects sufficient complexity from an optimization perspective.\nWe have also clarified in the introduction that finding good exploration strategies is known to be problem specific and that we thus introduce a general framework for the study and interpretation of exploration in policy gradient methods instead of trying to find the best exploration method for a given task."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2823/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700560100466,
                "cdate": 1700560100466,
                "tmdate": 1700560100466,
                "mdate": 1700560100466,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VxVvn2hMz5",
                "forum": "pUIANwOLBN",
                "replyto": "TAwaDz0rlq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2823/Reviewer_qvTF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2823/Reviewer_qvTF"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors, \n\nThank you for your response. It somewhat addressed my concern~\n\nI would like to acknowledge the contribution of theoretically defining the four criteria.\nBut to have a practical impact, it feels some further work needs to be donem,\neither some further theoretically investigation (e.g. some sufficient condition to check these four criteria ) \nor a more comprehensive simulation is needed to validate the proposed criteria. \n\nOtherwise, even we have these four criteria in mind, why these are good, how to design a policy that fit these etc are unclear.\nHence, i decide to keep my original score."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2823/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700687215094,
                "cdate": 1700687215094,
                "tmdate": 1700687330287,
                "mdate": 1700687330287,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "6AUM4NERbI",
            "forum": "pUIANwOLBN",
            "replyto": "pUIANwOLBN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2823/Reviewer_kiQP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2823/Reviewer_kiQP"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the impact of exploration in policy optimization algorithms from an optimization perspective. The authors propose different measures of analysis, discuss implications of the techniques and measures proposed, and insights gathered on illustrative examples. Their conclusion is that exploration techniques that smoothify the objective (by adding entropy) eliminate local optima and increase the probability of trajectories through the optimization landscape reaching the goal."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "I believe the properties of exploration bonuses on policy gradient algorithms are not entirely novel (for instance, their effect on smoothifying the optimization landscape was known, investigated empirically in \\citet{ahmed19} and theoretically in \\citet{mei2020}). The optimization perspective on exploration has been previously explored in \\citet{mei2020, chung21, mei2022}. The main strength of the paper I believe lies in its clarity and rigor of collecting and expressing these scattered insights. I think the paper makes a good effort in systematically investigating these properties together, and offering a way of looking at them in the same place, on simple illustrative examples. Because of this, I think it has a lot of value to the community. Particularly, I believe the optimization view on exploration is still not known within the community, or understood by RL practitioners, so explaining these things with clarity is very relevant and subtly impactful. I also found the illustrative examples really interesting, particularly the last one.\n\n\n@InProceedings{ahmed19,\n  title = \t {Understanding the Impact of Entropy on Policy Optimization},\n  author =       {Ahmed, Zafarali and Le Roux, Nicolas and Norouzi, Mohammad and Schuurmans, Dale},\n  booktitle = \t {Proceedings of the 36th International Conference on Machine Learning},\n  pages = \t {151--160},\n  year = \t {2019},\n  editor = \t {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},\n  volume = \t {97},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {09--15 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v97/ahmed19a/ahmed19a.pdf},\n  url = \t {https://proceedings.mlr.press/v97/ahmed19a.html},\n  abstract = \t {Entropy regularization is commonly used to improve policy optimization in reinforcement learning. It is believed to help with exploration by encouraging the selection of more stochastic policies. In this work, we analyze this claim using new visualizations of the optimization landscape based on randomly perturbing the loss function. We first show that even with access to the exact gradient, policy optimization is difficult due to the geometry of the objective function. We then qualitatively show that in some environments, a policy with higher entropy can make the optimization landscape smoother, thereby connecting local optima and enabling the use of larger learning rates. This paper presents new tools for understanding the optimization landscape, shows that policy entropy serves as a regularizer, and highlights the challenge of designing general-purpose policy optimization algorithms.}\n}\n\n@inproceedings{mei2020,\n author = {Mei, Jincheng and Xiao, Chenjun and Dai, Bo and Li, Lihong and Szepesvari, Csaba and Schuurmans, Dale},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},\n pages = {21130--21140},\n publisher = {Curran Associates, Inc.},\n title = {Escaping the Gravitational Pull of Softmax},\n url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/f1cf2a082126bf02de0b307778ce73a7-Paper.pdf},\n volume = {33},\n year = {2020}\n}\n\n@InProceedings{chung21,\n  title = \t {Beyond Variance Reduction: Understanding the True Impact of Baselines on Policy Optimization},\n  author =       {Chung, Wesley and Thomas, Valentin and Machado, Marlos C. and Roux, Nicolas Le},\n  booktitle = \t {Proceedings of the 38th International Conference on Machine Learning},\n  pages = \t {1999--2009},\n  year = \t {2021},\n  editor = \t {Meila, Marina and Zhang, Tong},\n  volume = \t {139},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {18--24 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v139/chung21a/chung21a.pdf},\n  url = \t {https://proceedings.mlr.press/v139/chung21a.html},\n  abstract = \t {Bandit and reinforcement learning (RL) problems can often be framed as optimization problems where the goal is to maximize average performance while having access only to stochastic estimates of the true gradient. Traditionally, stochastic optimization theory predicts that learning dynamics are governed by the curvature of the loss function and the noise of the gradient estimates. In this paper we demonstrate that the standard view is too limited for bandit and RL problems. To allow our analysis to be interpreted in light of multi-step MDPs, we focus on techniques derived from stochastic optimization principles&nbsp;(e.g., natural policy gradient and EXP3) and we show that some standard assumptions from optimization theory are violated in these problems. We present theoretical results showing that, at least for bandit problems, curvature and noise are not sufficient to explain the learning dynamics and that seemingly innocuous choices like the baseline can determine whether an algorithm converges. These theoretical findings match our empirical evaluation, which we extend to multi-state MDPs.}\n}\n\n@article{Mei2021,\n  author       = {Jincheng Mei and\n                  Bo Dai and\n                  Chenjun Xiao and\n                  Csaba Szepesv{\\'{a}}ri and\n                  Dale Schuurmans},\n  title        = {Understanding the Effect of Stochasticity in Policy Optimization},\n  journal      = {CoRR},\n  volume       = {abs/2110.15572},\n  year         = {2021},\n  url          = {https://arxiv.org/abs/2110.15572},\n  eprinttype    = {arXiv},\n  eprint       = {2110.15572},\n  timestamp    = {Thu, 29 Jun 2023 16:58:03 +0200},\n  biburl       = {https://dblp.org/rec/journals/corr/abs-2110-15572.bib},\n  bibsource    = {dblp computer science bibliography, https://dblp.org}\n}\n@inproceedings{Mei2022,\n author = {Mei, Jincheng and Chung, Wesley and Thomas, Valentin and Dai, Bo and Szepesvari, Csaba and Schuurmans, Dale},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},\n pages = {17818--17830},\n publisher = {Curran Associates, Inc.},\n title = {The Role of Baselines in Policy Gradient Optimization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/718d02a76d69686a36eccc8cde3e6a41-Paper-Conference.pdf},\n volume = {35},\n year = {2022}\n}"
                },
                "weaknesses": {
                    "value": "I would have liked to see however how these methods generalize over problem instances, as it is known from \\citet{mei2020} that PG methods are highly impacted by initialization and problem instances.\nSome statements are rather vague, e.g. the authors claim that there is a trade-off between quasiconcavity and eps-coherence, but i\u2019m not sure how general this statement is, and the authors do not provide proofs that guarantee these statements hold on all problem instances. At other times the authors use vague words like \u201cappears\u201d so its difficult to understand if these small illustrations would carry over to agents at scale, and are general enough.\nThe experiment illustrated in Fig 2 is interesting, but wasn\u2019t this already known, the problem generally is that we cannot know the value of lambda beforehand as these are problem dependent, sometimes state-dependent and might also change over the optimization landscape for different policies on the way toward the optimal solution.\n\n\n@inproceedings{mei2020,\n author = {Mei, Jincheng and Xiao, Chenjun and Dai, Bo and Li, Lihong and Szepesvari, Csaba and Schuurmans, Dale},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},\n pages = {21130--21140},\n publisher = {Curran Associates, Inc.},\n title = {Escaping the Gravitational Pull of Softmax},\n url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/f1cf2a082126bf02de0b307778ce73a7-Paper.pdf},\n volume = {33},\n year = {2020}\n}"
                },
                "questions": {
                    "value": "The authors claim at some point that exploration strategies bias the policy. An exception is which we constrain the policy to stay in the vicinity of the previous policy, in which case we maintain optimality. It would have been interesting to explore such methods which are theoretically optimal."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2823/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2823/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2823/Reviewer_kiQP"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2823/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698782740549,
            "cdate": 1698782740549,
            "tmdate": 1700703970274,
            "mdate": 1700703970274,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pUWaqRavxZ",
                "forum": "pUIANwOLBN",
                "replyto": "6AUM4NERbI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2823/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2823/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response Reviewer kiQP"
                    },
                    "comment": {
                        "value": "Dear reviewer,\n\nWe appreciate your feedback and the interest you give to our work.\nWe have taken your comments into account and included your references into the document.\nWe have significantly reworked the introduction and Section 4.1.\nPlease find our detailed responses to your questions below.\n\n> I would have liked to see however how these methods generalize over problem instances, as it is known from \\citet{mei2020} that PG methods are highly impacted by initialization and problem instances.\n\nIndeed, initialization plays a crucial role in the optimization.\nThis influence is implicitly embedded in our criteria that consider the complete parameter space and in our examples for which all (potentially initial) parameters are analysed in the light of the criteria.\nFurthermore, the criteria can be extended and defined over a set of parameters where they hold true, such that policies initialized within this set would benefit from the properties and guarantees outlined in the paper.\nSection 3 illustrates that, with a fixed exploration strategy, gradient ascent can compute the optimal solution when we are in the \"basin of attraction\" of the global optimum.\nAdditionally, the probabilities in the criteria from Section 4 may be low for certain parameters that will never be encountered by gradient ascent depending on the initialization, as illustrated in the example without exploration when the policy is initialized close to the optimum.\n\nRegarding generalization, we acknowledge that our illustrations were conducted on relatively simple use cases.\nThe low-dimensional spaces of states, actions, and parameters were chosen for clarity in the visual representations.\nIn addition, the experimental results were computed through exhaustive search in the parameter space combined with Monte-Carlo estimations, constraining the parameter space to low dimensions.\nMoreover, most exploration methods rely on function approximators for dealing with high dimensions, introducing complexities in learning dynamics that prevent isolating the sole effect of exploration.\nNevertheless, we believe that the strongly non-linear dynamics in the first environment and the sparse reward function in the second reflects sufficient complexity from an optimization perspective.\n\n> Some statements are rather vague, e.g. the authors claim that there is a trade-off between quasiconcavity and eps-coherence, but i\u2019m not sure how general this statement is, and the authors do not provide proofs that guarantee these statements hold on all problem instances. At other times the authors use vague words like \u201cappears\u201d so its difficult to understand if these small illustrations would carry over to agents at scale, and are general enough.\n\nAs mentioned above, we have reworked the paper to clarify a number of points.\nConcerning the trade-off, in the general case, modifying the objective has no guarantee of preserving the global optimum.\nSo, the greater the lambda weights, the greater the objective modification, and the greater the epsilon value.\nSimilarly, if you modify the objective only slightly, the shape of the learning objective will not be greatly modified, and the _concavity_ remains unchanged.\nThere is thus at best a trade-off, which we observe in the example, and at worst both criteria are violated.\n\n> The experiment illustrated in Fig 2 is interesting, but wasn\u2019t this already known, the problem generally is that we cannot know the value of lambda beforehand as these are problem dependent, sometimes state-dependent and might also change over the optimization landscape for different policies on the way toward the optimal solution.\n\nRelated works have been clarified in Section 1.\nA similar result had been stated (less formally) in the case of entropy regularization (maximum entropy RL) but had not been formalized nor stated in a general case to our knowledge."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2823/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700559989594,
                "cdate": 1700559989594,
                "tmdate": 1700559989594,
                "mdate": 1700559989594,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4wH5oqR2IL",
                "forum": "pUIANwOLBN",
                "replyto": "pUWaqRavxZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2823/Reviewer_kiQP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2823/Reviewer_kiQP"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal response"
                    },
                    "comment": {
                        "value": "Thank you for the response. \n\nUpon trying to reproduce the illustrations in the paper, I found that I could not. I do not believe there is sufficient information to reproduce them. You also did not provide any code for reproducibility, correct?\n\nSince the main contribution of the paper in these illustrations, without other theory or empirical results, the least the authors need to do is provide sufficient information in the paper, or a framework/codebase for others to test around hypotheses or exploration bonuses on the topic. \n\nFrom your results, I gather this avenue of reward bonuses is what you are exploring, so how can this be impactful to the community  in designing algorithms that better explore?"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2823/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700594787162,
                "cdate": 1700594787162,
                "tmdate": 1700594787162,
                "mdate": 1700594787162,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "c9w0WiggJt",
                "forum": "pUIANwOLBN",
                "replyto": "sGSL4jbH4T",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2823/Reviewer_kiQP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2823/Reviewer_kiQP"
                ],
                "content": {
                    "title": {
                        "value": "Re score"
                    },
                    "comment": {
                        "value": "Dear authors,\n\nThank you for your response. \nI think this paper is insightful and a promising direction of investigation.\nHowever, I tend agree with the other reviewers on requiring more evidence.\n\n\nI will lower my original total score, which was a bit of an outlier."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2823/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700703926166,
                "cdate": 1700703926166,
                "tmdate": 1700703926166,
                "mdate": 1700703926166,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]