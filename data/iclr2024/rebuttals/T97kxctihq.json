[
    {
        "title": "Revisiting Long-term Time Series Forecasting: An Investigation on Affine Mapping"
    },
    {
        "review": {
            "id": "7bYSCSynjf",
            "forum": "T97kxctihq",
            "replyto": "T97kxctihq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1688/Reviewer_TkUX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1688/Reviewer_TkUX"
            ],
            "content": {
                "summary": {
                    "value": "This paper offers a timely and rigorous critique of the recent trend of using transformer-based models for long-term time series forecasting, and arrives at several surprising and important empirical conclusions. One of these conclusions is that just a simple linear layer (as in DLinear) combined with RevIN (reversible instance normalization with a learnable affine mapping of statistics) results in superior long-term forecast performance. The second is, even more shockingly, that many popular transformer-based methods with random parameter initializations already outperform their trained counterparts on long-term multivariate point forecasting tasks. The paper further investigates failure modes of single-layer linear forecasters in the multivariate setting. \n\nThe paper is well-written and of interest to the community in terms of the findings it offers. However, the novelty of the paper is limited for both methodology and theory."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is well-written, and its empirical findings are surprising and insightful for time series forecasting practitioners. \n\nThe first such conclusion results in a new and easy-to-implement model: RLinear. RLinear is simply DLinear with RevIN, and offers competitive performance. The second empirical exploration calls into question the use of large transformer architectures for forecasting. When using transformers to \"extract features\" the authors find that the learned mappings are easily confused and fail to capture the most basic signals with periodicity. In fact, this effect of \"overfitting\" appears so pronounced that the random initializations of these models appear to generalize almost better than their fitted counterparts. This is a very suprising finding that I look forward to verifying with the experimental setup the authors will make available.\n\nThe paper then moves to discussing why linear maps suffice to fit periodic signals. They offer an interesting comparative study of how linear models behave with different methods for normalization/disentanglement."
                },
                "weaknesses": {
                    "value": "My main critique of the paper is its limited novelty. While the insights offered by the paper are quite useful, the main methodological advance offered simply combines two very recent ideas. Moreover, the validity of this method is also not rigorously tested with an ablation/comparative study. For example, there is no study as in Table 2 for the idea in Figure 8. \n\nThe theoretical results proposed in the paper are hardly novel. For example, Thm 1 barely needs to be denoted so (periodic functions can be reconstructed by shifting, and shifting is a linear operator..). Similarly, Thm 3 produces a somewhat unsurprising conclusion which is not contextualized well in the paper's presentation.\n\nAmong other points the authors may like to consider:\n- For the uninitiated reader, the architecture of RLinear (and its simplicity) are hard to understand, and Figure 1 are hard to understand. Perhaps they could be revisited\n- Footnote 4 and how it relates to the context is unclear.\n- Please cite the DLinear paper with the accepted venue.\n- Please revisit the conclusion section for grammar: e.g.,  \"investigate\" \"where they generally prone\" \"encounter\"\n- Just a suggestion: The recent TiDE paper attacks some of the same problems as the authors. Although that work does not appear to be peer-reviewed yet it may be worthwihle to briefly discuss it in context."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1688/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1688/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1688/Reviewer_TkUX"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1688/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698582795075,
            "cdate": 1698582795075,
            "tmdate": 1699636097138,
            "mdate": 1699636097138,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KIVrDWzzTL",
                "forum": "T97kxctihq",
                "replyto": "7bYSCSynjf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1688/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1688/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer TkUX (1/1)"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful review and for highlighting the strengths of our work. Your critiques are greatly appreciated and we believe they will significantly improve the quality of our paper. We would like to address your concerns as follows:\n\n> **C1: The limited novelty of the study and theoretical results**\n\nWe understand your perspective on the novelty of the study and theoretical results because they seems trivial and hardly novel. However, it is important to clarify that our primary objective in this study was not to introduce new algorithms, but rather to illuminate the intrinsic efficacy of affine mapping in LTSF tasks. We believe that our theoretical analysis and empirical studies do represent a substantial technical contribution. We not only elucidate why linear models perform well in LTSF tasks, but also remind future researchers to conduct more ablation experiments as we did in Section 2 to confirm the effectiveness of their methods. Additionally, we highlight the impact of the input horizon length and tricks like RevIN. This findings could potentially guide future research and enhance the development of more effective models.\n\n> **C2: The rigour of the comparative study in Table 2 for the idea in Figure 8.**\n\nWe acknowledge that a more rigorous ablation study would strengthen our paper. However, the primary aim of Table 2 is to show that simple baselines, such as RLinear and RMLP, can yield competitive performance compared to recent complicated methods on LTSF tasks, implying potential questions regarding the effectiveness of these methods and the suitability of the datasets used. On the other hand, Figure 8 illustrates the limitations of linear models in fitting trend terms, both with and without the help of disentanglement and normalization. The purposes served by the table and the figure are distinct and complementary, with each contributing unique insights to our analysis.\n\n> **C3: Clearer explanation and clarification of some points**\n\nThank you for your thoughtful suggestions to our presentation. We will revise Figure 1 and provide more detailed explanations of used baselines in Appendix for the readers. As for Footnote 4, we can write the full update rule of $W,b$ as $W=W-\\eta\\frac{\\partial L}{\\partial W}=W-\\eta X^\\top\\frac{\\partial L}{\\partial Y}$ and $b=b-\\eta\\frac{\\partial L}{\\partial b}=b-\\eta\\frac{\\partial L}{\\partial Y}$ where $L$ denotes the loss function. Apparently the update of the weight and bias is coupled due to $\\frac{\\partial L}{\\partial Y}$. We will provide additional explanations to clarify its importance in the context of our discussion. As for citing the DLinear paper formally, we thank you for pointing out the omission. We will update the citation with the accepted venue for the DLinear paper. As for grammar issues in the conclusion section, we will revise the conclusion section to correct these issues. As for discussing the TiDE paper, we are aware of the TiDE paper and agree that it tackles similar problems to those addressed in our paper. We will include a discussion on this paper in our revised manuscript, acknowledging its relevance to our work.\n\nWe hope that these responses address your concerns and that our revisions will make the paper clearer and more valuable. Thank you again for your thoughtful feedback."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1688/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700726733497,
                "cdate": 1700726733497,
                "tmdate": 1700726733497,
                "mdate": 1700726733497,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zDIbHeFuz6",
            "forum": "T97kxctihq",
            "replyto": "T97kxctihq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1688/Reviewer_SH4h"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1688/Reviewer_SH4h"
            ],
            "content": {
                "summary": {
                    "value": "The authors of this work study the long-term time series forecasting (LTSF) problem. They demonstrate that a single linear layer can perform competitively for LTSF compared to other complex architectures. Specifically, with theoretical analysis and empirical studies, the authors find that affine mapping is critical in LTSF tasks and inspect its efficacy. While dealing with multivariate time series data, the limitation of linear models is also discussed."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The authors study the fundamental mechanisms that affect the performance of recent LTSF models, which is important for the community. \n2. This work provides theoretical and empirical evidence to support its findings.\n3. The presentation is well-organized and easy to follow."
                },
                "weaknesses": {
                    "value": "1. As a research work submitted to a top-tier ML conference, the technical contribution is limited. In particular, novel solutions/algorithms that leverage the important findings are expected. \n2. It is better to investigate more real-world time series datasets with perplexing patterns."
                },
                "questions": {
                    "value": "What about the efficacy of affine mapping when the models deal with noisy and low-quality time series data?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1688/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698631886447,
            "cdate": 1698631886447,
            "tmdate": 1699636097060,
            "mdate": 1699636097060,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "99kaLwcB3a",
                "forum": "T97kxctihq",
                "replyto": "zDIbHeFuz6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1688/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1688/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer SH4h (1/1)"
                    },
                    "comment": {
                        "value": "Thank you for your valuable feedback and insights. We appreciate your recognition of the importance of our study and its contribution to the understanding of the fundamental mechanisms affecting the performance of recent LTSF models. We would like to address your concerns as follows.\n\n> **C1: Limited technical contribution**\n\nWe appreciate your feedback regarding the perceived absence of innovative solutions or algorithms in our paper. However, it is important to clarify that our primary objective in this study was not to introduce new algorithms, but rather to illuminate the intrinsic efficacy of affine mapping in LTSF tasks. We believe that our theoretical analysis and empirical studies do represent a substantial technical contribution. We not only elucidate why linear models perform well in LTSF tasks, but also remind future researchers to conduct more ablation experiments as we did in Section 2 to confirm the effectiveness of their methods. Additionally, we highlight the impact of the input horizon length and tricks like RevIN. This findings could potentially guide future research and enhance the development of more effective models. We are looking forward to see new algorithms that leverage these insights, such as how to cope with trend terms without RevIN, and how to process multi-channel datasets as mentioned in Section 4.\n\n> **C2: Need for more real-world datasets**\n\nWe agree with your suggestion that testing on more diverse real-world datasets could further strengthen our results. However, it is crucial to note that the datasets we currently utilize for evaluation are also widely used in other LTSF benchmarks such as Informer, FEDformer, DLinear, and PatchTST. Given the setting of LTSF tasks which covers longer input horizon and prediction length, these tasks inherently contain increased seasonality. Thus, we are prompted to question the rationale behind LTSF tasks, as they tend to heavily rely on inherent seasonality within datasets. This raises potential concerns about the datasets currently in widespread use for evaluation purposes. It is possible that these datasets, with their inherent biases, may not provide a comprehensive or fair assessment of different forecasting methodologies. As we stated in Section 3.1, this may explain why a single linear layer performs well in many real-world datasets, since they often exhibit seasonality in days, weeks, months, and years. We also list one \"abnormal\" dataset in C3 to further support our viewpoints.\n\n> **C3: What about the efficacy of affine mapping when the models deal with noisy and low-quality time series data?**\n\nIndeed, our current datasets are noisy and of low quality. However, in LTSF tasks, the introduction of a longer observation window allows for the incorporation of more seasonal features, which in turn helps mitigate the impact of these noises. Here we introduce one \"abnormal\" dataset, Exchange, which has been evaluated in DLinear and PatchTST. This dataset comprises changes in exchange rates across eight countries over several years and exhibits minimal periodic characteristics. The table below presents the forecasting results from both DLinear and PatchTST.\n\n|Method|DLinear||PatchTST||Repeat|&emsp;\n|:-:|:-:|:-:|:-:|:-:|:-:|:-:\n|H|MSE|MAE|MSE|MAE|MSE|MAE\n|96|**0.081**|0.203|0.082|0.199|**0.081**|**0.196**\n|336|**0.305**|0.414|0.319|0.407|**0.305**|**0.396**\n\nAlthough the MSE and MAE evaluation metrics appear low, the actual visualized prediction results are disappointing. DLinear discovered that simply repeating the final value of historical time series outperforms all the baselines, while PatchTST deemed 'Exchange' as \"chaotic\" and consequently excluded it from their evaluation. We believe that this instance further supports the viewpoints we expressed in C2.\n\n---\n\nOnce again, we appreciate your thoughtful comments and questions. They have provided us with valuable directions for improving our work and for future research."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1688/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700726632924,
                "cdate": 1700726632924,
                "tmdate": 1700726632924,
                "mdate": 1700726632924,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QEt6IcKzgW",
            "forum": "T97kxctihq",
            "replyto": "T97kxctihq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1688/Reviewer_uhGz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1688/Reviewer_uhGz"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates the recent approaches in Long-Term Time Series Forecasting (LTSF) especially around the one-pass prediction over a single linear layer that is seen to achieve competitive performance on par with other complex architectures.\nThe authors state that affine mapping in LTSF models effectively capture periodic patterns and thus dominates in the forecasting performance over the baseline models. However, there are no information about what affine mapping is or how affine mapping is effectively introduced in the LTSF models to improve performance.\nThe authors state that a single linear projection layer with Reversible Normalization (RevIN) outperforms the state-of-the-art models currently because the single layer feature extractor learns weights that are consistent with the projection layer which indeed imply a mapping pattern between input to output series. But this doesn\u2019t effectively prove, affine mapping is the reason for the performance improvement.\nThe authors investigate the disentanglement of seasonal and trend terms of the time series model and state that there is research gap on the advanced models still left to explore. The authors also question the effectiveness of temporal feature extractors on LTSF tasks but there is no investigation around it that leads to the impact of the temporal feature extractors on the model performance other than affine mapping.\nFinally, the authors run a series of experimental evaluation and state that the large models, PatchTST and TimesNet do not exhibit significant improvement over baseline single layer models and the authors are assuming it to be the models\u2019 efficacy to learn periodicity through affine mapping. Also, in multiple channel use-cases, a single layer is observed to fail without modelling each channel independently which again questions the findings of the investigation."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. The paper gives a detailed analysis of various use-cases of LTSF on public datasets and makes solid observations.\n2. The authors prove the importance of Reversible Normalization(RevIN) and increasing input horizon on multi-channel on the LTSF model performances over multiple in-depth experiments."
                },
                "weaknesses": {
                    "value": "1. The findings of the paper in the abstract are not evidently proved in the experiments.\n2. This being an investigation paper, the experiments around affine mapping don\u2019t include Mean\nBias Error for evaluation which effectively depicts the efficacy of affine transformation.\n3. The conclusion states that the affine mapping dominates in \u201csome\u201d LTSF models but there is no\ndetailed information on this generalized statement.\n4. TYPOS: It\u2019s a well-written paper. I had to point out just one on a high-level overview. Use of \u201cInstead\u201d and \u201cApart from\u201d with \u201cHowever\u201d in the Introduction were slightly confusing. Please consider checking those."
                },
                "questions": {
                    "value": "The work in this paper is detailed and had observations from extensive experiments on both simulated and real-world datasets, but the findings are inconsistent. The findings stated in the abstract are not derived across the experiments through the conclusion.\nI believe there are many studies on why and how affine transformations can improve time-series forecasting models. As an investigation paper, I would recommend to consider the pros and cons of the current research to effectively conclude the findings and future work that is required on the topic. For example, in this paper, X. M. Chen, Y. Li, R. Z. Wang; Performance study of affine transformation and the advanced clear-sky model to improve intra-day solar forecasts. J. Renewable Sustainable Energy 1 July 2020; 12 (4): 043703., the results about affine transformation is similar to the findings of our paper. This paper should consider investigating the use-case used in this paper and fetch out the results and challenges faced in this use-case."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1688/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698699620958,
            "cdate": 1698699620958,
            "tmdate": 1699636096955,
            "mdate": 1699636096955,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BchFLJTHxP",
                "forum": "T97kxctihq",
                "replyto": "QEt6IcKzgW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1688/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1688/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer uhGz (1/2)"
                    },
                    "comment": {
                        "value": "We appreciate the time of the reviewer and thank the reviewer for acknowleding that our paper makes solid observations and gives a detailed analysis. We provide the detailed response to each question in the following.\n\n> **Q1: However, there are no information about what affine mapping is or how affine mapping is effectively introduced in the LTSF models to improve performance.**\n\nThe affine function is clearly defined in this paper, plese see the footnote of Page 1 (i.e., \"In general, a linear layer is defined as an affine transformation $\\mathbf{Y}=\\mathbf{X}\\mathbf{W}^\\top+\\mathbf{b}$\") and Equation (1) of Section 3.1 (with a title ROLES OF AFFINE MAPPING IN FORECASTING).\n\n> **W1: The findings of the paper in the abstract are not evidently proved in the experiments.**\n\nWe are unclear to the logic why the reviewer draws this conclusion since the reviewer acknowledges that \"this paper gives a detailed analysis of various use-cases of LTSF on public datasets and makes solid observations.\" \n\nTo make it clear, we relist our findings stated in the abstract here again: 1) affine mapping in some LTSF models dominates forecasting performance across commonly utilized benchmarks; 2) affine mapping can effectively capture periodic patterns but encounter challenges when predicting non-periodic signals or time series with different periods across channels; and 3) using reversible normalization and increasing input horizon can significantly enhance the robustness of models. It is not hard to find through this paper that Finding 1) is supported by Section 2, for example in Figures 2, 3 and Table 1; Finding 2) is supported by Section 3.1 and Section 4, in which we have provided extensive empirical and theoretical study of linear models on LTSF tasks; and Finding 3) is supported by Section 3.2 and Section 4, as shown in Table 3 and Figures 7, 8, 9, 12, 13.\n\nMoreover our findings are supported by all the other three reviewers (SH4h, TkUX, NWqo). The evidences are as follows:\n* Reviewer SH4h \"This work provides theoretical and empirical evidence to support its findings.\"\n* Reviewer TkUX: \"This paper offers a timely and rigorous critique of the recent trend of using transformer-based models for long-term time series forecasting, and arrives at several surprising and important empirical conclusions.\"\n* Reviewer NWqo: \"This paper provides various types of visualization to support their claims and to present the results of experiments.\"\n\nTherefore, we sincrely urge the reviewer to carefully read our paper and other reviews."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1688/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700726461564,
                "cdate": 1700726461564,
                "tmdate": 1700726461564,
                "mdate": 1700726461564,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "80QxQo6pDY",
                "forum": "T97kxctihq",
                "replyto": "QEt6IcKzgW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1688/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1688/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer uhGz (2/2)"
                    },
                    "comment": {
                        "value": "> **Q2: As an investigation paper, I would recommend to consider the pros and cons of the current research to effectively conclude the findings and future work that is required on the topic. For example, in this paper, X. M. Chen, Y. Li, R. Z. Wang; Performance study of affine transformation and the advanced clear-sky model to improve intra-day solar forecasts. J. Renewable Sustainable Energy 1 July 2020; 12 (4): 043703., the results about affine transformation is similar to the findings of our paper. This paper should consider investigating the use-case used in this paper and fetch out the results and challenges faced in this use-case.**\n\nWe resectively disagree with the reviewer on \"This paper should consider investigating the use-case used in this paper and fetch out the results and challenges faced in this use-case.\" The reasons are as follows.\n\nThe goal of this paper, as stated in the abstract and the introduction, is to provide an analysis on \"previous studies have demonstrated that a single linear layer can achieve competitive forecasting performance compared to other complex architectures.\" As our research objects are other complicated architectures which are published in recent years, e.g., SCINet (NIPS 2022), TimesNet (ICLR 2023), DLinear (AAAI 2023), PatchTST (ICLR 2023), etc.\n\nThe paper by Chen et al. (2020) published in the Renewable Sustainable Energy journal does not provide an opportunity to investigate the pros and cons of recent Long-Term Traffic Speed Forecasting (LTSF) benchmarks. As a result, it is not relevant to our work. It is important to note that not investigating this paper does not impact the contribution of our paper to the LTSF community.\n\nMoreover, in our paper, the affine functions are implemented using a linear layer as $\\mathbf{Y}=\\mathbf{X}\\mathbf{W}^\\top+\\mathbf{b}$, as mentioned in the previous response. On the other hand, in the paper by Chen et al. (2020), the affine function is defined as $G_{\\text{past}}=aG_{\\text{est,past}}+b$, where $a$ and $b$ are scalar factors to control the bias. This definition is more similar to quantile mapping, as also mentioned in Chen et al. (2020). Besides, Chen et al. (2020) only provide experimental study on improving intra-day solar forecasts, while our study covers more fields and provides additional empirical and theoretical evidence.\n\nAdditionally, our paper has received positive feedback from Reviewer TkUX, who states,  \"This paper offers a timely and rigorous critique of the recent trend of using transformer-based models for long-term time series forecasting. \" Reviewer SH4h also praises our paper that \"The authors study the fundamental mechanisms that affect the performance of recent LTSF models, which is important for the community.\n\nTherefore, we are concerned that there may be a misunderstanding between our study and yours. We hope that these responses address your concerns. Thank you again for your feedback."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1688/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700726563253,
                "cdate": 1700726563253,
                "tmdate": 1700732471407,
                "mdate": 1700732471407,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "kbvjtb3BKZ",
            "forum": "T97kxctihq",
            "replyto": "T97kxctihq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1688/Reviewer_NWqo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1688/Reviewer_NWqo"
            ],
            "content": {
                "summary": {
                    "value": "Long-term time series forecasting (LTSF) is an important problem. Based on a finding from previous work that a linear layer can achieve forecasting performance comparable to complex models such as Transformers, in this paper, the authors study the effectiveness of recent approaches and provide various findings about their limitations. The authors provide theoretical and experimental explanations to support their findings."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper provides various types of visualization to support their claims and to present the results of experiments.\n2. Simplicity is always appreciated. It is nice to observe simple models can achieve performance similar to or even better than complicated models."
                },
                "weaknesses": {
                    "value": "1. The main claim of Section 2 is unclear. Are simple models always better than complex models for LTSF, or is it the case only to the specific framework shown in Figure 1? What if we adopt a different but still complicated framework, such as 1-dimensional CNNs? How do the results change if we include traditional models such as AR (autoregression) or ARIMA?\n2. This paper is not self-contained. The experiments in Section 2 play a crucial role to motivate this work, but there is not enough description about the models and experimental setup. For example, RevIN is mentioned several times throughout the paper, but there is no definition of it. If the page limit is a problem, the authors could have added the details to Appendix.\n3. Theorem 1 and 2, which are the main theoretical contributions of this work, seem trivial. If a time series can be clearly (and linearly) separated into seasonality and trend parts, I think it is obvious that a linear layer (or any linear function) is able to learn such separation."
                },
                "questions": {
                    "value": "1. What is the main improvement of this work from (Zeng et al., 2022)? This works seems like a re-verification of the claim made by the previous work.\n2. What do the authors suggest as a result of this work? Should we replace the benchmarks for LTSF with simple linear models?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1688/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698822603317,
            "cdate": 1698822603317,
            "tmdate": 1699636096866,
            "mdate": 1699636096866,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Qnr8DMaXZ9",
                "forum": "T97kxctihq",
                "replyto": "kbvjtb3BKZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1688/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1688/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer NWqo (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for taking the time to review our paper and for your valuable comments. We would like to address your concerns as follows.\n\n> **Q1: Are simple models are always better than complex models for LTSF, or is it only relative to the specific framework shown in Figure 1?**\n\nThank you for your insightful question. We'd like to clarify that our primary claim is not that simple models are always better than complex models in forecasting tasks. Rather, we argue that in the context of LTSF benchmarks, which are widely used in current LTSF research, the simple models we've discussed typically demonstrate robust performance. It is worth noting that Transformer-based methods such as Informer, Autoformer, and FEDformer were once predominant in LTSF tasks. However, DLinear (Zeng et. al, 2022) showed that a simple linear layer can outperform these more complex approaches, which indicates the significance of directly modeling the relationship between input and output time series. Consequently, a trend has emerged to move away from the encoder-decoder structure towards designing modules that capture temporal and channel dependencies. These methods generally use a single projection layer to generate the final forecasting results, largely aligning with the framework we simplified in Figure 1. This is why we've chosen to analyze the efficacy of each component in these methods. We discovered the importance of RevIN and projection layer, which were not discussed in detail in the original paper.\n\n> **Q2: How would the results change with a different, yet complex framework such as 1-dimensional CNNs?**\n\nWe have provided one typical CNN-based model, SCINet, specifically designed for LTSF tasks. SCINet employs a tree-like structure of 1D-CNNs to handle interleaved subsequences, downsampled from the original time series. The experimental results obtained from SCINet align well with those from other models. Notably, due to the equivalence of a single 1x1 convolutional layer and a linear layer, substituting the linear layer with a 1x1 convolutional layer will obtain the same results.\n\n> **Q3: How would the results change if traditional models such as AR or ARIMA were included?**\n\nThank you for your suggestion to incorporate traditional models such as AR and ARIMA. However, as extensively discussed in previous works like DLinear (Zeng et al., 2022) and Informer (Zhou et al., 2020), these models tend to underperform in LTSF tasks due to the accumulation of errors caused by regressive generation. This is the primary reason we chose not to include these traditional models in our study. We only include recent methods that utilize non-autoregressive generation, as illustrated in Figure 1.\n\n> **Q4: There is not enough description about the models and experimental setup, such as the definition of RevIN.**\n\nWe appreciate your insightful comments and recognize the need for enhanced clarity in our methodology, particularly for readers who may not be as familiar with the topic. To address this, we will include comprehensive descriptions of the utilized baselines and modules such as RevIN in Appendix A. We apologize for any initial lack of detail, and we are grateful for your constructive feedback.\n\n> **Q5: What is the main improvement of this work over DLinear (Zeng et al., 2022)?**\n\nIn the original paper of DLinear, the authors questioned whether transformers effective for time series forecasting, and then proposed a family of linear models which outperform Transformer-based methods across multiple LTSF datasets. However, their demonstration of the robust performance of linear models was largely experimental, lacking in-depth theoretical substantiation for why linear models excel in LTSF tasks. Our research builds upon the foundation laid by DLinear, offering comprehensive empirical and theoretical results to elucidate the efficacy of linear models in LTSF tasks. Our work also highlights their limitations and propose potential remedies.\n\nBesides, we have to emphasize that one crucial factor contributing to DLinear's success is the length of the input horizon. DLinear adopts a longer input horizon (336) compared to previous methods (96), without providing a comprehensive explanation \u2014 their reasoning was limited to 'underfitting' or 'overfitting' problems when varying the input horizon length. Subsequent works, such as PatchTST, have also adopted longer input horizons (512) to enhance their forecasting performance. Conversely, some approaches like TimesNet opt for shorter input horizons to undermine DLinear's performance in comparative studies. Our research underscores the importance of the input horizon length from both empirical and theoretical viewpoints, paving the way for more fair performance comparisons in the future."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1688/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700726326863,
                "cdate": 1700726326863,
                "tmdate": 1700726326863,
                "mdate": 1700726326863,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]