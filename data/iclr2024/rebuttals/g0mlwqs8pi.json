[
    {
        "title": "Adaptive Federated Learning with Auto-Tuned Clients"
    },
    {
        "review": {
            "id": "ZOY4gEuIvE",
            "forum": "g0mlwqs8pi",
            "replyto": "g0mlwqs8pi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6403/Reviewer_Yzoy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6403/Reviewer_Yzoy"
            ],
            "content": {
                "summary": {
                    "value": "This paper demonstrates an interesting phenomenon where not properly tuning the local step size of the GD algorithm might negatively impact performance of an FL system. To address this, this paper proposes an adaptive scheme for setting local step sizes called $\\Delta$-SGD, and provides an analysis of its convergence under mild assumptions. Finally, the paper provides a thorough empirical demonstration of the proposed method, comparing it with other optimizers when used by FedAvg."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is clearly written and well motivated.\n\nTheorem 1 and its proof in Appendix A seems to be the main contribution. I have checked and believed that the proofs are sound.\n\nThe empirical results presented are generally promising."
                },
                "weaknesses": {
                    "value": "1. Novelty:\n- I believe $\\Delta$-SGD is algorithmically similar to the Adaptive SGD method in the paper \"Adaptive Gradient Descent without Descent\" (Malitsky and Mishchenko, 2020). While there are some attempts to cite and discuss this paper, I think the authors should be more upfront about it, as it changes the perceived contribution entirely.\n- To the best of my knowledge, Malitsky and Mishchenko did not deal with the FL setting, so I believe theorem 1 and its proof are novel. However, I think the authors should provide a proof sketch to help readers understand the nuance of the proving strategy. What is hard/non-trivial about applying Adaptive SGD in the FL setting?\n\n2. Significance:\n- Despite the claim that $\\Delta$-SGD is complementary to other FL methods that perform server adaptation this is not demonstrated in the experiments.\n- I think Table 2b should be expanded to become one of the main result (conduct over all datasets, rather than as an ablation study on 2 datasets).\n-  The gain in performance is not significant enough in many cases. The authors should provide some error bar or standard deviation."
                },
                "questions": {
                    "value": "1.The result presentation is a little bit confusing. The small number on the right of Table 1 is the performance difference from the best result, but what is this \"best result\" referring to? Best among \\alpha = 1.0/ 0.1/ 0.01 ?\n\n2. For all methods, the authors performed grid search on CIFAR-10 and apply it on other test scenarios. Why not performing grid search on each individual setting? Is it because of the expensive cost of hyperparameter tuning?\n\n3. If hyperparameter tuning is the bottleneck (and the motivation), how will this method compare to the method FedEx proposed by the paper Federated Hyperparameter Tuning: Challenges, Baselines, and Connections to Weight-Sharing (Khodak et al., 2021)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6403/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698610458978,
            "cdate": 1698610458978,
            "tmdate": 1699636711555,
            "mdate": 1699636711555,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6uT9z5r5Zh",
                "forum": "g0mlwqs8pi",
                "replyto": "ZOY4gEuIvE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6403/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6403/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the review!"
                    },
                    "comment": {
                        "value": "Dear Reviewer Yzoy,\n\nThank you for your valuable comments and feedback. We are encouraged that you found our paper clearly written and the empirical results promising. We also appreciate you taking the time to check our proof.  \n\n$\\textcolor{purple}{\\textbf{Weaknesses}}$\n\n> **Q: $\\Delta$-SGD is algorithmically similar to the Adaptive SGD method in the paper [1]?** \n\n**A:** You are right. $\\Delta$-SGD is indeed inspired by [1]. We will make the connection more explicit when we introduce $\\Delta$-SGD. However, as we detail in the following answer, extending the proof to a distributed nonconvex setting is nontrivial. Further, any empirical studies on how [1] can be developed and how it works in FL settings were missing from the literature; this is the purpose of this work, among other goals.\n\n---\n\n\n> **Q: [1] did not deal with the FL setting, so Theorem 1 and its proof is novel. But proof sketch would help. What is hard/nontrivial about applying Adaptive SGD in the FL setting?**\n\n**A:** We are glad you asked this question so we can explain in detail the nontriviality of extending proof in [1] to the FL setting. Before delving into this, we first kindly remind you that Theorem 1 is not our only theoretical extension of [1]: \n- As you are aware, in Appendix A.1, we provide the proof of Theorem 1, which is for distributed nonconvex settings and includes stochasticity and local iterations.\n- Appendix A.2 provides proof that extends [1] to a distributed setting where $f_i$ is convex.\n- Appendix A.3 provides a simple proof in the centralized nonconvex setting.\n\nThat being said, there are several challenges in extending the theoretical result in [1] to the FL setting, as we briefly wrote before Assumption 1 in Section 3.1. A primary obstacle is that since $\\eta_{t, k}^i$ is prescribed using known quantities, we cannot simply *choose* a *common constant step size* to work for all clients. Our proof has to deal with adaptive step size, which is different across all clients. Moreover, we have to account for the \"model drift\" between the server and the client model parameters due to local iterations performed by each client, which is trickier to handle on top of stochasticity and the individual adaptive step size we use.\n\nWe can add this explanation before introducing Assumption 1 in the revision.\n\n---\n\n\n> **Q: Despite the claim that $\\Delta$-SGD is complementary to other FL methods that perform server adaptation, this is not demonstrated in the experiments.**\n\n**A:** Thank you for the comment. *We added FedAdam experiments in Appendix B.1* of the updated supplementary material. \n\n---\n\n\n> **Q: Table 2b should be expanded to become one of the main result (conduct over all datasets, rather than as an ablation study on 2 datasets).**\n\n**A:** Thank you for the suggestion. *We have added a new table in Appendix B.2* where we have conducted experiments using the FedProx loss function over all datasets, per the suggestion.\n\n---\n\n\n> **Q:\u200b\u200b The authors should provide some error bar or standard deviation.**\n\n**A:** Thank you for the suggestion! *We have included additional results in Appendix B.3* of the updated supplementary material, plotting and reporting the average and standard deviation of three independent trials for CIFAR-10 classification trained with a ResNet-18."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6403/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700478742137,
                "cdate": 1700478742137,
                "tmdate": 1700689332266,
                "mdate": 1700689332266,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "p1gv9HdeQc",
                "forum": "g0mlwqs8pi",
                "replyto": "ZOY4gEuIvE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6403/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6403/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the review!"
                    },
                    "comment": {
                        "value": "$\\textcolor{purple}{\\textbf{Questions}}$\n\n> **Q1: The small number on the right of Table 1 is the performance difference from the best result, but what is this \"best result\" referring to? Best among \\alpha = 1.0/ 0.1/ 0.01 ?**\n\n**A:** *Best result* refers to the highest accuracy within a fixed experiment (including a fixed $\\alpha$) among *different client optimizers*. For example, for CIFAR-10 classification trained with a ResNet-18 with $\\alpha=1$, the best result is achieved by $\\Delta$-SGD with the achieved accuracy 89.8. For this setting, SGD achieves 87.7$_{\\downarrow(2.1)}$, and $89.8 = 87.7 + 2.1$. We remind the reviewer that $\\alpha$ changes client data heterogeneity (c.f., Figure 7 in Appendix B.6), so different $\\alpha$ constitute a different experimental setup.\n\n---\n\n\n\n> **Q2: For all methods, the authors performed grid search on CIFAR-10 and apply it on other test scenarios. Why not performing grid search on each individual setting? Is it because of the expensive cost of hyperparameter tuning?**\n\n**A:** This is precisely why we are proposing $\\Delta$-SGD, as it can perform well ***without any additional tuning across different scenarios***. To provide a bit more perspective on how much computing/time effort is needed for step size tuning, let us focus on CIFAR-10 classification trained with a ResNet-18, with a fixed $\\alpha$. For this task, SGD takes about 22 seconds per communication round. Since we run 2000 epochs, this amounts to 44,000 seconds, roughly 12 hours. Since we tried four different step sizes, it takes 48 hours *just for SGD*. Now consider adding learning rate decay, on top of that adding momentum, and consider performing grid-search of other client optimizers, not to mention different $\\alpha$'s and larger-scale experiments using CIFAR-100 and ResNet-50. \n\n---\n\n\n> **Q3: How does $\\Delta$-SGD compare to the method FedEx proposed by the paper \u201cFederated Hyperparameter Tuning: Challenges, Baselines, and Connections to Weight-Sharing\u201d (Khodak et al., 2021)?**\n\n**A:** Thank you for the reference. While FedEx and $\\Delta$-SGD can be loosely connected in the sense of hyperparameter tuning, we clarify that FedEx *requires solving yet another optimization problem (Eq. (6) in [2])*. It is unclear whether solving that optimization problem is computationally more accessible or more challenging than performing a grid search. Our proposed method, $\\Delta$-SGD, is fundamentally different because it removes the need for client step size tuning while achieving outstanding performance.\n\n---\n\n\nWe hope the above answers clarify your concerns, and if so, we hope you could consider reflecting on the score. Thank you again for your time in reviewing our manuscript.\n\n**References**\n\n[1] Malitsky and Mishchenko (2020) \"Adaptive Gradient Descent without Descent\" \n\n[2] Khodak et al. (2021) \u201cFederated Hyperparameter Tuning: Challenges, Baselines, and Connections to Weight-Sharing\u201d"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6403/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700478759120,
                "cdate": 1700478759120,
                "tmdate": 1700689362422,
                "mdate": 1700689362422,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cxFshooFzN",
                "forum": "g0mlwqs8pi",
                "replyto": "ZOY4gEuIvE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6403/Reviewer_Yzoy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6403/Reviewer_Yzoy"
                ],
                "content": {
                    "title": {
                        "value": "Discussion"
                    },
                    "comment": {
                        "value": "Thanks for replying to my questions. \n\n- Regarding the claim that \"it can perform well without any additional tuning across different scenarios\" -- is this true when there are still hyperparameters in your framework (hyper-hyperparameter in this case) such as \\gamma, \\theta_0 and \\eta_0? \n\n- The extra experiment with error bar did more to unconvince me than to convince, seeing that \\Delta-SGD, Adam and Adagrad practically converge to the same performance.\n\n- The cost of grid search is quite interesting. It would be insightful to provide a heatmap of model performance on the grid of hyperparameters vs. \\Delta-SGD performance, which would demonstrate how likely can other optimizers match the performance of \\Delta-SGD if we simply choose a random set of hyperparameters (this is only a suggestion for future work -- I understand it would be hard to perform this exp now).\n\n- Regarding FedEx, I am aware of the inner optimization problem, and I am under the impression that it would be cheaper than doing a dense grid search. That being said, it would be more of an apples-to-apples comparison than your current experiments, seeing that current baselines do not auto-tune hyperparameters at all."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6403/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700586098212,
                "cdate": 1700586098212,
                "tmdate": 1700586127529,
                "mdate": 1700586127529,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yuAJWlYcqz",
                "forum": "g0mlwqs8pi",
                "replyto": "ZOY4gEuIvE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6403/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6403/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Discussion continued"
                    },
                    "comment": {
                        "value": "Thank you for the additional comments.\n\n> **Q:** Regarding the claim that \"it can perform well without any additional tuning across different scenarios\" -- is this true when there are still hyperparameters in your framework (hyper-hyperparameter in this case) such as \\gamma, \\theta_0 and \\eta_0?\n\n**A:** Yes, the fact that $\\Delta$-SGD can perform well without any additional tuning is the main point we are trying to convey in this paper. As we mention in Section 4 (footnote 3, pg. 6), we use $\\gamma = 2$, $\\eta_0 = 0.2$, and $\\theta_0=1$ for all experiments. From this setting, *without any additional tuning*, $\\Delta$-SGD achieves the performance summarized in Table 1: *TOP-1 test accuracy in 73%, and TOP-2 accuracy in 100% of the experiments (in Table 1).*\n\nWe also kindly remind you that among 9 additional experiments we added in Appendix B.1 (FedAdam) and B.2 (FedProx) to address your original comments on the weaknesses, $\\Delta$-SGD achieves TOP-1 test accuracy in 6/9 settings and TOP-2 test accuracy in 8/9 settings, without any additional tuning.\n\n---\n\n\n> **Q:** The extra experiment with error bar did more to unconvince me than to convince, seeing that \\Delta-SGD, Adam and Adagrad practically converge to the same performance.\n\n**A:** The main point we are trying to emphasize is that *$\\Delta$-SGD performs well without any additional tuning across different settings.* We have updated the supplementary material again, where we also added the average and standard deviation for three independent trials of FMNIST classification trained with a shallow CNN (Appendix B.3, Figure 4 and Table 5). For readability, we report the average test accuracies (among three trials) of two tasks below:\n\n|              | CIFAR-10 + ResNet-18 | FMNIST + CNN |\n|:--------------:|:----------:|:--------:|\n| SGD          | 73.96    | 83.43  |\n| SGD decay    | 67.79    | 77.83  |\n| SGDM         | 77.59    | 83.27  |\n| SGDM decay   | 72.11    | 81.36  |\n| Adam         | 83.24    | 79.98  |\n| Adagrad      | 83.86    | 53.06  |\n| SPS          | 68.38    | 84.59  |\n| $\\Delta$-SGD | 83.89    | 85.21  |\n\nFor FMNIST classification, for instance, Adam and Adagrad perform quite poorly. Critically, **none of the other client optimizers achieve good performance in _both settings_.** $\\Delta$-SGD, on the other hand, not only achieves the best average test accuracies in both settings, but also achieves extremely small (either the smallest or the second smallest) standard deviations.\n\n---\n\n\n> **Q:** The cost of grid search is quite interesting. It would be insightful to provide a heatmap of model performance on the grid of hyperparameters vs. \\Delta-SGD performance, which would demonstrate how likely can other optimizers match the performance of \\Delta-SGD if we simply choose a random set of hyperparameters (this is only a suggestion for future work -- I understand it would be hard to perform this exp now).\n\n**A:** Thank you for the suggestion! As you noted, we do not have time to include such plot during the discussion period, but we will be happy to include one for the final version.\n\n---\n\n\n> **Q:** Regarding FedEx, I am aware of the inner optimization problem, and I am under the impression that it would be cheaper than doing a dense grid search. That being said, it would be more of an apples-to-apples comparison than your current experiments, seeing that current baselines do not auto-tune hyperparameters at all.\n\n**A:** *The fact that current baselines are not able to auto-tune hyperparameters at all is exactly why we are proposing $\\Delta$-SGD*: it simply can auto-tune itself, and work well across many different settings. We will be happy to try FedEx with other client optimizers, but we want to clarify once again that *removing the need for expensive grid-search is a major point of our proposal, $\\Delta$-SGD*."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6403/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700606825527,
                "cdate": 1700606825527,
                "tmdate": 1700689379584,
                "mdate": 1700689379584,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DgDMkrLGkz",
                "forum": "g0mlwqs8pi",
                "replyto": "yuAJWlYcqz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6403/Reviewer_Yzoy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6403/Reviewer_Yzoy"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your responses"
                    },
                    "comment": {
                        "value": "I appreciate the explanations and the new results. Although I will keep my score for now (which already reflects my positive sentiment), I will consider raising it after my discussion with other reviewers to see if their concerns are addressed.\n\nBest,\nReviewer Yzoy"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6403/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700709828933,
                "cdate": 1700709828933,
                "tmdate": 1700709828933,
                "mdate": 1700709828933,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9zgDCc1ahC",
            "forum": "g0mlwqs8pi",
            "replyto": "g0mlwqs8pi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6403/Reviewer_RQMs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6403/Reviewer_RQMs"
            ],
            "content": {
                "summary": {
                    "value": "The authors proposed using a locality adaptive step size rule for client-side training in Federated Learning frameworks. Their method requires almost no hyperparameter tuning. They showed the superiority of their algorithm by comparing it with other adaptive client-side step-size methods experimentally. They inspired this technique from a similar technique proposed for centralized training. They proved its convergence guarantees for both convex and nonconvex functions."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tFederated learning frameworks need a careful design to handle the heterogeneity across clients. A local learning rate best for a client may be suboptimal for another. The authors successfully apply a similar technique proposed for centralized ML to the federated problem. Also, they prove the convergence.\n2.\tTheir method allows them to use a milder \u201cL-smooth\u201d assumption than the one commonly used in nonconvex optimization analyses. They use the maximum of \u201caverage local smoothness over local steps\u201d across clients and rounds."
                },
                "weaknesses": {
                    "value": "1.\tWhen you presented the results based on the learning rate tuned on one model/dataset and used for all, the baseline methods were not as successful as yours. One natural question is how good those methods are if we tune for each model/dataset separately. Are all methods comparable? Or, how much the not optimum learning rates of baselines are different from the ones optimal for each dataset? \n2.\tThe bounded gradient assumption can be seen as a strong assumption, and it doesn\u2019t hold for many common functions.  Can those terms be bounded using the other two assumptions, bounded variance and dissimilarity?"
                },
                "questions": {
                    "value": "1.\tIn (4), client $i$ seems to use exact gradients to choose the step size. Shouldn\u2019t it be $\\tilde{\\nabla}{f_i(x_t^i)}- \\tilde{\\nabla}{f_i(x_{t-1}^i)}$, or how does client $i$ know true gradient? I guess, it is just a notation thing because there doesn\u2019t seem any problem in Algorithm 1.\n2.\tI couldn\u2019t get the intuition behind setting the initial step $\\eta_{t,0}^i$ to an arbitrary $\\eta_0$ at the beginning of each local training after the first one (line 6 in Algorithm 1). Wouldn\u2019t it be good to use the final step size of each client's latest local training?\n3.\tWhat parameter(s) did you tune for $\\Delta-SGD$ except $\\gamma$?\n4.\t\u201cThis is a counter-intuitive behavior, as one would expect to get better accuracy by using a more powerful model.\u201d $\\rightarrow$ Here, mentioning Adagrad, Adam, and SPS is finetuned on ResNet18 can be useful. I guess, it should be one of the reasons. Also, it would highlight your method doesn\u2019t need any additional tuning.\n5.\tIn (B) and (C) of Figure 3, have the parameters been tuned on MNIST+CNN and CIFAR-10+ResNet-18, respectively?\n6.\tYou may enrich the study by reporting how the learning rate changes across time and across clients in $\\Delta-SGD$.  Also, you can compare this with the best learning rate of the other methods. \n7.\tThe two parts of local learning rate calculation (line 9 in Algorithm 1) are clear in how they are used in the theoretical analysis. It may be good to include an experimental report to see in how many iterations the learning rate is updated based on the first part $\\left(\\frac{\\gamma \\lVert x^i_{t, k} - x^i_{t, k-1} \\rVert}{2 \\lVert \\tilde{\\nabla} f_i(x^i_{t, k}) - \\tilde{\\nabla} f_i(x^i_{t, k-1}) \\rVert}\\right)$ or the second part $\\left(\\sqrt{1 + \\theta^i_{t,k-1}} \\eta^i_{t,k-1}\\right)$. It may show that the minimum of those two is useful in practice as well."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6403/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6403/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6403/Reviewer_RQMs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6403/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698624779824,
            "cdate": 1698624779824,
            "tmdate": 1699636711296,
            "mdate": 1699636711296,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NnUoRgZDXL",
                "forum": "g0mlwqs8pi",
                "replyto": "9zgDCc1ahC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6403/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6403/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the review!"
                    },
                    "comment": {
                        "value": "Dear Reviewer RQMs,\n\nThank you for your detailed review. We are glad you acknowledge the superiority of $\\Delta$-SGD without almost no hyperparameter tuning. Below, we answer your comments and questions in detail.\n\n$\\textcolor{purple}{\\textbf{Weaknesses}}$\n\n> **Q: How good those methods are if we tune for each model/dataset separately? Also, how much the not optimum learning rates of baselines are different from the ones optimal for each dataset?**\n\n**A:** Thank you for the question. As one can infer from Figure 1 (left panel), all methods perform reasonably well when we tune the step size for each client optimizer. However, we are precisely trying to *remove the need for additional tuning when one changes either the dataset or the model architecture*. We also kindly refer the reviewer to the updated supplementary Appendix B.3, where we added a plot and a table reporting the average and the standard deviation among three independent trials of each client optimizer using the best respective hand-tuned step size, for the task of CIFAR-10 classification trained with a ResNet-18. \n\nRegarding how much the optimal learning rates of baselines differ from those optimal for each dataset, we kindly refer the reviewer to [1], where exactly that information is already present. Table 8 (Appendix D.4, page 29) of [1] shows the best learning rate for different datasets, both for the server and the client. Quite intuitively, the best-performing step size for each dataset differs significantly for each task. \n\n---\n\n\n> **Q: Bounded gradient assumption can strong. Can it be bounded using the other two assumptions, bounded variance and dissimilarity?**\n\n**A:** In general, gradient norm cannot be bounded with bounded variance and function dissimilarity because bounded variance measures the departure of the stochastic gradients $\\tilde{\\nabla} f_i(\\cdot)$ from their deterministic mean $\\nabla f_i(\\cdot)$, and the function dissimilarity measures the difference in gradients of the local cost functions $\\nabla f_i(\\cdot)$ and the average cost function $\\nabla f(\\cdot)$. We also kindly remind the reviewer that bounded gradient is quite a common assumption in non-convex optimization [2, 3, 4, 5], especially when using an adaptive optimizer. Still, removing the bounded gradient assumption can be an interesting future work to improve the theoretical analysis of our proposed method."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6403/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700477447097,
                "cdate": 1700477447097,
                "tmdate": 1700689216497,
                "mdate": 1700689216497,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EVR62mQmMY",
                "forum": "g0mlwqs8pi",
                "replyto": "9zgDCc1ahC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6403/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6403/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the review!"
                    },
                    "comment": {
                        "value": "$\\textcolor{purple}{\\textbf{Questions}}$\n\n> **Q1: In (4), client $i$ seems to use exact gradients to choose the step size. How does client know true gradient? I guess, it is just a notation thing because there doesn\u2019t seem any problem in Algorithm 1.\u201d**\n\n**A:** Yes, you are right: we used the exact gradients in Eq. (4) for more explicit exposition with simpler notations. As we write right below Eq. (5) (now highlighted in blue color in the updated supplementary material), \u201cFor the FL settings, we extend (4) by including stochasticity and local iterations, as summarized in Algorithm 1.\u201d \n\n---\n\n\n> **Q2: What is the intuition behind setting the initial step $\\eta_{t, 0}^i$ to an arbitrary $\\eta_0$ at the beginning of each local training after the one (line 6 in Algorithm 1). Wouldn\u2019t it be good to use the final step size of each client\u2019s latest local training?**\n\n**A:** You are correct that using the final step size of each client\u2019s latest local training would be good. We used an arbitrary $\\eta_0$ due to how we implemented the federated learning experiments. Regardless, the reason why arbitrary $\\eta_0$ is fine is that, with some reasonable $\\eta_0$ (e.g., 0.01), $x_{t, 1}^i$ will be reasonably close to $x_{t, 0}^i$. Then $\\eta_{t, 1}^i$ is already a good estimate of the (local) smoothness of $f_i$, only with one local iteration (from $k=0$ to $k=1$). \n\n---\n\n\n> **Q3: What parameter(s) did you tune for $\\Delta$-SGD except $\\gamma$?**\n\n**A:** *We want to emphasize that we did not tune $\\gamma$. As we write right below the pseudocode in pg. 5, $\\gamma$ is only needed for Theorem 1.* In practice, we keep the default value $\\gamma=2$ from the original implementation; tuning for $\\gamma$ might result in even better performance. For other parameters, as we wrote in the paragraph starting with **Hyperparameters** in pg. 6 of the original submission, we append $\\delta$ in front of the second condition of $\\eta_{t,k}^i$. Still, we use $\\delta=0.1$ for all experiments. We also kindly refer the reviewer to Appendix B.3, where we compare the performance of $\\Delta$-SGD for different values of $\\delta$ to demonstrate the unnecessity of tuning for $\\Delta$-SGD. \n\n---\n\n\n> **Q4: \u201cThis is a counter-intuitive behavior, as one would expect to get better accuracy by using a more powerful model.\u201d Here, mentioning Adagrad, Adam, and SPS is finetuned on ResNet18 can be useful. I guess, it should be one of the reasons. Also, it would highlight your method doesn\u2019t need any additional tuning.**\n\n**A:** Thank you! We took your suggestion and modified the supplementary material accordingly (highlighted in blue on pg. 8).\n\n---\n\n\n> **Q5: In (B) and (C) of Figure 3, have the parameters been tuned on MNIST+CNN and CIFAR-10+ResNet-18, respectively?**\n\n**A:** No, the parameters are tuned using Figure 1 (A). Then, as the caption reads, \u201cThe same step size from (A) is intentionally used in settings (B) and (C).\u201d Similar to our main results in Table 1, the purpose of Figure 1 was to highlight two points: \n$i)$ $\\Delta$-SGD works well without any tuning across different datasets, model architectures, and degrees of heterogeneity; \n$ii)$ other optimizers perform suboptimally without additional tuning."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6403/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700477511729,
                "cdate": 1700477511729,
                "tmdate": 1700689249467,
                "mdate": 1700689249467,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RVbqkVqaUB",
                "forum": "g0mlwqs8pi",
                "replyto": "9zgDCc1ahC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6403/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6403/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the review!"
                    },
                    "comment": {
                        "value": "> **Q6: You may enrich the study by reporting how the learning rate changes across time and across clients in $\\Delta$-SGD. Also, you can compare this with the best learning rate of the other methods.**\n\n**A:** Thank you for the great suggestion! *We plotted the learning rate conditions of $\\Delta$-SGD in Figure 5 of Appendix B.4 of the updated supplementary material*. Our proposed step size adapts to the local smoothness and oscillates between the values as large as 0.3 and as small as $\\sim$ 0.0002. \n\n---\n\n\n> **Q7: The two parts of local learning rate calculation (line 9 in Algorithm 1) are clear in how they are used in the theoretical analysis. It may be good to include an experimental report to see in how many iterations the learning rate is updated based on the first part $\\left( \\frac{\\gamma \\|x_{t,k}^i - x_{t,{k-1}}^i \\|}{2 \\| \\tilde{\\nabla} f_i(x_{t,k}^i) - \\tilde{\\nabla} f_i (x_{t,{k-1}}^i)  \\|}  \\right)$ or the second part $\\left( \n\\sqrt{ 1 + \\theta_{t,k-1}^i } \\eta_{t,k-1}^i \\right)$. It may show that the minimum of those two is useful in practice as well.**\n\n**A:** Again, we appreciate your great suggestion. We took your recommendation and *plotted the first and second parts and their minimum in Figure 5 of Appendix B.4 in the updated supplementary material.* \n\nFrom that figure, we can see that *both conditions for $\\eta_{t,k}^i$ are necessary.* The first condition, plotted in green, approximates the local smoothness of $f_i,$ but can get quite oscillatory. The second condition, plotted in blue, effectively restricts the first condition from taking too large values. We believe this enriches our paper, as you suggested. \n\n---\n\n\nWe hope the above answers clarify your concerns, and if so, we hope you could consider reflecting on the score. Thank you again for your time in reviewing our manuscript.\n\n**References**\n\n[1] Reddi, et al., (2021) \"Adaptive Federated Optimization\"\n\n[2] Ward, et al. (2019)  \u201cAdaGrad Stepsizes: Sharp Convergence Over Nonconvex Landscapes\u201d\n\n[3] Koloskova, et al. (2022) \"Sharper convergence guarantees for asynchronous sgd for distributed and federated learning.\" Advances in Neural Information Processing Systems 35\n\n[4] Fallah, et al. (2020) \"Personalized federated learning with theoretical guarantees: A model-agnostic meta-learning approach.\" Advances in Neural Information Processing Systems 33\n\n[5] Nguyen, John, et al. (2022) \"Federated learning with buffered asynchronous aggregation.\" International Conference on Artificial Intelligence and Statistics."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6403/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700477524190,
                "cdate": 1700477524190,
                "tmdate": 1700689266716,
                "mdate": 1700689266716,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lA5WU9loh7",
                "forum": "g0mlwqs8pi",
                "replyto": "RVbqkVqaUB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6403/Reviewer_RQMs"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6403/Reviewer_RQMs"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the answers!"
                    },
                    "comment": {
                        "value": "I thank the authors for their detailed explanations and new experiments. They have resolved all of my concerns!\n\nI also have closely followed the discussions with other reviewers. I have already found that the paper is above the threshold for the conference. I am considering raising my score after the discussion period with other reviewers to see if their concerns are mostly resolved."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6403/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700711413250,
                "cdate": 1700711413250,
                "tmdate": 1700711413250,
                "mdate": 1700711413250,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9mVZaRwWae",
            "forum": "g0mlwqs8pi",
            "replyto": "g0mlwqs8pi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6403/Reviewer_WWBM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6403/Reviewer_WWBM"
            ],
            "content": {
                "summary": {
                    "value": "The authors provide a new learning rate schedule in federated learning scenarios. Each client can adjust their own learning rates by their own local gradients. The authors show the convergence analysis of the proposed algorithm in the non-convex case. The experiments show the benefit of the proposed algorithm."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The authors introduce a new learning rate schedule for each local client in the federated learning. Meanwhile, they show the convergence under mild assumptions in the nonconvex case.\n\n2. The experiments show that the proposed algorithm is more robust than other algorithms w.r.t. non-i.i.dness in federated learning."
                },
                "weaknesses": {
                    "value": "The proof does not seem to be correct to me.\n\n***For the proof of Lemma 2:***\n\n(i)  $\\eta_t^{i}$ is used for updating from $x_t$ to $x_{t+1}$, but in the proof it seems like the $\\eta_t^{i}$ is used for updating from $x_{t-1}$ to $x_t$. If the latter case is necessary for the proof, how can we get $x_t$ and $\\nabla f(x_t)$ without knowing $\\eta_t^{i}$.\n       \n(ii) In the algorithm $\\eta_x^{i}$ is based on the gradient of minibatch estimation, how to change them into the true gradient.\n\n***For the proof in A.1:***\n  \nInequality from (12) to (13) does not seem to be correct. Because $\\eta_{t,k}^i$ depends on $\\tilde{\\nabla} f_i (x_{t,k}^t)$, we need to show that the expected value of $\\eta_{t,k}^i \\tilde{\\nabla} f_i (x_{t,k}^t)$. Since they are not independent, it is trivial to see that $E [\\eta_{t,k}^i \\tilde{\\nabla} f_i (x_{t,k}^t]) = E [\\eta_{t,k}^i E[\\tilde{\\nabla} f_i (x_{t,k}^t)]]$"
                },
                "questions": {
                    "value": "Can you provide some detailed verification of the proof in the appendix?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6403/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6403/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6403/Reviewer_WWBM"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6403/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698654051706,
            "cdate": 1698654051706,
            "tmdate": 1700712983673,
            "mdate": 1700712983673,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FEqHkdyMgY",
                "forum": "g0mlwqs8pi",
                "replyto": "9mVZaRwWae",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6403/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6403/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the review!"
                    },
                    "comment": {
                        "value": "Dear reviewer WWBM, \n\nWe thank you for the comments and questions. There seems to be some confusion about our proof, which we clarify below. \n\n### ***For the proof of Lemma 2***\n\n\n> **Q: $\\eta_t^i$ is used for updating from $x_t$ to $x_{t+1}$, but in the proof it seems like the $\\eta_t^i$ is used for updating from $x_{t-1}$ to $x_t$. If the latter case is necessary for the proof, how can we get $x_t$ and $\\nabla f(x_t)$ without knowing $\\eta_t^i$.**\n\n**A**: For simplicity, let us explain with deterministic gradients without local updates in this response. (We will answer about stochasticity in $(ii)$ below.)\n\nIt is true that in the algorithm $\\eta_t^i$ is used to update $x_{t+1},$ as $x_{t+1}^i = x_t^i - \\eta_t^i \\nabla f_i (x_t^i).$ \n\n**In the proof, we are not *updating* $x_t^i$ *from* $x_{t-1}^i$ *using* $\\eta_t^i$, but just *utilizing the definition* of $\\eta_t^i$, as we clarify below.**\n\n$\\eta_t^i$ is defined as:\n\\begin{align}\n    \\eta_t^i = \\min ( \\tfrac{||x_t^i - x_{t-1}^i ||}{2 || \\nabla f_i(x_t^i) - \\nabla f_i (x_{t-1}^i)  ||},  \\sqrt{ 1 + \\theta_{t-1}^i } \\eta_{t-1}^i ) , \\quad \\theta_{t-1}^i = \\eta_{t-1}^i/\\eta_{t-2}^i.\n\\end{align}\nBased on the definition above, we have\n\\begin{align}\n    \\eta_t^i \\leq \\tfrac{||x_t^i - x_{t-1}^i ||}{2 || \\nabla f_i(x_t^i) - \\nabla f_i (x_{t-1}^i)  ||} \\implies \n    || \\nabla f_i(x_t^i) - \\nabla f_i(x_{t-1}^i) || \\leq \\frac{1}{2\\eta_t^i} ||x_t^i - x_{t-1}^i ||.\n\\end{align}\n\nWe hope the above clarifies the original confusion. That said, we slightly modified the statement and the proof of Lemma 2 in the updated supplementary material, where the source of the initial confusion is removed.\n\n---\n\n\n> **Q: In the algorithm $\\eta_x^i$ is based on the gradient of minibatch estimation, how to change them into the true gradient.**\n\n**A:** We are confused about what you mean by \"change to true gradient\". We assume by $\\eta_x^i$, you mean $\\eta_{t,k}^i$ used in Algorithm 1. In the updated supplementary material, we clarified the proof of Lemma 2 for readability, which we also explain below.\n\nSince by Assumptions (1a) (bounded variance) and (1b) (bounded gradient) as well as the independence of samples $z\\in\\mathcal{B}$, we have that stochastic gradients are bounded:\n\\begin{align*}\n    \\mathbb{E} || \\tilde{\\nabla}f_i(x) ||^2 = \\mathbb{E} || \\frac{1}{|\\mathcal{B}|} \\sum_{z\\in\\mathcal{B}} \\nabla F_i(x,z)  ||^2 \\leq \\mathbb{E} || \\frac{1}{|\\mathcal{B}|} \\sum_{z\\in\\mathcal{B}} \\nabla F_i(x,z) - \\nabla f_i(x)  ||^2 + || \\nabla f_i(x) ||^2.\n\\end{align*}\n\nTherefore, as we explain at the beginning of the proof of Lemma 2, Eq. (9) holds in the presence of stochastic gradients, and thus Eq. (12) holds.\n\n---\n\n\n### ***For the proof in A.1:***\n\n> **Q: Inequality from (12) to (13) does not seem to be correct. Because $\\eta_{t,k}^i$ depends on $\\tilde{\\nabla} f_i(x_{t,k}^i)$, we need to show that the expected value of $\\eta_{t, k}^i \\tilde{\\nabla} f_i(x_{t, k}^i)$. Since they are not independent, it is trivial to see that $E[ \\eta_{t, k}^i \\tilde{\\nabla} f_i(x_{t,k}^i) ] = E[ \\eta_{t, k}^i E [ \\tilde{\\nabla} f_i (x_{t,k}^i) ] ]$**\n\n**A:** We appreciate your question. As stated at the end of page 4, we use the notation\n\\begin{align*}\n    \\tilde{\\nabla} f_i(x) = \\frac{1}{|\\mathcal{B}|} \\sum_{z\\in\\mathcal{B}} \\nabla F_i(x,z)\n\\end{align*}\nas a shorthand for the stochastic gradients with batch $\\mathcal{B}$. When this term appears in the algorithm/proof, we sample a new independent batch with respect to data distribution $\\mathcal{D}_i$.\n\nTherefore, the stepsize and stochastic gradient in line (12) of the proof in A.1 are independent:\n\n\\begin{align*}\n    E_{z \\sim D_i} [\\eta_{t,k}^i \\tilde{\\nabla}f_i(x_{t,k}^i)] &= \n    \\eta_{t,k}^i E_{z \\sim D_i} [\\tilde{\\nabla}f_i(x_{t,k}^i)] ,\n\\end{align*}\n\nas $\\eta_{t,k}^i$ does not depend on $z,$ but on another independently sampled minibatch.\nThis is a well-known proof technique that is commonly used in the following works:\n- Malitsky, et al (2020). \"Adaptive gradient descent without descent.\" 37th International Conference on Machine Learning \n- Fallah, et al (2020). \"Personalized federated learning with theoretical guarantees: A model-agnostic meta-learning approach.\" Advances in Neural Information Processing Systems 33 \n- Fallah, et al (2020). \"On the convergence theory of gradient-based model-agnostic meta-learning algorithms.\" International Conference on Artificial Intelligence and Statistics.\n- Kayaalp, et al (2022). \"Dif-MAML: Decentralized multi-agent meta-learning.\" IEEE Open Journal of Signal Processing 3 (2022): 71-93.\n\nWe clarified this part in the updated supplementary material (in blue color).\n\nWe hope the above answers clarify your concerns, and if so, we hope you could consider reflecting on the score. Thank you again for your time in reviewing our manuscript, and please don't hesitate to follow up if anything is unclear."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6403/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700475569087,
                "cdate": 1700475569087,
                "tmdate": 1700689177132,
                "mdate": 1700689177132,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SkrerlCUVK",
                "forum": "g0mlwqs8pi",
                "replyto": "FEqHkdyMgY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6403/Reviewer_WWBM"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6403/Reviewer_WWBM"
                ],
                "content": {
                    "comment": {
                        "value": "I'm sorry for not making the second question clear.  In the deterministic version, we can get that $\\eta_{t,k}^{i} \\leq \\frac{\\gamma ||x_{t,k}^i - x_{t,k-1}^i||}{2 || \\tilde{\\nabla} f_i(x_{t,k}^i) - \\tilde{\\nabla} f_i(x_{t,k-1}^i)||}$.\nHowever, it seems that what we need is that \n\\\\[\nE(\\eta_{t,k}^{i}) \\leq E(\\frac{\\gamma ||x_{t,k}^i - x_{t,k-1}^i||}{2 ||\\nabla f_i(x_{t,k}^i) - \\nabla f_i(x_{t,k-1}^i)||})\n\\\\]\nHowever, it is not true that $E(1/x) = 1/E(x)$. Then, I wonder how to ensure the inequality, when the gradient estimation involves randomness."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6403/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700642938593,
                "cdate": 1700642938593,
                "tmdate": 1700642938593,
                "mdate": 1700642938593,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "psic2nCr46",
                "forum": "g0mlwqs8pi",
                "replyto": "9mVZaRwWae",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6403/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6403/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the follow-up question, and we are glad we clarified the other two questions.\n\nWe are still a bit confused, as *we simply do not use the inequality*  $E(\\eta_{t, k}^i) \\leq E ( \\frac{ \\gamma ||  x_{t, k}^i - x_{t, k-1}^i  ||  }{ 2 || \\nabla f_i( x_{t, k}^i ) -\\nabla f_i (  x_{t, k-1}^i )  ||  }   ) $ anywhere in the proof. \nWe agree that $E(1/x) \\neq 1/E(x)$, but again, we do not use such equality nor the inequality above in our proof."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6403/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700681637740,
                "cdate": 1700681637740,
                "tmdate": 1700687719999,
                "mdate": 1700687719999,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5lmLv6PzwG",
                "forum": "g0mlwqs8pi",
                "replyto": "psic2nCr46",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6403/Reviewer_WWBM"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6403/Reviewer_WWBM"
                ],
                "content": {
                    "comment": {
                        "value": "The major inequality is $||\\nabla f(x_{t-1}) - \\nabla f(x_t)\\| \\leq \\frac{1}{2\\eta_t} ||x_t - x_{t-1}||$, which is easyly to extend to stochastic version. I have no more concerns and raised my score."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6403/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700712970609,
                "cdate": 1700712970609,
                "tmdate": 1700712970609,
                "mdate": 1700712970609,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Heb4ZUeHKx",
            "forum": "g0mlwqs8pi",
            "replyto": "g0mlwqs8pi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6403/Reviewer_WDyG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6403/Reviewer_WDyG"
            ],
            "content": {
                "summary": {
                    "value": "Federated learning tasks are sensitive to the learning rate selection in client level. The paper argues it may be beneficial if we could enable different learning rate or learning rate scheduler per client due to the highly heterogeneous nature of FL clients. The paper proposes an auto-tuned learning rate scheduler, that could enable learning rate adapting to each client. The paper theoretically show the convergence of the proposed approach and experimental results demonstrate the effectiveness of $\\Delta$-SGD."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Disclaimer: the reviewer is not very familiar with the hyperparameter auto-tuning in centralized computing or in distributed federated computing. Thus, I may not fairly assess the novelty of the technique proposed by this paper. \n\nThe paper is well written and easy to follow. I find the paper enjoyable to read. \n\nThe hyperparameter tuning in client level is tedious and existing literature typically by default set universal learning rate for each client. Therefore, the paper tackles an under-explored problem and proposes a simple and effective approach."
                },
                "weaknesses": {
                    "value": "- Novelty and technical challenge: Client-level optimization is orthogonal to server-level optimization. Therefore, using auto-tuning, which has been studied extensively in the context of centralized computing, in client level is not a very challenging transferral, as we could directly use any auto-tuner directly in client level and check the performance of FL tasks. \n\nI am wondering whether the auto-tuner, used or partially inspired by any practice in centralized computing. And is there any unique challenge if we simply combine any centralized auto-tuner to FL clients?\n\n- Insufficient comparison with auto-tuner baseline: As I mentioned in the last point, it should not be very challenging to directly deploy auto-tuners to FL clients. However, there is limited comparison to this important baseline, i.e., some representative autotuners developed in centralized computing directly used in FL clients.\n\n- Non-standard Assumptions used in theory: bounded gradient is strong assumption, and a bit contradictory to the heterogeneous setting the paper is motivated from. But it may be understandable to use bounded gradient as many existing literature especially who study adaptive optimizers also use it. Strong growth of dissimilarity is less standard and few papers in FL use it. Not sure whether it has any realistic and intuitive correspondence in FL tasks."
                },
                "questions": {
                    "value": "My main questions lie in the previous weaknesses section.\n\nAlso an optional question: though it may be true there is no convergence guarantee given to the varying step size across clients, there are various papers that give convergence guarantee to the scenario where clients can have different number of local iterations, which seems to be a bit related. Is there any connection or difference in proving these two scenarios?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6403/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699039537694,
            "cdate": 1699039537694,
            "tmdate": 1699636710637,
            "mdate": 1699636710637,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VneiI1qbe8",
                "forum": "g0mlwqs8pi",
                "replyto": "Heb4ZUeHKx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6403/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6403/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the review!"
                    },
                    "comment": {
                        "value": "Dear reviewer WDyG,\n\nWe thank you for your review and comments. We are encouraged you found our paper well-written and enjoyable to read. As you noted, our work proposes a simple and effective approach that tackles an under-explored problem of client step size optimization in the federated learning setting. We hope our comments below clarify some of your concerns.\n\n---\n\n\n> **Q: Applying auto-tuner to client level is simple?**\n\n**A:** We are confused about what you mean by \u201cauto-tuner.\u201d To clarify and help the discussion, assuming by auto-tuning you mean *adaptive methods*, Adam and Adagrad (popular adaptive methods) perform poorly (c.f., Table 1). SPS, a recently proposed adaptive method, also performs quite suboptimally in most cases. We kindly remind the reviewer of two works we cite [1, 2] that study the inefficacy of naively employing adaptive methods like Adagrad to the client optimizer, leading to suboptimal performance (similarly to what we show in Table 1 of our manuscript), and hence required server-side modification. \n\nThe key difference is that our proposed method, *$\\Delta$-SGD, performs well across all the settings without additional tuning or modification to the server-side aggregation.*\n\n---\n\n> **Q: Non-standard assumptions like bounded gradient and strong growth of dissimilarity?**\n\n**A:** As the reviewer acknowledged, bounded gradient is quite a common assumption in nonconvex optimization [3, 4, 5, 6], especially when using an adaptive optimizer. We agree that strong growth of dissimilarity is non-standard, yet $\\Delta$-SGD enables each client to use its own step size, which is a significant difference and advantage compared to other FL methods. Therefore, the local iterates can diverge from each other more rapidly (by using a larger step size), which is intuitively why we need a slightly non-standard assumption. We note that relaxing the assumptions can be an interesting future work.\n\n---\n\n\n> **Q: Any connection to FL papers proving convergence where clients can have different numbers of local iterations?**\n\n**A:** We thank the reviewer for this comment. We do not see a clear connection between our proposed auto-tuned step size selection and existing analyses based on the number of local iterations. *A major difference is that using a different number of local iterations implies computing more gradients, whereas $\\Delta$-SGD enables a simple step size calculation without additional computation.* \nHyperparameters in gradient-based routines might indeed be connected. E.g., some works connect step size selection in stochastic cases with the mini-batch size per gradient calculation. We consider this question an interesting open question to study in the near future, but as of now, it is a bit orthogonal to this submission. \n\n---\n\n\nWe hope the above answers clarify your concerns, and if so, we hope you could consider reflecting on the score. Thank you again for your time in reviewing our manuscript.  \n\n\n**References**\n\n[1] Wang, et al. (2021) \u201cLocal Adaptivity in Federated Learning: Convergence and Consistency\u201d\n\n[2] Xie, et al. (2020) \u201cLocal AdaAlter: Communication-Efficient Stochastic Gradient Descent with Adaptive Learning Rates\u201d\n\n[3] Ward, et al. (2019)  \u201cAdaGrad Stepsizes: Sharp Convergence Over Nonconvex Landscapes\u201d\n\n[4] Koloskova, et al. (2022) \"Sharper convergence guarantees for asynchronous sgd for distributed and federated learning.\" Advances in Neural Information Processing Systems 35\n\n[5] Fallah, et al. (2020) \"Personalized federated learning with theoretical guarantees: A model-agnostic meta-learning approach.\" Advances in Neural Information Processing Systems 33\n\n[6] Nguyen, John, et al. (2022) \"Federated learning with buffered asynchronous aggregation.\" International Conference on Artificial Intelligence and Statistics."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6403/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700474157278,
                "cdate": 1700474157278,
                "tmdate": 1700689022949,
                "mdate": 1700689022949,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oSCtCqYtV6",
                "forum": "g0mlwqs8pi",
                "replyto": "Heb4ZUeHKx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6403/Reviewer_WDyG"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6403/Reviewer_WDyG"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the rebuttal"
                    },
                    "comment": {
                        "value": "Thanks authors for the detailed rebuttal. I am satisfied with the answers w.r.t. assumption and unequal number of local computation.\n\nWith respect to the first point, i.e., auto-tuner part, I am not specifically referring to adaptive approaches like adam or adagrad. For example, there are many learning rate schedulers in centralized computing, e.g. warmup, multistage [3], cosine annealing [1], cyclic [2] and so on. And also some auto-tuning rule, e.g. yellowfin [4]. \n\nThese approaches should be easily used in client-level and could also alleviate some learning rate tuning efforts. I am wondering whether the authors could compare or at least discuss these works used in client level.\n\n[1] SGDR: Stochastic Gradient Descent with Restarts.\n\n[2] Cyclical Learning Rates for Training Neural Networks\n\n[3] A Stagewise Hyperparameter Scheduler to Improve Generalization\n\n[4] YellowFin and the Art of Momentum Tuning"
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6403/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700714985569,
                "cdate": 1700714985569,
                "tmdate": 1700715023416,
                "mdate": 1700715023416,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "d7qRmjIzFK",
                "forum": "g0mlwqs8pi",
                "replyto": "Heb4ZUeHKx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6403/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6403/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We're glad we clarified the other two questions. \n\nThank you for clarifying what you meant by auto-tuners, and thanks for the references.\n\nWe would like to point out that all the methods from the references either require users to set/tune multiple hyperparameters or solve multiple subroutines per iteration, as we detail below. \n*Importantly, what we describe below should be done for each client separately, in order for each client to use their own step size, as $\\Delta$-SGD does.*\n\n---\n\n> [1] SGDR: Stochastic Gradient Descent with Restarts\n\nThis method requires the user to set $\\eta_{min}^i, \\eta_{max}^i, T_0,$ and $T_{mult}$, for the $i$-th run (Eq (5), pg. 4). \n\n> [2] Cyclical Learning Rates for Training Neural Networks\n\nThis method similarly requires the user to set lower (base_lr) and upper (max_lr) bounds on the learning rate, as well as the frequency to cycle between the lower and the upper bounds (Figure 2, pg. 2)\n\n> [3] A Stagewise Hyperparameter Scheduler to Improve Generalization\n\nThis method requires the user to set the \"Quasi-Hyperbolic Momentum triplets\" {$\\alpha_i, \\beta_i, v_i$} from $i=1$ to $M$, as well as the training length {$T_i$} from $i=1$ to $M$ (Algorithm 1, pg. 1532), where $M$ is the number of stage (another hyperparameter you have to set.) \n\n> [4] YellowFin and the Art of Momentum Tuning\n\nThis method requires computing four different subroutines per iteration: (i) CURVATURE_RANGE, (ii) VARIANCE, (iii) DISTANCE, and (iv) SINGLE_STEP, as can be seen in Algorithm 1 (pg. 8).\n\n---\n\nWe do not have enough time to perform additional experiments to compare to these methods within the discussion period, but we will happily include these references in the related work, and try to add comparison results for the final version."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6403/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700721499668,
                "cdate": 1700721499668,
                "tmdate": 1700721944457,
                "mdate": 1700721944457,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "R6PZsTOeml",
                "forum": "g0mlwqs8pi",
                "replyto": "d7qRmjIzFK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6403/Reviewer_WDyG"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6403/Reviewer_WDyG"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your response"
                    },
                    "comment": {
                        "value": "Thanks for the authors' response. My concern on this part is resolved. \n\nIt seems the mentioned approaches may require several new hyperparameters to tune. But it still makes sense to discuss these approaches or compare with these approaches (maybe simply some default hyperparameters without carefully tuning to see what happens), as they represent efforts to auto determine learning rate in centralized setting and could be deployed to clients in a rather straightforward manner.\n\nI keep my recommendation of acceptance. I will discuss with other reviewers in the discussion period to consider increasing score."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6403/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700722623520,
                "cdate": 1700722623520,
                "tmdate": 1700722623520,
                "mdate": 1700722623520,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]