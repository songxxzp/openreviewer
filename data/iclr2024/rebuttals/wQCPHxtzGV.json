[
    {
        "title": "RF-POLICY: Rectified Flows are Adaptive Decision Makers"
    },
    {
        "review": {
            "id": "2zhd3H3eQI",
            "forum": "wQCPHxtzGV",
            "replyto": "wQCPHxtzGV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8391/Reviewer_KiF7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8391/Reviewer_KiF7"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the issue of multi-modal policy generation and inference efficiency by proposing a novel offline imitation learning algorithm, RF-POLICY, to achieve a trade-off between policy diversity and model inference efficiency, especially compared to BC and DDIM.\nThe proposed method is based on Rectified Flow, and its training consists of two stages: one is to optimize the Rectified Flow to ensure the straightness so as to reduce inference time, and the other is to optimize the variance prediction network to determine the uncertainty of state so as to generate diverse policy when state's uncertainty or variance is high. While optimizing the Rectified Flow, it is proved that the model reduces to one-step generators, thus improving training and inference efficiency compared to DDIM. \nExperiments on a 2D maze environment and a simulated robot manipulation benchmark suggest that the proposed method can achieve high performance in task success rate, behavioral diversity and inference speed. \nThe main contribution of this paper is to derive an offline IL algorithm to improve the computational efficiency of diffusion-based policy generators while maintaining their ability of multi-modal policy generation."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper addresses an important problem of the application of diffusion-based policy generators in realistic scenarios, such as robot manipulation.\n2. The paper proposes an offline IL method to directly address the issue and provides a proof to explain why the Rectified Flow-based method can improve the training and inference efficiency, and meanwhile, in order to maintain the multi-model policy generation, the paper proposes a training objective by incorporating the state variance estimation into the loss of Rectified Flow optimization.\n3. The paper evaluates the proposed method with a new set of evaluation metrics not including the task success rate, but also the behavioral diversity and computational efficiency, to validate the method's ability. \n4. Empirical results show that the proposed method can achieve high performance on both computational efficiency and multi-modal policy generation."
                },
                "weaknesses": {
                    "value": "1. Some claims of the paper are not adequately supported with evidence. For example, the experiments evaluate the method on three dimensions only on two benchmark datasets (the third is in the appendix and performance is comparable), so the last claim in the abstract may be doubted. Another example is that Assumption 1 is too strong and there is no evidence nor data distribution visualization to support it, thus the use of the proposed method with real data and non-expert data (e.g., low-return trajectories) is not convincing based on the limited results shown in this paper. However, the focus of this work is to improve the computational efficiency of diffusion-based policy generators, thus more experiments on other datasets as used in Chi (2023) should be conducted, especially the experiments on real-world robot benchmark if possible.\n2. The proposed method is compared with only BC and Diffusion Policy, though the baselines are representative in either field.  Performance improvements over other diffusion-based polices are missing, such as Diffusion-QL (Wang et al., 2023), Decision Diffuser (Ajay et al., 2023), Difusion BC (Pearce et al., 2023), etc, so the significance of this paper is doubted.\n3. Description of some figures and experimental results is confusing and need further clarity.\n\nReferences:\n[1] Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric Cousineau, Benjamin Burchfiel, and Shuran Song. Diffusion policy: Visuomotor policy learning via action diffusion. RSS, 2023.\n[2] Anurag Ajay, Yilun Du, Abhi Gupta, Joshua B Tenenbaum, Tommi S Jaakkola, and Pulkit Agrawal. Is conditional generative modeling all you need for decision making? ICLR, 2023.\n[3] Wang, Zhendong, Jonathan J. Hunt, and Mingyuan Zhou. Diffusion policies as an expressive policy class for offline reinforcement learning. ICLR, 2023.\n[4] Tim Pearce, Tabish Rashid, Anssi Kanervisto, David Bignell, Mingfei Sun, Raluca Georgescu, Sergio Valcarcel Macua, Shan Zheng Tan, Ida Momennejad, Katja Hofmann, et al. Imitating human behaviour with diffusion models. ICLR, 2023."
                },
                "questions": {
                    "value": "1. Does the textual description corresponds correctly to the picture in Fig.4?\n2. The measurement unit of training and execution time is different as stated in Results in section 5.3, so is it appropriate to exhibit the training and inference efficiency in the same figure as in Fig.5? In addition, the training and execution efficiency of experiments on 2D maze are not quantified clearly.\n3. How are the experimental results calculated in Table 1 and Table 2? Are they average scores across several seeds? And some implementation details are missing, such as epochs.\n4. In Table 1, results on Maze 5 only exhibit SR and Maze 6 only exhibits DS."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8391/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698140838565,
            "cdate": 1698140838565,
            "tmdate": 1699637044924,
            "mdate": 1699637044924,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "upEeBpxXY7",
                "forum": "wQCPHxtzGV",
                "replyto": "2zhd3H3eQI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8391/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8391/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "We sincerely thank the reviewers for their insightful comments and constructive suggestions, which have been instrumental in enhancing the clarity and depth of our work.\n\n**Q1. Assumption 1 and Real Data Application:**\n\nRegarding Assumption 1, we acknowledge the need for empirical support. To this end, we have conducted further analysis to provide data distribution visualizations that substantiate our assumption. These additional results will bolster the credibility of applying RF-POLICY to real and non-expert data.\n\nTo substantiate this assumption, we have conducted additional analyses. \nWe take advantage of the variance predictions made by RF-POLICY across different states within a robot's state space. Figure 8 in our paper ([link](https://anonymous.4open.science/r/RF-POLICY-184B/libero_variance.png)) presents these predictions, offering a visualization of the diversity in potential actions given a robot's specific state. As illustrated in the figure, for the pick and place task, there is an observable increase in action diversity as the robot's end effector gets closer to the target object. This pattern suggests that while most states exhibit low action variance, implying a uni-modal distribution of potential actions, a subset of states, particularly those in proximity to the object interaction, display a significant increase in variance, reflecting a multi-modal distribution of actions.\n\n**Q2. Comparisons with Chi et. al.:**\n\nWe acknowledge the importance of real-world robot experimentation to validate our findings, however, at the current moment the authors do not have real robots to conduct such experiments. As a compensation, we conduct more experiments in simulation (Please see the general response).\n\nRegarding benchmarks from Chi et al.'s work, we have indeed included results from the robomimic benchmark in the appendix of our initial submission (Section 1 in Appendix). Notably, we observed that the success rates for BC, Diffusion Policy, and RF-POLICY were all exceedingly high, consistently above 0.95, indicating a ceiling effect that limits the differentiation between methods. To address this, we shifted our focus to the LIBERO benchmark, which presents more challenging and discriminative tasks.\n\nWe plan to undertake a comprehensive set of experiments, including those on RoboMimic, and will incorporate these results in an updated version of our work.\n\n**Q3: Comparison with other offline RL learning methods:**\n\nOur work focuses on offline imitation learning, not offline reinforcement learning, which means the rewards are not present. This is a more practical assumption at least in robotics, since learning complex manipulation/navigation behaviors often requires complex reward design, while providing demonstrations is much simpler. As a result, our method is not directly comparable against methods like Diffusion-QL and Decision Diffuser. Regarding Diffusion BC, it uses DDPM to train a diffusion model for action prediction, this objective is the same as Diffusion Policy we compared with. \n\nIn response to the review, we have expanded our comparative analysis to include additional relevant baselines within the scope of imitation learning (See general response). These additional comparisons are better aligned with our approach and further validate the efficacy of RF-POLICY in the intended setting.\n\nWe thank the reviewer for pointing out the typos in Figures and tables. We have fixed the typos and revise the descriptions of figures and experimental results for clarity. Please see the updated paper for these change. Considering the time constraints during rebuttal, we will include the average scores across multiple seeds to ensure that the results reflect a robust evaluation of our method."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8391/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700737614905,
                "cdate": 1700737614905,
                "tmdate": 1700738856579,
                "mdate": 1700738856579,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DmxeMalP0F",
            "forum": "wQCPHxtzGV",
            "replyto": "wQCPHxtzGV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8391/Reviewer_J2ZQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8391/Reviewer_J2ZQ"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents an imitation learning algorithm using rectified flow, a recent advancement in flow-based generative modeling combined with probability flow ordinary differential equations. The main idea is to improve the computational efficiency when action mapping from state is deterministic, in which ODE can be solved trivially. The resulting algorithm generates diverse policies yet avoid unnecessary computation whenever mapping from state to action has sufficiently low variance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed algorithm achieves a balance between computational complexity and diversity in generated policies."
                },
                "weaknesses": {
                    "value": "1. The paper lacks empirical or mathematical validation for Assumption 1. The authors posit that most demonstration datasets for robotic tasks exhibit deterministic behavior (or uni-modal states), yet they fail to support this claim with experimental evidence.\n2. As delineated by Theorem 1, the RF-Policy loss function (equation 5) optimizes the flow model (ODE model) to generate deterministic (uni-modal) behaviors, evident when the loss function goes to 0 as the variance of action given state reaches 0. This prevents the model to generate multi-modal behaviors. This seems to counter the purpose of using diffusion models.\n3. The study omits a comparative analysis with established offline RL baselines such as CQL and BCQ, as well as other diffusion-based methodologies like Diffuser (Janner et al., 2022) and Diffusion-QL (Wang et al., 2022).\n4. The paper would benefit from a detailed proof of Theorem 1."
                },
                "questions": {
                    "value": "1. How do linear flow models (linear ODE models), like RF-Policy, accurately encapsulate complex behaviors? Most existing methods have relied on non-linear SDEs, specifically DDPM, for policy estimation, yet this study utilizes a linear model. What rationale is provided for the superiority of this linear approach over its predecessors? (Related to equation 3)\n2. Figure 1 is intended to demonstrate that, unlike DDIM (an extension of DDPM), RF-Policy generates straight lines in deterministic areas (x < 0). However, I cannot see distinguishable difference between the red (DDIM) and blue (RF-Policy) lines in the figure. It could be considered a potential weakness of the paper.\n3. How does the variance prediction network determine whether a state is uni-modal or multi-modal? It is trained to estimate state variance using an offline dataset, encompassing both epistemic and aleatoric uncertainties. Given that the distinction between uni-modal and multi-modal states pertains to aleatoric uncertainty, how does the model address epistemic uncertainty?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8391/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8391/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8391/Reviewer_J2ZQ"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8391/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698663937469,
            "cdate": 1698663937469,
            "tmdate": 1699637044814,
            "mdate": 1699637044814,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0vypm46lqn",
                "forum": "wQCPHxtzGV",
                "replyto": "DmxeMalP0F",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8391/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8391/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "We sincerely thank the reviewers for their insightful comments and constructive suggestions, which have been instrumental in enhancing the clarity and depth of our work.\n\n**Q1. Validation of Assumption 1:**\n\nTo substantiate this assumption, we have conducted additional analyses. \n\nWe take advantage of the variance predictions made by RF-POLICY across different states within a robot's state space. Figure 8 ([link](https://anonymous.4open.science/r/RF-POLICY-184B/libero_variance.png)) presents these predictions, offering a visualization of the diversity in potential actions given a robot's specific state. As illustrated in the figure, for the pick and place task, there is an observable increase in action diversity as the robot's end effector gets closer to the target object. This pattern suggests that while most states exhibit low action variance, implying a uni-modal distribution of potential actions, a subset of states, particularly those in proximity to the object interaction, display a significant increase in variance, reflecting a multi-modal distribution of actions.\n\n**Q2: Theorem 1 proof:**\n\nTheorem 1 proves that for deterministic behaviors, where the variance of action given state is zero, the RF-POLICY loss function is optimized such that the flow model will produce straight-line trajectories. \nHowever, this does not limit RF-POLICY's ability to generate multi-modal behaviors. There\u2019s an expectation over the optimal velocity, $v^*(z, t | x) $, as outlined in the proof, captures the **expected trajectory**.\n\nIn the multi-modal case, this expectation accounts for the *multiple* possible directions $y - Z_0$ that can pass through point $z$ at time $t$. Consequently, for a multi-modal distribution, the variance would be greater than zero, allowing for the generation of diverse trajectories. \n\nOur empirical evaluations support the model's multi-modal generative capabilities. In scenarios with inherent uncertainty and multiple valid actions for a given state, RF-POLICY successfully generates a diverse set of plausible actions, which is proof of its ability to model multi-modal behavior effectively. \n\n**Q3: Comparison with other offline imitation learning methods:**\n\nOur work focuses on offline imitation learning, not offline reinforcement learning, which means the rewards are not present. This is a more practical assumption at least in robotics, since learning complex manipulation/navigation behaviors often requires complex reward design, while providing demonstrations is much simpler. As a result, our method is not directly comparable against methods like CQL and BCQ.\n\nIn response to the review, we have expanded our comparative analysis to include additional relevant baselines within the scope of imitation learning (See general response). These additional comparisons are better aligned with our approach and further validate the efficacy of RF-POLICY in the intended setting.\n\n**Q4: How does flow model capture complex behaviors?**\n\nRF wants to construct ODEs that follow straight trajectories, which is very different from assuming a linear model. Constructing straight ODE is a non-trivial task, requires quite a lot of non-linearity to achieve: the ODE has to control the move of the particle carefully to ensure the straight trajectory. Mathematically, the velocity field of straight ODEs is characterized by a nonlinear PDE called burger\u2019s equation[1]:\n\n$\\partial v/ \\partial t + (\\partial v/ \\partial z) v = 0$, where $v = v(Z_t, t)$ is the velocity field.\n\nThis is a challenging and highly non-linear problem to solve. RF seeks for straightness because it is easier for simulate at inference time (more straight => fewer steps).\n\n[1] See Liu, et. al., Flow Straight and Fast. 2023\n\n**Q5. Clarifying Trajectories in Figure 1:**\n\nFigure 1 presents a 1D example with mappings that highlight the behavior of both DDIM and RF-Policy. Although both methods fit the demonstration data well, the red line (DDIM) demonstrates a non-linear trajectory, indicating that multiple inference steps are necessary due to the nature of SDEs. In contrast, the blue line (RF-Policy) presents straight trajectories for deterministic cases (x \u2264 0), validating RF-Policy's ability to efficiently generate actions in a single step through one-step Euler approximation \u200b"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8391/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700737098076,
                "cdate": 1700737098076,
                "tmdate": 1700738679847,
                "mdate": 1700738679847,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Xu4mwrfx8B",
            "forum": "wQCPHxtzGV",
            "replyto": "wQCPHxtzGV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8391/Reviewer_Hbo4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8391/Reviewer_Hbo4"
            ],
            "content": {
                "summary": {
                    "value": "Recent papers in offline imitation learning substitute cross-entropy based behavioural cloning with diffusion-based models as a generative model. This paper proposes to use rectified flow instead, a formulation that, still using a mean-squared error objective, forces trajectories of the probability ODE to have no curvature whatsoever, sacrificing some generative accuracy (as it solves a whole family of transport problems) for maximum generation speed with 1 single function evaluation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The paper is well written and straightforward to follow. Besides, it feels clear that combining rectified flow and offline IL should work in practice, based off intuition from the purely supervised RF case."
                },
                "weaknesses": {
                    "value": "However, a big weakness in the paper consists in its novelty and magnitude of its contribution. Specifically, its unique contribution (apart from a log-variance additive regularizer) - compared to any standard diffusion-based offline IL approach - is to use rectified flow for acceleration, since the training time of the procedure is directly proportional to the NFE (number of function evaluations) required to perform inference for the diffusion model. It is indeed the case that rectified flow provides some of the best generative performance at 1 NFE amidst the class of extended diffusion-inspired models; but all that is proven here is that the approximation error in rectified flow is very compatible with the approximation error from offline IL. We feel the argument would be materially stronger if it was demonstrated 1. on a variety of more realistic domains than some of the toy domains (maze) treated here, for instance Atari-100k seeded with expert trajectories, which shouldn't require industrial levels of compute; and 2. most importantly, if an ablation study and exhaustive comparison with the performance of diffusion samplers specifically tailored to the low-NFE regime (UniPC [1], Heun and others [2] for pure samplers, even widening scope Consistency Models [3] or TRACT [4]) was performed. To me figure 5 simply means that DDIM 20 steps was used as baseline. This choice of DDIM feels arbitrary and it's not clear how much relative loss DDIM10, DDIM5 or another sampler would incur, thus minimizing any contribution that claims an NFE speedup. It is also not clear that Rectified Flow is a unique or best solution to this problem, as for say Consistency Models is also a class of diffusion-like models that could claim the same in the IL setting.\n\n[1] Zhao et al, UniPC: A Unified Predictor-Corrector Framework for Fast Sampling of Diffusion Models.\n[2] Karras et al, Elucidating the Design Space of Diffusion-Based Generative Models.\n[3] Berthelot et al, TRACT: Denoising Diffusion Models with Transitive Closure Time-Distillation.\n[4] Song et al, Consistency Models."
                },
                "questions": {
                    "value": "Being cognizant of deadlines and compute constraints, which additional empirical evidence (section 5) could the authors provide in order to bolster their claims ? I would be willing to raise my score if the experiment section were more convincing."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8391/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8391/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8391/Reviewer_Hbo4"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8391/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698680154662,
            "cdate": 1698680154662,
            "tmdate": 1699637044694,
            "mdate": 1699637044694,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0V6jHMvDvb",
                "forum": "wQCPHxtzGV",
                "replyto": "Xu4mwrfx8B",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8391/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8391/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "We sincerely thank the reviewers for their insightful comments and constructive suggestions, which have been instrumental in enhancing the clarity and depth of our work.\n\n**Q1: Addressing More Realistic Domains:**\n\nWe appreciate the reviewer's suggestion to test RF-POLICY on more complex domains, such as the Atari-100k benchmark. However, Atari games, including benchmarks like Atari-100k, use a discrete action space. This poses a challenge for diffusion models, and it may require additional methods to approximate the continuous processes that diffusion models use.\n\nIn light of this, and to demonstrate the versatility of RF-POLICY in more sophisticated and realistic scenarios, we have extended our experiments to include a 'pick and place' task. This task incorporates a set of demonstrations showcasing different methods of picking up a bowl. We believe this task mirrors real-world applications more closely, as it involves learning multiple viable solutions for a single task \u2013 a common requirement in practical settings. This extension of our experiments serves to better validate the model's effectiveness in complex, real-world applications where diverse solution strategies are essential. The following figure shows two ways of picking up the bowl (See this [link](https://anonymous.4open.science/r/RF-POLICY-184B/diverse_libero.png) or Figure 7 in our paper). The table shows the comparison between bc, standard RF, Rectified Flow and RF-POLICY (See this [link](https://anonymous.4open.science/r/RF-POLICY-184B/diverse_libero.png) or Table 8 in our paper). \n\n|            | SR     | DS   | NFE  |\n|------------|--------|------|------|\n| BC         | 0.92   | 0.90 | 1.00 |\n| DDIM       | 0.96   | 0.97 | 10.00|\n| RF-POLICY  | 0.96   | 0.95 | 1.37 |\n\nTable 8: Comparison of Behavioral Cloning (BC), Diffusion Policy (DDIM), and RF-POLICY in Pick and Place tasks. The table includes performance metrics such as Success Rate (SR), Diversity Score (DS), and Number of Function Evaluations (NFE) for each model.\n\n**Q2: Conducting Ablation Studies and Exhaustive Comparisons:**\n\nWe pick step=20 in the original paper because that is the number of steps used by Diffusion Policy. We appreciate the reviewer's emphasis on the necessity of a thorough comparison with diffusion samplers optimized for the low-NFE regime. To address this, we have expanded our comparative analysis to include DDIM and standard 1-RF at varying levels of NFEs. The following table presents this expanded evaluation, offering a clearer perspective on the relative performance trade-offs between these methods when operating under constrained NFEs (See this [link](https://anonymous.4open.science/r/RF-POLICY-184B/table7_low_nfes.png) or Table 7 in our paper)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8391/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700736275206,
                "cdate": 1700736275206,
                "tmdate": 1700736275206,
                "mdate": 1700736275206,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "RkSPsfAY1G",
            "forum": "wQCPHxtzGV",
            "replyto": "wQCPHxtzGV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8391/Reviewer_zCJH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8391/Reviewer_zCJH"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces RF-POLICY, an imitation learning algorithm that leverages Rectified Flow, a recent advancement in flow-based generative modeling. Traditional methods like Behavioral Cloning (BC) struggle with diverse behaviors, and while diffusion-based imitation learning addresses this, it does so at the cost of slower inference. RF-POLICY uniquely employs probability flow ordinary differential equations (ODEs) for policy generation, ensuring rapid inference without compromising on behavioral diversity. The core advantage of RF-POLICY is its adaptability: for uni-modal states, it behaves like a one-step generator, and for multi-modal states, it uses a multi-step approach. Empirical evaluations demonstrate that RF-POLICY offers superior performance across multiple metrics like success rate, behavioral diversity, and inference speed."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. RF-POLICY introduces a novel application of Rectified Flow in imitation learning, highlighting an adaptive mechanism to control generation efficiency based on demonstration variance.\n \n2. RF-POLICY efficiently addresses the trade-off between inference speed and behavioral diversity, which has been a challenge in traditional methods like BC and diffusion-based imitation learning.\n\n3. The paper not only introduces new evaluation metrics for imitation learning but also presents a detailed empirical analysis, demonstrating the algorithm's superior performance across various robotic problems.\n\n4. RF-POLICY is highlighted for its straightforward implementation and rapid training, providing practical advantages in real-world applications."
                },
                "weaknesses": {
                    "value": "1. There is a theoretical gap between the objective at eq.~(8) and the implementation at Alg.1. The implementation uses a rectified flow to train the policy function, and uses another neural network to train the variance prediction network. In execution, the variance prediction network is used to determine the update iteration. \n2. Considering that the variance prediction network and the policy are trained separately, the performance gain especially in training is only a contribution of the rectified flow instead of the proposed solution as a whole."
                },
                "questions": {
                    "value": "The paper is clearly written with good visualizations. However, the gap between the objective at eq.(8) and the implementation is not explained. \n1. I was wondering if there are any reasons supporting the implementation.\n2. I was wondering if using rectified flow to replace the DDPM in the DDIM models will lead to similar performances in both tasks."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8391/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698719718849,
            "cdate": 1698719718849,
            "tmdate": 1699637044577,
            "mdate": 1699637044577,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7iSRxJiQ6y",
                "forum": "wQCPHxtzGV",
                "replyto": "RkSPsfAY1G",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8391/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8391/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "We sincerely thank the reviewers for their insightful comments and constructive suggestions, which have been instrumental in enhancing the clarity and depth of our work.\n\n**Q1. Gap between the objective at Eq.(8) and the implementation at Alg.1**\n\nThe two-stage training framework is indeed deliberate and advantageous. \n\nFirstly, training the variance prediction network in fact takes a very short time compared to the training of the RF network. Training the rectified flow model requires a duration that is 10 times longer than that needed to train the variance prediction model. \n\nSecondly, we empirically found that training the two components separately leads to slightly better performance than training them jointly (See the Table here ([link](https://anonymous.4open.science/r/RF-POLICY-184B/table4_separate_joint_train.png)), or Table 4 in the paper). \n\n|                    | Maze 1      |            |       |      | Maze 2      |            |       |      | Maze 3      |            |       |      | Maze 4      |            |       |      | Average     |            |       |      |\n|--------------------|-------------|------------|-------|------|-------------|------------|-------|------|-------------|------------|-------|------|-------------|------------|-------|------|-------------|------------|-------|------|\n|                    | SR          | DS         | NFE   | LEI  | SR          | DS         | NFE   | LEI  | SR          | DS         | NFE   | LEI  | SR          | DS         | NFE   | LEI  | SR          | DS         | NFE   | LEI  |\n| RF-POLICY (Separate) | 1.00        | 0.98       | 1.11  | 0.91 | 1.00        | 0.79       | 1.03  | 0.83 | 1.00        | 0.96       | 1.99  | 0.78 | 0.98        | 0.93       | 1.85  | 0.59 | 0.99        | 0.91       | 1.50  | 0.78 |\n| RF-POLICY (Joint)    | 1.00        | 0.97       | 1.09  | 0.67 | 1.00        | 0.93       | 1.08  | 0.76 | 1.00        | 0.91       | 2.08  | 0.64 | 1.00        | 0.91       | 1.70  | 0.60 | 1.00        | 0.93       | 1.49  | 0.67 |\n\nTable 4: Performance comparison of separate training and joint training of RF-POLICY in Maze tasks. The table presents key performance metrics, including Success Rate (SR), Diversity Score (DS), Number of Function Evaluations (NFE), and Learning Efficiency Index (LEI), across various maze complexities. Results are shown for each model in different maze scenarios, as well as the average performance.\n\nTherefore, though in theory training the two networks jointly or sequentially should be the same, in practice, training two \u201ctasks\u201d simultaneously can lead to optimization challenges (See PCGrad for the conflicting gradient issue when more objectives are introduced). Moreover, as we found that once the RF net is trained, training the variance network adds negligble extra overhead, so we choose this way of training. We have modified the paper to illustrate the reason in detail.\n\n**Q2:  Performance gain in training:**\n\nFirst, regarding the performance in terms of the success rate and diversity, the benefit indeed comes from the RF network. It is in the original theory of RF that, if we ignore the simulation speed (how many steps to simulate the ODE), RF without any reflow performs the best. Applying RF on robotics problems is similar to applying DDIM/DDPM in the diffusion-policy paper. Moreover, we believe that RF is simpler to implement and easier to undertand, and is more computationally efficient.\n\nHowever, we would like to argue that regarding the performance in terms of inference speed, the contribution mainly comes from the **variance network**. This variance network helps reduce the ODE simulation steps whenever possible. For instance, if a state always leads to a deterministic action, RF-policy will be as efficient as BC. And the number of simulation steps completely depends on the \u201cvariance\u201d (or diversity) of the optimal actions given the state. This part alone is a core contribution."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8391/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700734585798,
                "cdate": 1700734585798,
                "tmdate": 1700734585798,
                "mdate": 1700734585798,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]