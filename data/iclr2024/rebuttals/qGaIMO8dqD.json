[
    {
        "title": "Explaining the Complex Task Reasoning of Large Language Models with Template-Content Structure"
    },
    {
        "review": {
            "id": "Pya36Fkpou",
            "forum": "qGaIMO8dqD",
            "replyto": "qGaIMO8dqD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5116/Reviewer_KnhL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5116/Reviewer_KnhL"
            ],
            "content": {
                "summary": {
                    "value": "The authors present a structured framework in this paper to elucidate the workings of language models, emphasizing a template-task structure for answer generation. The paper is articulate and well-structured. I do have some inquiries concerning its content"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The explanations in the paper are clearly written in simple terms."
                },
                "weaknesses": {
                    "value": "The authors frequently relegate crucial details to the appendix, disrupting the continuity of the reading experience.\n\nRelated papers:\n\nExplaining Agent Behavior with Large Language Models - Link: https://arxiv.org/pdf/2309.10346.pdf\n\niPrompt: Explaining Data Patterns in Natural Language via Interpretable Autoprompting - Link: https://arxiv.org/pdf/2210.01848.pdf\n\nIt would be valuable to examine cases with misclassification errors to assess the robustness of the proposed model."
                },
                "questions": {
                    "value": "Are there alternative explanation models in the existing literature? A detailed comparison, possibly supplemented with numerical experiments, would be beneficial."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5116/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5116/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5116/Reviewer_KnhL"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5116/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698765045680,
            "cdate": 1698765045680,
            "tmdate": 1699636503447,
            "mdate": 1699636503447,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vOsExknwc5",
                "forum": "qGaIMO8dqD",
                "replyto": "Pya36Fkpou",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5116/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5116/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your thoughtful and comprehensive review. Your feedback provides us with clear direction on how to improve our article. We summarize your concerns and suggestions as well as our responses as follows\uff1a\n\n1. > \"The authors frequently relegate crucial details to the appendix, disrupting the continuity of the reading experience.\"\n\n    Thank you for your valuable feedback. In our endeavor to present a completely novel perspective on LLMS, we encountered the challenge of incorporating motivation, examples, mathematical deduction, and experimental evidence within the confines of a succinct 9-page document. This limitation unavoidably constrained our capacity to delve into comprehensive details in the main body of the text. To facilitate readers in quickly comprehending our t-c structure, we gave precedence to elucidating the motivation and intuition. As a result, more intricate details were placed in the appendix, catering to readers with specific interests.\n    \n    In the final version of our paper, we intend to refine the organization further. We are particularly grateful for any feedback, and we highly value your insights regarding the elements that should receive greater emphasis in the main text. Your suggestions are instrumental in enhancing the quality of our work.\n\n\n2. **Related work**\n    > \"Related papers:\n    >\n    > Explaining Agent Behavior with Large Language Models - Link: https://arxiv.org/pdf/2309.10346.pdf\n    >\n    > iPrompt: Explaining Data Patterns in Natural Language via Interpretable Autoprompting - Link: https://arxiv.org/pdf/2210.01848.pdf\"\n\n    Thank you for your kind reminder. These papers indeed offer a valuable addition to our current section on related work. Contrary to the works we have cited that focus on elucidating LLMs themselves, these studies explore a distinct facet of LLM interpretability: employing LLMs to explain external systems. We will incorporate these references into the related work section of our final version.\n\n    - The first paper \"*Explaining Agent Behavior with Large Language Models*\" introduces a methodology for generating model-agnostic explanations for agent behaviors using LLMs. This approach involves translating an agent's decision-making process into a decision tree, which is then inputted into an LLM to produce explanations. Their crowd experiments show that LLMS generate better explanations than previous work.\n    - The second work \"*iPrompt*\" propose to find the pattern in the dataset, i.e. utilizing LLMs to elucidate the dataset itself. This method involves feeding \"input-output\" pairs ${(x_1,y_1),\\dots,(x_n,y_n)}$ into an LLM, resulting in the generation of semantic explanations for the relationships between $x$ and $y$ in natural language. The explanations thus generated have been highly rated in terms of human evaluative metrics.\n    \n    Both works show that LLMs can summarize and explain the input, including dataset patterns and the output of external systems. It suggests that LLMs learn a \"concept\" or \"task\" that is more abstract than specific data, which is evidence for our point."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5116/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700564380178,
                "cdate": 1700564380178,
                "tmdate": 1700564380178,
                "mdate": 1700564380178,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "q5AFfLVKjS",
            "forum": "qGaIMO8dqD",
            "replyto": "qGaIMO8dqD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5116/Reviewer_KmxN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5116/Reviewer_KmxN"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to show the existence of the template-content decomposition of complex natural language tasks. The paper first provides proof for the UAT of casual transformers, and then proves the existence of template transformers and content transformers. The paper is not easily readable. Even though the existence of the transformers model is proved, it is not clear the implication of the theorem. This is because the Transformers function $f_T$ and $f_C$ could be much more complex than a single transformer model. Also, there is no experimental result showing such decomposition benefits the tasks either in computation efforts or task performances."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* Provide proof of the existence of template transformers and content transformer functions for the template-content decomposition"
                },
                "weaknesses": {
                    "value": "* The importance of the existence of template-content decomposition is unknown\n* There is no experimental result showing such decomposition benefits the tasks either in computation efforts or task performances."
                },
                "questions": {
                    "value": "* Can $f_T$ and $f_C$ be more complex than a single transformer model? In other words, to decompose tasks that can be handled by an LLAMA 7B model, will the $f_T$ and $f_C$ be an LLAMA 70b function?\n\n* Will such model decomposition benefit the tasks either in computation efforts or task performances? Also, will such decomposition work for more general NLP tasks such as summarization, translation, or LLM benchmark tasks including [HELM](https://crfm.stanford.edu/helm/latest/) or [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5116/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699070975298,
            "cdate": 1699070975298,
            "tmdate": 1699636503368,
            "mdate": 1699636503368,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gmjcGOt8F1",
                "forum": "qGaIMO8dqD",
                "replyto": "q5AFfLVKjS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5116/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5116/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your insightful comments. We address your concerns as follows:\n\n1. **The importance of the existence of template-content decomposition**: \n    > \"It is not clear the implication of the theorem (the existence of template transformers and content transformers). The importance of the existence of template-content decomposition is unknown.\"\n\n    The t-c structure leads to **the reasoning ability** of modern LLMs, and we prove it in proposition 1 (section 4.4). The skeleton of our paper is to prove that (1) the t-c structure leads to reasoning ability, i.e., generating answers for complex reasoning problems, and (2) the transformers can learn the t-c structure. Combining these two parts, we can explain the reasoning ability of LLMs. Below is a detailed explanation.\n\n    Intuitively, one key point of complex reasoning lies in learning the correct combination of the multiple small steps, i.e., **the skeleton of a solving process**. This observation can be easily verified by examining answers generated by humans or LLMs. Taking math problems as an example, the process of human-generated answers typically consists of two parts: the overall problem-solving skeleton and the specific solution of each step. At the same time, powerful LLMs like chatGPT also show similar behavior (as shown in Figure 1). The skeleton determines the type of the problem and the solution method (such as a linear equation system composed of two variables in our example in Figure 1) while the specific steps perform concrete math operations (for example, determine the sum of two variables x + y is **35**). In our t-c structure, the skeleton tokens are defined as **template** and the specific step tokens are defined as **content**. The template should stay the same once the problem type is determined, while the content can vary according to concrete problems.\n\n    In Figure 1, we show such an important example to help understand the importance of the t-c structure. In the classic [rabbit and chicken problem](http://mathelogical.in/non-routine-mathematics/the-famous-rabbit-and-chicken-problem/), even if we change rabbit and chicken to correct and wrong answers (and also change the number of feet to the number of marks), the LLM uses the same template (of linear equation system) to solve the problem. In other words, the template remains the same for all problems of the same type, while the content is different for each problem. Therefore, we emphasize that if an LLM can generalize on various problems of the same type, rather than just remembering individual data points that appear in the training set, it **must be able to learn the template (invariant part) of the task**. Otherwise, if we modify the problem slightly, it will fail. So, the existence of the t-c structure is the key to the reasoning ability of LLMs.\n\n    Our theoretical analysis (sections 4.3 and 4.4) is to express the above intuition formally. Section 4.3 proves the existence of such a t-c transformer. Then, Proposition 1 (in section 4.4) shows the t-c transformers can generate the answer for a complex reasoning problem as long as they have seen a similar problem of the same task (even though its content may be completely different from the target problem), which paves the way for LLMs' generalizable reasoning ability.\n    \n    In summary, template-content decomposition is important because it explains where LLMs' strong reasoning ability comes from and shows that the transformer architecture can learn such a t-c structure. We illustrate it intuitively and theoretically in our paper. And we further verify it with additional experimental evidence in our response to the third point.\n\n2. **The complexity of the template (or content) transformers**:\n    > \"The template (and content) Transformers function could be much more complex than a single transformer model... Can $f_T$ and $f_C$ be more complex than a single transformer model? In other words, to decompose tasks that can be handled by a LLAMA 7B model, will the $f_T$ and $f_C$ be an LLAMA 70b function?\"\n    \n    No. Suppose a model can indeed handle the task, which means that any changes to the content of the problem will not affect its solution process. In this case, this model has successfully learned the t-c structure because our t-c structure is precisely defined by the independence of the template from variations in content. That is, if a Llama 7B model handles a task, it must have achieved the decomposition within its 7B parameters---a small model that can solve a type of problem indicates that this type of problem is not complex."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5116/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733174798,
                "cdate": 1700733174798,
                "tmdate": 1700733174798,
                "mdate": 1700733174798,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3ColdyL91Z",
                "forum": "qGaIMO8dqD",
                "replyto": "q5AFfLVKjS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5116/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5116/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "3. **Experimental results to show the importance of t-c decomposition**\n    > There is no experimental result showing such decomposition benefits the tasks either in computation efforts or task performances.\n\n    Thank you for your kind reminder. We are glad to explain our experimental results further and add some experimental evidence to show the importance of t-c decomposition.\n\n    1. First, our experimental results in section 6 show **the correlation between the capacity of LM to handle t-c structure with their reasoning performance**, which means for current LMs, a significant t-c structure is a condition of complex reasoning ability. To show the relation more clearly, we choose the t-c classification **AUC-ROC** as the t-c classification accuracy. We also report the **difference of the mean-variance** (**DMV**) $\\sum_{s\\in C} V_{s} / N_C-\\sum_{s\\in T} V_{s} / N_T$ where $N_T$ ($N_C$) is the number of template (content) tokens, and $V_s$ is the variance of the output probability at position $s$. DMV can measure how confident the model is about their classification results because the larger the DMV is, the more significant the different behaviors of the model for T and C. We compare them to their complex reasoning (CoT) ability and accuracy on GSM8k as the measure of their complex reasoning ability. \n    \n       | Model | DMV | AUC-ROC | CoT ability | Accuracy on GSM8k*** |\n       | ----------- | ----------- | ----- | ----- | --- |\n        | GPT2-335m | 0.15 | 0.87 | $\\times$ | 0.0 |\n        | GPT2-774m | 0.28 | 0.87 | $\\times$ | 0.0 |\n        | GPT2-1.5b | 0.27 | 0.97 | $\\times$ | 0.0 |\n        | OPT-1.3b | 0.29 | 0.96 | $\\times$* | 0.0 |\n        | OPT-13b | 0.42 | 0.99 | $\\times$* | 0.0 |\n        | OPT-30b | 0.46 | 1.00 | $\\times$* | 0.0 |\n        | Llama2-7b | 0.72 | 0.99 | $\\checkmark$** | 14.6 |\n        | Llama2-13b | 0.80 | 1.00 | $\\checkmark$** | 28.7 |\n        | Llama2-70b | 0.81 | 1.00 | $\\checkmark$** | 56.8 |\n    \n        \\* The OPT models have trouble following the instructions or simulating the examples of prompts; therefore, no CoT ability is shown. See [1] as a reference.\n\n        \\*\\* Llama-2 models have CoT ability to do some complex reasoning, such as GSM8k or MATH. See [2] for reference.\n\n        \\*\\*\\* Test in the 8-shot CoT setting to align with our framework. The CoT accuracy of the GPT2 and OPT models without CoT ability is 0.\n\n        The results show that the CoT ability, i.e., the complex reasoning ability, is available only for models that handle significant t-c structures. It shows that the t-c structure is the key to LLMs' complex reasoning (or CoT ability). For the model with less t-c classification ability, such as GPT2s, generating a reasonable CoT is too hard because just one wrong token will lead to failure under autoregressive generation, so the accuracy is 0. \n\n    2. The t-c structure can help the reasoning performance of LMs. For example, the template-content structure can be used as **data augmentation**, where we can replace the content in the training data to instruct our LMs to learn the decomposition and better reasoning ability. \n    Here, we collect 100 examples of the '*chicken and rabbit problem*' to form a dataset. For each question, we collect five answers generated by GPT-4. We use these question-answer pairs to finetune a Llama2-7b model. As a comparison, we train another Llama2-7b model with the same dataset and other settings but randomly **replacing the content** (denoted as **cr**) in each question-answer pair as data augmentation. In order to ensure that our content replacement does not just mitigate overfitting due to the introduction of data perturbation, we also conducted experiments on models with general data augmentation. Here, we selected EDA[3] as the baseline, a data augmentation method based on synonym replacement, random word deletion, and addition.\n    The efficacy of these approaches is depicted in our [Figure](https://imgur.com/a/R2cl6Ef), where the training steps are plotted along the x-axis, and the test accuracy (reported as pass@3) is represented on the y-axis. The results clearly demonstrate that content replacement enhances model performance. Specifically, the maximum performance increased from 75% (vanilla model, blue curve) to 85% (cr, orange curve), and from 85% (eda, green curve) to 95% (eda+cr, red curve) in the models incorporating cr augmentation. This experiment demonstrates that teaching the model the T-C structure through content replacement indeed helps the model to reason better, showcasing the importance of the T-C structure in reasoning."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5116/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733249692,
                "cdate": 1700733249692,
                "tmdate": 1700733249692,
                "mdate": 1700733249692,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "t7PsyJhTN0",
                "forum": "qGaIMO8dqD",
                "replyto": "q5AFfLVKjS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5116/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5116/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "4. **Other task**\n   \n   > \"Will such decomposition work for more general NLP tasks such as summarization, translation, or LLM benchmark tasks including HELM or lm-evaluation-harness?\"\n\n    Yes. While primarily demonstrated on complex reasoning tasks like arithmetic and symbolic reasoning, our template-content decomposition approach can apply to more conventional NLP tasks, provided these tasks incorporate relatively standardized processes. Take, for example, the task of summarizing a lengthy article. This task can be approached by initially extracting key information from each paragraph and then synthesizing these elements into a coherent summary. A possible template for this process might be: \"The first paragraph discusses [...]. The second paragraph addresses [...],\" and so forth. This method can be further granularized to summarizing individual sentences within a paragraph, using sub-templates like: \"Based on the sentence \\<sentence\\>, it appears to focus on \\<subject\\>.\"\n    \n    Another example could be commonsense reasoning such as [OpenbookQA](https://paperswithcode.com/dataset/openbookqa)[4] dataset. Here we show an answer generated by GPT-4.\n\n    >Question:\n    >Which of these would let the most heat travel through?\n    >A) a new pair of jeans.\n    >B) a steel spoon in a cafeteria.\n    >C) a cotton candy at a store.\n    >D) a calvin klein cotton hat.\n    >\n    >ChatGPT:\n    >\n    >To determine which of these items would allow the most heat to travel through, we can analyze each option based on the thermal conductivity of the materials involved:\n    >\n    >A) A new pair of jeans: Jeans are typically made from denim, a material woven from cotton fibers. Cotton is a poor conductor of heat, making it a good insulator. This means that jeans would not allow much heat to travel through them.\n    >\n    >B) A steel spoon in a cafeteria: Steel is a metal and metals are generally good conductors of heat. This means that a steel spoon would allow heat to travel through it quite efficiently.\n    >\n    >C) A cotton candy at a store: Cotton candy is made from sugar that is heated and spun into thin strands. While sugar can absorb some heat, the airy structure of cotton candy makes it a poor conductor of heat. It would not allow much heat to travel through it.\n    >\n    >D) A Calvin Klein cotton hat: Similar to the jeans, a hat made of cotton would also be a poor conductor of heat, as cotton is a good insulating material.\n    >\n    >Based on this analysis, option B) a steel spoon in a cafeteria, would let the most heat travel through due to the high thermal conductivity of steel.\n\n    This answer contains four steps that analyze each of the choices and a result. Each analyzing step can be decomposed into: 1) The index, 2) the object, 3) the property of this object, 4) the relation between the property and the question. LMs can use the same process to answer similar questions.\n\n    For these tasks, our t-c structure can also be applied. However, for some tasks like translation, the process is not fixed and the template is unclear, which is beyond our coverage.\n\n[1] Susan Zhang, Stephen Roller, et al: OPT: Open Pre-trained Transformer Language Models, 2022. <https://arxiv.org/pdf/2205.01068.pdf>.\n\n[2] Hugo Touvron, Louis Martin, et al: Llama 2: Open Foundation and Fine-Tuned Chat Models, 2023. <https://arxiv.org/pdf/2307.09288.pdf>.\n\n[3] Jason Wei, Kai Zou: EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks, 2019. <https://arxiv.org/abs/1901.11196>.\n\n[4] Todor Mihaylov, Peter Clark, Tushar Khot, Ashish Sabharwal: Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering, EMNLP 2018. <https://arxiv.org/abs/1809.02789>."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5116/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733279136,
                "cdate": 1700733279136,
                "tmdate": 1700733279136,
                "mdate": 1700733279136,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qvRY4OGKcf",
            "forum": "qGaIMO8dqD",
            "replyto": "qGaIMO8dqD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5116/Reviewer_oieR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5116/Reviewer_oieR"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a framework to explain language model's ability to solve complex reasoning tasks as parsing the input and generating the output in a \"template-content\" structured way. The paper theoretically formalizes this framework, and extends the framework to the hierarchical version to explain why models can solve arbitrarily complex tasks. Finally, the authors used the last-letter-concatenation task to show their T-C framework can help explain models' reasoning abilities in practice."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The proposed template-content framework is interesting, and the extension to the hierarchical version is also more flexible in explaining model's abilities in solving complex reasoning tasks."
                },
                "weaknesses": {
                    "value": "This paper made many simplified (possibly unrealistic) assumptions in its proposed theoretical framework and misses the connection to real-world tasks. \n- For example, the paper assumes a model is \"well trained\", meaning the model can memorize all answers perfectly. The authors claimed that this assumption is \"not challenging for the prevailing LLMs with a huge amount of parameters\", but many existing work has shown that even the largest models could struggle with tail knowledge [1].\n- The hierarchical extension assumes a model can decompose complex tasks well into sub-template and sub-content. But as the sub-template and sub-content can have many different combinations (especially if they are in slightly different forms as defined in Appendix A.1, based on label consistency), it is unrealistic to assume that the model is able to search in this combinatorial space efficiently and find the right template/content combination, especially when the task is very complex. Thus the presented theory in its current form, doesn't seem to be sufficient in explaining model's ability to solve very complex tasks.\n\n[1] Kandpal et al. Large Language Models Struggle to Learn Long-Tail Knowledge. ICML 2023.\n\nIn addition, the experiments are rather weak and do not support the theoretical part very well. \n- The two tasks used are very simple tasks, last-letter-concat and SingleEQ. In order to support the authors' claim on the *flexibility* and *generalizability* of this framework, a slightly more complex task should be presented as well. For example, on slightly more difficult algorithmic reasoning tasks like GSM8K or AQuA, can the hierarchical T-C be used to explain model's reasoning?\n- The results on T/C classification is a bit ad-hoc. Based on Figure 4 right, 1) as the authors mentioned, some green bars are too short to see, so it is unclear which model \"exhibits the clearest T/C distinction\"; 2) can the authors provide more quantitative analysis on the ratio between the green/blue bars? from the 2nd figure and the 3rd figure, the ratios seem not very significantly different, so I'm not sure if one can indeed judge a model has a better \"reasoning capability\" from those ratios.\n- In Figure 5, even on the simple SingleEQ task, the proposed classification already conflicts with the human intuition, so I'm not sure if this can be used in practice, especially for more complex reasoning tasks. Also on this task, the authors define content as \"names, objections, and Arabic numbers\", this also seems very ad-hoc and how can one apply this in general for any tasks?\n\n\nMissing references, the following paper also discusses how text (similar to the \"content\" part defined in this paper) and patterns (similar to the \"template\" part defined in this paper) affect reasoning performance in language models:\n- Text and Patterns: For Effective Chain of Thought, It Takes Two to Tango. Madaan et al. 2022."
                },
                "questions": {
                    "value": "- Can the authors provide more quantitative analysis on the ratio between the green/blue bars in Figure 4?\n- How can one define template/content in general for any tasks?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5116/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5116/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5116/Reviewer_oieR"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5116/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1700370833671,
            "cdate": 1700370833671,
            "tmdate": 1700370833671,
            "mdate": 1700370833671,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]