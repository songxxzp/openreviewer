[
    {
        "title": "Improving Gradient-guided Nested Sampling for Posterior Inference"
    },
    {
        "review": {
            "id": "JQljutArYV",
            "forum": "BjG6McP5nA",
            "replyto": "BjG6McP5nA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6559/Reviewer_C7my"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6559/Reviewer_C7my"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes GGNS, a performant, general-purpose gradient-guided nested sampling algorithm that scales well with dimensionality and demonstrates competitively on a range of synthetic and real-world problems. In particular, the gradients calculated with differentiable programming are combined with HSS to propose new points, dynamic nested sampling is used for parallelization, a new termination criterion and cluster identification are also proposed. Furthermore, the authors show the potential of combining nested sampling with generative flow networks, leading to faster mode discovery and convergence of evidence estimates compared with GFlowNets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "GGNS enables the use of gradient information through the differentiable programming frameworks. \nEmpirically, GGNS shows the best linear scaling and performs evidence estimation accurately even as the dimensionality approaches the number of live points. \nThe proposed method adds practical value in the nested sampling community. \nThe combination of GGNS with GFlowNets opens the door to a number of interesting future research."
                },
                "weaknesses": {
                    "value": "Novelty is limited to a combination of existing methods. \nThe proposal of using gradients in guiding the choice of new live points can be elaborated more in the Contribution section. \nExperiments are almost all synthetic, and some results do not seem to be better than GFlowNets."
                },
                "questions": {
                    "value": "In Figure 1, instead of plotting the error in the estimate of log Z versus the number of dimensions, how does the error grow versus the number of likelihood evaluations? \n\nIn Figure 1, the error bar (i.e., standard deviation across 10 runs) are much bigger for GGNS compared with the baselines, does it mean it is trading off variance for bias? \n\nTable 1, all methods except GGNS are under-biased for the Gaussian mixture example, is it just by chance? \n\nIn the 'Many Wells' experiment (Figure 4), are you comparing against FAB or FAB with buffer (which should give better results than FAB)? \n\nMinor:\nFormatting under Figure 1 needs to be modified. \npage 7, \"due to the high its high dimensionality\" -> \"due to its high dimensionality\""
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6559/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698610611153,
            "cdate": 1698610611153,
            "tmdate": 1699636741349,
            "mdate": 1699636741349,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BNy7j19DuN",
                "forum": "BjG6McP5nA",
                "replyto": "JQljutArYV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6559/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6559/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer C7my"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their very helpful feedback.\n\n\nWe have tried to highlight more clearly the original contributions from this paper: Firstly, we combine existing ideas that have never been combined before, resulting in a performance in sampling tasks that beats all existing state-of-the-art methods. Secondly, we use Nested Sampling with GFlowNets to generate a large number of samples from the target distribution. As the reviewer highlighted, we believe that this opens the door to many interesting research directions. \n\nTo clarify the value of the individual contributions, we have added ablation studies (Appendix G) to elucidate the value of each component. We have also highlighted more clearly how we use the gradient in guiding the choice of new live points in Section 3. We have also added an intuitive argument about why this update only requires $O(1)$ reflections, as opposed to baseline slice sampling. \n\nRegarding your questions:\n\n> In Figure 1, instead of plotting the error in the estimate of log Z versus the number of dimensions, how does the error grow versus the number of likelihood evaluations?\n\nWe have added a plot of log Z vs. the number of likelihood evaluations in a separate appendix (Appendix J). GGNS needs fewer likelihood evaluations to reach the same error.\n\n\n> In Figure 1, the error bar (i.e., standard deviation across 10 runs) are much bigger for GGNS compared with the baselines, does it mean it is trading off variance for bias?\n\nInteresting point. This is true inasmuch as the other methods do seem to have less variance while being more biased. At the same time, this is not an instance of the typical bias/variance tradeoff in probabilistic modeling: the other methods are biased because they are not correctly sampling the space (e.g., by missing modes, leading to an easier sampling problem). Therefore, it is not a matter of **more** model complexity or less regularization leading to less variance, but a matter of the **simpler** sampling methods being inaccurate.\n\n> Table 1, all methods except GGNS are under-biased for the Gaussian mixture example, is it just by chance?\n\nAs far as we understand, the reason other methods underestimate the normalization constant for the Gaussian mixture is that they are likely to have missed, or in some way underestimated, a mode. This leads to these methods thinking that there is less total posterior mass and, therefore, that logZ is smaller. We have added a comment about this in the table caption. \n\n\n> In the 'Many Wells' experiment (Figure 4), are you comparing against FAB or FAB with buffer (which should give better results than FAB)? \n\n\nThis is FAB with buffer, we have now clarified this in the text, thank you for pointing this out.\n\n> Minor: Formatting under Figure 1 needs to be modified. page 7, \"due to the high its high dimensionality\" -> \"due to its high dimensionality\"\n\nThank you; these have been fixed."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6559/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700491493929,
                "cdate": 1700491493929,
                "tmdate": 1700491493929,
                "mdate": 1700491493929,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5rDrE1bAES",
                "forum": "BjG6McP5nA",
                "replyto": "BNy7j19DuN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6559/Reviewer_C7my"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6559/Reviewer_C7my"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for addressing my concerns and the additional clarification in the paper, I appreciate the authors' effort in the additional ablation studies which has improved the paper. I maintain a positive score."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6559/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700696774096,
                "cdate": 1700696774096,
                "tmdate": 1700696774096,
                "mdate": 1700696774096,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3wFwE3liEp",
            "forum": "BjG6McP5nA",
            "replyto": "BjG6McP5nA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6559/Reviewer_vEK5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6559/Reviewer_vEK5"
            ],
            "content": {
                "summary": {
                    "value": "In this paper the authors propose a new nested sampling algorithm based on Hamiltonian slice sampling. Their sampling method removes the linear dependence of the number of live points on dimensionality. They show that their algorithm runs significantly faster using parallelization in state-of-the-art programming frameworks. Empirically they show that their algorithm can scale up to higher dimensional problems compared to prior work. In addition, they show potential integration into the generative flow networks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "I think it is an interesting and novel idea that they combine the learning-based samplers with nested sampling algorithms. In this sense this paper is well motivated. The paper is also clearly written and the math derivations are sound, to my best knowledge. They also provide comprehensive evaluations on various tasks and show that their sampling algorithm can scale to higher-dimensional problems."
                },
                "weaknesses": {
                    "value": "1. In the introduction section, the authors list 4 differences from prior work. Conceptually I am bit confused about how each of these 4 parts work with each other. I would hope that the authors can elaborate a bit.\n\n2. A minor formatting issue: the caption of Figure 1 got cluttered and needs to be fixed later."
                },
                "questions": {
                    "value": "Please see my questions in the previous section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6559/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6559/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6559/Reviewer_vEK5"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6559/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698719798203,
            "cdate": 1698719798203,
            "tmdate": 1699636741010,
            "mdate": 1699636741010,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "99ZD2VQlgg",
                "forum": "BjG6McP5nA",
                "replyto": "3wFwE3liEp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6559/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6559/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer vEK5"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their encouraging feedback. \n\n\nTo address the first point, we have added an appendix (Appendix H) with a complete GGNS algorithm. We have also added an ablation study (Appendix G) to analyse how our different improvements affect the algorithm\u2019s performance.\n\n\nWe have also corrected the formatting issue with the caption of Figure 1."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6559/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700491474008,
                "cdate": 1700491474008,
                "tmdate": 1700491474008,
                "mdate": 1700491474008,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "e9OPgDFzUC",
            "forum": "BjG6McP5nA",
            "replyto": "BjG6McP5nA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6559/Reviewer_NqDj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6559/Reviewer_NqDj"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces Gradient-Guided Nested Sampling (GGNS), a novel nested sampling algorithm that utilizes Hamiltonian Slice Sampling and gradient information for improved scalability and efficiency, especially in high-dimensional settings. By leveraging differentiable programming frameworks and parallelization, GGNS achieves significant speed advancements, overcoming the dimensionality dependence that hampers previous methods. The paper also demonstrates GGNS's ability to integrate with generative flow networks for effective sampling from complex posterior distributions."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is well structured and presents comprehensive experiments that highlight the merits of the proposed method. Notably, the method exhibits linear scaling with dimensionality, a feature not evident in prior techniques. Moreover, it outperforms existing approaches on several benchmark posterior sampling datasets. Furthermore, the authors present the idea clearly with the help of good visualizations."
                },
                "weaknesses": {
                    "value": "My primary concern with the paper is the lack of clarity regarding the proposed method. While the contributions section seems to align with the methods, it primarily outlines algorithmic tweaks to existing methods. These minor modifications collectively seem to yield an impact. However, the paper does not showcase the final algorithm that encapsulates these changes; I only found references to current algorithms in the appendix. This absence of a clear differentiation in algorithmic terms hinders my ability to give a higher score, even though the experiments are commendable. Additionally, I found section 5 ambiguous, especially concerning the objectives of the experiments outlined there."
                },
                "questions": {
                    "value": "-\tI didn\u2019t understand what was the advantage of the method for section 5, it seems a nice application that you can apply to generative flow networks, but what have we gained in training of the drift with your method, is there a baseline we can compare your method against?\n-\tPage 5 at the top there is some overlapping text below Figure 1. \n-\tI don\u2019t really understand how you took advantage of \u2018GPU interoperability\u2019?  \n-\tI don\u2019t quite understand why there error bars reduce with the number of dimensions in Figure 3."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6559/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6559/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6559/Reviewer_NqDj"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6559/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698765213653,
            "cdate": 1698765213653,
            "tmdate": 1699636740863,
            "mdate": 1699636740863,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wKAir0Sco1",
                "forum": "BjG6McP5nA",
                "replyto": "e9OPgDFzUC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6559/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6559/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer NqDj"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their very helpful feedback. We have tried our best to address these points with changes to the text.\n\n\n> lack of clarity regarding the proposed method\n\n\nWe appreciate the difficulty in understanding how all the components of the algorithm come together. We have now added an appendix with a concrete GGNS algorithm (Appendix H).\n\n\n> I didn\u2019t understand what was the advantage of the method for Section 5, it seems a nice application that you can apply to generative flow networks, but what have we gained in training of the drift with your method, is there a baseline we can compare your method against?\n\nIn summary, Nested Sampling provides only a limited number of samples, but by training a GFlowNet, we can get an arbitrarily large number of samples by amortization.\n\nGFlowNets are off-policy deep reinforcement learning algorithms. They are trained to sample a target density, but work best when the training is guided by a dataset of meaningful ground truth samples (to aid mode discovery), which need not be large or unbiased. The aim of Section 5 is thus twofold: we show that\n- By using samples from GGNS in the training policy of a GFlowNet, we promote rapid mode discovery and convergence;\n- After the GFlowNet has been trained (with GGNS guidance), we are able to use the trained model to obtain nearly-unbiased samples in a fixed number of sampling steps.\n\n> Page 5 at the top there is some overlapping text below Figure 1.\n\nWe have corrected the formatting error.\n\n> I don\u2019t really understand how you took advantage of \u2018GPU interoperability\u2019?\n\nWe have tried to clarify this point. The point here is that while Nested Samping algorithms had been written in differentiable programming languages before, none of those implementations took advantage of the gradient of the likelihood \u2013 their main advantage over implementations in non-differentiable programming languages arises from GPU interoperability. Therefore, what we meant to say (and we have tried to clarify), is that we do not take advantage only of GPU interoperability, but also of efficient parallel gradient computation.\n\n> I don\u2019t quite understand why there error bars reduce with the number of dimensions in Figure 3.\n\nThis is a very good question, which we had not fully explored in the original submission. The reason for this is that $\\mathcal{D}_{\\rm KL} (\\mathcal{P} | \\Pi)$ decreases as we increase the dimensionality, as we show in appendix F.2, meaning that the information gain when going from prior to posterior goes down. This could occur because of the cancellations between sine and cosine times, as the number of terms in the sum increases."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6559/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700491456887,
                "cdate": 1700491456887,
                "tmdate": 1700491456887,
                "mdate": 1700491456887,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uZuHMflQ7y",
                "forum": "BjG6McP5nA",
                "replyto": "wKAir0Sco1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6559/Reviewer_NqDj"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6559/Reviewer_NqDj"
                ],
                "content": {
                    "title": {
                        "value": "Response to rebuttal"
                    },
                    "comment": {
                        "value": "I think the authors for taking time to address some of my concerns, I however do not feel it fundamentally changes my opinion so will keep my score."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6559/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700654361787,
                "cdate": 1700654361787,
                "tmdate": 1700654361787,
                "mdate": 1700654361787,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "shtvUsBNu8",
            "forum": "BjG6McP5nA",
            "replyto": "BjG6McP5nA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6559/Reviewer_ZsRi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6559/Reviewer_ZsRi"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a way to effectively combine Hamiltonian slice sampling with nested sampling."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* Nested sampling is a widely used algorithm for model comparison in physics. Therefore, improving the scalability of nested sampling to higher dimensions is an important problem.\n* The paper provides a guideline on how to implement Hamiltonian slice sampling with nested sampling. It is well known that MCMC algorithms are very sensitive to such implementation detail. Therefore, this paper tackles a worthwhile issue."
                },
                "weaknesses": {
                    "value": "While the paper discussed how to implementat nested sampling, the exact details of the implementations are not self-contained within the paper. Therefore, it is difficult to actually grasp *how* the paper is proposing to implement these parts. I expect a paper of this type to be self-contained in terms of details. While the authors do provide code, (note that the reviewers are not expected to look at the supplementary material) the paper should explain the algorithmic details, provide insight, and contrast with previous implementations. The paper mostly relies on textual explanation, which lacks the required technical preciseness. Below are some specific examples:\n\n* Section 3 \"Adaptive Time Step Control\": \"This time step is adjusted dynamically ... increase or decrease ...\" How is it adjusted exactly?\n* Section 3 \"Trajectory Preservation\": What is exactly a \"trajectory\" here? It is the states of the Markov chain after multiple Markov chain transitions? Or the intermediate states of a Hamiltonian trajectory as in recycled MCMC methods [1]?\n* Section 3 \"Trajectory preservation\": \"select a new live point at random from the stored trajectories\" how random? Uniformly at random? Or weighted resampling as in typical Hamiltonian Monte Carlo implementations [2,3]?\n* Section 3 \"Pruning Mechanism\": \"This mechanism significantly improves the computational efficiency\" How/why does it improve the computational efficiency exactly?\n* Section 3 \"Differentiable Programming\": How is differentiable programming used here? Why does it help?\n\nFurthermore, for quantatitive/theoretical claims, quantatitive/theoretical evidence (rigorous if possible) is necessary. There are multiple claims that were not entirely obvious to me:\n\n* Section 3 second paragraph: \"the fact that gradients guide the path means one no longer requires $n_{\\text{live}} \\sim O(d)$\": I'm not sure if this is obvious. Is there a proof for this statement?\n* Section 3 \"Mode Collapse Mitigation\": \"... preventing them from converging prematurely to a single model\": Is there theoretical/empirical evidence for this? \n* Section 3 \"Robust Termination Criterion\": \"terminate ... has decreased by a predetermined fraction from its maximum value\": What is the theoretical principle behind this termination criterion?\n* Section 6: \"gradient-guided nested sampling ... makes use of the power of ... parallelization for significant speed improvements\": I couldn't find any empirical evidence on much this method takes advantage of parallelization. Did the authors measure the strong scaling of this method? Until how many cores does this scale? What is the efficiency? \n \nLastly, I found the experiments inconclusive both in terms of experimental design and the choice of baselines.\n* The paper claims that \"the ingredients in GGNS ... to significantly improve its performance in high-dimensional settings ...\" but none of the experiments are necessarily high-dimensional in todays standard. For instance the synthetic experiments in Section 4.1 Figure 1 only go as high as 128. See [4] Section 4.5 where the dimensionality goes as high as tens of thousands. While the method could be said to be scalable among nested sampling algorithms only, one could then question the significance of making nested sampling more scalable where more scalable alternatives exist.\n* The exact contribution of each design choices in Section 3 are not evaluated independently. Therefore, it is unclear how much each of the components are contributing to any performance improvement. Given that no theoretical evidence is provided, I would expect a thorough empirical analysis and motivations for the design decisions. A great example of this is the no-u-turn sampler paper [5], which provided two innovations: tuning the trajectory length and the stepsize. They provide separate evaluation for each: Figure 3,4 for tuning the stepsize and Figure 5 for the trajectory length.\n* Furthermore, some of the baselines are unclear. In Section 4.2, the paper states that HMC was used as a baseline. But HMC alone does not produce an estimate for the log-evidence unless special tricks are used like the harmonic mean estimator or Chib's method. How was HMC used to produce a log-evidence exactly? Similarly, the paper cites Halton (1962) for sequential Monte Carlo, but this paper seems unrelated to the sequential Monte Carlo used for estimating log-evidences as in [6]. Was this the intended citation? People usually attribute the genesis of sequential Monte Carlo to the bootstrap particle sampler [7] or the later seminal works [8,9]. \n* Moreover, the baselines are insufficient to really judge the performance of the method. At least in statistics, log-evidence estimation is popularly done using thermodynamic integration or bridge sampling [10]. Also, if the authors did intend to compare against sequential Monte Carlo as in [6], more implementation details are needed to really judge its validity, since SMC is notable for being sensitive to implementation details.\n* It is also curious why the authors did not use the same set of baselines for all problems. For instance, comparable nested sampling methods are only used in Section 4.1.\n\n### Minor Comments\n* Section 1 first paragraph: Hamiltonian Monte Carlo was initially developed by Duane et al. [11].\n* A similar reflective version of the HMC algorithm was developed by [12], although they did not consider the nested sampling setting. The authors might be interested to take a look.\n\n### References\nI am not the author of nor affiliated with the authors of the following papers.\n1. Nishimura, Akihiko, and David Dunson. \"Recycling intermediate steps to improve Hamiltonian Monte Carlo.\" (2020): 1087-1108.\n2. Neal, Radford M. \"MCMC using Hamiltonian dynamics.\" Handbook of markov chain monte carlo 2.11 (2011): 2.\n3. Betancourt, Michael. \"A conceptual introduction to Hamiltonian Monte Carlo.\" arXiv preprint arXiv:1701.02434 (2017).\n4. Buchholz, Alexander, Nicolas Chopin, and Pierre E. Jacob. \"Adaptive tuning of hamiltonian monte carlo within sequential monte carlo.\" Bayesian Analysis 16.3 (2021): 745-771.\n5. Hoffman, Matthew D., and Andrew Gelman. \"The No-U-Turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo.\" J. Mach. Learn. Res. 15.1 (2014): 1593-1623.\n6. Dai, Chenguang, et al. \"An invitation to sequential Monte Carlo samplers.\" Journal of the American Statistical Association 117.539 (2022): 1587-1600.\n7. Gordon, Neil J., David J. Salmond, and Adrian FM Smith. \"Novel approach to nonlinear/non-Gaussian Bayesian state estimation.\" IEE proceedings F (radar and signal processing). Vol. 140. No. 2. IET Digital Library, 1993.\n8. Chopin, Nicolas. \"A sequential particle filter method for static models.\" Biometrika 89.3 (2002): 539-552.\n9. Del Moral, Pierre, Arnaud Doucet, and Ajay Jasra. \"Sequential monte carlo samplers.\" Journal of the Royal Statistical Society Series B: Statistical Methodology 68.3 (2006): 411-436.\n10. Gelman, Andrew, and Xiao-Li Meng. \"Simulating normalizing constants: From importance sampling to bridge sampling to path sampling.\" Statistical science (1998): 163-185.\n11. Duane, Simon, et al. \"Hybrid monte carlo.\" Physics letters B 195.2 (1987): 216-222.\n12. Mohasel Afshar, Hadi, and Justin Domke. \"Reflection, refraction, and hamiltonian monte carlo.\" Advances in neural information processing systems 28 (2015)."
                },
                "questions": {
                    "value": "* Figure 3 why are the error bars decreasing as the dimensionality increases? I would assume higher dimensions are more challenging and therefore more variance. Is it not the case? In fact, in Figure 1, which I presume is the same type of plot, the results do seem intuitive.\n* Section 1 second paragraph: \"From the perspective of differentiable programming, less attention has been paid in recent years\" I did not quite understand the intention of this sentence. In what context does differentiable programming have something to do with sampling here?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6559/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6559/Reviewer_ZsRi",
                        "ICLR.cc/2024/Conference/Submission6559/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6559/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698803864399,
            "cdate": 1698803864399,
            "tmdate": 1700707221319,
            "mdate": 1700707221319,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JzycYwiejM",
                "forum": "BjG6McP5nA",
                "replyto": "shtvUsBNu8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6559/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6559/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ZsRi (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their very helpful and through feedback. We have tried our best to address the reviewer\u2019s concerns. \n\n\n> the exact details of the implementations are not self-contained within the paper\n\n\nFirst, we appreciate the comment about the lack of details of the algorithm. Therefore, we have added an Appendix H with the details of our implementation. We hope this will add clarity and transparency to the paper. \n\n\n> \"Adaptive Time Step Control\": \"This time step is adjusted dynamically ... increase or decrease ...\" How is it adjusted exactly?\n\n\nThe details of how adaptive time step happens are now in lines 21-24 of Algorithm 3. \n\n\n> \"Trajectory Preservation\": What is exactly a \"trajectory\" here? It is the states of the Markov chain after multiple Markov chain transitions? Or the intermediate states of a Hamiltonian trajectory as in recycled MCMC methods [1]?\n> \"Trajectory preservation\": \"select a new live point at random from the stored trajectories\" how random? Uniformly at random? Or weighted resampling as in typical Hamiltonian Monte Carlo implementations [2,3]?\n\n\nBy trajectory, we mean the steps of the Markov chain, we have clarified it in a footnote in Section 3. The points are sampled fully randomly, as we need to sample uniformly from the prior. We have also clarified this. \n\n\n> \"Pruning Mechanism\": \"This mechanism significantly improves the computational efficiency\" How/why does it improve the computational efficiency exactly?\n\n\nThe pruning mechanism improves the computational efficiency because, without it, we would continue to evaluate the likelihood for points that have drifted far away from the region of interest, leading to a waste of computational resources. We have tried to clarify this. \n\n\n> \"Differentiable Programming\": How is differentiable programming used here? Why does it help?\n\n\nDifferentiable programming is key here, in that our method requires calculating the gradient of the likelihood. Therefore, it will only work for likelihoods for which we can compute gradients\n\n\n> second paragraph: \"the fact that gradients guide the path means one no longer requires \": I'm not sure if this is obvious. Is there a proof for this statement?\n\n\nWe have added a intuitive argument for why we get a linear scaling with dimensionality for our method. The idea is that because the gradient is $SO(n)$-equivariant, HSS can explore the principal directions in a number of steps that is independent of the dimension of the ambient space. Please also see the answers to Reviewers skHM and Ec9X for intuition regarding this.\n\n\n> \"Mode Collapse Mitigation\": \"... preventing them from converging prematurely to a single model\": Is there theoretical/empirical evidence for this?\n\n\nWe have added empirical evidence for this in our ablation study (Appendix G.3). \n\n\n> \"Robust Termination Criterion\": \"has decreased by a predetermined fraction from its maximum value\": What is the theoretical principle behind this termination criterion?\n\n\nWe have added a more detailed motivation for our termination criterion in appendix I, as well as empirical evidence of it working better than the commonly used in in appendix G.4.\n\n\n> \"gradient-guided nested sampling ... makes use of the power of ... parallelization for significant speed improvements\": I couldn't find any empirical evidence on much this method takes advantage of parallelization. Did the authors measure the strong scaling of this method? Until how many cores does this scale? What is the efficiency?\n\n\nIt is hard to do an empirical study of how much we benefit from parallelization, as it would require rewriting the algorithm in a non-parallel way. Our argument is that, because our nested sampling steps are all run in an embarrassingly parallel way, our algorithm will benefit from an increased number of cores, up to $n_{live} / 2$, which is the number of points we can update simultaneously. \n\n\nA more detailed study with different computing configurations would be interesting but beyond our capabilities for the response period. Beyond that, we cast our contribution, in part, as an adaptation of nested sampling algorithms to hardware intended for modern machine learning workflows, featuring massive parallelization on GPUs. This is particularly important in data processing settings that combine nested sampling with deep learning, such as when the prior or likelihood models are given by deep neural networks. We have added this to the paper."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6559/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700491388077,
                "cdate": 1700491388077,
                "tmdate": 1700491422012,
                "mdate": 1700491422012,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "td7Qcmcrfk",
                "forum": "BjG6McP5nA",
                "replyto": "shtvUsBNu8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6559/Reviewer_ZsRi"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6559/Reviewer_ZsRi"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "I sincerely thank the authors for addressing my comments. In fact, I am impressed by the effort they have put in. Unfortunately, I have some major concerns that remain, that prevent me from increasing my score. In particular, one of the main claims of the paper that the method is scalable in terms of dimensionality, is still not clearly demonstrated. \n\n* **Pooled Baselines?**\n For objectively judging the performance in a controlled manner, I believe the experiments producing Figure 1 and Figure 2 are the most important. However, the baselines are \"pooled\" and not properly evaluated on the same problems. While the authors replied that, after showing that their method performs well on one experiment, they chose to \"move on\" to a different set of baselines, I do not quite agree that this is scientifically rigorous. Is it obvious that we will see the same experimental result despite a different setup?\n* **Lack of High-Dimensional Experiment**\nFurthermore, the experiment for Figure 2 is way too weak. For instance, take a look at Table 1 by Zhang and Chen (2022). There, they included a Log-Gaussian Cox process experiment, which is both geometrically challenging and also actually high-dimensional. On the other hand, the current paper only considers the 10-D funnel and a 2-D mixture. The remaining experiments have a different set of baselines for some reason, which, again, I do not find that it provides clear evidence of superiority.\n* **Paper Organization**\nLastly, while I appreciate the authors' effort to improve the paper and incorporate the comments of the reviewers, the paper is still not self-contained. This unfortunately, does not address the concern that other reviewers have also raised. I expect some major reorganization to make the paper meet the bar for the quality expected for an ICLR paper.\n\nLastly, I have some additional minor comments. The paper states, \"Differentiable Programming Whilst nested sampling algorithms written in differential programming languages exist,\" for which the authors clarified the meaning. I believe this wording is quite misleading. The sampler *itself* does not *internally* use differentiable programming. (There are, in fact, samplers that internally use differentiable programming in a non-trivial manner, for example, by differentiating through the Matropolis-Hastings correction step. So it's worth clarifying that the method is not doing something like that here.) All that it operates with, are the provided gradients right? I think it would be worth clarifying the language that the proposed methodology takes advantage of *gradients* not necessarily differentiable programming.  Furthermore, for the experiment in Figure 2, the authors mention that what is called \"HMC\" here used the harmonic mean estimator following Zhang and Chen. Unfortunately, the harmonic mean estimator is known as \"the worst Monte Carlo estimator ever\" due to its potentially infinite variance. Therefore, it cannot be considered to be a valid baseline for estimating marginal likelihoods. I recommend the authors look for better baselines, for instance, thermodynamic integration."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6559/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700530441967,
                "cdate": 1700530441967,
                "tmdate": 1700530612840,
                "mdate": 1700530612840,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dOimjdcXJj",
                "forum": "BjG6McP5nA",
                "replyto": "shtvUsBNu8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6559/Reviewer_ZsRi"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6559/Reviewer_ZsRi"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Authors,\n\n> > Paper Organization\n>\n> We are confused about this point, as we feel like we have addressed the points raised by the other reviewers in this regard. The paper now contains an implementation with all the algorithm details, an ablation study, and theoretical arguments for improved scaling. Therefore, we are not sure what the paper is lacking in terms of reorganization.\n\nI am specifically referring to my original comment about \"self-containment.\" By self-containment, I am saying that the details currently deferred to the appendix should have been blended in the main text. I believe readers should be able to have a rough idea of the proposed algorithm after having read only the main text, which does not seem to be the case. Recall that other reviewers have also expressed similar concerns, and I feel the main text still does not properly address this. Furthermore, given the current organization of the paper, it appears to me that this will require a major restructuring of the paper, which would require an additional round of thorough review, unfortunately.\n\nNote that all of this is for the benefit of the paper, I strongly believe the technical contribution of the paper deserves better presentation.\n\nNevertheless, given many of my original concerns have been addressed, I'll raise my score."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6559/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700705821503,
                "cdate": 1700705821503,
                "tmdate": 1700707207232,
                "mdate": 1700707207232,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "vWQLqMpgma",
            "forum": "BjG6McP5nA",
            "replyto": "BjG6McP5nA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6559/Reviewer_Ec9X"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6559/Reviewer_Ec9X"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a general-purpose gradient-guided nested sampling algorithm, GGNS, combining the state of the art in differentiable programming, Hamiltonian slice sampling, clustering, mode separation, dynamic nested sampling, and parallelization.  The authors show that the combination leads to faster mode discovery and more accurate estimates of the partition function."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The authors present a comprehensive review of related works."
                },
                "weaknesses": {
                    "value": "1. The main contribution of the paper is a combination of different existing methods. The paper lacks originality and significance. \n\n3. The reviewer cannot find the proposed GGNS  algorithm. The authors need to give a concrete GGNS  algorithm.\n\n2. There is no theoretical analysis regarding the GGNS. There is no theoretical guarantee for the advantages of the GGNS algorithm. \n\n4. Since the method is a combination of different strategies, the authors need to conduct an abolition study on the strategies to validate their effectiveness."
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6559/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6559/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6559/Reviewer_Ec9X"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6559/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699001883128,
            "cdate": 1699001883128,
            "tmdate": 1700703402768,
            "mdate": 1700703402768,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bj6ApEHhWb",
                "forum": "BjG6McP5nA",
                "replyto": "vWQLqMpgma",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6559/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6559/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Ec9X"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their feedback. We have tried to address all four points that were brought up:\n\n1. We have tried to further clarify our main contributions. Please see response to all reviewers for a summary.  \n\n\n2. We thank the reviewer for bringing up this point. We have now added an appendix (Appendix H) with a concrete GGNS algorithm, including all the details.\n\n\n3. We appreciate this point. To attempt to address it, we have tried to provide a justification for what is, in our opinion, the most important part of this work, which is the fact that each step only needs O(1) bounces; as this is what allows the algorithm to have a better scaling with the number of dimensions. This has been added to Section 3. In summary, while slice sampling only has information about one dimension per step (needing $O(n)$ steps to explore an $n$-dimensional space), when we use Hamiltonian slice sampling, the gradient provides information about all $n$ dimensions on each step. The $SO(n)$-equivariance of Hamiltonian slice sampling implies that exploration of a density concentrated around a low-dimensional manifold is less sensitive to the dimension of the ambient space. Please also see the answer to Reviewer skHM for intuition regarding this.\n\n\n4. This is a very good point. We have added an Appendix G with a detailed ablation study. Please see the response to all reviewers for discussion."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6559/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700491337634,
                "cdate": 1700491337634,
                "tmdate": 1700491337634,
                "mdate": 1700491337634,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ec0Y7E6E6e",
                "forum": "BjG6McP5nA",
                "replyto": "bj6ApEHhWb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6559/Reviewer_Ec9X"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6559/Reviewer_Ec9X"
                ],
                "content": {
                    "comment": {
                        "value": "Thank the authors for the clarification and improvements. However, the main contribution is still a combination of different strategies, and it is limited.  I would like to increase the score to 5."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6559/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700703527113,
                "cdate": 1700703527113,
                "tmdate": 1700703527113,
                "mdate": 1700703527113,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ERp2QpAFrJ",
            "forum": "BjG6McP5nA",
            "replyto": "BjG6McP5nA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6559/Reviewer_skHM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6559/Reviewer_skHM"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a new nested sampling algorithm which\nthrough a pruning method and adaptive step size results in\nasymptotically fewer likelihood evaluations compared to the\ncurrent state of the art. Experiments show that the method\nin addition to being more computational efficient yields\nhigher quality samples as compared to popular libraries\ndoing the same thing"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This is highly original work that makes a significant contribution\nto the literature on nested sampling. The method has compelling experiments\nto support it, and the underlying techniques are reasonably well-explained."
                },
                "weaknesses": {
                    "value": "As the paper includes many contributions, it would have been nice to see\nan ablation study showing how each of the contributions work to improve\nthe nested sampler. Additionally, some contributions like Mode Collapse\nMitigation only have supporting evidence in the appendix. It would be nice\nif some of those figures could be in the main paper.\n\nAn additional concern I have relates to the main claim that the sampler\nonly need O(1) bounces. I saw no proofs or even intuition for why this is\nthe case. For something so core to the paper, it would greatly improve the\npaper if even some intuition was provided for why that might be the case.\n\nMinor Issues:\n\nThere are a few typoes and misspellings that should be fixed."
                },
                "questions": {
                    "value": "What suggests only O(1) bounces are needed?\nWould it be possible to do an ablation study?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6559/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6559/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6559/Reviewer_skHM"
                    ]
                }
            },
            "number": 6,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6559/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699499000813,
            "cdate": 1699499000813,
            "tmdate": 1700713370224,
            "mdate": 1700713370224,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2cfBBNYnpK",
                "forum": "BjG6McP5nA",
                "replyto": "ERp2QpAFrJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6559/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6559/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer skHM"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their very helpful feedback and comments. We particularly appreciate the suggestion of an ablation study. We have now added an Appendix G with such a study. Please see the response to all reviewers for discussion.\n\n\nWith respect to the $O(1)$ bounces, the reviewer makes a good point that our original submission did not justify this claim. We have now added a theoretical justification for this point in Section 3. The main idea is that, while other methods like slice sampling only have information about one dimension per step (needing $O(n)$ steps to explore an $n$-dimensional space), when we use Hamiltonian slice sampling, the gradient provides information about all $n$ dimensions on each step, meaning we need only $O(1)$ steps. \n\n\nFor intuition, consider a density concentrated around a $d$-dimensional linear subspace of an $n$-dimensional space ($d\\ll n$). Usual (coordinatewise) slice sampling would require $O(n)$ bounces to mix within the subspace despite the space having only $d$ degrees of freedom. On the other hand, Hamiltonian slice sampling, which is equivariant to orthogonal transformations of the space, would take steps within the low-dimensional subspace and does not depend on the dimension of the ambient space.\n\n\nWe have also fixed multiple typos and misspellings."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6559/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700491313395,
                "cdate": 1700491313395,
                "tmdate": 1700491313395,
                "mdate": 1700491313395,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qAhui8jtFo",
                "forum": "BjG6McP5nA",
                "replyto": "2cfBBNYnpK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6559/Reviewer_skHM"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6559/Reviewer_skHM"
                ],
                "content": {
                    "comment": {
                        "value": "Yes, I think this greatly improves the paper! Thank you for the changes. I will update my score accordingly!"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6559/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700680588280,
                "cdate": 1700680588280,
                "tmdate": 1700680588280,
                "mdate": 1700680588280,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]