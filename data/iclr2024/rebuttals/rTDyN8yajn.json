[
    {
        "title": "Octavius: Mitigating Task Interference in MLLMs via MoE"
    },
    {
        "review": {
            "id": "wuGqdG5Qlh",
            "forum": "rTDyN8yajn",
            "replyto": "rTDyN8yajn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1570/Reviewer_7dhr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1570/Reviewer_7dhr"
            ],
            "content": {
                "summary": {
                    "value": "The work proposes a multimodal LoRA-MoE decoder for task- and modality-specific learning. The experimental results (about 20% improvement) have shown the effectiveness and versatility of our design in various 2D and 3D downstream tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The work adopts a simple but effective gate routing scheme allowing sparsely-activated LoRA modules to learn task- and modality-specific knowledge as an independent expert. \n2. The work can address various 2D/3D vision and language tasks, and conduct various experiments to validate the effectiveness and versatility with a few trainable parameters."
                },
                "weaknesses": {
                    "value": "1. The work focuses on SFT, whether it can be generalized to pre-training on massive data.\n2. Lack of evaluation on some of the latest MLLM Chat evaluation, such as MMBench, MME, SEED-Bench, etc.\n3. Lack of comparison with some classic and latest methods, such as MiniGPT4, mPLUG-Owl, LLAVA-1.5, Qianwen-VL, etc."
                },
                "questions": {
                    "value": "1. In Figure 5, Loras2 has a probability of 0 on 6 tasks. What could be the possible reasons for this phenomenon? Could you provide more visualizations of experts and provide some insightful analysis?\n2. Can you analyze the impact of k in the top-k sparse gate?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1570/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698648223348,
            "cdate": 1698648223348,
            "tmdate": 1699636085567,
            "mdate": 1699636085567,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3iWrZv0ZTQ",
                "forum": "rTDyN8yajn",
                "replyto": "wuGqdG5Qlh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1570/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1570/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> ***1. The work focuses on SFT, and whether it can be generalized to pre-training on massive data?***\n\n  - Generalizing to pre-training on massive data is not our focus. Our main motivation involves adding PEFT techniques (such as our LoRA-MoE) to the existing MLLMs to achieve better downstream performance generalization on more tasks and more modalities. \n\n  - Considering that other 2D MLLMs methods (except KOSMOS series, but they face replication problems) have also not mentioned training on massive datasets, for a fair comparison, we will not try to address this issue recently.\n\n  - Besides, collecting massive datasets for some modalities, such as point clouds, is quite challenging, thus preventing us from pre-training MLLMs on massive data in point cloud modality.\n\n> ***2. Lack of comparison with some classic and latest methods, such as MiniGPT4, mPLUG-Owl, LLAVA-1.5, Qianwen-VL, etc.***\n\n  - Our focus is not on proposing an entirely new SOTA 2D MLLM, but rather on addressing potential tug-of-war issues in downstream tasks by parameter-efficient fine-tuning existing MLLMs (such as LoRA). Therefore, direct comparison with the latest 2D MLLMs does not align with our motivation. \n\n  - For instance, as mentioned by Reviewer 3GST, InstructBLIP outperforms our baseline + LoRA-MoE on captioning/VQA tasks. It can only indicate that the baseline method used by InstructBLIP (BLIP-2) is much stronger than our chosen baseline method (LLaVA and LAMM). Additionally, such comparisons are completely unrelated to the motivation of our paper, because they cannot reflect improvements in task interference that our LoRA-MoE brings to the baseline model. Comparing the improvements with the baseline method itself on multi-task and multi-modal setup is more important, rather than comparing with other MLLMs. Therefore, we only compare the baseline model with LoRA and LoRA-MoE to explore the effectiveness of LoRA-MoE in this paper. \n\n  - A further discussion on this issue is why not use stronger baseline methods like InstructBLIP, KOSMOS series, and Shikra? It is because only LLaVA and LAMM are feasible options for us to adopt LoRA-MoE among existing 2D MLLM methods at that time.\n    \n    - LLaVA-1.5 was released as a preprint on 10/5, exceeding the submission deadline of ICLR.\n    \n    - KOSMOS series, InstructBLIP, and Qianwen-VL do not release their entire training datasets, so they face replication problems.\n    \n    - Shikra mainly focuses on 2D grounding tasks. Although Shikra is capable of performing other tasks such as captioning and VQA, it does not conduct a zero-shot evaluation on captioning/VQA tasks (Flicker30k and VQAv2). If we directly remove related datasets (Flickr30k, VQAv2) from Shikra's training set for zero-shot evaluation, we cannot guarantee the zero-shot performance of Shikra on captioning/VQA tasks.\n\n    - Other 2D MLLM methods (like MiniGPT4, mPLUG-owl, etc.) do not significantly outperform LLaVA and LAMM. Considering the simplicity of LLaVA and LAMM, and both of them provide implementation with LoRA, we choose these two methods as our baseline.\n\n  - Besides, even if we ignore the motivation discussed above, all of the mentioned methods, including but not limited to MiniGPT4, mPLUG-Owl, LLAVA-1.5, Qianwen-VL, are trained on different datasets, different training strategies (e.g., PEFT vs. full-parameters FT, one-stage vs. two-stage), different pre-training weights (e.g., Vicuna-13b vs. Vicuna-7b, Vicuna vs. LLaMA vs. BLIP-2). Especially the differences  in datasets have a significant impact on the performance of MLLMs. Considering that our focus is not on dataset design but on model design, directly comparing their performance is unfair.\n\n  - Although such comparisons are unfair and do not align with our motivation, we still provide comparisons with some latest methods in 2D modality in Table E. An apparent example of unfairness is that many models in the table fail in the detection evaluation. Although their training sets contain COCO, most of them mainly focus on captioning tasks, and the localization capability is not sufficiently trained, so they achieve poor results on VOC. (Note that **Table A** is in the next comment.)"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1570/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700147709491,
                "cdate": 1700147709491,
                "tmdate": 1700148376154,
                "mdate": 1700148376154,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GjKZ0bxJPw",
                "forum": "rTDyN8yajn",
                "replyto": "wuGqdG5Qlh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1570/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1570/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Table A**\n\n| Methods | FT. dataset | Training Strategy | VOC Recall@ 0.5 | ScienceQA Acc | Flickr30k CIDER | CIFAR10 Acc | Facial Hair Acc | Facial Smile Acc |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| LLaMA-Adapter-v2 | S1: LAION400M/COYO/MMC4/SBU/Conceptual Captions/COCO S2: GPT4-LLM/LLaVA-150k | S1: prefix + projection S2: bias + norm | Failed  | 50.22  | 37.67  | 85.30  | 46.88 | 53.18 |\n| Otter  | MIMIC-IT 2.8M | 1.3B parameters | Failed | 38.67 | 68.75 | 81.22  | 56.40 | 56.60 |\n| Shikra  | S1: COCO/VG/Flickr30k/VQAv2 S2: LLaVA-150k/VCR/Shikra-RD | Whole LLM | 38.87  | 40.31 | 73.90 (non zero-shot evaluation) | 81.36  | 55.71 | 70.20 |\n| miniGPT4  | S1: CC/SBU/LAION S2: CC  | projection | Failed  | 43.43 | 5.68 | 62.49 | 43.47 | 66.36 |\n| mPLUG-owl   | S1: LAION/COYO/CC/COCO S2: Aplaca/Vicuna/Baize/LLaVA-150k | S1: visual encoder + abstractor S2: LoRA | Failed  | 36.39 | 26.15 | 82.32 | 40.93 | 51.32 |\n| LLaVA-LoRA (Vicuna-13b) | LLaVA-80k  | LoRA + projector | Failed  | 52.35 | 30.75 | 2.89 | 12.50 | 50.23 |\n| LLaVA-MoE | LLaVA-80k  | LoRA-MoE + projector | Failed | 55.58 | 23.08 | 41.00 | 3.93 | 52.17 |\n| LAMM | COCO/Bamboo/Locount/TextVQA | LoRA + projector | 6.68  | 40.85 | 0.09 | 73.40 | 43.26 | 53.37 |\n| LAMM-MoE | COCO/Bamboo/Locount/TextVQA | COCO/Bamboo/Locount/TextVQA | 39.04 | 46.95 | 5.66 | 65.40 | 60.93 | 59.82 |"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1570/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700147775796,
                "cdate": 1700147775796,
                "tmdate": 1700149020344,
                "mdate": 1700149020344,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JI9p93QKDg",
                "forum": "rTDyN8yajn",
                "replyto": "wuGqdG5Qlh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1570/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1570/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">***3. Lack of evaluation on some of the latest MLLM Chat evaluations, such as MMBench, MME, SEED-Bench, etc.***\n  - Existing MLLM evaluation frameworks receive many concerns regarding data quality and evaluation accuracy. They require long-term observation:\n    - Some MLLM evaluation frameworks like MMBench/MME construct their evaluation data by mixing the ground truth (GT) with confusing options, turning visual tasks into some kind of multi-choice questions. However, the quality of evaluation data generated by the automatic program or GPT requires careful examination. For example, (see this [link](https://openreview.net/forum?id=BfMQIJ0nLc&noteId=Nnj067ngbg), Reviewer g871) a lot of VQA samples in MMBench can be solved without looking at the images. In this situation, the visual perception capabilities of the MLLM are not well tested.\n    - Other MLLM evaluation frameworks like SEED-Bench evaluate the performance of MLLM through PPL [1]. However, SEED-Bench also faces some concerns regarding the evaluation accuracy (see #[7](https://github.com/AILab-CVC/SEED-Bench/issues/7)).\n    - We will continuously follow the development of MLLM evaluation frameworks, and conduct a comprehensive assessment when they are more mature. In this situation, we believe it is reasonable and effective to use traditional metrics, like Recall/Precision/CIDEr, to evaluate the performance of visual tasks.\n  - Based on the above discussion, we provide the results on MME (excluding MMbench on data quality concern and SEED-bench on evaluation accuracy concern) in **Table B**.\n    \n    **Table B**\n    | methods | existence | count | position | color | posters | celebrity | scene | landmark | artwork | OCR | commonsense reasoning | numerical calculation | text translation | code reasoning | overall |\n    | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n    | LLaVA | 50.00 | 48.33 | 50.00 | 50.00 | 48.64 | 46.47 | 49.75 | 48.49 | 46.75 | 50.00 | 44.29 | 47.50 | 52.50 | 50.00 | 48.10 |\n    | LLaVA w/ MoE | 51.67 | 48.33 | 50.00 | 58.33 | 49.66 | 46.47 | 50.50 | 48.24 | 46.74 | 62.50 | 55.00 | 55.00 | 50.00 | 50.00 | 51.60 |\n\n[1] OpenNMT: Opensource toolkit for neural machine translation. 2017, Klein et al.\n\n>***4. In Figure 5, Loras2 has a probability of 0 on 6 tasks. What could be the possible reasons for this phenomenon?***\n  - Although LoRA#2 is not used in all 2D tasks (Figure 2), it is essential in the learning. We conducted experiments using three experts, as shown in **Table C**, and the performance is worse. It indicates that #2 is used in the middle-early stages of learning, but after convergence, the gate does not select #2 due to load balancing issues. We will add this table to the final version.\n    \n    **Table C**\n    | Gate | #Experts | VOC Recall@ 0.5 | VOC Prec@ 0.5 | ScienceQA Acc@1 |\n    | --- | --- | --- | --- | --- |\n    |  Top-2 | 1 | 7.61 | 5.95 | 40.31 |\n    |  Top-2 | 3 | 21.64 | 12.20 | 44.72 |\n    |  Top-2 | 4 | 39.04 | 35.21 | 46.95 |\n  - #2 is not used due to the load balancing issue. As mentioned in the Ablation, the sample-based gate is incompatible with the load balancing strategy. For example, it is more reasonable to assign detection samples to a LoRA expert proficient in localizing than the other experts for the purpose of load balancing. Additionally, the experimental results in Table 6 confirm this viewpoint.\n>****5. Could you provide more visualizations of experts and provide some insightful analysis?****\n  - We provide the results on the combined LAMMv2 + Scan2Inst [here](    https://github.com/iclrxoct/iclr2024xoct/blob/main/6dGg_3.png).\n    - In the 2D+3D setup, there are still load balancing issues (#2 and #5) in the routing distribution like the 2D setup.\n    - In the 3D tasks, 3D captioning and 3D classification mainly focus on instance-level tasks, such as the caption or category of a specific object (see Figure 1 in the main part of the paper and Figure 4 in the appendix), while 3D VQA focuses more on relationships between multiple objects in the scene and the understanding of the scene. Therefore, the routing weights have two different patterns on 3D Cap./3D Cls and 3D VQA.\n    - Some experts engage in knowledge sharing across different modalities (#2), while others may be more inclined towards a single modality (#1 and #5)."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1570/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700148571593,
                "cdate": 1700148571593,
                "tmdate": 1700148908344,
                "mdate": 1700148908344,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rxV7rmhbzf",
            "forum": "rTDyN8yajn",
            "replyto": "rTDyN8yajn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1570/Reviewer_3GST"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1570/Reviewer_3GST"
            ],
            "content": {
                "summary": {
                    "value": "Observing the tug-of-war problem between 2D detection and VQA, this paper proposes a Mixture-of-Expert (MoE) style architecture with LoRA for efficient optimization of multi-modality and multi-task. To further support 3D instruction-following tasks, the authors propose a novel point cloud encoding architecture called Object-as-scene. The simple yet effective architecture demonstrate strong experimental performance."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This paper constructs Octavius by collecting better detection annotation, combing multiple modality data and propose a novel MoE architecture.\n- The authors provide a throughout discussion between related works.\n- The proposed method is relatively simple but with strong performance."
                },
                "weaknesses": {
                    "value": "- About the tug-of-war problem:\n  - The authors demonstrate the existence of this problem simply by optimization 2D detection and VQA simultaneously.\n  - Considering the authors also include multi-modality, it would be better to include preliminary also in this direction (e.g., 2D & 3D captioning).\n- About the router input:\n  - As shown in Fig. 2, the input consists of the system prompt, modality embedding and the question embedding, which contradicts wit Tab. 5.  Would you mind clarifying how exactly to construct the router input?\n  - Also how to get the question embedding? Do you utilize other embedding model or just do that on-the-fly?\n\n- About the MoE deployment:\n  - Do you adopt LoRA-MoE in each Transformer block?\n  - Do you adopt LoRA only for the MLP, similarly with the original MoE?\n- About zero-shot evaluation:\n  - According to Sec. 4.1, the zero-shot evaluation conducted in this paper is only about zero-shot evaluation on novel datasets of the training tasks (e.g., ScienceQA for evaluation and VQA for training).\n  - Therefore, I wonder how it works if you have not seen any QA tasks during training, since it is difficult to understand a specialized architecture like MoE can generalize well to totally unseen tasks during pre-training.\n  - Moreover, if you still train with VQA, but change the prompt template of ScienceQA during testing, will the MoE router be robust to this kind of OoD generalization?\n- Overall, I think this is an interesting paper, but still with some problems to convince me about the effectiveness of the proposed method. I would consider increasing the score if my questions are well addressed."
                },
                "questions": {
                    "value": "- Implementation details:\n  - In Fig. 5, do you select a single layer to do the visualization or take an average of all layers?\n  - In Tab. 6, the authors claim that loading balancing loss has negative effect. Do you use loading balancing afterwards? If not, how would we prevent the MoE architecture from collapsing to always using a single expert?\n  - The 2D results in Tab. 4 have a significant performance gap with state-of-the-art VLLM like InstructBLIP (e.g., ScienceQA and Flickr30K captioning).\n  - The authors should have utilized stronger baseline methods. There is a significant performance gap between LAMN and state-of-the-art methods including InstructBLIP and Shikra."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1570/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698745231707,
            "cdate": 1698745231707,
            "tmdate": 1699636085487,
            "mdate": 1699636085487,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EKUhMww8g4",
                "forum": "rTDyN8yajn",
                "replyto": "rxV7rmhbzf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1570/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1570/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> ***1. About the tug-of-war problem: The authors demonstrate the existence of this problem simply by optimizing 2D detection and VQA simultaneously. Considering the authors also include multi-modality, it would be better to include preliminary also in this direction (e.g., 2D & 3D captioning).***\n\n* First, in the pilot study, we demonstrate the existence of the tug-of-war on the whole LAMM-v2 datasets, including detection, VQA, caption, and classification tasks, not only optimizing detection and VQA.\n\n* We reorganize the related results of multimodal tug-of-war issues (in Table 4) in **Table A**.\n\n  **Table A**\n  | FT. dataset | MoE | 2D Caption (ZS. Flickr30k) | 3D Caption (FT. Scan) | 3D Caption (ZS. NR3D) | Avg. |\n  | --- | --- | --- | --- | --- | --- |\n  | 2D | X | 0.21 | - | - | - |\n  | 3D | X | - | 35.1 | 16.19 | - |\n  | 2D + 3D | X | 0.04 | 19.76 | 8.26 | - |\n  | 2D + 3D | O | 10.06 | 33.29 | 17.22 | 43.91% \u2b06\ufe0f |\n\n  * From this table, we can find that the tug-of-war issues also exist in multimodal setup, and can even be more severe. Here we primarily compare 2D and 3D captioning tasks. When we introduce more modalities into instruction tuning, we can observe a huge performance drop, especially in 3D captioning tasks (35 -> 19, 16 -> 8). After applying LoRA-MoE, we can find that the performance of 3D captioning tasks can reach the level of performance that fine-tuned on the single 3D modality. Meanwhile, the performance of 2D captioning is also greatly improved.\n\n* This is a good perspective for analyzing the tug-of-war problems in the multimodal setup. We will add the tables and analysis to the final version. Thanks for your advice.\n\n> ***2. About the router input: As shown in Fig. 2, the input consists of the system prompt, modality embedding, and question embedding, which contradicts with Tab. 5. Would you mind clarifying how exactly to construct the router input? Also how to get the question embedding? Do you utilize other embedding model or just do that on-the-fly?***\n\n* We construct the router input following Equation 3, i.e., use question embedding only. And we obtain the question embedding on-the-fly.\n\n* We admit that Figure 2 is a little confusing. [Here](https://github.com/iclrxoct/iclr2024xoct/blob/main/3GST_1.png) is the updated version. We will modify it in the final version. Thanks for your advice.\n\n> ***3. About the MoE deployment: Do you adopt LoRA-MoE in each Transformer block? Do you adopt LoRA only for the MLP, similarly with the original MoE?***\n\n* In LoRA-MoE, LoRA experts are adopted in each Transformer block and each q/k/v/output projection layer. Specifically, we implement LoRA-MoE based on the original PEFT repository, so we replace all LoRA modules in LLMs with LoRA-MoE.\n\n* We will add these implementation details in the final version. Thanks for your advice.\n\n> ***4. In Fig. 5, do you select a single layer to do the visualization or take an average of all layers?***\n\nOctavius applies a global sample-based gating network, i.e., we employ one gating network to route LoRA experts in all layers, and Figure 5 illustrates overall gating scores."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1570/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700146276339,
                "cdate": 1700146276339,
                "tmdate": 1700146276339,
                "mdate": 1700146276339,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dxnncZenLl",
                "forum": "rTDyN8yajn",
                "replyto": "rxV7rmhbzf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1570/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1570/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> ***5. About zero-shot evaluation: According to Sec. 4.1, the zero-shot evaluation conducted in this paper is only about zero-shot evaluation on novel datasets of the training tasks (e.g., ScienceQA for evaluation and VQA for training). Therefore, I wonder how it works if you have not seen any QA tasks during training, since it is difficult to understand a specialized architecture like MoE can generalize well to totally unseen tasks during pre-training. Moreover, if you still train with VQA, but change the prompt template of ScienceQA during testing, will the MoE router be robust to this kind of OoD generalization?***\n\n**Table B**\n\n| FT. dataset | MoE | ScienceQA | Caption | Det Recall50 | Cls |\n| --- | --- | --- | --- | --- | --- |\n| LAMM-v2 | X | 40.31 | 0.21 | 7.61 | 73.50 |\n| LAMM-v2 | O | 46.95 | 5.66 | 39.04 | 65.40 |\n| LAMM-v2 wo/ QA | X  | 14.97  | 0 | 20.64  | 72.06 |\n| LAMM-v2 wo/ QA | O | 12.59 | 0  | 36.55 | 67.61 |\n\n* We provide results in **Table B**. Specifically, we remove QA dialogues from the instruction dataset (including TextQA and common sense QA generated on COCO, reducing about 100k data in total) and test its downstream QA capability on ScienceQA datasets.\n\n  * From Table C, we find both the baseline model and MoE have significant performance drops in the ScienceQA evaluation, indicating MoE can not generalize well to unseen ScienceQA tasks during pre-training. Although generalizing to unseen tasks is not our primary focus, it is an interesting topic that we will research in the future. \n\n  * Considering the reduction in data amount (from 293k to 193k), there is a performance decrease in most downstream tasks. However, an interesting observation is that in the absence of MoE, the baseline, which is fine-tuned on the instruction dataset without QA datasets, achieves better results on detection tasks, proving the conflict between detection and QA tasks once again.\n\n* For OOD generalization in QA tasks, we provide experimental results in **Table C**. The query prompt of ScienceQA datasets contains three parts: question, context, and image. We enriched the question and context using GPT-3.5-turbo, respectively. We evaluate the ScienceQA Acc@1 using the 2D Octavius model.\n\n  **Table C**\n\n  | type | Default wo/ enrichment | Enriched contexts | Enriched questions | Enriched contexts + questions |\n  | --- | --- | --- | --- | --- |\n  | results | 46.95 | 47.58  | 47.43  | 47.03  |\n\n  * Our model shows improvements in both enriched contexts and questions, demonstrating strong OOD generalization and robustness of LoRA-MoE on varying questions and contexts. We also pick some examples [here](    https://github.com/iclrxoct/iclr2024xoct/blob/main/3GST_2.png).\n\n  * We also provide the comparison of the routing weights between the default template and the enriched template [here](      https://github.com/iclrxoct/iclr2024xoct/blob/main/3GST_3.PNG). **It is evident that regardless of the enriched parts, our model consistently selects similar gates with similar weights.** This observation serves as evidence for the robustness of our model in addressing VQA questions/contexts with different patterns.\n\n* We will add the tables, illustrations, and analyses to the final version.\n\n> ***6. In Tab. 6, the authors claim that loading balancing loss has negative effect. Do you use loading balancing afterwards? If not, how would we prevent the MoE architecture from collapsing to always using a single expert?***\n\n* We do not use a load balancing strategy afterwards. As mentioned in the section 4.3 (ablation), sample-based gate is incompatible with the load balancing strategy. For example, it is more reasonable to assign detection samples to a LoRA expert proficient in localizing than the other experts for the purpose of load balancing. Unless the dataset itself is balanced, it's difficult to equally assign experts. Additionally, the experimental results in Table 6 confirm this viewpoint. The load balancing strategy seems more suitable for token-based gates in conventional MoE models, but in this tug-of-war scenario, its interpretability is worse. Moreover, the experimental results of using token-based gates (also with load balancing strategy) are poor in Table 6.\n\n* We claim the load balancing issue as a limitation in the paper and will attempt to address it in future work.\n\n> ***7. The 2D results in Tab. 4 have a significant performance gap with state-of-the-art VLLM like InstructBLIP (e.g., ScienceQA and Flickr30K captioning).*** \n\nInstructBLIP is built on powerful BLIP-2 (Vicuna-13b, caption 71.6 & ScienceQA 61.0, training with 129M caption/QA datasets), so it is reasonable that InstructBLIP has better results in caption and ScienceQA tasks."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1570/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700146756642,
                "cdate": 1700146756642,
                "tmdate": 1700149363023,
                "mdate": 1700149363023,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XWFGnj4Odc",
                "forum": "rTDyN8yajn",
                "replyto": "UfMGWPPnUm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1570/Reviewer_3GST"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1570/Reviewer_3GST"
                ],
                "content": {
                    "title": {
                        "value": "Response to Author Rebuttal"
                    },
                    "comment": {
                        "value": "Thanks for the detailed responses. My concerns mainly lie on two perspectives,\n\n**1. The choice of baseline methods.**\n\nInstead of answering why they do not select more suitable baseline methods, including InstructBLIP, Shirka and MiniGPT-4, the authors instead take Kosmos as a specific example to show LAMM is not worst baseline, ignoring the fact that all the aforementioned baseline methods are open to public at close time with LAMM. Considering the similarity of pre-training datasets among these baseline methods, it is highly possible that the proposed method cannot scale to better baselines.\n\n**2. The MoE architecture utilized in this paper.**\n\nI tend to agree with Reviewer 6dGg and 7dhr that there are many evidences suggesting that the reported results might be obtained via delicate designs instead of the flexible MoE achitecture, including 1) Octavius utilizes a global router for all the MoE layers instead of one for each layer, 2) activation maps in Figure 5 suggest that there are only two activate experts with one expert even totally not activated. It seems that the learnt model does not work as the authors claim in this paper.\n\nTherefore, it is hard to convince me the scalabiltiy of the proposed method for base models beyond LAMM and the considered 2D and 3D captioning and detection tasks. I choose to hold my initial judgment about this paper."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1570/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700732746663,
                "cdate": 1700732746663,
                "tmdate": 1700732746663,
                "mdate": 1700732746663,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "gMKs5GShFT",
            "forum": "rTDyN8yajn",
            "replyto": "rTDyN8yajn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1570/Reviewer_6dGg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1570/Reviewer_6dGg"
            ],
            "content": {
                "summary": {
                    "value": "This paper contributes a method for mitigating task interference in instruction tuning by learning LoRA-based Mixture-of-Experts. By using a sparse gate routing scheme, different LoRA experts can learn task- and modality-specific knowledge. In experiments, they do instruction tuning for MLLMs on both 2D image tasks and 3D point cloud tasks, each having individual vision encoders."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is well-motivated -- MoEs have been shown to be useful for distributing the different types of knowledge that are required for multi-task learning, and VL instruction tuning is a good application of this insight.\n\n- The experiments are performed on both 2D image and 3D point cloud tasks, both individually and with the two datasets combined."
                },
                "weaknesses": {
                    "value": "- I am primarily concerned by the analysis in Figure 5 -- it seems that all the 2D tasks are using only two experts! This makes me skeptical about the utility of MoE at all.  Could you run the ablation in Table 5 with 2 experts? so these two experts will get selected each time and there in routing involved, but the model has the capacity to learn with 2 LoRAs at once instead of one (which is what it seems to be doing in Fig5)\n\n- An additional analysis that is needed is how the gate routing is distributed between 2D and 3D tasks, for the model that is trained on the combined LAMMv2+ScanNet instruction tuning dataset.\n\nGiven the result in Figure 5, I am not convinced about the utility of MoE with routing, when it doesn't seem that different experts are even used.\n\n- I am not sure what the non-MoE baseline in Tables 2-3 is -- is it merely training the frozen model minus the LoRA parameters?"
                },
                "questions": {
                    "value": "- What is the actual LLM decoder that you use? it's never mentioned throughout the paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1570/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1570/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1570/Reviewer_6dGg"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1570/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698876574076,
            "cdate": 1698876574076,
            "tmdate": 1700983019760,
            "mdate": 1700983019760,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4nTG9Qjnvy",
                "forum": "rTDyN8yajn",
                "replyto": "gMKs5GShFT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1570/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1570/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> ***1. I am primarily concerned by the analysis in Figure 5 -- it seems that all the 2D tasks are using only two experts! This makes me skeptical about the utility of MoE at all. Could you run the ablation in Table 5 with 2 experts? so these two experts will get selected each time and there in routing involved, but the model has the capacity to learn with 2 LoRAs at once instead of one (which is what it seems to be doing in Fig5)***\n\n* We use top-2 gate for routing, i.e., selecting two experts from all experts for predicting. It is a very popular and common gating strategy used in MoE models [1-4]. \n\n* We provide the results of the model with 2 experts in **Table A**. In the \"2-learnable\" experiment, we apply normalized gating scores (a 2-dim tensor, e.g., [0.34, 0.66]) as expert weights, while in \"2-average\" experiments, we use [0.5, 0.5] as expert weights.\n\n  **Table A**\n\n  | Gate Type| #Experts | LoRA-Rank  | VOC Recall@ 0.5  | VOC Prec@ 0.5  | ScienceQA Acc@1 | # Trainable Params. |\n  |:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n  | Baseline | 1 | 32 | 7.61 | 5.95 | 40.31 | 0.4% |\n  | top-2 | 4 | 32 | 39.04 | 35.21 | 46.95 | 1.6% |\n  | top-2 | 2-learnable | 32 | 15.03 | 8.63 | 41.40 | 0.8% |\n  | top-2 | 2-average | 32 | 7.28 | 5.32 | 40.80 | 0.8% |\n\n  * We can observe a huge performance gap with 2 experts in detection (VOC) tasks. For the \"2-average\" experiment that directly applies [0.5, 0.5] as expert weights, it can be seen as simply increasing the model parameters. For example, you can replace 2 experts with a larger LoRA', i.e., LoRA' = 0.5*LoRA1 + 0.5*LoRA2. For the \"2-learnable\" experiment that applies weights generated by the gate, the gate can learn more kinds of combinations of routing weights for different tasks. To some extent, different combinations of routing weights can be seen as different routing results. Therefore, it achieves better results than the \"2-average\" experiment. \n\n  * When we use more experts (from 2 to 4), the gate has more choices and achieves the best results in **Table A**. However, as more experts are introduced, as mentioned in the section 4.3 (ablation), the model will be affected by the load balancing issues, leading to a decline in the utilization rate of different experts (this is why LoRA#2 is not used in Figure 5). Allocating too many tasks to the same experts eventually leads to a performance drop (8 experts perform lower than 4 experts in Table 5).\n\n* We will add the above results in Table 5 in the final version.\n\n[1] Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer. 2017, Shazeer et al.\n\n[2] GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding. 2022, Lepikhin et al.\n\n[3] GLaM: Efficient Scaling of Language Models with Mixture-of-Experts. 2022, Du et al.\n\n[4] Lifelong Language Pretraining with Distribution-Specialized Experts. 2023, Chen et al.\n\n> ***2. An additional analysis that is needed is how the gate routing is distributed between 2D and 3D tasks, for the model that is trained on the combined LAMMv2+ScanNet instruction tuning dataset.***\n\n* We provide the visualization of routing results on the combined LAMMv2 + Scan2Inst [here](    https://github.com/iclrxoct/iclr2024xoct/blob/main/6dGg_3.png).\n\n* In the 2D+3D setup, there are still load balancing issues (#2 and #5) in the routing distribution like the 2D setup. (see the discussion of the load balancing issues in section 4.3, ablation)\n\n* In the 3D tasks, 3D captioning and 3D classification mainly focus on instance-level tasks, such as the caption or category of a specific object (see Figure 1 in the main part of the paper and Figure 4 in the appendix), while 3D VQA focuses more on relationships between multiple objects in the scene and the understanding of the entire scene. Therefore, there are two patterns of routing weights between 3D Cap./3D Cls and 3D VQA.\n\n* Some experts engage in knowledge sharing across different modalities (#2), while others may be more inclined towards a single modality (#1 and #5)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1570/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700145154006,
                "cdate": 1700145154006,
                "tmdate": 1700145224088,
                "mdate": 1700145224088,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CfVkOUtLd7",
            "forum": "rTDyN8yajn",
            "replyto": "rTDyN8yajn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1570/Reviewer_WqWj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1570/Reviewer_WqWj"
            ],
            "content": {
                "summary": {
                    "value": "The paper mainly introduced 1) LoRA-MoE (Mixture-of-Experts) decoder to mitigate tug-of-war (interference) problem between different tasks/modalities, and 2) point cloud encoder called Object-As-Scene to extract language-aligned scene-level 3D features. \nExperiments on both 2D and 3D tasks show LoRA-MoE improves performance by ~20% over strong baselines like LAMM and LLaVA-LoRA. The model is also verified on multimodal learning with both images and point clouds."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- The idea of MoE with sample routing to mitigate task interference for MLLMs is novel.\n- The author conducted thorough experiments to validate the framework on a diverse set of 2D and 3D tasks. The gains are substantial.\n- The framework is modular and extensible to incorporate more modalities and tasks."
                },
                "weaknesses": {
                    "value": "- There is no analysis on how the routing among experts actually works. It would be great if the authors can provide some qualitative study of the predictions from sample-based gating network as responses to the input task, to show how the routing mechanism work. I wonder whether the gating network will simply act like a task classifier, or it's not the case.\n- The scaling behavior as more modalities and tasks are added is not studied. There may be limitations in very high multi-task settings."
                },
                "questions": {
                    "value": "- Can you show some qualitative study of the predictions from the sample-based gating network as responses to the input task? It can help us understand how network routing works."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1570/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699580416249,
            "cdate": 1699580416249,
            "tmdate": 1699636085344,
            "mdate": 1699636085344,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "d7PHadWJsX",
                "forum": "rTDyN8yajn",
                "replyto": "CfVkOUtLd7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1570/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1570/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> ***1. There is no analysis on how the routing among experts actually works. It would be great if the authors can provide some qualitative study of the predictions from sample-based gating network as responses to the input task, to show how the routing mechanism work. I wonder whether the gating network will simply act like a task classifier, or it's not the case.***\n\n* We provide the visualization of routing weights on 2D setup in Figure 5.\n\n  * In most downstream tasks, the questions of each task are the same in evaluation. Therefore, Figure 5 can be seen as the predictions from the gating network as responses to the input tasks. The only exception is ScienceQA task where different questions lead to different routing weights. We analyze the generated weights for all questions in ScienceQA and find that 3 experts (#0/1/3) are chosen in ScienceQA, but most of them are composed of LoRA#1 and LoRA#3. Therefore, we use the normalized average weights of #1 and #3 for visualization.\n\n  * Furthermore, we find that captioning/classification/facial attribute tasks share similar expert selection in Figure 5. It is because, although these tasks have different downstream applications, they are very similar in instructions and responses. I can give some examples below:\n\n    * Captioning:\n\n      **Q**: What is the caption of the image? \n\n      **A**: The image depicts an older man and a young man in a close embrace. The older man, who is bald and wearing a red shirt, appears to be hugging the younger man. They are standing in front of a black wall, which enhances the contrast of their positions.\n\n    * Classification:\n\n      **Q**: What is the most accurate category label for this image?\n    \n      **A**: The image depicts a cute and adorable small white fluffy dog.\n\n    * Facial Attribute\n\n      **Q**: What color is the person's hair in the image?\n\n      **A**: The image shows a young woman with dark, long, and curly hair.\n\n    From the above examples, we can find that in classification/facial attribute tasks, LLMs are inclined to provide detailed descriptions of the target object (including color, shape, adjectives, etc.) rather than just basic categories. They are similar to some simple captioning tasks, so it is reasonable that the gating network makes similar expert selections for captioning/classification/facial attribute tasks. **In these cases, the gating network can learn to allocate similar experts to similar but different tasks, and it can not be simply regarded as a task classifier. While in very different tasks like detection, the gating network tends to generate different expert selections.**\n\n* We also provide additional visualization on 2D+3D setup [here](https://github.com/iclrxoct/iclr2024xoct/blob/main/6dGg_3.png).\n\n  * In the 2D+3D setup, there are still load balancing issues (#2 and #5) in the routing distribution like the 2D setup. (see the discussion of the load balancing issues in section 4.3, ablation)\n\n  * In the 3D tasks, 3D captioning and 3D classification mainly focus on instance-level tasks, such as the caption or category of a specific object (see Figure 1 in the main part of the paper and Figure 4 in the appendix), while 3D VQA focuses more on relationships between multiple objects in the scene and the understanding of the entire scene. Therefore, there are two patterns of routing weights between 3D Cap./3D Cls and 3D VQA.\n\n  * Some experts engage in knowledge sharing across different modalities (#2), while others may be more inclined towards a single modality (#1 and #5).\n\n> ***2. The scaling behavior as more modalities and tasks are added is not studied. There may be limitations in very high multi-task settings.***\n\n* In the 2D/3D/2D+3D setups, our work already covers a wide range of tasks in most 2D/3D scenarios, including captioning, VQA, classification, etc., in both instruction tuning and downstream evaluations. Recent works such as VL-BERT[1], QWen-VL[2], and VisionLLM[3] involve 2D visual tasks that are similar to ours. If we want to further expand more datasets in these setups, it would primarily be expansions in terms of data amount.  Since this is a very high multitask learning in these setups, we believe that we can adequately demonstrate the generalization capability of LoRA-MoE in multitask learning and multimodal learning on the existing datasets and modalities. \n\n* Considering the workload, we will further explore more modalities on this topic beyond 3D like audio, and robotic tasks in future works.\n\n[1] Vl-beit: Generative vision-language pretraining. 2022, Bao et al.\n\n[2] Qwen-vl: A frontier large vision-language model with versatile abilities. 2023, Bai et al.\n\n[3] Visionllm: Large language model is also an open-ended decoder for vision-centric tasks. Wang et al."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1570/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700147986068,
                "cdate": 1700147986068,
                "tmdate": 1700147986068,
                "mdate": 1700147986068,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Kl4tfAQ0gi",
                "forum": "rTDyN8yajn",
                "replyto": "d7PHadWJsX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1570/Reviewer_WqWj"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1570/Reviewer_WqWj"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "The response addressed my concerns. I will keep my score unchanged."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1570/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700538350340,
                "cdate": 1700538350340,
                "tmdate": 1700538350340,
                "mdate": 1700538350340,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "XxemJdLl39",
            "forum": "rTDyN8yajn",
            "replyto": "rTDyN8yajn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1570/Reviewer_vGo5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1570/Reviewer_vGo5"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to use a combination of MoE with the LoRA technique to address the challenge of incorporating additional tasks in the MLLM. The MoE components are chosen using a sparsely gating network. The outcomes demonstrate the efficacy of this approach."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This method is straightforward but powerful, capable of integrating numerous vision tasks within this framework. Its adaptability is showcased as it seamlessly operates with both 2D images and 3D point clouds. Ultimately, the integration of LoRA with MoE for PEFT proves to be highly efficient."
                },
                "weaknesses": {
                    "value": "Given our awareness of each example's task, an important baseline involves employing a dedicated LoRA for each task individually. Additionally, conducting an ablation study on the impact of top-k would be informative."
                },
                "questions": {
                    "value": "How can you attain top-2 sparsity gates while ensuring compatibility with gradient flow?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1570/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699597736666,
            "cdate": 1699597736666,
            "tmdate": 1699636085269,
            "mdate": 1699636085269,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OGUOXKrIft",
                "forum": "rTDyN8yajn",
                "replyto": "XxemJdLl39",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1570/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1570/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> ***1. Given our awareness of each example's task, an important baseline involves employing a dedicated LoRA for each task individually. Additionally, conducting an ablation study on the impact of top-k would be informative.***\n\n**Table A**\n|method|VOC Recall@ 0.5|ScienceQA Acc@1|Flickr30k CIDEr|CIFAR-10 Acc@1|FacialAttr Smile Acc@1|\n|:----:|:----:|:----:|:----:|:----:|:----:|\n|LAMM (LoRA)|7.61|40.31|0.21|73.50|50.15|\n|LAMM (LoRA-MoE)|39.04|46.95|5.66|65.40|59.82|\n|LAMM (LoRA-individual)|28.38|48.54|5.83|82.11|*28.76|\n\n* We provide results in **Table A**. Specifically, we classify the 2D visual tasks as 6 types, i.e., QA, description, classification, detection, n-round conversations, and others, in instruction datasets, and employ 6 LoRAs for each task individually for joint training. During evaluation, we manually assign the expert corresponding to the most similar tasks in training to the downstream tasks.\n \n* The problem with employing separate LoRA for each task is that **it is difficult to assign a suitable expert for downstream tasks that are not present in instruction datasets**, e.g., Facial Attribute Recognition. In this task, we attempted to assign \"others\" or \"description\" experts to it, but both results were not good (see * in the table, we selected the \"others\" expert because it is relatively better). Moreover, **manually inputting the task type or the selected expert into the model during inference is not as efficient as the automatic gating based on questions in LoRA-MoE**.\n\n* Besides, **if we have no idea what downstream tasks will be performed, it is challenging to design a LoRA for each training task due to potential generalization issues**.\n\n> ***2. Additionally, conducting an ablation study on the impact of top-k would be informative.***\n\n**Table B**\n|Gate|#Experts|VOC Recall@ 0.5|VOC Prec@ 0.5|ScienceQA Acc@1|\n|:----:|:----:|:----:|:----:|:----:|\n|-|1|7.61|5.95|40.31|\n|top-1|4|22.42|21.23|36.88|\n|top-2|4|39.04|35.21|46.95|\n|top-3|4|38.57|36.02|43.89|\n\n* We provide results of adopting different top-k gates in **Table B**. We will add this table in the final version.\n\n* From the table, we can find:\n  * Top-1 gate has a poor performance due to its less flexibility in choice (4 combinations) compared with top-2/top-3 gate. \n  * While in the top-3 gate scenario, we find that the third choice is relatively small in most tasks (e.g., in classification tasks, the routing weight is [0.1, 0, 0.4, 0.5]), therefore it is very similar to the top-2 gate, and achieves similar results as the top-2 gate.\n\n> ***3. How can you attain top-2 sparsity gates while ensuring compatibility with gradient flow?***\n\n* We implement the top-2 gate based on an open-source implementation of [1] from [repo1](https://github.com/lucidrains/mixture-of-experts/tree/master) and [repo2](https://github.com/davidmrau/mixture-of-experts). We can give you a simple PyTorch-like pseudo code for top-2 gate implementation.\n  \n  ```python\n  class Top2Gate(nn.Module):\n      def __init__(self, num_experts, ...):\n          self.num_experts = num_experts\n          self.w_gating = nn.Parameter(torch.randn(input_dim, num_experts))\n          ...\n        \n      @staticmethod\n      def top1(tensor):\n          return tensor.topk(k=1, dim=-1)\n        \n      def forward(self, x):  # x: bs, n_token, dim\n          gate = torch.einsum('bnd,de->bne', x, self.w_gating)  # gate: bs, n_token, n_experts\n          gate = gate.mean(dim=-1).softmax(dim=-1)  # reduce token, gate: bs, n_experts\n        \n          gate_1, index_1 = self.top1(gate)\n          mask_1 = F.one_hot(index_1, self.num_experts)\n        \n          gate_without_top1 = gate * (1. - mask_1)\n          gate_2, index_2 = self.top1(gate_without_top1)\n\n          norm = gate_1 + gate_2 + eps\n          gate_1 /= norm\n          gate_2 /= norm\n        \n          top2_gate = torch.zeros(x.shape[0], self.num_experts)  # top2_gate: bs, n_experts\n          top2_gate.scatter_(1, index_1.view(-1, 1), gate_1.view(-1, 1))\n          top2_gate.scatter_(1, index_2.view(-1, 1), gate_2.view(-1, 1))\n          return top2_gate\n  ```\n\n* Other implementation details:\n  \n  * Given the input question embedding and N LoRA experts, we can obtain N-dim routing weights using the top-2 gate, where two experts with the highest scores are retained and the others are set to zero. Next, we can calculate the weighted sum of N LoRA experts. The non-activated experts are multiplied by a coefficient of 0 in routing weights.\n\n  * This approach has almost no significant impact on the computation cost when N=4/6 in our setting during fine-tuning. During inference, non-activated experts can be ignored, thus improving efficiency. Furthermore, if the tasks and questions are specified, LoRA-MoE can merge parameter weights with LLM like vanilla LoRA to reduce extra inference costs.\n\n[1] Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer. 2017, Shazeer et al."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1570/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700144010224,
                "cdate": 1700144010224,
                "tmdate": 1700145119047,
                "mdate": 1700145119047,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]