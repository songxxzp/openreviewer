[
    {
        "title": "Multi-View Causal Representation Learning with Partial Observability"
    },
    {
        "review": {
            "id": "83DEm5KW6x",
            "forum": "OGtnhKQJms",
            "replyto": "OGtnhKQJms",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7393/Reviewer_BPer"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7393/Reviewer_BPer"
            ],
            "content": {
                "summary": {
                    "value": "This paper generalizes existing theories on causal representation learning, multi-view learning by allowing multi-views, partial observability and dependency between latents. It provides identifiability theory for content variables and so-called identifiability algebra to combine content variables."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The literature review is extensive, although there are some recent works on CRL including identification under the nonparametric settings that might be relevant as well.\n\n[1] Liang, Wendong, et al. \"Causal Component Analysis.\" arXiv preprint arXiv:2305.17225 (2023).\n[2] Jiang, Yibo, and Bryon Aragam. \"Learning nonparametric latent causal graphs with unknown interventions.\" arXiv preprint arXiv:2306.02899 (2023).\n[3] Buchholz, Simon, et al. \"Learning Linear Causal Representations from Interventions under General Nonlinear Mixing.\" arXiv preprint arXiv:2306.02235 (2023).\n\n2. I like how the paper is structured with an intuition behind definitions and theorems for readability."
                },
                "weaknesses": {
                    "value": "1. Although the setting is different, the theoretical contributions and proof techiniques seem to be mostly derived from [1] and [2].\n\n[1] Roland S. Zimmermann, Yash Sharma, Steffen Schneider, Matthias Bethge, and Wieland Brendel. Contrastive learning inverts the data generating process. Proceedings of Machine Learning Research, 139:12979\u201312990, 2021\n\n[2] Julius von K\u00fcgelgen, Yash Sharma, Luigi Gresele, Wieland Brendel, Bernhard Sch\u00f6lkopf, Michel Besserve, and Francesco Locatello. Self-supervised learning with data augmentations provably isolates content from style. Advances in neural information processing systems, 34:16451\u201316467, 2021"
                },
                "questions": {
                    "value": "1. For definition 5.3, why assume l0 to be the same and not the two vectors are the same? For a given view, the content variables are fixed, right?\n\n2. Could you provide a little more motivation as to why view-specific encoders are needed? I understand the theoretical reason. But in practice, one would expect to train with all views available, right?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7393/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7393/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7393/Reviewer_BPer"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7393/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698723371064,
            "cdate": 1698723371064,
            "tmdate": 1699636885516,
            "mdate": 1699636885516,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uYGVro3p7P",
                "forum": "OGtnhKQJms",
                "replyto": "83DEm5KW6x",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7393/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7393/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer BPer"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the positive feedback about the structure of the paper. We also appreciate further references on identifiability under nonparametric settings [1, 2, 3], which we will discuss in the next revision. In the following we will answer the questions in detail.\n\n&nbsp;\n### *Question 1:  why not assume two vectors are the same?*\n\n  - **The shared content can be indexed at different positions**, especially when the number of views grows.  Since we consider a partially observable setup, each observation is generated by a subset of latents and, in general, does not have access to all of the latents. \n\n  + **For example**, say there are three underlying latent variables $z_1, z_2, z_3$. and two observations $x_1=f_1(z_1, z_2)\\, x_2 = f_2(z_2, z_3)$. In this case, the optimal selecting vectors for each should be $\\phi_1 = (0, 1), \\phi_2 = (1, 0)$, as different views are generated by different latents requiring different indexing. \n\n&nbsp;\n### *Question 2: motivation of view-specific encoders*\n  - **Our work focuses on a multi-view and potentially multimodal setup**, where data are potentially collected by different measuring devices. Each view with a different modality would require a separate encoder. Even if two views are from the same modality, we can only share parameters if they measure the same latents.\n\n  + We remark that the **joint intersection** (content by considering all available views) **differs from the intersection of all subsets of views** (and could be empty). **For example**, we have three partial views $x_1 = f_1(z_1, z_2)$,  $x_2 = f_2(z_2, z_3)$, $x_3 = f_3(z_1, z_3)$. In this case, the **joint content is empty** while the pairwise intersection would give us the latent variables of interest: e.g. we can identify $z_2$ from $(x_1, x_2)$; $z_3$ from $(x_2, x_3)$ and $z_1$ from $(x_1, x_3)$."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7393/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699968605573,
                "cdate": 1699968605573,
                "tmdate": 1699968605573,
                "mdate": 1699968605573,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jJVbci4MjG",
                "forum": "OGtnhKQJms",
                "replyto": "uYGVro3p7P",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7393/Reviewer_BPer"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7393/Reviewer_BPer"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your thoughtful rebuttal. I have decided to maintain my ratings."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7393/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700582400073,
                "cdate": 1700582400073,
                "tmdate": 1700582400073,
                "mdate": 1700582400073,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "OT0PxP37y4",
            "forum": "OGtnhKQJms",
            "replyto": "OGtnhKQJms",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7393/Reviewer_QCXK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7393/Reviewer_QCXK"
            ],
            "content": {
                "summary": {
                    "value": "This work studies identifying latent causal representations given multi-view data. This work shows that the latent subspaces if shared by more than one view, can be identified up to bijjective mappings. Further, a view-specific encoder is devised to make learning a large number of shared blocks tractable. Experiments on multi-modality/task data are provided to verify the theoretical insights."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "\u2022 The paper is well-written and communicates clearly motivations, formulation, technical details, and theoretical implications.\n\n\u2022 This paper extends previous content-identification results (mainly von Kugelgen et al. 2021, Daunhawer et al. 2023)  to multi-view distributions and further (identifiability algebra).\n\n\u2022 The empirical evaluation is thorough (over multi-view/modality/task datasets) and well-designed to substantiate the theoretical findings in the paper."
                },
                "weaknesses": {
                    "value": "\u2022 My primary concern is the novelty of the work. In comparison with prior work (von Kugelgen et al. 2021; Daunhawer et al. 2023), the technical contribution appears somewhat incremental.\n\n\u2022 The assumption that each view-specific generating function is invertible is strong, especially over multiple modalities. Intuitively, this requires the shared blocks to be duplicated multiple times and thus simplifies the identification problem. A recent work [1] has attempted to weaken this assumption.\n\n[1] https://arxiv.org/abs/2306.07916"
                },
                "questions": {
                    "value": "I would like to learn about the authors' response to the weaknesses listed above, which may give me a clearer perspective on the paper's contribution."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7393/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7393/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7393/Reviewer_QCXK"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7393/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698759963799,
            "cdate": 1698759963799,
            "tmdate": 1699636885388,
            "mdate": 1699636885388,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "roHUmyE22z",
                "forum": "OGtnhKQJms",
                "replyto": "OT0PxP37y4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7393/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7393/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer QCXK"
                    },
                    "comment": {
                        "value": "- We thank the reviewer for the valuable insights and the positive feedback on the written style and empirical evaluation. Our clarification regarding the novelty and contribution is listed in the *[general response](https://openreview.net/forum?id=OGtnhKQJms&noteId=qN34EnF5VV)*. \n\n+ We also thank the reviewer for the reference, which we will discuss in the revised version. **The key difference** is that  [1] assumes structural conditions on the underlying causal graph (Condition 2.4) whereas our work **does not assume anything about the underlying causal structure**. Overall, we completely agree that adding structural assumptions while weakening the assumption on the mixing functions is an interesting direction for future exploration."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7393/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699968244329,
                "cdate": 1699968244329,
                "tmdate": 1699968244329,
                "mdate": 1699968244329,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iR6bK821jc",
                "forum": "OGtnhKQJms",
                "replyto": "roHUmyE22z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7393/Reviewer_QCXK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7393/Reviewer_QCXK"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for taking the time to provide a thorough rebuttal and clarification. I appreciate the additional information and perspectives shared in your response. After careful consideration of your points and the initial review, I have decided to maintain the current rating."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7393/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700537135018,
                "cdate": 1700537135018,
                "tmdate": 1700537135018,
                "mdate": 1700537135018,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QtQSrJz8El",
            "forum": "OGtnhKQJms",
            "replyto": "OGtnhKQJms",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7393/Reviewer_rWVP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7393/Reviewer_rWVP"
            ],
            "content": {
                "summary": {
                    "value": "This work studies causal representation learning under partial observability. Causal representation learning is the emerging field of learning generative representations of data that could potentially have causal relationships among them. Identifiability refers to the existence of a unique generative model and is an important aspect of this field. Many works on CRL usually make certain simplifying functional assumptions or access to interventions, in order to exhibit identifiability. In this work, the authors provide a unified framework for identifiability in multi-view CRL, under various assumptions.\n\nThe data is assumed to be split into multiple views which are observed, where each view is a non-linear mixing function of a subset of the latent variables. Standard assumptions are made, including that the prior density is smooth and mixing functions are diffeomorphic. Under these assumptions, block-identifiability results are derived, when we have access to multiple partial views of the data. Because of the generality of the result, various prior results in this field can be viewed as special cases. The conceptual idea is that if we have a set of views, then we can identify a set of content encoders using a mix of an alignment and a regularization term. However, this leads to a potentially exponential number of encoders, which the authors handle via a single view-specific encoder. Experiments on synthetic data and visual/text data (Causal3DIdent and Multimodal 3DIdent) validate their theoretical ideas. The R^2 metric is reported, which measures how well the latents have been recovered. Overall, the paper is technically well-executed and fits the conference.\n\n### References:\n\n- [1] Learning Linear Causal Representations from Interventions under General Nonlinear Mixing\n\n- [2] Nonparametric Identifiability of Causal Representations from Unknown Interventions\n\n- [3] Identifiability of deep generative models without auxiliary information\n\n- [4] Identifying Weight-Variant Latent Causal Models"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- Causal representation learning has gotten a lot of attention recently, due to the promises it holds. This work is another important step in this direction.\n\n- The general identifiability result captures a variety of prior works on CRL, therefore it serves as a neat unifying contribution.\n\n- The approach to avoid using exponential set of encoders for each subset of views is very neat and is crucial for this work, for efficiency purposes."
                },
                "weaknesses": {
                    "value": "- R^2 metric is reported for validating identifiability, however as the authors also note, there are instances when R^2 scores can be inflated. Why didn't the authors also cite other standard identifiability metrics like MCC (such as the ones reported in [1], [2])?"
                },
                "questions": {
                    "value": "Some questions were raised above. Additional suggestions:\n\n- The recent work [1] also uses contrastive learning in CRL. Are there any resemblances to their approach, i.e. how do their loss function compare to content alignment?\n\n- The works [3], [4] also study nonlinear ICA without auxiliary variables. I believe they use variants of VaDEs in their experiments, can this also be viewed as a soft alignment inductive bias?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7393/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7393/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7393/Reviewer_rWVP"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7393/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698789694470,
            "cdate": 1698789694470,
            "tmdate": 1699636885250,
            "mdate": 1699636885250,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oATlPPHoaw",
                "forum": "OGtnhKQJms",
                "replyto": "QtQSrJz8El",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7393/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7393/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer rWVP"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer for providing further interesting references, which we will discuss in the revised version of the paper.  \n\n  - **Relation with [1]**: Although [1] also focus on identifiability in causal representation learning, their setting is completely different in that they assume (unknown) **single-node** interventions which we believe is a very strict assumption. They generalize prior work that uses single-node interventions in the class of mixing functions. *Their cross-entropy loss is related to our alignment and entropy terms*. Incorporating the NOTEARS objective is something we did not explore and may be interesting to combine the causal representation learning with causal discovery. We will add a citation in the revised manuscript, highlighting this connection as interesting future works. \n\n  + **Relation with [3-4]**: We fully agree with the reviewer that there is a connection with non-linear ICA with auxiliary variables, which we discussed in section 4, paragraph 4. We will add these references in this section for the next revision.\n\n&nbsp;\n\n\nWe also thank the reviewer for proposing another metric for identifiability. We remark that MCC measures **component-wise linear correlation** up to permutations, which differs from our setup: The definition of block-identifiability implies any type of bijective relation to the ground truth content variables, including **nonlinear** transformations, which MCC, in general, cannot capture. However, we fully agree with the reviewer that MCC is a very interesting metric that provides us more insight into the relation between the predicted and ground truth latents. Thus, we **report the MCC score** for the numerical experiments with independent (Sec 5.1) and dependent latent variables (Appendix D.1): \n\n|      | $(x_0, x_1)$    | $(x_0, x_2)$    | $(x_0, x_3)$    | $(x_1, x_2)$    | $(x_1, x_3)$    | $(x_2, x_3)$    | $(x_0, x_1, x_2)$ | $(x_0, x_1, x_3)$ | $(x_0, x_2, x_3)$ | $(x_1, x_2, x_3)$ | $(x_0, x_1, x_2, x_3)$ |\n| ----------- | ----------- | ----------- | ----------- | ----------- | ----------- | ----------- | ------------ | ------------ | ------------ | ------------ | ---------------- |\n| independent | 0.887\u00b10.000 | 0.881\u00b10.000 | 0.882\u00b10.000 | 0.885\u00b10.000 | 0.886\u00b10.000 | 0.880\u00b10.000 | 0.853\u00b10.000  | 0.854\u00b10.000  | 0.846\u00b10.000  | 0.851\u00b10.000  | 0.786\u00b10.000      |\n| dependent   | 0.956\u00b10.000 | 0.880\u00b10.002 | 0.891\u00b10.002 | 0.795\u00b10.002 | 0.805\u00b10.002 | 0.805\u00b10.002 | 0.945\u00b10.001  | 0.969\u00b10.001  | 0.858\u00b10.003  | 0.744\u00b10.003  | 0.944\u00b10.001      |"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7393/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699967955231,
                "cdate": 1699967955231,
                "tmdate": 1699967955231,
                "mdate": 1699967955231,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KSjzw9s0PO",
                "forum": "OGtnhKQJms",
                "replyto": "oATlPPHoaw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7393/Reviewer_rWVP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7393/Reviewer_rWVP"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their response. I am willing to keep my score and request the authors carefully revise the work, addressing all the feedback raised by the reviewers, especially on comparisons to prior works."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7393/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700536401691,
                "cdate": 1700536401691,
                "tmdate": 1700536401691,
                "mdate": 1700536401691,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "v7pyVn2vTr",
            "forum": "OGtnhKQJms",
            "replyto": "OGtnhKQJms",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7393/Reviewer_RoXZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7393/Reviewer_RoXZ"
            ],
            "content": {
                "summary": {
                    "value": "The authors study latent variable identifiability in the setting where there are multiple observations (views), each being a nonlinear mixture of a subset of latent components. Existing work identifies the shared latents for a given set of views $V$. This work generalizes the existing results, where the shared latents are identified for any subset of views $V_i \\subseteq V$. Moreover, this is done simultaneously for all possible $V_i$ in a single training run. Many related identifiability results can be framed as a special case of the authors\u2019 general framework."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper is written extremely well, and reads like a chapter from a textbook. The authors frequently refer to a running example and include \u201cintuition\u201d paragraphs to make it easier to understand their definitions and results. The loss that leads to identifiability (Eq. 3.1) is simple and intuitive, which makes me optimistic about the (eventual) practical applicability of this approach. The authors\u2019 framework unifies many existing theoretical results in nonlinear ICA and causal representation learning, and also explains the empirical success of certain existing methods. This is a very strong paper that made me want to learn more about the related literature."
                },
                "weaknesses": {
                    "value": "The main weakness of this work is its impracticality and lack of experimental results on realistic datasets. However, this is also acknowledged by the authors, and can also be said regarding most papers in this research area."
                },
                "questions": {
                    "value": "I understand that it is more general to be able to simultaneously identify the content blocks for all possible $V_i$, but what practical benefit does this generality bring? Was this necessary in order to unify a wider range of existing identifiability results? If so, can you give a  specific example how this generality helped?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7393/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7393/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7393/Reviewer_RoXZ"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7393/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699070985914,
            "cdate": 1699070985914,
            "tmdate": 1699636885131,
            "mdate": 1699636885131,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TchZTG0EST",
                "forum": "OGtnhKQJms",
                "replyto": "v7pyVn2vTr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7393/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7393/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer RoXZ"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the positive feedback, and the compliment about the significance of the paper. In the following, we list the **practical benefits** of simultaneously identifying all the blocks:\n\n- Our **intuitive motivation** is that we can measure the state of a system using multiple different sensors, each telling us something different. At the same time, we are interested in capturing the state of the system via high-level variables, and each block corresponds to a macro-variable (whose granularity depends on how fine-grained our sensors are). We believe this viewpoint will broaden the applicability of causal representation learning to more realistic and practical settings.\n\n+ This is also **important for unifying results** from the disentangled representation literature, where the goal is to recover all factors of variation. With this, we can perfectly cover the theoretical results of (Locatello et al., 2020, Ahuja et al., 2022).\n\n- Additionally, we provide an interpretation for recent disentanglement work (Fumero et al., 2023) that **scales to real-world data** in settings where factors of variations are not independent.\n\n+ Overall, we maintain that this setup is very flexible, covers important special cases, already explains some practical algorithms and we hope will lead to more."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7393/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699967482664,
                "cdate": 1699967482664,
                "tmdate": 1699967482664,
                "mdate": 1699967482664,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2StBO33MaB",
                "forum": "OGtnhKQJms",
                "replyto": "TchZTG0EST",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7393/Reviewer_RoXZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7393/Reviewer_RoXZ"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for pointing out that the generality is necessary in order to cover specific related works. I maintain my score recommending acceptance."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7393/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700502520422,
                "cdate": 1700502520422,
                "tmdate": 1700502520422,
                "mdate": 1700502520422,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]