[
    {
        "title": "Bridging Vision and Language Spaces with Assignment Prediction"
    },
    {
        "review": {
            "id": "85dFa83C5l",
            "forum": "lK2V2E2MNv",
            "replyto": "lK2V2E2MNv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9396/Reviewer_zAaX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9396/Reviewer_zAaX"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a method to align the visual representations of pretrained visual encoders into the input space of pretrained language models, using a linear projection layer. The linear layer is the only trainable part in the system, which is supervised by two losses: (a) assignment consistency - the visual features and text features are assigned to the word, and a similarity loss between the assignment results is applied (b) an image captioning objective. Using this method, experiments are done on 3 tasks, including image captioning, VQA, image-text retrieval to show that the method outperforms existing methods. Different variations of visual and text models are studied in the experiments. Additionally, some qualitative visual semantic arithmetic results are provided."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The method is simple and clear - train a linear layer with two losses including the newly proposed assignment prediction loss. \n2. Intensive experiments are provided on 3 tasks using different visual backbones (CLIP, BeiT) and text backbones (OPT-1.3B and T5-base), where results consistently outperforms existing methods.\n3. The paper is well-written and easy to follow."
                },
                "weaknesses": {
                    "value": "My major concern is (a) the lack of ablations and feature space visualizations to show the effectiveness of the proposed loss and (b) the contribution over existing works like MAGMA is not enough. \n1. The paper is an extension of MAGMA (Merullo et. al. Linearly mapping from image to text space. In ICLR, 2023.). While MAGMA is discussed in the paper, the difference is that this paper with MAGMA is the proposed assignment prediction loss. However, the effectiveness of the proposed loss is not shown clearly in the paper.\n2. No ablation results are provided to show the effectiveness of the proposed loss - this is related to weakness-1. Since the major contribution lies in this loss, an ablation to show the contribution of this loss in the final results is very critical.\n3. The finding that a linear layer can transform visual representations into language models is not surprising, given existing works LLaVA (\u201cVisual Instruction Tuning\u201d, as in its first training stage), which is not discussed in this paper, and MAGMA as discussed. Therefore, the contribution of this work is weakened.\n4. The authors motivate the work by criticizing the \u201cdistance-preserving nature of the linear layer\u201d. However, the proposed method is still a linear layer, which doesn\u2019t solve this problem. While Fig-4 provides several examples to show the visual semantic arithmetic, a visualization of feature space would be preferred to show the effects of the assignment loss\n5. The paper would be easier to read if the method names (abbreviations) in the results tables come with citations next to them, or are described in texts to show which is which."
                },
                "questions": {
                    "value": "1. Could abalations with and without the assignment loss be provided to show its effectiveness?\n2. Could visualizations (e.g. t-SNE) over the feature space with and without the assignment loss be provided, to show its effects in aligning the features?\n3. The difference/contribution over LLaVA or MAGMA can be more clearly discussed."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9396/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698518060187,
            "cdate": 1698518060187,
            "tmdate": 1699637184906,
            "mdate": 1699637184906,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EN1mSKnDQy",
                "forum": "lK2V2E2MNv",
                "replyto": "85dFa83C5l",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9396/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9396/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your constructive review and valuable suggestions! Below, we provide a detailed response to your questions and comments. If any of our responses fail to sufficiently address your concerns, please inform us, and we will promptly follow up.\n\n**[W1, W2, Q1] Ablation for the loss function**\n\nWe present the ablation analysis corresponding to the learning objectives in the general response.\nPlease refer to them.\n\n**[W3, Q3] Difference between VLAP and the previous linear transformation-based approaches**\n\nThanks for the reference.\nSimilar to the image captioning objective of LiMBeR, LLaVa learns the linear projection with the language generation objective, which aims to provide target answers from given transformed visual and language instruction representations.\nAlthough LiMBeR and LLaVa have shown two unimodal models can be bridged through the language modeling objective, they are insufficient to reduce the modality gap, as shown in our previous response.\nOur proposed objective not only improves overall performance in the linear transformation-based methods, but also significantly facilitates the connection of the visual encoder even if it is not trained with language guidance (i.e., CLIP encoder).\nIn addition, our objective can be an alternative to the conventional objectives for cross-modal alignment, such as the image-text matching and image-text contrastive objectives.\nWe add the missing reference LLaVa in the main paper. \n\n**[W4, Q2] Linear layer with assignment prediction**\n\nWe apologize for using the misleading term, \"distance-preserving nature of the linear layer\". We revise the Introduction section to clarify the contribution. \n\n**[W5] Model reference**\n\nThanks for your suggestion. We clarify the model names with citations in the table captions."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9396/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700487235358,
                "cdate": 1700487235358,
                "tmdate": 1700487235358,
                "mdate": 1700487235358,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TLa8YCTOeS",
                "forum": "lK2V2E2MNv",
                "replyto": "85dFa83C5l",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9396/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9396/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank you for your time and effort in reviewing our paper. Your insights have been highly valued, and your feedback is crucial to our progress. We understand that you have a busy schedule, and we again appreciate your time and attention to this matter. We have addressed all of your comments in our rebuttal. We would be grateful if you could please take a look at our rebuttal. If there are any additional materials or information you may require to facilitate the review process, please let us know. Your prompt attention to this matter is greatly appreciated.\n\nThank you again for your help. Sincerely Authors"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9396/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700577249859,
                "cdate": 1700577249859,
                "tmdate": 1700577249859,
                "mdate": 1700577249859,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "u2clOQUAEs",
                "forum": "lK2V2E2MNv",
                "replyto": "85dFa83C5l",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9396/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9396/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer zAaX,\n\nThanks for your constructive and valuable comments on our paper. We have revised our paper to address the reviewer's concerns.\n* **[W1, W2, Q1] Ablation regarding optimal transport.** We added ablation analyses for the components of VLAP, including the hyperparameters and optimal transport in Appendix Section D.\n\n* **[W3, Q3, W4] Difference between VLAP and the previous linear transformation-based approaches.** We included the missing reference (i.e., LLaVA). In addition, we extensively revised the paper to cover the reviewer's concerns about the novelty and to remove potentially misleading terms in Introduction.\n\n* **[W4, Q2] Visualization.** We presented the UMAP visualization corresponding to visual arithmetic and ablation for the learning objectives. We can verify the effectiveness of the proposed VLAP in the feature space.\n\n* **[W5] Citation.** We added references along with the model names to the table captions.\n\nWith the discussion deadline approaching in a few hours, we are eager to receive your feedback."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9396/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700723735032,
                "cdate": 1700723735032,
                "tmdate": 1700723757324,
                "mdate": 1700723757324,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7KGG0WrJCW",
            "forum": "lK2V2E2MNv",
            "replyto": "lK2V2E2MNv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9396/Reviewer_dYxb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9396/Reviewer_dYxb"
            ],
            "content": {
                "summary": {
                    "value": "In vision-language modeling, a significant challenge persists: bridging the modality gap between pretrained vision and language models. This gap arises primarily due to the models' pretraining exclusively on unimodal data, leading to inconsistencies in their embedding spaces. Motivated by this limitation and the computational costs of previous methods, this work introduces VLAP, a novel linear transformation-based approach that employs assignment prediction to connect vision encoders and large language models (LLMs). By harnessing the established word embeddings of LLMs and introducing an optimal transport-based assignment prediction objective, VLAP maps visual data representations to LLM's word embeddings, aiming for consistent modality representation. This not only results in visual data representations with the semantic richness of LLMs but also surpasses prior methods in computational and memory efficiency across various vision-language tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The limitations from SOA mentioned in the paper exist, and the motivation is valid.\n2. Resolving the modality gap problem with the cross-modal assignment prediction using word embeddings of LLMs is a better solution than previous methods."
                },
                "weaknesses": {
                    "value": "1.  A better alignment (reducing the gap) in multi-modality is the essential contribution of this work. However, it lacks studies or results, apart from the overall performance, to validate that the gap reduction is achieved by the current predicted assignments rather than the linear layers from previous works.\n2. The authors mentioned, ``Mapping visual data to LLM\u2019s word embeddings results in learned visual representations that hold a semantic taxonomy of LLMs.'' However, there's a lack of quantitative/qualitative results to validate that this allows visual representations to inherit a semantic taxonomy of LLMs.\n3. The final objectives are influenced by the assignment prediction loss and captioning loss. However, there's a lack of study on these hyperparameters. Also, which part contributes more to the learning remains a question.\n4. For the probability that the corresponding modality data belongs to each word, $P_{nk}$, what does $P_{nk}^{v}$ in the visual modality signify? Does this ``word'' refer to the single word token in the class label of that visual region?\n5. There's a lack of formal definitions for the terms/operations appearing in equations, i.e., $Tr(\\cdot)$, $[prefix]$.\n\n[Summary] The current limitations and motivations are valid, and the claimed contribution is significant. However, in the paper's delivery, there's a concern about how this performance is achieved by the proposed architecture and mechanism. Additionally, the paper lacks a depth of study beyond introducing a novel architecture."
                },
                "questions": {
                    "value": "Please also refer to the previous section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9396/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698772221901,
            "cdate": 1698772221901,
            "tmdate": 1699637184768,
            "mdate": 1699637184768,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ImOnf8pDhO",
                "forum": "lK2V2E2MNv",
                "replyto": "7KGG0WrJCW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9396/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9396/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your constructive review and valuable suggestions! Below, we provide a detailed response to your questions and comments. If any of our responses fail to sufficiently address your concerns, please inform us, and we will promptly follow up.\n\n**[W1] Validation for the modality gap reduction**\n\n| $\\mathcal{L}\\_\\text{cap}$ | $\\mathcal{L}\\_\\text{ITM}$ | $\\mathcal{L}\\_\\text{ITC}$ | $\\mathcal{L}\\_\\text{map}$ | CIDEr-D | CLIP-S | Ref-S | $\\Delta$ |\n|---------------------------|---------------------------|---------------------------|---------------------------|---------|--------|-------|--------------|\n| v                         |                           |                           |                           | 51.3    | 72.4   | 76.6  | 0.972        |\n| v                         | v                         | v                         |                           | 61.9    | 79.7   | 82.8  | 0.870        |\n| v                         | v                         | v                         | v                         | 65.2    | 82.8   | 86.0  | 0.840        |\n| v                         |                           |                           | v                         | 69.4    | 87.6   | 92.0  | 0.725        |\n\nTo provide a better understanding in terms of modality gap reduction, we measure the modality gap distance $||\\Delta||$ according to the learning objective.\nThe modality gap $\\Delta$ is defined as the difference between the center of visual and text representations [1]:\n\n$\\begin{equation}\n        \\Delta = \\frac{1}{n}\\sum\\_{i=1}^{n} \\bar{\\mathbf{z}}^{v}\\_i - \\frac{1}{n}\\sum\\_{i=1}^n \\bar{\\mathbf{z}}^t\\_i,\n    \\end{equation}$\n\nwhere $n$ is the total number of image-text pairs, $\\bar{\\mathbf{z}}\\_i^v$, and $\\bar{\\mathbf{z}}\\_i^t$ are the $i$-th averaged visual and text representations.\nThe default gap distance between CLIP ViT-B/32 and T5$\\_\\text{Base}$ before training is $||\\Delta|| = 1.378$.\nThe image captioning objective $\\mathcal{L}\\_\\text{cap}$ reduces the modality gap distance to $||\\Delta|| = 0.972$ and the additional objectives $\\mathcal{L}\\_\\text{ITM}$ and $\\mathcal{L}\\_\\text{ITC}$ further reduce the gap distance to $||\\Delta|| = 0.870$.\nThe comparison between VLAP with all objectives and with the original objective (i.e., $\\mathcal{L}\\_\\text{cap}+\\mathcal{L}\\_\\text{map}$) shows the ITM and ITC objectives have a limitation to reduce the modality gap.\nIn addition, since the overall performance is inversely proportional to the modality gap, reducing the gap is an important factor in training VLMs.\nWe also present UMAP visualization [2] and the additional analysis according to the learning objective in Figure 8 and Appendix. D.2 of the revised paper.\n\n    [1] W. Liang et al., ``Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning,\" NeurIPS'22\n    [2] T. Sainburg et al., ``Parametric UMAP embeddings for representation and semi-supervised learning,\" Neural Computation'21"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9396/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700486815284,
                "cdate": 1700486815284,
                "tmdate": 1700486815284,
                "mdate": 1700486815284,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UIK9HP8wp1",
                "forum": "lK2V2E2MNv",
                "replyto": "7KGG0WrJCW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9396/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9396/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**[W2] Additional analysis for visual arithmetic**\n\n|         | Building$\\rightarrow$Country |       |        | Country$\\rightarrow$Capital |      |        | CEO$\\rightarrow$Company |       |        | Food$\\rightarrow$Country |      |        |\n|---------|------------------------------|-------|--------|-----------------------------|------|--------|-------------------------|-------|--------|--------------------------|------|--------|\n| Method  | B@1                          | R@5   | CLIP-S | B@1                         | R@5  | CLIP-S | B@1                     | R@5   | CLIP-S | B@1                      | R@5  | CLIP-S |\n| ClipCap | 0.003                        | 0.035 | 0.24   | 0.0                         | 0.0  | 0.22   | 0.004                   | 0.005 | 0.18   | 0.0                      | 0.0  | 0.24   |\n| ZeroCap | 0.1                          | 0.32  | 0.7    | 0.14                        | 0.32 | 0.68   | 0.1                     | 0.3   | 0.64   | 0.03                     | 0.33 | 0.66   |\n| VLAP    | 0.3                          | 0.53  | 0.87   | 0.46                        | 0.62 | 0.89   | 0.25                    | 0.5   | 0.75   | 0.17                     | 0.5  | 0.81   |\n\nTo prove the visual representation that inherited the semantic taxonomy of LLM, we analyze that word analogies can often be solved with visual vector arithmetic. \nIn addition to existing experiments on visual semantic arithmetic, we supplement an experiment on the visual relation (VR) benchmark [3].\nThe VR benchmark consists of a total of 320 relations, including building$\\rightarrow$country, country$\\rightarrow$capital, CEO$\\rightarrow$company, food$\\rightarrow$country, and leader$\\rightarrow$country.\nSince the relation of leader$\\rightarrow$country includes out-of-date information (e.g. 'Obama'-'USA'), we exclude it from this experiment.\n    In the above table, we compare the performance between VLAP, ClipCap [4], and ZeroCap [3] in terms of BLEU-1, Recall@5, and CLIP-Score, following [3].\nFor fair comparisons, we identically use CLIP ViT-B/32 as the image encoder and GPT-2 as the LLM.\nThe results show that VLAP outperforms the previous methods by large margins.\nIn particular, we attain correlations of over 75\\% in all relations on the semantic distance-based metric, i.e., CLIP-Score.\nWe include the additional analysis with visualization in Figure 7 and Appendix. C.2 of the revised paper.\n\n    [3] Y. Tewel et al., ``ZeroCap: Zero-shot image-to-text generation for visual-semantic arithmetic,\" CVPR'22\n    [4] R. Mokady et al., ``ClipCap: Clip prefix for image captioning,\" arXiv'21\n\n**[W3] Ablation for the balancing parameters**\n\nWe present the ablation results corresponding to the balancing parameters in equation (7) in the general response.\nPlease refer to them.\n\n**[W4] Meaning of the probability in equation (5)**\n\nWe first explain the probability $\\mathbf{P}\\_{nk}^{t}$ for the text representation.\nSince LLMs provide text representations through sophisticated operations between word embeddings, text representations and word embeddings have high correlations with each other.\nTherefore, we can obtain the probability of the $k$-th word attending to the $n$-th text representation without any correspondence between language captions and word embeddings as in equation (5).\nMeanwhile, our assignment prediction objective enforces visual and text representations to contain the same information.\nSimply put, the probability $\\mathbf{P}\\_{nk}^{v}$, which represents the $k$-th word attending to the $n$-th visual representation, is learned to be similar to a pre-calculated probability $\\mathbf{P}\\_{nk}^t$ by predicting the assignment of text data without any image-word correspondence.\n\n**[W5] Clarity**\n\nWe denote the trace matrix as $\\text{Tr}(\\cdot)$ in equation (2) and the input text prompt (i.e., \"$\\texttt{A photo of}$\") as $\\texttt{[prefix]}$ in equation (6).\nWe clarify them in the revised paper."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9396/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700487004311,
                "cdate": 1700487004311,
                "tmdate": 1700487004311,
                "mdate": 1700487004311,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EjAWMKnj5L",
                "forum": "lK2V2E2MNv",
                "replyto": "7KGG0WrJCW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9396/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9396/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank you for your time and effort in reviewing our paper. Your insights have been highly valued, and your feedback is crucial to our progress. We understand that you have a busy schedule, and we again appreciate your time and attention to this matter. We have addressed all of your comments in our rebuttal. We would be grateful if you could please take a look at our rebuttal. If there are any additional materials or information you may require to facilitate the review process, please let us know. Your prompt attention to this matter is greatly appreciated.\n\nThank you again for your help. Sincerely Authors"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9396/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700577230641,
                "cdate": 1700577230641,
                "tmdate": 1700577230641,
                "mdate": 1700577230641,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "G0ahOcSaWq",
                "forum": "lK2V2E2MNv",
                "replyto": "7KGG0WrJCW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9396/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9396/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer dYxb,\n\nThanks for your constructive and valuable comments on our paper. We have revised our paper to address the reviewer's concerns.\n\n* **[W1, W3] Ablation regarding objectives.** We added ablation analyses for the learning objectives, including the hyperparameters and additional objectives in Appendix. Section D.\n\n* **[W2] Additional analysis for visual arithmetic.** We added the additional analysis for visual arithmetic on the visual relation benchmark, which demonstrates the learned visual representations can be used to reason semantic relations. We also provided the UMAP visualization to verify how the visual arithmetic of VLAP works in the feature space in Appendix. Section C.\n\n* **[W5] Clarity of notations.** We clarified some notations for readability.\n\nWith the discussion deadline approaching in a few hours, we are eager to receive your feedback."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9396/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700722841409,
                "cdate": 1700722841409,
                "tmdate": 1700722841409,
                "mdate": 1700722841409,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NE4BzEScxi",
            "forum": "lK2V2E2MNv",
            "replyto": "lK2V2E2MNv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9396/Reviewer_5A8P"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9396/Reviewer_5A8P"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces VLAP bridges vision encoders and language models through assignment prediction and the use of word embeddings to map visual representations into language space. \nAn optimal transport-based training objective is proposed to enforce the consistency of word assignments for paired multimodal data. This allows frozen LLMs to ground their word embedding space in visual data and use their robust semantic taxonomy visually. \nThe experiments demonstrate that VLAP outperforms the linear transformation-based approaches in a variety of vision-language tasks, such as image captioning, visual question answering, and cross-modal retrieval.\nIt also shows that the visual representations that have been acquired contain a semantic taxonomy of LLMs, thus making it possible to do visual semantic arithmetic."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is well-written and easy to follow.\nThe work proposed a straightforward way of learning the linear projection layer for visual modality to learn multimodal representation, which accommodates the LLM generation.\nThe visualization shows an impressive semantic arithmetic ability to combine multimodality understanding in LLM generation."
                },
                "weaknesses": {
                    "value": "(1) The main concern of this work is the methodology is relatively incremental without new concepts or findings.\nConcept-wise and architecture-wise, it is similar to Asano et al. (2020) Selavi, which performs optimal transport across modalities with similar pipelines. Mathematics using the Sinkhorn clustering Swav as Caron et al. (2020).\n(2) The main difference lies in 3 parts: word embedding as fixed center space, different distribution assumptions (polytope), and LLM application.\nThe first two are the most interesting part, which will be different from previous Sinkhorn-based work.\nHowever, there is no ablation study on these two components, which leads the readers to question whether borrowing existing Selavi and Swav will also work.\n(3) Also, there is no ablation on different objectives, such as existing next-word prediction on learning visual projection on LLM."
                },
                "questions": {
                    "value": "Either additional ablation, justification, or additional baseline can elaborate the concern in the weakness (2)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9396/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9396/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9396/Reviewer_5A8P"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9396/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698779201028,
            "cdate": 1698779201028,
            "tmdate": 1699637184640,
            "mdate": 1699637184640,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gd4btTqCtx",
                "forum": "lK2V2E2MNv",
                "replyto": "NE4BzEScxi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9396/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9396/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your constructive review and valuable suggestions! Below, we provide a detailed response to your questions and comments. If any of our responses fail to sufficiently address your concerns, please inform us, and we will promptly follow up.\n\n**[W1, W2] Novelty and ablation for assignment prediction**\n\nThanks for your valuable comments.\nAs the reviewer mentioned, our VLAP differs from the previous optimal transport-based (or clustering-based) approaches [1, 2, 3, 4] in the following aspects:\n\n(1) defining a fixed central space instead of a learnable central space (i.e., prototypes) and\n(2) defining the transport polytope with the word distribution of the training dataset instead of the equipartition assumption.\n\nThe previous works aim to learn visual representations [1, 2] or multimodal representations [3, 4] by \\textbf{training the whole networks}, including backbone networks and the central space, in an end-to-end manner.\nHowever, we need to connect two \\textbf{frozen} backbone networks using visual features as LLMs input.\nIn our setup, we empirically found that learnable space is heavily sensitive to the hyperparameters (e.g. $\\epsilon$ in equation (4)) and provides poor performance.\nTherefore, the conventional optimal transport-based approaches [1, 2, 3, 4] have been limited to be directly applied to our work.\n\n|       |                      |      | $\\epsilon$-MSCOCO (CIDEr-D) |      |       |\n|-------|----------------------|------|-----------------------------|------|-------|\n|       | Method               | 0.1  | 0.05                        | 0.01 | 0.005 |\n| (i)   | VLAP w/o word embed. | 20.4 | 43.8                        | 38.9 | -     |\n| (ii)  | VLAP w/o word dist.  | 50.7 | 57.9                        | 57.1 | 46.2  |\n| (iii) | VLAP                 | 61.8 | 68.0                        | 69.9 | 67.7  |\n\nTo verify our claim, we conduct additional experiments for the components in assignment prediction.\nAs shown in the above table, we evaluate each model for the zero-shot image captioning performance (CIDEr-D) on MSCOCO corresponding to various $\\epsilon$ values in equation (4).\nIn (i) \"VLAP w/o word embed.'', we first train VLAP with learnable prototypes as in [1, 2, 3, 4]. \nIn this setting, we define 3K prototypes following [1] and apply the equipartition assumption in equation (3).\nIn (ii) \"VLAP w/o word dist.'', we also verify the effectiveness of the word distribution.\nIn this setting, we use the word embeddings as a fixed central space and perform the optimal transport with the equipartition assumption.\nThe comparison between (i) and (ii) shows that the word embedding achieves substantial performance improvement, demonstrating the effectiveness of the word embedding.\nThe comparison between (ii) and (iii) original VLAP shows that the transportation polytope with the word distribution achieves additional performance improvement and provides more robust performance regarding $\\epsilon$.\n\n    [1] M. Caron et al., ``Unsupervised learning of visual features by contrasting cluster assignments,\" NeurIPS'20\n    [2] Y. M. Asano et al., ``Self-labeling via simultaneous clustering and representation learning,\" ICLR'20\n    [3] Y. M. Asano et al., ``Labelling unlabelled videos from scratch with multi-modal self-supervision,\" NeurIPS'20\n    [4] J. Duan et al., ``Multi-modal alignment using representation codebook,\" CVPR'22\n\n**[W3] Ablation for the loss function**\n\nOur image captioning objective is basically defined as the next-word generation prediction, i.e., generating a word from the previous information.\nInstead, we provide the results corresponding to the additional objectives for cross-modal alignment in the general responses.\nPlease refer to them."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9396/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700484764439,
                "cdate": 1700484764439,
                "tmdate": 1700484764439,
                "mdate": 1700484764439,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nTPK5tXISu",
                "forum": "lK2V2E2MNv",
                "replyto": "NE4BzEScxi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9396/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9396/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank you for your time and effort in reviewing our paper. Your insights have been highly valued, and your feedback is crucial to our progress. We understand that you have a busy schedule, and we again appreciate your time and attention to this matter. We have addressed all of your comments in our rebuttal. We would be grateful if you could please take a look at our rebuttal. If there are any additional materials or information you may require to facilitate the review process, please let us know. Your prompt attention to this matter is greatly appreciated.\n\nThank you again for your help. Sincerely Authors"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9396/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700577218579,
                "cdate": 1700577218579,
                "tmdate": 1700577218579,
                "mdate": 1700577218579,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JeceYmaBb0",
                "forum": "lK2V2E2MNv",
                "replyto": "NE4BzEScxi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9396/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9396/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer 5A8P,\n\nThanks for your constructive and valuable comments on our paper. We have revised our paper to address the reviewer's concerns.\n* **[W1, W2] Ablation regarding optimal transport.** We added ablation analyses for the components of VLAP, including the hyperparameters and optimal transport in Appendix Section D.\n\n* **[W3] Ablation regarding learning objectives.** We added ablation analyses with the additional learning objectives in Appendix Section D.\n\nWith the discussion deadline approaching in a few hours, we are eager to receive your feedback."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9396/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700722197448,
                "cdate": 1700722197448,
                "tmdate": 1700722902725,
                "mdate": 1700722902725,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Y3CIkpDJh7",
            "forum": "lK2V2E2MNv",
            "replyto": "lK2V2E2MNv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9396/Reviewer_oDtB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9396/Reviewer_oDtB"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposed to bridge the vision and language modalities by predicting the assignment between LLM word embeddings and those two modalities. Specifically, the optimal transport is employed to decide the assignment between LLM word embeddings and image/caption contextualized embeddings, and then the model is required to predict the assignment of one modality from the other modality. Experiments are conducted on multiple tasks/datasets to prove the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Demanding one modality's representation to predict the assignment between the other modality and common feature space (LLM word embedding) is an interesting idea to bridge two modalities. \n2. Evaluations on different tasks show a better performance than previous work."
                },
                "weaknesses": {
                    "value": "1. Comprehensive ablation of w/ and wo/ assignment prediction on the same vision/language backbones is missing. \n2. Comparison with other baselines that are designed for alignment is missing. For example, contrastive alignment in ALBEF, BLIP, and the first-stage alignment by BLIP2 which includes image-text matching, and image-grounded text generation.\n3. In experiments, the pre-training data is CC3M which is too small in terms of scale. Whether this method can be generalized to larger scale is not validated.\n4. In Tab1,2,3, when compared with previous works, the vision/language backbone is always different. I wonder if using the same backbones as previous works, will the proposed method still outperform them?"
                },
                "questions": {
                    "value": "1. Does the LLM word embedding have to be from the same LLM as used in language encoding?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9396/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698813431322,
            "cdate": 1698813431322,
            "tmdate": 1699637184517,
            "mdate": 1699637184517,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "O4Q5GrPjOq",
                "forum": "lK2V2E2MNv",
                "replyto": "Y3CIkpDJh7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9396/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9396/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your constructive review and valuable suggestions! Below, we provide a detailed response to your questions and comments. If any of our responses fail to sufficiently address your concerns, please inform us, and we will promptly follow up.\n\n**[W1] Ablation for assignment prediction**\n\nWe present the ablation results corresponding to the balancing parameters in equation (7) in the general response. Please refer to that section for more details.\nThe result without the assignment prediction objective corresponds to $\\lambda\\_\\text{map}:\\lambda\\_\\text{cap}=0:1$.\nTo briefly sum up, the assignment prediction objective combined with $\\mathcal{L}\\_\\text{cap}$ improves the performance.\n\n**[W2] VLAP with other cross-modal alignment**\n\nThanks for the constructive suggestion.\nWe present the results with the additional objectives, including the image-text matching (ITM) and image-text contrastive (ITC) objectives, in the general response.\nBriefly, those two objectives show less performance improvement than our proposed assignment prediction.\n\n**[W3] Scale of the pretraining dataset**\n\nThank you for your valuable suggestion.\nAs previous works for pretraining VLMs have proven, a larger pretraining dataset should bring better results on zero-shot vision-language tasks.\nTo verify the impact of the scale of the training dataset on zero-shot performance, we will train VLAP with a large-scale dataset (e.g. CC12M) and will include results in the revised paper.\nHowever, we would like to emphasize that our method mainly focuses on presenting an efficient methodology (i.e., linear transformation, using pretrained encoders, small-scale training) to make VLMs.\n\n**[W4] VLAP with other backbone**\n\n| Method | Vis. Encoder  | Lang. Model | NoCaps-In | NoCaps-Out | NoCaps-Near | NoCaps-All | NoCaps-CLIP-S | NoCaps-Ref-S | MSCOCO-CIDEr-D | MSCOCO-CLIP-S | MSCOCO-Ref-S |\n|--------|---------------|-------------|-----------|------------|-------------|------------|---------------|--------------|----------------|---------------|--------------|\n| MAGMA  | CLIP RN50x16  | GPT-J       | 30.4      | 43.4       | 36.7        | 38.7       | 74.3          | 78.7         | 47.5           | 75.3          | 79.6         |\n| LiMBeR | CLIP RN50x16  | GPT-J       | 34.3      | 48.4       | 41.6        | 43.9       | 74.7          | 79.4         | 54.9           | 76.2          | 80.4         |\n| VLAP   | CLIP ViT-B/32 | OPT         | 48.2      | 62.7       | 59.3        | 61.3       | 84.8          | 88.5         | 69.9           | 86.7          | 91.8         |\n| VLAP   | CLIP ViT-B/32 | T5          | 48.3      | 62.7       | 59.6        | 61.6       | 85.1          | 88.7         | 69.4           | 87.6          | 92.0         |\n| VLAP   | CLIP RN50x16  | GPT-J       | 53.8      | 67.5       | 65.7        | 64.5       | 88.3          | 90.1         | 75.3           | 90.6          | 72.2         |\n\nWhile we used the transformer-based vision models (i.e., CLIP ViT-B/32 and BEiT) and the relatively small scale of LLMs (i.e., OPT$\\_\\text{1.3B}$ and T5$\\_\\text{Base}$) in the main paper, VLAP can be applied to all publicly available vision models and LLMs.\nFollowing MAGMA and LiMBeR, we additionally evaluate VLAP with the CNNs-based vision model (i.e., CLIP RN50x16) and the larger LLM (i.e., GPT-J, which has 6B parameters) for zero-shot image captioning on NoCaps and MSCOCO.\nNot surprisingly, VLAP with the larger LLM achieves better performance than with the smaller LLMs, significantly outperforming MAGMA and LiMBeR by large margins across all metrics with the same backbones.\nWe add the results to Table. 9 in Appendix.\n\n**[Q1] Consistency of the word embedding in LLMs**\n\nWhile word embedding of any LLMs can be used, we exploit the word embedding within the LLM used for language encoding for two main reasons:\n(1) LLMs use word embeddings as a basis to produce text representations, indicating text representations already have high correlations with word embeddings.\nTherefore, the consistency of LLM for word embedding and language encoding makes the assignment for language data confident.\n(2) The inconsistency of LLMs for word embedding and language encoding requires unnecessary memory (e.g. OPT$\\_\\text{1.3B}$ has about 100M parameters for the word embedding)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9396/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700484293685,
                "cdate": 1700484293685,
                "tmdate": 1700484293685,
                "mdate": 1700484293685,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QLkhqPrUXi",
                "forum": "lK2V2E2MNv",
                "replyto": "Y3CIkpDJh7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9396/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9396/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank you for your time and effort in reviewing our paper. Your insights have been highly valued, and your feedback is crucial to our progress. We understand that you have a busy schedule, and we again appreciate your time and attention to this matter. We have addressed all of your comments in our rebuttal. We would be grateful if you could please take a look at our rebuttal. If there are any additional materials or information you may require to facilitate the review process, please let us know. Your prompt attention to this matter is greatly appreciated.\n\nThank you again for your help. Sincerely Authors"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9396/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700577204788,
                "cdate": 1700577204788,
                "tmdate": 1700577204788,
                "mdate": 1700577204788,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nR93uAPTEr",
                "forum": "lK2V2E2MNv",
                "replyto": "Y3CIkpDJh7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9396/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9396/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer oDtB,\n\nThanks for your constructive and valuable comments on our paper. We have revised our paper to address the reviewer's concerns.\n* **[W1, W2] Ablation regarding objectives.** We added ablation analyses for the learning objectives, including the hyperparameters and additional objectives in Appendix Section D.\n\n* **[W4] VLAP with other backbone.** We added the results of VLAP with CLIP RN50x16 and GPT-J, which are the same backbone as the baselinse, MAGMA and LiMBeR in Appendix Section D.\n\nWith the discussion deadline approaching in a few hours, we are eager to receive your feedback."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9396/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700722011442,
                "cdate": 1700722011442,
                "tmdate": 1700722931461,
                "mdate": 1700722931461,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uChrhO6LmP",
                "forum": "lK2V2E2MNv",
                "replyto": "Y3CIkpDJh7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9396/Reviewer_oDtB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9396/Reviewer_oDtB"
                ],
                "content": {
                    "comment": {
                        "value": "I have read the author's response and other reviewer's reviews. I appreciate the author's efforts and devotion to solving our concerns. Most of my concerns are solved. However...\n\nI still think it's necessary to experiment with the larger dataset to see how this method performs with the increase in data volume. Many methods, especially for efficient training, don't show performance improvement with larger data scales. These works have merit and contribution in terms of efficient training and fast learning in small/modest data, but it's also important to demonstrate the relationship between performance gain and data scale. Not generalizing to a larger data scale is not a weakness, but we have to let the audience know in which cases/scales our method shines and fails."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9396/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700724496012,
                "cdate": 1700724496012,
                "tmdate": 1700724496012,
                "mdate": 1700724496012,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Vjq2iwj6Mu",
            "forum": "lK2V2E2MNv",
            "replyto": "lK2V2E2MNv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9396/Reviewer_m8gW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9396/Reviewer_m8gW"
            ],
            "content": {
                "summary": {
                    "value": "The paper aims to align the LLMs (encoder/decoder or just decoder) with image encoders such that the LLMs can comprehend visual input better. It further restricts the design space to freeze the original LLM and visual encoder, just relying on a cheap learned linear transformation. To adapt such a transformation, the paper presents two learning objectives -- assignment prediction and image captioning. Empirical results are presented on 3 different tasks -- image captioning, VQA and cross-modal retrieval (I2T, T2I)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The problem is well motivated with wide applications.\n\n* The paper is mostly well written and explained.\n\n* The empirical results show a big delta which demonstrates the effectiveness of the approach. The studies are also conducted on wide range of problem settings."
                },
                "weaknesses": {
                    "value": "* The motivation for restricting the learned parameter space to just linear layers is unclear -- it would have been more interesting to see more analysis around different learned parameter space including non-linear layers."
                },
                "questions": {
                    "value": "-- Can the authors show ablation studies for the L_map and L_cap objectives to develop better understanding of each component?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9396/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698820125031,
            "cdate": 1698820125031,
            "tmdate": 1699637184404,
            "mdate": 1699637184404,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qKf5bzawfq",
                "forum": "lK2V2E2MNv",
                "replyto": "Vjq2iwj6Mu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9396/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9396/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your constructive review and valuable suggestions! Below, we provide a detailed response to your questions and comments. If any of our responses fail to sufficiently address your concerns, please inform us, and we will promptly follow up.\n\n**[W1] VLAP with non-linear space**\n\nWe would like to emphasize that our method diverges from traditional objectives in linear transformation-based approaches by introducing an alternative objective that avoids direct representation comparisons.\nIn addition, although our method dramatically boosted the performance of VLMs based on linear transformation, but nothing is specifically designed for this purpose only.\nAs shown in the general response, our assignment prediction objective outperforms other objectives used in the modular-based methods.\nThe results suggest that the proposed assignment prediction will contribute to the modular-based methods.\nTo address the reviewer's suggestion, we are conducting additional experiments for the non-linear space, such as multiple linear layers with activation functions and an adapter, which is a trainable bottleneck module [1].\nWe will include the analysis in the revised paper.\n\n    [1] J. He et al., ``Towards a unified view of parameter-efficient transfer learning,\" ICLR'22\n\n\n**[Q1] Ablation for the balancing parameters**\n\nThanks for the question. We present the ablation results corresponding to the balancing parameters in equation (7) in the general response.\nPlease refer to them."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9396/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700483461838,
                "cdate": 1700483461838,
                "tmdate": 1700483461838,
                "mdate": 1700483461838,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xx0Lm8slsm",
                "forum": "lK2V2E2MNv",
                "replyto": "Vjq2iwj6Mu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9396/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9396/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**[W1] VLAP with non-linear space**\n\n|      |                      |                                                   |                |         | MSCOCO |       |\n|------|:--------------------:|:-------------------------------------------------:|:--------------:|:-------:|:------:|:-----:|\n|      |        Method        |                     Objective                     | Train. Params. | CIDEr-D | CLIP-S | Ref-S |\n| (i)  | VLAP w/non-linearity | $\\mathcal{L}\\_\\text{cap}$                         | 1.8M           | 55.6    | 77.1   | 80.5  |\n| (ii) | VLAP w/non-linearity | $\\mathcal{L}\\_\\text{cap}+\\mathcal{L}\\_\\text{map}$ | 1.8M           | 71.1    | 88.9   | 92.4  |\n|  (iii)    | VLAP                 | $\\mathcal{L}\\_\\text{cap}+\\mathcal{L}\\_\\text{map}$ | 0.6M           | 69.4    | 87.6   | 92.0  |\n\nTo address the reviewer's suggestion, we conduct additional experiments to evaluate VLAP with the simplest non-linear mapping layers, i.e., three linear layers with the ReLU activation function, as shown in the above table.\nNote that CLIP ViT-B/32 and T5$\\_\\text{Base}$ are used as the vision model and LLM, respectively.\nThe comparison between (i) and (iii) shows that VLAP with the original setting outperforms VLAP with the linear layers, demonstrating that the learning objective has a greater impact on overall performance than the non-linearity.\nMeanwhile, the comparison between (ii) and (iii) shows that the performance can be slightly improved with the non-linear layers when the learning objective is fixed.\nAlthough we only presented results with the simplest form of non-linear modules, the results suggest that VLAP will show further improved performance when combined with sophisticated modular-based approaches (e.g. Q-Former in BLIP-2)."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9396/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700589243778,
                "cdate": 1700589243778,
                "tmdate": 1700625710869,
                "mdate": 1700625710869,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "obmtVYXpJg",
                "forum": "lK2V2E2MNv",
                "replyto": "Vjq2iwj6Mu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9396/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9396/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**[W1] VLAP with non-linear space (update)**\n\n|      |                      |                                                   |                |         | MSCOCO |       |\n|------|:--------------------:|:-------------------------------------------------:|:--------------:|:-------:|:------:|:-----:|\n|      |        Method        |                     Objective                     | Train. Params. | CIDEr-D | CLIP-S | Ref-S |\n| (i)  | VLAP w/linear layers | $\\mathcal{L}\\_\\text{cap}$                         | 1.8M           | 55.6    | 77.1   | 80.5  |\n| (ii) | VLAP w/linear layers | $\\mathcal{L}\\_\\text{cap}+\\mathcal{L}\\_\\text{map}$ | 1.8M           | 71.1    | 88.9   | 92.4  |\n| (iii) | VLAP w/adapter | $\\mathcal{L}\\_\\text{cap}+\\mathcal{L}\\_\\text{map}$ | 0.4M           | 69.7    | 87.9   | 92.1  |\n|  (iv)    | VLAP                 | $\\mathcal{L}\\_\\text{cap}+\\mathcal{L}\\_\\text{map}$ | 0.6M           | 69.4    | 87.6   | 92.0  |\n\nWe update the results of VLAP with the adapter.\nWe use a single adapter, which has the dimension of the bottleneck 256 and the ReLU activation between down-/up-projection layers.\nSimilar to the comparison between (ii) and (iv), the comparison between (iii) and (iv) shows that the non-linearity improves the performance despite the smaller number of parameters (0.4M vs 0.6M).\nThis result consistently suggests that VLAP will show further improved performance when combined with sophisticated modular-based approaches (e.g. Q-Former in BLIP-2)."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9396/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740440527,
                "cdate": 1700740440527,
                "tmdate": 1700740454651,
                "mdate": 1700740454651,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]