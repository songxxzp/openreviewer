[
    {
        "title": "Parameter Estimation of Long Memory Stochastic Processes with Deep Neural Networks"
    },
    {
        "review": {
            "id": "6Ahxf9TecQ",
            "forum": "lLhEQWQYtb",
            "replyto": "lLhEQWQYtb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8119/Reviewer_H3eJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8119/Reviewer_H3eJ"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the problem of estimating parameters of long stochastic processes with long-range dependencies. The proposed method generates high-quality synthetic training data to train neural networks that are able to capture the long-range dependencies in the data. The paper experimentally demonstrates the benefits of their method compared to a set of baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper studies a very relevant problem of learning long-range dependencies and estimating involved parameters. The key novelty I see in this work is the relation of the dependency in the data and the estimation of parameters to capture it."
                },
                "weaknesses": {
                    "value": "- The paper misses significant breakthroughs in the domain of learning long-term dependencies using neural networks. In particular, [1,2,3,4] pushed the boundary on the sequence length of data that can be learned by an ML model. It is important to see how the proposed methods work compared to and in conjunction with these approaches.\n- The writing of the paper can be improved. In particular, it is unclear what is new and what is just standard training methods. I think the paper would be much stronger if the contributions and relations to existing methods were stated more clearly.\n\n**References**. \n[1] Gu et al. 2021, Efficiently Modeling Long Sequences with Structured State Spaces.  \n[2] Rusch et al. 2021, Unicornn: A recurrent model for learning very long time dependencies.  \n[3] Morrill et al. 2021, Neural Rough Differential Equations for Long Time Series   \n[4] Rusch et al. 2022, Long expressive memory for sequence modeling."
                },
                "questions": {
                    "value": "Can't the stochastic process parameters in the paper much better be characterized using the log-signature? (e.g. see Morrill et al. 2021)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8119/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698698441570,
            "cdate": 1698698441570,
            "tmdate": 1699637006040,
            "mdate": 1699637006040,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YHCoHwI8F5",
                "forum": "lLhEQWQYtb",
                "replyto": "6Ahxf9TecQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8119/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8119/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for the review. We would like to make the following observations.\n\n> The paper misses significant breakthroughs in the domain of learning long-term dependencies using neural networks. In particular, [1,2,3,4] pushed the boundary on the sequence length of data that can be learned by an ML model. It is important to see how the proposed methods work compared to and in conjunction with these approaches.\n\nIn our opinion, the problem of parameter estimation is not the same as the (general) task of modeling long sequences. While in the latter case, the proposed articles are clearly relevant, for the problem studied in our article, neural networks processing the raw sequences can produce similar or better results. Nevertheless, in the introduction the paper Kidger et al. (2019) is also cited. In practice, the usable sequence length ranges from a few hundred to a few thousand, e.g. due to the frequently occurring time dependence of the parameters.\n\n> The writing of the paper can be improved. In particular, it is unclear what is new and what is just standard training methods.\n\nThe new insight provided by the article is the realization that sequence processing neural architectures trained on a sufficient amount of data, can in itself replace the role of even sophisticated statistical descriptors in determining the memory parameters characterizing processes, somewhat similarly to the role of convolutional networks in image processing. This is formulated in the Introduction and in full concreteness in the Conclusion. The main part of the measurements is to argue on the power of these latent calculations leading to the parameters.\n\n> Can't the stochastic process parameters in the paper much better be characterized using the log-signature? (e.g. see Morrill et al. 2021)\n\nThe advantage of signature-based approaches comes in the case of multi-dimensional series. In our case, the input data is one-dimensional, and the relations of the stochastic series are considered to be independent of each other.\nThus, the use of the signature will not give you more than the use of the spectrum domain, while it has a fairly significant calculation requirement. On the one hand, we carried out specific measurements according to Kidger et al. (2019), and we also took the raw Fourier spectrogram, which was then processed with MLP or LSTM. These measurements also show that it is not possible to achieve a better result than the sequence processing neural networks working on raw sequences."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8119/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700736714932,
                "cdate": 1700736714932,
                "tmdate": 1700736714932,
                "mdate": 1700736714932,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "b31ELdOWZl",
            "forum": "lLhEQWQYtb",
            "replyto": "lLhEQWQYtb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8119/Reviewer_Rt5S"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8119/Reviewer_Rt5S"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a method for fitting fractional Brownian motion using neural network sequence models. They test the method by creating many simulated datasets of varying lengths and assessing the each model's quality of fit with MSE on the observation, analysis of bias and deviation, recovery of the Hurst parameter."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The results seem to suggest that this method does in fact work better than traditional alternatives to model fitting on fBms and by a significant margin."
                },
                "weaknesses": {
                    "value": "I'm not an expert in this domain, but it was very challenging to tell what was actually being proposed in this paper. \n\nMy first interpretation was that the parameters of the fBm were parameterized and then a differentiable integration method was applied to the fBM to get sample paths which were compared with ground truth paths using MSE. Then the parameters could be found by backpropagating through the integration to the parameters. But naively this would not require a sequence model, which is describe to take in a sequence and output a single scalar through mean aggregation. It's not obvious to me what the neural network is actually being used for. Is it also being used to integrate the process in some way? The presentation could be improved significantly with appropriate explanatory figures or at least a description of the actual training loss and simulation procedure."
                },
                "questions": {
                    "value": "Is it possible the baselines are too weak? They seem to be *significantly* worse than a straightforward application of sequence models. It's a little hard to believe there aren't other deep learning methods that could be used as baselines here. There is has been substantial work on learning SDEs (e.g. Patrick Kidger's work) and other stochastic processes with neural networks (e.g. neural diffusion processes) and those methods might be applicable here. \n\nWhat is the intended use case for this model? In the paper it states that the setup implies there is infinite data. I see how this is true when fitting models to synthetically generated data, but it's almost certainly not true when attempting to fit real world data. If this is meant to be used in the types of financial or scientific applications described in the introduction to the paper, why not apply it directly to those applications and evaluate how fell it performs in terms of MSE?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8119/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698802979137,
            "cdate": 1698802979137,
            "tmdate": 1699637005833,
            "mdate": 1699637005833,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "F8sp3JEBWt",
                "forum": "lLhEQWQYtb",
                "replyto": "b31ELdOWZl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8119/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8119/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for the review. We add to the summary that a large part of the paper reflects on fBm, however, two other important processes, ARFIMA and fOU, are also in focus in the article. We are happy to read the review\u2019s mentioning that the selected benchmarks were significantly exceeded by our approach.\n\n> My first interpretation was that the parameters of the fBm were parameterized and then a differentiable integration method was applied to the fBM to get sample paths which were compared with ground truth paths using MSE. Then the parameters could be found by backpropagating through the integration to the parameters. But naively this would not require a sequence model, which is describe to take in a sequence and output a single scalar through mean aggregation. It's not obvious to me what the neural network is actually being used for. Is it also being used to integrate the process in some way? The presentation could be improved significantly with appropriate explanatory figures or at least a description of the actual training loss and simulation procedure.\n\nRegarding the content of the article, it is advisable to differentiate between\n(1) modeling long sequences, (2) modeling and forecasting processes with a long memory, (3) measuring how much memory a short or long sequence has, also covering the case when the sequence is stochastic.\nIn the paper, we dealt with the third case, where we chose three stochastic processes used in practice and their equidistant realizations as models. The parameters characterizing the memory (or fractal dimension) are estimated directly from the raw sequence data, using an LSTM architecture. We feel that the self-standing use of the neural network for this task is novel, especially in light of the achieved achievements. It is also new to cover the scaling problem of the processes.\n\n> Is it possible the baselines are too weak? They seem to be significantly worse than a straightforward application of sequence models. It's a little hard to believe there aren't other deep learning methods that could be used as baselines here. There is has been substantial work on learning SDEs (e.g. Patrick Kidger's work) and other stochastic processes with neural networks (e.g. neural diffusion processes) and those methods might be applicable here.\n\nIn our opinion, the baselines have been established carefully, based on the literature and very broad practice. As mentioned in the introduction, we looked at several (more or less well-described and supported) methods combining neural network methods with statistical descriptors. Among them was Kidger et al. (2019) signatures method. We found that the use of predefined statistical descriptors in the parameter estimation task could be offset by teaching a large amount of data.\n\n> What is the intended use case for this model? In the paper it states that the setup implies there is infinite data. I see how this is true when fitting models to synthetically generated data, but it's almost certainly not true when attempting to fit real world data. If this is meant to be used in the types of financial or scientific applications described in the introduction to the paper, why not apply it directly to those applications and evaluate how fell it performs in terms of MSE?\n\nThe reviewer's reasonable question is why it is sufficient to create a model for the class of stochastic processes often used in modeling real processes. When real sequential data is inferred by these models, the data is fit to the closest element of the chosen stochastic class measured in MLE. If we want to produce a model with another stochastic class, we train a neural network for the parameters of this next class. This is what happens in the case of the ARFIMA and fOU processes presented in the article. In this way, we avoid teaching on real data limited by the smaller volume of data and such phenomena as e.g. the Hurst parameter changes (in time) through a real data series. This also makes the evaluation on real series relative. Nevertheless, we have some measurements when we infer with our Hurst estimation model on real data series, and the obtained outputs are consistent with other results, e.g. with baseline methods.\n\nTeaching on independent records of large volume is an important factor. It requires a sufficiently efficient process generator, which produces an unlimited amount of usable teaching data, which we call infinite, with some exaggeration."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8119/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700736570451,
                "cdate": 1700736570451,
                "tmdate": 1700736570451,
                "mdate": 1700736570451,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "kQcZ4y5Rhc",
            "forum": "lLhEQWQYtb",
            "replyto": "lLhEQWQYtb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8119/Reviewer_pWyU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8119/Reviewer_pWyU"
            ],
            "content": {
                "summary": {
                    "value": "Fractional Brownian motion, Autoregressive Fractionally Integrated Moving Average and the fractional Ornstein-Uhlenbeck process are often used in real world. They are governed by the Hurst or differencing parameter which this paper estimates using neural networks. Having generated many different samples the network learns to output the true underlying parameter."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The paper is easy to follow. The baselines established in 2.4 are reasonable. The approach is simple but it is well motivated."
                },
                "weaknesses": {
                    "value": "The scope of the problem is very limited. Perhaps showing that this approach scales to different equations at once, or having a *foundation* model for symbolic regression.\n\nThe results in Figure 1 (right) are not that impressive. An interesting contribution would be to have a model that scales from short sequences to large ones.\n\nI believe that the novelty, significance and the results are not enough for this conference."
                },
                "questions": {
                    "value": "."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8119/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698860302870,
            "cdate": 1698860302870,
            "tmdate": 1699637005441,
            "mdate": 1699637005441,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1TWv5TGKry",
                "forum": "lLhEQWQYtb",
                "replyto": "kQcZ4y5Rhc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8119/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8119/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for the review. We appreciate the reviewer\u2019s feedback on the correctness of the methodology and baseline selection.\n\n> The scope of the problem is very limited. Perhaps showing that this approach scales to different equations at once, or having a foundation model for symbolic regression.\n\nWe agree that many interesting questions beyond the results of this paper can be studied from the topic of parameter estimation of fractional processes with neural networks. We provide several examples of that in the Appendix of the article. Nevertheless, we believe that it was important to demonstrate the pure neural network parameter estimation approach in such a way that it can be used in practice, is easy to overview, and thus appears as a method accessible to many.\n\nThank you for the idea of a neural estimation method that works accurately for multiple processes at the same time, which is an exciting suggestion. In many respects, the stress tests described in Appendix D can be considered steps in this direction. The connection to symbolic regression is indeed natural, but limiting the computing power of neural networks for the sake of transparent calculations can lead to a significant loss of accuracy. In this article, we clearly wanted to focus on the performance of the methods.\n\n> The results in Figure 1 (right) are not that impressive. An interesting contribution would be to have a model that scales from short sequences to large ones.\n\nAs for Figure 1 (right), we cannot use short series as input for the parameter estimation beyond a certain level of accuracy (see e.g. Weron 2002, Physica A 312). On the other hand, incremental learning with increasingly long sequences preserves the ability to estimate with shorter sequences."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8119/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700736300890,
                "cdate": 1700736300890,
                "tmdate": 1700736300890,
                "mdate": 1700736300890,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cbd1cH6bPG",
            "forum": "lLhEQWQYtb",
            "replyto": "lLhEQWQYtb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8119/Reviewer_4CiJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8119/Reviewer_4CiJ"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces the use of efficient process generators to estimate the long range parameters of stochastic process models via a purely deep neural network approach that does not use conventional statistical methods. Background information on these time series that exhibit long range dependence is provided and some experimental results are supplied to validate the approach."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The main strengths of this paper are quite limited and therefore, I have spent more time in highlighting the weaknesses and shortcomings of this paper."
                },
                "weaknesses": {
                    "value": "The major novel contribution of this work is not clearly specified at all. The paper has the following weaknesses:\n\n1) This work seems to be an application of available process generators to generate high quality synthetic data for the fBm and other long range stochastic processes to train standard neural network models and evaluate their performance. This contribution is not sufficient for an ICLR paper. \n\n2) The paper reads like a background on stochastic processes exhibiting long range dependence and a small section devoted to the actual tasks implemented by the authors. It does not make for good reading."
                },
                "questions": {
                    "value": "The line of work is interesting, and I urge the authors to undertake more detailed theoretical analysis and possibly even some guarantees on estimating these long range memory parameters using recurrent neural networks and other architectures."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8119/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699585754858,
            "cdate": 1699585754858,
            "tmdate": 1699637005325,
            "mdate": 1699637005325,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UUcdjaLEcy",
                "forum": "lLhEQWQYtb",
                "replyto": "cbd1cH6bPG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8119/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8119/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for the review. We would like to make the following comments.\n\n> The major novel contribution of this work is not clearly specified at all.\n\nThe new insight provided by the article is the realization that sequence processing neural architectures, trained on a sufficient amount of data, can in itself replace even sophisticated statistical descriptors in determining the memory parameters characterizing processes, somewhat similarly to the role of convolutional networks in image processing. This is formulated in the Introduction and, in full concreteness, in the Conclusion. The main part of the measurements is to argue on the power of these latent calculations leading to the parameters.\n\n> This work seems to be an application of available process generators to generate high quality synthetic data for the fBm and other long range stochastic processes to train standard neural network models and evaluate their performance.\n\n> The paper reads like a background on stochastic processes exhibiting long range dependence and a small section devoted to the actual tasks implemented by the authors.\n\nOur result does indeed operate with existing tools, but it shows many previously unknown or unachieved insights. Obviously, the most important thing is the accuracy and speed of parameter extraction; the superior nature of our method is discussed in detail in Section 4.\nThe question also arises whether it is possible to show better performance if we do not work directly from the raw data of realization but rather extract some complex features, e.g. statistical descriptors, and use them as input for a neural network. In this regard, we have carried out several measurements, and neither the use of spectrum domain nor the signatures give better performance while they require significantly more computing resources.\nWe note that the codes used for measurements will be available, which also indicates that the paper is more than a compilation of off-the-shelf methods.\n\n> The line of work is interesting, and I urge the authors to undertake more detailed theoretical analysis and possibly even some guarantees on estimating these long range memory parameters using recurrent neural networks and other architectures.\n\nIn response to the formulated question, we are happy to have received confirmation regarding the interest in the topic. Motivated by the suggestion, we are thinking about which mathematical-statistical methods could be used to achieve analytical results, e.g. lower estimate of achievable accuracy.\n\nFinally, we would like to remark that the title of the article draws attention to long-memory processes, however, the article is also about short-memory or rough processes (e.g. fBm with Hurst near 0)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8119/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700736183828,
                "cdate": 1700736183828,
                "tmdate": 1700736183828,
                "mdate": 1700736183828,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]