[
    {
        "title": "Exploring Unified Perspective For Fast Shapley Value Estimation"
    },
    {
        "review": {
            "id": "2i9dlupRTy",
            "forum": "CNZmaInj9n",
            "replyto": "CNZmaInj9n",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission173/Reviewer_UWrD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission173/Reviewer_UWrD"
            ],
            "content": {
                "summary": {
                    "value": "This work considers the topic of efficient Shapley value estimation and presents two contributions. The first is that several current Shapley value estimators can be understood via a shared perspective of a linear transformation of expectations of feature subsets. The second is that amortized Shapley value estimation can be viewed via comparison to the true Shapley values in a chosen metric space, which enables the development of a new approach, SimSHAP. Under this method, we simply calculate the similarity to estimated Shapley values, and this is competitive with an existing amortized method (FastSHAP)."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Efficient Shapley value estimation is an interesting topic that is in need of improved methods, particularly ones that leverage amortization to provide faster solutions than stochastic estimators. This work provides a new perspective on existing stochastic estimators, although it does appear to derive any practical insights or algorithms from this perspective. It then provides another new perspective on amortized methods, and the authors use this to propose SimSHAP. SimSHAP is simple and competitive with FastSHAP."
                },
                "weaknesses": {
                    "value": "About the main methodological contributions:\n\n- The main contribution of this paper is SimSHAP, which involves training a Shapley value estimation model by comparing to estimated values for each gradient step. While there is a lot of build-up to this proposal, this idea has been explored by at least two other works [1, 2], so the novelty is limited. These works are not cited, and I'm not sure there's much methodological innovation on top of them.\n\n- Regarding the unified perspective in Section 2.3, it seems like this view is clearly apparent for the semivalue-based estimator and does not add much here. After reading the paper closely, I'm not sure it's correct for the least squares estimator. Previous work by Covert & Lee (2021) showed a similar derivation from the KKT conditions, and the analytic solution involves two terms that are estimated: one that resembles the importance sampling term, and one that resembles the transform term (see their section 3.2). In this work, it looks like the transform term $T$ is assumed to be known, which does not correspond to KernelSHAP but rather an alternative version from that work called \"unbiased KernelSHAP.\" Are the authors perhaps analyzing that version rather than the original one? Note that the original one cannot be calculated without a sufficient number of samples due to invertibility issues (the authors here manage to use a very small number in their experiments, suggesting the unbiased version), and that the original one is not provably unbiased (whereas the authors claim zero bias here in eq. 14).\n\n- Regarding the unified view of amortized estimators, it is true that you can compare to the true Shapley values with any properly defined metric and learn the correct function. It is therefore not surprising that you can use the l2 distance. The authors might have been more careful about showing why it's okay to train on inexact values from the least squares estimator, because these training targets will be very noisy. What is the impact of this noise during training? Also, note again that [1, 2] have already explored this approach.\n\n- Regarding the unified view of amortized estimators, I'm not sure the proposed view of FastSHAP is correct. For the unnumbered equation defining $\\mathcal{L}(\\theta)$ in Section 2.3, there are two equations and I'm not able to see whether they are mathematically equivalent. The authors should consider adding this derivation, either here or in the appendix. Furthermore, the expression for $\\phi_x$ is not equal to the Shapley values, because it isn't the solution from the KKT conditions and doesn't incorporate the constraint. Currently, I do not see how this perspective is correct, and it doesn't seem helpful to view the FastSHAP loss as comparing to the true values under a complicated metric: it misses the main point of the FastSHAP loss, which is that it bypasses the need for the ground truth. This perspective is also not necessary to understand that comparing to the ground truth using l2 distance should work.\n\n- Related to the above point, I'm not sure the entries for FastSHAP in Table 2 are correct.\n\n[1] Schwarzenberg et al, \"Efficient Explanations from Empirical Explainers\" (2021)\n\n[2] Chuang et al, \"CoRTX: Contrastive Framework for Real-time Explanation\" (2023)\n\nAbout the experiments:\n\n- In Figure 2, SimSHAP does not appear to outperform FastSHAP. In fact it's noticeably worse for the census dataset. Why is that, and can it be made more accurate?\n\n- The results in Table 3 appear to contradict those in Figure 4. How can SimSHAP have the best AUC when its curves are far from the best?\n\nAbout the writing, which could be significantly improved:\n\n- The description of related work is incorrect in several places. In the introduction, \"semivalue\" and \"least squares value\" are not estimation methods, they are classes of game-theoretic solution concepts, and their mathematical perspectives are used to develop estimation methods (e.g., fitting a weighted least squares model with an approximate version of the least squares objective). Also in the introduction, model-specific methods do not \"reduce computation cost by merging or pruning redundant feature combinations,\" this is not true for TreeSHAP, LinearSHAP or DeepSHAP. It is also not true that \"the exact differences among these algorithms remain unclear,\" the differences seem very clear, as they are described in this work. The gap in missing work seems to be that the fastest solutions either are not general enough (TreeSHAP) or accurate enough (FastSHAP), although this work does not appear to solve the latter problem.\n\n- The \"large numeral law\" is not common terminology, it may be better to call this the \"law of large numbers\" as it is typically called (see [wikipedia](https://en.wikipedia.org/wiki/Law_of_large_numbers)). Also, the LLN does not imply an approximation error of $\\mathcal{O}(1/\\sqrt{M})$, it only implies convergence of the sample mean to the expectation. For the error rate, it might be better to cite a probabilistic error bound like Chebyshev's inequality.\n\n- For proposition 1, it would be helpful to refer to a proof. Or even better, because this is a known result, the authors might refer to one of the works that originally showed it to be true, for example Charnes et al 1988.\n\n- In eq. 8 regarding FastSHAP, the authors' notation neglects to show the input $x$ in the cooperative game $v(S)$. It may be difficult for readers to follow what $v(S)$ represents. It would also help to give a specific example of how $v(S)$ is defined in Section 2.1, because no equation is currently provided.\n\n- Regarding the additive efficient normalization, there are parts where FastSHAP is described as \"biased\" or that its predictions need to be rectified. The current description could be confusing to many readers, because it suggests that the loss is somehow incorrect, but that's not quite true. For example, you could write that the weighted least squares loss does not encourage the correct optimal value unless the predictions satisfy the efficiency constraint, which can be enforced by applying the normalization to the predictions.\n\n- There are small typos throughout the paper, for example \"remarkbly\" on page 3 and \"minimizaion\" on page 4. The paper could be more thoroughly proofread. \n\n- Overall, the paper's structure is extremely similar to the FastSHAP paper, to the point that the authors have replicated all the main text experiments, many of the supplementary experiments, and have nearly copied some of the writing. It would be helpful to either acknowledge this similarity, or deviate from it more significantly."
                },
                "questions": {
                    "value": "Several questions are mentioned in the weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission173/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698779072579,
            "cdate": 1698779072579,
            "tmdate": 1699635942898,
            "mdate": 1699635942898,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CbR22EplhS",
                "forum": "CNZmaInj9n",
                "replyto": "2i9dlupRTy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission173/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission173/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer UWrD (part 1/2)"
                    },
                    "comment": {
                        "value": "We thank reviewer UWrD for all the helpful critiques and suggestions regarding our work. We have made targeted revisions and additions.\nFor the revisions of the paper, we have marked them in **green** in the PDF.\n\n## About the main methodological contributions\n\n**W1:**\n\nThanks for reviewer's suggestions, we have added two references to the revised version.\n- **About missing references:** We also noticed these works in our methodological survey, and we acknowledge that there have been some research efforts on amortized estimators in recent years, such as the FastSHAP method cited in our paper. \n- **About contributions:** We apologize for the unclear parts in the paper, but we still want to emphasize the main contributions of this paper. \nMany recent works have studied different estimation methods for approximating Shapley values, such as **semi-value** (ordinary Monte Carlo estimation) and **least-square value** (using the weighted least squares property of Shapley values). \nHowever, we found that these methods are **essentially not different**.\nWe find that the least-square value is essentially still equivalent to some special probability Monte Carlo estimator. \nTherefore, we propose a unified perspective of stochastic estimators and build a unified perspective of amortized estimators on this basis.\nWe have emphasized the main contributions of the paper in the revised version.\n\n**W2:**\n\nSorry for the confusion in the original paper. \n- **Least-square value & unified perspective:** Regarding whether the least-square value can be incorporated into our unified perspective, we have provided detailed analysis and rigorous proof in **Appendix A.1**.\n- **About unbiased kernelSHAP:** Thanks to the reviewer for pointing out that we did use the **unbiased kernelSHAP** in this work, and we have made it clearer in the revised version to eliminate any possible confusion.\n- **Proof of Eq(14):** Regarding the proof of the unbiasedness of the original formula 14, we have provided detailed derivations in **Appendix A.2** and ensured its unbiasedness.\n\n**W3:**\n\nThanks for the suggestion. \n- **About the fitting priority of information with different frequency:** We acknowledge the need to clarify how the model can be trained on noisy data. We have added the analysis in **Appendix A.4**. \n- **Supplementary exprimental results:** The model can ensure a fit to the true value if the training set is large enough. \nAs the reviewer pointed out, a small dataset can cause the model to overfit the noise and fail to capture the true value. We plan to research ways to learn the true value with limited data as our future direction. We have demonstrated the above statement with experiments in **Appendix A.7**. Furthermore, our sampling strategy guarantees convergence to the ground truth value when operating on sufficient amount of data. Together with Table 12 in **Appendix A.6**, we can ensure the feasibility of our algorithm.\n\n**W4:**\n\n- **About the theoretical derivation of Eq(13):** We are sorry to have deleted the theoretical derivation due to the page limit. We can ensure its rightness and have included the whole proof in **Appendix A.3**. \n- **About $\\phi_x$'s analysis:** In addition, we admit the inaccuracy of $\\phi_x$ to Shapley values. That's why we add the $\\frac{v(N) - v(\\phi)}{d}\\textbf{1}$ term to make it strictly equal to Shapley values, which is also a technical difference between **SimSHAP** and **FastSHAP**. \n  - **For FastSHAP:** FastSHAP utilizes normalization/penalty punishment terms in the loss to ensure correctness. \n  - **For SimSHAP:** We have found that the issue can be resolved by simply incorporating this term into the calculation of the sampled ground truth, allowing for the construction of SimSHAP without the need for normalization or penalty in the end.\n- **About the main point of FastSHAP:** We acknowledge its contribution by bypassing the need for the ground truth labels. Here we would like to reiterate our contribution in unifying and proposing SimSHAP: \n  - for stochastic estimator (Table 1), we demonstrated that the least squares value with KKT condition is equivalent to direct Monte-Carlo sampling on the classic form of Shapley values computation. This serves as the answer to why we didn't discover a significant difference in computational complexity (see **Appendix A.1**). \n  - What's more, We would emphasize that SimSHAP bypasses the need for ground truth Shapley values labels as well, but we choose the simplest metric and function as a result of our unified framework.\n\n**W5:**\n\nWe have addressed the questions above, primarily on W4."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission173/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700448363089,
                "cdate": 1700448363089,
                "tmdate": 1700448363089,
                "mdate": 1700448363089,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MV9ToPkfgQ",
                "forum": "CNZmaInj9n",
                "replyto": "2i9dlupRTy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission173/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission173/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer UWrD (part 2/2)"
                    },
                    "comment": {
                        "value": "## About the experiments\n\n**Q1:**\n\nThanks for the insightful suggestion. \n- The results in **Appendix A.6-7** demonstrate that significantly enlarging the sample size can notably enhance the model's fitting performance, but it also introduces additional computational burdens.\n- To maintain consistent training times compared to the baseline approach, the results depicted in **Figure 2** do not incorporate a large sample size. Future research endeavors will focus on exploring how to achieve superior fitting outcomes with smaller sample sizes, as outlined in **W3 in part 1**.\n\n## About the writing\n\nWe appreciate the reviewer's attention to detail and comprehensive understanding of our paper. \n\n**Q1:**\n\nWe admit our incorrectness in the \"Related Work\" section and \"Introduction\" section. \n- Regarding terms such as semivalue, we follow the terminology used in [1].\n- We have revised the paper and modified \"reduce computation cost by merging or pruning redundant feature combinations\" to \"introduce model-specific information\" in order to be more appropriate and accurate. \n- Moreover, we have to clarify \"the exact differences among these algorithms remain unclear\". Our motivation is that there are so many model-agnostic methods for Shapley values estimation and we sometimes don't know which method to choose for a specific scenario, so we turned to explore their differences in essence. We thus articulated this motivation in the new version of our paper.\n\n**Q2:**\n\nWe apologize for not double-checking the official term for \"law of large numbers\". In addition, We are referring to the convergence of the sample mean to the expectation by mentioning $O(\\frac{1}{\\sqrt{M}})$, and we think the reviewer's suggestion makes the statement more precise. \n\n**Q3:** \n\nWe have proofed **proposition 1** in **Appendix A.1**, and we also accept the advice to emphasize the result by citing the work the reviewer mentioned.\n\n**Q4:** \n\nWe apologize for any confusion in the notation system. \nWe have organized the symbols in the new version.\n\n**Q5:**\n\nThe reviewer is right as we did not intend to demonstrate the FastSHAP loss as a biased loss, but it needs to be normalized which corresponds to satisfying the efficiency constraint. It's also worth mentioning that we added one term instead of performing normalization for Shapley values correction in our algorithm, as stated in **W4** for weakness.\n\n**Q6-7:** \n\nWe acknowledge that we followed FastSHAP's experimental setup to provide a systematic evaluation of our method. \nWe have clearly stated this in the revised version of the paper and have also corrected all the typos pointed out by the reviewer, which has contributed to a more organized and precise presentation.\n\n### Reference:\n\n[1] Algorithms to estimate Shapley value feature attributions, Chen, Hugh and Covert, Ian C and Lundberg, Scott M and Lee, Su-In, Nature Machine Intelligence, 2023"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission173/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700453266820,
                "cdate": 1700453266820,
                "tmdate": 1700453266820,
                "mdate": 1700453266820,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "B5xLdiLqzo",
                "forum": "CNZmaInj9n",
                "replyto": "MV9ToPkfgQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission173/Reviewer_UWrD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission173/Reviewer_UWrD"
                ],
                "content": {
                    "title": {
                        "value": "Response (1/2)"
                    },
                    "comment": {
                        "value": "Thanks to the authors for their response, and I think the new edits to the paper are helpful. I believe this work is close to being publication-ready, but there are still some significant issues:\n\n**Prior works.** It's not sufficient to simply cite Schwarzenberg et al and Chuang et al, what they propose is basically identical to SimSHAP: an explainer model is trained by penalizing the error relative to a common approximation algorithm. The paper should acknowledge this similarity and discuss differences, if the authors believe there are any. If there are not, I believe the main contribution of this paper is to provide a new perspective on FastSHAP being a Mahalanobis distance with a complex weighting. Personally, I don't think this perspective is important to motivate SimSHAP, given that it was independently proposed by these earlier works.\n\n**Similarity of semivalue and least squares estimators.** Thanks for correcting the work and acknowledging that it uses unbiased KernelSHAP rather than the original version. I still think it's confusing to call this the \"least squares estimator\" throughout the paper: readers will associate the least squares perspective with KernelSHAP rather than the unbiased version from Covert & Lee, because the latter converges slower and is rarely used (see that work's experiments). Also, the result about it being unbiased is known from that work, so it does not need to be proven again here (Appendix A.2). Finally, the perspective that it's equivalent to a complicated Monte Carlo estimator is unfortunately also known: see Section 4.2 of \"SHAP-IQ: Unified Approximation of any-order Shapley Interactions.\"\n\n**FastSHAP's equivalence to eq. 13.** Thank you for including the proof in Appendix A.3, it's very helpful. However, there's one detail that isn't handled carefully: for the final expression to be equivalent to the FastSHAP loss, we require that $W$ contains $\\infty$ for the entries corresponding to the empty and full sets. This detail is handled a bit casually, it's not even mentioned. Is every equality in this analysis valid when $W$ has these $\\infty$-valued entries?\n\n**Noisy labels.** I believe the authors are correct that it's okay to fit with noisy labels, but I can't follow the current reasoning about \"high-frequency information.\" This explanation is vague and very informal, but I think there's a simple explanation for why noisy labels are okay. If I may make a suggestion, I believe the rationale is the following:\n\nFor each input $x$, let the target be $\\phi_x = \\phi(x) + \\tilde \\phi(x)$ where the first term is the true SV and the second is an input-specific noise term such that $\\mathbb{E}[\\tilde \\phi(x)] = 0$. Define your loss function as $\\mathcal{L}(\\theta) = \\mathbb{E}[ ||g(x; \\theta) - \\phi_x ||^2 ]$. If we decompose this, we can see that $\\mathcal{L}(\\theta) = \\mathbb{E}[ ||g(x; \\theta) - \\phi(x) - \\tilde \\phi(x) ||^2 ] = \\mathbb{E}[ ||g(x; \\theta) - \\phi(x)||^2 ] + C = \\text{Error}(\\theta) + C$, where $C = \\mathbb{E} [||\\tilde \\phi(x)||^2 ] = \\mathbb{E}_x [\\text{Tr}(\\text{Cov}(\\tilde \\phi(x)))]$. In other words, minimizing $\\mathcal{L}(\\theta)$ is equivalent to simply minimizing the Shapley value estimation error: mean-zero noise in the target doesn't affect the optimum, because it becomes a constant in the objective function.\n\nZooming out, the implication is that by performing SGD on $\\mathcal{L}(\\theta)$, you're minimizing the Shapley value estimation error without having access to the ground truth. (The authors pointed this out in their response, but I think this proof makes the point more concrete.) This is interesting, and in addition to the Mahalanobis perspective on FastSHAP, I wonder if the authors can contextualize this finding by comparing with Theorem 1 from \"Learning to Estimate Shapley Values with Vision Transformers,\" which shows that minimizing the FastSHAP loss is equivalent to minimizing an upper bound on $\\text{Error}(\\theta)$. The current treatment of this topic is a bit shallow, and it could become a more interesting contribution in this work.\n\n**Definition 3.** I noticed that this definition mentions the possibility of using estimated SV's for $\\phi_x$ rather than the true values. I believe it's worth repeating the analysis above for different Mahalanobis distances to verify that the objective's optimum is unchanged regardless of $M$ when $\\phi_x$ is an unbiased estimate. If that's not true, the paper should probably mention that eq. 12 only allows the use of unbiased estimates when $M = I$."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission173/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700510458423,
                "cdate": 1700510458423,
                "tmdate": 1700510458423,
                "mdate": 1700510458423,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Z7JIRVpzvq",
                "forum": "CNZmaInj9n",
                "replyto": "B5xLdiLqzo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission173/Reviewer_UWrD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission173/Reviewer_UWrD"
                ],
                "content": {
                    "title": {
                        "value": "Response (2/2)"
                    },
                    "comment": {
                        "value": "**Appendix A.7 results.** The Iris experiment provides some empirical evidence that fitting with noisy labels is okay and improves with more data. But I would recommend repeating this experiment with a more realistic dataset.\n\n**Figure 4 vs. Table 3.** The authors' response did not address the inconsistency between these results. SimSHAP's results for the insertion curve in Figure 4 are much worse than KernelSHAP, so how can it have higher insertion AUC in Table 3?\n\nOther issues:\n\n- The notation $v(S) = f(1^S \u00b7 x)$ is hardly a clarification, unfamiliar readers will not be able to infer what this means.\n- Even with the now-clarified role of unbiased KernelSHAP, eq. 5 is not correct. The second line of this equation, beginning with $\\approx$, defines the original KernelSHAP rather than the unbiased version! It may be worth revisiting Covert & Lee to see the difference between \"an exact solution to the approximate problem\" and \"an approximate solution to the exact problem.\" Eq. 5 shows the former, but unbiased KernelSHAP is the latter.\n- Regarding the law of large numbers, the correction is still wrong. The LLN does not imply the quoted rate of $O(1/\\sqrt{M})$.\n- It might be worth clarifying how the authors suggest tuning parameters for SimSHAP. Given that we don't assume access to the ground truth, how should we perform model selection or early stopping?\n\nOverall, this work still has many issues and would benefit from more time to make revisions."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission173/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700510472038,
                "cdate": 1700510472038,
                "tmdate": 1700510472038,
                "mdate": 1700510472038,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "poDExFe7Yx",
            "forum": "CNZmaInj9n",
            "replyto": "CNZmaInj9n",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission173/Reviewer_MdhR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission173/Reviewer_MdhR"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the efficient approximation of Shapley coefficients in the context of local feature importance for machine learning predictors. After reviewing the different approaches that approximate these intractable coefficients (exponential in dimension), the authors propose a generalization of previous approaches and a different way of computing them (employing the \"LS value\" distribution in the context of what they define as unified amortized estimators. They compare their results in approximation quality for tabular data, and for deletion and insertion analysis for image data, showing reasonable results, whereas the running time at inference is order of magnitude faster."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* This paper studies an important problem, with the increasing popularity of Shapley values to explain machine learning predictors (with high dimensional features).\n\n* The paper provides a nice introduction of current approximation techniques, and their proposed variations (while not radically innovative) are simple and intuitive.\n\n* The numerical results support their method, and seems adequate."
                },
                "weaknesses": {
                    "value": "* Some typos and unclear passages makes the paper harder to read at times (see below).\n\n* The demonstrations of benefits are all empirical, and there is no guarantee that their proposed SimSHAP provides faster approximations than other alternatives."
                },
                "questions": {
                    "value": "* Why did the authors define $f:\\mathcal X \\to \\mathcal Y$ in Sect 2.1 if this is not used in the rest of the paper? Note that the fact that $f(x_S) = v(S)$ was never made.\n\n* The authors do a good job at commenting on related works for approximation of Shapley values, due to their complexity. However, there is no mentioning to works that provide tractable, exact or finite-sample approximate, computations of these values in cases where distributional assumptions are made: \n\ni) [Chen, Jianbo, et al. \"L-shapley and c-shapley: Efficient model interpretation for structured data.\" arXiv preprint arXiv:1808.02610 (2018).]\n\nii) [Teneggi, Jacopo et al. \"Fast hierarchical games for image explanations.\" IEEE Transactions on Pattern Analysis and Machine Intelligence 45.4 (2022): 4494-4503]\n\nThe authors should comment on these because, in these cases, approximations are not needed, and thus Shapley coefficients admit provably more efficient computations. Naturally, if these properties of the data are not met, the approximation strategies that the authors describe do provide useful estimation tools.\n\n* In sec 2.3, the authors first define their Unified Stochastic Estimator and then show that it is a generalization of other schemes. However, their Definition 2 reads \"Most existing stochastic estimators can be unified..\" which sounds like a claim to be proven, not a definition. Indeed, this is shown/proven immediately after in the form of running text. I think the authors should consider re-organizing this Definition + Remarks as Definition + Theorem, which states the strict generalization.\n\n* I find the AUC results reported in table 3 a bit unsatisfying: the standard error (i presume?) are at the same or larger order of magnitude than the means! This makes these comparisons have very little meaning.\n\n* On the other hand, the improvement in inference speed is massive, and the authors wait until the end, in section 4.2.4, to showcase this. This is the greatest benefit of the method, and the authors should consider stressing this throughout the text more.\n\n* I'm confused as to how the ground truth Shapley values are computed for the experiments in Fig 2: some of the datasets have up to 96 features, which would results in completely prohibitory computation (around 10^28 flops). Even if these are computed w.r.t. some reference surrogate approximation (as I believe the authors explain in the Implementation Details), how are these surrogate trained? don't they need the ground truth values?\n\nMinor: \n* I think that to say that \"Deep learning techniques have revolutionized various industries due to their learning capability by universal approximation theorem\" is a stretch. Many methods provide universal approximation. The reasons for deep learning becoming so popular are others and more diverse.\n\n* After Definition 1, the authors write \"*However, machine learning models are not cooperative games and the complexity of Eq. (1) grows exponentially with the data dimension d*\". How is \"*machine learning models are not cooperative games*\" relevant to \"*the complexity of Eq. (1) grows exponentially with the data dimension d*\" exactly?\n\n* \"the optimization object L in Eq. (5) need[s] to be approximated\"\n\n* There is an $\\arg\\min_\\eta$ missing in the right-most term in Eq(5),\n\n* capaicity -> capacity\n\n* in equation 14, remove \" \\to Shapley values\" as this reads as \"tends to\", which is not what the authors meant pressumably."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission173/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission173/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission173/Reviewer_MdhR"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission173/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698814397360,
            "cdate": 1698814397360,
            "tmdate": 1700608370410,
            "mdate": 1700608370410,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "58IVhfD7KH",
                "forum": "CNZmaInj9n",
                "replyto": "poDExFe7Yx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission173/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission173/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer MdhR"
                    },
                    "comment": {
                        "value": "We thank reviewer MdhR for the affirmation and pertinent suggestions on our work.\nIn response to the issues raised by the reviewer, we have made the following revisions and additions.\nFor the revisions of the paper, we have marked them in **green** in the PDF.\n\n---\n## For weaknesses\n\nThanks for all the weaknesses pointed out, which greatly helped us revise our work. \n\n**W1:** \n\nWe apologize for any confusion caused. We have carefully proofread and modified the text based on the reviewer's feedback.\n\n**W2:** \n\nIt is crucial to highlight that SimSHAP belongs to the class of amortized Shapley Value estimators, and amortized techniques generally exhibit faster speeds than their non-amortized counterparts. Compared to FastSHAP, SimSHAP can achieve a **comparable** inference speed, without the need for normalization during inference.\n\nFurthermore, this article places greater emphasis on proposing a unified framework for computing Shapley values, and in the future, we intend to further explore the acceleration issue of amortized estimators from a theoretical standpoint.\n\n---\n## For major questions\n\nWe really appreciate the reviewer's thorough examination of our work and all the insightful questions that the reviewer has raised. We have made the following adjustments to address the concerns:\n\n**Q1:**\n\nWe recognize that there is room for improvement in our notations. We have streamlined the notation and removed any potential confusion.\n\n**Q2:** \n\nThank you for recommending additional related literature. We have incorporated more references on model-specific Shapley value estimation into our revised manuscript.\n\n**Q3:**\n\nWe agree that the previous presentation of **Definition 2** could be confusing. We have restructured this definition as: \n> **Unified Stochastic Estimator** is defined as the linear transformation of the values obtained from sampled subsets $S$.\n\n**Q4:**\n\nYou're right that the high standard deviation values in our experiments might be misleading. \nTo address this concern, we re-examined the calculation methods for metrics and implemented the bootstrap strategy recommended by FastSHAP. \nAs a result, we observed a significant reduction in the standard deviation, which is clearly reflected in Table 3 of our updated version.\n\n**Q5:**\n\nThank you for highlighting the importance of our inference speed advantage. We have emphasized this aspect in multiple sections of the \"Experiments\" section.\n\n**Q6:**\n\nGround Truth Shapley value computation is not fully precise in our experiment. \nWe used the `shapreg` package[^1] for the ground truth computation, which allows users to specify an error bound and compute a satisfactory ground truth Shapley value within a reasonable time span.\n\n---\n## For minor questions\n\nWe are grateful for the minor suggestions by the reviewer in terms of writing.\n\n**Q1:**\n\nThe reviewer raises an important point in this statement. We have revised the Introduction section to more clearly highlight the profound impact of deep learning techniques on various industries, due to their remarkable ability to learn complex functions both quickly and accurately.\n\n**Q2:**\n\nWe acknowledge that the use of \"and\" in this sentence may cause some confusion, as it does not intend to establish a connection between the two sub-sentences. \nWhat we are trying to convey is that cooperative games do not involve the choice of value functions, whereas machine learning models are not considered cooperative games, necessitating the consideration of value function choices. \nAdditionally, the complexity of computing Shapley values grows exponentially with the data dimension $d$, thus requiring the exploration of efficient estimation strategies. We have revised this portion of the text in the latest version of our paper to clarify these points.\n\n**Q3-6:**\n\nWe appreciate the reviewer's careful reading. The suggestions for details are helpful and we have modified our paper based on them.\n\n### Reference:\n[1] https://github.com/iancovert/shapley-regression"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission173/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700304176354,
                "cdate": 1700304176354,
                "tmdate": 1700356547722,
                "mdate": 1700356547722,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AM7ApPLZu4",
                "forum": "CNZmaInj9n",
                "replyto": "poDExFe7Yx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission173/Reviewer_MdhR"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission173/Reviewer_MdhR"
                ],
                "content": {
                    "title": {
                        "value": "Responses"
                    },
                    "comment": {
                        "value": "I thank the authors for their responses, which have clarified my doubts and comments. Before I recommend publication, and after having read all of the reviews and your responses, I would like to see the authors addressing the points raised by reviewer UWrD. In particular, I see as most important clarifying the precise similarities and differences with the references pointed out (Schwarzenberg et al and Chuang et al): if there are novel aspects relative to these works, the authors should explain what these are in detail.\n\nI see most of the other comments as minor, and I'm confident that the authors can address them (but I would like to see a plan forward on how the authors plan on doing this)."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission173/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700608326350,
                "cdate": 1700608326350,
                "tmdate": 1700608408850,
                "mdate": 1700608408850,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NhRfnCE0XP",
                "forum": "CNZmaInj9n",
                "replyto": "AM7ApPLZu4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission173/Reviewer_MdhR"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission173/Reviewer_MdhR"
                ],
                "content": {
                    "title": {
                        "value": "Follow up"
                    },
                    "comment": {
                        "value": "As a small comment: the $\\tilde{\\mathcal O}(\\sqrt{1/M})$ rate does indeed not follow from the law of large number, but from concentration (if $v(\\cdot)$ is bounded)."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission173/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700612038970,
                "cdate": 1700612038970,
                "tmdate": 1700612038970,
                "mdate": 1700612038970,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xnm7bqgNTN",
            "forum": "CNZmaInj9n",
            "replyto": "CNZmaInj9n",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission173/Reviewer_Gvfa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission173/Reviewer_Gvfa"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents three main contributions. First, the paper shows that existing stochastic estimators for Shapley values (semivalue, random order value, least squares value) can all be written in the form of an affine transformation of a weighted average of the values of sampled subsets. In this unified formulation, each estimator is uniquely defined by the sampling distribution for subsets, the weights in the average, and the parameters of the affine transformation. Second, the authors show that the FastSHAP objective can be viewed as minimizing the distance to estimated Shapley values under a specific metric space, noting that one can generalize this idea to define other amortized estimators by choosing other metric spaces. Then, the authors choose a particular choice of stochastic estimator (which they call Sim-Semivalue) and metric space (Euclidean) to propose SimSHAP, a new amortized Shapley estimator."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The unified perspectives on stochastic Shapley estimators and objectives for amortized Shapley estimators is clarifying, to my knowledge novel, and potentially quite impactful (e.g. by stimulating further progress on proposing better estimators).\n2. SimSHAP seems to reach comparable performance to FastSHAP with slightly faster inference time, given FastSHAP computes an additional normalization step that SimSHAP does not."
                },
                "weaknesses": {
                    "value": "1. The proposed SimSHAP seems insufficiently motivated. Why did the authors make the specific choices for the stochastic estimator and metric space that they did? \n2. It is not clear from the experiments if there are any performance benefits from SimSHAP beyond faster inference.\n\nOther notes:\n1. It would be helpful if the authors were a bit more explicit about notation, e.g. explicitly defining $\\mathbf{J}$ in the definition of the least squares value transformation $\\mathbf{T}$ and $\\mathbf{1}^{\\mathbf{S}}$ in the definition of the least squares value.\n2. The insertion curve seems to have two green curves but no blue curve. There might be a plotting bug somewhere?"
                },
                "questions": {
                    "value": "1. Why did the authors design SimSHAP the way they did? \n2. What are the reasons for using SimSHAP over FastSHAP? Is the primary benefit of SimSHAP faster inference time, or are there other benefits? Also, can the authors explain why SimSHAP outperforms FastSHAP specifically in the deletion AUC for CIFAR-10 and if there is a generalizable takeaway there? \n3. What do the authors mean by \"That\u2019s mostly because of the requirement of number of mask is larger for SimSHAP\" when explaining why SimSHAP training is slower than FastSHAP training in Section 4.2.4 (speed evaluation)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission173/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission173/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission173/Reviewer_Gvfa"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission173/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1700608214328,
            "cdate": 1700608214328,
            "tmdate": 1700608214328,
            "mdate": 1700608214328,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]