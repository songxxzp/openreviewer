[
    {
        "title": "ObjectNet Captions: Models are not superhuman captioners"
    },
    {
        "review": {
            "id": "AfRdscQiaW",
            "forum": "U17KoLrXE8",
            "replyto": "U17KoLrXE8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4705/Reviewer_DRVD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4705/Reviewer_DRVD"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces ObjectNet Captions, a dataset created to mitigate the exploitation of spurious correlations by machine learning models in image captioning tasks. Along with this dataset, the authors present HUMANr, a novel captioning metric aimed at providing a robust and consistent measure of performance that can be easily replicated and crowdsourced. HUMANr is intended to be an absolute performance metric that provides a clear target for model improvement and the ability to recognize when human-level captioning has been achieved, addressing the overestimation of machine performance by current metrics.\n\n\n# Post-rebuttal\nI appreciate the efforts made by the author. Their responses partially address my concern about the comparison and the scale of dataset. Therefore, I raise my rating. I encourage the author to make their proposed dataset and metric easily to use, such as can easily download and running via `pip`, to let people use them in practical ways."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "There are several strengths for this paper:\n\n- Introduction of a new dataset that targets a key issue, specifically the reliance on spurious correlations by captioning models. \n- Development of HUMANr, and it can be easily implemented and crowdsourced.\n- Potential to recalibrate the understanding of machine captioning performance, as HUMANr contrasts with existing metrics by showing the superiority of human captions.\n- The paper provides tools for automatic computation of HUMANr (in supplementary), facilitating its adoption by the research community.\n- It examined several learning-based Captioning models and metrics."
                },
                "weaknesses": {
                    "value": "I feel there are two major flaw points:\n\n- The authors currently did not use GPT-related captioning models, such as BLIP2. According to my usage, BLIP2 outperforms the compared methods used in this paper.\n\n-  The proposed dataset only contains 17,674 images which are quite small-scale to evaluate a captioning model comprehensively."
                },
                "questions": {
                    "value": "Please address the concerns mentioned above. \n\nCould the author please also provide random sampled image-captions pairs. The current appendix only contains a few examples which cannot be assessed comprehensively."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4705/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4705/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4705/Reviewer_DRVD"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4705/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698734471116,
            "cdate": 1698734471116,
            "tmdate": 1700730232801,
            "mdate": 1700730232801,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kIxnmpLzVl",
                "forum": "U17KoLrXE8",
                "replyto": "AfRdscQiaW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4705/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4705/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review!"
                    },
                    "comment": {
                        "value": "Thank you for your review and appreciation of our contributions.\n\n>The authors currently did not use GPT-related captioning models, such as BLIP2. According to my usage, BLIP2 outperforms the compared methods used in this paper.\n\nIn response to reviewers\u2019 comments, we evaluated the publicly available 6.7B parameter BLIP-2 model on ObjectNet Captions, which is an LLM-based captioning model. It received a HUMANr score of -0.41 +/- 0.03 which places it ahead of GIT_L but far below human performance. This is further evidence of the arguments we articulate in our paper: ObjectNet Captions is a challenging dataset and HUMANr is invaluable in its ability to quantify model performance in comparison to humans. We will revise our paper to include this result.\n\nAlthough very promising, the recent LLaVA models are\u2014according to the ICLR reviewing guidelines\u2014considered contemporaneous with our submission (checkpoints and peer-review were released <4 months ago). Although this excuses us from engaging with the models, we will include them as compute power and time to run on mechanical turk allows. GPT4V is difficult to evaluate because OpenAI doesn't allow new signups and throttles current accounts to only 100 images per day. We are working on including both in the final submission. LLaVA so far does show promise and our preliminary results say that it does close some of the gap with humans. Having established ObjectNet Captions and HUMANr as effective tools for measuring captioning performance, we can apply these models in future work to exploring and measuring the performance gains of these recent models. \n\nAs models become increasingly good, automatic metrics will fail to reliably measure just how good they are. Indeed in preliminary results, GPT4v scores very poorly across all the automatic metrics we report in our paper despite performing much better in HUMANr. We need to explicitly ground model evaluation in systematic human judgment in order to be able to quantify this progress. It may be obvious anecdotally that GPT4v is an improvement over other methods, but HUMANr allows us to quantify that improvement in a way that is aligned with our conception of what qualitative improvement is.\n\n>The proposed dataset only contains 17,674 images which are quite small-scale to evaluate a captioning model comprehensively.\n\nThe dataset is rather small, but we don\u2019t believe that is a significant limitation. Because ObjectNet Captions inherits the ObjectNet license, it cannot be used to train models so its purpose is only to evaluate which we recommend is done using our new metric HUMANr. As shown in fig 4 in our paper, the variance in HUMANr score declines sharply with the number of images used in the evaluation. Even just 500 images drives the standard deviation very close to zero. If anything, with 17,674 images, ObjectNet Captions is much larger than it needs to be to report reliable HUMANr evaluation. It should also be noted that our dataset is not small compared to other captioning test sets. For example, the nocaps test set contains only 10,600 images.\n\n>Could the author please also provide random sampled image-captions pairs. The current appendix only contains a few examples which cannot be assessed comprehensively.\n\nOur apologies, because of the constraints on the size of the supplemental upload, we are unable to add any additional examples of images. However, you can run following script to visualize image/caption pairs. It requires downloading the ObjectNet dataset and using the ObjectNet Captions JSON file in the supplemental material.\n\n```\nimport os\nimport PIL\nimport json\nimport glob\nimport random\nfrom matplotlib import pyplot as plt\n\nOBJECTNET_DIR = # path to objectnet images directory\nimages = glob.glob(OBJECTNET_DIR)\n\n\nCAPTIONS_FILE = # path to objectnet captions json\nwith open(CAPTIONS_FILE, 'r') as f:\n    captions = json.load(f)\n\nimage_to_captions = {}\nfor x in captions:\n    img = x['image']\n    caption = x['caption']\n    if img not in image_to_captions:\n        image_to_captions[img] = []\n    image_to_captions[img].append(caption)\n    \nimages = [img for img in images if os.path.join(*img_full.split('/')[-2:]) in image_to_captions] \n\nrandom.shuffle(images)\nimg_full = images[0]\nimg = os.path.join(*img_full.split('/')[-2:])\nplt.imshow(PIL.Image.open(img_full));plt.show()\nfor i, cap in enumerate(image_to_captions[img]):\n    print(f'{i}: {cap}')\n```"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4705/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700534326445,
                "cdate": 1700534326445,
                "tmdate": 1700534326445,
                "mdate": 1700534326445,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Wd6GC3gkaF",
            "forum": "U17KoLrXE8",
            "replyto": "U17KoLrXE8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4705/Reviewer_zcyp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4705/Reviewer_zcyp"
            ],
            "content": {
                "summary": {
                    "value": "To evaluate the captions generated by machines, this paper collected a dataset and proposed a new human study protocol. The machine-generated captions are compared with human-generated captions and humans are involved in the evaluation loop. The human study is performed on three datasets, i.e., COCO, Nocaps, and ObjectNet Captions. Three models are evaluated in this experiment, i.e., GIT, ClipCap, ExpNet."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper focuses on an important problem for the image captioning community, i.e., how big is the difference between machine generate captions and human-generated captions.\nThe conclusion that the machine-generated captions still underperform human-generated captions on unusual datasets and fail to generate long sentences is insightful for the community."
                },
                "weaknesses": {
                    "value": "However, there are several unclear questions need clarification.\n\n1. Apart from revealing how big is the difference between machine-generated captions and human-generated captions, it would be meaningful to reveal what is the difference between machine-generated captions and human-generated captions. Though the authors have revealed some differences, such as spurious objects and caption lengths, the root cause seems still unclear.\n\n2. Some experiment details are missing. For instance, how to compute the HUMANr score? \n\n3. Asking human participants to rate between 1-9 seems subjective. If two new image captioning models are evaluated with two different groups of people, will the results be comparable? It would be interesting to show the deviation of two different groups of people rating the same model in Figure 4.\n\n4. The ObjectNet cannot be regarded as a contribution as the authors only select some images with longer captions."
                },
                "questions": {
                    "value": "In Section 4.3, paragraph 2, what does ``we eliminated all images where GITL failed any of the seven checks above\u2014human failures were not considered\u2019\u2019 mean?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4705/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698928969980,
            "cdate": 1698928969980,
            "tmdate": 1699636452055,
            "mdate": 1699636452055,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "n2zSrTdvGs",
                "forum": "U17KoLrXE8",
                "replyto": "Wd6GC3gkaF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4705/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4705/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review!"
                    },
                    "comment": {
                        "value": "Thank you for your appreciation. We also think this is an important issue for the image captioning community.\n\n\n>However, there are several unclear questions need clarification.\n\n>Apart from revealing how big is the difference between machine-generated captions and human-generated captions, it would be meaningful to reveal what is the difference between machine-generated captions and human-generated captions. Though the authors have revealed some differences, such as spurious objects and caption lengths, the root cause seems still unclear.\n\nWe agree! We are also interested in what differences exist between model and human captions such that human evaluators prefer human captions. In Section 4.3, we choose 7 dimensions to evaluate model and human captions on and do some controlled analysis. What we find is that models are more likely to misclassify objects, misattribute properties to objects, and misjudge the physical relationships between objects. However, all together, our 7 dimensions only account for 21% of the HUMANr score which underscores the difficulty of explicitly characterizing the reasons for the gap in performance. We are interested in working on this in future work, however. HUMANr provides a good way to establish human qualitative judgments that can be investigated to determine features that may explain the judgments.\n\n>Some experiment details are missing. For instance, how to compute the HUMANr score?\nAsking human participants to rate between 1-9 seems subjective. If two new image captioning models are evaluated with two different groups of people, will the results be comparable? It would be interesting to show the deviation of two different groups of people rating the same model in Figure 4.\n\nWe apologize for not explicitly describing the calculation of HUMANr. As described in the text, each task is given a rating of 1-9 by a human worker where 1 means only the caption on the left matches the image, 9 means only the caption on the right matches the image, and 5 means the two captions match equally well. That is, the magnitude of the value 1-9 should be interpreted as a slider to indicate which side to give preference to (1 and 9 are equally valent, just on opposite sides). This interpretation is made very clear to the workers.\n\nThen we transform the scores such that each caption is scored as if the human caption were on the left. So if a task with a model caption on the left and human caption on the right was given a score of 7, it would be transformed to a 3. Now that all the scores have the same alignment, we average them all together and then normalize to [-1, 1] where a negative score implies the human captions were preferred on average and a positive score means the model captions were preferred on average.\n\nAs figure 4 demonstrates, the difference in HUMANr score between random groups of workers is very small in our experiments, even when those groups are disjoint. Even though individual workers may have subjective preferences, when aggregated over hundreds or even thousands of images, the HUMANr score is quite stable and robust to the exact makeup of the worker pool. \n\n>The ObjectNet cannot be regarded as a contribution as the authors only select some images with longer captions.\n\nWe feel that by augmenting ObjectNet and Spoken Objectnet with transcriptions, we have enhanced its usefulness and made a significant contribution to the image captioning field.\n\n\n>In Section 4.3, paragraph 2, what does ``we eliminated all images where GITL failed any of the seven checks above\u2014human failures were not considered\u2019\u2019 mean?\n\nThis means that we took all the captions that we investigated for which GIT_L passed all 7 of the questions in our quality assessment, but we did not filter the human captions. The result is an optimistic comparison (in favor of GIT_L) between GIT_L captions that didn\u2019t fail the assessment on any dimension versus human captions that may have failed some dimension of the assessment. This shows that even when we consider GIT_L at its best against fallible humans, it still significantly underperforms a human level in terms of HUMANr score."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4705/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700534220170,
                "cdate": 1700534220170,
                "tmdate": 1700534220170,
                "mdate": 1700534220170,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7W5kcp9oHy",
                "forum": "U17KoLrXE8",
                "replyto": "Wd6GC3gkaF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4705/Reviewer_zcyp"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4705/Reviewer_zcyp"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to the authors for the reply. Still, my major concerns are not well-resolved.\n\nThis paper investigates an interesting topic but the experiment design and analysis still need polish.\n\nI would like to keep my original rating."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4705/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700662459142,
                "cdate": 1700662459142,
                "tmdate": 1700662459142,
                "mdate": 1700662459142,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "fBdkJrgLM1",
            "forum": "U17KoLrXE8",
            "replyto": "U17KoLrXE8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4705/Reviewer_X1XD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4705/Reviewer_X1XD"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on the task of image captioning and proposes a new dataset and a new metric. There are some findings, for example, there is a large gap betten human and models on the task of image captioning. The proposed dataset if challenging compared with existing ones, which contains much more unique tokens and n-grams and should be useful for the community."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. a new dataset is proposed. The dataset is more challenging and contains more unique tokens and n-grams.\n2. a new metric is proposed.\n3. analysing existing models vs. human using a wide range of metrics."
                },
                "weaknesses": {
                    "value": "1. the scale of the dataset is small. \n2. the auther only considers traditional image captioning models. Some LLM-based models like LLaVA should be considered and the comparison among these models should be more interesting.\n3. the findings that there is a large gap betten human and models is a common sense, so I do not think it is a significant contribution. But if the author can show that the most advanced models like GPT-4v is inferior to humans and the proposed metric is able to measure the gap, it should be more interesting."
                },
                "questions": {
                    "value": "Some important references related to image captioning metrics are missing.\n1. Learning to evaluate image captioning. CVPR 2018.\n2. Describing like humans: on diversity in image captioning, CVPR 2018.\n3. On diversity in image captioning: metrics and methods, TPAMI, 2022."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4705/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699068284607,
            "cdate": 1699068284607,
            "tmdate": 1699636451974,
            "mdate": 1699636451974,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dmtEzxNOUR",
                "forum": "U17KoLrXE8",
                "replyto": "fBdkJrgLM1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4705/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4705/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review!"
                    },
                    "comment": {
                        "value": "Thank you for your comments!\n\n>the scale of the dataset is small.\n\nThe dataset is rather small, but we don\u2019t believe that is a significant limitation. Because ObjectNet Captions inherits the ObjectNet license, it cannot be used to train models so its purpose is only to evaluate which we recommend is done using our new metric HUMANr. As shown in fig 4 in our paper, the variance in HUMANr score declines sharply with the number of images used in the evaluation. Even just 500 images drives the standard deviation very close to zero. It should also be noted that our dataset is not small compared to other captioning test sets. For example, the nocaps test set contains only 10,600 images.\n\n\n>the auther only considers traditional image captioning models. Some LLM-based models like LLaVA should be considered and the comparison among these models should be more interesting.\n\nAlthough very promising, the LLaVA models are\u2014according to the ICLR reviewing guidelines\u2014considered contemporaneous with our submission (checkpoints and peer-review were released <4 months ago). Although this excuses us from engaging with the models, we will include them as compute power and time to run on mechanical turk allows. GPT4V is difficult to evaluate because OpenAI doesn't allow new signups and throttles current accounts to only 100 images per day. We are working on including both in the final submission. LLaVA so far does show promise and our preliminary results say that it does close some of the gap with humans. Having established ObjectNet Captions and HUMANr as effective tools for measuring captioning performance, we can apply these models in future work to exploring and measuring the performance gains of these recent models. \n\nAs models become increasingly good, automatic metrics will fail to reliably measure just how good they are. Indeed in preliminary results, GPT4v scores very poorly across all the automatic metrics we report in our paper despite performing much better in HUMANr. We need to explicitly ground model evaluation in systematic human judgment in order to be able to quantify this progress. It may be obvious anecdotally that GPT4v is an improvement over other methods, but HUMANr allows us to quantify that improvement in a way that is aligned with our conception of what qualitative improvement is.\n\n\n>the findings that there is a large gap betten human and models is a common sense, so I do not think it is a significant contribution. \n\nWe know that the gap in performance between humans and models in image captioning is no secret. Every researcher knows anecdotally that models fail in ways that humans don\u2019t. However, until now, there has been no way to measure that gap. Automatic metrics cannot quantify this gap for us. Rather, if we relied solely automatic metrics and not on anecdotal evidence, we would be led to believe that the gap does not exist. We introduce HUMANr not to prove that the gap exists\u2014like the reviewer notes, everyone knows that the gap exists\u2014but rather to present a method for measuring that gap. Our contribution is not discovering that gap, but rather presenting and motivating a metric which actually reflects that gap which has not been done systematically in the literature. We will revise the language of the text to reflect this.\n\n>But if the author can show that the most advanced models like GPT-4v is inferior to humans and the proposed metric is able to measure the gap, it should be more interesting.\n\nPlease see above for discussion regarding GPT4v limited availability.\n\n>Some important references related to image captioning metrics are missing.\n\nThank you for these references! We will be sure to include them in the camera ready revision."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4705/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700534142592,
                "cdate": 1700534142592,
                "tmdate": 1700534142592,
                "mdate": 1700534142592,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "P4YLtYObEZ",
            "forum": "U17KoLrXE8",
            "replyto": "U17KoLrXE8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4705/Reviewer_V5WD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4705/Reviewer_V5WD"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces \"ObjectNet Captions,\" a challenging dataset for image captioning, and presents HUMANr, a new metric for evaluating caption quality. It highlights a significant performance gap between human and model-generated captions, emphasizing the limitations of current models in generating detailed, accurate captions. The study's findings challenge the notion that advanced models like GPT-4 surpass human capabilities in this domain."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Introduction of a challenging dataset and HUMANr metric.\n2. In-depth comparison of existing models with human performance using various metrics.\n3. The paper effectively showcases the limitations of current models in handling diverse and complex captioning scenarios."
                },
                "weaknesses": {
                    "value": "1. The dataset's focus on home environments and its relatively small size (17,674 images) may limit its generalizability.\n2. Not including state-of-the-art models like BLIP2 or LLM-based models in the analysis.\n3. The human-centric approach, while insightful, may introduce new biases and subjectivities.\n4. The cost and scalability of HUMANr in large-scale applications are not addressed.\n5. The revelation of a performance gap between humans and models is not a novel insight and lacks depth without comparing the most advanced models.\n6. The paper omits crucial experimental details, like the computation of HUMANr and handling discrepancies in human evaluations."
                },
                "questions": {
                    "value": "1. How can the ObjectNet Captions dataset be expanded to cover a broader range of environments and scenarios?\n2. What steps can be taken to include state-of-the-art models like BLIP2 in future evaluations?\n3. How does HUMANr address the subjectivity and potential bias in human judgment?\n4. Are there plans to adapt the dataset and HUMANr for non-English languages or diverse cultural contexts?\n5. How can the scalability and cost-effectiveness of HUMANr be improved for widespread adoption?\n6. Can the authors provide more details on the methodology, especially regarding the computation of HUMANr and the management of subjective human ratings?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4705/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4705/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4705/Reviewer_V5WD"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4705/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699856785260,
            "cdate": 1699856785260,
            "tmdate": 1699856785260,
            "mdate": 1699856785260,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GQo9vEoBAj",
                "forum": "U17KoLrXE8",
                "replyto": "P4YLtYObEZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4705/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4705/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review! (Part 1)"
                    },
                    "comment": {
                        "value": ">Introduction of a challenging dataset and HUMANr metric.\nIn-depth comparison of existing models with human performance using various metrics.\nThe paper effectively showcases the limitations of current models in handling diverse and complex captioning scenarios.\n\nThank you for your comments on the strengths of our paper. \n\n>The dataset's focus on home environments and its relatively small size (17,674 images) may limit its generalizability.\n\nThe restriction to in-home environments is a limitation, but it still allows ObjectNet to meaningfully probe models abilities. The in-home/indoor environment is an important context where many robots or future AI systems could be deployed. It provides a wide variety of objects and significant visual complexity. Also, non-expert human annotators have the appropriate visiolinguistic experience to caption indoor in-home images which makes captioning easy. Future work could explore captioning in other environments, but to start with, the indoor environment is likely the most broadly generalizable and the most difficult.\n\nAs for the size of ObjectNet Captions, we don\u2019t believe its size is a limitation. As shown in fig 4 in our paper, the variance in HUMANr score declines sharply with the number of images used in the evaluation. Even just 500 images drives the standard deviation very close to zero. It should also be noted that our dataset is not small compared to other captioning test sets. For example, the nocaps test set contains only 10,600 images.\n\n\n>Not including state-of-the-art models like BLIP2 or LLM-based models in the analysis.\n\nIn response to reviewers\u2019 comments, we evaluated the publicly available 6.7B parameter BLIP2 model on ObjectNet Captions. It received a HUMANr score of -0.41 +/- 0.03 which places it ahead of GIT_L but far below human performance (negative is worse than humans, 0 is human level, and positive is better than humans; with a range of -1 to +1). This is further evidence of the arguments we articulate in our paper: ObjectNet Captions is a challenging dataset and HUMANr is invaluable in its ability to quantify model performance in comparison to humans. We will revise our paper to include this result.\n\nAs for recent LLM-based methods, the LLaVA models are very promising but are\u2014according to the ICLR reviewing guidelines\u2014considered contemporaneous with our submission (checkpoints and peer-review were released <4 months ago). Although this excuses us from engaging with the models, we will include them as compute power and time to run on mechanical turk allows. GPT4V is difficult to evaluate because OpenAI doesn't allow new signups and throttles current accounts to only 100 images per day. We are working on including both in the final submission. LLaVA so far does show promise and our preliminary results say that it does close some of the gap with humans. Having established ObjectNet Captions and HUMANr as effective tools for measuring captioning performance, we can apply these models in future work to exploring and measuring the performance gains of these recent models. \n\nAs models become increasingly good, automatic metrics will fail to reliably measure just how good they are. Indeed in preliminary results, GPT4v scores very poorly across all the automatic metrics we report in our paper despite performing much better in HUMANr. We need to explicitly ground model evaluation in systematic human judgment in order to be able to quantify this progress. It may be obvious anecdotally that GPT4v is an improvement over other methods, but HUMANr allows us to quantify that improvement in a way that is aligned with our conception of what qualitative improvement is."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4705/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700533948255,
                "cdate": 1700533948255,
                "tmdate": 1700533948255,
                "mdate": 1700533948255,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QbAhaUxfLX",
                "forum": "U17KoLrXE8",
                "replyto": "P4YLtYObEZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4705/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4705/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review! (Part 2)"
                    },
                    "comment": {
                        "value": ">The human-centric approach, while insightful, may introduce new biases and subjectivities.\n\nThere are, of course, some trade-offs to be made when deciding to use human evaluation. \n\nAlthough we note that any human annotators bring their own biases to every experiment, we attempted to make instructions as clear as possible to maximize reproducibility. The fact that workers agree on HUMANr (as evidenced by the fact that the variance is small / inter-coder agreement is high) means that workers understood the instructions and answered consistently. AMT is spread around the world, its population changes radically across continents during a long experiment, we would not have this level of consistency if each group was strongly influenced by local biases. There may be AMT-specific biases that are shared by all workers, but there is little evidence for this.\n\nEven if human workers were to be biased in a particular way, there\u2019s no clear reason why we would want to remove this bias. Parity with human capability is, after all, our goal. We want to build models that are as good as humans as judged by humans. If a model scores 0 HUMANr, that means that a large group of humans did not find its captions to be distinguishable on average from human captions. That is no small feat regardless of what bias you think might exist in the individual worker subjectivity. In future work, as models move closer to human level performance on this general captioning task our same method could be used to study vision tasks with more culturally subjective responses and the background of human evaluators could then be taken into account when aggregating results.\n\n>The cost and scalability of HUMANr in large-scale applications are not addressed.\n\nWe address the cost and scalability of HUMANr in the discussion section of our manuscript. Was there a particular portion of our discussion that the reviewer found insufficient?\n\nIt does, of course, cost money to use human evaluation, but we feel that the costs are outweighed by the benefits. HUMANr gives us a measurement of captioning performance that is grounded in qualitative assessment by humans. Automatic metrics are cheap and easy, but they give us no guarantees about how quantitative improvement translates to qualitative improvement. We don\u2019t intend HUMANr to replace automatic metrics completely, however. Automatic metrics can be efficiently computed during training and have the added advantage of being differentiable. HUMANr is primarily intended to be used for final evaluation as reported in a paper when presenting a model. As we have shown, this final evaluation can be quite cost efficient as even just 500 images drives HUMANr variance close to zero. This small (but not nonexistent) cost can even be a good thing as it discourages excessive hyperparameter tuning to overfit to metrics.\n\nFor large applications in high-resource labs, there may be big advantages to integrating HUMANr into training pipelines. Indeed, recent improvements using reinforcement learning with human feedback have demonstrated this kind of approach to be very fruitful. HUMANr could be computed while training large models to get a reliable performance signal. This could be very expensive, but for labs with the funds to do RLHF, it could lead to substantial improvements.\n\nBasically, HUMANr is very cheap to use in its originally intended application, but could also be scaled up if so desired."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4705/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700533990079,
                "cdate": 1700533990079,
                "tmdate": 1700533990079,
                "mdate": 1700533990079,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BlwscPt01E",
                "forum": "U17KoLrXE8",
                "replyto": "P4YLtYObEZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4705/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4705/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review! (Part 3)"
                    },
                    "comment": {
                        "value": ">The revelation of a performance gap between humans and models is not a novel insight and lacks depth without comparing the most advanced models.\n\nWe know that the gap in performance between humans and models in image captioning is no secret. Every researcher knows anecdotally that models fail in ways that humans don\u2019t. However, until now, there has been no way to measure that gap. Automatic metrics cannot quantify this gap for us. We introduce HUMANr not to prove that the gap exists, but rather to present a method for measuring that gap which we show is not measured by automatic metrics and is only anecdotally recognized. We will revise the language of the text to reflect this.\n\nAs for analysis with the most advanced models, please refer to our response above. Models such as BLIP2 have improved in their HUMANr score, and we plan to include even more advanced models on a leaderboard website as those become available. However, automatic metrics fail to measure this performance improvement which further supports the need to systematize human-in-the-loop evaluation like we do with HUMANr.\n\n>The paper omits crucial experimental details, like the computation of HUMANr and handling discrepancies in human evaluations.\n\nAs described in the text, each task is given a rating of 1-9 by a human worker where 1 means only the caption on the left matches the image, 9 means only the caption on the right matches the image, and 5 means the two captions match equally well. We randomize which caption is on the left and right so our rating scale requires a simple transformation before we can aggregate the scores such that each caption is scored as if the human caption were on the left. So if a task with a model caption on the left and human caption on the right was given a score of 7, it would be transformed to a 3. Now that all the scores have the same alignment, we average them all together and then normalize to [-1, 1] where a negative score implies the human captions were preferred on average and a positive score indicates that the model captions were preferred on average. We will include this more detailed explanation in the revision.\n\nAs for discrepancies in human evaluations, each task contains an attention check, as described in our paper on page 7, in which a caption from a random image was added to the task. If the worker fails the attention check by selecting the random caption, all of their responses to any task are ignored when computing HUMANr. That way, we ensure that all our results come from attentive workers.\nWe also show in figure 4 that the HUMANr score variance between disjoint sets of the worker pool is very low. When aggregated over hundreds of images, the subjectivity of individual workers is largely averaged out resulting in a convergent HUMANr score with low variance.\n\n>How can the ObjectNet Captions dataset be expanded to cover a broader range of environments and scenarios?\n\nThe spoken caption collection paradigm used to generate captions for ObjectNet Captions is applicable to any dataset. Our work demonstrates the value of using both more complex images and transcribed spoken language to collect more thorough captions. We would love for others to expand on our work and we have made this as easy as possible by releasing a toolkit for collecting HUMANr judgments and basing our captioning method on the publicly available spoken captions toolkit.\n\n>What steps can be taken to include state-of-the-art models like BLIP2 in future evaluations?\n\nSee above for discussion regarding more recent model evaluations.\n\n>How does HUMANr address the subjectivity and potential bias in human judgment?\n\nPlease see above.\n\n>Are there plans to adapt the dataset and HUMANr for non-English languages or diverse cultural contexts?\n\nNot at this time, but we would love for others to build on our work. The value of ObjectNet Captions lies in the challenge it poses to models; encouraging models to improve in fundamental ways in order to tackle a more challenging AI problem. We expect the trends in the HUMANr results to hold across languages. As for diverse cultural contexts, the images in ObjectNet were collected from Mechanical Turk workers from across the world from a wide array of cultural and socioeconomic backgrounds. Although not perfect, a cursory look through the dataset will convince you that it represents a broader diversity of contexts than most benchmark datasets.\n\nHUMANr is not language- or dataset-specific! In order to run HUMANr, all one needs is images, reference captions, and candidate captions. The images, references, and candidates can come from any source which makes HUMANr applicable to any and all captioning tasks. We\u2019ve developed and will publicly release a code tool which allows anyone to run HUMANr automatically on any dataset of their choice with a single command line instruction. The code is included in the supplemental material."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4705/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700534011392,
                "cdate": 1700534011392,
                "tmdate": 1700534011392,
                "mdate": 1700534011392,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "80lKwn8AKP",
                "forum": "U17KoLrXE8",
                "replyto": "P4YLtYObEZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4705/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4705/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review! (Part 4)"
                    },
                    "comment": {
                        "value": ">How can the scalability and cost-effectiveness of HUMANr be improved for widespread adoption?\n\nAs discussed in the paper, HUMANr is very cost-effective. A report of HUMANr to benchmark a model in a publication requires, of course, enough images to ensure high confidence which may be over $100 or so, but unofficial runs do not need to be costly. Even just a couple of tens of images for a couple dollars would give a good performance signal for your model. Relative to the cost of building a dataset or training a modern LLM based captioning model these evaluation costs are quite low.\n\nAs for usability, we\u2019ve developed a code tool that automatically runs HUMANr evaluation from the command-line. The user only needs to supply the images and captions. The script will post tasks on MTurk and then collect and save HUMANr results. This makes HUMANr not only cheap, but easy to use. The code for this tool is included in the supplemental material.\n\n>Can the authors provide more details on the methodology, especially regarding the computation of HUMANr and the management of subjective human ratings?\n\nPlease see response above."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4705/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700534024947,
                "cdate": 1700534024947,
                "tmdate": 1700534024947,
                "mdate": 1700534024947,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]