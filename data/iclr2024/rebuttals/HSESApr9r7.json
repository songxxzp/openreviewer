[
    {
        "title": "FedEve: On Bridging the Client Drift and Period Drift for Cross-device Federated Learning"
    },
    {
        "review": {
            "id": "GFH7Cygwdn",
            "forum": "HSESApr9r7",
            "replyto": "HSESApr9r7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5775/Reviewer_NDKx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5775/Reviewer_NDKx"
            ],
            "content": {
                "summary": {
                    "value": "This work investigates the decoupling of classic client drift which is widely studied in previous works, and proposed \"period drift\" which is less explored. Authors propose a simple method FedEVE based on their predict-observe framework, and extensive experiments support their proposed method's performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This work is well organized and written.\n- This work proposes a very simple and effective method based on the Bayesian filter (or Kalman filter). The experimental results support their claim.\n- The proposed \"period drift\" concept is good for federatede learning community to  further study. Authors are encouraged to open-source their source codes for FedEvE and other compared methods which helps to broaden the influence of this work.\n- Personally I like the analysis of Kalman Gain a lot : )\n- One less studied area discusses how to perform FL under noisy labels [1], future studies can explore this area with the light of  authors' proposed framework.\n\n[1] Jiang X, Sun S, Wang Y, et al. Towards federated learning against noisy labels via local self-regularization[C]//Proceedings of the 31st ACM International Conference on Information & Knowledge Management. 2022: 862-873."
                },
                "weaknesses": {
                    "value": "- Since this work is tightly related to the client selection, so the random seeds to conduct experiments on their proposed method and baseline methods should be given to increase the reproducibility.\n- The total client number for other datasets (CIFAR10/100) seems not given."
                },
                "questions": {
                    "value": "See above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5775/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5775/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5775/Reviewer_NDKx"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5775/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698066033755,
            "cdate": 1698066033755,
            "tmdate": 1699636606876,
            "mdate": 1699636606876,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "MoBb3CpQ9i",
            "forum": "HSESApr9r7",
            "replyto": "HSESApr9r7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5775/Reviewer_GeAV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5775/Reviewer_GeAV"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes the period drift, which means that, participating clients at each communication round may exhibit distinct data distribution. Authors claim that it could be more harmful than client drift since the optimization objective shifts with every round. To this end, this paper investigates the interaction between period drift and client drift, finding that period drift can have a particularly detrimental effect on cross-device FL as the degree of data heterogeneity increases. Then, a predict-observe framework and an instantiated method, FEDEVE is proposed, where these two types of drift can compensate each other to mitigate their overall impact."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Using Bayesian filter to compensate two sources of drift is novel.\n2. The connection between server momentum and Kalman Filter is interesting.\n3. The paper is written clearly."
                },
                "weaknesses": {
                    "value": "1. The so called ``period drift'' comes from the stochastic sampling of clients. If we see sampling clients as sampling data in SGD, such a period drift also happens during SGD -- each batch of data has distinct data distribution from other batches. Authors should provide a more rigorous definition of period drift and show that how the period drift harms training.\n2. The Figure 3 shows the period drift that the sampled data on one client varies across different rounds. This may still be similar to [1], as indicated in related work. Moreover, to address this varying effect, the clients can traverse the whole local dataset using sampling without replacement. \n3. Experiment result show little improvements than baselines.\n\n[1] Diurnal or nocturnal? federated learning of multi-branch networks from periodically shifting distributions."
                },
                "questions": {
                    "value": "1. See weakness 2. When clients traverse the whole local dataset using sampling without replacement, does the period drift still happen?\n2. As shown in Figure 3, how fedavg_perod_drift_only is drawn? Specifically, how to guarantee that only period drift happens, but client drift not happens?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5775/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698514752425,
            "cdate": 1698514752425,
            "tmdate": 1699636606773,
            "mdate": 1699636606773,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "iPzGYhf8RU",
            "forum": "HSESApr9r7",
            "replyto": "HSESApr9r7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5775/Reviewer_UbMd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5775/Reviewer_UbMd"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the effect of various \"drifts\" in Federated learning settings. In particular, the paper focuses on period drift, which arises due to partial participation of clients in FL settings. The paper proposes a predict-observe framework and provides an instantiation of the framework, FedEve, to handle these drifts. Experiments are provided to demonstrate the effectiveness of these approaches. While the paper has interesting elements, I have the following primary concerns:\n\n(1) The so-called \"period drift\" arises in almost all stochastic optimization methods. Of course, this could be severe in FL settings due to higher data heterogeneity\u00a0but the presentation of the paper is misleading since it is presented as if it is a new concept.\n\n(2) Missing mathematical rigor: It felt like the paper was missing mathematical rigor. For instance, period drift was not defined in the whole paper. The exact definition of it is missing. Furthermore, at places, terms were introduced without proper mathematical definition (e.g. w_server in Assumption 3.1).\n\n(3) Assumptions in the paper are very strong. While the authors tried to provide some vague justification, this does not represent any realistic scenario.\u00a0 Assumption 3.2 especially looks very strong and I do not believe it happens in practice. Are there any empirical evidence provided to support these Assumptions (which I may have missed)?\n\n(4) The empirical analysis looks fairly weak. The improvement on most datasets seems somewhat small and experiments do not provide any justification for the assumptions made in the paper.\n\nOverall, while the paper has interesting elements, I believe there are severe shortcomings need to be addressed before publication."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Refer to summary"
                },
                "weaknesses": {
                    "value": "Refer to summary"
                },
                "questions": {
                    "value": "Refer to summary"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5775/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5775/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5775/Reviewer_UbMd"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5775/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699742939155,
            "cdate": 1699742939155,
            "tmdate": 1699742939155,
            "mdate": 1699742939155,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]