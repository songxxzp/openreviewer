[
    {
        "title": "DreamSmooth: Improving Model-based Reinforcement Learning via Reward Smoothing"
    },
    {
        "review": {
            "id": "aRsZs2ihMs",
            "forum": "GruDNzQ4ux",
            "replyto": "GruDNzQ4ux",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1586/Reviewer_Q6F6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1586/Reviewer_Q6F6"
            ],
            "content": {
                "summary": {
                    "value": "This paper propose DreamSmooth which is based on Dreamer-V3. The technique used in this paper is to smooth the ground truth rewards when trainining Dreamer-V3. Experiments show that this modification works well on both dense and sparse rewards environments."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The proposed method solves a problem in model based RL when the gt rewards are sparse in these environments. Smoothing the rewards would make the reward function prediction process (reward leanring in model based RL) much better than before. At the same time, DreamSmooth shows it also performs well on dense rewards environments. It makes the whole algorithm more convincing. I think it is a good paper to investigate the reward smoothing technique for model based RL."
                },
                "weaknesses": {
                    "value": "Dreamer-V3 has a symlog prediction function with reward learning process. I think different reward prediction function would contribute to the reward learning process. Do the authors conduct some experiments with different prediction function head to justify whether it could solve the sparse rewards problem? I think the issue discussed in this paper is mainly about the reward generalizability problem. It is hard for reward function in MBRL to generalize in sparse reward setting."
                },
                "questions": {
                    "value": "I am curious about why DreamSmooth works comparable without reward smooth technique in dense reward environments. The reward smoothing technique changes the  reward distribution. As far as I am concerned, using this kind of technique would decline the final performance. Since MBRL like Dreamer is so important to this community, I lean to weak accept for this paper. However, I do have some concerns about this simple yet effective algorithms, especially why the final performance doesn't decline (especially on dense reward environments)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1586/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1586/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1586/Reviewer_Q6F6"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1586/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698397805575,
            "cdate": 1698397805575,
            "tmdate": 1699636087385,
            "mdate": 1699636087385,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "MJUr9W1BNg",
                "forum": "GruDNzQ4ux",
                "replyto": "aRsZs2ihMs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1586/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1586/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer\u2019s thorough and constructive feedback about our paper. We provide detailed answers to the questions below.\n\n&nbsp; \n\n**[Q] Do the authors conduct some experiments with different prediction function head to justify whether it could solve the sparse rewards problem?**\n\nThis is a really good point! As suggested by the reviewer, we conducted additional experiments to see whether different reward head type (regression) and loss (L1 and L2) can solve the reward prediction problem.\n\nIn our experiments on RoboDesk and Hand, both L1 and L2 losses with a typical MLP reward head miss many sparse reward signals, similar to the results with DreamerV3\u2019s two-hot categorical regression. Furthermore, these reward modeling approaches also showed improved performances with DreamSmooth. We summarized these results in the updated paper (Figure 17, Appendix E).\n\nIn summary, we found that the reward prediction problem exists in diverse commonly used regression models and losses, and DreamSmooth effectively alleviates this problem.\n\n&nbsp; \n\n**[Q] why DreamSmooth works comparable without reward smooth technique in dense reward environments.**\n\nWe would like to refer the reviewer to Theorem A.1 in Appendix A, which shows that the proposed EMA reward smoothing guarantees an optimal policy even with the changes in reward distribution. Thus, smoothing the past rewards does not deteriorate the performance of MBRL.\n\nIn the case when reward smoothing involves future rewards, Theorem A.2 explains that an optimal policy under any of our reward smoothing functions is also optimal under the original reward function. This still does not guarantee that MBRL can achieve optimal performance with reward smoothing; but, in practice, blurring reward signals a few steps before and after does not completely change learning dynamics, especially under the dense reward setting.\n\n&nbsp; \n\n---\n\n&nbsp; \n\nWe thank the reviewer again for the time and effort put into improving our paper. Please feel free to let us know if there are any additional concerns or questions."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1586/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700467028554,
                "cdate": 1700467028554,
                "tmdate": 1700467028554,
                "mdate": 1700467028554,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kBntzQ4dO1",
                "forum": "GruDNzQ4ux",
                "replyto": "MJUr9W1BNg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1586/Reviewer_Q6F6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1586/Reviewer_Q6F6"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your response"
                    },
                    "comment": {
                        "value": "Thanks for your response. I would like to keep my score."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1586/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700729555892,
                "cdate": 1700729555892,
                "tmdate": 1700729555892,
                "mdate": 1700729555892,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "uJmDhVokRK",
            "forum": "GruDNzQ4ux",
            "replyto": "GruDNzQ4ux",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1586/Reviewer_7VX8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1586/Reviewer_7VX8"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the challenges of learning reward models in the context of Model-Based Reinforcement Learning (MBRL). The authors argue that existing methods in MBRL fail to learn a good reward model in sparse / partially observable/stochastic environments/tasks and show empirical evidence for this. Based on the intuitive idea that, in such challenging scenarios, one only has to rely on rough reward estimates as humans do, they propose a method called DreamSmooth. In DreamSmooth the reward model is now tasked to predict a temporally smoothed reward instead of the exact reward. The authors propose 3 reward-smoothing schemes and empirically show that the approach can significantly improve the performance on most sparse reward scenarios."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "These are the strengths of the paper in my opinion:\n\n1) Studies a largely ignored, yet important problem of reward modelling in the context of MBRL.\n2) Propose a simple yet effective solution for the same.\n3) Well Written."
                },
                "weaknesses": {
                    "value": "The major weaknesses are as follows:\n\n1) Counter-intuitive results in Crafter, where the method performs worse even after having a much better reward model.\n2) The need to experiment with 3 different reward smoothing schemes each with its own hyperparameters (since none of them seems to consistent favourite across tasks)."
                },
                "questions": {
                    "value": "1) Have you experimented with different loss functions (on the unsmoothed rewards)? For example, what would happen if you use an L1 Loss instead of an L2 Loss commonly used in literature?\n2) The rationale behind why the counter-intuitive results in crafter is not convincing. Did the authors perform further empirical studies / analysis ?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1586/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1586/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1586/Reviewer_7VX8"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1586/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698790231682,
            "cdate": 1698790231682,
            "tmdate": 1699636087305,
            "mdate": 1699636087305,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hHCUI0cRlO",
                "forum": "GruDNzQ4ux",
                "replyto": "uJmDhVokRK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1586/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1586/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer\u2019s thorough and constructive feedback about our paper. We provide detailed answers to the questions below.\n\n&nbsp;\n\n**[Q] Have you experimented with different loss functions (on the unsmoothed rewards)?**\n\nThis is a really good point! As suggested by the reviewer, we conducted additional experiments to see whether different reward head type (regression) and loss (L1 and L2) can solve the reward prediction problem.\n\nIn our experiments on RoboDesk, both L1 and L2 losses with a typical MLP reward head miss many sparse reward signals, similar to the results with DreamerV3\u2019s two-hot categorical regression. Furthermore, these reward modeling approaches also showed improved performances with DreamSmooth. We summarized these results in the updated paper (Figure 17, Appendix E).\n\nIn summary, we found that the reward prediction problem exists in diverse commonly used regression models and losses, and DreamSmooth effectively alleviates this problem.\n\n\n&nbsp;\n\n**[Q] The need to experiment with 3 different reward smoothing schemes each with its own hyperparameters**\n\nThank you for pointing this out. We agree that it could be more convincing with the use of consistent hyperparameters across tasks. \n\nWe have updated the results (Figure 8) in our experiments with the same hyperparameters across all tasks and the hyperparameters can be found in Table 1: $\\sigma=3$ for Gaussian smoothing, $\\alpha=0.3, 0.33$ for EMA smoothing, and $\\delta=9$ for Uniform smoothing. Our reward smoothing methods show consistent results across tasks and we could observe that the effect of the smoothing hyperparameters are marginal.\n\n&nbsp;\n\n**[Q] The rationale behind why the counter-intuitive results in crafter is not convincing.**\n\nWe have extended our analysis on Crafter in Appendix G. After carefully inspecting reward prediction of unsmoothed and smoothed rewards on Crafter, we found that Gaussian and uniform smoothing results in **more false positives** while achieving better prediction accuracy, as shown in Figure 20. We suspect this could be due to predicting rewards for the unknown future states. Asymmetric variants of the uniform smoothing kernel (Uniform [-4, 0] and Uniform [0, 4]) in Figure 21 clearly show that reward smoothing kernels dependent on future rewards are prone to predicting positive rewards even when the ground truth reward is 0.\n\nDreamSmooth\u2019s high rate of false positives can be one reason for the poor performance on Crafter despite its high precision and recall on reward prediction. It is still surprising how DreamerV3 could perform well while missing most of the reward predictions. We leave this investigation as future work.\n\n&nbsp;\n\n---\n\n&nbsp;\n\nWe thank the reviewer again for the time and effort put into improving our paper. Please feel free to let us know if there are any additional concerns or questions."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1586/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700466986385,
                "cdate": 1700466986385,
                "tmdate": 1700466986385,
                "mdate": 1700466986385,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uS6RnsaKXH",
                "forum": "GruDNzQ4ux",
                "replyto": "hHCUI0cRlO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1586/Reviewer_7VX8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1586/Reviewer_7VX8"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your reply and further analysis!! The reviewers have addressed most of my questions and concerns... I maintain my postiive opinion of the paper!"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1586/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700578744101,
                "cdate": 1700578744101,
                "tmdate": 1700578744101,
                "mdate": 1700578744101,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "buQga83YGt",
            "forum": "GruDNzQ4ux",
            "replyto": "GruDNzQ4ux",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1586/Reviewer_BjnM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1586/Reviewer_BjnM"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces DreamSmooth, a simple and effective method that improves the performance of model-based RL on sparse reward environments. The authors observe that on sparse reward tasks, it is challenging to fit an accurate reward model due to data imbalance. This in turn bottlenecks the performance of model-based RL methods like Dreamer. To mitigate this problem, the authors apply a smoothing function to the reward, effectively spreading the reward signal to adjacent states in the trajectory. The authors propose three reward smoothing functions: Gaussian, Uniform, and EMA. While EMA is the only function that guarantees policy invariance, all three are empirically found to work well, resulting in more accurate reward predictions and higher performance in sparse reward tasks compared to baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Reward sparsity is a long-standing problem in model-based RL. Even with an accurate dynamics model, policy optimization would not work if the reward model fails to capture sparse reward signals.\n- The method is extremely simple, with only a one-line change to the Dreamer code, yet it brings a significant improvement across a suite of challenging sparse reward tasks.\n- The authors performed extensive analysis and ablation studies to identify the root cause of MBRL failure and demonstrate the effectiveness of their algorithm."
                },
                "weaknesses": {
                    "value": "- While reward prediction error is indeed one consequence of reward sparsity, the fundamental challenge that comes with sparse rewards is exploration. If there is no reward signal in the first place, then reward smoothing does not work either. While this paper provides a simple remedy to alleviate reward sparsity, it does not address the fundamental exploration problem.\n- Two of the smoothing functions are unable to guarantee policy invariance, and it is possible to construct adversarial examples (see questions)."
                },
                "questions": {
                    "value": "- It seems that reward smoothing can potentially lead to reward ambiguity. For example, if a bad state is visited right after a successful state vs. after a sequence of bad states, it would get assigned different smoothed reward values. How does reward ambiguity affect MBRL methods? I suspect the recurrent architecture of Dreamer helps mitigate this issue. To verify, can you run a Markovian MBRL method like MBPO with reward smoothing and see if there's any improvement there?\n- There are inductive biases built into each smoothing function. For example, Gaussian and Uniform smoothing functions assume symmetry. However, this may not align with the environment dynamics. Consider a ball rolling off a staircase and receiving a sparse reward right at the edge of the staircase. The smoothing function bumps up the reward of the states before and after falling off the staircase, but in practice, it is much harder to climb back up from the lower platform than to roll down. In other words, reward smoothing can give rise to overoptimistic behaviors. Do you see this reflected in any task?\n- Does reward smoothing benefit model-free methods such as policy gradient?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1586/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698822050870,
            "cdate": 1698822050870,
            "tmdate": 1699636087230,
            "mdate": 1699636087230,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YlGr62Hrau",
                "forum": "GruDNzQ4ux",
                "replyto": "buQga83YGt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1586/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1586/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer\u2019s thorough and constructive feedback about our paper. We provide detailed answers to the questions below.\n\n&nbsp;\n\n**[Q] the fundamental challenge that comes with sparse rewards is exploration. \u2026 it does not address the fundamental exploration problem.**\n\nAs the reviewer pointed out, our paper is **not** tackling all the challenges in solving sparse-reward tasks. Instead, we try to bring attention to one of the challenges in sparse-reward tasks, **the reward prediction problem** in model-based RL, which can happen even when exploration is not very difficult. For example, vanilla DreamerV3 could get sparse rewards in all our experiments but fails to correctly predict these sparse rewards and ends up with poor final performance. We made this point clearer in the abstract and introduction of the revised paper by removing some \u201csparse reward\u201d.\n\n&nbsp;\n\n\n**[Q] reward smoothing can give rise to overoptimistic behaviors. Do you see this reflected in any task?**\n\nOur reward smoothing will assign some positive reward to a bad state after a good state. This is not a big problem because of the following reasons. \n\n* After reward smoothing, the bad state (A) after the good state (B) can be assigned with some positive reward. But, if there is a better state (C) after (B) then (C) will get a larger reward than (A) after reward smoothing. Thus, an RL policy will learn to move toward the state with relatively larger original rewards, which will be also larger after smoothing.\n\n* As the reviewer mentioned, the recurrent state space model used in Dreamer helps discern bad states followed after good states. In the staircase example, the rewards will be bumped up right after falling off the staircase but this reward doesn\u2019t last long since the past history tells an agent that it is far away from a rewarding state. \n\nMany of our experiments reflect this property: once an agent gets a sparse reward, coming back to the same state does not give additional rewards.\n\n&nbsp;\n\n\n**[Q] How does reward ambiguity affect MBRL methods? \u2026 can you run a Markovian MBRL method like MBPO with reward smoothing and see if there's any improvement there?**\n\nThank you for an interesting suggestion! We conducted additional experiments with a Markovian model-based RL method, MBPO [1], on the state-based environments, Hand and RoboDesk. Unfortunately, MBPO failed to learn any of these tasks, so we could not verify whether reward smoothing improves the performance or not. We will further investigate this in an easier environment in the camera-ready version. \n\nOn the dense-reward DMC tasks, DreamSmooth does not hurt the Markovian MBRL method, MBPO, as shown in Appendix, Section F and Figure 18.\n\n&nbsp;\n\n\n**[Q] Does reward smoothing benefit model-free methods?**\n\nThe benefit of our reward smoothing approach is easier reward model learning. Thus, our approach does not help model-free methods, which do not learn to predict rewards.\n\n&nbsp;\n\n\n---\n&nbsp;\n\n\nWe hope our response addresses all your concerns and questions. Please let us know if there are any additional concerns or questions.\n\n&nbsp;\n\n\n\n[1] Janner et al. \u201cWhen to Trust Your Model: Model-Based Policy Optimization\u201d NeurIPS 2019"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1586/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700466912222,
                "cdate": 1700466912222,
                "tmdate": 1700466912222,
                "mdate": 1700466912222,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HdXLu5xOaq",
                "forum": "GruDNzQ4ux",
                "replyto": "YlGr62Hrau",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1586/Reviewer_BjnM"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1586/Reviewer_BjnM"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for addressing my comments. I have the feeling that the effectiveness of reward smoothing hinges on the recurrent structure of Dreamer. This feels like overfitting to the Dreamer line of work and can be included in e.g. Dreamer V4. I would really like to see the broader impact that reward smoothing has, for example, on model-based RL with Markovian models."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1586/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700516800778,
                "cdate": 1700516800778,
                "tmdate": 1700516800778,
                "mdate": 1700516800778,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "aN8uYm4f6f",
            "forum": "GruDNzQ4ux",
            "replyto": "GruDNzQ4ux",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1586/Reviewer_4wgT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1586/Reviewer_4wgT"
            ],
            "content": {
                "summary": {
                    "value": "In model-based reinforcement learning (MBRL), it is crucial to correctly estimate the reward model. However, when the rewards in the environment are sparse, it poses challenges in learning the reward function. The authors have shown convincing examples that the algorithm may achieve a smaller loss by simply predicting zero rewards, than predicing the sparse rewards at an incorrect time step.\n\nThis work remedies this problem by asking the algorithm to learn a smooth reward function. The proposed algorithm is based on the DreamerV3 algorithm and evaluated on a wide range of tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This work proposes a simple yet effective approach to improve an MBRL agent\u2019s performance in environments with sparse rewards. The algorithm is evaluated on simulated robotic control, 2D navigation, and Atari game domains. The authors also conducted ablation studies to show that this approach outperforms some other baseline algorithms to address the sparse reward issue, including oversampling sequences with sparse rewards, increasing reward model size, etc.\n\nAdditionally, this work also empirically verifies the challenges of reward learning in MBRL, finding out that the agent may achieve a smaller loss by predicting zero rewards than by predicting wrong rewards."
                },
                "weaknesses": {
                    "value": "**Novelty.** This is more like an engineering trick that the community has considered as an ad-hoc approach to resolve to learn in environments with sparse rewards. Although this smoothing technique intuitively makes sense, I didn\u2019t see justifications for the correctness of this approach. See Question 1 below.\n\n**Baseline method.** DreamV3 is the only baseline method for almost all the tasks, except that TD-MPC is used for the Hand Task. Unless this work only considers robotic tasks, other more popular RL algorithms need to be included. Also, if this work is indeed only constrained to robotic tasks, I believe the authors need to make that clear in the paper, and also explain why this simple technique cannot be applied to other RL algorithms."
                },
                "questions": {
                    "value": "1. When we change the reward function, is the agent\u2019s policy guaranteed to have a high value under the original reward function?\nWhen sparse rewards indeed specify critical states that have high rewards, would a smooth reward function blur out the true critical states, so that the optimal policy does not visit the critical states?\n\n2. Is there any rationale for using DreamV3 as the baseline for most tasks?\n\n---\n\nMy questions and concerns about weaknesses are addressed in the rebuttal."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1586/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1586/Reviewer_4wgT",
                        "ICLR.cc/2024/Conference/Submission1586/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1586/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698826491943,
            "cdate": 1698826491943,
            "tmdate": 1700546596214,
            "mdate": 1700546596214,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jLqKM8lTDw",
                "forum": "GruDNzQ4ux",
                "replyto": "aN8uYm4f6f",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1586/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1586/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer\u2019s thorough and constructive feedback about our paper. We provide detailed answers to the questions below.\n\n&nbsp;\n\n**[Q] This is more like an engineering trick\u2026**\n\nWe would like to highlight that the major contribution of this paper is to bring attention to reward modeling in MBRL, which has been largely overlooked and not actively discussed in the field. We provide empirical evidence that shows the importance of accurate reward modeling and the effectiveness of our simple approach. \n\nIn deep learning, many engineering tricks have been playing a critical role despite their simplicity. We believe it\u2019s worth discussing such engineering tricks, which suggests a new perspective or problem to the community; and we believe our paper does the same.\n\n&nbsp;\n\n**[Q] Is there any rationale for using DreamerV3 as the baseline for most tasks?**\n\nWe mainly used DreamerV3 since it is the **only** model-based RL method that consistently works on a variety of tasks with high-dimensional inputs, sparse rewards, and long episode lengths. In Figure 9 of the original submission, TD-MPC could learn the Hand task and TD-MPC w/ DreamSmooth significantly improved TD-MPC. But, TD-MPC failed to learn other sparse-reward tasks.\n\nWe also conducted additional experiments with another model-based RL method, MBPO [1], on the state-based environments, Hand and RoboDesk. However, MBPO couldn\u2019t learn any of these tasks. We included these results in Section F and Figure 18. \n\n&nbsp;\n\n**[Q] I didn\u2019t see justifications for the correctness of this approach. \u2026 would a smooth reward function blur out the true critical states, so that the optimal policy does not visit the critical states?**\n\nWe would like to refer the reviewer to Theorem A.1 in Appendix A, which shows that the proposed EMA reward smoothing guarantees an optimal policy. This requires a policy to access the history of states and the recurrent state structures used in many model-based RL methods enable a policy under smoothed rewards to hold Theorem A.1. \n\nIn the case when reward smoothing involves future rewards, Theorem A.2 explains that an optimal policy under any of our reward smoothing functions is also optimal under the original reward function. This still does not guarantee that MBRL can achieve optimal performance with reward smoothing; but, in practice, blurring reward signals a few steps before and after does not completely change learning dynamics since a policy learns from a value function (or MC value estimate in TD-MPC), which already blurs out the rewards on the critical states.\n\n&nbsp;\n\n**[Q] if this work is indeed only constrained to robotic tasks, I believe the authors need to make that clear in the paper\u2026**\n\nThis work is *not* specifically designed for the robotic domain. Our paper includes the most popular RL benchmarks, Atari and DMC. Moreover, Crafter and Atari benchmarks in our experiments are not robotic tasks. \n\nWe used many robotic environments since these tasks have challenging properties of partial observability and sparse reward, which makes reward prediction difficult. We are happy to try our method on other non-robotic RL benchmarks with similar properties.\n\n&nbsp;\n\n---\n\n&nbsp;\n\nWe hope our response addresses all your concerns and questions. Please let us know if there are any additional concerns or questions.\n\n&nbsp;\n\n[1] Janner et al. \u201cWhen to Trust Your Model: Model-Based Policy Optimization\u201d NeurIPS 2019"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1586/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700466722412,
                "cdate": 1700466722412,
                "tmdate": 1700466722412,
                "mdate": 1700466722412,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ndS4tXMlLr",
                "forum": "GruDNzQ4ux",
                "replyto": "jLqKM8lTDw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1586/Reviewer_4wgT"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1586/Reviewer_4wgT"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your response"
                    },
                    "comment": {
                        "value": "Thanks for your responses and the additional results. You have mostly addressed my concerns:\n\n* There are indeed theoretical justifications for EMA (based on reward shaping).\n* I agree that \"engineering tricks\" are not necessarily negative, as long as there are theoretical justifications and being evaluated on a wide range of tasks.\n* The evaluation domains include two tasks that are not robotic tasks -- Craft and Atari.\n\nI still have a question about Theorem A.1 and A.2. The authors claimed that \"there is no theoretical guarantee\" for smoothing functions that require access to future rewards. However, Theorem A.2 claims that a broader class of reward smoothing functions (expressed as Eq. 6), which can have access to future rewards, can also preserve optimal policies. What kind of theoretical guarantees does A.2 provide?"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1586/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700521204574,
                "cdate": 1700521204574,
                "tmdate": 1700521204574,
                "mdate": 1700521204574,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SpeZrRQlIo",
                "forum": "GruDNzQ4ux",
                "replyto": "WaSQMnfcFi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1586/Reviewer_4wgT"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1586/Reviewer_4wgT"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the clarifications"
                    },
                    "comment": {
                        "value": "Thanks for the clarifications! Indeed, the new MDP after reward smoothing is not conventional as the reward of a state depends on the states before and after it in a trajectory. It may be helpful to understand the theoretical implications on optimal policy perservence in this kind of MDPs, which may not be within the scope of this paper though.\n\nI have increased my score."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1586/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700547735566,
                "cdate": 1700547735566,
                "tmdate": 1700547735566,
                "mdate": 1700547735566,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]