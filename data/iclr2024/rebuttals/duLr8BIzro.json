[
    {
        "title": "A Fast and Effective Alternative to Graph Transformers"
    },
    {
        "review": {
            "id": "vEiSYLJ5Y2",
            "forum": "duLr8BIzro",
            "replyto": "duLr8BIzro",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6167/Reviewer_Utmf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6167/Reviewer_Utmf"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on a good question, i.e., the scalability of Graph Transformers (GTs). GTs suffer from quadratic complexity when the node number in a certain graph is very large. The authors propose a somewhat improvement of GTs. An improved (the authors define it as an alternative) dense attention mechanism is utilized to reduce the computing complexity of GTs. It is claimed that the proposed GECO can capture long-range dependencies. The proposed method shows some improvements in a limit number of datasets. The appendix is of a lot content including details of experimental settings, related work, a brief discussion of computational complexity discussion, etc."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "-This paper focuses on a good and significant research question and poses a very smart improvement by modifying dense attention mechanisms of GTs.\n\n-This paper includes rich contents. An additional appendix containing many details that can help to clearly understand the paper.\n\n-The proposed method shows some acceptable experimental results when comparing with baselines and good ablation experiments."
                },
                "weaknesses": {
                    "value": "-The innovation is not strong enough, which diminishes the significance of the paper. Essentially, the authors replaced the multi-head self-attention module in the original Transformer with a global convolutional module, and then they claim their proposal is to improve efficiency. However, the necessity of this replacement needs to be considered, and it appears to be of limited significance. The primary issue lies in the absence of self-attention mechanism, resulting in a diminished capability to capture long-range dependencies. Experimental results on large datasets, such as PCQM and COCO, indicate that the model's performance is inferior to other Graph Transformer methods.\n\n-Lacking experimental results to verify \u201cfast\u201d of the proposed method. Specifically, there is no emphasis in the experimental results, no parameter complexity analysis, no comparison of computing resource consumption or computing time. These are fundamental experiments in verifying \u201cfast\u201d of a certain method. And the results provided to demonstrate the 'effectiveness' of the proposed method in capturing long-distance dependencies, as shown in Tables 1, 2, and 4, may not offer sufficiently strong evidence for its superior performance. Overall, the title of this paper is ambitious and likely to capture attention with insufficient innovative approach, even though the authors claimed \u201cthey are the first to\u201d.\n\n-The design, organization, and writing of this paper are not very clear to me. Firstly, the motivation seems to enhance GT, but the authors care a lot about capturing long-range dependencies, which I have illustrated in last point, the results are not impressive enough. If the authors want to show the outperformance of trade-off between capturing long-range dependencies and fast calculation/computation, there is a lack of comparison of baselines including those methods not using Transformers. Then, If the authors want to show the improvement of the enhanced GT in effectiveness, the results are not competitive. And I think the authors also need to refer to some recent studies such as \u201cHierarchical Transformer for Scalable Graph Learning\u201d. Next, the authors aim to illustrate that their proposed method is fast and has distinct difference from GraphGPS. But why GraphGPS? It confuses me."
                },
                "questions": {
                    "value": "-What exact problem the authors want to solve? And how you directly verified that the problem is well solved, with what metric/way? \n\n-How to balance the trade-off between fast and efficiency? Why is your method the best?\n\n-I am well aware that the comparison of computation complexity (theoretically) among several models including GECO. But what about the experimental verification?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6167/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6167/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6167/Reviewer_Utmf"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6167/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698322533370,
            "cdate": 1698322533370,
            "tmdate": 1700635655757,
            "mdate": 1700635655757,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KzMJStcDls",
                "forum": "duLr8BIzro",
                "replyto": "vEiSYLJ5Y2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6167/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6167/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Many thanks for a thorough and insightful review. Below, we address each concern, organized to avoid repetitive discussions\n\n> @W3 \"Next, the authors aim to illustrate that their proposed method is fast and has distinct difference from GraphGPS. But why GraphGPS? It confuses me.\"\n\nWe understand your confusion. Let us clarify it by reviewing the literature chronologically.\n\n1. Graph Transformer has been proposed, suggesting that attention should only be applied to connected nodes, consequently utilizing the adjacency matrix as an attention mask [1].\n2. GraphTrans [2] and Graphormer [3] have been concurrently proposed, utilizing a self-attention mechanism, demonstrating that attention can be applied to the entire set of nodes and significantly improve predictive performance. Specifically, Graphormer achieved impressive results by incorporating several effective structural encodings.\n3. Many GTs have been proposed, many of which are approaches proposing different structural/positional encodings, extracting additional substructure tokens, or applying attention to specific substructures centered around a node, which we detail in our manuscript. Many of these approaches are orthogonal approaches and they utilize self-multi-head attention as their kernel.\n4. GraphGPS [4] has been proposed, combining GNN and attention modules side by side at each layer, offering recipes for off-the-shelf GNN, attention and positional/structural encodings combinations, and achieving state-of-the-art results across many graph datasets.\n5. **We proposed GECO as a compact layer consisting of local propagation and global convolution model blocks.** As both GECO and GraphGPS utilizes local and global blocks, we clarify their distinction to address potential reader questions. Unlike previous methods, in GECO, we designed a compact layer comprising local and global modules without intermediate parameters and non-linearities, applying skip connections to the layer as a whole. Please refer to our response to R-XpCe @W1 for more details. In addition, our work can complement other GT approaches focusing on various positional/structural encodings, or clustering/coarsening/substructure-based strategies.\n\n> @W3 \"If the authors want to show the outperformance of trade-off between capturing long-range dependencies and fast calculation/computation, there is a lack of comparison of baselines including those methods not using Transformers\"\n@W3 \"And I think the authors also need to refer to some recent studies such as \u201cHierarchical Transformer for Scalable Graph Learning\u201d\n\nWe have added the Hierarchical Transformer for Scalable Graph Learning (HSGT) [5] along with additional baselines from [5] to our revised manuscript. Below, we summarize the comparison between the two works on common datasets:\n\n\n| Model         | Flickr  | Yelp   | Arxiv   |\n|---------------|:------------:|:--------:|:----------------:|\n| Graphormer [5] | OOM | OOM | OOM | \n| Graphormer-SAMPLE [5] | 51.93 \u00b1 0.21 | 60.01 \u00b1 0.45  | 70.43 \u00b1 0.20  | \n| SAN [5] |  OOM | OOM | OOM | \n| SAT [5] |  OOM | OOM | OOM | \n| GraphGPS (Transformer) |  OOM | OOM | OOM | \n| SAT-SAMPLE [5] | 50.48 \u00b1 0.34 | 60.32 \u00b1 0.65 | 68.20 \u00b1 0.46  |\n| Exphormer [6]| NA | NA | 72.44 \u00b1 0.28 |\n| ANS-GT [5] | NA | NA | 68.20 \u00b1 0.46|\n| HSGT [5] | 54.12 \u00b1 0.51        | **63.47 \u00b1 0.45**          | 72.58 \u00b1 0.31             |\n| GECO   | **55.55 \u00b1 0.25**         | 63.18 \u00b1 0.59        | **73.10 \u00b1 0.24**           |\n\n- **HSGT and GECO are orthogonal approaches.** HSGT's attention components could be swapped with GECO's for a hierarchical graph learning model using global convolutions, offering a potential future direction. GECO provides kernel-wise scalability in place of self-multi-head attention, while HSGT achieves structural scalability through graph partitioning and coarsening.\n- **The comparison shows competitive performance:** GECO outperforms HSGT on Flickr and Arxiv, while HSGT surpasses GECO on Yelp.\n- **GECO vs Dense GTs (Graphormer/Graphormer-Sample/GraphGPS)**: While Graphormer and GraphGPS, which rely on self-attention, encounters out-of-memory issues in every scenario, GECO effectively scales to these datasets. Furthermore, Graphormer with sampling falls short in achieving competitive quality when compared to GECO across all datasets.\n\nAdditionally, Reddit dataset comes in multiple variants. In our study, we used the variant originally proposed in [7] available in PyG [8]. However, we noticed that HSGT uses a different variant. Below, we summarize the difference.\n\n\n| Dataset       |      N   (Ours)    |    M  (Ours)   |    N  (HSGT [5])    |    M (HSGT [5])  | \n|---------------|:------------:|:--------:|:----------------:|:----------------:|\n| Reddit        |  233K     |114.6 M|  233K |  11.6M |"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6167/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700200098186,
                "cdate": 1700200098186,
                "tmdate": 1700200098186,
                "mdate": 1700200098186,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "06y9hkPXqc",
                "forum": "duLr8BIzro",
                "replyto": "vEiSYLJ5Y2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6167/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6167/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">@W1 \" However, the necessity of this replacement needs to be considered, and it appears to be of limited significance. The primary issue lies in the absence of self-attention mechanism, resulting in a diminished capability to capture long-range dependencies.\"\n@W1 \"Experimental results on large datasets, such as PCQM and COCO, indicate that the model's performance is inferior to other Graph Transformer methods.\"\n@W2 \"And the results provided to demonstrate the 'effectiveness' of the proposed method in capturing long-distance dependencies, as shown in Tables 1, 2, and 4, may not offer sufficiently strong evidence for its superior performance.\"\n@W3 \"Firstly, the motivation seems to enhance GT, but the authors care a lot about capturing long-range dependencies, which I have illustrated in last point, the results are not impressive enough.\"\n@W3 \"If the authors want to show the improvement of the enhanced GT in effectiveness, the results are not competitive.\"\n\nWe understand that the reviewer's main concern lies around understanding capability of GECO as a replacement for self-attention.\n\nFirst and foremost, we emphasize that if the concern is to make a direct comparison between self-attention and GECO without using heuristics, one should compare the GECO and GraphGPS rows for a fair comparison in Tables 1, 2 and 3. Next, in the table below, we summarize (1) relative improvements w.r.t. GraphGPS's self attention and (2) relative improvements w.r.t. best GT baselines including orthogonal approaches (with available results).\n\nWe highlight empirical evidence showing that replacing self-attention with GECO does not diminish performance on most datasets; in fact, it often leads to improvements:\n\n- **GECO has superior predictive quality on 8 out of 10 datasets w.r.t. GraphGPS with self-attention (Table 1, 2 and 3).**  Specifically, on the first five columns (long-range benchmark datasets), GECO achieves relative improvements of 12.53%, 6.31%, 1.44%, and 5.77%, with a -2.20% decrease in only on COCO-SP. For datasets in columns 6-10, GECO achieves significant relative improvements of by up to 10.36% on 4 datasets, with only a -0.41% decrease only on ogbg-ppa. \n\n\n- **GECO achieves superior quality to Dense GTs utilizing self attention (Graphormer/Graphormer-sample/GraphGPS) on 12 out of 14 datasets (Table 1, 2, 3 and 4).**  Please see table below and also refer to previous discussion and its table. Empirical evidence further supports that GECO does not have diminishing capability as a replacement GT's self-attention; in fact, it leads to improvements.\n\n- **GECO has superior predictive quality on 9 out of 14 datasets w.r.t. across available GT Baselines including orthotogonal ones(Table 1, 2, 3 and 4).**  Please see below.\n\n\n| Model           | PascalVOC-SP | COCO-SP | Peptides-func| Peptides-struct| PCQM-Contact  |ogbg-molhiv  | ogbg-molpcba  | ogbg-ppa| ogbg-code2 | PCQM4Mv2 | Flickr | Reddit | Yelp | Arxiv |\n|-----------------|-------------------------|--------------------|--------------------|------------------------|---------------------|-----------------------|------------------------------|----------------------|------------------------|------------------------|--------------------|--------------------|------------------------|------------------------|\n|| F1 score \u2191 | F1 score \u2191 | AP \u2191 |  MAE \u2193 | MRR \u2191 |AUROC \u2191 |  Avg. Precision \u2191 |Accuracy \u2191 | F1 score \u2191 | MAE \u2193  |Accuracy \u2191 |Accuracy \u2191|Accuracy \u2191|Accuracy \u2191|\n| GraphGPS             | 0.3748 \u00b1 0.0109         | 0.3412 \u00b1 0.0044    | 0.6535 \u00b1 0.0041    | 0.2500 \u00b1 0.0005        | 0.3337 \u00b1 0.0006     |0.7880 \u00b1 0.0101       | 0.2907 \u00b1 0.0028              | 0.8015 \u00b1 0.0033      | 0.1894 \u00b1 0.0024        | 0.0938     | OOM | OOM | OOM | OOM |\n| Best GT  (Model Name)  | 0.3975 \u00b1 0.0037 (Exphormer)         | 0.3455 \u00b1 0.0009 (Exphormer)    | 0.6535 \u00b1 0.0041 (GraphGPS)    | 0.2481 \u00b1 0.0007 (Exphormer)       | 0.3637 \u00b1 0.0020 (Exphormer)     |  0.7880 \u00b1 0.0101 (GraphGPS)       |  0.2907 \u00b1 0.0028 (GraphGPS)              | 0.8015 \u00b1 0.0033 (GraphGPS)      | 0.1937 \u00b1 0.0028 (K-Subtree SAT)      | 0.0858 (GraphGPS)   | 54.12 \u00b1 0.51 (HSGT) | NA | 63.47 \u00b1 0.45 (HSGT) | 72.58 \u00b1 0.31 (HSGT) |\n| GECO (Ours)    | 0.4210 \u00b1 0.0080         | 0.3320 \u00b1 0.0032    | 0.6975 \u00b1 0.0025    | 0.2464 \u00b1 0.0009        | 0.3526 \u00b1 0.0016     | 0.7980 \u00b1 0.0200       | 0.2961 \u00b1 0.0008              | 0.7982 \u00b1 0.0042      | 0.1915 \u00b1 0.002         | 0.08413     | 55.55 \u00b1 0.25 | 96.65 \u00b1 0.05 | 63.18 \u00b1 0.59 | 73.10 \u00b1 0.24 |\n| Relative Improvement w.r.t. GraphGPS with self-attention (%)|  12.53%    |  -2.20%  | 6.31% | 1.44%   | 5.78%  | 1.27% |  1.85%   | -0.41% |  1.11%  |  10.36%  | NA | NA | NA | NA |\n| Relative Improvement w.r.t. Best GT Baseline (%)|  5.91%    |  -3.91%  | 6.31% | 0.69%   | -3.05%  | 1.27% |  1.85%   | -0.41% |  -1.36%  |  10.36%  | 2.64% | NA | -0.46 | %0.72 |\n\nPlease refer to the *Dataset Clarification* on the general discussion regards to definition of large graphs in our context."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6167/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700200134978,
                "cdate": 1700200134978,
                "tmdate": 1700200134978,
                "mdate": 1700200134978,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vIh0SYWmx5",
                "forum": "duLr8BIzro",
                "replyto": "vEiSYLJ5Y2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6167/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6167/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> @W2 \"Lacking experimental results to verify \u201cfast\u201d of the proposed method. Specifically, there is no emphasis in the experimental results, no parameter complexity analysis, no comparison of computing resource consumption or computing time. These are fundamental experiments in verifying \u201cfast\u201d of a certain method.\" \n@W2 \"Overall, the title of this paper is ambitious and likely to capture attention with insufficient innovative approach, even though the authors claimed \u201cthey are the first to\u201d.\n@Q3 \"I am well aware that the comparison of computation complexity (theoretically) among several models including GECO. But what about the experimental verification?\"\n\nWe have taken this concern to heart, recognizing its significance, particularly given the title of our paper, \"fast\". Please see the general discussion and revised manuscript.\n\nAdditionally, we imposed the same parameter budget constraints as previous research did for the datasets listed in Tables 1, 2, and 3. We included these details in the revised manuscript. Table 3 demonstrates that GECO outperforms the GraphGPS self-attention variant by a significant margin, using 8 times fewer parameters on PCQM4Mv2. When both models use the same number of parameters, GECO achieves a 12.53% relative improvement over GraphGPS. Furthermore, GECO's parameter usage is comparable to that of some traditional GNN architectures.\n\n> @Q1 \"What exact problem the authors want to solve? And how you directly verified that the problem is well solved, with what metric/way?\"\n\nOur primary motivation is to replace computationally intensive self-attention mechanism within graph transformers with a more efficient operator, while not sacrificing the prediction quality. To this end, we designed a compact layer combining local propagation and global convolutions. Our evaluation includes 3 steps to verify our approach:\n\n1. **Verify competitive quality on commonly used Graph Transformer Datasets**: Please see discussions and tables above.\n\n2. **Verify scalability and quality to larger node prediction datasets**: Please see discussions and tables above.\n\n3. **Verify Runtime Efficiency**: In our revised version, we have included runtime efficiency benchmarks. We appreciate reviewers' feedbacks for encouraging us to include this study.\n\n\n> @Q2 \"How to balance the trade-off between fast and efficiency? Why is your method the best?\"\n\nThere are multiple ways to enhance the scalability and efficiency of Graph Transformers, such as coarsening (HSGT [5]), expander graphs (Exphormer [6]) or sampling strategies [5]. Our work, GECO, takes another direction by directly replacing the self-attention kernel. Unlike prior attempts that compromised predictive quality with self-attention approximations [4], GECO maintains robust performance while improving scalability. Moreover, its design is orthogonal with existing scalability methods like HSGT, offering a promising future research direction. Please also see the discussion above.\n\n> @W1 \"The innovation is not strong enough, which diminishes the significance of the paper.\"\n\nPlease refer to general discussion.\n\n[1] A Generalization of Transformer Networks to Graphs, AAAI Workshop'21\n\n[2] Representing Long-Range Context for Graph Neural Networks with Global Attention, NeurIPS'21\n\n[3] Do transformers really perform badly for graph representation?, NeurIPS'21\n\n[4] GraphGPS: General Powerful Scalable Graph Transformers, NeurIPS'22\n\n[5] Hierarchical Transformer for Scalable Graph Learning, IJCAI'23\n\n[6] EXPHORMER: Sparse Transformers for Graphs, ICML'23\n\n[7] Inductive Representation Learning on Large Graphs, NeurIPS'17\n\n[8] Fast Graph Representation Learning with PyTorch Geometric, ICLR Workshop'19, [Reddit Dataset Link](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.datasets.Reddit.html#torch_geometric.datasets.Reddit)"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6167/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700200174287,
                "cdate": 1700200174287,
                "tmdate": 1700200174287,
                "mdate": 1700200174287,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zU2ihSXEWT",
                "forum": "duLr8BIzro",
                "replyto": "vIh0SYWmx5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6167/Reviewer_Utmf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6167/Reviewer_Utmf"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the authors' efforts in expanding the experimental results, which have helped to address several of my concerns, such as runtime efficiency, and less # parameters. To reflect this, I have revised my score to 5."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6167/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700635625544,
                "cdate": 1700635625544,
                "tmdate": 1700635625544,
                "mdate": 1700635625544,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "sjDuoUS62P",
            "forum": "duLr8BIzro",
            "replyto": "duLr8BIzro",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6167/Reviewer_vPHt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6167/Reviewer_vPHt"
            ],
            "content": {
                "summary": {
                    "value": "This paper targets the quadratic complexity issue in training graph transformers with full-attention over large graph datasets, and proposes GECO, which is a Hyena-based operator that captures both local and global dependencies to replace the original attention operator. The authors conduct extensive experiments in demonstrating GECO\u2019s effectiveness over long-range and large graph datasets. In addition, the authors empirically demonstrate GECO\u2019s insensitivity to node ordering wrt. performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tThe targeted quadratic complexity issue reside in graph transformers is meaningful. The designed GECO not only makes it sub-quadratic, but also remains in a considerable performance level.\n2.\tThe experiments are conducted extensively. The results seem promising."
                },
                "weaknesses": {
                    "value": "1.\tThe presentation of this paper may be improved for coherence. For example, in Sec. 3.3, the GCB module is designed/modified based on Hyena, the key component for sub-quadratic complexity. The authors may want to include a short description of it in the main context rather than the appendix. Otherwise, it may introduce difficulties in comprehension. In addition, the proposition in the main context assists in analyzing the complexity, which is presented in the appendix. It seems like they can be excluded from the main context.\n2.\tThe motivation is to make the model parameters sub-quadratic to the number of nodes. While theoretical analysis is conducted, I would like to see empirical results (e.g., training time) in GECO\u2019s training efficiency compared with other baselines."
                },
                "questions": {
                    "value": "1.\tIn the Graph-to-sequence conversion part, the authors state \u201ctime-correlated sequences, aligning node IDs with time (t)\u201d. Where does the \u2018time\u2019 come from? What does it mean?\n2.\tThe LCB module conducts neighborhood information propagation for each node. It directly utilizes the connectivity information via adjacency matrix. In the meantime, GECO is implicitly learning this \u2018connectivity\u2019 via the convolutional filters. Is there any information overlap here?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6167/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6167/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6167/Reviewer_vPHt"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6167/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698546422362,
            "cdate": 1698546422362,
            "tmdate": 1699636669679,
            "mdate": 1699636669679,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Dy4Brjjdny",
                "forum": "duLr8BIzro",
                "replyto": "sjDuoUS62P",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6167/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6167/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for a thorough and insightful review.  Below we address the raised questions and concerns.\n\n\n> @W1 \"The presentation of this paper may be improved for coherence. For example, in Sec. 3.3, the GCB module is designed/modified based on Hyena, the key component for sub-quadratic complexity. The authors may want to include a short description of it in the main context rather than the appendix. Otherwise, it may introduce difficulties in comprehension. In addition, the proposition in the main context assists in analyzing the complexity, which is presented in the appendix. It seems like they can be excluded from the main context.\"\n\nThanks for your suggestion. We have reorganized the text as suggested in order to improve clarity and comprehension. Please see the revised version of the manuscript.\n\n> @W2 \"The motivation is to make the model parameters sub-quadratic to the number of nodes. While theoretical analysis is conducted, I would like to see empirical results (e.g., training time) in GECO\u2019s training efficiency compared with other baselines.\"\n\nThank you for your suggestion. Please see the general rebuttal above.\n\n> @Q1 \"In the Graph-to-sequence conversion part, the authors state \u201ctime-correlated sequences, aligning node IDs with time (t)\u201d. Where does the \u2018time\u2019 come from? What does it mean?\"\n\nThe original operator was initially designed for sequence models, where the input has a specific order correlated with position t. In this context, two consecutive tokens represents time points t and (t + 1). We aim to clarify how this concept applies to our scenario. As we explained in Section 3.4, our approach involves initially permuting the graph with a specific ordering. Then, we treat these ordered vertices as a sequence.\n\n> @Q2 \"The LCB module conducts neighborhood information propagation for each node. It directly utilizes the connectivity information via adjacency matrix. In the meantime, GECO is implicitly learning this \u2018connectivity\u2019 via the convolutional filters. Is there any information overlap here?\"\n\nYes, there is a slight information overlap. LCB block tells us where to pay attention using adjacency matrix, however if adjacency lacks information due to missing/spurious edges, GCB compliments it by enabling information flow from the rest of the vertices.\n\nThe ablation study at Table 5 can be seen as an empirical support for our motivation to use LCB and GCB together. For clarity, we will use the relative rows of Table 5 below:\n\n| Model         | PascalVOC-SP   | Peptides-func   | Peptides-struct   |\n| ------------- | -------------- | --------------- | ----------------- |\n|               | F1 score \u2191     | AP \u2191            | MAE \u2193             |\n| GECO (Conv-1)| 0.2752         | 0.6589          | 0.2587            |\n| GECO (LCB)   | **0.3220**         | **0.6876**          | **0.2454**            |\n\nAbove, Conv-1 refers to an alternative approach in GECO where LCB is substituted with a 1D-Convolution having a filter size of 1, essentially replacing LCB with an identity function.  This scenario represents the case where we do not utilize the adjacency matrix and explicitly learn connectivity information using only GCB. Notably, when LCB is employed, we observe significant improvements in predictive performance across all datasets."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6167/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700200037379,
                "cdate": 1700200037379,
                "tmdate": 1700200037379,
                "mdate": 1700200037379,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xVM6os04a2",
                "forum": "duLr8BIzro",
                "replyto": "sjDuoUS62P",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6167/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6167/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Kindly Reminder"
                    },
                    "comment": {
                        "value": "Dear Reviewer vPHt,\n\nWe wanted to follow up to see if the response and revisions have addressed your concerns. We would be happy to provide further clarifications and revisions if you have any more questions. If not, we would greatly appreciate it if you would reevaluate our paper. Thank you again for your reviews, which have helped improve our paper!"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6167/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700678465966,
                "cdate": 1700678465966,
                "tmdate": 1700678465966,
                "mdate": 1700678465966,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "RavV926M4o",
            "forum": "duLr8BIzro",
            "replyto": "duLr8BIzro",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6167/Reviewer_XpCe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6167/Reviewer_XpCe"
            ],
            "content": {
                "summary": {
                    "value": "This article proposes a new operator-GECO to replace the graph transformer to solve the computational complexity problem of MHA (multi-head attention) on large-scale graphs. GECO introduces the Hyena architecture into graph convolution calculations, using a combination of long convolutions and gating to compute local and global context. Subsequent experiments have proven that GECO can ensure accuracy while reducing time complexity, on large-scale and small-scale data sets. The main contributions of the article are 1. There is no trade-off between quality and scalability while ensuring both; 2. It confirms that the Hyena architecture can replace MHA in graph neural networks, and global context can improve the performance of GNN."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "A new operator-GECO to replace the graph transformer to solve the computational complexity problem of MHA (multi-head attention) on large-scale graphs."
                },
                "weaknesses": {
                    "value": "- The technical contribution is limited. According to  this survey [2], the proposed LCB module can be treated as the GNN-as-Auxiliary-Modules in the Alternatively form (Figure 1 in [2]). Additionally, the writing of this paper seems rushed. Many details are missing and hard to understand. For example, in Algorithm 1, line 3, what is $V_t \\leftarrow (P)_t FFTConv(F_i, V)_t$. Actually, I found more details of this algorithm in Algorithm 3,  page 8, [link](https://arxiv.org/pdf/2302.10866.pdf)[1]. The forward pass of GCB Operator is nearly identical to Hyena, which is not new. \n\n- Using positional  embedding to encode the graph structural information is not new. \n\n- The paper claims that the proposed model is \"fast\", and provides detailed time complexity analysis. Unfortunately, from the theoretical perspective, GECO has the same level complexity as Message-passing GNN $O(NlogN+M)$ and it can only surpass vallia transformer when $M<<N^2$. Additionally, no experiments regarding the running time efficiency are presented. \n\n- It's necessary make more comparisons with more baselines of Graph Transformer. Please refer to [2] for more baselines. \n\n[1] Hyena Hierarchy: Towards Larger Convolutional Language Models\n[2] Transformer for Graphs: An Overview from Architecture Perspective"
                },
                "questions": {
                    "value": "See weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None."
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6167/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698904934592,
            "cdate": 1698904934592,
            "tmdate": 1699636669572,
            "mdate": 1699636669572,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LF2IL4Gel7",
                "forum": "duLr8BIzro",
                "replyto": "RavV926M4o",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6167/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6167/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for an insightful review. Below we address each of the individual concerns.\n\n> @W1 \"The technical contribution is limited. According to this survey [2], the proposed LCB module can be treated as the GNN-as-Auxiliary-Modules in the Alternatively form (Figure 1 in [2]).\"\n\nThanks for pointing out the related work which we included in the revised manuscripts.\n\nApproaches combining of-the-shelf GNNs and attention mechanisms were already mentioned in our submission. In GraphTrans [3] (Before Trans type in [2]), GNN and Transformer blocks are applied sequentially, while GraphGPS [4] (Parallel type in [2]) employs them in parallel at each layer. It is worth noting that these models straightforwardly combine pre-existing GNN and Transformer models, resulting in separate parameters and intermediate non-linearities for each module, with skip connections applied independently.\n\n**GECO does not precisely align with the taxonomy defined in [2].** In GECO, we did not just use LCB as an auxiliary module to Transformer. Instead, we designed a new compact layer comprising local and global blocks. We eliminated the intermediate non-linearities and parameters to reduce the overall number of parameters, simplifying the model. We applied skip connections to the entire GECO layer as a whole, rather than separately. These deliberate design choices distinguish GECO from the use of off-the-shelf methods.\n\nMesh Graphormer [5], the only related work categorized under 'Alternatively form' in [2], similarly diverges from GECO regarding intermediate non-linearities, parameters, and skip connections, as discussed above. It also differs in the order of local and global modules, with Mesh Graphormer placing attention before the GNN block, whereas in GECO, local follows global.\n\n\n> @W2 \"Additionally, the writing of this paper seems rushed. Many details are missing and hard to understand. For example, in Algorithm 1, line 3, what is V_t \\leftarrow (P)_t FFTConv(F_i, V)_t.\"\n\nIn Section 3.3's last paragraph, we detail Algorithm 1's logic and aim to enhance clarity with pseudocode comments. The GCB operator conducts global convolutions sequentially for projections and filters, gating output with subsequent results. P_i is for projections, F_i for filters, and V for values in pseudocode comments. FFTConv represents FFT Convolutions. We added more clarifications to the pseudocode in the revised manuscript.\n\nWe have put genuine effort into creating a comprehensive manuscript that includes details on related work, background, pseudocode, implementation, and dataset information, with appropriate citations for further details. If there are particular details you would like us to provide, please let us know."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6167/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700199589261,
                "cdate": 1700199589261,
                "tmdate": 1700204988636,
                "mdate": 1700204988636,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "g7eznswsli",
                "forum": "duLr8BIzro",
                "replyto": "RavV926M4o",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6167/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6167/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> @W2 \"Actually, I found more details of this algorithm in Algorithm 3, page 8, link[1]. The forward pass of GCB Operator is nearly identical to Hyena, which is not new.\"\n\nWe list the two design innovations of our work in our previous response to @W1 and also in the general rebuttal.\n\nThe last paragraph of Section 3.3 already emphasizes that we adapt and modify the notation used in Hyena for Algorithm 1 with proper citation. Our novelty for GCB is centered on the design of the convolution filters and their utilization for graph domain. We list 4 main design points for GCB: (1) Graph-to-Sequence conversion, (2) Bi-directional information flow, (3) Graph-aware context, and (4) Window of the global convolution. On the other hand, Algorithm 1 (or Alg. 3 [1]) is a generic pseudocode for models following the Hyena architecture. Below, we highlight some differences in the global convolution pseudocodes using comments labeled as \"Difference:\"\n\n```\nAlgorithm 1 [2]: Projection\nInput: - Input sequence u in R^(L x D)\n\n1. In parallel across L: \\hat{z} = Linear(u), Linear: R^D -> R^((N+1)D)\n2. In parallel across D: z = DepthwiseConv1d(h, \\hat{z}) # ----> Difference 1: Removed 1D conv as position under given node ordering, does not imply promixity. Refer to Section 3.3.\n3. Reshape and split z into x^1, ..., x^N, v. Dimensions of one element are x^n in R^(D x L)\nReturn x^1, ..., x^N, v, x^n\n```\n\n\n```\nAlgorithm 2 [1]: HyenaFilter\nInput: - Sequence length L, positional embedding dimension D_e # \n1. t = PositionalEncoding(L), t in R^(L x D_e)\n2. In parallel across N, L: \\hat{h} = FFN(t), FFN: R^D_e -> R^(N D), \\hat{h} in R^(L x N D)\n3. Reshape to \\hat{h} in R^(N x D x L)\n4. h = \\hat{h} * Window(t), h in R^(N x D x L) #  ----> Difference 2: Removed decay, we need to treat all nodes equally, regardless of their distance under given ordering. Refer to Section 3.3.\n5. Split h into h^1, ..., h^N\nReturn h^1, ..., h^N\n```\n\n\n```\nAlgorithm 3 [1]: Hyena Operator\nInput: Input sequence u in R^(L x D), order N, model width D, sequence length L, positional embedding dimension D_e \n#  ----> Difference 3: Set L = N, to ensure global convolution encapsulates all nodes. Refer to Section 3.3.\n\n1. x^1, ..., x^N, v = Projection(u)\n2. h^1, ..., h^N = HyenaFilter(L, D_e)\n\nFor n = 1,...,N\n    3. In parallel across D: v_t <- x^n_t * FFTConv(h^n, v)_t  ----> Difference 4: # Removed causality to enable bi-directional information blow between vertices. Refer to Section 3.3.\nReturn y = v\n```\n\n```\nAlgorithm 5 (Ours): Forward GECO Operator\nInput: - Adjacency matrix Adj in R^(N x N) - Node embeddings X in R^(N x d)\n\n1. X = BN( Propagate(X, Adj) ) #  ----> Difference 5: Incorporated LCB to capture local dependencies. Refer to Section 3.3.\n#  ----> Difference 6: We apply normalization after local block\n2. X = GCB(X, Adj)\nReturn X\n```\n\nTo state that our method is \"identical\" would suggest that the off-the-shelf Hyena is on par with its graph adaptation counterpart, GCB. However, this is not the case. To reinforce our argument, we conducted the ablation study below. In this study, we employed off-the-shelf Hyena, with one modification \u2013 setting the window size as the entire graph (Difference 3). The results demonstrate that GECO consistently outperforms off-the-shelf Hyena, which exhibits significant quality declines across all datasets. We added these results to the revised manuscript as well.\n\n\n| Model         | Flickr  | Yelp   | Reddit | Arxiv   |\n|---------------|:------------:|:--------:|:--------:|:----------------:|\n| Off-the-shelf Hyena [1] |   46.97 \u00b1 0.08  |  50.08 \u00b1 0.31 | 69.24 \u00b1 0.54  | 56.04 \u00b1 0.61| \n| GECO   |  **55.55 \u00b1 0.25**          | **63.18 \u00b1 0.59**     | **96.65 \u00b1 0.05**   | **73.10 \u00b1 0.24**           |\n| Relative Improvement (%)   | 18.24% | 26.11% | 39.57% | 30.39% |\n\n\nPlease further refer to ablation study at Table 5, showing that omitting some elements of design choice 2 (Difference 1 & 6), \"Graph-aware context,\" has also significant effects, reducing quality by up to 55%.\n\n> @W3 \"Using positional embedding to encode the graph structural information is not new.\"\n\nWe agree. This is not new, consequently we do not claim any novelty with the usage of positional/structural encodings within our framework. In Section 3.1 we note that we follow the foundational work laid out by the literature. However, we need to provide the reader necessary information on how to train our model end-to-end including the usage of positional/structural encodings."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6167/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700199762782,
                "cdate": 1700199762782,
                "tmdate": 1700205118008,
                "mdate": 1700205118008,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LrB4DhvmLT",
                "forum": "duLr8BIzro",
                "replyto": "RavV926M4o",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6167/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6167/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> @W4 \"The paper claims that the proposed model is \"fast\", and provides detailed time complexity analysis.  Unfortunately, from the theoretical perspective, GECO has the same level complexity as Message-passing GNN  and it can only surpass vallia transformer when M << N^2\"\n\n**M << N^2 always holds for real-world datasets.** This property is the main reason why we need sparse computations to train GNNs. To put in more concrete form, we provide a table where we explicitly calculate M/N^2 for each dataset we use. Specifically, large node prediction, the graphs are even sparser. We also added these columns to the revised version of the manuscript.\n\n| Dataset       |      N       |    M     |    M / N^2       |\n|---------------|:------------:|:--------:|:----------------:|\n| PCQM-Contact  |     30.1     |   61.0   | 6.79 x 10^-2      |\n| Peptides-func |    150.9     |  307.3   | 1.36 x 10^-2      |\n| Peptides-struct|   150.9     |  307.3   | 1.36 x 10^-2      |\n| COCO-SP       |    476.9     | 2,693.7  | 1.20 x 10^-2      |\n| PascalVOC-SP  |    479.4     | 2,710.5  | 1.20 x 10^-2      |\n| PCQM4Mv2      |     14.1     |   14.6   | 7.25 x 10^-2      |\n| Molhiv        |     25.5     |   27.5   | 4.29 x 10^-2      |\n| Molpcba       |     26.0     |   28.1   | 4.13 x 10^-2      |\n| Code2         |    125.2     |  124.2   | 1.59 x 10^-2      |\n| PPA           |    243.4     | 2,266.1  | 3.25 x 10^-2      |\n| Flickr        |   89,250     | 899,756  | 1.12 x 10^-4      |\n| ogbn-arxiv    |  169,343     | 1,166,243| 3.97 x 10^-5      |\n| Reddit        |  232,965     |114,615,892| 1.95 x 10^-6     |\n| Yelp          |  716,847     |13,954,819| 2.26 x 10^-6      |\n\nAdditionally, while our approach entails higher complexity than MPNN, it exhibits near-linear complexity which is a significant advancement compared to graph transformers with dense attention, all the while having a robust predictive quality, as demonstrated through Table 1-4.\n\n> @W4 \"Additionally, no experiments regarding the running time efficiency are presented.\"\n\nThank you for your suggestion. Please see the general rebuttal above."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6167/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700199967363,
                "cdate": 1700199967363,
                "tmdate": 1700237166154,
                "mdate": 1700237166154,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CljhhflWip",
                "forum": "duLr8BIzro",
                "replyto": "RavV926M4o",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6167/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6167/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> @W5 \"It's necessary make more comparisons with more baselines of Graph Transformer. Please refer to [2] for more baselines.\"\n\nThanks for pointing out the related work. Below, we present a comparison with [2], selecting the best variant for each model/dataset configuration. Our comparison covers common datasets used by both works, encompassing all datasets in [2], except for two.\n\nWe highlight that GECO consistently outperforms the models in [2] across all datasets. Specifically, on arxiv and molphcha, GECO achieves significant relative improvements of up to 28.11% and 11.23%, respectively. We also note that, some approaches discussed in [2] and reported below such as positional encoding (PE) and improved attention matrices from graphs (AT) are orthogonal approaches to our work. We have also incorporated these comparisons in the revised version of the manuscript.\n\n|      |                   | molhiv | molpcba | Flickr | ogbn-arxiv |\n|------|-------------------|--------|---------|--------|------------|\n|      |                   |  ROC-AUC\u2191|   AP\u2191   |   Acc\u2191 |   Acc\u2191     |\n| TF   | vanilla           |  0.7466 | 0.1676  | 0.5279 | 0.5598     |\n| GA   | before            | 0.7339 | 0.2269  | 0.5369 | 0.5614     |\n|      | alter             | 0.7433 | 0.2474  | 0.5374 | 0.5599     |\n|      | parallel          | 0.7750 | 0.2444  | 0.5379 | 0.5647     |\n| PE   | degree            |  0.7506 | 0.1672  | 0.5291 | 0.5618     |\n|      | eig               | 0.7407 | 0.2194  | 0.5278 | 0.5658     |\n|      | svd               | 0.7350 | 0.1767  | 0.5317 | 0.5706     |\n| AT   | SPB               |  0.7589 | 0.2621  | 0.5368 | 0.5605     |\n|      | PMA               |  0.7314 | 0.2518  | 0.5288 | 0.5571     |\n|      | Mask-1            | 0.7960 | 0.2662  | 0.5300 | 0.5598     |\n|      | Mask-n            |  0.7423 | 0.2619  | 0.5359 | 0.5603     |\n| GECO  (Ours) |   | **0.7980 \u00b1 0.0200**       | **0.2961 \u00b1 0.0008**               | **0.5555 \u00b1 0.0025**           | **0.7310 \u00b1 0.0024**   |\n\n\nAdditionally, directly quoting from [2] is two of the three feature directions highlighted in the survey, which perfectly align with our motivations:\n\n1. \"**New paradigm of incorporating the graph and the Transformer**: Most studies treat the graphs as strong prior Transformer model. There is a great interest to develop the new paradigm that not just takes graphs as a prior, but also better reflects the properties of graphs.\" [2]\n2. \"**Extending to large-scale graphs.** Most existing methods are designed for small graphs, which might be computationally infeasible for large graphs. As illustrated in our experiments, directly applying them to the sampled subgraphs would impair performance. Therefore, designing salable Graph-Transformer architecture is essential.\" [2]\n\nMoreover, Table 1 in [2] lists various models, many of which our evaluation already covers, including well-known methods like Graphormer, GraphiT, EGT, SAN, and GraphTrans. Our submission also included additional models not in the list, such as GraphGPS and Exphormer. In our revised version, we incorporated more baselines from [6], as suggested by R-vPHt. If there is a particular comparison you would like us to provide, please let us know.\n\n[1] Hyena Hierarchy: Towards Larger Convolutional Language Models, ICML'2023\n\n[2] Transformer for Graphs: An Overview from Architecture Perspective, Arxiv'2022\n\n[3] Do transformers really perform badly for graph representation?, NeurIPS'21\n\n[4] GraphGPS: General Powerful Scalable Graph Transformers, NeurIPS'22\n\n[5] Mesh Graphormer, ICCV'2021\n\n[6] Hierarchical Transformer for Scalable Graph Learning, IJCAI'23"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6167/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700199979325,
                "cdate": 1700199979325,
                "tmdate": 1700200002310,
                "mdate": 1700200002310,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "m7n080fXl9",
                "forum": "duLr8BIzro",
                "replyto": "RavV926M4o",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6167/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6167/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Kindly Reminder"
                    },
                    "comment": {
                        "value": "Dear Reviewer XpCe,\n\nWe wanted to follow up to see if the response and revisions have addressed your concerns. We would be happy to provide further clarifications and revisions if you have any more questions. If not, we would greatly appreciate it if you would reevaluate our paper. Thank you again for your reviews, which have helped improve our paper!"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6167/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700678426932,
                "cdate": 1700678426932,
                "tmdate": 1700678426932,
                "mdate": 1700678426932,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]