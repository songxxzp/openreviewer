[
    {
        "title": "Making Predictors More Reliable with Selective Recalibration"
    },
    {
        "review": {
            "id": "RtdiFNS7os",
            "forum": "0kvrymILfy",
            "replyto": "0kvrymILfy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6404/Reviewer_RXXm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6404/Reviewer_RXXm"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an extension to a recent line of work on calibrated selective classification (Fisch et al., 2022), which consists of selectively predicting inputs so as to minimize calibration error. The authors' method jointly optimizes a selection function along with a recalibration model, so as to minimize calibration error on the selected subset of the data. They show that this approach can significantly outperform doing only selective prediction, and also show that it can outperform selection and recalibration done in sequence."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- **Originality:** The paper introduces an objective that combines selective classification and recalibration (S-TLBCE, Equation 8) and proves for a simple class of data distributions that the joint procedure can outperform each individual procedure. While the objective and analysis seem new, the practical advancement of this work is a straightforward extension of [1].\n- **Quality:** Overall the quality of the work is good, with the problem of selective calibration being well-described/well-motivated and experiments designed appropriately to evaluate the proposed method.\n- **Clarity:** The work is easy to follow, with the experiments and theory described with adequate detail for the most part.\n- **Significance:** Selective classification optimizing for calibration is relatively new, being (to the best of my knowledge) largely introduced in the recent (2022) work of [1]. This paper constitutes a natural extension of [1], and while the empirical results correspond to marked improvements in some regimes, I have some reservations regarding the overall contribution of the work in the context of [1].\n\n[1] https://arxiv.org/abs/2208.12084"
                },
                "weaknesses": {
                    "value": "## Weaknesses\n1. **Novelty/improvement of proposed approach.** The approach is conceptually a minor change to the methodology of [1]. While S-TLBCE seems like a new objective in this context, it does not perform better (in fact it even performs worse on ImageNet) than the jointly optimized version of S-MMCE from [1], except for the OOD tests on CIFAR-100-C. Furthermore, the table describing these latter experiments (Table 1) is unclear - the description claims to be reporting AUC over various coverage levels but only ECE-1 and ECE-2 are reported? Also, the naive confidence-based rejection strategy (which should be described in the main paper) performs very well on the ImageNet/Camelyon17 experiments - the authors say that this strategy falls apart in the OOD case, but is this considering confidence-based rejection with recalibration (in sequence)? \n\n## Recommendation\nOverall I think the paper is tackling an interesting and relatively new problem, but I feel the contribution is too marginal in its current form to warrant a clear acceptance. Furthermore, the experimental results do not seem to me to be convincingly better than the previous work of [1], and for this reason I recommend **weak reject**.\n\n[1] https://arxiv.org/abs/2208.12084"
                },
                "questions": {
                    "value": "- Several questions are stated in weaknesses above.\n- Figure 1 is difficult to understand. What subsets of the data do the blue and green curves correspond to? Shouldn't there only be one curve for 1(a) and 1(b) since no selection is being done for these? \n- The definition of R-ECE seems a bit strange to me; linearly rescaling the predicted *probabilities* by $T$ will not be the same as rescaling the predicted scores, so this is not exactly temperature scaling.\n- The ECE calculations in the appendix are hard to follow due to the presentation, improving spacing/detail would help here."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6404/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6404/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6404/Reviewer_RXXm"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6404/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698358635019,
            "cdate": 1698358635019,
            "tmdate": 1699636712057,
            "mdate": 1699636712057,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0fHSJaKLaR",
                "forum": "0kvrymILfy",
                "replyto": "RtdiFNS7os",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6404/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6404/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for taking the time to read and offer feedback on our work.  Please see our response to your concerns below.\n\n - Please see the response to all reviewers for an explanation of the novelty/performance improvement of our method as compared to [1].\n - S-TLBCE shows more consistent performance than S-MMCE.  See Table 1 CIFAR-100-C results for an example of S-MMCE failing, whereas S-TLBCE always improves on the recalibration baseline.\n - We are also going to include Brier Score results for all experiments in the next version of this paper.\n - Recalibration was applied in tandem with the confidence-based rejection strategy.\n\nWith respect to the contribution, once again we point to our comment to all reviewers above.  If the concern is performance in particular, then we believe that our algorithm is quite significant, since we show that considering recalibration along with selection is a necessary step in achieving the best selective calibration error, while the importance of recalibration is mostly ignored by [1]."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6404/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700068158014,
                "cdate": 1700068158014,
                "tmdate": 1700068158014,
                "mdate": 1700068158014,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mUCwZlcgur",
                "forum": "0kvrymILfy",
                "replyto": "0fHSJaKLaR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6404/Reviewer_RXXm"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6404/Reviewer_RXXm"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thank you for taking the time to respond to my comments.\n- I understand that the proposal of joint vs sequential makes a significant difference, and this is definitely valuable. My concern with novelty was more-so that, conditioned on joint optimization, the value of S-TLBCE over S-MMCE seems marginal. One way to improve this could be more exploration of the OOD setting where the gap seems to be more significant between the two, but I understand there is not a lot of time left to run those experiments during the review period.\n- My questions about Figure 1, as well as the definition of R-ECE remain."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6404/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700235325944,
                "cdate": 1700235325944,
                "tmdate": 1700235325944,
                "mdate": 1700235325944,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "RTBsbRsty1",
            "forum": "0kvrymILfy",
            "replyto": "0kvrymILfy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6404/Reviewer_6mVa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6404/Reviewer_6mVa"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to combine selective prediction and post-hoc calibration to achieve more reliable performance for classification tasks. The proposed optimization framework is based on another recent work which combines selective prediction and training-time calibration. In the proposed selective recalibration framework, a new loss function, Selective Top-Label Binary Cross Entropy (S-TLBCE) is proposed for training such a recalibrator and a selective model at the same time. The proposed approach is evaluated on real-world medical diagnosis datasets and image classification datasets, and the results show its effectiveness."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The idea of combining selective prediction and model calibration is very useful and realistic in many safety aware tasks. Also, previous work studied the combination of selective prediction and train-time calibration, the combination of selective prediction and recalibration seems more important as post-hoc calibration achieves better calibration performance generally.\n\n2. Except for the experiments on real-world datasets, the authors also give theoretic results based on a simple and intuitive data generation model."
                },
                "weaknesses": {
                    "value": "1. The technical novelty and overall contribution is quite limited, primarily because the combination of selective prediction and model calibration has been studied in previous work and the proposed method is a straightforward combination of existing optimization framework and recalibration model. \n\n2. The writing and organization of this paper are not good enough. Many places involving notations are confusing, for example, some loss functions appear in the Methodology section but are not used in the following optimization framework, and some notations are not defined before their first appearance.\n\n3. The empirical evaluation is also limited, as a result, the effectiveness and soundness cannot be sufficiently shown. For example, some benchmark datasets for image classification, which are commonly used in the context of model calibration like SVHN/CIFAR-10, are not used in experiment section. Moreover, the experimental part does not provide sufficient information about the dataset used and the way it was divided (for training and validation), as well as the model structure."
                },
                "questions": {
                    "value": "Please refer to the weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6404/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6404/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6404/Reviewer_6mVa"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6404/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698762857031,
            "cdate": 1698762857031,
            "tmdate": 1700623070423,
            "mdate": 1700623070423,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FH60qWjH4r",
                "forum": "0kvrymILfy",
                "replyto": "RTBsbRsty1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6404/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6404/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Rebuttal"
                    },
                    "comment": {
                        "value": "We appreciate your feedback.  Here we respond to the specific concerns that you have raised.\n\n - Please see the note to all reviewers for a detailed explanation of the differences with previous work.  We believe that the connection between selection and recalibration is important and unexplored, and that our algorithm results in major performance improvements over the approach taken in Fisch et al.\n - Could you please point to specific examples where the notation is confusing?  Also, which loss functions are introduced that are not used?  We appreciate the feedback on the presentation, but would please need more specific information in order to use it to improve the paper.\n - While we appreciate this suggestion with respect to evaluation, we strongly disagree here.  We produced results for 10-15 models (including baselines) across 2 important settings with 2 datasets each.  While we could include an MLP recalibrator and significance testing as suggested by the other reviewers, we do not believe that the current evaluation is limited.  Additionally, we think that the datasets that we explored are much better than SVHN/CIFAR-10.  These are typically treated as toy datasets at this point in the evolution of deep learning, and are much less challenging than either image classification dataset that we used (CIFAR-100, ImageNet).  We also believe that our inclusion of the medical tasks (Camelyon, RxRx1) is much more important, as we contend that this method is appropriate for the medical domain.  Since we are limited in space, switching these out for SVHN/CIFAR-10 would significantly weaken our results.  The validation and test sets are randomly drawn, we will be more explicit about this in an updated draft.  The model structure is described in the appendix, however we will consider whether more detail can be added there (or moved to the main paper)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6404/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700068064817,
                "cdate": 1700068064817,
                "tmdate": 1700074459384,
                "mdate": 1700074459384,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8RqWJEEs9E",
                "forum": "0kvrymILfy",
                "replyto": "FH60qWjH4r",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6404/Reviewer_6mVa"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6404/Reviewer_6mVa"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "Thanks for the response.\n\n- After reading the general response regarding the novelty and the distinctions from previous work, my concerns about the novelty and contribution remain. Given the work in [On Calibration of Modern Neural Networks, https://arxiv.org/abs/1706.04599], which suggest that recalibration can improve calibration performance with very large margin, applying recalibration on the Calibrated Selective Classification framework is too straightforward.\n\n\n- There are some notations like $\\lambda$ in Eq.(5), {$\\theta^*, \\epsilon, \\alpha$} in Definition 1 (also, indicating that their range would be better) are not defined before their first appearance. Eq. (5-8) introduce the overall loss function in a hierarchical structure. However, the representation of each term in the loss function is quite complex, which may lead readers to misunderstand whether a specific term is used in the optimization objective. I suggest that instead of dedicating separate sections for each term in the loss function, the authors can consider presenting them in a more integrated manner. For instance, coverage loss may not require its own section but could be introduced directly below Eq.(5).  Consequently, Eq.(5) could be revised as follows:\n\n$$\\qquad\\qquad\\min _{g, h} L _\\phi = \\frac{\\phi(f, \\hat{g}, h, x, y)}{\\frac{1}{n} \\sum_i \\hat{g}\\left(x_i\\right)} + \\left(\\beta-\\frac{1}{n} \\sum_i \\hat{g}\\left(x_i\\right)\\right)^2,$$\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;where the specific choice of $\\phi$ can be indicated in the the overall loss term $ L _\\phi$.\n\n- Furthermore, in $h^{\\text {Platt }}(f(x))=\\frac{1}{1+\\exp (w f(x)+b)}$, it seems that $w$ and $b$ should be a vector. Further clarification for this or distinguishing vectors from scalars by bolding can enhance the clarity of symbols.\n\n- I agree that the used datasets are more challenging, and the authors\u2018 argument about the use of additional space with other datasets weakening the results has convinced me.\n\nBased on the author's response, I raise my rating to 5."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6404/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700623055595,
                "cdate": 1700623055595,
                "tmdate": 1700623055595,
                "mdate": 1700623055595,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "k0RaK2854H",
            "forum": "0kvrymILfy",
            "replyto": "0kvrymILfy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6404/Reviewer_DcsN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6404/Reviewer_DcsN"
            ],
            "content": {
                "summary": {
                    "value": "This paper extends the selective calibration setting of Fisch et. al. (2022) to allow for joint training of both selection _and_ recalibration functions. Reliable confidence estimation is a key property for predictors operating in sensitive domains, but is not typically easy to achieve. Selective calibration combines \"selective prediction\" (i.e., allowing for abstention) with \"calibration\", in the sense that selectively calibrated predictors prioritize having calibrated predictions on the non-rejected population of inputs. Previous work by Fisch et. al. (2022) only considered optimizing the \"selection\" part given a fixed model. This paper follows the natural motivation of also training a recalibrator jointly, which is similar to previous work in standard selective classification such as SelectiveNet (Geifman and El-Yaniv, 2019). It is intuitive to see why such an approach would be a good idea for some input distributions and models (e.g., where the optimal calibrator across the full input space is not in the sigmoid family for Platt scaling, but is after selection). The authors also provide empirical and additional theoretical analysis by example that supports their joint design.\n\n[1] Calibrated Selective Classification. Adam Fisch, Tommi Jaakkola, Regina Barzilay. 2022.\n\n[2] SelectiveNet: A Deep Neural Network with an Integrated Reject Option. Yonatan Geifman, Ran El-Yaniv. 2019."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is well written and its proposed method is well-motivated. Though it is a fairly straightforward extension of prior work (Fisch et. al. + Geifman and El-Yaniv), it also presents additional contribution in the form of a simplified selective calibration loss (S-TLBCE) which appears to work better in practice. The empirical and theoretical arguments for justifying joint vs. sequential training of $(g, h)$ is also useful."
                },
                "weaknesses": {
                    "value": "I think that the paper does a good job at addressing the narrow question of improving selective calibration through joint selection _and_ recalibration. More broadly, I would have been excited to see a more nuanced approach to selective calibration (which Fisch et. al. also misses), in the sense that not all types of calibration error are necessarily equal. For example, under a selective recalibration framework, all of the following example rejections would be preferred (assuming that they don't have any other structure that makes recalibration easier post selection):\n\n- reject(Predictions with confidence 95% but accuracy 90%) > reject(predictions with confidence 4% but accuracy 0%)\n- reject(Predictions with confidence 80% but accuracy 100%) > reject(predictions with confidence 80% but accuracy 61%)\n\nObviously it depends on the application, but generally speaking it seems that rejecting the top-performing half of predictions simply because their confidences are slightly too high is not a useful strategy. Similarly for under- vs. over-confidence. Considering other calibration objectives (e.g., Decision Calibration from Zhao et. al., 2021) that are more expressive than ECE-1/ECE-2 could add significantly to this work's potential impact. \n\nI'm also a bit worried about the data requirement for selective recalibration to work. If labeled examples from the target domain are not available (e.g., following the setting of Fisch et. al.), can the method still produce reliable rejections? Does recalibration on the training domain help here?\n\nMinor formatting points:\n- It would be helpful to keep equation numbering for all display equations.\n- Unnumbered equation for $h^\\mathrm{Temp}$ would benefit from \\left \\right parentheses.\n\n[3] Calibrating Predictions to Decisions: A Novel Approach to Multi-Class Calibration. Shengjia Zhao, Michael P. Kim, Roshni Sahoo, Tengyu Ma, Stefano Ermon. 2021."
                },
                "questions": {
                    "value": "- Figure 1 is a bit confusing to me. It's not easy to see what strategy the selective and selective recalibration algorithms take. The caption could use some additional clarification. For example, what is the desired coverage level? It seems like the basic strategy that is followed is to reject either the blue or the green (selective vs. selective recalibration, respectively). Though I would still expect a selective only approach to reject the mid-confidence blue examples disproportionately more than the upper/lower ranges (since this is where calibration error is highest per the reliability diagram).\n\n- It makes sense that jointly learning selection and recalibration can help when the calibration error is too complex to be fit by the family of recalibration functions specified by Platt or temperature scaling. I'd be curious to see how this would compare to simply fitting a slightly more expressive family of calibrators (e.g., 2-layer NN), especially for data without distribution shifts.\n\n- This is more of a half-baked suggestion than a question, but I'm curious if the authors ever experimented with the following setup. Suppose we modeled recalibration as a hard mixture of experts, where $h(x) = \\sum_{i = 1}^{n} g_i(x) h_i(x)$ with $g(x)$ being 1-hot. On new distributions $p(x')$, one could \"turn off\" the lowest performing $g_i$ via rejection until $\\sum_{i \\in \\text{rejected}} \\mathbb{E}(g_i(x')) = \\beta$. This is a generalization of the current setup with $g(x)$ being binary, and only training one $h(x)$ (as the other examples are discarded). On new distributions, however, dynamically reconfiguring this mixture (which will have been trained to fit subdomains in the training data) could prove to be effective, without the need for much new data to jointly recalibrate $h$."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None."
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6404/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698785942096,
            "cdate": 1698785942096,
            "tmdate": 1699636711456,
            "mdate": 1699636711456,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oN9FTQKHV7",
                "forum": "0kvrymILfy",
                "replyto": "k0RaK2854H",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6404/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6404/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for your positive feedback.  Below we respond to the particular questions that you raised. \n\n - While it is only meant to be illustrative/high-level, we agree that Figure 1 is still a bit unclear.  The idea is that the desired coverage level is equal to the proportion of the data represented by the blue group.  Your suggestion regarding selection and the mid-confidence blue examples vs. upper/lower ranges is extremely helpful, and we will make this change.\n - We agree that the expressive family of calibrators is a missing comparison, as we may be able to use the extra parameters for recalibration instead of selection.  We plan to implement and include this baseline in a future version of the work.\n - Thank you for this suggestion regarding a MoE approach.  We have not explored this, but it sounds like it could be a very promising direction and we plan to explore it further in the future."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6404/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700067925735,
                "cdate": 1700067925735,
                "tmdate": 1700067925735,
                "mdate": 1700067925735,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "F7B7ZYm3DT",
                "forum": "0kvrymILfy",
                "replyto": "oN9FTQKHV7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6404/Reviewer_DcsN"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6404/Reviewer_DcsN"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the reply to my comments and questions. Just checking: were you planning to upload a new revision with those changes?"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6404/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700674735872,
                "cdate": 1700674735872,
                "tmdate": 1700674735872,
                "mdate": 1700674735872,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3fMuLbTsrT",
            "forum": "0kvrymILfy",
            "replyto": "0kvrymILfy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6404/Reviewer_NrpW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6404/Reviewer_NrpW"
            ],
            "content": {
                "summary": {
                    "value": "The submission proposes selective recalibration, which combines ideas from selective classification with calibration. The proposed method builds on work by Fisch et al. (2022) by optimizing rejection and calibration jointly. Experiments show that this yields better calibration for a given level of coverage than standard out-of-distribution detection methods applied separately to standard calibration methods. It also performs better in most cases than the \"sequential\" alternative to the joint optimization approach. Performance is evaluated for both i.i.d. settings and settings where an existing pretrained model is used in a new domain, without fine-tuning this model, by performing selective recalibration. There are also some theoretical results for a particular synthetic domain showing that better performance can be achieved with joint optimization."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The proposed approach makes sense, and it is plausible that joint optimization performs better than the alternatives considered."
                },
                "weaknesses": {
                    "value": "One important baseline that is missing in the experiments is a calibration approach that is as complex as the multi-layer perceptron used in the proposed approach to perform selection. Platt scaling and temperature scaling, the calibration methods used in the experiments, have very few parameters, and discretization-based approaches are also very simple. The argument in the submission is that selective calibration can be beneficial because it may not be possible to achieve good fit for the calibration model to the entire data. However, an obvious approach to tackle this problem is to simply make the calibration model more complex: rather than using a linear logistic regression model as in Platt scaling, one can use a multi-layer perceptron instead.\n\nI am also wondering about the sequential baseline used. A single epoch of calibration is performed before the selection model is trained. It is unclear why a single epoch is used. Also, it seems that improved performance could trivially be obtained by recalibrating again after the selection model has been trained. (This could be iterated, but that would probably be very similar to joint optimisation using gradient descent.)\n\nNo significance testing is performed and no confidence intervals are provided for estimated performance measures.\n\nThe first result in Section C.2 is disturbing: classification accuracy goes down with decreased coverage when applying the proposed method. It is unclear to me whether one would ever accept this in practical applications.\n\nOther comments and typos:\n\n\"whether an instance is correctly classifier\"                                   \n                                                                                \n\"outputs, We\"                                                                   \n                                                                                \nSection B.2.1: what is $\\tilde f$?                                              \n                                                                                \nSection B.4.2 does not mention validation data."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6404/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698830678405,
            "cdate": 1698830678405,
            "tmdate": 1699636711275,
            "mdate": 1699636711275,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jhE2CeI6gu",
                "forum": "0kvrymILfy",
                "replyto": "3fMuLbTsrT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6404/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6404/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for your careful feedback.  Below we respond to the individual concerns that you have raised.\n\n- We appreciate the suggestion with respect to the MLP recalibration baseline, to see if the performance gain comes from simply having more parameters.  We plan to implement this and include the comparison in a final paper.\n- With respect to the single epoch of pretraining, this was a poor explanation on our part, which we plan to correct.  The Platt/temperature scaling models are optimized in the typical manner on the full validation set prior to the training of g.\n- We agree that significance testing should be included.  We will run more trials with random validation and test splits in order to better characterize the performance differences.\n- While we agree that it would be ideal if accuracy and calibration could improve together, we believe that in many important cases they are separate concerns, and classification accuracy is often not the primary concern.  For example, in other research we are working to develop a medical monitoring system to be deployed in ICU\u2019s that predicts the probability of a serious condition developing.  This system will not output 0/1 predictions, but only calibrated risk scores.  We believe that many such human-in-the-loop decision scenarios exist, and that our algorithm can be extremely useful in those settings.  We will however add further exposition of this dynamic in a future version of the paper."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6404/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700067800429,
                "cdate": 1700067800429,
                "tmdate": 1700067800429,
                "mdate": 1700067800429,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6tdCb59dN4",
                "forum": "0kvrymILfy",
                "replyto": "jhE2CeI6gu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6404/Reviewer_NrpW"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6404/Reviewer_NrpW"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors' comments"
                    },
                    "comment": {
                        "value": "Thank you very much for your response. However, to change my recommendation, at the very least, I would need to see the results of the suggested baseline as evidence that the increased number of parameters is not the reason for the observed improvements in performance."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6404/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700700698469,
                "cdate": 1700700698469,
                "tmdate": 1700700698469,
                "mdate": 1700700698469,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DtmjWrBmOV",
                "forum": "0kvrymILfy",
                "replyto": "STAiBRVp82",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6404/Reviewer_NrpW"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6404/Reviewer_NrpW"
                ],
                "content": {
                    "title": {
                        "value": "New response"
                    },
                    "comment": {
                        "value": "Matrix scaling is not the same as using a multi-layer perceptron though. It seems conceivable that a hidden layer with nonlinearities is the key, so it does seem that a direct comparison to this is needed."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6404/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700732643034,
                "cdate": 1700732643034,
                "tmdate": 1700732643034,
                "mdate": 1700732643034,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "a07tomjxBI",
                "forum": "0kvrymILfy",
                "replyto": "3fMuLbTsrT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6404/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6404/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response"
                    },
                    "comment": {
                        "value": "Thank you, we agree that the MLP is technically different than matrix scaling.  We would also ask that you consider [2], wherein an extra set of parameters are used for a domain selector that infers a datapoint's domain and chooses a temperature for scaling.  Comparison is only made to typical temperature scaling.  We believe that our 4 recalibration baselines are very strong.\n\n[2] Yaodong Yu, Stephen Bates, Yi Ma, Michael Jordan. Robust Calibration with Multi-domain Temperature Scaling. https://proceedings.neurips.cc/paper_files/paper/2022/file/b054fadf1ccd80b37d465f6082629934-Paper-Conference.pdf"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6404/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733808536,
                "cdate": 1700733808536,
                "tmdate": 1700734028377,
                "mdate": 1700734028377,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]