[
    {
        "title": "A Theoretical Analysis of In-context Task Retrieval and Learning"
    },
    {
        "review": {
            "id": "gQTt3ifXjk",
            "forum": "8fQlGQkj0S",
            "replyto": "8fQlGQkj0S",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3000/Reviewer_v8Fu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3000/Reviewer_v8Fu"
            ],
            "content": {
                "summary": {
                    "value": "They assume that the pretraining data and the downstream task data is generated from a Gaussian/Linear generation process.  They then theoretically analyze the Bayesian-optimal predictions in the in-context learning setting. They divide in-context learning for downstream tasks into two categories depending on the parameters of their generation processes: (1) task retrieval, and (2) task learning. They derive error bounds for these two categories. Their theoretical results show that having more in-context examples may hurt the performance of the model."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. A clear/rigorous/mathematical definition of two types of in-context learning (task retrieval and task learning) may be helpful for future works.\n2. They derive the error bounds for the given setting that decreases quadratically."
                },
                "weaknesses": {
                    "value": "Because Xie et al. (2022) has proposed to explain in-context learning with a latent variable model, I would expect new studies, if also adopt a latent variable model, to propose some refinement on the data generation process which should be more realistic. However, in my opinion, the date generation process in this work is not more realistic for the following reasons:\n\n1. Firstly, it\u2019s a gaussian model, so it\u2019s very different from the discrete case of NLP. Because it\u2019s not discrete, it is even less realistic than the HMM model used by Xie et al.\n2. Secondly, this work assumes that the generation process of the pretraining data is the same as the downstream task. Again, this is worse than Xie et al., because Xie et al. discuss the distribution mismatch problem between pretraining and downstream tasks at least to some extent.\n\nAdditionally, it\u2019s not clear to me what the main takeaway of this paper is. Indeed, the authors derive the error bounds for the two kinds of downstream tasks, however\n\n1. It\u2019s not clear to me how the definition of the two kinds of tasks is relevant in the real-world scenario.\n2. Empirically, we do not observe that having more examples hurt the performance.\n3. And again, it\u2019s not clear how the generation process is related to the real-world data."
                },
                "questions": {
                    "value": "I suggest the authors elaborate more on the implications of the bounds they prove."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3000/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697868221306,
            "cdate": 1697868221306,
            "tmdate": 1699636244714,
            "mdate": 1699636244714,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cl5EvS3ttG",
                "forum": "8fQlGQkj0S",
                "replyto": "gQTt3ifXjk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3000/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3000/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to express our gratitude to the reviewer for dedicating your valuable time to provide constructive feedback on our work. We hope our responses have addressed your concerns and answered your questions. If our responses have resolved your concerns, we kindly request you to consider increasing your score.\n___\n(1) \u201cI would expect new studies\u201d & \u201cIt\u2019s not clear to me what the main takeaway of this paper\u201d\n\nOur paper makes two major novel contributions.\n\nFirstly, we are the pioneering work that formally defines and analyzes task learning and retrieval. This can assist future researchers in reconciling seemingly conflicting observations made with in-context learning in the past, as they can be interpreted as non-conflicting results within the two different regimes.\n\nSecondly, we are the first to precisely predict two non-monotonic performance patterns of ICL in real-world LLMs as the number of samples increases: the U-shaped curve we predicted in the original submission, and the flipped one we presented in the rebuttal. The U-shaped curve was confirmed by our additional experiments. The flipped curve was initially presented in the original GPT3 paper (and later was re-confirmed by Xie et al.\u2019s work too!) and explained via experiment under our framework. Experiments are described in \u201csupplementary material\u201d.\n\nThese two contributions lead us to strongly believe that our paper offers substantial contributions to the research community, particularly theoretical analysis of ICL, and to some extent, practical LLM research.\n___\n(2) (paraphrased) \u201cdata generation process should be more realistic\u201d & \u201cIt\u2019s not clear how the generation process is related to the real-world data\u201d\n\nWe deliberately selected the linear regression mixture model, which, as the reviewer correctly pointed out, is slightly less realistic (non-discrete) than the HMM model. Our choice was motivated by the model's analytical tractability, which provides a precise theoretical understanding of how task retrieval and task learning operate. \n\nNumerous theoretical studies adopt the same linear regression mixture model inluding recent works  [1-5] (see these papers in \u201cTo AC and All Reviewers\u201d due to the character's length). Although the linear regression mixture model deviates from the actual pretraining data model, the insights it offers may still predict real-world phenomena.\n___\n(3) \u201cThe generation process of the pretraining data is the same as the downstream task\u201d\n\nWe do *not* make such an assumption. Although we assume that the pretraining task parameters are drawn from a probability distribution whose support spans the entire task parameter space, this does not imply that every task is included in a finite pretraining dataset. In fact, since our task parameters are continuous random variables, with probability 1, the downstream task is *not* included in a finite pretraining dataset.\n___\n(4) \u201cHow is the definition of the two kinds of tasks relevant in the real-world scenario?\u201d\n\nOur work is largely motivated by various observations made with real-world LLMs that appear to be conflicting. Here are the most prominent examples:\n\nPaper \"Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?\" demonstrates that in-context learning with random labels sometimes outperforms in-context learning with true labels. How can this be explained?\n\nThe GPT3 paper and the paper by Xie et al. both show that zero-shot performance is often higher than few-shot performance in various tasks. Notably, these findings are all based on ground truth labels. How can this be explained?\n\nFrom a more traditional supervised-learning perspective, these two observations seem counterintuitive. The general expectation is that the more samples provided, the better the performance should be. Indeed, most theoretical ICL papers assume this setting and prove that ICL performance should monotonically increase as the number of samples increases. We refer to this setting as \"task learning\".\n\nTo explain these phenomena, we introduce \"task retrieval\", which happens when in-context examples are drawn from a different task than the target task (This can also be viewed as a form of concept shift, using distribution shift terminology). This might happen with label noise or when there is a subtle mismatch between the way labels are created for in-context samples versus evaluation samples. In this regime, we can rigorously show that when the number of samples is relatively small, the performance may not be monotonic.\n___\n(5) \u201cEmpirically, we do not observe that having more examples hurt the performance.\u201d\n\nIn response to your insightful suggestion, we conducted experiments with GPT-4 and discovered that the U-shaped loss curve is observable in practice. The experiment arise described in \u201csupplemental material\u201d. Another related non-monotonic performance was reported in the original GPT-3 paper and reconfirmed by the paper by Xie et al. (please refer to Figure 7 therein)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3000/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700535309836,
                "cdate": 1700535309836,
                "tmdate": 1700535533777,
                "mdate": 1700535533777,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Lg29f675Bs",
            "forum": "8fQlGQkj0S",
            "replyto": "8fQlGQkj0S",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3000/Reviewer_ZoJM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3000/Reviewer_ZoJM"
            ],
            "content": {
                "summary": {
                    "value": "NLP tasks like LLMs can be better with in-context examples, and in-context learning (ICL) in the community approaches explanation of the area. In this work, authors summarize two modes of ICL - in-context task learning and in-context task retrieval, which are able to learn a new task or retrieve related tasks during pre-training LMs. In the past, theorists somehow ignore the importance of pre-training distribution. Hence, authors propose a data generative approach based on the distribution to explain those two modes."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. As a theoretical work, beyond upper bounds for risks of two modes, this work provides evidences based on numerical computations and conducts experiments with Transformer. That would be helpful for practitioners in the future work.\n2. Visual illustration like Figure 2 helps to understand the prior distribution."
                },
                "weaknesses": {
                    "value": "1. I observed that Lemma 1 has been used a lot in the manuscript and is lack of proof. Please at least provide high-level idea of the proofing before using it.\n2. Texts and figures are overlapped in Page 8.\n3. Similarly, for Theorem 3,4 and Lemma 5, please provide at least few sentences about proofing instead of pointing the appendix."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3000/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698617271335,
            "cdate": 1698617271335,
            "tmdate": 1699636244633,
            "mdate": 1699636244633,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "b26he0b1q0",
                "forum": "8fQlGQkj0S",
                "replyto": "Lg29f675Bs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3000/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3000/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your strong support of our work!\n\nWe also would like to express our sincere gratitude to the reviewer for dedicating your valuable time to provide constructive feedback on our work. We added experiments to show the U-shaped loss curve actually exists in real-world LLM (GPT-4) and our framework can also be used to explain a flipped U-shaped loss curve researchers have observed in real-world LLM. We share the experiments in \"supplementary material\". We kindly request you to consider increasing your score and championing our paper if these new experiments significantly improve the quality and confidence of this paper.\n___\n(1) \u201cHigh level proof idea for lemma, theory\u201d\n\nWe will incorporate additional sentences to describe the high-level idea behind the proof.\n___\n(2) \u201cTexts and figures are overlapped in Page 8\u201d\n\nThanks! fixed."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3000/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700537053358,
                "cdate": 1700537053358,
                "tmdate": 1700537053358,
                "mdate": 1700537053358,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "oo7JshtpeU",
            "forum": "8fQlGQkj0S",
            "replyto": "8fQlGQkj0S",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3000/Reviewer_KzaL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3000/Reviewer_KzaL"
            ],
            "content": {
                "summary": {
                    "value": "The authors study in context learning (ICL) for task retrieval and task learning. The ICL is modeled using a mixture of linear Gaussian tasks. Using component shifting and component reweighting they derive closed form expressions for posterior distribution given k observatios. This is leveraged to derive risk bounds for the two tasks, retrieval and learning, under squared loss. The theoretical results are backed up by numerical, and experimental results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The authors highlight the task retrieval and task learning aspects of ICL. \n- The authors provide risk bounds for both the setup under a mixture of linear Gaussian generative model."
                },
                "weaknesses": {
                    "value": "- The generative model is simplistic as it is limited to linear Gaussian mixture. In contrast HMM based ICL is already studied in other works already mentioned in the paper. The authors should explain the importance of studying this model. \n\n- The authors leave out some area of works that are related, e.g. meta-learning and retrieval augmented learning. A few recent examples of the latter are -  'A Statistical Perspective on Retrieval-Based Models' by Basu et al., 'Generalization and stability in in-context learning' Li  et al. \n\n- The dependence of U shaped risk bound on component reweighting and shifting is not discussed properly. Furtheremore, U shape is also observed through the bias-variance tradeoff in optimization literature. The authors don't connect the U shaped mention here with the bias-variance tradeoff. \n\n- The novelty in deriving the posterior distributions for the mixture of Gaussian distribution is unclear to me."
                },
                "questions": {
                    "value": "Please look at the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3000/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699372110356,
            "cdate": 1699372110356,
            "tmdate": 1699636244563,
            "mdate": 1699636244563,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7HsnDrNob3",
                "forum": "8fQlGQkj0S",
                "replyto": "oo7JshtpeU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3000/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3000/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to express our gratitude to the reviewer for dedicating your valuable time to provide constructive feedback on our work. We hope our responses have addressed your concerns and answered your questions. If our responses have resolved your concerns, we kindly request you to consider increasing your score.\n___\n(1) \u201cExplain the importance of studying this model\u201d\n\nWe selected the linear regression mixture model, which is slightly less realistic (non-discrete) than the HMM model. Our choice was motivated by the model's analytical tractability, which provides a precise theoretical understanding of how task retrieval and task learning operate. \n\nNumerous theoretical studies adopt the same linear regression mixture model. Notable recent works include [1] through [5]. All theoretical work necessitates some form of simplified model to comprehend complex real-world phenomena. Although the linear regression mixture model deviates from the actual pretraining data model, the insights it offers may still predict real-world phenomena.\n\n[1] What can transformers learn in-context? a case study of simple function classes\n\n[2] What learning \u00a8 algorithm is in-context learning? investigations with linear models\n\n[3] Transformers as algorithms: Generalization and implicit model selection in in-context learning\n\n[4] Transformers learn in-context by gradient descent\n\n[5] Transformers can optimally learn regression mixture models\n___\n(2) \u201cSome area of works that are related are left\u201d\n\nThank you for sharing these papers! We will incorporate these papers into the related works section.\n___\n(3) \u201cThe dependence of U shaped risk bound on component reweighting and shifting is not discussed properly.\u201d\n\nWe apologize if our current explanation is unclear. We will revise it to improve clarity.\n\nThe U-shape is observed under conditions of low prior task variance, and we aim to retrieve a prior task center that is closest to the in-context task. When k is small, due to the low prior task variance, component reweighting takes effect faster than component shifting. Thus, when all prior centers are nearly unchanged, the task center closest to the in-context task receives a high weight close to 1, thereby reducing task retrieval loss. However, as k continues to increase and becomes large, component shifting causes all posterior task centers to shift towards the in-context task. Therefore, when the in-context task differs from the target task, this results in an increase in task retrieval loss.\n___\n(4) \u201cConnect the U shaped with the bias-variance tradeoff.\u201d\n\nOur U-shaped curve can **not** be directly explained by the bias-variance tradeoff. This is because the bias-variance tradeoff is for increasing model complexity, while our curve is for increasing the number of samples. Under the standard supervised learning setting, our U-shaped curve is not observed. Instead, our task retrieval setting is closely related to the distribution shift setting, particularly the concept shift.\n___\n(5) \u201cThe novelty in deriving the posterior distributions for the mixture of Gaussian distribution is unclear to me\u201d\n\nWe have **not** claimed novelty in the calculation of the posterior distribution (although we believe it forms part of our contribution, given the extensive calculations required). Instead, our paper presents two major contributions in terms of novelty.\n\nFirstly, we are the pioneering work that formally defines and analyzes the two regimes of in-context learning: task learning and retrieval. This can assist future researchers in reconciling seemingly conflicting observations made with in-context learning in the past, as they can be interpreted as non-conflicting results within the two distinct regimes.\n\nSecondly, we are the first to precisely predict two non-monotonic performance patterns of ICL in real-world LLMs as the number of samples increases: the U-shaped curve we presented in the original submission, and the flipped one we presented in the rebuttal. The U-shaped curve was first predicted by our theory and then confirmed by our additional experiment with GPT4. The flipped curve was initially presented in the original GPT3 paper (and later was re-confirmed by Xie et al.\u2019s work too!) and explained via experiment under our framework. The experiments are provided in \u201csupplementary material\u201d\n\nThese two contributions lead us to strongly believe that our paper offers substantial contributions to the research community, particularly those focused on the theoretical analysis of ICL, and to some extent, practical LLM research."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3000/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700535968480,
                "cdate": 1700535968480,
                "tmdate": 1700535968480,
                "mdate": 1700535968480,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wbj18Q03p0",
            "forum": "8fQlGQkj0S",
            "replyto": "8fQlGQkj0S",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3000/Reviewer_4TUM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3000/Reviewer_4TUM"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates in-context learning for task retrieval and task learning from the lens of generative models for pre-training data and in-context samples. A Gaussian mixture model is proposed for the pre-training data generation process. The paper first demonstrates that in-context prediction with a Bayes-optimal next-token predictor corresponds to the posterior mean of the label given the in-context samples. The paper establishes upper bounds for both task retrieval and learning risks, revealing a quadratic decrease in the risk bound of task learning and a U-shaped pattern for the risk bound of task retrieval. The findings are validated through numerical simulations, and with experiments using Transformers."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The theoretical analysis using data generative models using a Gaussian mixture model is novel, and presents a new approach to investigating in-context learning. \n- The theoretical results are valid, and are backed by simulations with the data generated from a Gaussian mixture model with Transformers.\n- The U-shaped pattern for the risk bound of task retrieval is shown for the first time, which has not been observed in previous works."
                },
                "weaknesses": {
                    "value": "- The claim regarding the similarity to real-world settings is not quite accurate, as the setting studied in the paper is quite different from in-context learning in NLP tasks with practical LLMs. \n- The assumptions in Section 3.3 are restrictive, and the scope of the theoretical analysis is limited to a very specific setup based on a particular generative model."
                },
                "questions": {
                    "value": "- \"A highly expressive F can be viewed as K separate models F0, . . . , FK\u22121, where Fk takes exactly 2k + 1 tokens as input. Thus, pretraining can be decomposed into K separate optimization problems.\" Can the authors provide more explanation regarding this statement?\n- Can the authors provide more details about the Transformer? The code seems to suggest the architecture used is GPT-2, and NanoGPT, but these details should be provided in the paper.\n\nSuggestions:\n- The experimental results provided in Appendix A with Figures 6,7, and 8 are central results for the paper, and are not quite accessible for readers. It would be beneficial to add more experiments in the main paper, and move some of the analysis to the supplementary material.\n- There are typographical issues and formatting issues (e.g. text overlaps in Figure 5) that should be fixed."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3000/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3000/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3000/Reviewer_4TUM"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3000/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699400182244,
            "cdate": 1699400182244,
            "tmdate": 1699636244485,
            "mdate": 1699636244485,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QxFSwlxnMu",
                "forum": "8fQlGQkj0S",
                "replyto": "wbj18Q03p0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3000/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3000/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your strong support of our work!\n\nWe also would like to express our sincere gratitude to the reviewer for dedicating your valuable time to provide constructive feedback on our work. We hope our responses have addressed your concerns and answered your questions. If our responses have resolved your concerns, we kindly request you to consider increasing your score and championing our paper.\n___\n(1) \u201cthe similarity to real-world settings is not quite accurate, as the setting studied in the paper is quite different from in-context learning in NLP tasks\u201d &  \u201cassumptions in Section 3.3 are restrictive, and the scope of the theoretical analysis is limited to a very specific setup\u201d\n\nWe deliberately selected the linear regression mixture model, which, as the reviewer correctly pointed out, is slightly less realistic (non-discrete) than the HMM model. Our choice was motivated by the model's analytical tractability, which provides a precise theoretical understanding of how task retrieval and task learning operate. Numerous theoretical studies adopt the same linear regression mixture model. Notable recent works include [1] through [5]. \n\nAll theoretical work necessitates some form of simplified model and strong assumptions to comprehend complex real-world phenomena. Although the linear regression mixture model and our strong assumptions may not look very realistic, the analysis we performed with this model still predict real-world phenomena.\n\n[1] What can transformers learn in-context? a case study of simple function classes\n[2] What learning \u00a8 algorithm is in-context learning? investigations with linear models\n[3] Transformers as algorithms: Generalization and implicit model selection in in-context learning\n[4] Transformers learn in-context by gradient descent\n[5] Transformers can optimally learn regression mixture models\n___\n(2) \u201cA highly expressive F can be viewed as K separate models F0, . . . , FK\u22121, where Fk takes exactly 2k + 1 tokens as input. Can the authors provide more explanation regarding this statement?\"\n\nF is a function that takes variable-length input. Thus, F can be fully characterized by specifying F's behavior for each input length. That is,\n\nF = F_0 * 1(# of in-context examples = 0) + F_1 * 1(# of in-context examples = 1) + \u2026\n\nNow, consider a sequence of tokens [x1, y1, x2, y2]. Pretraining can be viewed as\n\nmin_F L(F([x1]),y1) + L(F([x1,y1,x2]),y2).\n\nBy the above input-length decomposition, this is equivalent to\n\nmin_{F_0, F_1} L(F_0([x1]),y1) + L(F_1([x1,y1,x2]),y2).\n\nThus, this becomes two separate optimization problems. Now, if F is highly expressive, then F_0 and F_1 are also highly expressive. Thus, each F_i can become the Bayes-optimal solution.\n___\n(3) \u201cThe code seems to suggest the architecture used is GPT-2, and NanoGPT, but these details should be provided in the paper.\u201d \n\nThe code is constructed on the GPT2Model from the transformers package supported by Hugging Face. The number of attention heads is set to 8, the depth is set to 10, and the embedding dimension is set to 1024. We use the Adam optimizer with a learning rate and weight decay both set to 0.00001. The batch size is set to 256. We train the model for 3 epochs, with each epoch consisting of 10,000 steps.\n___\n(4) \u201cFigures 6,7, and 8 are central results for the paper, and are not quite accessible for readers. It would be beneficial to add more experiments in the main paper,\u201d & \u201ctext overlaps in Figure 5\u201d\n\nThank you for your insightful suggestions! We will incorporate these changes into our revision."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3000/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700536176973,
                "cdate": 1700536176973,
                "tmdate": 1700536176973,
                "mdate": 1700536176973,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9RXDLXoARE",
            "forum": "8fQlGQkj0S",
            "replyto": "8fQlGQkj0S",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3000/Reviewer_8Pgy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3000/Reviewer_8Pgy"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a theoretical analysis of in-context learning. This work separates in-context learning into task retrieval and task learning modes. The authors formalise in-context learning as Bayesian inference over pre-trained tasks using proposed generative models. Closed-form posterior distribution and its component re-weighting and shifting mechanisms are presented clearly. They prove task learning risk decreases quadratically as number of examples increases and task retrieval risk follows a U-shaped curve, decreasing then increasing with more examples. They validate analysis theoretically and via simulations on Transformer models.\nIn summary, the paper provides a formal understanding of in-context learning grounded in Bayesian principles. The analysis reveals unique insights into the distinct behaviors of task retrieval versus learning based on modification of the posterior distribution over pre-trained tasks."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Originality: \nPrincipled Bayesian framework unifying task retrieval and learning modes of in-context learning.\nInteresting generative model provides formal grounding for the analysis.\nDerives unique insights such as U-shaped bound revealing limitations of task retrieval.\n\nQuality:\nGood mathematical analysis and detailed proofs.\nInteresting experiments supporting the theory.\nCode provided for reproducibility.\n\nClarity:\nClearly explains concepts like component re-weighting and shifting.\nDelineates assumptions underlying the analysis.\n\nSignificance:\nHelps advances understanding of in-context learning in large language models.\nFormal analysis provides basis for improving real-world in-context performance. \nInsights like U-shaped bound have significant implications for practical model."
                },
                "weaknesses": {
                    "value": "The generative modeling makes strong simplifying assumptions like Gaussian distributions that limits applicability to complex real-world textual data. Expanding the analysis to more realistic data distributions will strengthen it. Or perhaps even showing that any of their conclusions hold in LLMs, for example does the u shaped phenomena occur in LLMs?\n\nThe empirical validation relies heavily on synthetic data simulated from the assumed generative process. Evaluating on real-world NLP tasks would better assess wider applicability.\n\nThe criteria for determining the number of examples k for optimal retrieval remains unspecified. Providing more precise theoretical or empirical guidance could improve utility.\n\nRisk is evaluated using squared loss. Evaluating other loss functions (Cross entropy) could expand usefulness across different domains.\nAs task retrieval becomes more mainstream in LLMs, societal impacts of failures in retrieved tasks can be added. Characterizing the potential downsides and suggesting caution."
                },
                "questions": {
                    "value": "Please see the weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3000/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3000/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3000/Reviewer_8Pgy"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3000/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699513345003,
            "cdate": 1699513345003,
            "tmdate": 1699636244420,
            "mdate": 1699636244420,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QYGNVFxXzV",
                "forum": "8fQlGQkj0S",
                "replyto": "9RXDLXoARE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3000/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3000/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your strong support of our work!\n\nWe also would like to express our sincere gratitude to the reviewer for dedicating your valuable time to provide constructive feedback on our work. We hope our responses have addressed your concerns and answered your questions. If there are any remaining questions, please do not hesitate to let us know. If our responses have resolved your concerns, we kindly request you to consider increasing your score and championing our paper.\n___\n(1) \u201cThe generative modeling makes strong simplifying assumptions like Gaussian distributions that limits applicability to complex real-world textual data. Expanding the analysis to more realistic data distributions will strengthen it. Or perhaps even showing that any of their conclusions hold in LLMs, for example does the u shaped phenomena occur in LLMs?\u201d\n\nIn response to your valuable suggestion, we conducted additional experiments to demonstrate that the U-shaped curve can indeed be observed with a real-world LLM (GPT-4). Please refer to Section 1 of the supplemental file for details.\n\nMoreover, we demonstrate that under a slightly different setting, our task learning analysis can explain another intriguing phenomenon, which we term the 'flipped U curve' \u2013 a pattern where the loss increases and then decreases as the number of samples increases. This effectively explains the phenomenon reported in the original GPT-3 paper as well as in subsequent research. Please refer to Section 2 of the supplemental file for details.\n\nThus, even if our linear regression mixture model deviates from the actual pretraining data model, the insights it offers can predict real-world phenomena!\n___\n(2) \u201doptimal retrieval remains unspecified\u201d\n\nThe question essentially requires finding the minimizer of $f(k) = a \\exp(-bk) + c \\exp(-dk^{1/2}) + e k^2$, which is equivalent to solving $f\u2019(k)=0$. The author believes that there is no closed-form solution for this.\n___\n(3) \u201cEvaluating other loss functions (Cross entropy)\u201d\n\nThank you for the excellent suggestion. We are currently exploring it; however, it's challenging to find a suitable prior for classification that is akin to the Gaussian mixture used in the linear regression setting. The Gaussian mixture provides flexibility for mathematical calculation and exploration of the prior's properties. We have added evaluations on GPT-4 which is trained with cross-entropy to reproduce the U-shaped loss curve, in the Section 1 of the supplemental file."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3000/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700536638633,
                "cdate": 1700536638633,
                "tmdate": 1700536638633,
                "mdate": 1700536638633,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]