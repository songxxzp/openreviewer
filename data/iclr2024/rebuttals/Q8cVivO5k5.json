[
    {
        "title": "Large-Batch, Iteration-Efficient Neural Bayesian Design Optimization"
    },
    {
        "review": {
            "id": "FZjlsGgcYG",
            "forum": "Q8cVivO5k5",
            "replyto": "Q8cVivO5k5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5310/Reviewer_2FsD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5310/Reviewer_2FsD"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a Multi objective optimisation algorithm with a focus on large batch sizes (up to 1000s of points) and few iterations (as low as 10).\n\nGiven an objective functions $f:\\mathcal{X}\\to \\mathbb{R}^M$ where $\\mathcal{X}\\subset \\mathbb{R}^d$,  the method is a Bayesian model based algorithm, they propose to use Bayesian neural networks as the surrogate model to predict $\\underline{y}=\\hat{f}(x)$ as well as the epistemic uncertainty $Var(\\hat{f(x)})$, these models which can scale to large dataset sizes much more effectively than the more traditional Gaussian processes.\n\nIn order to determine a new batch of points to be evaluated, the authors propose to concatenate the model predictions and uncertainties $[\\hat{f(x)}, Var(\\hat{f}(x))] \\in \\mathbb{R}^{2M}$, which can then be fed into NSGA-II, a popular evolutionary aglgorithm, which can find the set of pareto optimal points $x_1,...,x_B\\in \\mathcal{X}$ that form the pareto front in the augmented output space $\\mathbb{R}^{2M}$. In other words, these are point that are predicted to have high value and/or high uncertainty.\n\nThe authors perform experiments with a range of off-the-shelf Bayesian neural network methods and determine Deep Ensembles to be the best candidate surrogate model."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Simplicty, elegance.\n  - Bayesian neural networks have become a work horse surrogate model in the Bayesian Optimization community in recent years\n  - NSGA-II is a very popular well established mainstream algorithm in the multi objective community\n  - concatenating predictions and uncertainties to be fed into NSGA-II seems a very reasonable good idea\n  - altogether the method avoids introducing any sophisticated new engineering, and instead opts to intelligently combine established components from the community with some well justified tweaks.\n\n- clearly written, I enjoyed the exposition of related work.\n\n- Section 5.4, running algorithm without using uncertainties I felt was a very nice experiment and cleawrly demonstrated their benefit."
                },
                "weaknesses": {
                    "value": "I only have minor comments\n\n- I see the authors discuss this in Appendix E but MOO is an very large field and I would be very surprised if batch construction by finding the pareto front of concatenated predictions and uncertainties has not been considered before, (it _seems_ so obvious!), \n- upon first reading, I felt the title was somewhat cluttered."
                },
                "questions": {
                    "value": "- presumably for small use cases, optimising 2 simple objectives over 2 dimensions batchsize 2, i.e. the ideal use case for any GP-BO, the proposed method would suffer, is there a crossover from where more simple GP-BO methods fail and LBN-MOBO would be best?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5310/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698692409761,
            "cdate": 1698692409761,
            "tmdate": 1699636532224,
            "mdate": 1699636532224,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0pp49OgQv9",
                "forum": "Q8cVivO5k5",
                "replyto": "FZjlsGgcYG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5310/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5310/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your feedback. We are heartened to know that you appreciate our method\u2019s simplicity and its potential impact. We provide responses to the raised questions below.\n\n## Is there a crossover from where more simple GP-BO methods fail and LBN-MOBO would be best?\nIn our empirical studies on batch multi-objective Bayesian Optimization (BO), we've identified that a batch size of 100 serves as a threshold, beyond which traditional BO methods begin to encounter difficulties. Interestingly, there are some ingenious adaptations to BO that enable operation with batch sizes approaching 1000. However, for even larger batch sizes, the shift towards Bayesian Neural Networks (BNNs) in place of Gaussian Processes (GPs) and replacing the acquisition function with the 2MD function becomes essential. As we scale up to batch sizes of 10,000, only methods like Monte Carlo Dropout (MCDropout) and Deep Ensembles in combination with 2MD acquisition prove capable of effectively handling such extensive batch sizes. \n## New title.\nWe are open to consider renaming the paper and we appreciate to know your feedback in this regards. We think we can drop \u2018iteration-efficient\u2019 and \u2018design\u2019 from the title."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5310/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700579201721,
                "cdate": 1700579201721,
                "tmdate": 1700579201721,
                "mdate": 1700579201721,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "X5bLGGrm80",
            "forum": "Q8cVivO5k5",
            "replyto": "Q8cVivO5k5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5310/Reviewer_9JiQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5310/Reviewer_9JiQ"
            ],
            "content": {
                "summary": {
                    "value": "The paper considers a setting in which BO is applied to solve black-box optimization problems where there are multiple objectives and a query is expensive, but the batch size can be extremely large. To address this challenge, the authors propose an acquisition function that is more scalable and takes into account the uncertainty of candidates. Empirically, this BO algorithm is applied to solve two realistic black-box optimization problems in this setting."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tThe paper considers a novel black-box optimization setting where there are multiple objectives and the batch size can be very large. The authors empirically observe that contemporary multi-objective batch acquisition functions do not scale well with respect to the batch size.\n2.\tTo solve this issue, the authors propose a modified version of Deep Ensembles to approximate BNN and an acquisition function to maximize both predicted objectives and the uncertainty measure."
                },
                "weaknesses": {
                    "value": "1.\tContribution is not enough. The innovation of the paper can be summarized into a new predictive model with a minor modification on the original Deep Ensembles model and a new multi-objective batch acquisition function. For the predictive model, the reason for modifying the uncertainty measurement part from aleatory noise to epistemic noise is unclear. Also, the benefit from this change is not verified in the paper. Moreover, the novelty of 2$\\textit{M}$D acquisition compared to the other acquisition functions is not clear either, except for being more scalable.\n2.\tThe empirical results presented are somewhat unconvincing. The reason why the authors choose deep ensembles as a surrogate to test in the two subsequent realistic tasks is that its runtime is shorter and achieves higher hypervolume. However, since this appears in only one experiment, its generalized performances to other tasks are not necessarily better than other surrogates. \n3.\tMore benchmark models should be considered in the two realistic tasks. In these two tasks, only two models are considered, i.e., modified deep ensemble + 2$\\textit{M}$D and dropout + 2$\\textit{M}$D. Therefore, whether the modified deep ensemble + 2MD indeed performs well enough is not clear. It would be great if the authors can also consider more models as benchmarks.\n4.\tLimited theory is developed for this new BO method."
                },
                "questions": {
                    "value": "1.\tHow is the \u201ctime\u201d defined in Figures 1 and 3? \n2.\tHow well does the modified deep ensemble quantify uncertainty, compared to the original deep ensemble?\n3.\tHow does 2MD work? It looks like a key ingredient of the new acquisition is the NSGA-II. However, this is not explained in the main body of the paper. Also, how to implement the acquisition function, one of the most important parts in this paper, is not clearly explained. The only relevant statement is the last three lines in page 6. \n4.\tIn reality, the magnitude or the range of $F$ is usually unknown. How do the authors suggest to balance the tradeoff between the output of the predictive model and the uncertainty?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5310/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5310/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5310/Reviewer_9JiQ"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5310/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698696183292,
            "cdate": 1698696183292,
            "tmdate": 1700717753973,
            "mdate": 1700717753973,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ChQq3uGfMq",
                "forum": "Q8cVivO5k5",
                "replyto": "X5bLGGrm80",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5310/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5310/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the thorough and thoughtful review. We appreciate the detailed feedback. We provide responses to the raised questions below.\n\n## The reason for modifying the uncertainty measurement part to include only epistemic uncertainty and its benefits. \nUncertainty in BO primarily serves as a guide for effective exploration. Epistemic uncertainty represents a lack of knowledge about a phenomenon, which can be mitigated through additional observations. Conversely, aleatoric uncertainty relates to the inherent noise in the system and is not a function of data accumulation.\n\nIn our study, we worked with the underlying Native Forward Processes (NFPs) that are essentially noiseless. Consequently, addressing noisy NFPs is earmarked for future exploration (Limitation section). Given this noiseless scenario, aleatoric uncertainty is presumed negligible, leading us to focus solely on the epistemic uncertainty. This decision significantly simplified and stabilized our model's training phase. This means our training process closely resembles that of conventional neural network, avoiding potential complexities and instabilities in training Deep Ensembles [1][2]. This methodological choice not only enhances the robustness of LBN-MOBO but also increases its accessibility and practicality as any neural net architecture can be utilized.\n\n## Usefulness of aleatoric uncertainty.\nPrompted by the reviewer's insightful question, we explored a toy noisy NFP to see whether considering aleatoric uncertainty helps with the exploration. This example shows that in the presence of significant noise, using only the epistemic uncertainty could encourage sampling of noisy regions (likely due to Deep Ensembles mechanism), which will deteriorate the exploration. Very interestingly, including the aleatoric uncertainty and avoiding the regions with such uncertainty improves the exploration significantly. This has highly appealing implications for BO in the presence of noise and we plan to study it intensively in the future.  This is detailed in our supplementary material (Rebuttal PDF_LBN_MOBO_noisy.pdf). \n\nThese findings highlight the potential challenges and benefits of aleatoric uncertainty in scenarios with inherent noise, paving the way for future research in this domain. \n\n## Comparing 2MD acquisition to the other acquisition functions, except for being more scalable.\nThe traditional approaches to acquisition, such as the Upper Confidence Bound (UCB), relies on some form of weighted sum of uncertainty and prediction. This weighting is crucial as it dictates the balance between exploration and exploitation within the optimization process. However, the scale differences between prediction and uncertainty necessitate careful tuning of this hyperparameter. Additionally, as the optimization progresses, the requirements for exploration or exploitation may change, potentially requiring ongoing adjustments to this weight. In the realm of multi-objective optimization, the complexity escalates further due to the need to fine-tune multiple parameters to achieve an optimal balance in exploring various objectives and their respective uncertainties.\n\nIn contrast, the 2MD acquisition function, as described in the paper (Section 4.2 and Equation 3), adopts a fundamentally different approach. Here, uncertainties and predictions are treated as separate optimization objectives, which are then jointly optimized to construct a Pareto front of potential candidates. This method does not need the summation of objectives, thereby remedies the issue of scale mismatch between objectives. Consequently, our approach is free from the constraints of hyperparameter tuning, simplifying the optimization process. It's important to note that, to our knowledge, there are no alternative methods capable of scaling to the large batch sizes required for addressing real-world problems of the magnitude we are tackling, positioning our algorithm as a unique solution in this domain.\n\n## More benchmark algorithms other than BNNs on real problems.\nIn appendix section C5 you can find complementary experiments comparing LBN-MOBO using other algorithms that are partially or completely capable of handling such large batch sizes.\n\n## How is the \u201ctime\u201d defined in Figures 1 and 3?\nIt is the summation of acquisition time plus training the surrogate (a BO cycle) for 10 iterations.\n\n## How well does the modified deep ensembles quantify uncertainty, compared to the original deep ensemble?\nThere is a widely established way of implementing Deep Ensembles where the training of prediction networks is separated from the uncertainty estimation networks [2]. The proposed simplified implementation is identical to that approach when it comes to computing the epistemic uncertainty: we ensemble prediction networks to calculate the epistemic uncertainty."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5310/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700566296998,
                "cdate": 1700566296998,
                "tmdate": 1700566296998,
                "mdate": 1700566296998,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UmqvxV2ESr",
                "forum": "Q8cVivO5k5",
                "replyto": "qOmnKG43v5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5310/Reviewer_9JiQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5310/Reviewer_9JiQ"
                ],
                "content": {
                    "title": {
                        "value": "Response to the Authors"
                    },
                    "comment": {
                        "value": "I appreciate the authors for their efforts on the additional experiments and detailed explanations. My concerns are mostly addressed. I raised my score to 5. \n\nNevertheless, I still think the paper could benefit from a deeper theoretical interpretation of why the proposed algorithm should work better than others, especially, on the part of only modeling epistemic uncertainty versus the traditional deep ensembles. Moreover, the absence of tests on the algorithms\u2019 performance in noisy environments, a more common real-world scenario, limits its credibility on practical applicability and impact."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5310/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700717725075,
                "cdate": 1700717725075,
                "tmdate": 1700717725075,
                "mdate": 1700717725075,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kwZY3GntzK",
                "forum": "Q8cVivO5k5",
                "replyto": "X5bLGGrm80",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5310/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5310/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate the reviewer's openness and for raising our score. The answer below will hopefully addresses the remaining concerns of the reviewer. \n## The benefits of using only epistemic uncertainty in noiseless NFPs and the advantages of aleatoric uncertainty in noisy settings. \nGiven your feedback and the one of Reviewer acfL, we conducted additional experiments to study the effect of including the aleatoric uncertainty in our acquisition function in noiseless NFPs. These experiments are shown in the appended document (Rebuttal PDF_LBN_MOBO_aleatoric.pdf). We used the examples and procedures described in Sections C1.4 and C1.5 of the paper. The outcomes indicate that incorporating aleatoric uncertainty does not affect the exploration process in a beneficial manner\u2014as showcased by the ZDT1 results\u2014and may even delay the convergence, as observed in the ZDT2 and ZDT3 experiments. This is attributed to the nature of the aleatoric uncertainty, that measures inherent noise which cannot be diminished. This follows our original intuition that aleatoric uncertainty cannot be an effective tool for driving the exploration. \n\n**However**, the noisy example detailed in our supplementary document (PDF-LBN_MOBO_noisy.pdf) demonstrates that for noisy Native Forward Processes (NFPs), incorporating aleatoric uncertainty is advantageous for _avoiding_ (and not exploring) the system's irreducible noise during BO. Motivated by these findings, we are applying LBN-MOBO with both epistemic and aleatoric uncertainties on our printer\u2019s color gamut problem where we intentionally add noise to one the printer channels (in simulation). This new experiment will be incorporated into the revised version of our paper."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5310/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700739898034,
                "cdate": 1700739898034,
                "tmdate": 1700740344376,
                "mdate": 1700740344376,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "967WZP44Qj",
            "forum": "Q8cVivO5k5",
            "replyto": "Q8cVivO5k5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5310/Reviewer_s2Pz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5310/Reviewer_s2Pz"
            ],
            "content": {
                "summary": {
                    "value": "The paper considers the problem of designing Bayesian optimization algorithms for the setting of large batches of evaluations in order to optimize a black-box function. An acquisition function is constructed as multiobjective optimization over multiple predictive mean and uncertainty functions modeled by a deep ensemble. Experiments are performed on two real-world benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper considers an important problem relevant to real-world applications in engineering design.\n\n- I especially like the real world evaluation on two interesting benchmarks: airfoil design and 3D printing. It would be an interesting contribution to the BO community if they are released in the open-source code. \n\n- The idea is simple and works well on the benchmarks."
                },
                "weaknesses": {
                    "value": "- Although I like the simplicity of the approach, the reasoning behind choosing this instantiation of multiobjective optimization is not entirely clear. Please considering some more analysis about the principles behind the proposed acquisition function. \n\n- Some relevant related work that can be useful to discuss in the paper:\n\t- A very similar idea utilizing multiobjective acquisition function with predicted mean and variance as objectives. \n\n\t[1] Gupta, S., Shilton, A., Rana, S., & Venkatesh, S. (2018, March). Exploiting strategy-space diversity for batch Bayesian optimization. In International conference on artificial intelligence and statistics (pp. 538-547). PMLR.\n\t- There has been a bunch of work on making thompson sampling work for large batch sizes in both continuous and combinatorial design spaces. \n\n\t[2] Hern\u00e1ndez-Lobato, J. M., Requeima, J., Pyzer-Knapp, E. O., & Aspuru-Guzik, A. (2017, July). Parallel and distributed Thompson sampling for large-scale accelerated exploration of chemical space. In International conference on machine learning (pp. 1470-1479). PMLR.\n\n\t[3] Deshwal, A., Belakaria, S., & Doppa, J. R. (2021, May). Mercer features for efficient combinatorial Bayesian optimization. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 35, No. 8, pp. 7210-7218).\n\n\t[4] Vakili, S., Moss, H., Artemev, A., Dutordoir, V., & Picheny, V. (2021). Scalable Thompson sampling using sparse Gaussian process models. Advances in neural information processing systems, 34, 5631-5643.\n\n- Probably a nit, but I think calling deep ensembles as a bayesian neural network is not entirely correct."
                },
                "questions": {
                    "value": "Please see weaknesses section above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5310/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698738230902,
            "cdate": 1698738230902,
            "tmdate": 1699636532003,
            "mdate": 1699636532003,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "t6q15LAzC9",
                "forum": "Q8cVivO5k5",
                "replyto": "967WZP44Qj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5310/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5310/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your insightful feedback. Your appreciation of our work, particularly the real-world evaluations and the potential contribution to the Bayesian Optimization community, is highly encouraging. Following your suggestion to release our benchmarks in open-source code, we will certainly consider this moving forward. We provide responses to the raised questions below.\n\n## More analysis on 2MD acquisition function.\nWe have provided the following insights and analyses about our acquisition function: \n- In our paper, Section 5.4 is dedicated to analyzing the influence of uncertainty on the performance of the 2MD acquisition function. Through this analysis, we aim to emphasize the critical role that epistemic uncertainty plays in determining the overall quality of the LBN-MOBO approach.\n\n- We have conducted a preliminary evaluation on the feasibility of applying the 2MD Pareto front approach to noisy Native Forward Processes (NFPs). This examination, detailed in a separate supplementary material (Rebuttal PDF-LBN_MOBO_noisy.pdf), explores the impact of incorporating aleatoric uncertainty into the acquisition function. Our findings reveal that relying solely on epistemic uncertainty in noisy scenarios can inadvertently lead the optimization process to select candidates from these noisy regions. In contrast, by taking into account aleatoric uncertainty and striving to minimize it during the optimization process, we can effectively avoid these noisy areas. This insight highlights the importance of considering both types of uncertainty in the optimization strategy when dealing with noisy NFPs.\n\n- We have introduced a new subsection that delves into how the 2MD Pareto front can be paired with any arbitrary acquisition function. This aspect is crucial as it positions 2MD as a generic acquisition function adaptable to various contexts. In Section 5.1, we demonstrated this adaptability by coupling the 2MD approach with a range of Bayesian Neural Networks (BNNs). \n\n- In Section C3, we revisited the evaluations from Section 5.1, this time using a new problem, i.e., the DTLZ5 test suite. This problem has 20 input and 3 output dimensions. (When applying 2MD acquisition function, this translates to a 6D problem.) Although Monte Carlo Dropout (MCDropout) and Deep Ensembles showed the best runtime performance, the quality of the solutions across all surrogates was highly competitive. This finding suggests that the 2MD acquisition approach is capable of delivering strong performance with various surrogate models, indicating its versatility and effectiveness in handling complex multi-dimensional optimization tasks.\n\n\n## Expansion of complementary related work.\nWe appreciate the reviewer for mentioning these missing related works. We have included them in the complementary related work in Sections A2 and A3."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5310/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700579919917,
                "cdate": 1700579919917,
                "tmdate": 1700580515213,
                "mdate": 1700580515213,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "BDsvy4Rsna",
            "forum": "Q8cVivO5k5",
            "replyto": "Q8cVivO5k5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5310/Reviewer_acfL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5310/Reviewer_acfL"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a new method to perform multi-objective BO with the large-batch setting. It proposes to use a Bayesian Neural Network (BNN) created by Deep Ensembles as a surrogate model. It also proposes an NSGA-II based acquisition function that is claimed to be able to scale to large-batch setting better than current acquisition functions. The main idea for the proposed acquisition function (2MD acquisition function) is to simultaneously maximize the predicted objectives and the associated uncertainties, both are given by the BNN surrogate model.\n\nThe method (LBN-MOBO) is evaluated on synthetic functions (1 in the main paper and 4 in the appendix), and 2 real-world problems."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper tackles an important problem which is performing multi-objective BO with the large-batch setting.\n- The paper proposes an acquisition function for applying large-batch when performing BO while the current acquisition functions (qEHVI, qParEGO, qNEHVI) struggle, in terms of computation time. The concept of the proposed acquisition function is intuitive: it seems to further encourage explorative behavior, because it also maximizes the uncertainties in the surrogate model."
                },
                "weaknesses": {
                    "value": "- Some technical details are not described clearly, making it sometimes hard to catch the main idea of the paper. For example, the formal problem statement is not described, the proposed method makes use of only epistemic uncertainty but the concept of epistemic uncertainty is not explained in the Background. The organization of the paper is sometimes a bit confused, for example, the overall process of BO should not be placed in method section.\n- The use of BNN as the surrogate model to enhance the performance is surely promising, however, BNN has many problems. For example, the tuning of its hyperparameters could be another optimization problem or the uncertainty provided by the BNNs could be inaccurate. However, these problems are not discussed in the paper. Furthermore, I don't understand why the proposed method only requires the epistemic uncertainty. There are no motivation, explanation, or insights about this choice and why it work.\n- The proposed acquisition function of optimizing both the prediction and the uncertainty and the usage of NSGA-II to optimize this acquisition function seems to be not too novel for me. The idea is very similar to UCB. There are no deep analysis regarding this proposed acquisition function and why it will work well.\n- The experimental evaluation is very limited. It doesn't compare against other baselines in the main paper. In Section 5.1, it is not convincing to choose the surrogate model (inference method) by using only 1 synthetic experimental result. \n- Related works should mention other types of surrogate models apart from GP and BNN, such as TPE and RF. And also, it is worth mentioning why BNN is preferred over these models.\n- Section 4.1 only covers the modification for Deep Ensembles method. It is not clear how to apply the modification for other inference methods (SGHMC, HMC, DKL, IBNN), so as to compare in Figure 3. \n\nMinor:\n- Authors should use \\citep{} and \\citet{} separately when citing the references."
                },
                "questions": {
                    "value": "Apart from my comments in the Weaknesses section, the authors can answer the following questions:\n- The concept of the 2MD acquisition function is quite similar to Upper Confidence Bound with a specific exploration factor. It seems that in UCB, both the prediction and the uncertainty are incorporated to compute the acquisition function, while 2MD use the two values as separate objective to optimize. Can the authors point out some differences between the UCB and 2MD?\n- In Figure 1, why there is no surrogate SGHMC, HCM, Deep Ensembles paired with qNEHVI and qParEGO. How many function evaluations in total for this experiment?\n- Is batch size b > 1000 a normal batch size in real-world problem? There seems to be no reference to any applications using such large batch.\n- The two real-world problems use b=15,000 for airfoil problem and b=20,000 for printer problem, on a total of 10 iterations. With such a large number of function evaluations (150,000 and 200,000), can LBN-MOBO outperform Evolutionary Computation methods, e.g., MOEA/D, NSGA-II? These two EC methods are quite powerful for solving multi-objective optimization problems.\n\n[1] B. Paria, K. Kandasamy, and B. P\u00f3czos. A flexible framework for multi-objective Bayesian optimization using random scalarizations. In Proceedings of The 35th Uncertainty in Artificial Intelligence Conference, volume 115, 2020\n\n[2] Daulton, Samuel, David Eriksson, Maximilian Balandat, and Eytan Bakshy. \"Multi-objective bayesian optimization over high-dimensional search spaces.\" In Uncertainty in Artificial Intelligence, pp. 507-517. PMLR, 2022."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5310/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698833519351,
            "cdate": 1698833519351,
            "tmdate": 1699636531912,
            "mdate": 1699636531912,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tYTLWEoREy",
                "forum": "Q8cVivO5k5",
                "replyto": "BDsvy4Rsna",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5310/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5310/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the thorough and thoughtful review. We appreciate the raised questions that help us clarify the paper. We provide responses to the raised questions below.\n\n## Explanation of the epistemic uncertainty is missing.\nWe have elaborated on epistemic and aleatoric uncertainty and their differences in Section 4.1. \n\n\n## Why does LBN-MOBO only needs epistemic uncertainty (and not aleatoric uncertainty)? \nEpistemic uncertainty represents the lack of knowledge about a phenomenon and can be reduced with more observation. The aleatoric uncertainty is a sign of inherent noise which cannot be reduced. In this paper we assumed that our underlying Native Forward Processes (NFPs) do not feature significant noise and left handling noisy NFPs for the future work (See Limitation section). In the absence of significant noise, the inclusion of aleatoric uncertainty does not affect the solutions. Thus, we decided to modify the Deep Ensembles to calculate only the epistemic uncertainty which is necessary for the exploration. This decision significantly simplifies and stabilizes our model's training [1] phase, as we don\u2019t need to train variance networks typically needed for aleatoric uncertainty [2]. \n\nPrompted by the reviewer's insightful question, we explored a toy noisy NFP to see whether considering aleatoric uncertainty helps with the exploration. This example shows that in the presence of significant noise, using only the epistemic uncertainty could encourage sampling of noisy regions (likely due to Deep Ensembles mechanism), which will deteriorate the exploration. Very interestingly, including the aleatoric uncertainty and avoiding the regions with such uncertainty improves the exploration significantly. This has highly appealing implications for BO in the presence of noise and we plan to study it intensively in the future.  This is detailed in our supplementary material (Rebuttal PDF_LBN_MOBO_noisy.pdf). \n\n## Discussion of the problems inherent to BNNs (e.g., tuning and uncertainty quality).\nThe reviewer is right about these challenges of BNNs. While we don\u2019t focus on improving on the BNNs side, our simplified approach in Sections 4.1 and 4.2 addresses both concerns to a great extent. Regarding the surrogate model, by excluding the aleatoric uncertainty we have avoided several complexities and instabilities during the training phase [1][2], making it completely similar to training a group of conventional neural networks. Moreover the way we calculate the epistemic uncertainty is identical to the original Deep Ensembles methods and it is among the best (quasi-)BNNs in terms of the trade off between the quality of uncertainty and scalability [3]. \n\n## Similarity and differences between UCB and 2MD Pareto front.\nAs reviewer correctly points out UCB and 2MD perform the optimization while exploiting the uncertainties in different ways.\nUCB works based on a weighted sum of uncertainty and prediction. In UCB tuning is important as it regulates the trade-off between exploration and exploitation. Given the magnitude difference between prediction and uncertainty, one needs to tune this hyperparameter. Moreover, as we progress through the optimization the need for exploration or exploitation might change and as a result we might need to constantly adapt this hyperparameter. In the case of multi-objective optimization, the number of tuning parameters increases as we need to tune and find the right balance in exploring different objectives in the presence of their uncertainties. \n\nIn the 2MD acquisition function, however, the uncertainties and predictions remain independent objectives and are jointly optimized to generate a Pareto front of viable candidates (Section 4.2 and Equation 3). Since each objective is optimized jointly with other objectives without any form of summation we do not need to worry about the scale difference and LBN-MOBO does not introduce extra hyperparameters. \n\nUnfortunately, we could not find any implementation of UCB extension capable of handling both multi-objective and batch optimization to compare it against our method. If pointed to such an implementation, we will be happy to compare it with our 2MD algorithm. To the best of our knowledge, qEHVI and qNEHVI are the state-of-the-art algorithms for multi-objective batch Bayesian optimization. We showed that these methods are not suitable for the large batch setting we are proposing (Section 3)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5310/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700565404909,
                "cdate": 1700565404909,
                "tmdate": 1700565981402,
                "mdate": 1700565981402,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bWYybk9zGq",
                "forum": "Q8cVivO5k5",
                "replyto": "BDsvy4Rsna",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5310/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5310/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## More examples to show the advantage of our proposed surrogate over its possible alternatives.\nWe have included another example with larger input and output dimensions to demonstrate the robust performance and scalability of our pipeline. We used the DLTZ5 test, which has 20 input design parameters and 3 output dimensions, providing a rigorous challenge. The Section C3 in the revised manuscript shows that this problem behaves similarly to our previous tests. While the performance of all methods is comparable, dropout and Deep Ensembles stand out for their significantly higher scalability.\n\nTo align these experiments more closely with our real-world scenarios, we conducted additional tests using HMC and SGHMC surrogates (the two BNNs that could scale beyond 1000 sample batch sizes) with large batch sizes of 5000 and 10000 on the 30-dimensional ZDT3 problem (in a setup similar to Sections 5.1 and C1.4). The results, including runtime and objective values, were captured within a 24-hour GPU time frame.\n\n| Method         | Batch Size | Objective | Runtime  |\n|----------------|------------|-----------|----------|\n| HMC            | 5000       | 0.2217    | 53433s   |\n| HMC            | 10000      | Failed    | Failed   |\n| SGHMC          | 5000       | 0.1921    | 43749s   |\n| SGHMC          | 10000      | Failed    | Failed   |\n| DeepEnsembles  | 5000       | **0.3001**    | **721s**     |\n| DeepEnsembles  | 10000      | **0.3001**   | **1377s**    |\n\nThe data in our table illustrates that the Deep Ensemble not only achieves a better Pareto front but also does it so much faster compared to other BNNs. Furthermore, the table highlights the impracticality of using other BNNs in the context of our real-world problems, as we employed batch sizes exceeding 10,000 samples. This finding reinforces the effectiveness and efficiency of our chosen methods in handling large-scale, complex problems.\n\n## Implementation details of using 2MD Pareto front with other BNNs \nIn principle, one of the main advantages of 2MD acquisition function is that it only queries the surrogate models (without the need of gradients for example). 2MD performs by iteratively using these queried information and evolves the solution until a practically good convergence. We included a section and a conceptual figure in the appendix to clarify how it works (Section B2).\n\n## Is batch size b > 1000 a normal batch size in real-world problem?\nAbsolutely! Specifically in the contex of manufacturing, fabrication and simulation (for which we have shown two example experiments in the paper). In many such cases we are capable of producing very large batches of data with almost the same cost of a single sample but iterative lab visits are costly. As a result, LBN-MOBO makes the overall process efficient by reducing the necessity of lab visits (second paragraph of the introduction).\n\n## Can LBN-MOBO outperform Evolutionary Computation methods?\nYes. In section C1.4 of appendix we have compared LBN-MOBO against NSGA-II and a few other advanced (not necessarily neural) BO methods on ZDT1, ZDT2, ZDT3, DLTZ1, and DLTZ4 test suits as well as on **both our real-world problems** (Sections C5.1 and C5.2). In all these experiments LBN-MOBO outperforms all other methods handily.\n\n## Why qEHVI and qNEHVI do not include BNNs other than IBNN and DKL?\nPlease refer to the final paragraph of section 3.\n\n[1] Seitzer, Maximilian, et al. \"On the pitfalls of heteroscedastic uncertainty estimation with probabilistic neural networks.\" arXiv preprint arXiv:2203.09168 (2022).\n\n[2] Nix, David A., and Andreas S. Weigend. \"Estimating the mean and variance of the target probability distribution.\" Proceedings of 1994 ieee international conference on neural networks (ICNN'94). Vol. 1. IEEE, 1994.\n\n[3] Lakshminarayanan, Balaji, Alexander Pritzel, and Charles Blundell. \"Simple and scalable predictive uncertainty estimation using deep ensembles.\" Advances in neural information processing systems 30 (2017)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5310/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700565813372,
                "cdate": 1700565813372,
                "tmdate": 1700565854336,
                "mdate": 1700565854336,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bubVW4FqfI",
                "forum": "Q8cVivO5k5",
                "replyto": "bWYybk9zGq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5310/Reviewer_acfL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5310/Reviewer_acfL"
                ],
                "content": {
                    "comment": {
                        "value": "Hi author(s),\n\nThank you for your response. The response has addressed some of my concerns, but my main concerns still remain.\n\nThe first concern is regarding the choice of using only epistemic uncertainty (and not aleatoric uncertainty), I still don't feel clear and convincing enough regarding the explanation. Only one toy example doesn't make it clear on why it is more beneficial using only epistemic uncertainty, more studies with aleatoric uncertainty and without aleatoric uncertainty are needed to be conducted on the problems used in the paper to clearly demonstrate the benefit of using only epistemic uncertainty.\n\nThe second concern is that I think it's important to discuss the design choices (hyperparameters) of the BNNs as they are important factors to ensure the accuracy of the surrogate model. If it is argued that any sets of hyperparameters will enable the similar levels of accuracy of the proposed method then this needs to be demonstrate clearly in the paper.\n\nFinally, I still maintain my opinion regarding the novelty of the proposed acquisition function, I think it is not novel enough by optimizing both the prediction and the uncertainty and use NSGA-II to optimize this acquisition function. For a simple method, I normally expect some deep analysis on how and why the method works well but the paper doesn't provide such analysis.\n\nFor all these reasons, I still maintain my score as is."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5310/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700657334592,
                "cdate": 1700657334592,
                "tmdate": 1700657334592,
                "mdate": 1700657334592,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TlAXAXmxCY",
                "forum": "Q8cVivO5k5",
                "replyto": "BDsvy4Rsna",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5310/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5310/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your insightful feedback, we hope the answer below will address the raised concerns.\n## More studies with and without aleatoric uncertainty using the problems in the paper. \nGiven your feedback and the one of Reviewer 9JiQ, we conducted additional experiments to study the effect of including the aleatoric uncertainty in our acquisition function. These experiments are shown in the appended document (Rebuttal PDF_LBN_MOBO_aleatoric.pdf). We used the examples and procedures described in Sections C1.4 and C1.5 of the paper. The outcomes indicate that incorporating aleatoric uncertainty does not affect the exploration process in a beneficial manner\u2014as showcased by the ZDT1 results\u2014and may even delay the convergence, as observed in the ZDT2 and ZDT3 experiments. This is attributed to the nature of the aleatoric uncertainty, that measures inherent noise which cannot be diminished. This follows our original intuition that aleatoric uncertainty cannot be an effective tool for driving the exploration. If the reviewer sees any reason for further studies, we can repeat these experiments for our 3D printer and airfoil applications as well. Please instruct.  \n## More intuition on using only epistemic uncertainty and its benefits.  \n**Why only epistemic uncertainty?**\n- Epistemic uncertainty is crucial for exploration as it identifies knowledge gaps. Aleatoric uncertainty, hinting at the intrinsic noise, doesn't aid exploration.\n- The training of prediction networks for epistemic uncertainty is fully independent from aleatoric uncertainty process. This ensures that ignoring aleatoric uncertainty introduces no errors in estimating epistemic uncertainty [1].\n\n**What are the benefits of using only epistemic uncertainty?**\n- Training with only epistemic uncertainty, yielding a faster and more stable training without the need to manage aleatoric uncertainty networks (variance networks) [1]. \n- Omitting aleatoric uncertainty reduces model complexity, leading to fewer hyperparameters and a more straightforward training phase without balancing different uncertainties [2].\n\n\n## Hyperparameter of the BNNs.\nThe main focus of our paper is on the utilization of the simplified Deep Ensembles as the surrogate model. The main benefit of this surrogate is that it does not introduce any additional hyperparameters and it is sufficient to ensure that the sub-networks in the ensemble are trained accurately. \nRegarding other evaluated BNNs, we have adopted the implementation and the recommended hyperparameters from [3] and we also made sure that the training accuracy are reasonable. We will report the training accuracies for all methods in the final paper. \n\n## Deeper analysis of the method.\nPlease see an explicit list of our analyses in response to Reviewer s2Pz. \nIn addition, we have observations to believe that the regret (the difference between the ideal and the current solution) grows at most sub-linearly. We will include in the revised paper (at least an empirical study) to show the regret growth in a batch scenario. \n\n\n[1] Nix, David A., and Andreas S. Weigend. \"Estimating the mean and variance of the target probability distribution.\" Proceedings of 1994 ieee international conference on neural networks (ICNN'94). Vol. 1. IEEE, 1994.\n\n[2] Ansari, Navid, et al. \"Autoinverse: Uncertainty aware inversion of neural networks.\" Advances in Neural Information Processing Systems 35 (2022): 8675-8686.\n\n[3] Li, Yucen Lily, Tim GJ Rudner, and Andrew Gordon Wilson. \"A Study of Bayesian Neural Network Surrogates for Bayesian Optimization.\" arXiv preprint arXiv:2305.20028 (2023)."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5310/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700739555942,
                "cdate": 1700739555942,
                "tmdate": 1700740028175,
                "mdate": 1700740028175,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]