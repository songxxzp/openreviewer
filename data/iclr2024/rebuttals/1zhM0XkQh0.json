[
    {
        "title": "ProFeAT: Projected Feature Adversarial Training for Self-Supervised Learning of Robust Representations"
    },
    {
        "review": {
            "id": "dQlxeVqsn9",
            "forum": "1zhM0XkQh0",
            "replyto": "1zhM0XkQh0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9090/Reviewer_eRyw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9090/Reviewer_eRyw"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed to bridge the gap between the self-supervised and supervised adversarial training methods, with good scalability for larger models.\n\nThis paper is well-written and easy to follow."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The topic of the paper, bridging the gap between supervised and self-supervised adversarial training, is interesting. \n\nThe result is convincing. \n\nThis paper applied the proposed method to different DNN models, including popular VIT.\n\nExtensive ablation studies are provided, which shows the insight of the proposed method.\n\nExtensive results are shown in the appendices, which are helpful for the readers to understand the whole story."
                },
                "weaknesses": {
                    "value": "First, the author should give some basic explanation for the results, which will be appreciated. For example:\n1. In Table 3, the author should at least explain what is \"SA\" (standard accuracy?). \n2. In Table 4, the author should at least highlight which method has better results for each network structure (each row). \n3. what is the adversarial perturbation upper bound for testing in the table 3 and 4?\n\nIn Table 3, for DeACL (reproduced), the author modified the original teacher model. In this case, the word \"reproduced\" is misleading. In fact. it is not \"reproduced\", but modified. \n\n\"DynACL is run for 500 epochs rather than....\":\nIn this case, the reported result of DynACL should not be used for comparison. Because the result is not from the proposed settings, the result is not convincing. The explanation in Appendix D cannot overcome this problem."
                },
                "questions": {
                    "value": "1. In Table 3, why not highlight the best result in column PGD-20?\n\n2. According to the results, the proposed method gives a better improvement on the larger model (WRN-34-10) than the smaller one (RES-18). This is very interesting. Can the author give some explanation and insight about this?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "n/a"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9090/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9090/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9090/Reviewer_eRyw"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9090/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698799242867,
            "cdate": 1698799242867,
            "tmdate": 1699637144828,
            "mdate": 1699637144828,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "upLZw6hV9z",
                "forum": "1zhM0XkQh0",
                "replyto": "dQlxeVqsn9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9090/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9090/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer eRyw"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their encouraging comments and valuable feedback. We clarify their concerns below and upload a new version of the paper incorporating their feedback.\n\n- Clarifications on tables:\n    - Yes, SA stands for standard accuracy in the tables. We mention this in the captions of all tables in the updated draft for better clarity.\n    - In the updated draft, we bold the best results in each row of Table-4 as suggested.\n    - Our training and evaluation uses an $\\ell_\\infty$ perturbation bound of 8/255 as mentioned in Section-2.\n    - We update the terminology used for DeACL results with our teacher model from \"Reproduced\" to \"Our Teacher\" for better clarity.\n- DynACL: While initially we presented only 500 epoch result on the WideResNet-34-10 architecture due to limited compute, we have later verified that our method outperforms even the 1000 epoch run. We present results of a 1000 epoch run for DynACL++ on the WideResNet-34-10 architecture below:\n\n|                        |  |   **CIFAR-10**           |                        | |     **CIFAR-100**          |                         |\n|------------------------|:------------:|:------------:|:----------------------:|:-------------:|:------------:|:----------------------:|\n|                        |    **SA**    | **RA-PGD20** | **RA-AA (AutoAttack)** |     **SA**    | **RA-PGD20** | **RA-AA (AutoAttack)** |\n|  DynACL++ (500 epochs) |     82.27    |     49.60    |          47.12         |     52.59     |     24.22    |          21.27         |\n| DynACL++ (1000 epochs) |     80.97    |     48.28    |          45.50         |     52.60     |     23.42    |          20.58         |\n|          DeACL         |  83.83  |  57.09  |       48.85       |   52.92 |  32.66  |       23.82       |\n|     ProFeAT (Ours)     |  **87.62**  |  54.50 |       **51.95**       |   **61.08**  |  31.96  |       **26.81**       |\n\n\nWe note that the 500 epoch DynACL++ run gives better results, possibly due to robust overfitting in a 1000 epoch run. We added these results in Table-3 of the updated draft.\n\n\n  - Highlighting the best result in column PGD-20:\n     -  The gap between the true robustness of a model (which can be approximated by accuracy against AutoAttack or GAMA) and the robustness against an attack such as PGD-20 (with a small number of optimization steps) depends on the local linearity of the loss surface. For a perfectly linear loss surface, a single-step attack such as FGSM can also find the adversary effectively. However, for more convoluted loss surfaces, multiple attack steps with adaptively reducing step size and better loss functions are required to find the true loss maxima. Thus, a lower gap between PGD-20 and AutoAttack implies better smoothness of the loss surface.\n     - Let us consider a case where the proposed method has better robust accuracy against AutoAttack and worse accuracy against PGD-20 (such as DeACL vs. Ours in Table-3). This happens because the proposed method has a smoother loss surface (due to better robustness) and thus PGD 20 step attack is able to reliably find the adversarial attack that exists. Hence, for a given value of Robust Accuracy against AutoAttack, a lower value of PGD-20 is better than having a higher value. Thus, we believe it is misleading to highlight the best PGD-20 accuracy. We clarify this in Section-2 of the updated draft.\n\n  - Better results on larger models: For a sufficiently complex task, a scalable approach should result in better performance on larger models given enough data. Although the task complexity of adversarial self-supervised learning is high, the gains in prior approaches are marginal with an increase in model size, while the proposed method results in significantly improved performance on larger capacity models. We discuss the key factors that result in better scalability below:\n    - As discussed in Section-4.1, a mismatch between training objectives of the teacher and ideal goals of the student causes a drop in student performance. This primarily happens because of the overfitting to the teacher training task. As model size increases, the extent of overfitting increases. The use of a projection layer during distillation alleviates the impact of this overfitting and allows the student to retain more generic features that are useful for the downstream robust classification objective. Thus, a projection layer is more important for larger model capacities where the extent of overfitting is higher.\n    - Secondly, as the model size increases, there is a need for higher amount of training data for achieving better generalization. The proposed method has better data diversity as it enables the use of more complex data augmentations in adversarial training by leveraging supervision from weak augmentations at the teacher.\n  \nWe hope this clarifies the concerns of the reviewer. We will be happy to answer any further questions as well."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9090/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700016867912,
                "cdate": 1700016867912,
                "tmdate": 1700016867912,
                "mdate": 1700016867912,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "G5L906EYco",
            "forum": "1zhM0XkQh0",
            "replyto": "1zhM0XkQh0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9090/Reviewer_e5nc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9090/Reviewer_e5nc"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a novel self-supervised learning (SSL) method named ProFeAT to enhance the robustness of Deep Neural Networks (DNNs) against adversarial attacks. While supervised adversarial training has proven effective, it demands extensive labeled data, leading to high costs. Previous SSL attempts, including SimCLR and Decoupled Adversarial Contrastive Learning (DeACL), have shown limitations in performance and increased training complexity, particularly with larger models. ProFeAT addresses these issues by incorporating a projection head in the adversarial training step, defining specific attack and defense losses, and employing a mix of weak and strong augmentations for the teacher-student setting. This strategy aims to close the performance gap between self-supervised and supervised adversarial training, enhancing generalization without adding to the training complexity."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. ProFeAT introduces a novel method to improve the robustness of DNNs through self-supervised learning, addressing the challenges of previous SSL adversarial training methods.\n2. The paper provides extensive experimental results, demonstrating the effectiveness of ProFeAT across different datasets and model architectures."
                },
                "weaknesses": {
                    "value": "1. The paper primarily relies on linear probing for evaluation, which is just one of several methods to assess the quality of a trained encoder. It is crucial to explore alternative evaluation techniques such as K-nearest neighbors (KNN) to validate the model's performance comprehensively. Additionally, the effectiveness of the pretraining method in downstream tasks with the finetuning method should be rigorously verified.\n\n2. Section 4.1 lacks compelling evidence and in-depth analysis. The paper lacks a thorough explanation of **why** objective matching leads to improved performance, and there is insufficient exploration of **how** aligning the linear probing objective with pretraining aids in distillation. The correlation between high cosine similarity and low performance is demonstrated in both Table 1, and 2 but lacks meaningful context.\n\n3. While the empirical exploration of simple/difficult augmentation combinations is commendable, the paper lacks a robust analysis or rationale behind why the proposed combinations are deemed the most effective.\n\n4. Although the paper explores various approaches, the methodology lacks strong justification, making it challenging to establish the credibility of the proposed methods. A more thorough and convincing demonstration of these approaches is needed."
                },
                "questions": {
                    "value": "In my view, this paper demonstrates that the use of the freeze teacher projector can provide a slightly more robust constraint, promoting alignment between the student's feature space and that of the teacher, thus facilitating stable student learning. Consequently, the results show only marginal improvements compared to DeACL for smaller models, while proving more effective in scenarios where distillation regularization becomes challenging, particularly as the model scales up in size. I wonder what the authors think about this interpretation."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9090/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698839062609,
            "cdate": 1698839062609,
            "tmdate": 1699637144659,
            "mdate": 1699637144659,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "R1xnM5Hs2o",
                "forum": "1zhM0XkQh0",
                "replyto": "G5L906EYco",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9090/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9090/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer e5nc [1/3]"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their insightful comments and valuable feedback. We clarify their concerns below.\n\n1. **Additional methods for evaluating the pretrained models**: As discussed in Section-2, we primarily rely on the standard linear probing evaluation, as freezing the backbone can give a more reliable estimate of the robustness of the representations learned when compared to a finetuning framework that can alter the representations significantly. However, we agree that other types of evaluations such as KNN and finetuning can be valuable in comprehensively comparing the nature of representations learned. Therefore, as suggested, we present below the performance of the proposed method when compared to the best baseline DeACL on evaluation metrics such as KNN and Adversarial Full Finetuning (AFF):\n - We compare the performance of DeACL (best baseline) and ProFeAT (Ours) on CIFAR-10 and CIFAR-100 datasets with WideResNet-34-10 architecture. The model is first pretrained using the respective self-supervised adversarial training algorithm, and further we compute the standard accuracy (SA) and robust accuracy against GAMA (RA-G) using several methods such as standard linear probing (LP), training a 2 layer MLP head (MLP), and performing KNN in the feature space (k=10).\n\n|       CIFAR-10      | SA (LP) | RA-G (LP) | SA (MLP) | RA-G (MLP) | SA (KNN) | RA-G (KNN) |\n|:-------------------:|:-------:|:---------:|:--------:|:----------:|:--------:|:----------:|\n|        DeACL        |  83.60  |   49.62   |   85.66  |    48.74   |   87.00  |    54.58   |\n|       ProFeAT       |  87.44  |   52.24   |   89.37  |    50.00   |   87.38  |    55.77   |\n\n|      CIFAR-100      | SA (LP) | RA-G (LP) | SA (MLP) | RA-G (MLP) | SA (KNN) | RA-G (KNN) |\n|:-------------------:|:-------:|:---------:|:--------:|:----------:|:--------:|:----------:|\n|        DeACL        |  52.90  |   24.66   |   55.05  |    22.04   |   56.82  |    31.26   |\n|       ProFeAT       |  61.05  |   27.41   |   63.81  |    26.10   |   58.09  |    32.26   |\n\n  -  We note that the proposed method achieves improvements over the baseline across all evaluation methods.       \n      - Since the training of classifier head in LP and MLP is done using standard training and not adversarial training, the robust accuracy reduces as the number of layers increases (from linear to 2-layers), and the standard accuracy improves.\n      - The standard accuracy of KNN is better than the standard accuracy of LP for the baseline, indicating that the representations are not linearly separable. Whereas, as is standard, for the proposed approach, LP standard accuracy is higher than that obtained using KNN.\n      - The adversarial attack used for evaluating the robust accuracy using KNN is generated using GAMA attack on a linear classifier. The attack is suboptimal since it is not generated by using the evaluation process (KNN), and thus the robust accuracy against such a weak attack is higher.\n\n  - We next present the Adversarial Full Finetuning (AFF) performance on the downstream task of transfer learning from CIFAR-10/ CIFAR-100 to STL-10 and Caltech-101 respectively on WideResNet-34-10 architecture. AFF is performed using TRADES algorithm for 25 epochs.\n\n|                     | | CIFAR-10 -> STL10          |       |  |  CIFAR-100 -> STL10        |       |\n|---------------------|:-----------------:|:--------:|:-----:|:------------------:|:--------:|:-----:|\n|          |         **SA**        | **RA-PGD20** |  **RA-G** |         **SA**         | **RA-PGD20** |  **RA-G** |\n| Supervised (TRADES) |       64.58       |   39.83  | 32.78 |        64.22       |   34.20  | 31.01 |\n|        DeACL        |      61.65\t|31.88\t|28.34\t|60.89\t|33.06\t|30.00|\n|       ProFeAT       |       74.12       |   40.15  | 36.04 |        68.77       |   35.35  | 31.23 |\n\n|                     |  | CIFAR-10 -> Caltech101         |       | |  CIFAR-100 -> Caltech101         |       |\n|---------------------|:----------------------:|:--------:|:-----:|:-----------------------:|:--------:|:-----:|\n|          |           **SA**           | **RA-PGD20** |  **RA-G** |            **SA**           | **RA-PGD20** |  **RA-G** |\n| Supervised (TRADES) |    62.46    |   40.77  | 39.40 |          64.97          |   42.95  | 41.02 |\n|        DeACL        |   62.65    |   41.39  | 39.18 |   61.01   |   40.56  | 39.09 |\n|       ProFeAT (Ours)      |   66.11 |   45.29  | 42.12 |    64.16      |   42.95  | 41.25 |\n\n   -  The proposed method outperforms DeACL by a large margin. Further, by using merely 25 epochs of adversarial finetuning, the proposed method achieves improvements of around 4% on CIFAR-10 and 11% on CIFAR-100 when compared to the linear probing accuracy presented in Table-5, highlighting the practical utility of the proposed method. The AFF performance of the proposed approach is better than that of a supervised TRADES pretrained model as well.\n\n                     (response continued in the next comment)"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9090/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700245130851,
                "cdate": 1700245130851,
                "tmdate": 1700415262928,
                "mdate": 1700415262928,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Pwt2DIZqxX",
            "forum": "1zhM0XkQh0",
            "replyto": "1zhM0XkQh0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9090/Reviewer_s59e"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9090/Reviewer_s59e"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes Projected Feature Adversarial Training (ProFeAT) to address the gap between the target of self-supervised training and adversarial training in the teacher-student distillation setting. ProFeAT uses a frozen pretrained projection head from the teacher to isolate impact of distillation loss and prevent the overfitting of the student to the teacher training objective. The performance of ProFeAT is better than existing SSL adversarial training method especially on larger models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is clearly written.\n\nThe experimental result is solid, including models of different sizes and extensive ablation study.\n\nThe performance of ProFeAT is competitive on large-scale models."
                },
                "weaknesses": {
                    "value": "1. There are multiple components in the proposed method, including an additional loss of projection layers, a new attack generation method and weak data augmentation. Although the ablation study includes variants of each component, it is still unknown the specific contribution of each component, or the combination of any two components. A mechanism for why the combination of the three components works is needed. \n\n2. The experiment is done on CIFAR10 and CIFAR100, which contains sufficient data to train adversarially robust models even without distillation and pre-trained models [1]. It is more interesting to see the performance of ProFeAT on low-data tasks such as Caltech [2] as [3] shows that the benefit of pre-trained models mainly manefests in low-data tasks. \n\n[1] Wu, Dongxian, Shu-Tao Xia, and Yisen Wang. \"Adversarial weight perturbation helps robust generalization.\" Advances in Neural Information Processing Systems 33 (2020): 2958-2969.\n[2] Griffin, Gregory, Alex Holub, and Pietro Perona. \"Caltech-256 object category dataset.\" (2007).\n[3] Liu, Ziquan, et al. \"TWINS: A Fine-Tuning Framework for Improved Transferability of Adversarial Robustness and Generalization.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023."
                },
                "questions": {
                    "value": "1. Could the authors show the effect of each component in the proposed method?\n\n2. It is more convincing to show the effectiveness of the proposed method on small datasets such as Caltech. I believe the benefit of using self-supervised pre-training would be significant on such small datasets."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9090/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699281664731,
            "cdate": 1699281664731,
            "tmdate": 1699637144527,
            "mdate": 1699637144527,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KVRGLOOLYO",
                "forum": "1zhM0XkQh0",
                "replyto": "Pwt2DIZqxX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9090/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9090/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer s59e [1/2]"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their encouraging comments and valuable feedback. We present our responses to their questions below. \n\n\n1. **Effect of each component in the proposed method:** While we present extensive ablation experiments for each component of the proposed method, we agree with the reviewer that one table that summarizes it all, and shows the impact of each component/ combination of any two components is a valuable addition to our work. We present a table that shows this below (SA:Standard Accuracy or Clean accuracy, RA-PGD20: Robust Accuracy against PGD-20, RA-G: Robust Accuracy against GAMA):\n\n|E#| Projector | Augmentations | Attack loss  |   SA  | RA-PGD20 |  RA-G |                Comments               |\n|:--:|:---------:|:---------------:|:------------:|:-----:|:--------:|:-----:|-------------------------------------|\n|E1|           |               |              | 52.90 |   32.75  | 24.66 | Baseline (DeACL)                      |\n|E2|     \u2714     |               |              | 57.66 |   31.14  | 25.04 |                                       |\n|E3|           |       \u2714       |              | 52.83 |   35.00  | 27.13 |                                       |\n|E4|           |               |       \u2714      | 51.80 |   31.37  | 24.77 |                                       |\n|E5|           |       \u2714       |       \u2714      | 55.35 |   35.89  | **27.86** |                                       |\n|E6|     \u2714     |               |       \u2714      | 56.57 |   30.54  | 25.29 |                                       |\n|E7|     \u2714     |       \u2714       |              | **62.01** |   31.62  | 26.89 |                                       |\n|E8|      \u2714    |       \u2714       |       \u2714      | 59.65 |   33.03  | 26.90| Defense loss only at the projector  |\n|E9|     \u2714     |       \u2714       |       \u2714      | 61.05 |   31.99  | 27.41 | ProFeAT (Ours) |\n\n - A tick mark in the *Projector* column imples that a frozen pretrained projector is used for the teacher and student, with the defense loss being enforced at the feature and projector as shown in Eq.3 of the main paper.\n\n -  E1 represents the baseline or DeACL defense, and E9 represents the proposed defense or ProFeAT. \n \n - *Projector*: A key component of the proposed method is the introduction of the projector. We observe significant gains in clean accuracy (4.76%) by introducing the frozen pretrained projector along with defense losses at the feature and projector (E1 vs. E2). The importance of the projector is also evident by the fact that removing the projector from the proposed defense results in a large drop (5.7%) in clean accuracy (E9 vs. E5). In combination with other components in the table as well, the projector improves the performance significantly (E4 vs. E6, E3 vs. E7, E5 vs. E9). We observe an improvement of 9.18% in clean accuracy when the projector is introduced in the presence of the proposed augmentation strategy (E3 vs. E7). This is significantly higher than the gains obtained by introducing the projector over the baseline, which is 4.76% (E1 vs. E2). \n\n - *Augmentations*: The proposed augmentation strategy is seen to improve performance, specifically robust accuracy, across all combinations. Introducing the proposed augmentation strategy in the base DeACL defense improves the robust accuracy by 2.47% (E1 vs. E3). The importance of the proposed augmentation strategy is also evident by the fact that in the absence of the same, there is a 4.48% drop in clean accuracy and 2.12% drop in robust accuracy (E9 vs. E6). Further, when combined with other components as well, the proposed augmentation strategy shows good improvements (E4 vs. E5, E2 vs. E7, E6 vs. E9). \n\n - *Attack loss*: The proposed attack objective is designed to be consistent with the proposed defense strategy, where the goal is to enforce smoothness at the student in the feature space and similarity with the teacher in the projector space. As shown in Table-16, the proposed method is not very sensitive to different choices of attack losses. The impact of the attack loss in feature space can be seen in combination with the proposed augmentations, where we observe an improvement of 2.5% in clean accuracy alongside a marginal improvement in robust accuracy (E3 vs. E5). However, in presence of projector, the attack results in only marginal robustness gains, possibly because the clean accuracy is already high (E9 vs. E7). \n\n - *Defense loss*: We do not introduce a separate column for defense loss as it is applicable only in the presence of the projector. We show the impact of the proposed defense losses in the last two rows (E8 vs. E9). The proposed defense loss improves the clean accuracy by 1.4% and robust accuracy marginally.  \n\n          (response continued in the next comment)"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9090/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700326763874,
                "cdate": 1700326763874,
                "tmdate": 1700326763874,
                "mdate": 1700326763874,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DUBf1TZFkK",
                "forum": "1zhM0XkQh0",
                "replyto": "Pwt2DIZqxX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9090/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9090/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer s59e [2/2]"
                    },
                    "comment": {
                        "value": "- *A mechanism for why the combination is needed*: For a sufficiently complex task, a scalable approach should result in better performance on larger models given enough data. Although the task complexity of adversarial self-supervised learning is high, the gains in prior approaches are marginal with an increase in model size, while the proposed method results in significantly improved performance on larger capacity models. We discuss the key factors that result in better scalability below:\n    - As discussed in Section-4.1, a mismatch between training objectives of the teacher and ideal goals of the student causes a drop in student performance. This primarily happens because of the overfitting to the teacher training task. As model size increases, the extent of overfitting increases. The use of a projection layer during distillation alleviates the impact of this overfitting and allows the student to retain more generic features that are useful for the downstream robust classification objective. Thus, a projection layer is more important for larger model capacities where the extent of overfitting is higher.\n    - Secondly, as the model size increases, there is a need for higher amount of training data for achieving better generalization. The proposed method has better data diversity as it enables the use of more complex data augmentations by leveraging supervision from weak augmentations at the teacher.\n\n2. **Performance on small datasets**: We agree with the reviewer that it is important to show the transfer learning performance on small datasets. Towards this, we show the transfer learning performance from CIFAR-10/CIFAR-100 to STL-10 with linear probing in Table-5 of the main paper. However, in practice it is common to finetune the full network for better downstream performance. We thus present transfer learning results to STL-10 and Caltech-101 using lightweight adversarial full finetuning (AFF).\nCaltech-101 contains 101 object classes and 1 background class, with 2416 samples in the train set and 6728 samples in the test set. The number of samples per class range from 17 to 30, and thus this is a suitable dataset to highlight the practical importance of adversarial self-supervised pretrained representations for low-data regime.\nFor Adversarial Full Finetuning (AFF), a base robustly pretrained model is finetuned using the TRADES adversarial training for 25 epochs. We present results on WideResNet-34-10 models that are pretrained on CIFAR-10 and CIFAR-100 respectively. (SA: Standard Accuracy, RA-PGD20: Robust Accuracy against PGD-20, RA-G: Robust Accuracy against GAMA)\n\n\n|                     | | CIFAR-10 -> STL10          |       |  |  CIFAR-100 -> STL10        |       |\n|---------------------|:-----------------:|:--------:|:-----:|:------------------:|:--------:|:-----:|\n|                     |         **SA**        | **RA-PGD20** |  **RA-G** |         **SA**         | **RA-PGD20** |  **RA-G** |\n| Supervised (TRADES) |       64.58       |   39.83  | 32.78 |        64.22       |   34.20  | 31.01 |\n|        DeACL        | 61.65 | 31.88 | 28.34 | 60.89 | 33.06 | 30.00 |\n|       ProFeAT (Ours)      |       74.12       |   40.15  | 36.04 |        68.77       |   35.35  | 31.23 |\n\n\n|                     |  | CIFAR-10 -> Caltech101         |       | |  CIFAR-100 -> Caltech101         |       |\n|---------------------|:----------------------:|:--------:|:-----:|:-----------------------:|:--------:|:-----:|\n|                     |           **SA**           | **RA-PGD20** |  **RA-G** |            **SA**           | **RA-PGD20** |  **RA-G** |\n| Supervised (TRADES) |          62.46         |   40.77  | 39.40 |          64.97          |   42.95  | 41.02 |\n|        DeACL        |          62.65         |   41.39  | 39.18 |          61.01          |   40.56  | 39.09 |\n|       ProFeAT (Ours)      |          66.11         |   45.29  | 42.12 |          64.16          |   42.95  | 41.25 |\n\n - We note that the proposed method outperforms DeACL by a large margin. Further, we note that by using merely 25 epochs of adversarial finetuning using TRADES, the proposed method achieves improvements of around 4% on CIFAR-10 and 11% on CIFAR-100 when compared to the linear probing accuracy presented in Table-5, highlighting the practical utility of the proposed method. The AFF performance of the proposed approach is better than that of a supervised TRADES pretrained model as well.\n\nWe sincerely hope our rebuttal addresses the reviewers concerns. We will be happy to answer any further questions as well."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9090/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700326828917,
                "cdate": 1700326828917,
                "tmdate": 1700415975836,
                "mdate": 1700415975836,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CtEjoaPtnz",
            "forum": "1zhM0XkQh0",
            "replyto": "1zhM0XkQh0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9090/Reviewer_q8j3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9090/Reviewer_q8j3"
            ],
            "content": {
                "summary": {
                    "value": "This paper analyzed and improved the robustness of representation learned by self-supervised contrastive learning methods against adversarial attacks. They find that there has a significant gap in performance in the bigger models (e.g. WideResNet-34-10) for the existing techniques. They attribute for the mismatch between the objective of the teacher and student models. And they find that an additional projector layer with some losses can help to mitigate such discrepancy and improve performance. The evaluations have been done on CIFAR-10 and CIFAR-100."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The method is simple with the use of a projector network for adversarial training\n- The effectiveness in the metrics standard accuracy (SA) and robustness against AutoAttack (AA) is encouraging\n- The writing is clear"
                },
                "weaknesses": {
                    "value": "\u2022\tThe novelty is marginal: The projector is widely used in SSL which can significant boost performance when evaluating the representations after the backbone network. Here, the use of such projector can improve the representation (as demonstrated in SA metric) is not surprising in self-supervised learning for adversarial attack. Also, the use of weak augmentation for the teacher and strong one for student is exploited in several prior works. \n\u2022\tThe robust accuracy (RA) is an important metric in evaluation of the robustness but it seems to be omitted in the main paper (table 3,4,5,6), could the author provide the results and analysis of this metric side by side, too?\n\u2022\tThe PGD-20 metric of the proposed method is pretty worse than the other SOTA in most cases but it is not adequately discussed or mentioned. Could the authors provide some intuitions why does such degradation on PGD-20 happen, any investigation to address that drawback?\n\u2022\tClarity: 1) It should be consistent in the style, for example, table 3, 5 where the bold results show the best performance but table 4, 6, \u2026 are not highlighted, making it hard to follow which one is better. 2) It should be also consistent to report the metric in table 3,4,5,6, for example, table 3 used the mixture \u201cSA\u201d and \u201cAutoAttack\u201d, while table 4 used the full metric \u201cStandard Accuracy\u201d, etc\u2026 the consistency should be done for all tables. 3) Since AA has been used for \u201cAutoAttack\u201d, it should be used differently for AA when referring the AutoAugment (table 8) to avoid confusing."
                },
                "questions": {
                    "value": "See weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9090/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699525335920,
            "cdate": 1699525335920,
            "tmdate": 1699637144370,
            "mdate": 1699637144370,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "e4pUH0nP7F",
                "forum": "1zhM0XkQh0",
                "replyto": "CtEjoaPtnz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9090/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9090/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer q8j3 [1/2]"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their valuable feedback. We clarify their concerns below and upload a new version of the paper incorporating their feedback. \n\n-  Novelty:\n   - The removal of last few layers for better generalization was first introduced in a transfer learning setting in 2014 [1] and later adapted to self-supervised learning in the seminal work SimCLR [2]. Despite this, there have been several recent works on the use of projection layer and understanding its role in self-supervised and supervised learning, as highlighted by Bordes et al. (2023)[3], and this is still an active area of research. We propose to use a frozen pretrained projector for self-supervised adversarial training using distillation from a standard SSL trained network, which is significantly different from all existing works. We further hypothesize why the projector helps, and verify the same using well-designed experiments in both standard and adversarial self-supervised distillation.\n   - We find that the use of strong augmentations results in more diverse attacks as shown in Fig.2. While this results in better robust generalization, it also leads to a more complex training objective. The proposed approach maps the representations corresponding to these diverse strong augmentations at the student to the representations corresponding to weak augmentations (which are close to the unaugmented images) at the teacher, allowing the student to be invariant to strong augmentations without increasing the training complexity. Although a combination of weak and strong augmentations is used in other areas such as semi-supervised learning as well, the justification for using this strategy is different.\n   \n    The effective use of augmentations in supervised and self-supervised *adversarial training* has been very challenging, and it is common to use the basic pad+crop augmentation alone for adversarial training [4-7]. Thus, although the use of weak and strong augmentations together is not new, we believe there is significant value in leveraging this for self-supervised adversarial training using a distillation framework, where the need for such a method is different from prior works.\n   \n - We list our contributions below for better clarity -\n\n    - Our finding that the use of a projection layer during self-supervised distillation can bridge the gap in clean accuracy between self-supervised adversarial training and supervised adversarial training methods\n    - Our hypotheses on why the projection layer helps, which is not only a valuable contribution to adversarial self-supervised learning but can also motivate a better understanding of the role of the projector in standard self-supervised distillation. We support our hypotheses using well-designed experiments in Tables-1 and 2.\n    - The use of weak and strong augmentations for the teacher and student respectively, to strike a balance between improving perturbation diversity and limiting training complexity.\n    - Understanding the role of different training losses at the feature and projector.\n    - Strong empirical results that push the state-of-the-art in self-supervised adversarial training by a large margin, and bridge the gap with respect to supervised adversarial training methods.\n\n[1] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep neural networks? In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger (eds.), Advances\nin Neural Information Processing Systems, volume 27. Curran Associates, Inc., 2014.\n\n[2] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. ICML 2020\n\n[3] Florian Bordes, Randall Balestriero, Quentin Garrido, Adrien Bardes, and Pascal Vincent. Guillotine regularization: Improving deep networks generalization by removing their head., TMLR 2023.\n\n[4] L. Rice, E. Wong, and J. Z. Kolter. Overfitting in adversarially robust deep learning. ICML 2020\n\n[5] S. Gowal, C. Qin, J. Uesato, T. Mann, and P. Kohli. Uncovering the limits of adversarial training against norm-bounded adversarial examples. arXiv preprint arXiv:2010.03593, 2020.\n\n[6] H. Zhang, Y. Yu, J. Jiao, E. Xing, L. El Ghaoui, and M. I. Jordan. Theoretically principled trade-off between robustness and accuracy. ICML 2019\n\n[7] Chaoning Zhang, Kang Zhang, Chenshuang Zhang, Axi Niu, Jiu Feng, Chang D Yoo, and In So Kweon. Decoupled adversarial contrastive learning for self-supervised adversarial robustness. ECCV 2022\n\n           (response continued in the next comment)"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9090/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699948008598,
                "cdate": 1699948008598,
                "tmdate": 1700374728231,
                "mdate": 1700374728231,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]