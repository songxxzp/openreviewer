[
    {
        "title": "Hierarchical-Latent Generative Models are Robust View Generators for Contrastive Representation Learning"
    },
    {
        "review": {
            "id": "aC6HN2wZTS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3563/Reviewer_5q8s"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3563/Reviewer_5q8s"
            ],
            "forum": "vYF1q2Fqhk",
            "replyto": "vYF1q2Fqhk",
            "content": {
                "summary": {
                    "value": "In this paper, the authors introduce and explore a category of generative models termed Hierarchical-Latent (HL) models. This work demonstrates how the distinctive characteristics of these models, which operate using multiple latent spaces in a hierarchical fashion, can be harnessed to generate robust positive views for contrastive representation learning. By employing HL models as data sources for self-supervised learning and devising specific perturbation strategies for different latent vectors, the authors achieve significant improvements in representation learning compared to previous methods and even surpass the performance of training on real data. Additionally, this study proposes a continuous sampling approach for generating additional training data in real time, revealing its competitive or faster performance compared to traditional data loading techniques. Overall, the paper formalizes the HL model category, highlights its effectiveness in self-supervised learning, and introduces a practical method for augmenting training data."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "(1) This paper introduces the new concept of Hierarchical-Latent (HL) models and their application in self-supervised learning. The formalization of HL models as a distinct category within generative models, along with their utilization for data generation, presents a fresh perspective.\n\n(2) The paper meticulously conducts experiments, compares the proposed approach with existing methods, and includes comprehensive ablation studies to validate its efficacy. Furthermore, the introduction of the continuous sampling technique to augment training data is practical.\n\n(3) The paper is clear in presentation. This work effectively describes the concepts, with a logical flow from defining HL models to their application in self-supervised learning. The use of illustrative figures enhances comprehension.\n\n(4) This paper holds significance in the fields of self-supervised learning and generative models. The proposal of continuous sampling as a means to augment training data online addresses a pertinent issue, potentially reducing the gap between synthetic and real data in classifier training."
                },
                "weaknesses": {
                    "value": "(1) The paper lacks a deeper theoretical analysis of why and how the Hierarchical-Latent models would significantly enhance the paper. Providing theoretical insights into the relationships between hierarchical latent, perturbations, and representation learning could strengthen the paper's contributions.\n\n(2) This work lacks a comprehensive comparison with strong baseline methods. A more extensive set of baseline models and techniques should be considered to provide a more thorough evaluation of the proposed approach.\n\n(3) Visualization of the hierarchy of latent spaces in HL models or diagrams illustrating this hierarchical structure would enhance understanding. \n\n(4) A more in-depth discussion of the trade-offs and limitations of using generative models for self-supervised learning compared to real data would be valuable."
                },
                "questions": {
                    "value": "(1) Could the authors provide more theoretical insights into the hierarchical nature of HL models' latent spaces and their implications for representation learning? \n\n(2) Could the authors expand the baseline comparisons by including more state-of-the-art self-supervised learning methods and possibly discussing how the proposed approach compares to conventional supervised learning using real data?\n\n(3) Would it be possible to include visualization to illustrate the hierarchical structure of latent spaces in HL models? \n\n(4) Have the authors examined or experimented with the adaptability of HL models and the suggested approach in fields beyond images, such as text or audio data?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3563/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697351268377,
            "cdate": 1697351268377,
            "tmdate": 1699636310933,
            "mdate": 1699636310933,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "k8BQ4tsBl5",
                "forum": "vYF1q2Fqhk",
                "replyto": "aC6HN2wZTS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3563/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3563/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Theoretical insights into the relationships between hierarchical latent, perturbations, and representation learning."
                    },
                    "comment": {
                        "value": "As per the relation between latent perturbations and the generation of views for representation learning, the used procedure (Eq. 3 of our paper) points at maximizing the distance between the two latent vectors $d(\\mathbf{z}, T_\\mathbf{z}(\\mathbf{z}))$, while maximizing the mutual information between the generated views $g(\\mathbf{z}), g(T_\\mathbf{z}(\\mathbf{z}))$. As demonstrated by Oord et al. (2018), maximizing a lower bound on mutual information is equivalent to minimizing InfoNCE Loss, which is the term optimized in Equation 3. This procedure has been previously introduced and theoretically motivated in Li et al. (2022b). \n\nIn our work, we extended such findings and procedures to generative models presenting multiple layers of latent variables, which we name Hierarchical Latent. In detail, we show a relation between the latent variables' hierarchical level and the amount of perturbation that can be applied to them, while still maintaining semantic consistency. \n\nIn the updated version of the paper, we included some insights in the paragraph above Equation 3, to better explain why InfoNCE loss can be used to find optimal latent transformations in the context of representation learning.\n\n_References_\n\nLi et al. (2022 b): Yinqi Li, Hong Chang, Bingpeng Ma, Shiguang Shan, and Xilin Chen. Optimal positive generation via latent transformation for contrastive learning. Advances in Neural Information Processing\nSystems, 35:18327\u201318342, 2022b.\n\nOord et al. (2018) Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive\ncoding. arXiv preprint arXiv:1807.03748, 2018"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3563/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699974123784,
                "cdate": 1699974123784,
                "tmdate": 1699974123784,
                "mdate": 1699974123784,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GAsjvWRk6r",
                "forum": "vYF1q2Fqhk",
                "replyto": "aC6HN2wZTS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3563/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3563/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Requests for more experiments comparing with different Self-Supervised methods and Supervised Training"
                    },
                    "comment": {
                        "value": "We would like to point out that the main scope of the paper is to show the benefits that a smart use of Hierarchical Latents can provide in the context of __view generation__ for representation learning. Under this perspective, the baselines to consider are methods that use a generative model for view generation. To the best of our knowledge, the only two methods that actually perform such task are Li et al. (2022b) and Jahanian et al. (2021), which are extensively compared in the experimental sections. It is also valuable to note that we already extended the benchmark proposed by these papers, which solely utilized SimCLR as the self-supervised framework. For this purpose, all the baselines were re-implemented and further evaluated using the SimSiam framework, as reported in the right part of Table 2. \n\nAs for the comparison with supervised methods using real data, we included new results as an additional row in Table 4 of the Appendix. In detail, results are computed for the same ResNet50 backbone, trained on Imagenet-1k in a supervised manner and evaluated on all the transfer classification datasets presented in the paper. As expected, supervised training performs better than self-supervised. This is true for both the source dataset (Imagenet-$1$K) and most of the transfer target datasets. We thank the Reviewer for the suggestion, which broadens the experimental evaluation.\n\n_References_\n\nLi et al. (2022 b): Yinqi Li, Hong Chang, Bingpeng Ma, Shiguang Shan, and Xilin Chen. Optimal positive generation via latent transformation for contrastive learning. Advances in Neural Information Processing\nSystems, 35:18327\u201318342, 2022b.\n\nJahanian et al. (2021): Ali Jahanian, Xavier Puig, Yonglong Tian, and Phillip Isola. Generative models as a data source\nfor multiview representation learning. In International Conference on Learning Representations,\n2021"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3563/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699974271833,
                "cdate": 1699974271833,
                "tmdate": 1699974271833,
                "mdate": 1699974271833,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jZPpepDN2A",
                "forum": "vYF1q2Fqhk",
                "replyto": "aC6HN2wZTS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3563/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3563/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Other Questions (points 3 and 4)"
                    },
                    "comment": {
                        "value": "3. __About visualization of the hierarchical structure of latent spaces.__ We added a section in the Supplementary material (appendix A), where a schematic structure of a generic HL generative model is given. We hope that this can help in better understanding the HL models architecture. We also point the Reviewer to Brock et al (2018) and Karras et al (2020), where complete pictures of the architectures used can be found.\n\n4. __About the adaptability of HL models on other modalities.__ We thank the reviewer for the precious suggestion, which we did not consider in our paper and poses an interesting basis for possible future research directions. In our work, we considered image data since HL generative models are well established in such domain. In fact, images present a global-to-local structure that well fits the HL framework. Interestingly, similar structures can be found also for audio data. For example, Hono et al. (2020) proposed a HL model for speech synthesis. On the other hand, we did not find similar architectures for text data, where generative models do not usually consider a hierarchical structure with multiple input latent vectors.\n\n_References_\n\nBrock et al. (2018): Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural\nimage synthesis. In International Conference on Learning Representations, 2018\n\nKarras et al. (2020): Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Ana-\nlyzing and improving the image quality of stylegan. Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pp. 8107\u20138116, 2020.\n\nHono et al. (2020): Hono, Y., Tsuboi, K., Sawada, K., Hashimoto, K., Oura, K., Nankaku, Y., Tokuda, K. (2020). Hierarchical multi-grained generative model for expressive speech synthesis. arXiv preprint arXiv:2009.08474."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3563/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699974386408,
                "cdate": 1699974386408,
                "tmdate": 1699974386408,
                "mdate": 1699974386408,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "lUGm7cqiaF",
            "forum": "vYF1q2Fqhk",
            "replyto": "vYF1q2Fqhk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3563/Reviewer_DrZA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3563/Reviewer_DrZA"
            ],
            "content": {
                "summary": {
                    "value": "This paper discusses the use of hierarchical-latent generative models as robust view generators for contrastive representation learning. The authors propose a framework that utilizes the properties of hierarchical-latent models to create robust views for contrastive learning, outperforming previous methods and even surpassing approaches trained with real data. The model is novel and the performance is shown to be better than the current baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "(1) The problem that the paper aims to solve is significant and the method propose is feasible.\n(2) The paper forms the problem in a theoretical way, which is technically sound.\n(3) The performance of the proposed method is superior based on the experimental section."
                },
                "weaknesses": {
                    "value": "(1) In proposition 2.2, the paper says g(.) is a mapping to x but in the equation it says g(.) maps to y instead.\n(2) The paragraph above assumption 3.1, the paper says the last layer model fine-grained details of the data, but I think this can also be done but the last layer of any neural networks, such as MLP.\n(3) I would also suggest the paper to add complexity analysis to their method as this might be a concern if too many latent variables have to be generated."
                },
                "questions": {
                    "value": "Please refer to my comments under \"Weakness\"."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3563/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698517575792,
            "cdate": 1698517575792,
            "tmdate": 1699636310860,
            "mdate": 1699636310860,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4Dfcv0Cpds",
                "forum": "vYF1q2Fqhk",
                "replyto": "lUGm7cqiaF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3563/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3563/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "1. __Notation in Proposition 2.2__: The purpose of the $\\rightarrow$ operator in the Proposition was to denote that the two generated images $g(\\mathbf{z}_i, \\pmb{\\theta})$ and $g(\\mathbf{z}', \\pmb{\\theta})$ are mapped into the same semantic label $\\mathbf{y}$. \nWe acknowledge that it is ambiguous, and thus we changed it in the newly uploaded version of the paper: \n\n\\\\[ \n\\delta_i = \\max_{\\mathbf{z'}} \\text{dist} (\\mathbf{z}_i, \\mathbf{z'}) \\quad \\text{ such that } \\quad F\\_{\\mathcal{T}}(g(\\mathbf{z}_i, \\pmb{\\theta})) = F\\_{\\mathcal{T}}(g(\\mathbf{z'}, \\pmb{\\theta})) = \\mathbf{y} \n\\\\]\n\nwhere $\\mathbf{z'} \\in \\mathbb{R}^n$ is a generic vector in the generator's latent space and $F_{\\mathcal{T}}$ is an oracle classification function for task $\\mathcal{T}$, which assigns the true semantic label $\\mathbf{y}$ to any image.\n\n2. __Paragraph above Assumption 3.1__: The reviewer is right. In fact, in the paragraph and in Definition 3.1, we are not assuming any particular internal structure for the generator network $g$, which could possibly be a simple MLP for simpler tasks. What we want to stress here is that latent variables $\\{ \\mathbf{z}^0, \\mathbf{z}^1, \\dots, \\mathbf{z}^{n-1} \\}$ operate at progressive layers as inputs of the generator. This makes their contributions different, since later layers model more fine-grained details of the data.\n\n3. __Complexity__: our method does not introduce any additional complexity, which only depends on the choice of the generator architecture. In our work, we utilize two well-known Generative Adversarial Networks (BigBiGan and StyleGan2), which, as any GAN, present the advantage of a constant inference time. We take advantage of this fact by also proposing _continuous sampling_, as outlined in the last part of Section 4.1."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3563/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699973960932,
                "cdate": 1699973960932,
                "tmdate": 1699973960932,
                "mdate": 1699973960932,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "EXEuGSGv8E",
            "forum": "vYF1q2Fqhk",
            "replyto": "vYF1q2Fqhk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3563/Reviewer_o5fU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3563/Reviewer_o5fU"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on using image generation models for contrastive self-supervised learning. Specifically, it employs a hierarchical structure of multiple latent vectors in the generative models to change the global to local information of generated images. New images are generated based on progressive perturbations of the latent vectors through hierarchies. The key message is that as the level in the hierarchy increases (from global latent variables to local ones), the perturbation distances in the latent space to change the semantics of generated images increase (i.e., at more local levels of latent vectors, changing the semantics of generated images requires more aggressive perturbation.) The authors then use the hierarchical regime to generate images for self-supervised learning and propose to continuously sample generated images to increase the total training size. The authors show SOTA results on training SimCLR and SimSiam using BigBiGan and StyleGAN2 to generate images.\n\nDespite good efforts, the current shape of the paper lacks important technical details to make it sound and clear enough to be accepted at ICLR."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Originality: this work starts from an interesting perspective: using hierarchical levels of latent codes to generate images for representation learning and study the effect of perturbation in terms of the maximum distance allowed in the latent space to avoid semantic change. This is new to the reviewer\u2019s knowledge.\n\nClarity: overall, the paper is clearly written, with an easy-to-follow presentation. The theoretical results are clearly stated, generated samples are nicely illustrated, and empirical results are concisely presented.\n   \nSignificance: using generative models as data sources is an interesting and important problem, and this paper provides potentially useful techniques and insights for future researchers."
                },
                "weaknesses": {
                    "value": "Quality: there are severe gaps in the theoretical part despite the adequate empirical results and analyses. The details are in the questions section."
                },
                "questions": {
                    "value": "1. **Proposition 2.2**: it is very unclear how the authors deduced such an important statement, which itself also has ambiguity. The authors claim Proposition 2.2 is a reformulation from Li et al. (2022b), but there are no proofs or theoretical discussions that support this (upon the reviewer\u2019s inspection, there is no directly equivalent statement in Li et al. (2022b)). In Li et al. (2022b), semantic consistency is defined as a bounded difference between mutual information terms, where the mutual information is between the generated images and the label. However, none of these is clearly stated in this submission, nor are there any discussions or linkages. In terms of the ambiguity, what does the right arrow mean? Is it convergence in probability, in distribution, or something else? Why does such convergence (or the mathematical property the authors intended to show) suffice the expressing the semantic consistency?\n\n2. **Table 1**: it is quite debatable if InfoNCE loss is a good metric to measure semantic shift/consistency of images, which is also a central component this work\u2019s conclusions rely on (Table 1 directly supports Assumption 3.1 empirically, which is the key message of the paper and does not have other rigorous theoretical justification). The authors did not justify at all why InfoNCE loss can align well with the intention of Proposition 2.2 regarding Semantic Consistency. In Proposition 2.2, semantic consistency is defined in terms of the consistency of true labels. However, the authors did not show how InfoNCE can *preserve* or approximate such consistency property when labels are not present. The absence of task information/labels does not automatically justify InfoNCE as the valid metric.\n\nOther questions or comments:\n1. It is not very clear to the reviewer why, in Table 1 left, different chunks use a different number of training samples for the Monte Carlo simulations.\n2. Whether \u201crobust\u201d is the best word for this submission is debatable when it essentially means task-agnostic."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3563/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3563/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3563/Reviewer_o5fU"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3563/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698694075048,
            "cdate": 1698694075048,
            "tmdate": 1699636310765,
            "mdate": 1699636310765,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wY3d8FQifi",
                "forum": "vYF1q2Fqhk",
                "replyto": "EXEuGSGv8E",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3563/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3563/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Notation and doubts about Proposition 2.2"
                    },
                    "comment": {
                        "value": "we acknowledge that the meaning of the $\\rightarrow$ symbol is ambiguous. It was meant to denote that the two generated images $g(\\mathbf{z}_i, \\pmb{\\theta})$ and $g(\\mathbf{z}', \\pmb{\\theta})$ are mapped to the same semantic label $\\mathbf{y}$. For this reason, the symbol has been removed and replaced with a new formulation:\n\n__Proposition 2.2__: (Semantic Consistency in the Latent Space - (reformulation))  \n\nLet $g(\\mathbf{z}, \\pmb{\\theta}) = \\mathbf{x}$ be a generative model with parameters $\\pmb{\\theta}$ that maps from latents $\\mathbf{z} \\in \\mathbb{R}^n$ to images $\\mathbf{x} \\in \\mathbb{R}^d$. Consider a downstream task $\\mathcal{T}$ with label $\\mathbf{y} \\in \\mathcal{Y}$, and some distance metric *dist* defined in the generator's latent space. \nThen, for any random latent vector $\\mathbf{z}_i$, the term $\\delta_i$ indicates the maximum distance in the latent space of $g$ to ensure semantic consistency for task $\\mathcal{T}$:\n\n\\\\[ \\delta_i = \\max_{\\mathbf{z'}} \\text{dist} (\\mathbf{z}_i, \\mathbf{z'}) \\quad \\text{such that} \\quad  F\\_{\\mathcal{T}}(g(\\mathbf{z}_i, \\pmb{\\theta})) =  F\\_{\\mathcal{T}}(g(\\mathbf{z'}, \\pmb{\\theta}))  =  \\mathbf{y} \\\\]\n\nwhere $\\mathbf{z'} \\in \\mathbb{R}^n$ is a generic vector in the generator's latent space and $F_{\\mathcal{T}}$ is an oracle classification function for task $\\mathcal{T}$, which assigns the true semantic label $\\mathbf{y}$ to any image.  \n __********__\n\nRegarding the connection with Proposition 3.1 of Li et al. (2022b), it states:\n\n\"Given a well-trained unconditional generative model $g$, $\\mathbf{z}$ and $\\mathbf{z}'$ are two latent vectors, __if the distance__ $d(\\mathbf{z}, \\mathbf{z}') \\le \\delta$, __then__ $g(\\mathbf{z})$ __and__ $g(\\mathbf{z}')$ __will have the similar semantic label__ $\\mathbf{y}$.  \n\nIn the form of mutual information, we have: \n\n\\\\[\n    |I(g(\\mathbf{z}); \\mathbf{y}) - I(g(\\mathbf{z}' ); \\mathbf{y})| \\le \\epsilon\n\\\\]\n\nwhere $\\epsilon$ stands for tolerable semantic difference, and $\\delta$ __is the maximum shifted distance to maintain semantic consistency__.\"\n\nIn our proposed reformulation (Proposition 2.2), 1) we omitted the mutual information part since not in the scope of our work: we are mostly interested in defining what the term $\\delta$ represents, that is, the semantic consistency in the latent space of a generator $g(\\mathbf{z}, \\pmb{\\theta})$; 2) we formalize the concept of \"__having the similar semantic label__ $\\mathbf{y}$\" by means of the oracle classifier $F_{\\mathcal{T}}$.\n\nIn conclusion, __both propositions state__ that for each latent vector $\\mathbf{z}_i$, it exists a $\\delta_i$ ensuring that an image $g(\\mathbf{z}', \\pmb{\\theta})$ shares the same semantic label as $g(\\mathbf{z}_i, \\pmb{\\theta})$, as long as $d(\\mathbf{z}_i, \\mathbf{z}') \\le \\delta_i$.\n\n_References_ \n\nLi et al. (2022 b): Yinqi Li, Hong Chang, Bingpeng Ma, Shiguang Shan, and Xilin Chen. Optimal positive generation via latent transformation for contrastive learning. Advances in Neural Information Processing\nSystems, 35:18327\u201318342, 2022b."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3563/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699973255469,
                "cdate": 1699973255469,
                "tmdate": 1699973489970,
                "mdate": 1699973489970,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hWMPcxSAO0",
                "forum": "vYF1q2Fqhk",
                "replyto": "EXEuGSGv8E",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3563/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3563/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Table 1 and InfoNCE loss"
                    },
                    "comment": {
                        "value": "As expressed in Section 3.2, the results of Table 1 are obtained utilizing the procedure reported in Eq. 3, which uses InfoNCE as a mutual information estimator. This procedure has been introduced and used by Li et al. (2022b) (the same formula is presented in Equation 6 of their paper). In the same work, a theorem and formal proof are given, supporting the validity of the procedure.\n\nIn short, the equation (their Eq. 6) consists of a min-max game between the minimization of InfoNCE Loss and the maximization of $d(\\mathbf{z}, T_\\mathbf{z}(\\mathbf{z}))$. In this context, the minimization of InfoNCE Loss corresponds to a maximization of the Mutual Information between views, making it a valid choice for our purposes. This property of InfoNCE loss __is demonstrated in Oord et al. (2018)__\n\nWe thank the Reviewer for raising this point. In the new version of the paper, we modified the paragraph above Equation 3, in order to stress the relation between mutual information and InfoNCE loss.\n\n_References_ \n\nLi et al. (2022 b): Yinqi Li, Hong Chang, Bingpeng Ma, Shiguang Shan, and Xilin Chen. Optimal positive generation via latent transformation for contrastive learning. Advances in Neural Information Processing\nSystems, 35:18327\u201318342, 2022b.\n\nOord et al. (2018) Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive\ncoding. arXiv preprint arXiv:1807.03748, 2018"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3563/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699973459973,
                "cdate": 1699973459973,
                "tmdate": 1699973459973,
                "mdate": 1699973459973,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mVFW0jXFnO",
                "forum": "vYF1q2Fqhk",
                "replyto": "EXEuGSGv8E",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3563/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3563/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Further Questions or Comments"
                    },
                    "comment": {
                        "value": "1. Table 1 shows that different chunks require a different number of training samples for the Monte Carlo simulations to achieve a similar InfoNCE loss. The intuition is that walkers $T_\\mathbf{z}$ responsible for fine grained details (i.e. acting on high level chunks) must be trained a lot more to obtain a level of perturbation consistent with previous walkers (i.e. low level chunks). Note that each training starts with the same hyperparameters (optimizer, learning rate) and is initialized as the identity mapping function (at training step 1 we have $T_\\mathbf{z}(\\mathbf{z}) = \\mathbf{z}$).\n\n2. The term task-agnostic can indicate almost any self-supervised learning method, since in general the downstream task is always unknown (as also in Li et al. (2022b)). However, we show in the experimental results that our method is robust across a range of downstream tasks, thus the term \"robust\" is used.\n\n_References_\n\nLi et al. (2022 b): Yinqi Li, Hong Chang, Bingpeng Ma, Shiguang Shan, and Xilin Chen. Optimal positive generation via latent transformation for contrastive learning. Advances in Neural Information Processing\nSystems, 35:18327\u201318342, 2022b."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3563/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699973629028,
                "cdate": 1699973629028,
                "tmdate": 1699973629028,
                "mdate": 1699973629028,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eVT11qwSeq",
                "forum": "vYF1q2Fqhk",
                "replyto": "hWMPcxSAO0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3563/Reviewer_o5fU"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3563/Reviewer_o5fU"
                ],
                "content": {
                    "title": {
                        "value": "Response to the authors"
                    },
                    "comment": {
                        "value": "The reviewer thanks the authors for their substantial efforts to improve the draft and clarify the questions. \n\nInfoNCE is a lower bound of mutual information between views, but why it is a good measurement in Table 1 for semantic consistency / semantic shift with respect to labels is not theoretically justified. If the reviewer understands correctly, from Li et al. (2022 b), semantic consistency in latent space holds *only if* $d\\left(\\mathbf{z}, T_{\\mathbf{z}}(\\mathbf{z})\\right) \\leq \\delta$. Then, we can create optimal views and learn representation by maximizing InfoNCE bound (or equivalently minimizing the loss). This means semantic consistency with respect to labels is deduced from $d\\left(\\mathbf{z}, T_{\\mathbf{z}}(\\mathbf{z})\\right) \\leq \\delta$, not simply InfoNCE. In an oversimplified fashion, it seems that the logic is $d\\left(\\mathbf{z}, T_{\\mathbf{z}}(\\mathbf{z})\\right) \\leq \\delta \\rightarrow \\text{semantic consistency} \\rightarrow \\text{InfoNCE}$. \n\nHowever, in Table 1, InfoNCE is *directly* used to measure the semantic consistency/shift *without* any condition from $d\\left(\\mathbf{z}, T_{\\mathbf{z}}(\\mathbf{z})\\right)$. It is even the other way around: InfoNCE becomes a measurement to indicate whether different $d\\left(\\mathbf{z}, T_{\\mathbf{z}}(\\mathbf{z})\\right)$ preserves semantic consistency. In this case, an oversimplified logic is $\\text{InfoNCE} \\rightarrow \\text{semantic consistency} \\rightarrow d\\left(\\mathbf{z}, T_{\\mathbf{z}}(\\mathbf{z})\\right) \\leq \\delta$. This is quite different from the first logic shown above. \n\nWhy InfoNCE can measure semantic consistency w.r.t. labels is not clear. An example would be two images: one depicting a small seagull in the sky and the other depicting a small sailboat in the sea. There can be substantial mutual information between the two structurally and pixel-wise similar images (a largely blue background with a small white object at the center), but the labels are different. The reviewer would appreciate any further clarification from the authors."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3563/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700231176327,
                "cdate": 1700231176327,
                "tmdate": 1700231176327,
                "mdate": 1700231176327,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]