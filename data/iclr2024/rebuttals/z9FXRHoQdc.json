[
    {
        "title": "Best Response Shaping"
    },
    {
        "review": {
            "id": "G7OOQ9bDcz",
            "forum": "z9FXRHoQdc",
            "replyto": "z9FXRHoQdc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3705/Reviewer_1vtA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3705/Reviewer_1vtA"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a learning method to discover an NE solution that discovers maximum social welfare. The idea extends the LOLA and POLA works by directly applying MARL with backpropagation through the entire opponent policy. The experiments are conducted on the IPD game and the coin game."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "In general, I appreciate the attempt to extend the existing works further, from a few RL steps to the policy change via policy conditioning. However, the paper has significant technique issues."
                },
                "weaknesses": {
                    "value": "### Fundamental Issue\n\nThe fundamental issue of this work is that the paper directly assumes **symmetric NEs** throughout the entire paper. If we know **in advance** that the desire NE **must** be symmetric, then all the technical derivations in this paper are correct. However, although IPD falls into this category, this remains an extremely strong assumption and you cannot really leverage this assumption during algorithmic design, which, to some extent, is a cheating strategy. \n\nI want to remark that LOLA does not adopt parameter sharing during learning. \n\nI also want to mention that a symmetric game does not mean that the NE is symmetric. Let's consider a particular symmetric 2-player matrix game with a payoff matrix as\n```\n(0, 0), (1,1)\n(1,1), (0,0)\n```\nThis is an XOR game where the NE should be two agents taking different actions, which cannot represented by policies with shared parameters. You may refer to [this paper](https://arxiv.org/abs/2206.07505) for a discussion on the XOR game.\n\nAnother example would be a modified chicken game as shown below. \n```\n(0, 0), (0,1)\n(1,0), (-1,-1)\n```\nThe NE solution is the same as the XOR game. \n\nI also want to point out that the claims in Section 4.3.2 are particularly problematic. \n1. \"_this is equivalent to training an agent with self-play with reward sharing_\". This claim is **wrong**. Equation (7) is only equivalent to **reward sharing _with parameter sharing_**. \"Self-play with reward sharing'' should refer to the case where \"you learn two different policies with different parameters while the game reward is shared across agents\". So, self-play with reward sharing should be able to learn an NE in the XOR game while equation 7 can never due to parameter sharing\n2. \"_In zero-sum games, this update will have no effect as the gradient would be zero_\". To be frank, I was deeply shocked by such a conclusion at the very beginning when I read this sentence. Then I find it correct but meaningless: a game is both symmetric and zero-sum if and only if it is a zero game, which means you have zero gradient everywhere. \n\nFinally, the concept of _symmetric game_ only applies to two-player games. For general $N$-player games, you are essentially assuming the game is cooperative. \n\n### Minor Issues\n\n1. Section 2.1 presents a $N$-player game formulation. However, the entire paper adopts a two-player setting. \n2. At the end of section 4.1, it is stated that \"_Note that ($\\theta^{\u2217\u22171}$, $\\theta^{\u22172}$) is a nash equilibrium by definition._\" This sounds wrong to me. An NE should be defined over a minimax operator rather than two separate equations. A simple counter-example here is the rock-paper-scissor game. Suppose initially you have $\\theta^1$ to be rock. Then $\\theta^{*2}$ will be paper, and $\\theta^{\u2217\u22171}$ becomes scissor, which is clearly not an NE. \n3. The presentation of Section 4.2 is hard to follow. A few examples. What do you mean by \"detective\u2019s conditioning\"? I would be better to have a formulation of what you want to present. It is also mentioned \"the behavior of the agent against a random agent\" in Section 4.2.2. Here the word \"agent\" is mentioned twice. So which is agent 1 and which is agent 2? I guess you are using agent to refer to agent 1 while the detective to agent 2 but it is not always consistent. \n4. There are also notation issues. I'm always confused by which agent an action refers to. Moreover, in Equation 6, $b$ is used to denote the action from agent 2. However, the notation of $b$ is never introduced before."
                },
                "questions": {
                    "value": "First of all, I think the paper should be substantially rewritten.\n\nBesides, I would be also curious about how your algorithm will perform on cases where (1) the NE is asymmetric and (2) the parameter sharing is turned off, i.e., two agents have two separate policies."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3705/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697640697821,
            "cdate": 1697640697821,
            "tmdate": 1699636326799,
            "mdate": 1699636326799,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hNezyCGdf6",
                "forum": "z9FXRHoQdc",
                "replyto": "G7OOQ9bDcz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3705/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3705/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their time and feedback.\n\n**Weaknesses**:\n\n**Fundamental Issue**: \n\nWe would like to remind the reviewer that we are not making the assumption that the solutions to our games are symmetric Nash Equilibria: the detective and agent do not share parameters with each other therefore the claim that we assume symmetric NE\u2019s is **false** and not incorporated into the design of the algorithm. We recognize that the main variation of the algorithm incorporates a self-play loss that, if this were to be the only loss, would make it impossible to learn non-symmetric NE\u2019s in 1-step Matrix form games. But we would like to remind the reviewer that our scope is iterated social dilemmas. \n\nThe reviewer states *\u201cThis is an XOR game where the NE should be two agents taking different actions, which cannot represented by policies with shared parameters.\u201d*\nWe present a **counter example** in the one-step history version of the XOR game. Consider the policy that randomly samples an action uniformly at random until the opponent chooses a different action, and subsequently sticks to their action thereafter. In a self-play setting (therefore a parameter sharing setting) this policy is a Nash equilibrium of the game, and corresponds to the same policy issuing different actions.\n\nRegarding the **problematic claims**: \n\nThe reviewer states *\u201cThis claim is wrong. Equation (7) is only equivalent to reward sharing with parameter sharing.\u201d* We would like to remind the reviewer that, by definition, self-play amounts to parameter sharing, therefore the original claim is **not wrong**. The reviewer also writes \u201cSelf-play with reward sharing'' should refer to the case where \"you learn two different policies with different parameters while the game reward is shared across agents\". This by definition cannot be self-play.\n\nThe reviewer writes *\u201ca game is both symmetric and zero-sum if and only if it is a zero game\u201d*, this claim is **wrong**. We are only aware of the definition of \u201czero-game\u201d in the context of combinatorial games (from wikipedia): \u201cthe zero game is the game where neither player has any legal options\u201d. A fundamental assumption of combinatorial games is a turn-taking nature. Therefore any non turn-taking symmetric zero sum game (e.g. rock paper scissors) is not a zero game.\n\n\nThe reviewer states \u201cthe concept of symmetric game only applies to two-player games.\u201d, this statement is **wrong**. N-player symmetric games exist and are clearly defined. Tragedy of the Commons is one of them. \n\nRegarding **minor issues**:\n\n1. The n-player formulation is standard in the literature and provides a general framework in which the two-player formulation is an instance.\n\n2. This is a correct observation and we currently suspect our statement is not valid. We will remove this statement from the paper.\n\n3. We are aware of some notational inconsistencies and we thank the reviewer for pointing them out, we will definitely correct these.\n\n4. We agree with the reviewer that this part of the notation should be made explicit and we will incorporate it when we introduce it in section 4.1.\n\nWe ask the reviewer to reconsider their score."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3705/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699819165030,
                "cdate": 1699819165030,
                "tmdate": 1699819165030,
                "mdate": 1699819165030,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eAHMi8Eshy",
                "forum": "z9FXRHoQdc",
                "replyto": "G7OOQ9bDcz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3705/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3705/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Discussion Period is Nearing its End"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their time and feedback. \nWe hope we have addressed the raised concerns. As the discussion period is drawing to a close, we kindly ask the reviewer if there are any further concerns or points."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3705/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700359518832,
                "cdate": 1700359518832,
                "tmdate": 1700359536602,
                "mdate": 1700359536602,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KeGv9MGxyI",
                "forum": "z9FXRHoQdc",
                "replyto": "G7OOQ9bDcz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3705/Reviewer_1vtA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3705/Reviewer_1vtA"
                ],
                "content": {
                    "comment": {
                        "value": "The authors assume the solution is symmetric with parameter sharing in the self-play process. This assumption raises two primary issues:\n\n1. Solving the game with parameter sharing does not necessarily lead to NEs since symmetric NEs may not exist. This has been shown in the XOR game example and the chicken game example. Instead of offering examples, it would be more appropriate for the authors to rigorously prove that the solution is indeed a NE when the game is solved with parameter sharing. However, this seems unattainable, as demonstrated in the XOR game and the chicken game examples\n\n2. Furthermore, I also want to point out that solving the game with parameter sharing and reward sharing does not necessarily lead to cooperative behavior. Consider a symmetric matrix game as shown below.\n\n```\n(0,0), (0,1)\n(1,0), (0,0)\n```\n\nSolving the game with reward sharing and parameter sharing by equation 7. yields a policy $\\pi=(0.5,0.5)$. For $\\pi=(0.5,0.5)$, the expected shared reward is $0.5\\times0.5+0.5\\times0.5=0.5$. However, the solution that maximizes the shared reward in this game is not symmetric, $\\pi_1=(1,0)$ and $\\pi_2=(0,1)$. The shared reward of this non-symmetric solution is $1$. \n\n\nRegarding the problematic claims in Section 4.3.2.\n\n1. *\"In zero-sum games, this update will have no effect as the gradient would be zero.\"* This is claim is correct but meaningless because in symmetric zero-sum two-player games, the expected return would always be zero if both players are using the same policy.\n\n2. *\"this is equivalent to training an agent with self-play with reward sharing\".*  Self-play is not equivalent to either parameter-sharing or a symmetric policy. The error lies in the assumption that both players use precisely the same policy in self-play; in self-play, players can employ different policies at the same state."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3705/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733917461,
                "cdate": 1700733917461,
                "tmdate": 1700733917461,
                "mdate": 1700733917461,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "hVx0b1EG40",
            "forum": "z9FXRHoQdc",
            "replyto": "z9FXRHoQdc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3705/Reviewer_FRpg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3705/Reviewer_FRpg"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel opponent-aware framework to improve cooperation. The framework involves training a detective to approximate the opponent, training the agent based on the interaction between the detective and agent, and conducting self-play to learn to cooperate. The experiments demonstrate that the proposed method can improve social welfare."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1) The proposed method that involves training a detective to help training the agent is interesting."
                },
                "weaknesses": {
                    "value": "1) My major concern is that the explanation why BRS outperforms POLA is not very clear. The authors mention that \"When the opponent is specifically trained to maximize its own return against a fixed policy trained by POLA, the first exploits the former.\" However, it is not clear whether this issue would be avoided when POLA is replaced by the proposed BRS. Additionally, the paper claims that \"POLA can't differentiate through all opponent optimization steps,\" but more experiments are needed to support this claim. Besides, the authors also state that \"the interactions between the agent and the detective mirror the foundational Stackelberg setup,\" but it is still not clear whether convergence to Stackelberg equilibrium helps to improve social welfare. More detailed analysis is needed to clarify these points.\n\n2) Recently, [1] proposed a method for reasoning about players' future strategy through multiple lookahead steps. This suggests that scalability issues for POLA (or other related works) may not be a serious problem. It would be helpful if the authors could discuss some details about [1].\n\n3) The training curves for baselines and proposed methods are missing. Also it would be helpful to test the proposed method in more complicated social dilemma environments, such as those presented in [2].\n\n4) Some typos need to be fixed, including Alg 1., $\\theta_1' \\leftarrow \\theta_1+z$ should be $\\theta_1 \\leftarrow \\theta_1+z$."
                },
                "questions": {
                    "value": "1) In Corollary D.3, the paper uses $r^1\\left(s_t, a_t, b_t\\right)+r^2\\left(s_t, b_t, a_t\\right)=0$ which implies $R^1(\\tau)=-R^2(\\tau)$. However, in proposition D.2, the authors use Lemma D.1 which assumes $r^1\\left(s_t, a_t, b_t\\right)=r^2\\left(o_t, b_t, a_t\\right)$ so as to imply  ${\\mathbb{E}}\\left[R^1(\\tau)\\right]={\\mathbb{E}}\\left[R^2(\\tau)\\right]$. Since Corollary D.3. also involves proposition D.2,This creates a potential inconsistency, and the proof of Corollary D.3 may not be correct. The authors are requested to provide clarification on this matter.\n\n\n\n\n\n\nReferences:\n\n[1] Recursive Reasoning in Minimax Games: A Level k Gradient Play Method\n\n[2] Multi-agent Reinforcement Learning in Sequential Social Dilemmas"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3705/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3705/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3705/Reviewer_FRpg"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3705/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698727290915,
            "cdate": 1698727290915,
            "tmdate": 1699636326710,
            "mdate": 1699636326710,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sSnv8ZheUF",
                "forum": "z9FXRHoQdc",
                "replyto": "hVx0b1EG40",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3705/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3705/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to thank the reviewer for their time and thoughtful feedback\n\n**Weaknesses:**\n\n1. What we intended to demonstrate is that BRS produces a final, fixed policy that is not exploitable by a learning policy trained against it. We believe that the performance against MCTS demonstrates that while POLA is exploited, BRS is not. Given that both the MCTS and the learning opponent approximate the best response (but the MCTS is stronger) then we did not feel the necessity of showing the performance against a learning opponent. We do have experimental results against a learning opponent for BRS and POLA and will incorporate them in the paper. We also share the reviewer\u2019s concern that a Stackelberg equilibrium might be undesirable for social welfare and suspect this is the fundamental reason for the need of the prosocial self-play loss. However, it would be helpful for us if the reviewer clarifies what kind of analysis would help solidify these points. \n\n2. Regarding [1], we believe that a similar algorithm that approximates the proximal point method would be prohibitively expensive in the context of reinforcement learning. [1] scales linearly in $k$ but the underlying function w.r.t. which gradients would be computed is an approximation of the value function. This implies that to compute a single parameter update $k$ different unrollings of different policies would be required in the environment. Moreover, by the recursive structure of the update equation, these unrollings would have to be computed sequentially. This is not even considering the high variance that the chained estimators of the value function would have. Also, notice that [1] is not too different from POLA in terms of complexity. \n\n3. We will include the training curves for these algorithms, but considering more complicated social dilemmas puts BRS to a higher standard than its predecessors, namely POLA. \n\n4. We thank the reviewer for pointing out these typos and we will fix them.\n\n**Questions:**\n\n1. We thank the reviewer for taking the time to read through the proof and will do our best effort to clarify it. These two statements refer to different properties of the game. In particular $r^1(s_t, a_t, b_t) = -r^2(s_t, b_t, a_t)$ refers to the zero sum property and $r^1(s_t, a_t, b_t) = r^2(o_t, b_t, a_t)$ refers to the symmetry property. The key observation is that $s_t \\neq o_t$, therefore $r^1(s_t, a_t, b_t) = r^2(o_t, b_t, a_t)$ does not imply $\\mathbb{E}\\left[R^1(\\tau)\\right] =\\mathbb{E}\\left[R^2(\\tau)\\right]$. The correct implication would look something of the form  $\\mathbb{E}\\left[R^1(\\tau)\\right] =\\mathbb{E}\\left[R^2(\\tau_o)\\right]$ where for each $\\tau$ there exists a $\\tau_o$ with the same probability (in the self-play case). As an example consider a state $s_t$ in the Coin Game where the blue player takes the blue coin and the red player does not. The $o_t$ state refers to an analogous state in which the red player takes the red coin and the blue player does not. By symmetry of the game it should be the case that $r^1(s_t, a_t, b_t) = r^2(o_t, b_t, a_t)$. And, if the game rewards are set appropriately, it can simultaneously hold that $r^1(s_t, a_t, b_t) = -r^2(s_t, b_t, a_t)$ (that would correspond to the red player being punished with a negative reward of what the blue player got for taking the blue coin in state $s_t$).\n\nReferences:\n\n[1] Recursive Reasoning in Minimax Games: A Level k Gradient Play Method"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3705/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699993624807,
                "cdate": 1699993624807,
                "tmdate": 1699993624807,
                "mdate": 1699993624807,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WF5hmdZCQ2",
                "forum": "z9FXRHoQdc",
                "replyto": "7IBcmXVOVj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3705/Reviewer_FRpg"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3705/Reviewer_FRpg"
                ],
                "content": {
                    "comment": {
                        "value": "I would like to thank the authors for their response and the effort they have put into enhancing the manuscript. Some of my concerns have been addressed. I am considering increasing my score from 5 to 6, but I also would like to see the feedbacks from other reviewers before making decision."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3705/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700730837621,
                "cdate": 1700730837621,
                "tmdate": 1700730837621,
                "mdate": 1700730837621,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4VelzND3wP",
            "forum": "z9FXRHoQdc",
            "replyto": "z9FXRHoQdc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3705/Reviewer_NynT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3705/Reviewer_NynT"
            ],
            "content": {
                "summary": {
                    "value": "The authors introduce a new scalable method to tackle general-sum games called Best Response Shaping (BRS). BRS works by differentiating through an opponent that is approximating a best response. They introduce a way for the detective to condition on the agent's policy through question answering. They show promising results in the IPD and Coin Game."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Originality:\n\nThe author's proposed method is original and builds and contributes on top of prior work. The \"detective\" mechanism is an interesting approach to approximating the best response that I have not seen in previous literature.\n\nQuality:\n\nThe author's evaluate in the challenging Coin Game environment and the IPD and run a large number of relevant ablatoins. \n\nClarity:\n\nThe authors describe the IPD, Coin Game, and related literature in depth.\n\nSignificance:\n\nAI systems that perform well in general-sum scenarios are becoming increasingly important as AI systems become more widely-deployed."
                },
                "weaknesses": {
                    "value": "Originality:\n\n- The training paradigm described here is closely related to [PSRO](https://arxiv.org/pdf/1711.00832.pdf), which is not mentioned in the paper.\n\nClarity:\n\n- The writing does not make it clear that the detective and agent take turns during optimization. The title of Algorithm 1 (that it says \"a single iteration\") is the *only* way I knew that this is how the training is done. Otherwise, it seems as though the detective is first trained on a pre-selected buffer B, which *significantly changes the reader's understanding of the method*.\n\n- Section 4.1 and 4.2 really make it seem like the detective is pre-trained when it is not.\n\n- BRS-NOSP is an important part of Figure 2, but is not defined until significantly later. \n\n- It's still not clear which player is $\\theta_1$ and which is $\\theta_2$. Presumably the agent would be $\\theta_1$ and the detective would be $\\theta_2$, but that is flipped in Equation 4 where $\\theta_r$ takes the place of $\\theta_1$ but actually represents the detective and the agent is now $\\theta_2$ (?)\n\n- The whole description of \"Simulation-Based Question Answering\" makes the setup far more confusing than it needs to be. Here is my understanding: The conditioning vector consists of the Q-Values of each action of a random opponent policy in that state. The Q-Values are estimated by sampling trajectories. It is not described in the paper like this, and is instead treated as a complicated \"Question-Answering\" scheme.\n\n- Describing and defining $\\delta_A$ before describing or motivating what $Q^\\text{simulation}$ was confusing as a reader, as it is uncertain how $\\delta_A$ is related to anything.\n\n- It's unclear why the authors have a paragraph about Diplomacy in the related work and a paragraph about self-play from human data. It's fine to include it, but the authors should explain, *in the paper*, why they include it.\n\n- The authors do not write out the bi-level optimization in Equation 12, making it seem like a single-level optimization. Writing it out would make this significantly more clear that it's a bi-level process.\n\nQuality:\n\n- The analysis is only done on three seeds (stated in the Appendix). This is  small, considering the high variance we observe from POLA. Using more seeds for POLA because the authors \"observed higher variance\" is problematic from a statistical point of view. (See: Stopping rules in statistics).\n\n- The authors state that \"in this paper, [they] advocate that a reasonable point of comparison is the agent\u2019s outcome when facing a best response opponent, which we approximate by Monte Carlo Tree Search (MCTS).\" I cannot find where in the paper the authors advocate for this. This is a significant claim, since arguably POLA and LOLA are not interested in this setting as much and seem to be far more interested in learning *online* against opponents that are also *learning online*. This is rather important, as it makes head-to-head comparisons with POLA a bit more questionable, seeing as it was designed for a different setting. \n\nSignificance:\n\n- BRS is *more exploitable than POLA* in Figure 3 against AD, seemingly contradicting the purpose of the paper. BRS-NOSP performs better, but at the cost of social welfare. However, I understand that you can't get the best of everything in general-sum games (See Q4 below).\n\n- The authors collected head-to-head results of POLA and BRS (in the Appendix Figure 7), but have elected not to show them in the main text. It seems as though POLA outperforms BRS in the head-to-head, further indicating that BRS is exploitable.\n\n- The authors write that the Good Shepherd is not scalable; however, their approach also seems similarly difficult to scale. (See Q3: below). What is the sample efficiency in terms of simulated environment steps?"
                },
                "questions": {
                    "value": "1. Why not just train directly against the MCTS opponent?\n\n2. BRS-NORB performs near-identically to BRS in Figure 4 and is *significantly simpler*. Why not just make the method BRS-NORB?\n\n3. Isn't calculating $\\delta_A$ at every single timestep extremely expensive and not very scalable? What is the sample efficiency of BRS in terms of the total number of simulator steps? \n\n4. Why did you run head-to-head results for the BRS variants and POLA in the Appendix, but not include them in the main text? I understand that evaluations in general-sum games can be odd, since the ultimate objective is not well-defined. I would be more convinced that BRS is valuable if there was a more explicit metric demonstrating reciprocity.\n\n5. What is the intuition for the choice of conditioning? Why is an estimate of Q-Values of a random policy in the current state a good metric?\n\n6. Misc: What is the reasoning for the separate optimizers? (Equation 8). Have you ablated this?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3705/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3705/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3705/Reviewer_NynT"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3705/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698806231411,
            "cdate": 1698806231411,
            "tmdate": 1700587270719,
            "mdate": 1700587270719,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7aE8T3Bdvn",
                "forum": "z9FXRHoQdc",
                "replyto": "4VelzND3wP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3705/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3705/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We deeply thank the reviewer for their thorough, detailed, and in-depth feedback. Here is our response. (Sorry, we needed to cut the response to two parts)\n\n**Weaknesses**:\n\n**Originality**:\n\nWe will indeed include PSRO in our literature review as it highlights the contribution of BRS over previous existing approaches. PSRO extends an existing set of policies by the best response policy to a meta-strategy of previous policies. PSRO does not differentiate through this best response generation mechanism. This is in contrast to the essence of BRS. Let\u2019s imagine PSRO on IPD. First, it starts with a random policy. Next, always defect as the best response is added. Next, the best response to the existing set is added which is again always defect. Therefore, PSRO cannot discover tit for tat(TFT) on IPD. However, BRS is different. BRS trains an agent against a best response opponent by being aware that the opponent will be the best response. That pushes BRS to learn TFT. \n\n**Clarity**:\n1. We thank the reviewer for their deep reading of the paper. We will rewrite the description of the algorithm to make it clear that the agent and the detective are being trained simultaneously.\n2. We will define BRS-NOSP when we describe BRS.\n 3. The QA description shows that the detective can use any conditioning mechanism that requires querying the agent in different states. The choice for the coin game was an instance of the QA. \n4. Diplomacy is mentioned as a large-scale game in which without forming local alliances it is impossible to win the game. Because naive RL agents don\u2019t develop the necessary reciprocation based cooperation the successful approaches train on human data. Therefore, one long term goal is to train agents that do well on Diplomacy without any human data to teach them the tit for tat like behavior. \n\n5. We don't have an Equation 12; perhaps you're referring to Equation 2? Indeed, we will make it clear that theta2_star represents the entire optimization process, encompassing differentiation throughout. We appreciate the reviewer's clarification on this.\n\n**Quality**:\n\nWe agree that more seeds for POLA is more desirable to hinder variance. We will include more POLA seeds for the camera ready version. \n\nPOLA is the only method that gives us a strong baseline for the coin game. We agree with the reviewer that while we care about generating a static policy, POLA cares more about learning online. However, the POLA authors themselves evaluate the final POLA against always defect and always cooperate opponents indicating interest in studying the properties of static final policy. Also, producing a final static policy that does well against learning opponents is valuable. \n\n**Significance**:\n\nWe argue if one wants to deploy an agent in an environment full of other learning opponents maximizing their own return (naive RL) - which we believe is most cases in the real world - one would rather deploy an BRS agent than a POLA agent. \n\nTo make our argument more concrete, we show BRS agents are closer to tit for tat in IPD. \n\n**First**, the best response to TFT on IPD is always cooperate. This is important as learning opponents find the best response to the deployed agent eventually. Our MCTS experiments show that while the best response to BRS is almost always cooperation, this is not the case for POLA. \n\n**Second**, TFT agents always cooperate with themselves. This is not the case for POLA while it is for BRS agents. \n**Third**, As TFT starts with cooperation at the first move it gets a lower return in head to head game with always defect. But, that is a desirable property of TFT for social settings, assuming cooperation and also keeping the door open for forgiveness. BRS agents are exploited slightly more by always defect than POLA agents. But, first, still compared to always cooperate they are exploited much less. Second, the always defect is not being a rational opponent against BRS here as it could have a much higher return if it had cooperated. Third, this tendency of BRS to break the defect loops while making it a bit more exploited against always defect gives it its ability to cooperate with itself. \n\nRegarding the head to head results, we don\u2019t think solving social dilemmas is about comparing head to head results between two algorithms and indicating the algorithm with the higher return as the winner. TFT gets exploited by always defect but still tit for tat is a much better strategy. We can\u2019t have complete non-exploitability and complete cooperation at the same time but we argue BRS makes a better trade off. \n\nRegarding scalability, BRS is more scalable than Good Shepherd which differentiates through a gigantic computational graph of the opponent learning from scratch to convergence. But, we agree with the reviewer, BRS needs to do the required simulations and this limits its scalability. More advanced question answering mechanisms can improve the sample efficiency."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3705/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700354950710,
                "cdate": 1700354950710,
                "tmdate": 1700354950710,
                "mdate": 1700354950710,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "C1SHZcprZ6",
                "forum": "z9FXRHoQdc",
                "replyto": "HwxFYxy6Db",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3705/Reviewer_NynT"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3705/Reviewer_NynT"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the detailed response"
                    },
                    "comment": {
                        "value": "I would like to thank the authors for their detailed response and updated manuscript. Their clarifying responses have helped me understand the method much better. It would be good if the authors could incorporate some of this discussion into the paper (e.g. the intuition in Q5 or their argument about why they \"would rather deploy an BRS agent than a POLA agent\")\n\nI believe the achieved results are impressive and the method is interesting enough to warrant an accept. I have updated my score accordingly.\n\nIf I were to update the score further, it would be good if the authors could have a more detailed analysis of the return in Coin Game. For example, could they have an explicit measure of reciprocity in Coin Game so that we know whether BRS is being exploited by AD / POLA or if it is just reciprocating? Could we get a table reporting the head-to-head results? I understand that an individual head-to-head result is not informative, but I think such a table would make it very clear which algorithm is better on \"average\" and which algorithm is the best in each situation. e.g. Tit-for-tat would indeed get slightly exploited by Always Defect, but it would generally be one of the best performing algorithms overall."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3705/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700587258761,
                "cdate": 1700587258761,
                "tmdate": 1700587258761,
                "mdate": 1700587258761,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "T5ylW1SgBy",
            "forum": "z9FXRHoQdc",
            "replyto": "z9FXRHoQdc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3705/Reviewer_Hmjc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3705/Reviewer_Hmjc"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a new opponent modelling tool which differentiates through a constructed opponent that approximates the\nagent's best response policy in a best-response algorithm-like manner. The paper include various empirical evaluations to validate the proposed mechanisms."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper introduces a neat method for overcoming some of the weaknesses in current opponent modelling approaches. The paper is both well-written and well structured with good empirical results to support the authors' claims."
                },
                "weaknesses": {
                    "value": "**Missing references**\n\nSome important references have been missed such as [1] and [2].\nFor example [1] introduces a method for incorporating diversity into best-response dynamics leading to a diverse fictitious play/diverse best response dynamics. Since fictitious play is based on a type of best-response method, a discussion on how the authors\u2019 paper compares to [1] is needed to be able to properly evaluate the authors\u2019 current contribution.\n\n**Limitations**\nIt seems that method requires the agent to have access to the detectives received rewards (and vice-versa) \u2013 this seems to limit the application to various non-cooperative multi-agent settings where at least the reward may be privately received.\n\nIn the abstract, the authors state the focus of the paper is in partially-competitive games, however the study of the cooperation regularization method does not shed any light on games that may be of mixed character (for example games in which $r^1(s,\\cdot)=-r^2(s,\\cdot)$ and $r^1(s',\\cdot)=r^2(s',\\cdot)+k$ for $s\\neq s'$). \n\nIn the conclusion, the authors state that one of the aims is improving the scalability and non-exploitability of agents. However, it is unclear about the practicalities of such a method in larger scale games as well as how it could ever be adapted beyond two-player games.\n\n \n\n[1] Perez-Nieves, Nicolas, et al. \"Modelling behavioural diversity for learning in open-ended games.\" International conference on machine learning. PMLR, 2021.\n[2] Yang, Yaodong, et al. \"Multi-agent determinantal q-learning.\" International Conference on Machine Learning. PMLR, 2020."
                },
                "questions": {
                    "value": "Q1: How does the method compare to other approaches that introduce diversity into opponent modelling such as [1] and [2]?\n\nQ2: Can the authors comment on whether or not (and how) this could be applied in games where the opponent\u2019s rewards are unobserved?\n\nQ3: Although the cooperation regularization method yields a useful tool that does not produce cooperative effects in zero-sum games, its effect in games in which some game states the players ought to behave adversarially and other game states cooperatively (for example the weakest link) is unclear to me. Can the authors shed any light on this?\n\n[1] Perez-Nieves, Nicolas, et al. \"Modelling behavioural diversity for learning in open-ended games.\" International conference on machine learning. PMLR, 2021.\n[2] Yang, Yaodong, et al. \"Multi-agent determinantal q-learning.\" International Conference on Machine Learning. PMLR, 2020."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3705/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698843206616,
            "cdate": 1698843206616,
            "tmdate": 1699636326571,
            "mdate": 1699636326571,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Cq18hKZCcx",
                "forum": "z9FXRHoQdc",
                "replyto": "T5ylW1SgBy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3705/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3705/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to thank the reviewer for their time and thoughtful feedback\n\n**Weaknesses:**\n\n**Missing References**\n\nWe appreciate the reviewer pointing these references out, we will include them in the literature review.  We would like to clarify that [1] makes two major assumptions that significantly simplify the problem. First, [1] is restricted to normal form games so the best response that is being computed corresponds to a mixed strategy. In contrast, BRS computes an approximation to the best response in policy space, which is significantly harder (and unclear how to do using [1]). Second, because [1] deals with matrix form games, it is relatively straightforward to define a sampling method for the policies, in their case a Determinantal Point Process, due to the linear nature of the underlying strategies. In contrast BRS adds diversity to fictitious play by keeping a replay buffer of encountered agents and adding noise to the policy parameters. So it remains unclear to us how a) [1] is able to scale beyond normal form games into games that require a neural network policy parameterization and b) that [1] provides a solution to social dilemmas. \n\n**Limitations**:\n\n Our method assumes that the rewards for all agents are visible but we would like to remind the reviewer that this kind of assumptions are normal in the literature: POLA makes this assumption and LOLA makes a stronger one (assuming access to policy parameters). The reviewer correctly points out that this limits the application of BRS to non-cooperative settings where the reward may be privately received. \n\nWhat we intended to say by writing \u201cpartially-competitive games\u201d was social dilemmas so we will probably change all the appearances of this term to clarify the scope of our paper. We are not providing a solution for zero sum or fully cooperative games.\n\nRegarding the conclusion, the reviewer correctly points out that BRS is not a method that works for n-player games as it is stated in the limitations of the paper. However, because BRS allows for the use of neural network policies it can scale to more complex games that use higher dimensional representations of the environment.\n\n**Questions**\n\n1. In BRS since the policies are non-linear, diversity is found using a replay buffer of policies and adding gaussian noise to the parameters of these policies. Which contrasts with the Determinantal Point Processes used in [1] and [2]. However, diversity seems to be not necessary for the Coin Game experiments as shown in the ablation section.\n\n2. BRS cannot be applied to games where the reward for the other player remains unobserved unless that reward can be inferred from the behavior of the other player. Nonetheless, similar assumptions are done in the literature and used only during training; at deployment time the assumption is not necessary.\n\n3. We do not know the answer to this question but we believe it is a very important one that should be explored in future work. Weakest link resembles Pure Diplomacy in the necessity for cooperation during the initial stages of the game and the need for competitive policies afterwards, and Pure Diplomacy has been one of the setups that we are interested in pursuing. We also believe BRS may work in a two player game with this structure, because the first gradient component (against the detective) induces adversarial behavior whereas the second one (self-play) induces cooperative behavior.\n\nUnder these considerations, would you be willing to increase your score?\n\nReferences:\n\n[1] Perez-Nieves, Nicolas, et al. \"Modelling behavioural diversity for learning in open-ended games.\" International conference on machine learning. PMLR, 2021.\n\n[2] Yang, Yaodong, et al. \"Multi-agent determinantal q-learning.\" International Conference on Machine Learning. PMLR, 2020."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3705/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699993364820,
                "cdate": 1699993364820,
                "tmdate": 1700358437927,
                "mdate": 1700358437927,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]