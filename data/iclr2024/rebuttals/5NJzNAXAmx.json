[
    {
        "title": "Informed POMDP: Leveraging Additional Information in Model-Based RL"
    },
    {
        "review": {
            "id": "6tn5k9kazn",
            "forum": "5NJzNAXAmx",
            "replyto": "5NJzNAXAmx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3550/Reviewer_EdBW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3550/Reviewer_EdBW"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a new formalisation for learning optimal policies in POMDPs where the agent is allowed to additional information during training. The additional information, denoted as i, is assumed to be a function of the underlying state s, which makes the observation o conditionally independent of s given i. \n\nThe paper then proves that a representation of the history is sufficient for optimal control if it is recurrent and can accurately predict the next reward and the next information i. This is different to the prior works, which looks for sufficient representation that can predict the next reward and the next observation o. \n\nAfter providing a learning objective for such sufficient representations, the paper presents a practical method that combines the learning objective and DreamerV3, a state-of-the-art MBRL method. \n\nThrough experiments, the paper investigates main two research questions. First, does the use of priviledged information during training improve the convergence of the agent? It also briefly studies the impact of different information on the speed of the agents' training."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is well written. \n\nSufficient relevant works have been discussed. \n\nExperimental evaluation is conducted on a diverse set of environments."
                },
                "weaknesses": {
                    "value": "## Novelty\n\nOne of the main contributions of this paper is the proposal of the informed POMDP formalization. \nThe key novelty I find form this formalization is that it enables a new objective for learning sufficient statistics in POMDPs, which relies on predicting the next reward and information instead of the next reward and observation as in prior works. \nBuilding on top of DreamerV3, this formalization leads to a practical MBRL algorithm that leverages additional information during training and does not need to reconstruct observations. \n\nWhile I acknowledge that this is a new and promising idea, I don't find it very novel. \nAs discussed in the paper, asymmetric learning for learning policies in POMDPs has already been well explored. \nThe paper leverages this idea and combines it with MBRL approaches for POMDPs, which are not new neither. \nThe theoretical result is not suprising. Intuitively, if a representation of the history is predictive for the reward and information, it should also be predictive for the reward and observation by the construction of information i. The later has already been proven sufficient for optimal control by Subramanian et al. (2022).\n\n## Experimental evaluation\nI also have concerns on the experimental evaluation. \n\n1. The authors hypothesize that leveraging this additional information will improve the convergence of the agents. However, I don't think this hypothesis is clearly supported by the results as I don't see significant improvement from informedDreamer. Moreover, for domains in which the authors claim that informedDreamer performs better at the end of training, I don't find the results very convincing due to the large standard errors. In Table 2, the large standard errors make the confidence intervals of informed and uniformed heavily overlap with each other. I would strongly suggest to run more random seeds to reduce the standard errors. \n\nI would also like to see more reasoning for the hypothesis that leveraging such additional information will improve convergence. I disagree with the reasoning that because the information i contains more information than the observation o, the new objective will be better than the classical objective. Rather, I would argue that learning to predict i, a more complex variable, instead of o, a simpler varibale as it's function of i, might actually make the objective harder to optimize. And it is not necessary. \n\n2. To understand the proposed method well, I think it is important to investigate the impact of different information on the training. The paper explores this question but only in one environment. I think more ablation study on this question would greatly increase the value of this research. For example, one can conduct similar controlled experiments in other domains. Or dive deeper by looking at the losses of different components of the learning objective."
                },
                "questions": {
                    "value": "Questions:\n- It seems that in Figure 4(b), there is no confidence interval. Does this mean that the standard error is 0?\n- Does the proposed method introduce new hyperparameters? How are they tuned? For example, are there any coefficients used to balance different losses in the learning objective?\n\nMinor:\n- I would suggest to add the uninformed baseline in Figure 4(b) as well for comparison. \n- Typo: Section 3.1 the discount factor \\gamma \\in [0,1[\n- There seems to be a double citation in the second paragraph of introduction: Gregor et al. 2019. \n- Figure 4 can be made larger."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3550/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3550/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3550/Reviewer_EdBW"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3550/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698593956925,
            "cdate": 1698593956925,
            "tmdate": 1699636309353,
            "mdate": 1699636309353,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gijCxA9VyP",
                "forum": "5NJzNAXAmx",
                "replyto": "6tn5k9kazn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3550/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3550/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer to Reviewer EdBW - Part 1/3"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nThank for your valuable feedback.\nWe are glad to read that you found the paper promising and appreciated the novel learning paradigm.\nBelow, we responds to your concerns with some clarifications, explanations or discussions that we added to the paper.\nNotably, we clarified the novelty of the paper, proposed to better discuss the hypothesis of the information improving convergence, and added additional analyses to better support this hypothesis, as requested.\nMoreover, we identified and fixed an ambiguous presentation of the result in the tables (conflicting with the figures) that may have lead you to wrong conclusions about the standard error of the mean.\n\nIf these clarifications happens to address some of your concerns, or that you agree that these additional results and discussions strengthen the paper, would you be willing to raise your rating of this first step in asymmetric model-based RL?\n\nThank you for your time and consideration.\n\nBest regards, \\\nThe authors.\n\n## Novelty\n\nWhile we agree that asymmetric learning is not novel, it was completely left unexplored in the model-based setting.\nTo the best of our knowledge, this work is indeed the first to exploit the additional information for learning a model of the environment, together with a concurrent work (Avalos et al., 2023).\nIn addition, this work is the first to propose a method within this more reasonable setting of non-Markovian additional information.\nMoreover, while the theory of sufficient statistic recently developed by Subramanian et al. (2022) found a lot of interest in the partially observable RL literature, this work is the first step in extending this theory to asymmetric learning, together with a concurrent work (Sinha & Mahajan, 2023).\nFinally, this work is the first to link (symmetric) recurrent world models with the theory developed by Subramanian et al. (2022), and generalizing this to _approximate_ sufficient statistic, or approximate information state in Subramanian et al. (2022), could pave the way for deriving convergence guarantees for model-based RL in POMDP.\n\nWe would also like to emphasise that the apparent simplicity of the theorem and algorithm generalizations are a direct consequence of the key conditional independence requirement, that can always be met in practice.\nIndeed, even when the eventual additional information $o^+$ does not make the observation $o$ and the state $s$ conditionally independent, it is possible to design such an information $i = (o, o^+)$ that would satisfy this independence.\nWe would also like to clarify the fact that the information is not necessarily a deterministic function of the state.\n\n## Limited performance gain\n\nWe agree that the performance gain of the informed Dreamer is not as important as we might have hoped.\nHowever, we disagree that the result are not conclusive for several reasons.\nFirst, in several environments, it can be seen that the Informed Dreamer reaches the final performance of the Uninformed Dreamer in about half the number of environment steps.\nWe also have conducted a new experiment, at the request of Reviewer SwZJ, in which we see that there exist environments for which the Informed Dreamer converges to a near-optimal policy while the Uninformed Dreamer completely fails to learn anything, see Appendix E.2.\nMoreover, in environments for which the Informed Dreamer converges at a higher return, the standard errors are generally lower than the order of magnitude of the difference in performance.\nOur ambiguous presentation of the tables might have mislead you in interpreting the standard deviation as the standard error of the mean in tables.\nThis was a mistake not to indicate clearly what these confidence intervals represent, and we fixed it now.\nIf you think that it is preferable to report the standard error of the mean in the tables, we obviously agree to modify this.\nThe smaller scale of the standard error of the mean can be visualised in Figures 4 to 6, in which the confidence interval is the standard error.\nWe apologise for this confusion.\n\nThat being said, motivated by your remark, we also propose to better discuss the current limitations of the proposed adaptation in Section 5, and to better explain the avenues for future works in the conclusion.\nNotably, we highlight the limitations of our ELBO learning objective, in which the variational encoder is conditioned on the observation only, limiting the expressiveness of the reconstructed distribution.\nFuture work may improve on this aspect by having a second encoder that is not used in the recurrence and thus not required at execution time.\nWhile this would come at the cost of reconstructing the observation distribution, it may also be possible to implement the latter without training the observation encoder to reconstruct the observation, but by KL-regularising its distribution to the information encoder distribution."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3550/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700465808354,
                "cdate": 1700465808354,
                "tmdate": 1700465808354,
                "mdate": 1700465808354,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1PbMw75QF9",
                "forum": "5NJzNAXAmx",
                "replyto": "6tn5k9kazn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3550/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3550/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer to Reviewer EdBW - Part 2/3"
                    },
                    "comment": {
                        "value": "## Theoretical justification for the improved convergence speed\n\nWe agree that asymmetric learning lacks theoretical motivations for exploiting the additional information, even in model-free RL.\nThis is discussed extensively in Section 5.1 by Baisero & Amato (2022) and more recently in Section III.C of Sinha & Mahajan (2023).\nDespite our method being rooted in the theory of sufficient statistic (or information state) proposed by Subramanian et al. (2022), proving a faster convergence will be challenging.\nIt would probably involve the notion of _approximate_ sufficient statistic, known as approximate information state in Subramanian et al. (2022), to bound the performance of the dynamic program given the error on the learned distributions.\nThis would require to express the error on the observation distribution that is implicitly encoded in the information distribution, through the exploitation of the motivating inequality $I(s', i' | h, a) \\geq I(s', o' | h, a)$.\nIt is thus quite an ambitious program that we are considering as a future work.\n\nWe also completely agree that the information could hurt learning, notably by containing irrelevant variables or exogenous variables.\nMoreover, the conditional information distribution may be more complex to approximate in practice than the observation distribution, while not being that useful to the control task.\nThese reasons are now discussed in details in Section 4.\nWe also want to point out that the presence of irrelevant or exogenous variables is a problem that may also concern the state / observation in symmetric RL, and that is well studied in the exogenous RL literature.\nWhile this body of work asks the question of what is _necessary_ in the state, the challenge in recurrent RL is still about finding a _sufficient_ representation of the history.\nReconciling the apparent tension between these considerations is obviously a fantastic avenue for future work, but is out of the context of this article in our opinion.\n\nWe also want to remind that we have shown for a simple environment in which partial observability is quite challenging (need of inferring its orientation from noisy position observations) that the speed of learning was clearly proportional to the quality of the additional information.\nThis is, in our honest opinion, already a convincing empirical demonstration of our hypothesis.\nWe have extended this study, as explained in the following.\n\n## Empirical study for justifying the improved convergence speed\n\nWe agree that we should have added more experiment in order to further demonstrate empirically our hypothesis.\nWe thus have added some results in that direction in Appendix E.\nFirst, in Appendix E.2., we show for a pathological example that the Informed Dreamer can learn near-optimal in environments in which the Uninformed Dreamer is completely unable to learn, at request of Reviewer SwZJ.\nConcerning the proposition of generalizing the study about the quality of the information to other domains, we consider it to be non feasible.\nIndeed, in the other domains, we only get access to the Markovian state, and it would be difficult to artificially construct an information $i$ that provides partial information about the state but that still makes the observation conditionally independent of the state given it.\nHowever, we propose to generalise this to the other Mountain Hike environments, which offer very similar conclusions, see Appendix E.1.\nThis constitutes in our opinion a rather convincing empirical evidence in favour our our hypothesis.\n\n## Questions\n\n###  No confidence interval in Figure 4(b). Does this mean that the standard error is 0?\nNo it doesn't, we decided not to display them for the sake of readability. We now stated it clearly in our article.\n\n### Does the proposed method introduce new hyperparameters?\nNo, we do not introduce any new hyperparameter (the log likelihood of the observation is just replaced by the log likelihood of the information in the losses), and we kept exactly the hyperparameters as in the original Dreamer release.\n\n## Minor\n\n### I would suggest to add the uninformed baseline in Figure 4(b) as well for comparison.\nExcellent idea, it is done.\n\n### Typo: Section 3.1 the discount factor \\gamma \\in [0,1[\nThank you, it is fixed.\n\n### Double citation in the second paragraph of introduction: Gregor et al. 2019.\nThank you, it is fixed.\n\n### Figure 4 can be made larger.\nIt has now become difficult because of the additional discussions that we added to the article. But since we have considered additional environments at your request for the analyses of the quality of information, you can now consult these larger figures in Appendix E."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3550/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700465947470,
                "cdate": 1700465947470,
                "tmdate": 1700465947470,
                "mdate": 1700465947470,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0rfXHlEyCt",
                "forum": "5NJzNAXAmx",
                "replyto": "6tn5k9kazn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3550/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3550/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer to Reviewer EdBW - Part 3/3"
                    },
                    "comment": {
                        "value": "## References\n\n- Avalos, R., Delgrange, F., Now\u00e9, A., P\u00e9rez, G. A., & Roijers, D. M. (2023). The Wasserstein Believer: Learning Belief Updates for Partially Observable Environments through Reliable Latent Space Models. In European Workshop on Reinforcement Learning.\n- Baisero, A., & Amato, C. (2022, January). Unbiased Asymmetric Reinforcement Learning under Partial Observability. In Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems.\n- Sinha, A., & Mahajan, A. Asymmetric Actor-Critic with Approximate Information State.\n- Subramanian, J., Sinha, A., Seraj, R., & Mahajan, A. (2022). Approximate information state for approximate planning and reinforcement learning in partially observed systems. The Journal of Machine Learning Research, 23(1), 483-565."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3550/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700465985784,
                "cdate": 1700465985784,
                "tmdate": 1700465985784,
                "mdate": 1700465985784,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vaS3LoGEcG",
                "forum": "5NJzNAXAmx",
                "replyto": "0rfXHlEyCt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3550/Reviewer_EdBW"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3550/Reviewer_EdBW"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. \n\nI appreciate the value of being the first to explore asymmetric learning within MBRL. However, this aspect does not sufficiently address my concerns on the overall novelty of the approach. Also taking into account the limited performance gain of the proposed method, I choose to keep my current score."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3550/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700677139244,
                "cdate": 1700677139244,
                "tmdate": 1700677139244,
                "mdate": 1700677139244,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "alLJXq7F8o",
            "forum": "5NJzNAXAmx",
            "replyto": "5NJzNAXAmx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3550/Reviewer_1Mpb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3550/Reviewer_1Mpb"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a model-based RL method for partially observable environments that exploits additional information during training.\n\nThe paper introduces a nice framework called the informed POMDP, which introduces an additional variable \"i\" (information variable) between the state and observation such that the observation is independent of the state given i.\nPredicting this variable i is supposed to be easier than o --- accelerating the representation learning --- but also sufficient for optimal control (based on the fact that its sufficient to predict the observation), hence they show theoretically sound.\n\nIn practice, they do this by adjusting dreamerv3 [1] to learn to decode the state rather than the observation.\nIn some domains this improves the learning rate, presumably because is it easier / quicker to learn to predict the (more informative or more compactly represented) state than the observation.\n\nAltogether I believe this is a fantastic step into a promising direction, that of exploiting additional information during training, which has been more common in model-free approaches (typically through auxiliary learning tasks, which has parallels with the proposed work).\nTheir dreamerv3 seems to work at least as good as the original one, fairly consistently beating it with somewhat, on domains including \"mountain hike\", \"velocity control\", and \"pop gym\".\n\n[1] Hafner, D., Pasukonis, J., Ba, J., & Lillicrap, T. (2023). Mastering diverse domains through world models. arXiv preprint arXiv:2301.04104."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper is clearly written and proposes a solution method that should be relevant to a significant portion of the RL community: those that care about partial observability or dreamer-like solution methods.\nThe proposed setting, that of exploiting additional information during training in partially observable environments, is reasonable and a promising direction that has not been explored for model-based RL much yet.\nLastly, I found the formalization of the informaed POMDP and the theoretical support helpful.\n\nSo, altogether, this is a good fit for ICLR based on those reasons."
                },
                "weaknesses": {
                    "value": "The main points of improvement, in my opinion, is in the actual implementation of the theoretical ideas in this paper.\nIn particular, the resulting algorithm is a minor change in which dreamerv3 is learned to decode the state, rather than the observation.\nIt is not hard to see that this, likely, will lead to an easier learning task, hence improving performance.\n\nFurthermore, the results are not nearly as impressive as they should be for a method that suddenly assumes access to the state during training.\nThis is an incredibly strong assumption in most real applications and thus heavily limits its applicability. \nYet, performance-wise, we see a minor improvement on most and only one some significant performance boost.\n\nLastly, while the theoretical set up was nice to see and thorough, I did not find the findings particularly surprising or promising:\nIt is rather obvious that if predicting the observation is \"good enough\" than predicting anything that can fully explain (predict) the observation also has that property.\n\nLastly, I found it particularly frustrating how hard it was to piece together exactly the difference between the proposed method and dreamer, since the notation is just slightly different enough that it takes a lot of puzzling to align the two."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3550/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3550/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3550/Reviewer_1Mpb"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3550/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698698271795,
            "cdate": 1698698271795,
            "tmdate": 1699636309281,
            "mdate": 1699636309281,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yXtpAY8Ewy",
                "forum": "5NJzNAXAmx",
                "replyto": "alLJXq7F8o",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3550/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3550/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer to Reviewer 1Mpb - Part 1/1"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nThank your for your valuable feedback.\nWe are pleased to see that you appreciated our new framework and this first step in asymmetric model-based RL.\nEven if you found the algorithmic contribution minor and the generalised theorem quite trivial, it is nice to read that your appreciated that the method was supported by the theory.\nBelow, we give an explanation that may address your first concern about applicability, and we provide a discussion for explaining both theoretically and practically the limited performance gain that we also propose to add to the article, along with some propositions of future works for improving this in the conclusion.\n\nIf you agree that our answers and added discussions addressed some of your concerns, would you be willing to improve your rating for this work, which is both a first step in the theory of sufficient statistic for asymmetric learning and in model-based asymmetric learning?\n\nThank you for your valuable feedback and for your consideration.\n\nBest regards, \\\nThe authors.\n\n## Assumption of conditional independence and applicability\n\nWhile we agree that assuming state observability at training time is strong in some environments, we would like to point out that we are able to deal with any kind of additional information, which is a much more reasonable assumption, and a novelty.\nIn other words, the problem that we propose to tackle is a strict generalization of the standard learning paradigm for POMDP.\nEven when the eventual additional information $o^+$ does not make the observation $o$ and the state $s$ conditionally independent, it is possible to design such an information $i = (o, o^+)$ that would satisfy this independence.\nWe would also like to emphasise that the apparent simplicity of the theorem and algorithm generalizations are a direct consequence of this key conditional independence requirement, that can always be met in practice.\nFinally, to the best of our knowledge, this work is the first to propose a method within this more reasonable setting of non-Markovian additional information.\n\n## Minor change with respect to Dreamer and limited performance gain\n\nWhile we agree that, when introducing this conditional independence requirement for the information, the theory motivates a very slight modification of existing algorithm, we really see this work as a first step towards developing asymmetric model-based RL methods.\nIn the light of your remark, we propose to better discuss the current limitations of the proposed adaptation in Section 5, and to better explain the avenues for future works in the conclusion.\nNotably, we highlight the limitations of our ELBO learning objective, in which the variational encoder is conditioned on the observation only, limiting the expressiveness of the reconstructed distribution.\nFuture work may improve on this aspect by having a second encoder that is not used in the recurrence and thus not required at execution time.\nWhile this would come at the cost of reconstructing the observation distribution, it may also be possible to implement the latter without training the observation encoder to reconstruct the observation, but by KL-regularising its distribution to the information encoder distribution.\n\n## Clarifications on the difference with the original Dreamer\n\nWe apologise for the important change in notation with respect to the initial Dreamer algorithm.\nWe tried to find a comprise between those used in the motivating theory of Subramanian et al. (2022) and those used in the practical algorithm of Hafner et al. (2023).\nWhile we know that reviewers are not required to read the appendices, we believe that Appendix C may help disambiguating notations.\nIndeed, in view of this change in notations, we have written a precise pseudocode for the Informed Dreamer in Appendix C, that specifically highlight differences with the original Dreamer algorithm.\n\n## References\n\n- Hafner, D., Pasukonis, J., Ba, J., & Lillicrap, T. (2023). Mastering diverse domains through world models. arXiv preprint arXiv:2301.04104.\n- Subramanian, J., Sinha, A., Seraj, R., & Mahajan, A. (2022). Approximate information state for approximate planning and reinforcement learning in partially observed systems. The Journal of Machine Learning Research, 23(1), 483-565."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3550/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700465616558,
                "cdate": 1700465616558,
                "tmdate": 1700465616558,
                "mdate": 1700465616558,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Wsp51U5xhX",
                "forum": "5NJzNAXAmx",
                "replyto": "yXtpAY8Ewy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3550/Reviewer_1Mpb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3550/Reviewer_1Mpb"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. I do not disagree with your observations but I do not believe it quite addresses the concerns well enough for me to change my mind regarding the score."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3550/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700491236811,
                "cdate": 1700491236811,
                "tmdate": 1700491236811,
                "mdate": 1700491236811,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "kh7EdymAyL",
            "forum": "5NJzNAXAmx",
            "replyto": "5NJzNAXAmx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3550/Reviewer_cAA6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3550/Reviewer_cAA6"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes informed POMDP, a formalization that utilizes additional information about the state (information beyond the agent\u2019s observations) that is only available during training time. It is assumed that this additional training information is designed such that observation is conditionally independent of the state given this information. Using this information, the authors propose a world model, obtained by leveraging the information for learning a recurrent sufficient statistic, to sample latent trajectories. The authors then adapt Dreamer model-based RL algorithm to use the informed world model and show improvement in convergence speed when compared to Dreamer on a variety of environments."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The informed POMDP is a natural and useful formalization that clearly articulates how additional training information can be incorporated in model-based RL. Such additional information is well motivated, especially when the agents are trained in simulation and have access to privileged information/full state information.\n- The theoretical results connecting predictive models and sufficient statistics for optimal control look technically sound and are in line with prior results in similar existing work.\n- The proposed approach is simple and intuitive, and can be easily adapted in many existing model-based RL approaches. The authors demonstrate this by adapting Dreamer with a modified objective and world model.\n- The empirical results demonstrate clear benefits on a variety of POMDP environments when compared to Dreamer. The informed model leads to substantially faster convergence in some environments."
                },
                "weaknesses": {
                    "value": "- The theoretical justification for why an informed model-based policy should converge faster, particularly in the case of informed Dreamer, isn\u2019t completely clear. Is this solely because the recurrent state-space model in the informed world model has access to complete state information, used as the additional information, in all examples?\n- While the experiments demonstrate that informed Dreamer converges faster than Dreamer in the environments tested, I don\u2019t think this is necessarily indicative of the question of how useful the additional information is in solving POMDPs - I believe all it shows is that having access to full state information during training outperforms Dreamer in convergence speed. There should be comparison with other SOTA methods that are focused on POMDP and can exploit handle the additional information (that the Dreamer baseline doesn\u2019t have access to in the experiments)."
                },
                "questions": {
                    "value": "- How sensitive are the improvements in convergence speeds to the choice of additional information? What happens when only a subset of the full state information (in addition to observations) is shared as the additional information? Do they degrade gracefully? (I acknowledge the comments on learning degradation in varying mountain hike example but this question still stands).\n- Could you provide any theoretical analysis characterizing what types of additional information are most useful? Perhaps in more restricted, simpler POMDPs?\n- I\u2019m curious how consistent/different were the reconstructed observations in the case of informed world model and the baseline dreamer world model in imagined rollouts."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3550/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698911457834,
            "cdate": 1698911457834,
            "tmdate": 1699636309194,
            "mdate": 1699636309194,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pLEzLNrEjY",
                "forum": "5NJzNAXAmx",
                "replyto": "kh7EdymAyL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3550/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3550/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer to Reviewer cAA6 - Part 1/2"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nWe are pleased to read that you found our generalised formalisation of asymmetric learning natural and well motivated.\nIt is also nice to read that your are enthusiastic about the established connection between asymmetric predictive models and sufficient statistics, and that your liked the simplicity of our method.\n\nWe agree with your concerns on the justification for why the convergence speed would improve, and propose some explanations to address these below.\nWhile the theoretical motivation is indeed not satisfactory yet, we elaborate on the difficulty of establishing it but propose several avenues for future work in that direction.\nFor now, we see this work as a first step in asymmetric model-based RL, as well as in the theory of sufficient statistic in asymmetric learning.\n\nIf our explanations and the discussions added to the manuscript happened to address some of your concerns, would you be willing to improve your rating for the article?\n\nThank you for your valuable review and for your consideration.\n\nBest regards, \\\nThe authors.\n\n## Theoretical justification for the improved convergence speed\n\nWe agree that asymmetric learning lacks theoretical motivations for exploiting the additional information, even in model-free RL.\nThis is discussed extensively in Section 5.1 by Baisero & Amato (2022) and more recently in Section III.C of Sinha & Mahajan (2023).\nDespite our method being rooted in the theory of sufficient statistic (or information state) proposed by Subramanian et al. (2022), proving a faster convergence will be challenging.\nIt would probably involve the notion of _approximate_ sufficient statistic, known as approximate information state in Subramanian et al. (2022), to bound the performance of the dynamic program given the error on the learned distributions.\nThis would require to express the error on the observation distribution that is implicitly encoded in the information distribution, through the exploitation of the motivating inequality $I(s', i' | h, a) \\geq I(s', o' | h, a)$.\nIt is thus quite an ambitious program that we are considering as a future work.\nWe now better discuss this compelling future work in our conclusion.\n\nWe also want to remind that we have shown for a simple environment in which partial observability is quite challenging (need of inferring its orientation from noisy position observations) that the speed of learning was clearly proportional to the quality of the additional information.\nThis is, in our honest opinion, already a convincing empirical demonstration of our hypothesis.\nMoreover, we extended this study in Appendix E.1 for the other Mountain Hike environments and it provides similar conclusions.\n\n## Characterization of the usefulness of the additional information\n\nYou have raised the interesting point of characterizing the usefulness of the additional information.\nIt is indeed easy to see that badly designed additional information could hurt training, because the latter may contain irrelevant or exogenous information.\nMoreover, the conditional information distribution may be more complex to approximate in practice than the observation distribution, while not being that useful to the control task.\nThese reasons are now discussed in details in Section 4.\nWe also want to point out that the presence of irrelevant or exogenous variables is a problem that also concerns the state / observation in symmetric RL, and that is well studied in the exogenous RL literature.\nWhile this body of work asks the question of what is _necessary_ in the state, the challenge in recurrent RL is still about finding a _sufficient_ representation of the history.\nReconciling the apparent tension between these considerations is obviously a fantastic avenue for future work, that we mentioned in our conclusion.\n\n## Comparison to related works in asymmetric learning\n\nWe agree that it would have been richer to gather these results of additional asymmetric learning methods in the article.\nHowever, these model-free RL algorithms would not benefit from the sample-efficiency of model-based RL methods such as Dreamer, and a fair comparison would be made difficult.\nMoreover, we emphasise that none of these methods were designed to deal with non Markovian additional information.\nFinally, we will not be capable of providing a fair comparison of these methods before the end of the discussion period.\n\n## Comparison of the observation reconstructions\n\nWhile we agree that our article may lack more qualitative visualization of the results, other than the learning curves, we are unfortunately unable to provide this comparison.\nIndeed, one of the advantage of our method is that it does not require to reconstruct the observation but only the (hopefully more compact) information.\nConsequently, we cannot compare the quality of the reconstructions, but we known that the distributions of the observation is encoded in the distribution of information."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3550/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700465480702,
                "cdate": 1700465480702,
                "tmdate": 1700465480702,
                "mdate": 1700465480702,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dt75o5J0OI",
                "forum": "5NJzNAXAmx",
                "replyto": "kh7EdymAyL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3550/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3550/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer to Reviewer cAA6 - Part 2/2"
                    },
                    "comment": {
                        "value": "## References\n\n- Baisero, A., & Amato, C. (2022, January). Unbiased Asymmetric Reinforcement Learning under Partial Observability. In Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems.\n- Sinha, A., & Mahajan, A. Asymmetric Actor-Critic with Approximate Information State.\n- Subramanian, J., Sinha, A., Seraj, R., & Mahajan, A. (2022). Approximate information state for approximate planning and reinforcement learning in partially observed systems. The Journal of Machine Learning Research, 23(1), 483-565."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3550/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700465512271,
                "cdate": 1700465512271,
                "tmdate": 1700465512271,
                "mdate": 1700465512271,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NeDanrOPGk",
            "forum": "5NJzNAXAmx",
            "replyto": "5NJzNAXAmx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3550/Reviewer_SwZJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3550/Reviewer_SwZJ"
            ],
            "content": {
                "summary": {
                    "value": "This paper considers the problem of learning in POMDPs with privileged information during training time. The motivation is that POMDPs are in general very hard to solve; however, often during training there can be substantially more information revealed to help learn the policy than what is available at test time. This work makes progress towards this goal by proposing the Informed Dreamer algorithm which attempts to model a sufficient statistic that is enough for optimal control, combined with the model-based Dreamer algorithm. Experiments across a variety of domains are presented."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The problem is very well motivated and I think this is relevant to people in the RL community.\n- The solution is also well motivated by the theory and technically interesting from that standpoint. The method also appears to be fairly flexible to the level of privileged information that is available.\n- The experiments are conducted on many different environments, which helps paint a fairly complete picture of the performance of the method.\n- The paper is clearly presented."
                },
                "weaknesses": {
                    "value": "- The gains are only marginally better than without privileged information. There are also no comparisons to alternative algorithms (like those mentioned in the related work), so it\u2019s hard to judge the merits beyond how it can potentially outperform the uniformed version.\n- There are a few examples of the informed method converging to a reward above the convergence of the uninformed method. There are also a few showing the opposite. Given this, I think this paper could really strengthen its position if it studied a practically interesting POMDP that would otherwise be completely intractable to solve alone (without information), but becomes solvable with training information. I believe this would constitute a very convincing result of the importance of privileged information empirically.\n- The main paper does not spend much time investigating the failures that arise or trying to explain why they do. Based on the motivating theory it is not clear to me why they would happen since there is strictly more information available in the training time and the procedure would otherwise be the same. Thus, I wonder: what are the causes of informed dreamer failing to keep up with uninformed dreamer? Could it just be hyperparameters or issues with optimization? I think it would have been nice to investigate this."
                },
                "questions": {
                    "value": "- Why does the reward decrease over time for some of the environments? E.g. Noisy position cart pole.\n- In (10) it may be helpful to say that I is the mutual information (I assume?) to distinguish it from \\tilde{I}.\n- In 3.1 there\u2019s a typo on $\\gamma \\in$...\n- Beyond settings where $i = s$, what are practically relevant scenarios where you would see $s \\rightarrow i  \\rightarrow o$ non-trivially? For the sake of exposition, do you also have non-examples where you might have $i$ a training time but $s$ is not conditionally independent of $o$?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3550/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699430057726,
            "cdate": 1699430057726,
            "tmdate": 1699636309127,
            "mdate": 1699636309127,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HUkpZldAyY",
                "forum": "5NJzNAXAmx",
                "replyto": "NeDanrOPGk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3550/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3550/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer to Reviewer SwZJ - Part 1/2"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nWe are pleased to read that you found the asymmetric learning setting well motivated, and that you appreciated the theoretical foundations as well as the flexibility of the proposed method.\nBelow, we propose additional experiments and discussions to address your concerns and provide answers to your questions.\n\nIf you happened to find that our additional results and explanations address your concerns and questions, would you be willing to improve your rating for this work, that in our opinion is an interesting first step both in the theory of sufficient statistic for asymmetric learning and in model-based asymmetric learning?\n\nThank you for your valuable feedback and for your consideration.\n\nBest regards, \\\nThe authors.\n\n## Comparison to related works in asymmetric learning\n\nWe agree that it would have been richer to gather these additional result in the article.\nHowever, these algorithms would not benefit from the sample-efficiency of model-based RL methods such as Dreamer, and a fair comparison would be made difficult.\nFurthermore, we will not be capable of providing a fair comparison of these methods before the end of the discussion period.\n\n## No clear overperformance after convergence in some environments\n\nWe first want to further emphasise the partial observability of the informed policy, which means that the optimal informed policy has the same performance as the optimal uninformed policy.\nThis can be clarified in the paper if you think it is worth emphasizing the focus on improving the speed of convergence.\n\nNevertheless, we agree that beyond improving the speed of convergence, such informed objectives could permit learning in challenging environment where standard algorithms would not learn.\nFirst, we want to highlight that we already observed this, by obtaining a better performance after convergence in nearly all environments of the Velocity Control benchmark.\nIn order to fully address your remark, we considered harder version of one of the most basic Pop Gym environments: _Repeat First_.\nIn this benchmark, the agent is observing noise, and is rewarded for outputting the observation that it got $k$ time steps ago.\nWhile we had only considered the default _Easy_ version ($k = 4$) of the environment so far, we now considered the _Medium_ ($k = 32$) and _Hard_ ($k = 64$) versions of this environment.\nAs can be seen from the results in Appendix E.2, the Informed Dreamer clearly learns near-optimal policies while the uninformed Dreamer does not learn at all."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3550/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700465232496,
                "cdate": 1700465232496,
                "tmdate": 1700465232496,
                "mdate": 1700465232496,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "27TZSIM60T",
                "forum": "5NJzNAXAmx",
                "replyto": "NeDanrOPGk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3550/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3550/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer to Reviewer SwZJ - Part 2/2"
                    },
                    "comment": {
                        "value": "## Discussion of failures with respect to the theory and in practice\n\nWe addressed your last remark by better discussing the failures that arise, both from the theory perspective in Section 4 of the article and from the practical implementation perspective in Section 5.\n\nFirst, we want to point out that proving a faster convergence when learning the state/information distribution instead of the observation distribution is challenging.\nSuch considerations would probably involve the notion of _approximate_ sufficient statistic, known as approximate information state in Subramanian et al. (2022), to bound the performance of the dynamic program given the error on the learned distributions.\nThis would require to express the error on the observation distribution that is implicitly encoded in the information distribution, through the exploitation of the motivating inequality $I(s', i' | h, a) \\geq I(s', o' | h, a)$.\nIt is thus a very ambitious task that we are considering as a future work.\nWe now better discuss this compelling future work in our conclusion.\n\nIt is more easy to explain why considering (badly designed) additional information could hurt training.\nIndeed, it is straightforward to see that the information can contain irrelevant or exogenous variables.\nMoreover, the conditional information distribution may be more complex to approximate in practice than the observation distribution, while not being that useful to the control task.\nThese reasons are now discussed in details in Section 4.\nWe also want to point out that the presence of irrelevant or exogenous variables is a problem that also concerns the state / observation in symmetric RL, and that is well studied in the exogenous RL literature.\nWhile this body of work asks the question of what is _necessary_ in the state, the challenge in recurrent RL is still about finding a _sufficient_ representation of the history.\nReconciling the apparent tension between these considerations is obviously a fantastic avenue for future work, that we mentioned in our conclusion.\n\nWe also better discuss in Section 5 the limitations of our ELBO learning objective, in which the variational encoder is conditioned on the observation only, which is another plausible explanation for the limited performance gain.\nFurther work may improve on this aspect by having a second encoder that is not used in the recurrence and thus not required at execution time.\nWe have made that clear in our conclusion.\n\n## Questions\n\n### Why does the reward decrease over time?\nThere is not guarantee of monotonic improvement with such policy gradient algorithms, especially when the world model that provides the latent representations to the policy is trained jointly with the policy.\n\n### In (10), it may be helpful to say that $I$ is the mutual information.\nThank you, it is fixed.\n\n### In 3.1, there's a typo in the range of $gamma$.\nThank you, it is fixed.\n\n### Beyond settings where $i=s$, what are practically relevant scenarios where $s \\rightarrow i \\rightarrow o$.\nAs example for which $s \\neq i$, we can consider a grasping robot for which the information are some scene variables that can be provided during training (e.g., objects types and positions) and allow to give the observation distribution but are not Markovian (e.g., objects velocities are not included).\n\n### Do you have examples where $s$ is not conditionally independent of $o$ given the information?\nYes, any situation in which a candidate information $\\bar{i}$ (or $o^+$) is drawn independently of $o$ given $s$.\nFor example when $\\bar{i}$ is an additional realisation of the observation $o$, because you have the opportunity to query your sensor $O(o | s)$ several times at training time.\nIn this case, the conditional independence between $o$ and $s$ given $o^+$ does not hold.\nHowever, as explained in Section 3 (with notation $o^+$), whatever the candidate information $\\bar{i}$, the information $i = (\\bar{i}, o)$ can be used and satisfies the conditional independence.\n\n## References\n\n- Subramanian, J., Sinha, A., Seraj, R., & Mahajan, A. (2022). Approximate information state for approximate planning and reinforcement learning in partially observed systems. The Journal of Machine Learning Research, 23(1), 483-565."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3550/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700465329323,
                "cdate": 1700465329323,
                "tmdate": 1700465329323,
                "mdate": 1700465329323,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "acDvI2aeYa",
                "forum": "5NJzNAXAmx",
                "replyto": "27TZSIM60T",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3550/Reviewer_SwZJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3550/Reviewer_SwZJ"
                ],
                "content": {
                    "title": {
                        "value": "Thanks"
                    },
                    "comment": {
                        "value": "Thank you. I appreciate the responses to the questions and will now discuss with the other reviewers."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3550/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700687246349,
                "cdate": 1700687246349,
                "tmdate": 1700687246349,
                "mdate": 1700687246349,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]