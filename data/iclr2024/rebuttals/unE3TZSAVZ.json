[
    {
        "title": "Breaking Neural Network Scaling Laws with Modularity"
    },
    {
        "review": {
            "id": "cgbKYmD4zQ",
            "forum": "unE3TZSAVZ",
            "replyto": "unE3TZSAVZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6027/Reviewer_VLQY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6027/Reviewer_VLQY"
            ],
            "content": {
                "summary": {
                    "value": "The paper seeks to understand scaling laws for modular neural networks and proposes a method for training them. Modular neural network here refers to models that sum the output of their constituent modules each of which receive (different) low-dimensional projections of the input. The paper theoretically shows that when the modules are linear neural networks that receive a linear projection of the input into a fixed dimensional space, and the data comes from a model of the same form, sample complexity is independent of the task intrinsic dimension $m$ (in contrast to the monolithic case where it is exponential in $m$). The paper then proposes a kernel-based rule to learn the initializations of the input projections from data and test the empirical performance on a sine wave regression task and compositional CIFAR."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Understanding the sample complexity of training modular vs. monolithic neural networks is an important open problem for which a theoretical contribution is potentially impactful.\nThe theory identifies a reasonable setting for a tractable analysis and is overall convincing (without having checked the proofs in the appendix).\nOverall the paper is well presented and transparent about the merits and limitations of the analysis."
                },
                "weaknesses": {
                    "value": "The scaling behaviour is studied theoretically in the case of linear neural networks for tractability. A more thorough empirical investigation to what extent this scaling law is practically relevant in the nonlinear setting would have been useful. As far as I understand the experiments conducted do not reflect the theoretical result of constant sample complexity in the input dimension. I was missing a discussion on this point.\n\nI am a bit worried about the reproducibility of the empirical part of the paper since no code was provided as part of the submission. I also encourage the authors to specify the exact number of seeds per experiment in Figure 3b as \"up to five seeds\" as stated in the caption could technically mean only one seed is reported."
                },
                "questions": {
                    "value": "1. The modular learning rule minimizing the norm of the $\\theta_i$ is applied as a pretraining step assuming that the $\\varphi(X;\\hat{U}_i$ are sufficiently expressive. Since this is before training, can you elaborate why this assumption might be justified and to what extent the algorithm is robust to a violation of it? \n2. There are discrepancies between the theory and toy model in Figure 5 as the paper points out in App A.2. Can you elaborate why this is not a matter of concern for the theory, i.e. what exactly causes the mismatch?\n3. Figure 5 is missing labels and the caption is a bit sparse. Could you specify how exactly the four plots differ? Maybe adding a colour bar to indicate the values of the light lines could be helpful? How do the theoretical predictions look like for individual (light) lines?\n4. Figure 12 is missing a legend for what the colours encode. Could you please clarify?\n\nSuggestions / typos:\n- I think it would be useful to show both the theoretical prediction and empirical validation in Figure 2 (similar to Figure 5 in Appendix A).\n- Page 7 \"and the test loss and dependence of the test loss\""
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6027/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6027/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6027/Reviewer_VLQY"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6027/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698684142962,
            "cdate": 1698684142962,
            "tmdate": 1699636647779,
            "mdate": 1699636647779,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7Gsu2Q4nig",
                "forum": "unE3TZSAVZ",
                "replyto": "cgbKYmD4zQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6027/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6027/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response Part 1/2"
                    },
                    "comment": {
                        "value": "Thank you for your valuable and constructive comments and suggestions. We are glad to hear of your positive impression of our work.\n\n**Practical relevance of scaling law in nonlinear setting**\n\nWe first would like to clarify that the empirical validation of Section 3.3 is performed on nonlinear neural networks; thus, the generalization error of linear models can predict generalization in *nonlinear* networks.\n\nWe also note that although we primarily consider nonlinear model architectures containing linear module projections (as in Eqn 3), in Section 4.3 Table 2, we also conduct experiments on nonlinear module projections. As we discuss in Section 5, we believe that further studying nonlinear module projections is an important direction for future work, particularly in practical settings where nonlinear module projections may be more realistic.\n\n**Discrepancy with theory for sample complexity trend**\n\nIndeed, as the reviewer notes, while the theory predicts that in modular architectures, sample complexity scales as a constant with input dimensionality, our empirical results indicate that sample complexity grows with input dimensionality. We believe the discrepancy with the theory arises due to the optimization challenges associated with modular architectures (as previously found in Csordas et al. and Mittal et al.). Specifically, with higher dimensional inputs, the gap between the performance of the optimal modular model and of the modular model learned by gradient descent grows. Our modular learning rule introduced in Section 4.2 is aimed at easing these optimization difficulties and is effective as illustrated in Figure 3. However, it still does not find the optimal modular architecture parameters, thus maintaining an input dimensionality dependence for sample complexity. We regard further improving the optimization of modular architectures as an important future direction.\n\nWe have added a discussion of this point in our revised Section 5.\n\n**Code**\n\nThank you for the suggestion. In our latest revision, we have included code to reproduce the empirical validation of Section 3.3 and the empirical results of Section 4.3.\n\n**Number of seeds**\n\nDue to experimental limitations at the time of submission, we were unable to run 5 seeds for each of the experimental settings for Compositional CIFAR-10. We have now updated our results with all 5 random seeds for each setting. Note that the error bars in Figure 3 indicate standard errors individually for each point.\n\n**Sufficient expressivity assumption**\n\nThank you for highlighting this important point: indeed, we assume the features of Equation 18 are sufficiently expressive in the sense that $pK > dn$. Effectively, this assumption states that for any choice of module projections $\\hat U_i$, some set of module parameters can perfectly fit the training data. This assumption is natural when modules are neural networks since they are typically capable of fitting a range of training datasets. When this assumption does not hold, the pseudoinverse solution of Equation 19 finds a solution minimizing the mean squared error between the predicted and actual training targets $y(X)$. In this case, the training set prediction error is generally not $0$ (except when $\\hat U_i$ are at their correct values). Importantly, minimizing the norm of $\\theta$ with respect to $\\hat U_i$ *may not necessarily* yield a lower prediction error. Thus, we do not necessarily expect our algorithm to be effective when modules are not sufficiently expressive.\n\nWe have added a discussion of this point in our revised Section 4.2."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6027/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700344459656,
                "cdate": 1700344459656,
                "tmdate": 1700739699896,
                "mdate": 1700739699896,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UaCgnxOISM",
                "forum": "unE3TZSAVZ",
                "replyto": "cgbKYmD4zQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6027/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6027/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response Part 2/2"
                    },
                    "comment": {
                        "value": "**Discrepancy in Figure 5**\n\nWe note two key discrepancies between our theory and empirical results: first, the loss is empirically larger than predicted for small amounts of training data, and second, the error spike at the interpolation threshold is smaller than predicted by the theory.\n\nWe believe the first discrepancy is due to imperfect optimization of neural networks, especially in low data regimes. Note that the linearized analysis assumes that the linear model solution finds the exact global optimum. However, the actual optimization landscape for modular architectures is highly non-convex, and the global optimum may not be found especially for small datasets (indeed, we find a significant discrepancy between predicted and actual training loss values for small data size n; in the overparameterized regime, the predicted training error is exactly $0$). We believe this causes the discrepancy between predicted and actual test error in low data regimes. \n\nWe hypothesize that the second discrepancy is also partly due to imperfect optimization. This is because the interpolation threshold spike can be viewed as highly adverse fitting to spurious training set patterns. This imperfect optimization is more pronounced at smaller $m$. Despite these discrepancies, we nevertheless find that our theory precisely captures the key trends of empirical test error.\n\nWe have added a more thorough discussion of this point in our revision.\n\n**Comments on Figure 5**\n\nThank you for the suggestion. In response, we have edited the caption of Figure 5 and modified the Figure to indicate the meaning of the light vs dark lines as well as indicate the difference between the light lines. We do not include the theoretically predicted curves for the light lines for visual ease of understanding; however, for larger $m$, the predicted test loss is higher while the location of the interpolation threshold is retained, and for larger $p$ the predicted test loss shifts vertically up and down.\n\nThe four plots indicate trends of training and test loss along four different dimensions: $k$ (number of modules), $m$ (input dimensionality), $p$ (model size) and $n$ (training set size).\n\n\n**Figure 12 clarification**\n\nNote that the Compositional CIFAR-10 inputs have shape 32x32kx3. In our modular architecture, the module input dimensionality is 512 (see Appendix E.3). Therefore, each learned input projection $\\hat{U}$ each has shape 32x32kx3x512. In Figure 12, we plot a representative slice of $\\hat{U}$ with shape 32x32kx3; note that since it has the same shape as the original image, we may plot it as an image with channel dimensions encoding RGB values.\n\nWe would be happy to clarify this in our revision if necessary.\n\n**Empirical result on Figure 2**\n\nThank you for this suggestion. We are unable to use our current empirical results to generate an empirical sample complexity curve since our current experiments in this setting compute the test set error at a given pair of training set size n and input dimensionality m. However, we are currently running binary search over training set size n to compute the minimum n required to achieve the desired test set error of 1.2. We will update the figure with empirical results once available.\n\n**Page 7 typo**\n\nThank you for pointing this out; we have corrected this in our revision."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6027/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700344529660,
                "cdate": 1700344529660,
                "tmdate": 1700370805573,
                "mdate": 1700370805573,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kW2M5XGyzD",
                "forum": "unE3TZSAVZ",
                "replyto": "UaCgnxOISM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6027/Reviewer_VLQY"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6027/Reviewer_VLQY"
                ],
                "content": {
                    "title": {
                        "value": "Response to rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for your thorough response to my review that has helped clarify my questions. I appreciate that you now provide the code to reproduce the empirical verification. Conditioned on the promised addition of the currently missing seeds and given no corresponding surprises as a result (I find it worrying that some unspecified part of the result relies on a single seed), I maintain my score for acceptance."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6027/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700581501632,
                "cdate": 1700581501632,
                "tmdate": 1700581501632,
                "mdate": 1700581501632,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "i1UhWlFIrT",
            "forum": "unE3TZSAVZ",
            "replyto": "unE3TZSAVZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6027/Reviewer_MGmV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6027/Reviewer_MGmV"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a theoretical model of NN learning, specifically predicts that while the sample complexity of non-modular NNs varies exponentially with task dimension, sample complexity of modular NNs is independent of task dimension. The authors then develop a learning rule to align NN modules to modules underlying high-dimensional modular tasks, and presents empirical results which demonstrate improved performance of modular learning."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper presents the first theoretical model to explicitly compute non-asymptotic expressions for generalization error in modular architectures, develops new modular learning rules based on the theory and empirically demonstrated the improved performance of the new method."
                },
                "weaknesses": {
                    "value": "Validation of theoretical results is only shown in the appendix, with large discrepancy between theoretical predictions and numerics, I think more empirical evaluations are needed to verify the theoretical result."
                },
                "questions": {
                    "value": "1. What causes the large deviation of the test loss between actual and predicted in Figure 5?\n2. In figure 4 (also figure 3b), the total range of the similarity score is quite small, it is therefore difficult to say whether the result is a significant improvement from baseline."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6027/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6027/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6027/Reviewer_MGmV"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6027/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698721898802,
            "cdate": 1698721898802,
            "tmdate": 1699636647663,
            "mdate": 1699636647663,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QkZoiFIdpI",
                "forum": "unE3TZSAVZ",
                "replyto": "i1UhWlFIrT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6027/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6027/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your valuable comments.\n\n**Concerns on the discrepancy between actual and predicted test loss**\n\nFirst, we'd like to note that the prediction of NN test loss in Figure 5 is relatively strong considering that we are using a *linear* model to approximate the generalization trends of a highly *nonlinear* neural network. Importantly, we are able to predict the location of the interpolation threshold, and we believe the results are overall competitive with prior work modeling NN generalization.\n\nWe note two key discrepancies between our theory and empirical results: first, the error spike at the interpolation threshold is smaller than predicted by the theory, and second, the loss is empirically larger than predicted for small amounts of training data. We believe the discrepancies are due to imperfect optimization, especially in regimes of small training data and small model size. Note that the linearized analysis assumes that the linear model solution finds the exact global optimum. However, in the actual optimization landscape for modular architectures is highly non-convex, and the global optimum may not be found especially for small models and datasets (indeed, in Figure 5, note the discrepancy between predicted and actual training loss values for small model size p and small data size n). We believe this causes the large discrepancy between predicted and actual test error in low data regimes. Moreover, we believe this imperfect optimization leads to a smaller-than-expected test error at the interpolation threshold given that the interpolation threshold spike can be viewed as highly adverse fitting to spurious training set patterns.\n\nWe have added a more thorough discussion of this point in our revised Section 3.3.\n\n**Significance of improvement over baseline**\n\nRegarding Figure 3b, note that results are averaged over 5 seeds and the standard error margins plotted indicate that the improvement over the baseline is statistically significant. Note that for the highest number of images tested, the improvement in accuracy is roughly 5% which we believe is a significant improvement.\n\nRegarding Figure 4, we believe that while the difference in raw similarity score between our method and the baseline appears small, this corresponds to a large difference in the qualitative behavior of the two networks. To illustrate this, in Figure 12 we have plotted an illustration of a learned module direction under our method relative to the baseline. We find that the module direction learned by our method is visibly sensitive to only one component image while the baseline is not; thus, our method learns the underlying module structure of the task."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6027/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700344380661,
                "cdate": 1700344380661,
                "tmdate": 1700344380661,
                "mdate": 1700344380661,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bnmINjC8VE",
            "forum": "unE3TZSAVZ",
            "replyto": "unE3TZSAVZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6027/Reviewer_Jw6v"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6027/Reviewer_Jw6v"
            ],
            "content": {
                "summary": {
                    "value": "This paper analyzes the sample complexity of modular neural networks and shows theoretically how the sample complexity of modular networks doesn't depend on the intrinsic dimensionality of the input. This is proven for linear models. The theory is supported by experiments on 1) sin wave regression and 2) compositional CIFAR10. The paper further proposes a learning rule to ensure the modularity of the task is aligned with the modularity of the network."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This is the first paper to conduct a rigorous theoretical analysis of modular neural networks. Understanding the empirical success of modular neural networks is an important open problem. \n\n2. The theoretic analysis and the effect of different terms in the generalization bound are presented clearly. \n\n3. Assumptions for the theoretical analysis are presented clearly. \n\n4. Related work is covered well and in thorough detail."
                },
                "weaknesses": {
                    "value": "1. Including synthetic experiments in the linear model to demonstrate how the sample complexity changes for modular and non-modular networks in a specific setting."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6027/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698816218313,
            "cdate": 1698816218313,
            "tmdate": 1699636647539,
            "mdate": 1699636647539,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2gjZNu9VnR",
                "forum": "unE3TZSAVZ",
                "replyto": "bnmINjC8VE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6027/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6027/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your valuable comments. We are glad to hear of your positive impression of our work.\n\n**Comments on synthetic experiments on linear model**\n\nAs the reviewer notes, our theoretical analysis hinges on modeling *nonlinear* neural networks as linear models. However, our experiments are conducted on nonlinear networks and tasks. This is because our theoretical analysis is aimed at capturing generalization trends in actual, nonlinear neural networks, and can do so accurately as demonstrated in Appendix A.2.\n\nMoreover, our construction of modular tasks and models (in Equations 3 and 6) is *fundamentally nonlinear*: the input is linearly projected, fed through nonlinear functions, and then summed. The linearization analysis of Section 4.1 separately linearizes the network with respect to the projections and the parameters of the nonlinear functions; this separation of parameters ultimately yields a superior sample complexity scaling for modular networks relative to monolithic networks. However, for linear models, there is no corresponding notion of notion of modularity: fundamentally, there is no division of parameters into \"projection\" parameters and \"module\" parameters. We believe our linearization analysis is relevant only for nonlinear models.\n\nThus, while we understand the spirit of the reviewer's suggestion, we are unclear on what exact experiment would be most valuable to run. However, we would be happy to include any specific experiments suggested by the reviewer."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6027/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700344337275,
                "cdate": 1700344337275,
                "tmdate": 1700344337275,
                "mdate": 1700344337275,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]