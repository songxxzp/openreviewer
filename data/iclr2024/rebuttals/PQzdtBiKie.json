[
    {
        "title": "Fractal Patterns May Unravel the Intelligence in Next-Token Prediction"
    },
    {
        "review": {
            "id": "KsEIh8F5Ur",
            "forum": "PQzdtBiKie",
            "replyto": "PQzdtBiKie",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4084/Reviewer_jKTM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4084/Reviewer_jKTM"
            ],
            "content": {
                "summary": {
                    "value": "This paper purports to apply fractal theories to language. They claim that language is \u2018self-similar\u2019 at all levels of granularity and \u2018long-range dependent\u2019. Through a few experiments with the PaLM language model and a few disparate datasets, a few hints of self-similarity, narrowly defined, are given. A connection to scaling laws is also briefly discussed."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- It is good that four datasets were considered. It is not clear why the 4K word token cutoff is important."
                },
                "weaknesses": {
                    "value": "- With regards to the supposed value of Figure 1 \u2014 a single (tripartite) anecdote showing self-similarity would not nearly be enough to show a trend, but this graph doesn\u2019t even show unambiguous similarity \u2014 the mere presence of \u2018burstiness\u2019 is not sufficient evidence. Moreover, the H\u01d2lder exponent of 0.638 is claimed as sufficient to confirm self-similarity but no context is provided to contextualize this single statistic.\n- The amalgamation of fractals and intelligence seems somewhat unmotivated from either an empirical or a deductive perspective.\n- It seems that the authors are claiming, by means of their experimental setup, that PaLM-8B represents all of \u2018language\u2019. Later, PaLM-540B was used (in Sec 2.5) to measure \u2018more accurate\u2019 probability distributions, but wholly other architectures should have been considered and compared, too. Results should be broken down by various language families.\n- The relationship between the predictability of the form that would emerge from fractal structures and those that have been approached by Zipf, Shannon, and their respective camps should be explored. I.e., despite the brief mentions of Zipfian distributions and entropy, they are not contrasted at all with the theory behind the current work; it is not clear if the authors mean to present their line of work as an alternative to well-established methods, but any deeper comparison is advised.\n- Attempting to draw links between language and, say, phenomena of large bodies of water is very tenuous when the only connection is that they each share a Hurst parameter of 0.75. The authors are recommended to establish more concrete outcomes in their empirical results and conclusions, relevant to either understanding the learning process, improving outcomes, or interpreting outcomes of LLMs."
                },
                "questions": {
                    "value": "- \u2018Creative content creation\u2019?\n- Is it the case that LLMs <<prompted some researchers to ponder if there was more to intelligence than\n\u201con-the-fly improvisation>>?\n- you seem to have defined the $t-1^{st}$ word in terms of itself (and its prefix) in the experimental setup. You have also seemed to apply the Markov property with length $L=1$ in your definition of $z_t$. Are these true?\n- By means of the $\\tau$-increments, are you suggesting there are fixed, expected lengths for clauses, sentences, paragraphs (and then presumably\u2026sections? Chapters? Articles/novels/scripts/\u2026?).\n- Could you please check your references for completeness (e.g., Aref, 1998; Bubeck et al, 2023)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4084/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698675043279,
            "cdate": 1698675043279,
            "tmdate": 1699636372873,
            "mdate": 1699636372873,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NPi6L2kWt4",
                "forum": "PQzdtBiKie",
                "replyto": "KsEIh8F5Ur",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4084/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4084/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thank you for your comments and the time you have taken to review our manuscript. Please see our response below.  \n\n**Figure 1**\n\nAddressing your concern about the value of Figure 1, we would like to emphasize that the purpose of this figure is to visually demonstrate what a self-similar process looks like. We never used a figure to prove self-similarity. \nThe correct and well-established approach in the (extensive) literature is to show that the auto-correlecation function decays algebraically (i.e. with a power law), giving rise to the H\u01d2lder exponent. This is precisely what we establish! The fact that a power law holds is shown in Figure 3 (top).\n\nWe elaborate on this in detail in Section 2, where we discuss the nature of self-similarity in language\u200b\u200b. The burstiness observed in Figure 1 is an intuitive and useful representation of self-similarity. Additionally, we provide extensive discussion on the significance and context of the H\u01d2lder exponent, particularly in Section 2.2, where its calculation and relevance to language are explained in detail\u200b\u200b.\n\n**PaLM-8B**\n\nRegarding your second point about the generalization of our findings from PaLM-8B to other models and language families, our claim is that we can obtain reasonable probability scores using LMs, such as PaLM-8B. There have been prior works that show this (we cite examples in the paper and we illustrate this calibration property in Figure 2). The reason we use PaLM-8B is merely because of the amount of compute required to conduct all the experiments.\n\nHowever, we did acknowledge this concern in the paper and that is precisely why we expanded our analysis to PaLM-540B, as detailed in Section 2.5 in one task. This was done to verify the robustness of our findings, and the results affirm the presence of self-similarity and long-range dependence (LRD) in language, irrespective of the model size\u200b\u200b. We believe this approach underscores the general applicability of our findings across different model architectures and scales.\n\n**Connection to Zipf's Law**\n\nYour suggestion to explore the connection between fractal structures in language and the works of Zipf and Shannon is discussed in Section 4. As we mention there, Zipf\u2019s law and Shannon\u2019s method of estimating the entropy of English only use first order statistics so they cannot be used to answer questions about long-range dependence or self-similarity. If there is anything missing that needs to be clarified, please let us know and we would be happy to elaborate.\n\n**Drawing links between language and natural phenomena**\n\nWe only mention that the Hurst parameter of 0.75 is quite common in natural phenomena and the fact that it also holds for language is interesting. Regarding the relation to learning processes, we have explored this in Section 3 in which we show a connection between scaling law parameters and fractal dimensions. For example, we find that when the H\u01d2lder exponent and the Hurst parameter are large in a particular domain, language models would benefit from having longer contexts during inference. We also studied the connection to the context length at training time but with no conclusive results, as we discussed in Section 3 using the Big Bench benchmark. There are many other similar questions of how fractal dimensions relate to learning and we believe we are only scratching the surface here. \n\n**Answers to the Questions**\n\n* LLMs have been used to create creative content. One example is asking GPT4 to write a proof that there are infinitely many primes in the form of a poem (https://arxiv.org/pdf/2303.12712.pdf ).\n* We do provide a reference in the paper for that particular statement. It\u2019s in Page 9 in https://arxiv.org/pdf/2303.12712.pdf. \n* In the notation, there is $w_{t-1}$, which is a single token, and there is $w_{[t-1]}$, which is the entire sequence of length t-1 tokens. So, both statements in the question are incorrect. When we query the LM to predict the probability of token $w_t$, we condition on $w_{[t-1]}$ not $w_{t-1}$. We will make the distinction easier by using boldface for $\\bf w_{[t-1]}$ .\n* The lengths are random variables. Since the process is stationary, those lengths have definite expected values, but the value of course varies from clauses to sentences and sections. How the expected length changes will determine the level of self-similarity. These are standard definitions as we state in the paper and we provide references for them. We do not invent them.\n* Thank you. We will fix the missing fields in those references.\n\n\nIf there are any other questions or concerns, we would appreciate it if you let us know so we can respond to them during the discussion and improve the manuscript.\n\nThank you"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4084/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699708567378,
                "cdate": 1699708567378,
                "tmdate": 1699725625762,
                "mdate": 1699725625762,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "365ilhHakU",
                "forum": "PQzdtBiKie",
                "replyto": "KsEIh8F5Ur",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4084/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4084/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up"
                    },
                    "comment": {
                        "value": "Dear reviewer, \n\nWe would like to kindly inquire if our responses have satisfactorily addressed your concerns and questions. We are open to further clarification if needed. If we have addressed your concerns, we would appreciate it if you consider revising your review/score accordingly. Otherwise, please let us know so we can respond to your inquiries during the discussion period.\n\nSincerely"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4084/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700235620203,
                "cdate": 1700235620203,
                "tmdate": 1700235620203,
                "mdate": 1700235620203,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "6h8PqwZotF",
            "forum": "PQzdtBiKie",
            "replyto": "PQzdtBiKie",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4084/Reviewer_iZo5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4084/Reviewer_iZo5"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to quantify and characterize self-similarity and long range dependency in language (token sequences) by borrowing concepts from analysis of sequential processes like Weiner process/ Brownian motion, fractal processes etc. Specifically, the natural language sequences/sentences are represented as a sequence of negative conditional logprobs (related to bits of information per token) of the tokens obtained via autoregressive language models. The actual token identities are ignored and only the conditional probability under the language model is considered. Using this representation, \u201cincrement\u201d and \u201cintegral\u201d processes are defined to estimate the degree of self-similarity and long-range dependency via estimating the appropriate parameters (\u201cself-similarity exponent\u201d and \u201cHurst coefficient\u201d) via standard methods for analysing real-valued sequential processes. The language model used for this purpose is Palm-8B and 4 language corpora (Big Patent, Newsroom, Scientific papers, Wikipedia) are analyzed using the proposed method. The estimated coefficients indicate that language exhibits self-similarity i.e. it exhibits burstiness over different granualrities (sentence/paragraphs and so on), and it also exhibits long range dependency. This analysis is extended to varying model sizes and training and inference context lengths. Overall higher degree of self-similarity and long range dependence is observed as the model size and context length is increased."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "\u2013 This is a fresh perspective on analysis of natural language and autoregressive language models. It provides connection to insights about the hierarchical nature of language and interconnected and recursiveness in language.\n\n\u2013 The paper is well-written and the technique is clearly described. The approach is well-motivated and connections to prior work and techniques are illuminating.\n\n\u2013 The experiments are well-designed. The results show robustness across a variety of domains and the relationship to scaling laws substantiates the understanding about how language models improve with scale."
                },
                "weaknesses": {
                    "value": "\u2013 The identity of tokens is ignored in the analysis and the representation is just a sequence of negative logprobs. While this is still indicative of some macro features of language, this analysis is limited to an impoverished representation of language.\n\n\u2013 As an upshot of the point above, this analysis doesn\u2019t provide detailed insight into the exact latent hierarchical organization of language and is limited to merely an indication that language exhibits burstiness over long-range. I am unable to imagine how these concepts and methods can be extended to yield deeper insights about language and LMs beyond this paper.\n\n\u2013 While this is a fresh perspective, the main finding that next-token prediction is a difficult task because language tends to show burstiness at varying granularities and exhibits long-range dependence is not surprising or novel. For example, the hierarchical and recursive nature of language is an actively studied area of research in cognitive sciences and linguistics.\n\n\u2013 \u201cSelf-similarity\u201d and \u201cfractalness\u201d are not identical concepts. The findings in fact suggest that language is not necessarily fractal which makes predictability challenging. This is acknowledged in the paper but yet the two concepts seem to have been used interchangeably in the discussion."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4084/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699048653191,
            "cdate": 1699048653191,
            "tmdate": 1699636372783,
            "mdate": 1699636372783,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "A1h2KLY9Qu",
                "forum": "PQzdtBiKie",
                "replyto": "6h8PqwZotF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4084/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4084/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thank you for your valuable insights and the careful review of our manuscript. We are grateful for your recognition of the novelty of our approach as well as your positive comments on the clarity of our writing and the design of our experiments.\n\nAddressing your first concern regarding the representation of language as a sequence of negative log probabilities (i.e. bits), we acknowledge that this approach may simplify the rich and complex nature of language. As you mention, it is unclear how the approach can be extended to take the identities of tokens into account. However, the approach can be interpreted as a lower bound to the complexity of natural language. In particular, long-range dependence on the sequence of bits implies the existence of long-range dependence on the sequence of tokens (by the data processing inequality).\n\nRegarding the concern that our findings might not be perceived as surprising or novel, we appreciate this viewpoint. While the hierarchical and recursive nature of language is a subject of study in cognitive sciences and linguistics, our work aims to bridge these concepts with the computational perspective of language models, focusing on how these inherent properties of language influence model performance. This interdisciplinary approach, we hope, adds value to the existing body of research by providing a fresh perspective to understand LLMs.\n\nWe thank you for emphasizing on the distinction between \u201cself-similarity\u201d and \"fractalness.\" It is not our intention to use these terms interchangeably, and we have acknowledged that they are different concepts in the paper. We will take care to avoid this in the revised version. \n\nIf there are any other questions or concerns, we would appreciate it if you let us know so we can respond to them during the discussion and improve the manuscript.\n\nThank you again for your careful and thorough critique."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4084/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699725596092,
                "cdate": 1699725596092,
                "tmdate": 1699870234534,
                "mdate": 1699870234534,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QJWk5xNanz",
                "forum": "PQzdtBiKie",
                "replyto": "6h8PqwZotF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4084/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4084/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up"
                    },
                    "comment": {
                        "value": "Dear reviewer, \n\nWe would like to kindly inquire if our responses have satisfactorily addressed your concerns and questions. We are open to further clarification if needed. If we have addressed your concerns, we would appreciate it if you consider revising your review/score accordingly. Otherwise, please let us know so we can respond to your inquiries during the discussion period.\n\nSincerely"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4084/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700235657314,
                "cdate": 1700235657314,
                "tmdate": 1700235657314,
                "mdate": 1700235657314,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "iqAcffMSGH",
            "forum": "PQzdtBiKie",
            "replyto": "PQzdtBiKie",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4084/Reviewer_hBB9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4084/Reviewer_hBB9"
            ],
            "content": {
                "summary": {
                    "value": "This paper examines the fractal structure of language and makes connections between self-similarity, long-range dependence, and the capabilities of large language models (LLMs). Fractal parameters like the Hurst exponent and self-similarity exponent are estimated on text datasets. The key ideas are that the self-similar nature of language allows LLMs to capture patterns across levels of granularity, while long-range dependence requires modeling broader contexts spanning indefinitely into the past. Experiments with PaLM on different datasets (e.g., Wikipedia, patent) empirically estimates fractal parameters like the Hurst exponent and self-similarity exponent on text datasets, finding values that suggest the presence of self-similarity and long-range dependence in language."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "(+) The perspective of analyzing language through fractal patterns and quantifying self-similarity and long-range dependence provides a novel angle for understanding LLMs. While intuitions about self-similarity have existed, this paper makes them concrete and rigorous.\n\n(+) The analyses seem technically sound overall. Estimation of fractal parameters is done carefully and situated within the literature. \n\n(+) these insights about fractal structure connecting to capabilities of LLMs could potentially be an important conceptual advance. It formalizes useful notions about complexity and long-range dependencies in language."
                },
                "weaknesses": {
                    "value": "(-) The connection between fractal structure and capabilities of LLMs is suggestive but not conclusively proven. The arguments linking self-similarity to modeling patterns across granularities seem reasonable but remain speculative without more analysis.\n\n(-) The analysis is conducted solely on text datasets encoded using the PaLM language model. The conclusions may not generalize to other modalities or other LLMs, which is a limitation. Evaluating universality across models and data types could strengthen the results. Also, only PaLM-8B was examined, while the comparison between small and large LMs provides additional insight beyond confirming consistency. The pretraining experiment is quite brief. More in-depth studies could be beneficial."
                },
                "questions": {
                    "value": "See weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4084/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699274962282,
            "cdate": 1699274962282,
            "tmdate": 1699636372717,
            "mdate": 1699636372717,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "izull6zIB6",
                "forum": "PQzdtBiKie",
                "replyto": "iqAcffMSGH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4084/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4084/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful and constructive feedback on our manuscript. We are grateful for your recognition of the novelty and technical soundness of our approach and its potential impact in advancing the conceptual understanding of large language models (LLMs).\n\nAddressing your concern about the connection between fractal structure and the capabilities of LLMs, we acknowledge that our current work is an initial step in this direction. As you suggested, extending the analysis to other modalities and models would offer interesting insights, and finding further connections between fractal dimensions and learning could uncover other results that are useful for practitioners. We plan to explore these directions in the future.\n\nIn our work, we propose a precise mathematical formalism (using fractals and self-similarity) to quantify properties of language that may have been known intuitively but not formally shown, such as long-range dependence, and connected those to properties of learning. We hope this work lays the foundation for further explorations, highlighting the potential of fractal analysis as a lens for understanding and improving LLMs. We believe that additional research in this direction could provide substantial value to the field and enhance the applicability of our findings.\n\nIf there are any other questions or concerns, we would appreciate it if you let us know so we can respond to them during the discussion and improve the manuscript. \n\nThank you again for your constructive critique."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4084/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699722519735,
                "cdate": 1699722519735,
                "tmdate": 1699722519735,
                "mdate": 1699722519735,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6zk4hCNZyZ",
                "forum": "PQzdtBiKie",
                "replyto": "iqAcffMSGH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4084/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4084/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up"
                    },
                    "comment": {
                        "value": "Dear reviewer, \n\nWe would like to kindly inquire if our responses have satisfactorily addressed your concerns and questions. We are open to further clarification if needed. If we have addressed your concerns, we would appreciate it if you consider revising your review/score accordingly. Otherwise, please let us know so we can respond to your inquiries during the discussion period.\n\nSincerely"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4084/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700235676814,
                "cdate": 1700235676814,
                "tmdate": 1700235676814,
                "mdate": 1700235676814,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]