[
    {
        "title": "FasterViT: Fast Vision Transformers with Hierarchical Attention"
    },
    {
        "review": {
            "id": "jb66dyETnt",
            "forum": "kB4yBiNmXX",
            "replyto": "kB4yBiNmXX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission467/Reviewer_oF4Y"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission467/Reviewer_oF4Y"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a new vision transformer architecture, FasterViT, aiming to improve the GPU throughput for vision tasks. The key innovations of the paper include i) using convolutional blocks for the early stages of the model, which are typically memory bound; ii) a new hierarchical attention module (HAT) that interleaves global attention over carrier tokens and self-attention within local windows. Experimental results show that, compared to prior works, FasterViT achieves a better trade-off between accuracy and GPU throughput."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "(S1) [Method] the idea of incorporating convolutional blocks for memory-bound stages is interesting, which is actually similar to the design of MobileViT.\n\n(S2) [Performance] Compared to the latest approaches, FasterViT presents improved throughputs on A100.\n\n(S3) [Ablation] Ablations show that the proposed HAT is a plug-and-play module that can also improve the performance of other models\n\n(S4) [Writing] The paper is easy to follow."
                },
                "weaknesses": {
                    "value": "(W1) The proposed HAT looks similar to the hierarchical attention proposed in EdgeViT (Pan et al. 2022). While comparisons with EdgeViT is included, it would be beneficial if the author could further clarify the key difference between HAT and the attention block in EdgeViT (maybe just a small paragraph)\n\n(W2) If the reviewer understood correctly, FasterViT incorporated two new designs to improve the throughout i) conv-blocks in memory-bound stages ii) HAT. Clarifying how each element contributes to the throughput would be insightful. Here, the impact of ii) is shown in Table 5, an ablation study for i) would be a valuable addition.\n\n(W3) The authors are encouraged to expand the evaluation to include accelerators beyond A100, such as different GPUs, or even NPUs, so as to provide a broader performance perspective.\n\n\nThe reviewer appreciates the improved performance of FasterViT, but has some concerns about clarity and evaluation. At this moment, the reviewer would like to rate the paper as weak accept."
                },
                "questions": {
                    "value": "N.A."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission467/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698776451026,
            "cdate": 1698776451026,
            "tmdate": 1699635973191,
            "mdate": 1699635973191,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7nhORG5czf",
                "forum": "kB4yBiNmXX",
                "replyto": "jb66dyETnt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission467/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission467/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer oF4Y"
                    },
                    "comment": {
                        "value": "We sincerely thank the reviewer for their time and efforts in reviewing our work and providing valuable feedback that can further strengthen our manuscript. Below please find our detailed responses: \n\n> **Differences between HAT and attention block in EdgeViT**\n\nThere are key differences between our proposed HAT and the attention block in EdgeViT. The attention in EdgeViT follows a \u201clocal-global-local\u201d paradigm. Specifically, a convolutional layer first aggregates local information into representative tokens. A global (sparse) attention is then computed on the aggregated tokens. Lastly, a depthwise convolutional layer is used to compute local information from global information. In contrast, HAT computes self-attention between: (1) each carrier token and their corresponding local regions (2) all carrier tokens. The carrier tokens are only computed once and propagated to next layers, whereas EdgeViT repeats the computation for each operation at every transformer block.  One disadvantage of the proposed scheme in EdgeViT is the loss of information in transitioning from one attention block to another, which is sub-optimal. Another potential drawback is that the way information is propagated is restricted by convolution operations; meanwhile, HAT allows one to learn the aggregation and propagation mechanics via attention. \n\n> **Impact of conv-blocks on throughput**\n\nThank you for your comment. We have conducted an additional ablation study to demonstrate the effect of conv-based block on both accuracy and throughput as shown below. \n\n| Model   |Top-1 | Throughput\n|-------------------|----------------|----------------|\n| FasterViT-0 |82.1|5802|\n| FasterViT-0 wo Conv-block |81.7|3616|\n| FasterViT-1|83.2|4188|\n| FasterViT-1 wo Conv-block |82.8|3280|\n| FasterViT-2 |84.2|3161|\n| FasterViT-2 wo Conv-block |83.8|2085|\n| FasterViT-3 |84.9|1780|\n| FasterViT-3 wo Conv-block |84.5|1397|\n| FasterViT-4 |85.4|849|\n| FasterViT-4 wo Conv-block |84.9|712|\n\n\nAccording to our experiments, replacing Conv-based blocks with Transformer-based counterparts significantly reduces the throughput while also reducing the accuracy. As expected, the Conv-based blocks are more efficient than the transformer counterparts for processing larger input sizes. The model with conv-based blocks also has higher accuracy compared to their fully-transformer-based counterparts due to incorporating inductive biases such as locality.  \n\n\nThe combination of Conv-based (stage 1 and 2) and transformer-based (stage 3 and 4) architecture as presented in FasterViT strikes the right balance between accuracy and efficiency. \n\nWe have added this ablation study to the revised manuscript. \n\n\n> **Throughput comparison on other platforms**\n\nWe have added additional throughput comparison on different platforms such as V100, TITAN RTX and A6000 GPUs, Jetson Nano\nand Intel(R) Xeon(R) E5-2698 v4 CPU. For all comparisons, we show that FasterViT achieves a Pareto-front in for ImageNet Top-1 and throughput trade-off, hence validating the effectiveness and scalability of our model to different hardware platforms. \n\n\nIn V100 ([Figure](https://drive.google.com/file/d/1FP0pDnZMdxw2Sy0z1tX-8A54T6eKD0cq/view?usp=sharing)) or TITAN RTX ([Figure](https://drive.google.com/file/d/1elnLkDMM5s2ch-E5MKb5YDj1vV_Rsx1s/view?usp=sharing)) comparisons for instance, we observe a strong performance for all FasterViT models. Even on CPU ([Figure](https://drive.google.com/file/d/1iVsev1XxgNaFEgFrhV0i4lGO-Fldl3z9/view?usp=sharing)), FasterViT still outperforms competitors by a significant margin. \n\nPlease see the supplementary materials of the revised manuscript for all the figures regarding throughput and Top-1 accuracy."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission467/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740327269,
                "cdate": 1700740327269,
                "tmdate": 1700740641778,
                "mdate": 1700740641778,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QA63Wk2e6j",
            "forum": "kB4yBiNmXX",
            "replyto": "kB4yBiNmXX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission467/Reviewer_Wvdn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission467/Reviewer_Wvdn"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a novel family of hybrid CNN-ViT neural networks called FasterViT, designed for high image throughput in computer vision applications. FasterViT leverages the strengths of both CNNs and ViT by introducing a Hierarchical Attention (HAT) approach that decomposes global self-attention into a multi-level attention with reduced computational costs. This approach efficiently combines local and global representation learning. FasterViT consists of four stages, reducing input image resolution while doubling feature maps in the early stages and employing transformer blocks in later stages. The proposed HAT mechanism efficiently captures long-range spatial dependencies and cross-window interactions. Extensive validation on various computer vision tasks, including image classification, object detection, and segmentation, demonstrates that FasterViT achieves state-of-the-art performance in terms of accuracy and image throughput, especially for high-resolution images, outperforming competitive models like Swin Transformer V2."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- FasterViT is tailored for high-resolution input images and demonstrates faster image throughput compared to competitive models, particularly in handling images with higher resolutions.\n- The proposed HAT approach efficiently decomposes global self-attention into a multi-level attention mechanism, reducing computational complexity and enabling effective local and global representation learning. Overall, the idea of carrier tokens is novel and interesting.\n- The paper extensively validates FasterViT on various computer vision tasks, including image classification, object detection, and semantic segmentation, showcasing its state-of-the-art performance across a wide range of applications."
                },
                "weaknesses": {
                    "value": "- The comparisons in Table 5 and 7 demonstrate minor improvements in terms of accuracy, while the throughput in Table 5 is reduced when using HAT. \n- Performance is compared on A100 GPUs. More platforms should be used to see if throughput results are consistent. At present results are not conclusive."
                },
                "questions": {
                    "value": "- The authors should better explain the difference between the proposed attention and window local approaches line Swin."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission467/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698848727253,
            "cdate": 1698848727253,
            "tmdate": 1699635973117,
            "mdate": 1699635973117,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ka8ZyOTaky",
                "forum": "kB4yBiNmXX",
                "replyto": "QA63Wk2e6j",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission467/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission467/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Wvdn"
                    },
                    "comment": {
                        "value": "We sincerely thank the reviewer for their time and efforts in reviewing our work and providing valuable feedback that can further strengthen our manuscript. Below please find our detailed responses: \n\n> **Differences between proposed attention and local approaches like Swin**\n\nSpecifically, Swin uses a local-only window-based attention and window shifting. However, window shifting is sub-optimal for modeling cross-region interactions due to the limited stride. On the contrary, in our work, we use carrier tokens to model cross-region interactions (global attention) as well as interaction between the carrier token and local regions. Hence, this allows for effectively modeling both short and long range dependencies in a very efficient manner. \n\n> **Minor improvements in terms of accuracy while Reduced throughput when using HAT**\n\nThank you for your comment. We believe that accuracy improvements in Table 5 are significant when compared to other models. Specifically, for FasterViT-1, using HAT improves the performance by 1.1% in terms of Top-1 accuracy. For comparison, a mere 1.1% improvement in terms of ImageNet Top-1 accuracy equates to scaling from ConvNeXt-T (82.1%) to ConvNeXt-S (83.2%) models. Furthermore, the drop in the throughput is expected since HAT effectively models the cross-region interactions that may not be properly captured by Twins and EdgeViT.  \n\n> **Throughput comparison on other platforms**\n\nWe have added additional throughput comparison on different platforms such as V100, TITAN RTX and A6000 GPUs, Jetson Nano\nand Intel(R) Xeon(R) E5-2698 v4 CPU. For all comparisons, we show that FasterViT achieves a Pareto-front in for ImageNet Top-1 and throughput trade-off, hence validating the effectiveness and scalability of our model to different hardware platforms. \n\n\nIn V100 ([Figure](https://drive.google.com/file/d/1FP0pDnZMdxw2Sy0z1tX-8A54T6eKD0cq/view?usp=sharing)) or TITAN RTX ([Figure](https://drive.google.com/file/d/1elnLkDMM5s2ch-E5MKb5YDj1vV_Rsx1s/view?usp=sharing)) comparisons for instance, we observe a strong performance for all FasterViT models. Even on CPU ([Figure](https://drive.google.com/file/d/1iVsev1XxgNaFEgFrhV0i4lGO-Fldl3z9/view?usp=sharing)), FasterViT still outperforms competitors by a significant margin. \n\nPlease see the supplementary materials of the revised manuscript for all the figures regarding throughput and Top-1 accuracy."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission467/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740265271,
                "cdate": 1700740265271,
                "tmdate": 1700740613535,
                "mdate": 1700740613535,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "sREUNz6nHj",
            "forum": "kB4yBiNmXX",
            "replyto": "kB4yBiNmXX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission467/Reviewer_tsUS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission467/Reviewer_tsUS"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a fast ViT architecture using hierarchical attention module (HAT). The HAT module is a modified version of window attention by introducing a new carrier token that summarizes the information of a local window. In this way, the model can preserve a certain level of global information while being more efficient than conventional global attention. With such architectural changes, the proposed FasterViT is able to achieve fast inference on modern GPUs and produces good accuracy on classification and downstreaming tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is overall well written and organized. The motivation of proposing the HTA module is technically sound - it is a good practice to do more memory intensive operations in early stages while putting the computational intensive operations to the later stage. This is also verified in the experiments, where the proposed model is more GPU friendly compared to existing models.\n- The HAT module is not unnecessarily complex and intuitively easy to implement. It is also shown that it can be a plug-in replacement for conventional attention blocks, which makes it more flexible."
                },
                "weaknesses": {
                    "value": "The major weakness of this work is experiment. The paper claims that with HAT and all the optimization regarding model architecture, the model has much less complexity compared to conventional attention and it compares the proposed model to a few efficient ViT. However, the comparison is on A100 GPU only and there is no comparison on any other platforms. Some recent works, such as EfficientFormer, FastViT and NextViT compared in Figure 1, all benchmarked their models on different mobile platforms. Therefore, it is unclear whether the proposed change applies similarly to other platforms, which could be very different from A100 GPu due to various software and hardware optimizations. In addition, the ablation study is not sufficient. To really understand the HAT module, a more detailed analysis on it should be provided."
                },
                "questions": {
                    "value": "- While the proposed HAT is claimed to be more efficient, only benchmarks on A100 GPUs using a batch 128 are provided. Due to various software and hardware differences, conclusion drawn from the current benchmark may not generalize. For example, it is well known that depthwise conv is more efficient on mobile CPU than on GPU and some ops such as reshape/transpose on CPU is slower due to L2 cache limitation. Even on GPU only, the performance may also vary on different variants. Given that most recent works on efficient ViT have presented benchmark results on multiple platforms - GPU, CPU and DSP, the evaluation from this work is worse and incomplete. I would like to see more benchmarks otherwise the current results are not so convincing.\n- The comparison with existing models is not complete. Although in Figure 1, the proposed model is compared to many recent works in terms of image classification task, the comparison on other downstreaming tasks such as detection and segmentation is quite limited. For example, it is only compared with 3 other models (which are not even designed for efficiency) on the semantic segmentation task.\n- The ablation study only shows that the HAT module can be plugged into other model architecture and improves the performance marginally, but misses analysis on the module itself: how do you determine the architecture for the model variants from 0 to 6? Are there any guidelines or empirical results? How does the latency change with respect to changing the parameters in HAT, such as window size, conv kernel size, pooling size, etc? The current setting seems quite ad-hoc.\n- The parameter count and flops of the proposed model seems on the higher end. Under similar latency, it's much larger than most models. This may prevent it from being adopted in realistic scenarios, where there may be restrictions on model size and memory footprint. How do you aim to resolve this?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission467/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698883319141,
            "cdate": 1698883319141,
            "tmdate": 1699635973056,
            "mdate": 1699635973056,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "MmkZmuPgUi",
                "forum": "kB4yBiNmXX",
                "replyto": "sREUNz6nHj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission467/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission467/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer tsUS"
                    },
                    "comment": {
                        "value": "We sincerely thank the reviewer for their time and efforts in reviewing our work and providing valuable feedback that can further strengthen our manuscript. Below please find our detailed responses: \n\n> **Throughput comparison on other platforms**\n\nWe have added additional throughput comparison on different platforms such as V100, TITAN RTX and A6000 GPUs, Jetson Nano\nand Intel(R) Xeon(R) E5-2698 v4 CPU. For all comparisons, we show that FasterViT achieves a Pareto-front in for ImageNet Top-1 and throughput trade-off, hence validating the effectiveness and scalability of our model to different hardware platforms. \n\n\nIn V100 ([Figure](https://drive.google.com/file/d/1FP0pDnZMdxw2Sy0z1tX-8A54T6eKD0cq/view?usp=sharing)) or TITAN RTX ([Figure](https://drive.google.com/file/d/1elnLkDMM5s2ch-E5MKb5YDj1vV_Rsx1s/view?usp=sharing)) comparisons for instance, we observe a strong performance for all FasterViT models. Even on CPU ([Figure](https://drive.google.com/file/d/1iVsev1XxgNaFEgFrhV0i4lGO-Fldl3z9/view?usp=sharing)), FasterViT still outperforms competitors by a significant margin. \n\nPlease see the supplementary materials of the revised manuscript for all the figures regarding throughput and Top-1 accuracy. \n\n\n\n> **Additional Downstream Benchmarks**\n\nWe thank the reviewer for their comment. We provide additional experiments for both object detection and semantic segmentation with more models, across different sizes, to demonstrate the effectiveness and efficiency of our work. \n\nFirstly, we present additional object detection experiments with DINO on MS-COCO dataset as shown below:\n\n| Backbone   |Model|AP_box |Throughput\n|-------------------|----------------|---------------|---------------|\n|Swin-L |HTC++ |57.1|-|\n|Swin-L |DINO|58.5|71|\n|**FasterViT-4**|DINO|58.7|84|\n\nThe DINO model with FasterViT-4 is 18.30% faster than its counterpart with Swin-L backbone in terms of image throughput and outperforms it by +0.1 in terms of box AP. \n\nWe also added a semantic segmentation study on the ADE20K dataset with the FPN network, as shown below. Specifically, we compare against PoolFormer and PVT backbones: \n\n\n| Backbone   |Model|mIoU |Throughput\n|-------------------|----------------|---------------|---------------|\n|PoolFomer-S36 |FPN |42.0|453|\n|**FasterViT-1**|FPN|42.7|491|\n|PoolFomer-M36 |FPN |42.4|368|\n|**FasterViT-2**|FPN|43.5|405|\n\n\nThe model with FasterViT-1 backbone outperforms counterpart PoolFormer-S36 by +0.7 in terms of mIoU while also being 8.38% faster in terms of image throughput. Similarly, the model with FasterViT-2 backbone significantly outperforms PoolFomer-M36 counterpart by +1.1 in terms of mIOU while being 10.05% faster. \n\n\nWe believe that the above experiments validate the effectiveness of FasterViT as an efficient backbone for downstream tasks such as segmentation and detection across different model sizes.  \n\n\n> **How does latency change with respect to change of HAT parameters**\n\nThank you for your question. We have conducted an additional ablation study to investigate the effect of HAT hyperparameters on the latency and accuracy of the model. Specifically, we examine the effect of window size (W) and carrier token window size (W_C) as shown below for 224x224 resolution. We use FasterViT-2 with 7x7 window size and 2x2 carrier token window size as the base model. Latency ratio is measured against this base model. \n\n\n| Resolution   |Window Size|Carrier Token Window Size |Latency Ratio|Top1|\n|-------------------|----------------|---------------|---------------|---------------|\n|224 |7 |2|1|84.2|\n|224 |7 |1|1.05|83.9|\n|224 |7 |5|0.67|84.3|\n|224 |7 |6|0.57|84.4|\n|224 |7 |9|0.47|84.9|\n|224 |14 |0|0.9|84.4|\n\nWe observe that increasing the carrier token window size can improve the performance at the cost of increased latency, sometimes by a significant margin. The 2x2 carrier token window size offers a great trade-off between accuracy and latency. \n\nIn addition, increasing the window size from 7 to 14 increases the Top-1 accuracy by +0.2%. However, as expected, it increases the latency by 10%. We also note that although increasing the window size results in better performance, it does not scale properly to higher resolution images. As a result, HAT is a more effective and efficient mechanism that can be employed without sacrificing image throughput. \n\n> **How do you determine the architecture for the model variants from 0 to 6**\n\nKey design choices for each model variant are : (1) stage-wise depth (2) embedding dimension (3) number of attention heads. Our base model starts with a simple configuration that consists of 2, 3, 6, 5 layers per stage 1, 2, 3 and 4 respectively, an embedding dimension of 64 and 2, 4, 8, 16 attention heads for stages 1, 2, 3 and 4. From this base configuration, we increased the model capacity by increasing the embedding dimension or stage-wise depth to create a Pareto front of Top-1 and throughput trade-off."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission467/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740173088,
                "cdate": 1700740173088,
                "tmdate": 1700740580449,
                "mdate": 1700740580449,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QHfTJqGVug",
            "forum": "kB4yBiNmXX",
            "replyto": "kB4yBiNmXX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission467/Reviewer_B19F"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission467/Reviewer_B19F"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a hybrid conv+transformer network, optimizing throughputs for high-resolution image processing.  It proposes a Hierarchical Attention to combine local window tokens and carrier tokens (one per window).  Results are mostly on ImageNet and ImageNet21k, and the results in Figure 1 are quite impressive."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Intuitive idea:  combing conv + attention is not new, and is often considered more efficient than pure conv or pure transformer for image processing. Window attention is also not new, but the hierarchical window attention is interesting.\n2. Impressive results:  results in Figure 1 are pretty impressive. FastViT outperforms other models by pretty good margin on ImageNet.\n3. Well written paper and easy to follow."
                },
                "weaknesses": {
                    "value": "1. It is unclear how significant the proposed hierarchical attention (HAT) is.  Table 5 shows this HAT is better than Twins and EdgeViT; however, Table 7 show only marginal gains when comparing to the vanilla SwinTransformer if treating HAT as a plug-and-play module.\n2. The idea of HAT is not well motivated.  Though it shows good empirical results, it is unclear why simply adding a per-window CT token can significantly improve quality.  Would be nice to add a few more ablation studies or insights.\n3. All latencies are measured on A100 GPU. It would be nice to measure the latency on more diverse hardware platforms, such as different kinds of GPUs, and CPUs.\n4. Some image texts (e.g. in Figure 4 and 5) are too small to read. I recommend enlarging the text in these images."
                },
                "questions": {
                    "value": "1. Could you provide more intuition and insights why simply adding one CT token per window will significantly improve model quality?\n2. Could you show the comparisons of FastViT with and without hierarchical attention for ImageNet, e.g., one is vanilla SwinTransformer-style attention and one is HAT?\n3. In Table 7, could you also add the throughput comparison?  I am curious about the overhead of HAT.\n4. Could you add latency for more hardwares (different GPUs and CPUs)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission467/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698894754633,
            "cdate": 1698894754633,
            "tmdate": 1699635972987,
            "mdate": 1699635972987,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "u0SBMcqZav",
                "forum": "kB4yBiNmXX",
                "replyto": "QHfTJqGVug",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission467/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission467/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer B19F"
                    },
                    "comment": {
                        "value": "We sincerely thank the reviewer for their time and efforts in reviewing our work and providing valuable feedback that can further strengthen our manuscript. Below please find our detailed responses: \n\n> **Intuition on why HAT and CT improve the model quality**\n\nHAT allows for modeling global contextual information with minimal impact on computational efficiency. Specifically, the carrier tokens in HAT facilitate capturing cross-region interactions in an efficient and scalable manner, especially for high-resolution images. Without carrier tokens, the self-attention can only process local information inside every window and lack the capability of capturing long-range information. \n\n> **Comparison of FasterViT with and without hierarchical attention for ImageNet**\n\nWe have conducted an additional ablation study to demonstrate the effect of removing HAT for every FasterViT variant as shown below:  \n\n| Model   |HAT |Top-1 |\n|-------------------|----------------|----------------|\n| FasterViT-0 |Yes|82.1|\n| **FasterViT-0 wo HAT** |No|81.7|\n| FasterViT-1 |Yes|83.2|\n| **FasterViT-1 wo HAT** |No|82.8|\n| FasterViT-2 |Yes|84.2|\n| **FasterViT-2 wo HAT** |No|83.7|\n| FasterViT-3 |Yes|84.9|\n| **FasterViT-3 wo HAT** |No|84.2|\n| FasterViT-4 |Yes|85.4|\n| **FasterViT-4 wo HAT** |No|84.3|\n\n\nWe observe that removing HAT can significantly decrease the accuracy, especially for larger variants such as the FasterViT-4 model. \n\n\nIn addition to the above experiments, Figure S.1 offers more insights on the effectiveness of HAT. Specifically, in this figure, the carrier tokens have been appended to the left top corner. The vertical line in  dense attention maps indicates that all other tokens append to the carrier tokens, hence allowing for cross-region communications to capture both short and long-range spatial dependencies. \n\n\n> **Significance of HAT and plug-and-play module gains in Swin Transformers**\n\n Swin Transformers use window shifting to capture the local cross-region interactions. However, this scheme is sub-optimal as it is limited to a small area of coverage which cannot effectively capture the long-range spatial dependencies. However, HAT proposes an efficient and effective way of modeling global information which is important for downstream tasks and high-resolution images. \nThe improvements shown in Table 7 are significant as no additional hyper-parameter tuning was done. Specifically, as shown in In Table 7, using HAT with Swin improves the performance by +0.9 in terms of mIoU for semantic segmentation. Similarly, it improves the performance by +0.5 and +0.6 interim of box AP and mask AP for object detection and instance segmentation. In addition, FasterViT-4 outperforms counterpart Swin-L model by +0.2% when fine tuning on 384x384 resolution. \n\n> **Throughput numbers and HAT overhead in Table 7**\n\nWe provide throughput numbers for experiments in Table 7 in the following:\n\n| Model   |Top-1 |Throughput (ImageNet) |AP_box|AP_mask|Throughput (det)|mIoU|Throughput (seg)|\n|-------------------|----------------|----------------|----------------|----------------|----------------|----------------|----------------|\n| Swin-T |81.3|2758|50.4|43.7|161|44.5|330|\n| **Swin-T+HAT** |81.7|2721|50.9|44.3|150|45.4|338|\n\nAs seen above, HAT has a minor impact on overhead across different tasks and can be used as a viable standalone self-attention mechanism.   \n\nWe have also added throughput numbers in Table 7 of the revised manuscript (all numbers are shown in red). \n\n> **Throughput comparison on other platforms**\n\nWe have added additional throughput comparison on different platforms such as V100, TITAN RTX and A6000 GPUs, Jetson Nano\nand Intel(R) Xeon(R) E5-2698 v4 CPU. For all comparisons, we show that FasterViT achieves a Pareto-front in for ImageNet Top-1 and throughput trade-off, hence validating the effectiveness and scalability of our model to different hardware platforms. \n\n\nIn V100 ([Figure](https://drive.google.com/file/d/1FP0pDnZMdxw2Sy0z1tX-8A54T6eKD0cq/view?usp=sharing)) or TITAN RTX ([Figure](https://drive.google.com/file/d/1elnLkDMM5s2ch-E5MKb5YDj1vV_Rsx1s/view?usp=sharing)) comparisons for instance, we observe a strong performance for all FasterViT models. Even on CPU ([Figure](https://drive.google.com/file/d/1iVsev1XxgNaFEgFrhV0i4lGO-Fldl3z9/view?usp=sharing)), FasterViT still outperforms competitors by a significant margin. \n\nPlease see the supplementary materials of the revised manuscript for all the figures regarding throughput and Top-1 accuracy. \n\n\n> **Small text in figures**\n\nThank you for your suggestion. We have improved the figures and enlarged their embedded text."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission467/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700739955524,
                "cdate": 1700739955524,
                "tmdate": 1700740535938,
                "mdate": 1700740535938,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]