[
    {
        "title": "EcoAssistant: Using LLM Assistant More Affordably and Accurately"
    },
    {
        "review": {
            "id": "rjxjWsDUTC",
            "forum": "vxZW1aROZA",
            "replyto": "vxZW1aROZA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission436/Reviewer_UE2g"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission436/Reviewer_UE2g"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes an economically efficient language model agent that can interact with API. It incorporates three techniques: 1) conversationally interact with the execution environment, 2) saves cost by using a hierarchy of LLM assistant, 3) using successful demonstration as the in-context examples. Empirical results show that the proposed approach is indeed effective."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper proposes an economically efficient system that can interact with APIs via code. The empirical results look convincing to me."
                },
                "weaknesses": {
                    "value": "This paper delivers a good system and represents a reasonable engineering contribution. However, I am a bit skeptical about its novelty: while probably no one has combined all these three tweaks together before, each of them seems relatively straightforward to me. Can fellow reviewers comment on the novelty for each of the three tweaks? \n\n(sorry that I am not following the related works very closely so I do not know exactly how novel these ideas were; however, I think they are very straightforward ideas to try after gpt-4 release and does not require conceptual innovations)"
                },
                "questions": {
                    "value": "- Would you mind commenting on the novelty of the proposed approach, or say, the most surprising part of this paper? Thanks!"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission436/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission436/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission436/Reviewer_UE2g"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission436/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698624168583,
            "cdate": 1698624168583,
            "tmdate": 1700751662687,
            "mdate": 1700751662687,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SiBLZGeRhP",
                "forum": "vxZW1aROZA",
                "replyto": "rjxjWsDUTC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission436/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission436/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review!"
                    },
                    "comment": {
                        "value": "Please refer to the general response for clarifications on the novelty.\n\n**Q: What is the most surprising part of this paper?**\n\nThe most surprising part of this work is the effectiveness of \u201cindirect advising\u201d (also explained in the general response): while the assistant hierarchy is designed to reduce the cost and the solution demonstration is to improve the success rate, their combination further reduces the cost. Such a synergistic effect emerges because the solution from a more powerful model (GPT-4) would be later used to guide a weaker model (GPT-3.5) automatically without a specialized design since the query-code database is shared across assistants. By looking at the solution from the more powerful model, the weaker model could solve more tasks and therefore reduce the overall cost by reducing the reliance on the more powerful model."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission436/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700630228306,
                "cdate": 1700630228306,
                "tmdate": 1700630410398,
                "mdate": 1700630410398,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "gtUSlNygbZ",
            "forum": "vxZW1aROZA",
            "replyto": "vxZW1aROZA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission436/Reviewer_R6M4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission436/Reviewer_R6M4"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a framework, EcoAssistant for LLMs to generate API calls for answering user\u2019s queries that require external knowledge. The framework consists of three components: iterative refinement based on automatic feedback from executors; a priority queue of LLMs where cheaper LLMs are used first; cache previously high-quality response as demonstration for further generation. The resulting system demonstrates better performance, and it\u2019s more cost-efficient.  \n\nOverall, the design of EcoAssistant makes a lot of sense, but it lacks novelty and research depth considering many related work in this direction."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- clear presentation of the framework and results\n- significant empirical improvement"
                },
                "weaknesses": {
                    "value": "- EcoAssistant relies on a set of known techniques (e.g., iterative refinement, demonstration library), the system per se is not novel from the technical perspective.\n- from the research perspective, it does not investigate (or focus on) several key problems in this system: 1) how do you reliably collect feedback from executors? are the automatic feedbacks reliable, 2) how to decide whether a generated response is good enough to be put in the demonstration library, 3) how to design the policy for back off in the general case?\n\nOverall, I think the design of EcoAssistant is not a significant contribution, and the authors do not go further beyond showing the empirical results of it. Though cost-efficiency is an appealing property, it\u2019s very unclear to me what the back-off policy looks like in general."
                },
                "questions": {
                    "value": "how do you decide whether a response is a success or not?  is it based on execution error?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission436/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698899054869,
            "cdate": 1698899054869,
            "tmdate": 1699635970038,
            "mdate": 1699635970038,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vU88tyrA5E",
                "forum": "vxZW1aROZA",
                "replyto": "gtUSlNygbZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission436/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission436/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review!"
                    },
                    "comment": {
                        "value": "**Q1: how do you reliably collect feedback from executors? are the automatic feedbacks reliable?**\n\nAs the executor would execute the code and send the output as feedback, the feedback is either an error message when the code has bugs or the information that the code prints. This feedback is naturally reliable because if it is an error message, it would help the assistant to debug their code like humans do; if it is printed information, it is essentially what the assistant is asking for and would help the assistant to address the user query.\n\n**Q2: how to decide whether a generated response is good enough to be put in the demonstration library**\n\nIn EcoAssistant, we store a query-code pair only when the query is successfully addressed. By this way, the stored code snippets are all solutions to previous queries and therefore have good quality.\n\n**Q3: how to design the policy for back off in the general case?**\n\nOur design of the assistant hierarchy provides a general back-off policy: the order of assistant invocation is based on the cost. Specifically, it always invokes a cheaper assistant before a more expensive one. This design aims to reduce the reliance on more expensive models in order to reduce the cost of the system."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission436/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700630271886,
                "cdate": 1700630271886,
                "tmdate": 1700630395946,
                "mdate": 1700630395946,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "KKnKsXUsd3",
            "forum": "vxZW1aROZA",
            "replyto": "vxZW1aROZA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission436/Reviewer_wYWS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission436/Reviewer_wYWS"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors present EcoAssistant, a framework for using existing LLMs to generate responses to invoke API calls in a cost effective manner and in a more autonomous manner."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Having cost-effective solutions are useful and having this paper especially optimize for the cost is a useful strategy. \n\n- The authors present an intuitive system that's easy to replicate, and have shown useful empirical results."
                },
                "weaknesses": {
                    "value": "- I think this paper does suffer from lack of novelty. I think the paper does show an intelligent combination of existing techniques and models, but in my opinion it doesn't meet the threshold for a full paper. It would have been useful if the authors presented a methodology/algorithm that would help automatically optimize given a set of LLMs, or presented an empirical analysis on a much larger dataset with more complex APIs."
                },
                "questions": {
                    "value": "- How does this method scale with # of APIs? For instance, the ToolLLM[1] paper had >16,000 APIs in their dataset. This would require some shortlisting using a retriever to make it compatible but I think adding that part would significantly help improve the novelty aspect of the paper.\n\n- I think more error analysis would also be needed to identify what kind of queries are problematic for which models. For instance, if we can identify if smaller LLMs can easily answer easy queries then we don't need to ever invoke the larger LLMs - are you already doing this? \n\n- Can you help me understand how do you define an exit criterion? For instance, what if the agent gets stuck in an infinite loop where the larger LLM and the smaller LLM agent keep going back and forth? \n\n\nReferences\n\n[1] Qin, Y., Liang, S., Ye, Y., Zhu, K., Yan, L., Lu, Y., ... & Sun, M. (2023). Toolllm: Facilitating large language models to master 16000+ real-world apis. arXiv preprint arXiv:2307.16789."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission436/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission436/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission436/Reviewer_wYWS"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission436/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699662977905,
            "cdate": 1699662977905,
            "tmdate": 1699662977905,
            "mdate": 1699662977905,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LSbxIfVFzM",
                "forum": "vxZW1aROZA",
                "replyto": "KKnKsXUsd3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission436/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission436/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review!"
                    },
                    "comment": {
                        "value": "**Q1: How does this method scale with # of APIs?**\n\nAs we only provide the assistant with the API name and a fake key, we can include as many APIs as we would like as long as it can be fit into the context window. But it is possible that more APIs might confuse the model and decrease the success rate. In that case, we can directly use the API retriever from ToolLLM as a plug-and-play module for EcoAssistant. We would like to leave it to future work since our focus is the cost-effective deployment of LLM services.\n\n**Q2a: More error analysis would also be needed to identify what kind of queries are problematic for which models.**\n\nThanks for the suggestion. We conduct error analysis on the Mixed-100 dataset (100 queries, details can be found in section 4.4), and the results are shown below. We identified 5 categories of error: 1) exceeding the context window limit of the model; 2) Inability to output correctly formatted code for execution; 3) incorrect answer or 4) partially-incorrect answer due to erroneous API use or problematic code; and 5) incorrect date (eg, asking for tomorrow\u2019s weather but actually output today\u2019s)\n\n| Method    | exceeds context window | code format | incorrect answer | partially-incorrect answer  | incorrect date | total number of failures\n|---------|----------|----------|---------|----------|----------|---------|\n| GPT-3.5-turbo | 3 | 56 | 14 | 1 | 1 | 75 |\n| GPT-4   | 3 | 0 | 25 | 7 | 6 | 40 |\n| EcoAssistant  | 0 | 0 | 16 | 3 | 1 | 20 | \n\nWe can see that the EcoAssistant demonstrated the best performance among the three, with a total of only 20 failures. It did not exceed the context window limit or have issues with code formatting. Compared with GPT-4, it showed fewer instances of incorrect answers, partially incorrect answers, and incorrect dates. This suggests that the solution demonstration enables EcoAssistant to leverage past solutions and process queries more accurately.\n\n**Q2b: if we can identify if smaller LLMs can easily answer easy queries, then we don't need to ever invoke the larger LLMs**\n\nThis idea is already realized in our assistant hierarchy: because EcoAssistant would always start with the cheapest assistant, if it succeeds, then it will not invoke more expensive assistants.\n\n**Q3: how do you define an exit criterion?**\n\nFor each query, the EcoAssistant will terminate after trying out all the assistants. In particular, it always invokes the cheapest assistant first, and invokes the next one only when the current one fails to address the query. The order of invocation is based on the cost of the LLM and each assistant will be invoked at most once for each query. If the last and most expensive assistant still fails, the EcoAssistant will terminate, so it won\u2019t get stuck in an infinite loop."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission436/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700630381127,
                "cdate": 1700630381127,
                "tmdate": 1700630381127,
                "mdate": 1700630381127,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]