[
    {
        "title": "Neural Contractive Dynamical Systems"
    },
    {
        "review": {
            "id": "OizrLboDSf",
            "forum": "iAYIRHOYy8",
            "replyto": "iAYIRHOYy8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5107/Reviewer_2Vh1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5107/Reviewer_2Vh1"
            ],
            "content": {
                "summary": {
                    "value": "### Problem Statement\nThe paper discusses the significant problem concerning the assurance of stability of data-driven controlling of robots, especially when the learned dynamics are controlled by neural networks. Stability is critical to prevent robots from executing harmful or undesirable actions. However, achieving global stability in dynamical systems learned from data proves challenging.\n\n### Main Contribution\nThe primary contribution of this paper is the introduction of a new methodology called Neural Contractive Dynamical Systems (NCDS), which can guarantee contractive stability for the dynamics it learns, in both Euclidean and SO(3) manifolds.\nThis makes NCDS a highly adaptable learning architecture offering contractive stability guarantees and obstacle avoidance capabilities. The empirical results show that their approach encodes desired dynamics more accurately compared to existing state-of-the-art methods while providing stronger stability guarantees. Through NCDS, the authors aim to bridge the gap between learning robot dynamics from demonstrations and ensuring stability, which has been a notable challenge due to the extrapolating behavior of neural network models.\n\n### Methodology\nThis methodology is designed to learn stable dynamical systems by constructing negative definite Jacobian from the output of neural network to ensure contraction, thus leading to global stability. The authors extend this method to address high-dimensional dynamical systems by developing a variant of the variational autoencoder using flow-based diffeomorphisms, which learns dynamics in a low-dimensional latent space while maintaining contractive stability post-decoding. They further extend their methodology to include contractive systems on the Lie group of rotations, catering to full-pose end-effector dynamic motions. The method can also be incorporated with a matrix-modulation technique to enable obstacle avoidance.\n\n### Experiments\nThe paper evaluates the effectiveness and scalability of Neural Contractive Dynamical Systems (NCDS) through synthetic and real-world tasks. Initial tests on 2D trajectories from the LASA dataset demonstrated NCDS's capability in capturing and replicating underlying dynamics, even in regions not covered by original data. Compared to baseline methods like Euclideanizing flow, Imitation flow, and SEDS, NCDS was the only method showing contractive behavior indicative of stability. When scaled to higher-dimensional data (LASA-4D and LASA-8D datasets), NCDS maintained a good performance, contrary to the deteriorating performance of baseline methods.\n\nFurthermore, the obstacle avoidance capability of NCDS was showcased on the LASA dataset, with successful generation of safe trajectories around obstacles. Real-world robot experiments on a 7-DoF Franka-Emika robotic manipulator underlined NCDS's effectiveness in reproducing demonstrated dynamics and adapting to physical perturbations. The experiments collectively underline NCDS's potential in managing various aspects of robotic motion learning, ensuring stability, and navigating obstacles, crucial for advancing real-world robotic applications."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "### Originality and Significance\n\nThis is the first work to my knowledge that endows neural network based dynamics modeling with guaranteed contractive stability, which is, as the authors point out, important to robotics as people are trying to take advantage of the modeling capacity of neural networks. The conservative and non-diverging extrapolation that comes with the contractiveness enabled by this work could also benefit neural modeling of other dynamical systems apart from robotics, as the neural ordinary differential equations are known to have difficulty modeling dynamical systems when data is not enough to cover the state space, as the unregularized extrapolation of neural networks could easily lead to numerical instability during integration. Therefore this work is not only innovative but also has potentially significant impact to the community.\n\n### Quality\n\nThe work derives the method from contraction theory and designed practical algorithms for data-driven robotics tasks, and compared with several state of the art baselines.\n\n### Clarity\n\nThe writing of the paper is great. It excellently motivates the work and explains the method and results well with proper details."
                },
                "weaknesses": {
                    "value": "1. Some implementation details are missing and the codes are not available. See more in the Questions section.\n\n2. Limitation of baseline method choices: The baselines compared to are all focused on asymptotic stability guarantees. While these are the most relevant methods for comparison, it would be interesting to see how imitation learning methods without any stability guarantee works on the tasks, to demonstrate the necessity of stability guarantee."
                },
                "questions": {
                    "value": "1. Implementation details\n   1. Is training end-to-end? What are the loss functions used for training?\n   2. How are the sequential data used for training? Is each trajectory used as a whole as one data point, or cut into segments as multiple data points?\n   2. Second-order?\n\n1. Multi-modality"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5107/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5107/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5107/Reviewer_2Vh1"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5107/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698553028099,
            "cdate": 1698553028099,
            "tmdate": 1699636502673,
            "mdate": 1699636502673,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gVd9ANP1kh",
                "forum": "iAYIRHOYy8",
                "replyto": "OizrLboDSf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5107/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5107/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We are grateful to the reviewer for their interest in our work (which was excellently summarized in the review) and for valuable feedback.\n\n- *\u201cIs training end-to-end? What are the loss functions used for training?\u201d*\n    - **Short answer:** Training **NCDS is end-to-end**, but the **latent NCDS is not fully end-to-end**.\n    - **Detailed answer:** Training NCDS is end-to-end, but the latent NCDS is not fully end-to-end. In the latter case, we first train the VAE (end-to-end), and then train the latent NCDS using the encoded data. In more detail, the VAE is trained using the standard evidence lower bound (ELBO):\n        \n        $$\\\\mathcal{L} _{ELBO} = \\\\mathbb{E} _{q _{\\\\mathbf{\\\\xi }}(\\\\mathbf{z}|\\\\mathbf{x})}\\\\left[\\\\log(p _{\\\\mathbf{\\\\phi}}(\\\\mathbf{x}|\\\\mathbf{z}))\\\\right] - \\\\mathrm{KL}\\\\left(q _{\\\\mathbf{\\\\xi }}(\\\\mathbf{z}|\\\\mathbf{x})||p(\\\\mathbf{z})\\\\right) , $$\n        \n        where $\\mathrm{KL}$ denotes the Kullback-Leibler divergence.\n        \n        Thereafter, the Jacobian network is trained according to the loss function $\\mathcal{L}_{\\text{Jac}}$, which measures the distance between the observed and approximated next state:\n        \n        $$ \\\\mathcal{L} _{\\\\text{Jac}} = \\\\|\\\\mathbf{z} _{t+1} - (\\\\mathbf{z} _{t} + \\\\hat{\\\\dot{\\\\mathbf{z}}} _t)\\\\|^2,$$\n        \n        where $\\\\mathbf{z} _{t}$*, $\\\\mathbf{z} _{t+1}$*, and $\\\\hat{\\\\dot{\\\\mathbf{z}}}_t$ represent the current and next observed latent states, along with the calculated latent velocity.\n        \n    - **Paper changes:** we have updated the paper and added the loss functions and the algorithms in the Appendix A.4: Implementation details.\n\n---\n\n**Algorithm 1**: *Neural Contractive Dynamical Systems (NCDS): Training in task space*\n\n---\n\n**Data**: Demonstrations: $\\\\tau _n = \\\\left \\\\{(\\\\mathbf{x} _t, \\\\mathbf{R} _t) \\\\right \\\\},  n \\\\in [1, N], t \\\\in [1, T _n]$\n\n**Result**: Learned contractive dynamical system\n\n1. $\\\\mathbf{r} _{n,t} = \\\\operatorname{Log}(\\\\mathbf{R} _{n,t})$; // Obtaining skew-symmetric coefficients\n2. $\\\\mathbf{p} _{n, t} = [\\\\mathbf{x} _{n, t}, \\\\mathbf{r} _{n, t}]$; *//* Create new state vector\n3. $ \\\\operatorname{argmin}_\\\\xi \\\\mathcal{L} _{\\\\text{ELBO}}(\\\\xi^*; \\mathbf{p} _{n,t})$; // Train the VAE\n4. $\\\\mathbf{z} _{n,t} = \\\\mu _ {\\\\mathbf{\\xi}} ^{\\\\sim 1} (\\\\mathbf{p} _{n,t})$; // Encode all states using the trained VAE\n5. $\\\\dot{\\mathbf{z}} _{n,t} = \\\\frac{\\\\mathbf{z} _{n,t+1} - \\\\mathbf{z} _{n,t}}{\\Delta t}$; // Compute latent finite differences\n6. $ \\\\operatorname{argmin}_\\\\theta \\mathcal{L} _{\\\\text{Jac}}(\\\\theta^*; \\\\mathbf{p} _{n,t})$; // Train the Jacobian network\n\n---\n\n**Algorithm 2**: Algorithm Neural Contractive Dynamical Systems (NCDS): Robot Control Scheme\n\n---\n\n**Data:** Current state of the robot end-effector at time $t$: $[\\\\mathbf{x} _t, \\\\mathbf{R} _t]$\n\n**Result**: Velocity of the end-effector at current time step $\\\\dot{\\\\mathbf{x}} _t$\n\n1. $\\\\mathbf{r} _{t} = \\\\operatorname{Log}(\\\\mathbf{R} _{t})$; // Obtaining skew-symmetric coefficients\n2. $\\\\mathbf{p} _{t} = [\\\\mathbf{x} _{t}, \\\\mathbf{r} _{t}]$; // Create new state vector\n3. $\\\\mathbf{z} _t = \\\\mu _{\\\\mathbf{\\\\xi}} ^{\\\\sim 1} (\\\\mathbf{p} _t)$; // Compute the latent state\n4. $\\\\hat{\\\\dot{\\\\mathbf{z}}} _t = f(\\\\mathbf{z} _t)$; // Compute the latent velocity\n5. $J_{\\\\mu _{\\\\mathbf{\\\\xi}}}(\\\\mathbf{z} _t) = \\\\frac{\\\\partial\\\\mu _{\\\\mathbf{\\\\xi}}}{\\\\partial \\\\mathbf{z} _t}$; // Compute the Jacobian of the decoder\n6. $\\\\dot{\\\\mathbf{x}} _t = J _{\\\\mu _{\\\\mathbf{\\\\xi}}}(\\\\mathbf{z} _t)\\\\hat{\\\\dot{\\\\mathbf{z}}} _t$; // Compute input space velocity\n\n---\n\n- \u201cHow are the sequential data used for training? Is each trajectory used as a whole as one data point, or cut into segments as multiple data points? Second-order?\u201d\n\n   - During training, data points from all the demonstrations undergo shuffling and are then fed into the network as a batch containing $B$ data points.\n\n- *\u201cMulti-modality\u201d*\n    - We do not quite understand the reviewer\u2019s question here. The term \"multimodality\" in the context of a dynamical system could mean different things. We are happy to do our best to answer the raised question but request a bit more detail.\n- \u201c*Limitation of baseline method choices: The baselines compared to are all focused on asymptotic stability guarantees. While these are the most relevant methods for comparison, it would be interesting to see how imitation learning methods without any stability guarantee works on the tasks, to demonstrate the necessity of stability guarantee.\u201d*\n    - This is a fair point. We opted for the chosen baselines as our work was really driven by a desire to provide stability. We will try to conduct these additional baseline experiments within the discussion period and report back with results later."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5107/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700165400438,
                "cdate": 1700165400438,
                "tmdate": 1700165400438,
                "mdate": 1700165400438,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Hfi8HbndlF",
                "forum": "iAYIRHOYy8",
                "replyto": "gVd9ANP1kh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5107/Reviewer_2Vh1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5107/Reviewer_2Vh1"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for the detailed explanation and response to the questions.\n\nI am also very sorry for submitting an unfinished draft for the Questions section due to a browser glitch, which caused confusion. \n\nIn the Question 1.3, I intended to confirm whether training the Jacobian network involves second-order derivatives, as the forward propagation includes calculating Jacobian as first-order derivatives.\n\nIn the Question 2, the full question is as follows:\nDoes contraction conflict with demonstrations that have a multimodal distribution? By \"multimodal\", I mean there exist multiple \"modes\" (potentially due to various styles or goals of the demonstrators) in the distribution of trajectories. If contractive property means prompt correction of any deviation from a template trajectory, would if ever allow for the generation of more than one modes of trajectories, as one mode of trajectories would indeed \"deviate from\" another mode of trajectories? For example, in the second subplot from left in the top row in Figure 5, demonstrations seem to have three branches, joining at the center point, while the generated trajectories seem to only replicate one branch when starting from the center point.\n\nOther than the incomplete questions mentioned above, my questions are properly addressed by the authors."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5107/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700463376488,
                "cdate": 1700463376488,
                "tmdate": 1700463376488,
                "mdate": 1700463376488,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OlnVuSwS3g",
                "forum": "iAYIRHOYy8",
                "replyto": "wfgVfhNrDz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5107/Reviewer_2Vh1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5107/Reviewer_2Vh1"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response.\n\nFor Question 1.3, I meant that if the forward propagation involves derivatives, the back propagation through the computation graph would involve second order derivatives. This would be automatically done by the deep learning framework that is used, so it's just a minor clarifying question.\n\nI'm glad to see that NCDS is capable of handling multimodal trajectory distributions (with varied starting point, but not from the same starting point, which is a limitation for all deterministic approaches), and the comparison with unconstrained methods is convincing.\n\nI will keep my rating of 8 unchanged."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5107/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700718252077,
                "cdate": 1700718252077,
                "tmdate": 1700718252077,
                "mdate": 1700718252077,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dEydy6FGQB",
            "forum": "iAYIRHOYy8",
            "replyto": "iAYIRHOYy8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5107/Reviewer_fKvz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5107/Reviewer_fKvz"
            ],
            "content": {
                "summary": {
                    "value": "This work addresses the challenging problem of learning contractive dynamical systems using neural networks. The key idea is to utilize the fact that contractivity is invariant under diffeomorphisms. This motivates the use of autoencoders to learn contractive dynamics in a low-dimensional latent space. The proposed method includes a VAE architecture which naturally enforces the contractivity of the dynamics in the latent space. These results are then applied to several interesting applications, including obstacle avoidance, and learning dynamics in $SO(3)$."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper has several strengths.\n\n* The idea of learning a low dimensional latent space embedding of the dynamics is interesting and novel, with a variety of interesting potential applications\n* The construction of the VAE ensures that the contractivity is invariant to the mapping to the latent space (and vice versa)\n* The proposed framework is extended to a variety of scenarios, including dynamics over Lie groups ($SO(3)$) and obstacle avoidance\n* The paper itself is, generally speaking, well written."
                },
                "weaknesses": {
                    "value": "There are a few weaknesses.\n\n* It would be nice to have the invariance of the contractivity stated formally. \n* In the discussion section (page 9), it is mentioned that the choice of integration scheme can siginificantly affect the behaviour of the learned model. This requires further discussion. For instance, how significantly does the computation time affect the performance of the model? Is there a choice of integrator that doesn't require adaptive step-sizes (perhaps a symplectic integrator)?\n* It would be helpful if the training algorithm were stated explicitly in Section 3."
                },
                "questions": {
                    "value": "* This work focuses on learning contractive dynamics. However, suppose the system $\\dot{x} = f(x)$ is *not* contractive. What can you say about the solution to the optimization problem (i.e. the feasibility of eqns (4) and (5)) in this scenario?\n* In a similar vein, it seems like this work could easily be extended to (neural) controller synthesis in the latent space. Can the authors comment on this?\n* Can the authors comment on the practical effectiveness of the model in greater detail? As mentioned in the discussion section (p9), the cost of numerical integration can be extensive. Have the authors come across examples where this has been an impediment to performance?\n* Could the authors also address the concerns raised in the weaknesses section?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5107/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5107/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5107/Reviewer_fKvz"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5107/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698740580139,
            "cdate": 1698740580139,
            "tmdate": 1700325258804,
            "mdate": 1700325258804,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qXCseC260C",
                "forum": "iAYIRHOYy8",
                "replyto": "dEydy6FGQB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5107/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5107/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate the reviewer for taking the time to provide thoughtful and constructive feedback.\n- **Reviewer\u2019s comment:** \u201c*This work focuses on learning contractive dynamics. However, suppose the system $\\dot{x} = f(x)$ is not contractive. What can you say about the solution to the optimization problem (i.e. the feasibility of eqns (4) and (5)) in this scenario?\u201d*\n    - **Short answer:** Good question. When the system dynamics $\\dot{x}=f(x)$ are not contractive, **the dynamics described by equations may integrate to arbitrary (potentially unstable) dynamics**. The system will most likely follow the trends of the training data when evaluated near the data, and then exhibit arbitrary behavior everywhere else (thereby violating stability guarantees)**.**\n    - **Detailed answer:** If the system dynamics $\\dot{x}=f(x)$ are not contractive, i.e. the Jacobian of the function $f$ is not uniformly negative definite, the trajectory described by equations (4) and (5) may not exhibit stable behavior. It is worth noting that **these equations do not depend on the Jacobian being uniformly negative definite.** The optimization of equations 4 and 5 without considering the contraction formulation in equation 3 means the system might be only able to reconstruct the trajectories if started on the data support regions (not guaranteed) but the global behavior of the system outside of the data support will not be stable. **In Fig. 13 in the updated paper, you can see the differences between contractive and unconstrained dynamical systems.** As the figures show, when the resulting vector field is **not contractive, the reproduced motion follows the trend of the data only on the demonstrations region and not on the outside regions**. Overall, this system does not show any stable behavior.\n    - **Revision**: We have added a new subsection in Appendix A.6.3: Unconstrained Dynamical System discussing these results.\n---\n- **Reviewer\u2019s comment:** \u201c*In a similar vein, it seems like this work could easily be extended to (neural) controller synthesis in the latent space. Can the authors comment on this?\u201d*\n    - **Our interpretation** of this comment is the potential extension of the current work to **incorporate a controlled formulation of the dynamical system,** i.e.  $f(x,u)$, where $u$ is the control input**.** If this is correct, then **it is plausible to envision formulating the system within the latent space.** The latent space provides a compact yet semantically rich representation of the system dynamics that captures essential features of the underlying system in a more abstract and expressive manner. Extending the model to **integrate a neural controller within this latent space framework is a neat idea for control strategies at a higher level of abstraction.** The method could then explore the synergy between the latent space representation and the dynamics of the controlled system. This is a very interesting idea, but not one we have explored\n---\n- **Reviewer\u2019s comment:** \u201c*Can the authors comment on the practical effectiveness of the model in greater detail? As mentioned in the discussion section (p9), the cost of numerical integration can be extensive. Have the authors come across examples where this has been an impediment to performance?*\u201c\n    - **Short answer:** No such examples have been encountered.\n    - **Detailed answer:**  numerical integration is **one of four steps affecting performance**, where the others are: (1) encoding the current state, (2) computing the Jacobian, and (3) calculating input space velocity. Each iteration in our robot control scheme (Algorithm 2) takes up to 10 milliseconds (100\ufeff Hz). Current experiments involve 7 DOF robots, encoding either pose (6-dimensional) or joint configuration  (7-dimensional), with a 2-dimensional latent space. **With respect to the cost of the numerical integration, then this depends mostly on the latent dimension and less so on the input dimension** (Table 3 in the appendix). Finally, it is worth noting that **our Python implementation is optimized for correctness rather than performance**, so there is substantial room for performance improvements."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5107/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700163569564,
                "cdate": 1700163569564,
                "tmdate": 1700164839194,
                "mdate": 1700164839194,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eyNVa26nEW",
                "forum": "iAYIRHOYy8",
                "replyto": "dEydy6FGQB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5107/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5107/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "- **Reviewer\u2019s comment:** \u201c*It would be nice to have the invariance of the contractivity stated formally.\u201d*\n    - **Short answer:** We agree that the paper was not precise enough here.\n    - **Detailed answer**: Let $\\mu: \\mathbb{R}^d \\rightarrow \\mathbb{R}^D$ be the injective decoder mapping from the low-dimensional latent space to the high-dimensional observation space. Further, let $z_t$ denote a contractive dynamical system in the latent space. Geometrically, the decoder spans a $d$-dimensional manifold embedded in $\\mathbb{R}^D$ defined as $\\mathcal{M} = f(\\mathbb{R}^d)$. By design, the **decoder is a diffeomorphism between $\\mathbb{R}^d$ and $\\mathcal{M}$**; this construction is extensively discussed by Brehmer & Cranmer [1], which we build upon. **By Theorem 1 in our paper, the above implies that $f(z_t)$ is contractive.** The above further implies that the dynamical system $f(z_t)$ is along a low-dimensional manifold $\\mathcal{M} \\subset \\mathbb{R}^D$, i.e. it does not cover all of $\\mathbb{R}^D$. Such constructions are common in latent variable models. The control loop assumes that the initial robot configuration is in $\\mathcal{M}$ such that it can be written $f(z_0)$**.** **Following the dynamical system moves the robot along the manifold, and the resulting motion follows a contractive system.** If the assumption is not satisfied, such that the initial configuration $x_0 \\not\\in \\mathcal{M}$, then we have to rely on the encoder to approximate a projection onto $\\mathcal{M}$, i.e. produce $z_0 = \\mu^{\\sim 1}(x_0)$. Note that the movement from $x_0$ to $f(z_0)$ need not be contractive, such that the distance to the nominal trajectory may increase. This is, however, a finite-time motion, after which the system is contractive.\n    - **Revisions:** In Section 3.2, we have refined and formalized the correlation between injection and contraction.\n- **Reviewer\u2019s comment:** \u201c*In the discussion section (page 9), it is mentioned that the choice of integration scheme can siginificantly affect the behaviour of the learned model. This requires further discussion. For instance,\u201d*\n    - \u201c*How significantly does the computation time affect the performance of the model?*\u201d\n        - **Short answer:** Our findings show that as long as we keep the latent space low dimensional, the computation of the integration does not affect the overall performance of the system.\n        - **Detailed answer**: We have extensively investigated this question in Section A.6.2: *Dimensionality and execution time*, and our findings show that **as long as we keep the latent space low dimensional, the computation of the integration does not affect the overall performance** of the system. It is important to acknowledge that while maintaining a low-dimensional latent space is favorable for real-time applications, **the input space dimensionality introduces computational challenges.** Specifically, computing the Jacobian of the injective generator at each integration step becomes more resource-intensive. Furthermore, to answer this question in a more extreme scenario **we have conducted an experiment on higher-dimensional ($44D$) human motion dataset.** The new experiment showcases the performance of injective generators in encoding complex, high-dimensional data into a low-dimensional latent space ($2D$). Certainly, the considerable dimensionality of the data space inevitably impacts the runtime, with each iteration of the control loop in Algorithm 2 requiring 121 milliseconds.\n        - **Revision**: We have added a new subsection in the Appendix A.6.2 titled \u201cLearning human motion\u201d. We also updated Table 2 in the appendix with the new results from human motion experiments execution time."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5107/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700163710411,
                "cdate": 1700163710411,
                "tmdate": 1700163710411,
                "mdate": 1700163710411,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "isCluPGVCB",
                "forum": "iAYIRHOYy8",
                "replyto": "dEydy6FGQB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5107/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5107/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "- **Reviewer\u2019s comment:** \u201c*Is there a choice of integrator that doesn't require adaptive step-sizes (perhaps a symplectic integrator)?\u201d*\n    - **Detailed answer**: Given our limited experience with symplectic integrators, specifically designed for Hamiltonian systems, we find them less applicable to our current ODE-based system. Our research indicates that symplectic integrators excel in preserving certain properties, such as energy conservation, which are integral to Hamiltonian dynamics. Since our system does not exhibit these characteristics, opting for a symplectic integrator might introduce unnecessary complexity. On the contrary, **the Runge-Kutta 4th order (RK4) method,** **while not inherently adaptive in step size, is a fixed-step-size numerical integration technique widely employed for solving ordinary differential equations (ODEs)**. This method strikes a **balance between accuracy and computational efficiency**, particularly in ODE systems with straightforward dynamics where **stiffness is not a prominent concern.** As highlighted in Figure 13 of the paper, RK4 exhibits a certain level of accuracy, although it is not as precise as adaptive step size methods such as the Dopri5 method. This observation underscores the importance of considering the specific characteristics and requirements of our ODE system when selecting an integration method. I**n cases where our system dynamics are uncomplicated and adaptive step sizes are not imperative, RK4 remains a viable and pragmatic choice.**\n- **Reviewer\u2019s comment:** \u201c*It would be helpful if the training algorithm were stated explicitly in Section 3.\u201d*\n    - **Short answer:** We agree that a training algorithm should be added to the paper.\n    - **Detailed answer:** Here, we provide detailed steps of the training and robot control schemes for NCDS. The sequential steps for training the Variational Autoencoder (VAE) and Jacobian network are listed in Algorithm 1. Simultaneously, the procedural steps for employing NCDS to control a robot are shown in Algorithm 2.\n    - **Revision**: A new subsection titled \"Algorithms\" has been included in the Appendix A.4, containing the respective algorithms for reference.\n---\n\n**Algorithm 1**: *Neural Contractive Dynamical Systems (NCDS): Training in task space*\n\n---\n\n**Data**: Demonstrations: $\\\\tau _n = \\\\left \\\\{(\\\\mathbf{x} _t, \\\\mathbf{R} _t) \\\\right \\\\},  n \\\\in [1, N], t \\\\in [1, T _n]$\n\n**Result**: Learned contractive dynamical system\n\n1. $\\\\mathbf{r} _{n,t} = \\\\operatorname{Log}(\\\\mathbf{R} _{n,t})$; // Obtaining skew-symmetric coefficients\n2. $\\\\mathbf{p} _{n, t} = [\\\\mathbf{x} _{n, t}, \\\\mathbf{r} _{n, t}]$; *//* Create new state vector\n3. $ \\\\operatorname{argmin}_\\\\xi \\\\mathcal{L} _{\\\\text{ELBO}}(\\\\xi^*; \\mathbf{p} _{n,t})$; // Train the VAE \n4. $\\\\mathbf{z} _{n,t} = \\\\mu _ {\\\\mathbf{\\xi}} ^{\\\\sim 1} (\\\\mathbf{p} _{n,t})$; // Encode all states using the trained VAE\n5. $\\\\dot{\\mathbf{z}} _{n,t} = \\\\frac{\\\\mathbf{z} _{n,t+1} - \\\\mathbf{z} _{n,t}}{\\Delta t}$; // Compute latent finite differences\n6. $ \\\\operatorname{argmin}_\\\\theta \\mathcal{L} _{\\\\text{Jac}}(\\\\theta^*; \\\\mathbf{p} _{n,t})$; // Train the Jacobian network\n \n---\n\n**Algorithm 2**: Algorithm Neural Contractive Dynamical Systems (NCDS): Robot Control Scheme\n\n---\n\n**Data:** Current state of the robot end-effector at time $t$: $[\\\\mathbf{x} _t, \\\\mathbf{R} _t]$\n\n**Result**: Velocity of the end-effector at current time step $\\\\dot{\\\\mathbf{x}} _t$\n\n1. $\\\\mathbf{r} _{t} = \\\\operatorname{Log}(\\\\mathbf{R} _{t})$; // Obtaining skew-symmetric coefficients\n2. $\\\\mathbf{p} _{t} = [\\\\mathbf{x} _{t}, \\\\mathbf{r} _{t}]$; // Create new state vector\n3.  $\\\\mathbf{z} _t = \\\\mu _{\\\\mathbf{\\\\xi}} ^{\\\\sim 1} (\\\\mathbf{p} _t)$;  // Compute the latent state\n4.  $\\\\hat{\\\\dot{\\\\mathbf{z}}} _t = f(\\\\mathbf{z}  _t)$;  // Compute the latent velocity\n5.  $J_{\\\\mu _{\\\\mathbf{\\\\xi}}}(\\\\mathbf{z} _t) = \\\\frac{\\\\partial\\\\mu _{\\\\mathbf{\\\\xi}}}{\\\\partial \\\\mathbf{z} _t}$;  // Compute the Jacobian of the decoder\n6. $\\\\dot{\\\\mathbf{x}} _t = J _{\\\\mu _{\\\\mathbf{\\\\xi}}}(\\\\mathbf{z}  _t)\\\\hat{\\\\dot{\\\\mathbf{z}}} _t$;  // Compute input space velocity"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5107/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700164767944,
                "cdate": 1700164767944,
                "tmdate": 1700164767944,
                "mdate": 1700164767944,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aJVInQtZ1G",
                "forum": "iAYIRHOYy8",
                "replyto": "isCluPGVCB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5107/Reviewer_fKvz"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5107/Reviewer_fKvz"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "I'd like to thank the authors for their detailed responses. You (generally) satisfactorily addressed my concerns. Based on the responses I received, as well as reading the responses to the other reviewers, I am happy to raise my score."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5107/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700325231717,
                "cdate": 1700325231717,
                "tmdate": 1700325231717,
                "mdate": 1700325231717,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "KoGRKP6tjp",
            "forum": "iAYIRHOYy8",
            "replyto": "iAYIRHOYy8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5107/Reviewer_4cHX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5107/Reviewer_4cHX"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a method to learn contractive dynamical systems. One method of constructing contractive systems has been to apply a diffeomorphism to provide a change of coordinates to a known contractive system. This paper proposes to use a VAE instead of a diffeomorphism, and constructs the decoder such that it is injective. As such, by enforcing that the latent dynamics is contractive, the dynamics in the data space will also be contractive. The paper also extends the method to Lie groups to account for end-effector orientation.\n\nThe paper is generally well-written, the method appears sound, and the motivations are clear. My main concerns are as follows:\n\n1. Theorem 1 states that the contractivity is perserved under a diffeomorphism. There is a lack of analysis and formal guarentees around the assumption that an injective function acting on a lower dimensional contractive system produces a higher-D contractive system. Just as a thought experiment, what happens to coordinate points in the higher-D data space where there does not exist a coordinate in the lower-D latent space? \n\n2. How is the collision-avoidance on the entire manipulator handled? The learned dynamical system seems to model the end-effector pose, but collision-avoidance should be handled across the body of the robot. One approach to handle this is to pull the dynamical system to the C-space of the robot and define body-points for the collision-avoidance, as done in (Zhi 2022, L4DC). This is a relevant reference and should be reviewed, as it also takes a diffeomorphic learning approach.\n\nOverall, I believe this is a neat idea, but more clarity around the theoretical insights is needed. I'm happy to raise my score when my concerns have been address."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "See above."
                },
                "weaknesses": {
                    "value": "See above."
                },
                "questions": {
                    "value": "See above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5107/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699222702477,
            "cdate": 1699222702477,
            "tmdate": 1699636502467,
            "mdate": 1699636502467,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Z877AoicCG",
                "forum": "iAYIRHOYy8",
                "replyto": "KoGRKP6tjp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5107/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5107/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate the reviewer's engagement with our work and the valuable feedback provided.\n\n- \"Theorem 1 states that the contractivity is perserved under a diffeomorphism. There is a lack of analysis and formal guarentees around the assumption that an injective function acting on a lower dimensional contractive system produces a higher-D contractive system.\u201c\n    - **Short answer:** We agree that the paper was not precise enough here. Shortly put, **the injective decoder is a diffeomorphism between the latent space and the image of the decoder**, which is sufficient to ensure that the resulting system is contractive. We detail this argument below.\n    - **Detailed answer**: Let $\\mu: \\mathbb{R}^d \\rightarrow \\mathbb{R}^D$ be the injective decoder mapping from the low-dimensional latent space to the high-dimensional observation space. Further, let $z_t$ denote a contractive dynamical system in the latent space. Geometrically, the decoder spans a $d$-dimensional manifold embedded in $\\mathbb{R}^D$ defined as $\\mathcal{M} = f(\\mathbb{R}^d)$. By design, the **decoder is a diffeomorphism between $\\mathbb{R}^d$ and $\\mathcal{M}$**; this construction is extensively discussed by Brehmer & Cranmer [1], which we build upon. **By Theorem 1 in our paper, the above implies that $f(z_t)$ is contractive.** The above further implies that the dynamical system $f(z_t)$ is along a low-dimensional manifold $\\mathcal{M} \\subset \\mathbb{R}^D$, i.e. it does not cover all of $\\mathbb{R}^D$. Such constructions are common in latent variable models. The control loop assumes that the initial robot configuration is in $\\mathcal{M}$ such that it can be written $f(z_0)$**.** **Following the dynamical system moves the robot along the manifold, and the resulting motion follows a contractive system.** If the assumption is not satisfied, such that the initial configuration $x_0 \\not\\in \\mathcal{M}$, then we have to rely on the encoder to approximate a projection onto $\\mathcal{M}$, i.e. produce $z_0 = \\mu^{\\sim 1}(x_0)$. Note that the movement from $x_0$ to $f(z_0)$ need not be contractive, such that the distance to the nominal trajectory may increase. This is, however, a finite-time motion, after which the system is contractive.\n    - **Revision**: In Section 3.2, we have clarified and formalized the correlation between injection and contraction.\n---\n- \u201cJust as a thought experiment, what happens to coordinate points in the higher-D data space where there does not exist a coordinate in the lower-D latent space?\u201d\n    - This matches the last remarks of the detailed answer above. The model provides a contractive dynamical system on top of a $d$-dimensional manifold embedded in $\\mathbb{R}^D$. **When the initial robot configuration is not on this manifold, we first have to project it there.** We approximate this with our encoder $\\mu^{\\sim 1}$ and note that the finite-time motion from the initial configuration to the one on the manifold need not follow a contraction. **The robot video in the supplementary material includes an example of this.** **Here, the operator often pushes the robot away from the manifold spanned by the decoder after which the robot first moves back on the manifold** and thereafter follows a stable behavior.\n---"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5107/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700163192274,
                "cdate": 1700163192274,
                "tmdate": 1700163192274,
                "mdate": 1700163192274,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oB9ZQvAuZ5",
                "forum": "iAYIRHOYy8",
                "replyto": "KoGRKP6tjp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5107/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5107/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "- \u201cHow is the collision-avoidance on the entire manipulator handled? The learned dynamical system seems to model the end-effector pose, but collision-avoidance should be handled across the body of the robot. One approach to handle this is to pull the dynamical system to the C-space of the robot and define body-points for the collision-avoidance, as done in (Zhi 2022, L4DC). This is a relevant reference and should be reviewed, as it also takes a diffeomorphic learning approach.\u201d\n    - **Short answer:** Thank you for bringing this to our attention; we indeed overlooked this aspect in the paper.\n    - **Detailed answer:** The paper mentioned by the reviewer [2] is indeed interesting: it ensures collision avoidance in the C-space by leveraging Diffemorphic Transforms (DTs). Specifically, it employs smooth obstacle gradients obtained from continuous maps based on occupancy information to construct a vector field. Then, it uses the natural gradient of this vector field to smoothly warp around the obstacle when approaching its boundary. This is similar to obstacle avoidance approach proposed in Beik-Mohammadi et al. [3] where the C-space obstacle avoidance is accomplished by employing a pull back metric derived from the VAE decoder. This approach formulates the C-Space obstacle avoidance in the latent space of the VAE instead of the task space. To clarify, as mentioned in the paper **our current modulation matrix is formulated in the input space of the VAE, therefore, the ability to perform multiple limb obstacle avoidance can be achieved when using joint space demonstration trajectories.** However, to handle obstacle avoidance across the robot body we need to transform the joint space information into the task space and define body-points for multiple-limb collision-avoidance. Of course, **this will be computationally expensive**. Certainly, adopting the obstacle avoidance concepts introduced in [2] or [3] **eliminate the necessity for a meticulous computation of the modulation matrix, particularly for multiple limb obstacle avoidance**. Moreover, these methods **allow for the transition of the obstacle avoidance task into the latent space, mitigating any impact on real-time performance due to dimensionality of the input space.** While these approaches show potential in refining our obstacle avoidance capabilities, it is important to note that obstacle avoidance is not the central focus of our work. Nevertheless, they play a valuable role in the broader context of exploring potential enhancements in future works.\n    - **Revisions**: We have clarified C-space obstacle avoidance in the Section 3.4.\n---\n- **References**:\n    - [1] *Johann Brehmer and Kyle Cranmer. Flows for simultaneous manifold learning and density estimation. In Neural Information Processing Systems (NeurIPS, pp. 442\u2013453, 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/051928341be67dcba03f0e04104d9047-Paper.pdf.*\n    - [2] *Weiming Zhi, Tin Lai, Lionel Ott, Fabio Ramos . Diffeomorphic Transforms for Generalised Imitation Learning. Proceedings of The 4th Annual Learning for Dynamics and Control Conference, in Proceedings of Machine Learning Research. 168:508-519 Available from https://proceedings.mlr.press/v168/zhi22a.html.*\n    - [3] Beik-Mohammadi Hadi, Hauberg Soren, Arvanitidis Georgios, Neumann Gerhard, Rozo Leonel. Reactive motion generation on learned Riemannian manifolds. The International Journal of Robotics Research. 2023;42(10):729-754. doi:10.1177/02783649231193046. https://journals.sagepub.com/doi/10.1177/02783649231193046"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5107/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700163347277,
                "cdate": 1700163347277,
                "tmdate": 1700163347277,
                "mdate": 1700163347277,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]