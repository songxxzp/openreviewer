[
    {
        "title": "SAFLEX: Self-Adaptive Augmentation via Feature Label Extrapolation"
    },
    {
        "review": {
            "id": "obs1F2TGgJ",
            "forum": "qL6brrBDk2",
            "replyto": "qL6brrBDk2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3847/Reviewer_NKYa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3847/Reviewer_NKYa"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the training of high-performance neural networks on small amounts of training data by optimizing data augmentation. Existing methods in this field optimize the transformation itself in the feature space, which limits the available data augmentation transformations and has high computational complexity. This paper differs from these existing approaches by optimizing the importance weights of the input features and the soft labels to be assigned to the augmented data, thereby meta-learning the data augmentation available in the existing data augmentation pipeline. Focusing on features and labels in data augmentation has not received much attention, and the idea is novel. Furthermore, the paper proposes a method to approximate the bi-level optimization of meta-learning with validation data to a single-level optimization by borrowing the idea of gradient matching and developing an efficient algorithm. Experiments verify the effectiveness of the proposed method on various datasets, tasks, and combinations of data augmentation methods. On the other hand, the paper does not provide an evaluation of the first-order approximation or the computation cost, and there is room for improvement in this aspect."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ The paper proposes a novel data augmentation optimization strategy that optimizes the importance and labels of input features. The idea is very interesting and original.\n+ The paper proposes a first-order approximation method to efficiently compute meta-learning that requires bi-level optimization.\n+ The paper applies and evaluates the proposed method not only on image datasets but also on table datasets. This evaluation is important in supporting the paper's claim that the method can be applied to any data augmentation pipeline.\n+ The paper provides experimental results on the recently widely used CLIP pre-trained models, effectively demonstrating the impact of the proposed method."
                },
                "weaknesses": {
                    "value": "- Even though the paper proposes a first-order approximation method, it does not provide an evaluation of this method. In other words, the paper should provide a performance comparison with the usual bi-level optimization and a computation cost comparison with other data augmentation strategies such as Fast AutoAugment.\n- The writing of the paper is not necessarily of high quality. For example, Theorem 1 is very difficult to read because it contains multiple claims that make up the entire solution. Theorems and corollaries should be split for each claim, or if the propositions are ambiguous, they should be replaced with detailed explanations for each component, rather than in the form of a theorem. In fact, the proofs provided by the Appendix are almost obvious and make little theoretical contribution."
                },
                "questions": {
                    "value": "- Do you think the proposed method can be applied to consistency-based semi-supervised learning with data augmentation, e.g., FixMatch [a]? The study of estimating the importance of samples and labels is well studied in the field of semi-supervised learning rather than in the field of data augmentation (e.g., FreeMatch [b]). If it can be shown that the scheme of the proposed method can be implemented in semi-supervised learning, the impact of this paper on the community will be even greater.\n\n[a] Sohn, Kihyuk, et al. \"Fixmatch: Simplifying semi-supervised learning with consistency and confidence.\" NeurIPS 2020.\n\n[b] Wang, Yidong, et al. \"Freematch: Self-adaptive thresholding for semi-supervised learning.\" ICLR 2023."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3847/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698386087732,
            "cdate": 1698386087732,
            "tmdate": 1699636342846,
            "mdate": 1699636342846,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dxQjs6tfpI",
                "forum": "qL6brrBDk2",
                "replyto": "obs1F2TGgJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3847/Reviewer_NKYa"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3847/Reviewer_NKYa"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors,\n\nDo you have rebuttals to my concerns? If there is no discussion, I will be forced to lower my rating and confidence scores since my concerns will continue to remain. I look forward to your rebuttal.\n\nBest,\n\nReviewer NKYa"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3847/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700622485113,
                "cdate": 1700622485113,
                "tmdate": 1700622485113,
                "mdate": 1700622485113,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "liwom7qyv4",
                "forum": "qL6brrBDk2",
                "replyto": "obs1F2TGgJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3847/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3847/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer NKYa"
                    },
                    "comment": {
                        "value": "We extend our sincere gratitude to reviewer NKYa for their insightful and constructive feedback. We are particularly encouraged by reviewer NKYa's recognition of the novelty of our work, including our innovative approach to optimizing the importance and label of input features, our novel first-order approximation method, our comprehensive evaluation of both tabular and image data, and the relevance of our findings to CLIP pre-trained models.\n\nBelow, we would like to address concerns and answer questions. \n\n---\n\n> **Weakness 1:** Even though the paper proposes a first-order approximation method, it does not provide an evaluation of this method. In other words, the paper should provide a performance comparison with the usual bi-level optimization and a computation cost comparison with other data augmentation strategies such as Fast AutoAugment.\n\n\n**Response to Weakness 1:**\n\n- We appreciate reviewer NKYa's valuable suggestion for an ablation study on the first-order approximation method. We agree that conducting such a study would further solidify our findings.\n\n\n- However, due to space constraints, we prioritized presenting the core message of this paper: rectifying any upstream data augmentation techniques for downstream tasks. To effectively convey this message, we conducted extensive experiments encompassing diverse data types, comprehensive coverage of upstream augmentation techniques, various downstream tasks, and multiple backbone models. As a result, we were unable to include an extensive ablation study on the first-order approximation method.\n\n- It is important to note that there is a significant body of research on first-order approximations of bilevel problems, such as the work by [R1]. Given the extensive nature of this field, we have opted to defer an in-depth exploration of first-order approximation methods to future investigations. This decision allows us to focus on the core contribution of this paper, which is the development of a novel data augmentation framework for downstream tasks. \n    > [R1] Liu, Bo, Mao Ye, Stephen Wright, Peter Stone, and Qiang Liu. \"Bome! bilevel optimization made easy: A simple first-order approach.\" Advances in Neural Information Processing Systems 35 (2022): 17248-17262.\n\n---\n\n> **Weakness 2:** The writing of the paper is not necessarily of high quality. For example, Theorem 1 is very difficult to read because it contains multiple claims that make up the entire solution. Theorems and corollaries should be split for each claim, or if the propositions are ambiguous, they should be replaced with detailed explanations for each component, rather than in the form of a theorem. In fact, the proofs provided by the Appendix are almost obvious and make little theoretical contribution.\n\n**Response to Weakness 2:**\n\nWe express our sincere gratitude to reviewer NKYa for their valuable suggestions on improving the accessibility of our paper. We have carefully considered reviewer NKYa's feedback and have implemented the following changes to enhance the clarity and readability of our manuscript:\n\n- Specifically for Theorem 1, we separate the notation definitions and the final formula for the approximated soft label and sample weight. We have made every component more readable, including the gradient, Jacobian-vector product, and the final formula.\n\n---\n\n> **Question:** Do you think the proposed method can be applied to consistency-based semi-supervised learning with data augmentation, e.g., FixMatch [a]? The study of estimating the importance of samples and labels is well studied in the field of semi-supervised learning rather than in the field of data augmentation (e.g., FreeMatch [b]). If it can be shown that the scheme of the proposed method can be implemented in semi-supervised learning, the impact of this paper on the community will be even greater.\n[a] Sohn, Kihyuk, et al. \"Fixmatch: Simplifying semi-supervised learning with consistency and confidence.\" NeurIPS 2020.\n[b] Wang, Yidong, et al. \"Freematch: Self-adaptive thresholding for semi-supervised learning.\" ICLR 2023.\n\n**Response to Question:** \n\nIndeed, at the end of section 3, we briefly discuss how to apply SAFLEX for contrastive learning, which is indeed the consistency-based self-supervised or semi-supervised learning suggested by reviewer NKYa. Due to space limitations, detailed equations are in Appendix A, equations (12) and (13). \n\nWe have also verified this through experiments. Specifically, we did fine-tune a CLIP model (Radford et al., 2021) on the iWildCam dataset from Wilds (Koh et al., 2021), using a contrastive fine-tuning paradigm called \u201cFinetune Like You Pretrain\u201d (FLYP) (Goyal et al., 2023).\n\n---\n\nAgain, we sincerely thank reviewer NKYa for their valuable service in reviewing our manuscript."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3847/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700633229930,
                "cdate": 1700633229930,
                "tmdate": 1700651063000,
                "mdate": 1700651063000,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UghuXLOl6w",
                "forum": "qL6brrBDk2",
                "replyto": "obs1F2TGgJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3847/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3847/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer NKYa"
                    },
                    "comment": {
                        "value": "Dear Reviewer NKYa,\n\nWe apologize for the delay in our response and appreciate your patience. Please be assured that we have worked diligently to address all the reviews thoroughly, including your concerns. Our detailed responses to other reviewers and our comprehensive global response will be available very soon.\n\nBest regards,\n\nPaper 3847 Authors"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3847/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700633725695,
                "cdate": 1700633725695,
                "tmdate": 1700633767683,
                "mdate": 1700633767683,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KF9nxpT6Sw",
                "forum": "qL6brrBDk2",
                "replyto": "UghuXLOl6w",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3847/Reviewer_NKYa"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3847/Reviewer_NKYa"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the detailed response. \n\n**Re-response to weakness 1**\n\n> due to space constraints\n\nI understand that you prioritized the content according to the research question of the paper, but I still think this evaluation is necessary. I hope that the camera-ready version will include this ablation study in the Appendix.\n\n**Re-response to weakness 2**\n\nI have checked the revised version. I think it is somewhat easier to read. Thank you.\n\n**Re-response to question**\n\nThere seems to be a slight misunderstanding. I was pointing out the applicability of the proposed method to **semi**-supervised learning using pseudo-labels, not **contrastive** learning. This question is not important for the acceptance or rejection of this paper, but I think that this work also can be an important insight for the semi-supervised learning research community. \n\n--- \n\nIn summary, I would like to keep my rating and confidence as my concerns have been addressed, though not completely.\n\nBest,\n\nReviewer NKYa"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3847/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700637142606,
                "cdate": 1700637142606,
                "tmdate": 1700637142606,
                "mdate": 1700637142606,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0EtBJmJ3ed",
            "forum": "qL6brrBDk2",
            "replyto": "qL6brrBDk2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3847/Reviewer_PN9Y"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3847/Reviewer_PN9Y"
            ],
            "content": {
                "summary": {
                    "value": "This article contributes a workflow named as SAFLEX as data augmentation. Here is a summry:\n\n(1) Authors unveil a novel parametrization for learnable augmentation complemented by an adept bilevel\nalgorithm primed for online optimization.\n(2) Author's SAFLEX method is distinguished by its universal compatibility, allowing it to be effortlessly\nincorporated into a plethora of supervised learning processes and to collaborate seamlessly with an\nextensive array of upstream augmentation procedures.\n(3) The potency of authors' approach is corroborated by empirical tests on a diverse spectrum of datasets\nand tasks, all underscoring SAFLEX\u2019s efficiency and versatility, boosting performance by1.2% on\naverage over all experiments."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "They have considered experiments of different data types and model training as downstream tasks, which demonstrate their workflow as a robust one."
                },
                "weaknesses": {
                    "value": "From a model perspective, this is a good one as topic of adaptive learning, though a  little bit off the topic of this conference. \nFrom data augmentation perspective, it is better to demo some more experiments in downstream task involves with high dimensional data."
                },
                "questions": {
                    "value": "Is there any empirical experiments that SAFLEX can contribute to some other applicable downstream task like model training?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3847/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698642834749,
            "cdate": 1698642834749,
            "tmdate": 1699636342720,
            "mdate": 1699636342720,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tn1G76uLQK",
                "forum": "qL6brrBDk2",
                "replyto": "0EtBJmJ3ed",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3847/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3847/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer PN9Y"
                    },
                    "comment": {
                        "value": "We thank reviewer PN9Y for their constructive feedback. We are particularly encouraged that reviewer PN9Y recognizes the robustness of our SAFLEX methodology, as demonstrated by its effectiveness across diverse data types and model training as downstream tasks. We value reviewer PN9Y's insights and will carefully consider their suggestions for improvement.\n\n---\n\n> **Weakness:** From a model perspective, this is a good one as a topic of adaptive learning, though a little bit off the topic of this conference. From the data augmentation perspective, it is better to demo some more experiments in downstream tasks involving high-dimensional data.\n\n\n**Response to Weakness:**\n- Thank you for highlighting the potential of our work in the context of adaptive learning. We agree that it presents an exciting avenue for future exploration.\n\n- Regarding the data augmentation aspect, we have conducted experiments with high-dimensional data to demonstrate the effectiveness of our method. Specifically, we utilized the iWildCam subset from the Wilds dataset (Koh et al., 2021), which features images of size 448x448. These images are representative of high-dimensional data in our context.\n\n- We would be delighted to receive further suggestions from the reviewer regarding other types of high-dimensional data that could be explored.\n\n---\n\n> **Question:** Are there any empirical experiments that SAFLEX can contribute to some other applicable downstream task like model training?\n\n**Response to Question:**\n- In our manuscript, we fine-tuned a CLIP model (Radford et al., 2021) on the iWildCam dataset from Wilds (Koh et al., 2021), using a contrastive fine-tuning paradigm termed \u201cFinetune Like You Pretrain\u201d (FLYP) (Goyal et al., 2023).\n\n- We would be delighted to receive further suggestions from the reviewer regarding other model training tasks that could be explored.\n\n---\n\nAgain, we sincerely thank reviewer PN9Y for their valuable service in reviewing our manuscript."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3847/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700635551718,
                "cdate": 1700635551718,
                "tmdate": 1700652020174,
                "mdate": 1700652020174,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rQpvcuWHjB",
            "forum": "qL6brrBDk2",
            "replyto": "qL6brrBDk2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3847/Reviewer_GM6j"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3847/Reviewer_GM6j"
            ],
            "content": {
                "summary": {
                    "value": "The paper argues that data augmentations can suffer with two main issues - 1. The augmented samples can become out of distribution to the training distribution and 2) the augmented samples can belong to a different class than the original sample. To tackle the first issue, the authors propose to add sample weights (w_i) to the augmented samples. Samples which are farther from the training distribution can be assigned a smaller weight. To tackle the second issue, the authors propose to make the one-hot label as soft-label to capture the uncertainties. \nTo learn the sample weights and the soft-label the authors pose a bi-level optimization problem where in the inner loop, the model parameters are optimized over the training and augmented samples and in the outer loop the optimal augmentation parameters are optimized for. \n\nThe authors conduct experiments across three settings - 1. medical datasets, 2. tabular datasets and 3. for contrastive learning approaches. Across all the experiments the authors show improved performance on top of standard augmentations such as RandAug, Mixup and CutMix."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The motivation in the paper about identifying the two issues with standard augmentation and then solving it by learning sample weights and soft-labels is really clear."
                },
                "weaknesses": {
                    "value": "1. The main issue is a lack of proper baselines. Papers such as [1] have already explored using soft labels for augmentations where the softness is derived on the basis of augmentation strength. This paper's novelty thus gets limited. There is no comparison with [1] in any of the experiments. The authors should do a proper comparison with [1] and justify how their approach is better than it. \n\n2. To solidify the experimental results the authors should also experiment with stronger architectures and datasets such as ResNet-101 over ImageNet as done in [1].\n\nI am willing to update my ratings if my concerns are addressed. \n\nReferences - \n\n1. Soft Augmentation for Image Classification. Liu et al. https://arxiv.org/pdf/2211.04625.pdf"
                },
                "questions": {
                    "value": "I have already mentioned it in the weakness section"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3847/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698799107058,
            "cdate": 1698799107058,
            "tmdate": 1699636342616,
            "mdate": 1699636342616,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "l4SJxHtn7Q",
                "forum": "qL6brrBDk2",
                "replyto": "rQpvcuWHjB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3847/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3847/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Apology for Delay in Rebuttal Response"
                    },
                    "comment": {
                        "value": "Dear Reviewer GM6j,\n\nWe sincerely apologize for the delay in responding to your review. Our team is diligently working to finalize our rebuttal, ensuring that it is thorough and addresses all of your insightful points. We appreciate your patience and understanding in this matter and expect to submit our response very soon. Thank you for your time and valuable feedback.\n\n**Edit**: The response, including the additional experimental results, is now complete. Please refer to the detailed response below.\n\nBest regards,\n\nPaper 3847 Authors"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3847/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700665284323,
                "cdate": 1700665284323,
                "tmdate": 1700689031586,
                "mdate": 1700689031586,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ylAnD0Lcw5",
                "forum": "qL6brrBDk2",
                "replyto": "rQpvcuWHjB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3847/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3847/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer GM6j (1/3)"
                    },
                    "comment": {
                        "value": "We thank reviewer GM6j for their constructive feedback. We are glad to see that reviewer GM6j acknowledges the motivation of our paper and our method of solving the issues with standard augmentation by learning sample weights and soft-label. \n\nBelow, we would like to address the concerns reviewer GM6j has. \n\n---\n\n> **Weakness 1:** The main issue is a lack of proper baselines. Papers such as [1] have already explored using soft labels for augmentations where the softness is derived on the basis of augmentation strength. This paper's novelty thus gets limited. There is no comparison with [1] in any of the experiments. The authors should do a proper comparison with [1] and justify how their approach is better than it.\n[1] Soft Augmentation for Image Classification. Liu et al. https://arxiv.org/pdf/2211.04625.pdf\n\n**Response to Weakness 1:**\nWe acknowledge that our SAFLEX approach shares a similar spirit with the Soft Augmentation method proposed by Liu et al. (2022), in that both methods consider soft labels/targets and soft samlpe weights (i.e., loss reweighting). However, we believe there are huge methodological differences between the two methods in how they model the soft labels and weights. These methodological distinctions lead to significant differences in applicability. Below, we elaborate on the methodological and applicability differences between the two approaches and provide empirical comparisons to further highlight the novelty and improved performance of our method.\n\n1. **Methodology Differences**. Our *SAFLEX* employs a learnable, augmentation-method agnostic, and more automatic and principled approach for generating soft labels and sample weights.\n    1. In *Soft Augmentation*, the authors implement a **specific approach to generating soft labels**, namely through label smoothing. Label smoothing modifies the indicator value '1' (representing the ground-truth class label) with $p = 1 \u2212 \\alpha(\\phi)$, where the adaptive smoothing factor $\\alpha(\\phi)$ is determined by the degree/strength $\\phi$ of the specific sampled augmentation applied to input $x_i$. Notably, the remaining probability mass $\\alpha(\\phi)$ is **uniformly distributed** across all other class labels. The formula of $\\alpha(\\cdot)$ **requires human modeling with domain expertise**. And since different upstream augmentation methods have different definitions of the strength factor $\\phi$, remodeling of $\\alpha(\\cdot)$ for each new augmentation method is required. The discussion in *Soft Augmentation* mainly **focuses on crop augmentations** on images, which impressively draws insights from human visual classification experiments.\n    2. Our *SAFLEX*, in contrast, differs in these key aspects:\n        (a) **Flexible Soft Labels:** *SAFLEX* employs a more flexible approach to modeling soft labels, moving beyond label smoothing's limitations. We believe that uniformly distributing the probability mass across all classes may not always be the most effective strategy. This limitation of *Soft Augmentation* is also acknowledged in the *Soft Augmentation* paper, which states: \"the smoothed target label of a highly-occluded truck example could place more probability mass on other vehicle classes, rather than distributing it equally across all classes.\"\n        (b) **Learned Soft Labels and Sample Weights:** In *SAFLEX*, both soft labels and sample weights are learned from a bilevel optimization problem, which is agnostic to the type and strength of the upstream augmentation method.\n        \\(c\\) **Bilevel Optimization Problem:** *SAFLEX* confronts the inherent challenge of soft augmentation by framing it as a bilevel optimization problem. This approach represents the first rigorous formulation of the problem, underscoring an important theoretical contribution. Additionally, we introduce novel and efficient algorithms specifically designed to tackle this bilevel optimization challenge."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3847/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700674916141,
                "cdate": 1700674916141,
                "tmdate": 1700688609256,
                "mdate": 1700688609256,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "O604hOFKfa",
                "forum": "qL6brrBDk2",
                "replyto": "rQpvcuWHjB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3847/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3847/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer GM6j (2/3)"
                    },
                    "comment": {
                        "value": "2. **Applicability Differences**. Our *SAFLEX* approach offers broader applicability compared to the *Soft Augmentation* method:\n    1. **Universal Compatibility to Upstream Augmentations:** Unlike *Soft Augmentation*, which requires an explicit augmentation strength parameter $\\phi$, *SAFLEX* seamlessly integrates with any upstream data augmentation mechanism, including diffusion models that lack the strength parameter $\\phi$. This versatility enables *SAFLEX* to effectively **handle a wider range of data types**, including medical and tabular data.\n    2. **Compatibility with Diverse Tasks:** *SAFLEX* demonstrates its versatility by effectively handling a variety of tasks, including (standard) classification, fine-grained classification, out-of-distribution (OOD) generalization, and self-supervised learning. This broad applicability is evident in our comprehensive experiments. Conversely, *Soft Augmentation* primarily focuses on image classification, with specific emphasis on model occlusion performance and calibration error, thus limiting its applicability to a narrower range of tasks.\n3. **Empirical Comparisons**. Since the *Soft Augmentation* paper focuses on improving crop augmentation and does not provide formulas to generate soft labels and sample weights for the upstream augmentations we considered, we test it with crop augmentation on the MedMNIST medical image datasets (see Section 5 for dataset details and experiment setups). We use the tuned hyperparameters for crop augmentation and Soft-Augmentation as described in the paper. The experiment results are shown below. We see that except on the DermaMNIST dataset, the performance of *Soft Augmentation* is even lower than the 'No Augmentation' baseline. While *SAFLEX*'s performance is consistently higher than the 'No Augmentation' baseline (see Table 1). Applying crop augmentation directly decreases the performance on most of the MedMNIST datasets. This is not surprising as we observed that applying RandAugment or Mixup directly also lowers the performance (see Table 1). However, the main reason for *Soft Augmentation*'s relatively poor performance is that it cannot consistently improve performance over the crop augmentation baseline (it shows improvement on Derma, Blood, OrganC, OrganS, but decreases performance on Path, Tissue, OCT, OrganA). This suggests that in situations with a high prevalence of poor-quality augmented samples (e.g., crop augmentation on medical images), *Soft Augmentation*'s relatively conservative strategy is inadequate in overcoming the significant noise and label errors introduced by these samples.\n\n| Method\\Dataset              | Path         | Derma        | Tissue       | Blood        | OCT          | OrganA       | OrganC       | OrganS       |\n|-----------------------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|\n| No Aug                      | 94.34 \u00b1 0.18 | 76.14 \u00b1 0.09 | 68.28 \u00b1 0.17 | 96.81 \u00b1 0.19 | 78.67 \u00b1 0.26 | 94.21 \u00b1 0.09 | 91.81 \u00b1 0.12 | 81.57 \u00b1 0.07 |\n| Crop                        | 92.68 \u00b1 0.82 | 76.61 \u00b1 0.14 | 67.38 \u00b1 0.19 | 95.38 \u00b1 0.12 | 77.50 \u00b1 0.11 | 94.46 \u00b1 0.14 | 90.29 \u00b1 0.09 | 80.19 \u00b1 0.06 |\n| Soft Augmentation (w/ Crop) | 91.95 \u00b1 0.59 | 77.05 \u00b1 0.24 | 67.06 \u00b1 0.44 | 95.96 \u00b1 0.28 | 76.92 \u00b1 0.46 | 93.90 \u00b1 0.25 | 91.44 \u00b1 0.24 | 80.92 \u00b1 0.17 |\n\nWe thank the reviewer for highlighting this related paper to us. Soft Augmentation is indeed a closely related work we should compare with. It is worth noting that, besides Soft Augmentation, we have also considered another recent advanced augmentation method, Adversarial Auto-Augment with Label Preservation (LP-A3) (https://arxiv.org/pdf/2211.00824.pdf), in our experimental comparison (see Appendix C). In addition to these two advanced methods, many classical augmentation strategies, including RandAugment, Mixup, and CutMix, are compared as the upstream augmentation baselines. **Based on these, we think that we do not lack proper baselines**.\n\nWe have incorporated the aforementioned discussions and experiments into Appendix B and C and mentioned the Soft Augmentation paper in relevant places in the main text."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3847/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700674998357,
                "cdate": 1700674998357,
                "tmdate": 1700693873090,
                "mdate": 1700693873090,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NgtLeGYdUI",
                "forum": "qL6brrBDk2",
                "replyto": "D79fojQocV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3847/Reviewer_GM6j"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3847/Reviewer_GM6j"
                ],
                "content": {
                    "title": {
                        "value": "Improper experimental comparison"
                    },
                    "comment": {
                        "value": "I think the authors for addressing the issue and adding the Soft Augmentation paper discussion in their appendix. While I agree that the Soft Augmentation paper applies a specific approach to generating soft labels compared to this paper which applies a more automatic and principled approach I am not convinced by the experimental setups of just comparing over MedMNIST.\n\nIs there a reason why the authors don't want to compare between the two approaches on a much larger dataset like ImageNet?"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3847/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700705708158,
                "cdate": 1700705708158,
                "tmdate": 1700705708158,
                "mdate": 1700705708158,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "m0Tl5ccrLX",
                "forum": "qL6brrBDk2",
                "replyto": "rQpvcuWHjB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3847/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3847/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer GM6j's New Comments"
                    },
                    "comment": {
                        "value": "Thank you for acknowledging our efforts in incorporating the Soft Augmentation paper discussion into our appendix. We appreciate your feedback and would like to address your concerns regarding our experimental setups.\n\n1. **Significance of MedMNIST Results:** We chose to focus on medical imaging data because it represents a uniquely challenging domain for augmentation. Unlike natural image datasets, medical images do not benefit from conventional augmentation techniques. The improved performance of our SAFLEX method on such complex data is a testament to its effectiveness. Similar to some prior papers on auto/learnable augmentation like the LP-A3 (https://arxiv.org/pdf/2211.00824.pdf, NeurIPS 2022), which also evaluated on medical images, this choice was deliberate to highlight the robustness and adaptability of SAFLEX in challenging scenarios, underlining its practical relevance and merit. Our primary focus is enhancing upstream augmentations across diverse domains and tasks, not solely for natural image classifications. We not only have experiments on MedMNIST, but many other domains and tasks in the paper. For comparison with the Soft Augmentation method, since Soft Augmentation focuses on improving crop augmentation on images and does not provide formulas to generate soft labels and sample weights for the other upstream augmentations and data types we considered, MedMNIST images are the most suitable datasets to compare on, where algorithms of both methods can be directly applied.\n2. **ImageNet Experiments:** We understand your interest in seeing a comparison on a larger natural image classification dataset like ImageNet. While we initially struggled to run comparative experiments (to make their codebase runnable) for large-scale experiments within the limited rebuttal period, we are committed to including an ImageNet augmentation experiment in the updated manuscript. However, we wish to clarify that our primary focus is on enhancing upstream augmentations across diverse domains, not solely on achieving superior performance on a single, large dataset for natural image classification. We anticipate that SAFLEX, especially when in combination with powerful and suitable upstream augmentation methods, could demonstrate superior performance on ImageNet, too, given its wide compatibility with advanced augmentation methods, which is already well-demonstrated through many experiments across domains and tasks in the paper.\n3. **Contribution Reminder and Scoring:** As we all agree, data augmentations now have broad applications to many domains and tasks in machine learning. Therefore, we think natural image classification on classical datasets like ImageNet is not the only way to demonstrate the effectiveness of an augmentation method. We want to kindly remind you of the contributions of our work. SAFLEX presents a comprehensive solution to a wide range of augmentation challenges, particularly in complex domains like medical imaging and tabular data. Given the already demonstrated merits and potential applications of SAFLEX across many domains and tasks, we believe our work warrants a higher score than 3.\n\nIn conclusion, we respect your perspectives and are actively working to address your concerns. However, we assert that the strength of our contributions and the demonstrated effectiveness of SAFLEX, particularly in challenging domains, justify a more favorable evaluation of our work. Thank you again."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3847/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700710173939,
                "cdate": 1700710173939,
                "tmdate": 1700711997547,
                "mdate": 1700711997547,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NjAubkI7qx",
            "forum": "qL6brrBDk2",
            "replyto": "qL6brrBDk2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3847/Reviewer_EBBs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3847/Reviewer_EBBs"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a principled method for data augmentation. To this end, the paper presents a bilevel optimization framework for weighing and soft-labelling the augmented data in order to compensate for the adverse generalization effects of weak, strong and sometimes meaningless augmented examples. Although the impact of data augmentation for generalization, in particular deep learning frameworks, has been substantial, there is still a lack of principled ways of doing data augmentation. This paper has identified this gap and convincingly addressed the problem."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The paper is well-written and easy to understand. \n\nThe diagrams and the equations are easy to follow.\n\nThe experiments are performed on diverse datasets with various tasks, including medical imaging and tabular data.\n\nThe results are highly encouraging."
                },
                "weaknesses": {
                    "value": "A few important previous works on sampling and purifying GAN synthetic data are relevant to this paper.  It is important to acknowledge and discuss their contributions in the paper. \n\nCaramalau, Razvan, Binod Bhattarai, and Tae-Kyun Kim. \"Sequential graph convolutional network for active learning.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021.\nBhattarai, Binod, et al. \"Sampling strategies for gan synthetic data.\" ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020."
                },
                "questions": {
                    "value": "I like the paper. Please see a few comments above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3847/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699042537294,
            "cdate": 1699042537294,
            "tmdate": 1699636342531,
            "mdate": 1699636342531,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "akYH93MjmK",
                "forum": "qL6brrBDk2",
                "replyto": "NjAubkI7qx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3847/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3847/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer EBBs"
                    },
                    "comment": {
                        "value": "We thank reviewer EBBs for their positive feedback. We are especially encouraged that reviewer EBBs acknowledge that our SAFLEX is a \"principled way of doing data augmentation.\" We are also happy to see that reviewer EBBs points out that our paper is well-written easy-to-understand, the diagrams and equations are easy to follow, the experiments are performed on diverse datasets with various tasks, and the results are encouraging.\n\nBelow, we would like to address the weakness mentioned by reviewer EBBs. \n\n---\n\n> **Weakness:** A few important previous works on sampling and purifying GAN synthetic data are relevant to this paper. It is important to acknowledge and discuss their contributions in the paper.\n> Caramalau, Razvan, Binod Bhattarai, and Tae-Kyun Kim. \"Sequential graph convolutional network for active learning.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021. \n> Bhattarai, Binod, et al. \"Sampling strategies for gan synthetic data.\" ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020.\n\n**Response to Weakness:** We have included the papers by Bhattarai et al. (2020) and Caramalau et al. (2021) in the revised manuscript's related work section and have acknowledged and discussed their contributions. Bhattarai et al. (2020) proposed a progressive sampling strategy for GAN synthetic data, while Caramalau et al. (2021) introduced a sequential graph convolutional network for active learning. Our work extends these findings by developing a novel sampling and purifying method for augmented data that is specifically designed to improve the performance of downstream tasks.\n\n---\n\nAgain, we sincerely thank reviewer EBBs for their valuable service in reviewing our manuscript."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3847/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700634516395,
                "cdate": 1700634516395,
                "tmdate": 1700634516395,
                "mdate": 1700634516395,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]