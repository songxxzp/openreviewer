[
    {
        "title": "LUM-ViT: Learnable Under-sampling Mask Vision Transformer for Bandwidth Limited Optical Signal Acquisition"
    },
    {
        "review": {
            "id": "eu8H9nX4ji",
            "forum": "wkbeqr5XhC",
            "replyto": "wkbeqr5XhC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9181/Reviewer_wWh6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9181/Reviewer_wWh6"
            ],
            "content": {
                "summary": {
                    "value": "The proposed method of this paper is LUM-ViT, a learnable under-sampling mask vision transformer for bandwidth-limited optical signal acquisition. It is a novel approach that utilizes deep learning and prior information to reduce acquisition volume and optimize for optical calculations. The methodology unfolds in two primary stages: training from pre-trained models in a solely electronic domain using existing datasets, followed by inference to evaluate model performance, and assessing the real-world performance of LUM-ViT with a DMD signal acquisition system. During acquisition, target information undergoes a single instance of DMD optical modulation before capture, and then is funneled into the electronic system for further processing."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "(i) The studied problem about under-sampling hyperspectral data acquisition, achieving data reduction from signal collection while preserving model performance. This accelerates the HSI processing in real applications such as remote sensing, object tracking, medical imaging, etc.\n\n(ii) The idea of using a learnable mask refined during training to selectively retain essential points for downstream tasks from the patch embedding outputs, and thereby achieving under-sampling (reducing the required sampling instances) is interesting.\n\n(iii) The performance is good. On the ImageNet-1k classification task, the proposed LUM-ViT maintains accuracy loss within 1.8% at 10% under-sampling and within 5.5% at an extreme 2% under-sampling.\n\n(iv) This work not only conducts experiments on the synthetic data but also sets up hardware (as shown in Figure 6) to evaluate the effectiveness of the proposed method. It is a good and non-trivial exploration. The accuracy loss of LUM-ViT does not exceed 4% compared to the software environment, demonstrating its practical feasibility."
                },
                "weaknesses": {
                    "value": "(i) The detailed formulations for DMD are missing, which is confusing. More explanations are required.\n\n(ii) Code and pre-trained weights are not submitted. The reproducibility of this work cannot be checked. \n\n(iii) The multi-stage training pipeline is tedious, which makes the whole technical route unreliable. What's worse, the finetuning details about stage 3 are missing.  Other researchers cannot re-implement this complex approach. \n\n(iv) The writing should be further improved, especially the mathematical notations in Section 3.3. The formula for binary compression is not formal.\n\n(v) The experiments are not sufficient and many critical comparisons are missing. For example, the backbone is ViT variants. However, the ViT is very computationally expensive because its computational complexity is quadratic to the input spatial size. This also embeds the real-time applications of HSI processing. In contrast, MST [1] or MST++ [2] are specially designed for HSI processing. They treat spectral feature maps as a token to capture the interdependencies between spectra with different wavelengths. Most importantly, they are very efficient with linear computational complexity regarding spatial resolution. Thus, it is better to add a comparison with the spectral Transformer in Figure 5.\n\n(vi) The binarization mechanism is out of fashion. BiSCI [3] provides a specially designed binarized convolution unit BiSR-conv block to process HSI data cubes. It is also better to add a comparison with this new technique.\n\n[1] Mask-guided Spectral-wise Transformer for Efficient Hyperspectral Image Reconstruction. In CVPR 2022.\n\n[2] MST++: Multi-stage Spectral-wise Transformer for Efficient Spectral Reconstruction. In CVPRW 2022, NTIRE 2022 Winner in Spectral Recovery.\n\n[3] Binarized Spectral Compressive Imaging. In NeurIPS 2023."
                },
                "questions": {
                    "value": "The technical route and core idea in this paper is to accelerate the HSI processing, which is similar to Coded Aperture Spectral Snapshot Imaging (CASSI). Could you please analyze the differences, advantages, and disadvantages of these two systems?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9181/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9181/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9181/Reviewer_wWh6"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9181/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698692259477,
            "cdate": 1698692259477,
            "tmdate": 1699637155196,
            "mdate": 1699637155196,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PUduI2n1gA",
                "forum": "wkbeqr5XhC",
                "replyto": "eu8H9nX4ji",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9181/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9181/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to reviewer wWh6"
                    },
                    "comment": {
                        "value": "## Reply to reviewer wWh6\n\nDear Reviewer,\n\nWe sincerely appreciate your professional and insightful review of our manuscript. Your detailed feedback has been immensely helpful and provided valuable guidance in improving our work. We are pleased with your interest in our learnable mask method and appreciate your acknowledgment of the LUM-ViT's performance. This acknowledgment serves as a significant encouragement to our team. Additionally, we value your appreciation and attention to our hardware work, which, though time-consuming and complex, is an indispensable part of our research.\n\nWe apologize for the delay in our response. We have conducted additional experiments with two comparative models and on three real hyperspectral datasets. These experiments were time-consuming but have significantly contributed to making our experimental section more robust and convincing.\n\nPlease note that all modifications in the manuscript are marked in red, except for the content in the Appendix, which is entirely new.\n\nWe will now address each of the weaknesses and questions you raised in detail.\n\n---\n\n> **W-1: The detailed formulations for DMD are missing, which is confusing. More explanations are required.**\n\n---\n**REPLY**: Thank you for pointing out the need for a more detailed explanation of Digital Micromirror Devices (DMDs). DMDs are vital optical semiconductor devices that form the core of Digital Light Processing (DLP) technology. They consist of an array of up to millions of micromirrors, each with dimensions in the order of a few micrometers.\n\nThe primary functionality of a DMD is its ability to spatially modulate light. Each micromirror can be individually tilted by electrostatic forces between \\(+12\\) to \\(-12\\) degrees, corresponding to on and off states, respectively. This tilt determines whether light is directed towards the projection lens (on state) or absorbed by a light trap (off state), thus controlling the brightness of each pixel in the projected image.\n\nWhile the DMD is a key component of the hardware system in our study, and the DMD signal acquisition system is central to optical computing and signal collection, this system is not the focus or the novelty of our work. Therefore, it was not elaborately detailed in the main manuscript. However, understanding the DMD system is fundamental to comprehending many designs of LUM-ViT. Acknowledging this, we have included a detailed explanation of the DMD system in Section D of the Appendix, considering the length constraints of the main text.\n\n---\n\n> **W-2: Code and pre-trained weights are not submitted. The reproducibility of this work cannot be checked.**\n\n---\n**REPLY**: We acknowledge your concern regarding the reproducibility of our work, particularly the absence of code and pre-trained weights. We are actively working on organizing and annotating the code for LUM-ViT's model construction and will make it available on the current platform as soon as possible. Furthermore, following the publication of our paper, we plan to open source all related codes and checkpoints on GitHub.\n\nThis effort to provide detailed and well-documented code reflects our commitment to the scientific community's standards for reproducibility and transparency. We recognize the importance of your emphasis on these aspects and are dedicated to upholding these essential scientific principles."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9181/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700495979908,
                "cdate": 1700495979908,
                "tmdate": 1700495979908,
                "mdate": 1700495979908,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wmebRA4XBY",
            "forum": "wkbeqr5XhC",
            "replyto": "wkbeqr5XhC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9181/Reviewer_vQM3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9181/Reviewer_vQM3"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel approach using pre-acquisition modulation with a deep learning model called LUM-ViT. Specifically, it utilizes ViT as the backbone network and a DMD signal acquisition system for patch-embedding. Moreover, a kernel-level weight binarization technique and a three-stage fine-tuning strategy is proposed for optimizing the optical calculations. With low sampling rates, LUM-ViT maintains high accuracy on ImageNet dataset."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tThe idea is of the paper is interesting. The proposed method performs calculations of the patch-embedding layer instead of directly sampling the whole images. The proposed LUM-ViT is suited for both dataset and downstream tasks.\n2.\tThe accuracy loss is low with extreme under-sampling"
                },
                "weaknesses": {
                    "value": "1.\tThe description of the entire system and methods is not clear and intuitive enough, and the figures are also misleading. The RGB image is used as an example in Figure 1, which does not reflect the characteristics of hyperspectral imaging. \n2.\tLack of experiments on real hyperspectral imaging. The author has emphasized hyperspectral imaging in the introduction section, but in reality, it has not been verified using real hyperspectral images. The performance of this method in real hyperspectral imaging tasks still needs to be discussed"
                },
                "questions": {
                    "value": "1.\tThe training phase uses images of 3 color channels while the real-world experiment uses 7 color images. What is the meaning of \u2018reconfigured\u2019? Or the author just fine tuned the LUM-VIT with 7 color samples? Can the pre trained model be used directly for images in different bands without the need for matching data?\n2.\tThis acquisition system seems to have to work together with vit to obtain intermediate features of images. Can it reconstruct a complete hyperspectral image in a real-world environment?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9181/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698747496359,
            "cdate": 1698747496359,
            "tmdate": 1699637155086,
            "mdate": 1699637155086,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "F3xrK3Duwm",
                "forum": "wkbeqr5XhC",
                "replyto": "wmebRA4XBY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9181/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9181/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to reviewer vQM3"
                    },
                    "comment": {
                        "value": "## Reply to reviewer vQM3\n\nDear Reviewer,\n\nWe sincerely thank you for your constructive suggestions, which have been beneficial in enhancing our research. Your interest in the paper's idea and recognition of the LUM-ViT model's performance are greatly appreciated and serve as valuable encouragement to our team.\n\nWe apologize for the delay in our response. This was due to conducting comprehensive experiments on three hyperspectral datasets, as suggested in your review. These crucial experiments aimed to address the weaknesses you pointed out and to strengthen the validation of our method in real hyperspectral imaging scenarios.\n\nFor ease of review, all modifications made in the manuscript are marked in red, except for the content in the Appendix, which is entirely new.\n\nWe will now address each of the weaknesses and questions you raised in detail."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9181/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700415689433,
                "cdate": 1700415689433,
                "tmdate": 1700415689433,
                "mdate": 1700415689433,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "88HuKEghJp",
            "forum": "wkbeqr5XhC",
            "replyto": "wkbeqr5XhC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9181/Reviewer_rvtr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9181/Reviewer_rvtr"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a learnable under-sampling mask vision Transformer, which incorporates a learnable undersampling mask tailored for pre-acquisition modulation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ The paper is well-organized and clearly written.\n\n+ The proposed three-stage training strategy for training LUM-ViT is effective."
                },
                "weaknesses": {
                    "value": "- Technical details should be clear. How to achieve the learnable under-sampling mask? How is this learnable achieved? Is the learning accurate? Relevant visualization results should be provided.\n\n- The experimental results seem insufficient. The author only conducted validation on the ImageNet-1k classification task, and other tasks should also be further explored.\n\n-----------------------After Rebuttal---------------------------\n\nThank you for your feedback. The rebuttal addressed my concerns well. Considering other reviews, I have decided to increase my score."
                },
                "questions": {
                    "value": "See the above Weaknesses part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9181/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9181/Reviewer_rvtr",
                        "ICLR.cc/2024/Conference/Submission9181/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9181/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698821511658,
            "cdate": 1698821511658,
            "tmdate": 1700746364317,
            "mdate": 1700746364317,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fnPgvSIAFn",
                "forum": "wkbeqr5XhC",
                "replyto": "88HuKEghJp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9181/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9181/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to reviewer rvtr"
                    },
                    "comment": {
                        "value": "## Reply to reviewer rvtr\n\nDear Reviewer,\n\nFirst and foremost, we would like to express our sincere gratitude for your thoughtful and constructive feedback on our paper. Your positive remarks about the organization and clarity of our manuscript, as well as the effectiveness of the proposed three-stage training strategy for training LUM-ViT, are greatly appreciated. Such recognition from an expert in the field is both encouraging and affirming for our work.\n\nWe apologize for the delay in our response. The additional time was utilized to conduct extensive experiments with two comparative methods and on three hyperspectral datasets. These experiments, while time-consuming, were essential in addressing the weaknesses you highlighted. We believe that the results of these additional experiments have significantly strengthened our paper and have allowed us to thoroughly address your concerns.\n\nTo ensure clarity and ease of review, we have marked all modifications made in the manuscript in red for easy identification, with the exception of the content in the Appendix, as it is newly added. These changes include adjustments and additions in response to valuable feedback, as well as results from additional experiments.\n\nIn the following sections of this response, we will specifically address each weakness you have identified, providing detailed explanations and supplementary information.\n\nThank you once again for your valuable insights and for the opportunity to enhance the quality of our work."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9181/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700371527959,
                "cdate": 1700371527959,
                "tmdate": 1700371682973,
                "mdate": 1700371682973,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HSeKVzBjSN",
                "forum": "wkbeqr5XhC",
                "replyto": "88HuKEghJp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9181/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9181/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "---\n\n> **W-1: Technical details should be clear. How to achieve the learnable under-sampling mask? How is this learnable achieved? Is the learning accurate? Relevant visualization results should be provided.**\n\n---\n\n**REPLY**: Thank you for highlighting the need for clearer technical details regarding the learnable under-sampling mask in our LUM-ViT model. We acknowledge that our initial manuscript may not have adequately explained the implementation of the learnable mask.\n\nTo clarify, the learnable under-sampling mask in LUM-ViT is achieved through a binary decision mask with trainable parameters. During the training phase, these parameters determine the mask values, which influence the loss function values of two critical metrics: the accuracy of downstream tasks and the under-sampling rate. These parameters are updated through backpropagation, resulting in a mask that adheres to the under-sampling rate criteria and is optimally adapted for the downstream tasks.\n\nIt is important to note that, unlike DynamicViT or AdaViT architecture, which dynamically adjust their masks during inference, LUM-ViT requires pre-acquisition determination of the mask scheme. LUM-ViT focuses on under-sampling, which means reducing the number of optical computations and detections. For a single inference, optical computations are carried out before the entire system detects the target object and cannot adjust the masking strategy based on yet-to-know information on the target object. Consequently, LUM-ViT generates a mask that remains fixed post-training, *i.e.*, during the inference stage. This mask is designed to suit the validation set and downstream tasks through training on a compatibly distributed training set.\n\nTo further elucidate the principle of the learnable mask, we have revised our manuscript to replace the term 'learnable parameters' with 'trainable parameters'. This change emphasizes that the learnability is achieved through training. Additionally, we have substantially revised the subsection 'Learnable Under-sampling Mask' in the manuscript and revised the description in the introduction regarding the different requirements for learnable masks in under-sampling tasks versus those in pruning tasks.\n\n---\n\n**Top-1 Acc comparison.**\n\n| Model             | Acc (%) - 10% mask | Acc (%) - 2% mask |\n| ----------------- | ------------------------ | ----------------------- |\n| Random-mask-ViT   | 74.6                     | 62.1                    |\n| Mag-mask-ViT      | 76.2                     | 65.4                    |\n| **LUM-ViT**       | **81.9**                 | **78.3**                |\n\n---\n\nWe have also incorporated new experiments to validate the efficacy of the learnable mask approach. We introduced two alternative mask schemes: Random-mask-ViT, which employs a random mask, and Mag-mask-ViT, which uses a mask based on data magnitude. The results of these experiments, which show a significant performance gap between these methods and LUM-ViT, indirectly corroborate the effectiveness of our learnable mask method.\n\nThese experiments are detailed in the subsection 'The Training Phase Experiments', with results showcased in Fig. 4. Due to space constraints, we have included detailed descriptions of the Random-mask-ViT and Mag-mask-ViT models, along with previously base models like DU-ViT and CS-method, in Section B of the Appendix.\n\nFurthermore, we have added a visualization analysis to investigate the internal mechanism of the learnable mask. Since LUM-ViT requires the mask to be fixed before sample detection during inference and does not allow for dynamic adjustment based on individual samples, we are unable to provide visualizations that compellingly illustrate non-essential parts of an individual sample being selectively masked. To compensate for this, we have visualized the masking results of LUM-ViT on convolutional kernels and patches at different mask ratios, using heatmaps and histograms. This analysis reveals LUM-ViT's tendency to select convolutional kernels under low under-sampling pressure and delve deeper into patch selection under high pressure. Due to space constraints, only the heatmap for LUM-ViT-2% is retained in the main text, with a detailed analysis provided in Section C of the Appendix.\n\nOnce again, we thank you for your insightful comments, which have significantly contributed to the improvement of our manuscript."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9181/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700371572475,
                "cdate": 1700371572475,
                "tmdate": 1700412305825,
                "mdate": 1700412305825,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ssYtrgISWU",
                "forum": "wkbeqr5XhC",
                "replyto": "88HuKEghJp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9181/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9181/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "---\n\n> **W-2: The experimental results seem insufficient. The author only conducted validation on the ImageNet-1k classification task, and other tasks should also be further explored.**\n\n---\n\n**REPLY**: Thank you for your insights regarding the scope of our experimental validation. We appreciate your point on the necessity of a broader range of tests to comprehensively assess the performance of our model, LUM-ViT, particularly in the context of hyperspectral imagery.\n\n---\n\n**Results on HSI classification.**\n\n| Dataset           | OA (%) - before mask | OA (%) - after mask |\n| ----------------- | ---------------------------------- | --------------------------------- |\n| Indian Pines      | 87.4                               | 86.2                              |\n| Pavia University  | 89.5                               | 88.6                              |\n| Salinas           | 99.4                               | 99.1                              |\n\n---\n\nRecognizing the importance of diversifying our validation scenarios, we have conducted additional experiments on three widely-used hyperspectral remote sensing datasets: Indian Pines, Pavia University, and Salinas. These datasets are commonly utilized for hyperspectral pixel and region classification tasks, and we have tested LUM-ViT on these tasks. The results on these three datasets were promising, demonstrating LUM-ViT's capability in processing actual spectral data effectively.\n\nWe have added a new subsection titled 'The Hyperspectral Image Classification Experiments' in the 'Experiments' section of our manuscript to report the results of LUM-ViT on these three hyperspectral datasets. Due to space constraints, details regarding the datasets and parameter adjustments for these experiments have been included in Section A of the Appendix.\n\nOnce again, thank you for your insightful feedback, which has prompted us to further strengthen our manuscript with these additional experiments."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9181/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700371614726,
                "cdate": 1700371614726,
                "tmdate": 1700412350967,
                "mdate": 1700412350967,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HeylCWmRWH",
                "forum": "wkbeqr5XhC",
                "replyto": "88HuKEghJp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9181/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9181/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer,\n\nI am writing in reference to manuscript ID 9181. We have previously addressed the feedback provided and are keen to hear any additional thoughts or comments you might have. Your insights are invaluable to the continued refinement of our work.\n\nWe have diligently worked on improving the manuscript, with significant enhancements in clarity and detail, especially in the Appendix, where we have elaborated on the experimental procedures. The main code for the LUM-ViT model is also available now, aiming to support the reproducibility of our findings.\n\nUnderstanding the many demands on your time, we appreciate any further input you can provide at your earliest convenience. As the review process deadline nears, your further guidance will be instrumental in ensuring the quality and accuracy of our research.\n\nThank you very much for your attention and contribution to our work. We look forward to your valuable feedback.\n\nWarm regards,\nManuscript ID 9181"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9181/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631685737,
                "cdate": 1700631685737,
                "tmdate": 1700631685737,
                "mdate": 1700631685737,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]