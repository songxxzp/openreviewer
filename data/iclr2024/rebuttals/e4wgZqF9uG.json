[
    {
        "title": "On the Viability of Monocular Depth Pre-training for Semantic Segmentation"
    },
    {
        "review": {
            "id": "UvAwe4keP4",
            "forum": "e4wgZqF9uG",
            "replyto": "e4wgZqF9uG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1450/Reviewer_N1Lo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1450/Reviewer_N1Lo"
            ],
            "content": {
                "summary": {
                    "value": "This paper is about using monocular depth estimation training as pre-training for the semantic segmentation task. The idea is quite simple, whether such a pre-training can be better than pretraining with classification task on ImageNet. Extensive experiments are carried out to verify the hypothesis. The conclusion of this paper is that compared to classification, using depth estimation as pretraining on average improves the segmentation performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ The idea to use monocular depth estimation as pretraining for semantic segmentation is sensible, considering that it is relatively easy to collect depth data. \n+ The experimental results support the hypothesis."
                },
                "weaknesses": {
                    "value": "- It would be interesting to see the segmentation performance on PASCAL VOC and COCO with the depth pretraining. \n- The writing is poor, sometimes the notations are not well defined or clarified. For instance, in EQ.1, since we are doing pretraining for depth estimation, why in the loss function, the depth term is missing?  Instead, there is the wrapped image? Similarly, the input of the loss function in EQ.2 is not clarified as well. \n-  Some results are not sufficiently analyzed. For example, in the left figure of Fig.3, it seems that with larger training size, the bigger gain could be obtained.  However, this is counter-intuition, which needs proper explanations."
                },
                "questions": {
                    "value": "We see the the main comparisons are between ImageNet classification-based pre-training vs Depth pre-training. One thing to consider is that depth pre-training happens in-domain with the same data to be fine-tuned. My question is whether the superiority of depth pre-training over ImageNet pre-training mainly relies on in-domain knowledge or data distribution. What if the depth pre-training is conducted on a different dataset rather than the one to be fine-tuned. Will it still has such significant improvement over classification pre-training?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1450/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1450/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1450/Reviewer_N1Lo"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1450/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698806244344,
            "cdate": 1698806244344,
            "tmdate": 1699636073883,
            "mdate": 1699636073883,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9eSbizL5aq",
                "forum": "e4wgZqF9uG",
                "replyto": "UvAwe4keP4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1450/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1450/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response to the initial review"
                    },
                    "comment": {
                        "value": "**W1:** *It would be interesting to see the segmentation performance on PASCAL VOC and COCO with the depth pre-training.*\n\n**R:** We cannot report results on PascalVOC and COCO due to a lack of camera calibration on both datasets, unlike KITTI, CityScapes, and NYU-V2. Consequently, we cannot perform depth pre-training accordingly. Although it is possible to use a more generic depth model (e.g., DPT trained on a mixed dataset) for fine-tuning on both datasets, we attempted to conduct the experiment as suggested by the reviewer but encountered restrictions in compute resources (batch size restricted by GPU memory).\n\n**W2:** *The writing is poor, sometimes the notations are not well defined or clarified. For instance, in EQ.1, since we are doing pre-training for depth estimation, why in the loss function, the depth term is missing? Instead, there is the wrapped image? Similarly, the input of the loss function in EQ.2 is not clarified as well.*\n\n**R:** Eq. 1 and 2 refer to the standard structure-from-motion formulation, which is a convention for inferring depth from multi-view/video data (please see Appendix B.1 for a discussion regarding this). Inferring depth does not require depth measurements as supervision. One only needs images with sufficiently exciting textures that are co-visible between them. We will clarify this in the paper revision.\n\n**W3:** *Some results are not sufficiently analyzed. For example, in the left figure of Fig.3, it seems that with larger training size, the bigger gain could be obtained. However, this is counter-intuition, which needs proper explanations.*\n\n**R:** For Fig. 3-left, a larger training set size for fine-tuning semantic segmentation improves results. This shows that pre-training with depth not only improves sample complexity but also scales well with respect to it, which is not the case for pre-training with ImageNet, where it saturates after 64 samples.\n\n**Q1:** *We see the the main comparisons are between ImageNet classification-based pre-training vs Depth pre-training. One thing to consider is that depth pre-training happens in-domain with the same data to be fine-tuned. My question is whether the superiority of depth pre-training over ImageNet pre-training mainly relies on in-domain knowledge or data distribution. What if the depth pre-training is conducted on a different dataset rather than the one to be fine-tuned. Will it still has such significant improvement over classification pre-training?*\n\n**R:** The superiority of depth pretraining over ImageNet is not contingent on in-domain pretraining. The last row of Table 2 shows initializing with weights from DPT, which does not include KITTI data for pretraining. Despite being out-of-domain, depth pretraining still improves over ImageNet pretraining. Please also see *'A general note to the reviewers and AC'* for a detailed discussion.\n\nWe hope this response addresses the reviewer's concerns. If there are further concerns, we are more than willing to provide additional explanations."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1450/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700713557187,
                "cdate": 1700713557187,
                "tmdate": 1700713745664,
                "mdate": 1700713745664,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "odCT2ZjSLb",
            "forum": "e4wgZqF9uG",
            "replyto": "e4wgZqF9uG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1450/Reviewer_Ynow"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1450/Reviewer_Ynow"
            ],
            "content": {
                "summary": {
                    "value": "The paper shows that depth pre-training exceeds performance relative to ImageNet pre-training on various downstreaming tasks (segmentation / depth prediction). The authors conduct extensive experiments with various architectures and settings."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper conducts extensive experiments to demonstrate that pre-training on depth images is beneficial to downstream tasks (depth / segmentation).\n\n2. It\u2019s interesting to see depth pre-training helps for some downstreaming tasks in a few-shot manner.\n\n3. The paper is in general easy to understand."
                },
                "weaknesses": {
                    "value": "1. The paper studies how depth-pretraining helps for downstreaming tasks. Since the main downstreaming task is still depth prediction, I feel that pre-training on depth prediction shows some improvements that are not that impressive to the community. Though segmentation is also studied on CityScape, maybe testing on more datasets would be more convincing.\n\n2. Regarding optical flow, I wonder if there is more clarification on why optical flow is not a good choice for pre-training? Is it because the prediction is more inaccurate / harder? Furthermore, how about optical flow as a downstreaming task? Does depth pre-training still help? \n\n3. In Figure 2, I wonder why Depth-rand is on par with ImageNet pre-trained for ResNet-18, but the observation is very different for ResNet-50. I wonder if the authors have any thoughts on this?\n\n4. Suppose more images are available, would the pre-training in depth still be beneficial? For example, in Figure 3, what if there are hundreds of images, maybe using the full training set? Similarly, for segmentation, how much gain could the model obtain if there are more images? Is it also the case that only 16 images are used for fine-tuning for the results obtained?\n\n5. It would be helpful to clarify the experiment settings in Table 1 at the beginning of the experiment section. I wonder if the authors may further clarify the difference between  \u2018Depth-Rand\u2019 and \u2018Depth\u2019?\n\n6. Since this is mainly a paper comparing different existing strategies, in general it is of little technical novelty."
                },
                "questions": {
                    "value": "Please see my questions above. In general, I feel the paper shows some interesting findings, but I am not sure if there is enough contribution to the community."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1450/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1450/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1450/Reviewer_Ynow"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1450/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698881295018,
            "cdate": 1698881295018,
            "tmdate": 1699636073785,
            "mdate": 1699636073785,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RLROU3cVlD",
                "forum": "e4wgZqF9uG",
                "replyto": "odCT2ZjSLb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1450/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1450/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response to the initial review"
                    },
                    "comment": {
                        "value": "**W1:** *The paper studies how depth-pretraining helps for downstreaming tasks. Since the main downstreaming task is still depth prediction, I feel that pre-training on depth prediction shows some improvements that are not that impressive to the community. Though segmentation is also studied on CityScape, maybe testing on more datasets would be more convincing.*\n\n**R:** There may be some misunderstanding but the downstream task in this manuscript is always semantic segmentation. We show that pre-training on monocular depth estimation can in general improve downstream semantic segmentation.\n\n**W2:** *Regarding optical flow, I wonder if there is more clarification on why optical flow is not a good choice for pre-training? Is it because the prediction is more inaccurate / harder? Furthermore, how about optical flow as a downstreaming task? Does depth pre-training still help?*\n\n**R:** We provided a detailed discussion about using optical-flow on page 7. We validated that the flow obtained by pre-training is accurate (also see Figure 7 for visualization), so the improvement is not related to the inaccuracy or difficulty of the task. Moreover, see our response to Reviewer gFUe, where using miscalibrated camera intrinsic in depth estimation (so that the results are not accurate) can also lead to improvement. \n\nUsing depth pre-training may help optical flow in specific domains. However current main-stream flow methods (e.g. RAFT, PWC-Net) do not incorporate a standard backbone, so a direct comparison is infeasible. We would like to remind the reviewer that the purpose of this paper is to validate whether training on a geometry task can actually help improve a seemingly distantly related semantic task. \n\n**W3:** *In Figure 2, I wonder why Depth-rand is on par with ImageNet pre-trained for ResNet-18, but the observation is very different for ResNet-50. I wonder if the authors have any thoughts on this?*\n\n**R:** One possibility is ResNet50 has bigger capacity than ResNet18, so there can be more local minimum and the model is subject to overfitting. We will include the discussion in the paper as suggested.\n\n**W4:** *Suppose more images are available, would the pre-training in depth still be beneficial? For example, in Figure 3, what if there are hundreds of images, maybe using the full training set? Similarly, for segmentation, how much gain could the model obtain if there are more images? Is it also the case that only 16 images are used for fine-tuning for the results obtained?*\n\n**R:** On KITTI, we keep the 16-image training set except for Figure 3, as stated on page 5. We do this \u201cfew-shot\u201d experiment to highlight the effect of pre-training. However, on Cityscapes, we do use the full training set, and adopting monocular depth as pre-training still yields improvement.\n\n**W5:** *It would be helpful to clarify the experiment settings in Table 1 at the beginning of the experiment section. I wonder if the authors may further clarify the difference between \u2018Depth-Rand\u2019 and \u2018Depth\u2019?*\n\n**R:** We will clarify. Depth-rand refers to depth model trained from scratch (randomly initialized weights), while depth refers to the best depth model, trained from standard ImageNet initialization. We show that despite not using any annotation, only training for monocular depth on 3 hours of driving video, can leads to similar results to training on ImageNet, which is collected through crowd-sourcing with the assistance of around 15,000 paid annotators. Nevertheless, since ImageNet is there, one can make best use of it to train a better depth model, which further improving the performance yielding the best result. \n\n**W6:** *Since this is mainly a paper comparing different existing strategies, in general it is of little technical novelty.*\n\n**R:** Regarding the innovation of the paper, please refer to *A general note to the reviewers and AC*.\n\nWe hope this response addresses the reviewer's concerns. If there are further concerns, we are more than willing to provide additional explanations."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1450/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700608892427,
                "cdate": 1700608892427,
                "tmdate": 1700609081039,
                "mdate": 1700609081039,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "fMrra7dx3K",
            "forum": "e4wgZqF9uG",
            "replyto": "e4wgZqF9uG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1450/Reviewer_gFUe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1450/Reviewer_gFUe"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors explore how pre-training a model to infer depth from a single image compares to pre-training the model for a semantic task for the purpose of downstream transfer to semantic segmentation. The intuition of their work is to avoid human annotation by pre-training a model on the depth estimation task in which the Ground Truth can be acquired through video, multi-view stereo, or range sensor. They carefully design experiments to prove that depth pre-training exceeds performance relative to ImageNet pre-training, and optical flow estimation is less effective."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper is well-organized, which help readers easy to read and understand. Expecially, the intuition of this paper is described clearly and sounds make sense. The topic studied in this paper is critical to the industry. It is a good practice guidline. \n2. The experiments on multiple datasets are extensive, which covers almost all potential variations. It helps the authors conclude multiple guidances and helps convince readers. The conclusions are usefual to industry. \n3. More details are reported in the appendix. These information is a good addition to the main paper. Implementation details, training details, more results analysis are clear and should be able to help other researchers to duplicate their work."
                },
                "weaknesses": {
                    "value": "The whole paper sounds like a experimental report. My biggest concern about this paper is its lack of innovation. Many existing work has explored ways to combine depth and segmentation, while this paper did more extensive experiments to summarize a more helpful pipeline. Beside that, there is no other contribution."
                },
                "questions": {
                    "value": "1. Almost all experiments in this paper assume the depth ground truth are reliable and calibrated with the RGB images. What if the depth  ground truths are not reliable (have noises, does not calibrated with RGB images)? How does it impact the performance of the proposed pipeline?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1450/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698953911399,
            "cdate": 1698953911399,
            "tmdate": 1699636073719,
            "mdate": 1699636073719,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hfTUPEjhfD",
                "forum": "e4wgZqF9uG",
                "replyto": "fMrra7dx3K",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1450/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1450/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response to the initial review"
                    },
                    "comment": {
                        "value": "Thank you for your support of the paper.\n\n**W1:** *The whole paper sounds like a experimental report. My biggest concern about this paper is its lack of innovation. Many existing work has explored ways to combine depth and segmentation, while this paper did more extensive experiments to summarize a more helpful pipeline. Beside that, there is no other contribution.*\n\n**R:** Regarding the innovation of the paper, please refer to *A general note to the reviewers and AC*.\n\n\n**Q1:** *Almost all experiments in this paper assume the depth ground truth are reliable and calibrated with the RGB images. What if the depth ground truths are not reliable (have noises, does not calibrated with RGB images)? How does it impact the performance of the proposed pipeline?*\n\n**R:** This question is very interesting. As suggested by the reviewer, we introduce miscalibration to the depth model by allowing camera intrinsics to vary by up to 10%. Surprisingly, we achieve on-par improvement compared with using the ground-truth calibration on CityScapes. We posit that the connection between depth and object semantics is influenced by the object scale. A miscalibrated camera affects the object's scale, but it does not significantly change the object's semantics. We are more than happy to include this discussion in the paper.\n\nWe hope this response addresses the reviewer's concerns. If there are further concerns, we are more than willing to provide additional explanations."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1450/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700606667442,
                "cdate": 1700606667442,
                "tmdate": 1700606667442,
                "mdate": 1700606667442,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8E7B928lSG",
                "forum": "e4wgZqF9uG",
                "replyto": "hfTUPEjhfD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1450/Reviewer_gFUe"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1450/Reviewer_gFUe"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you authors' responses. All of my questions are answered. No more questions. Thanks."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1450/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700630643043,
                "cdate": 1700630643043,
                "tmdate": 1700630643043,
                "mdate": 1700630643043,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "s6EoiLDTdS",
            "forum": "e4wgZqF9uG",
            "replyto": "e4wgZqF9uG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1450/Reviewer_r9ta"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1450/Reviewer_r9ta"
            ],
            "content": {
                "summary": {
                    "value": "This paper explores leveraging monocular depth estimation as a pre-training objective and discusses its impact on downstream tasks of semantic segmentation. To validate the pre-training effectiveness, it takes ImageNet as pre-training baseline and three segmentation benchmarks (KITTI, NYU-V2, and Cityscapes) as downstream tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The idea of pre-training (representation learning) on the geometry task (monocular depth estimation) is novel. The authors prove that the pre-training of depth estimation is beneficial for downstream semantic segmentation.\n2. The quantitative results on the given three segmentation benchmarks are valid (KITTI, NYU-V2, Cityscapes), demonstrating the effectiveness of depth estimation as the pre-training objective."
                },
                "weaknesses": {
                    "value": "1. The authors provided many experimental results. However, many key details are missed.\n    -  How many training images are used for fine-tuning in the CityScape experiments in Section 4.2? Did the authors use all 20000 images for pre-training? Which backbone models are used for these experiments (ResNet18 or ResNet50)? \n    - Similar questions for Section 4.3 for the NYU-V2 dataset.\n    - Table 3 (COCO), I assume it reports the results over the testing set of KITTI (rather than COCO)? Also, which dataset do authors use for monocular depth pre-training (KITTI or COCO)?  Did all methods (Depth, MAE, DINO, MOCO v2) use the same pre-training data? Which backbone models are used for these experiments (ResNet18 or ResNet50)? These serve as important references for evaluation and reproduction, which are not clearly specified in this paper.\n 2. Where is Table 4.1 (Page 6)?\n 3. Potential unfairness in comparison. \n    - In Table 2, \"Depth\" used KITTI as pre-training set while testing on the same dataset. This comparison to the baseline (ImageNet pre-training) is unfair. This is because the training distribution is much closer to the testing distribution for \"Depth\". In contrast, ImageNet is characterized as object-centric and thus has a pretty large domain gap to KITTI collected by autonomous cars. What about performance comparison on COCO between ImageNet pre-train and Depth pre-train?\n    - In Table 3, the \"Supervised Segmentation\" performance should be results that trained on KITTI instead of MS-COCO if the authors used KITTI as pre-training and testing data. \n 4. Lack of analysis on representation transferability. For example, how is the performance of Depth Pre-train over other segmentation datasets such as COCO and ADE20K? How about transferring to object detection? Transferability is one of the major functions of pre-training. The authors should present relevant experimental results.\n 5. Despite the authors proving the effectiveness of pre-training with depth estimation, it still requires significant RGB-D data which cannot be collected online for large-scale pre-training data. Compared to self-supervised studies, e.g. MoCo-variants and MAE, this method has limited application scope."
                },
                "questions": {
                    "value": "Please see the weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1450/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1450/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1450/Reviewer_r9ta"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1450/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699360683832,
            "cdate": 1699360683832,
            "tmdate": 1699636073642,
            "mdate": 1699636073642,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PVDkkBfFjZ",
                "forum": "e4wgZqF9uG",
                "replyto": "s6EoiLDTdS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1450/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1450/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response to the initial review"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful review.\n\n**W1:** *How many training images are used for fine-tuning in the CityScape experiments in Section 4.2? Did the authors use all 20000 images for pre-training? Which backbone models are used for these experiments?*\n\n**R:** We will make it clear in the revised version. On Cityscapes, we use the default dataset partition for pre-training and finetuning, which includes 20000 binocular image pairs for depth, and 2975 images for semantic fine-tuning (page 8). We use ResNet 50 as the backbone model. On NYU-V2, we also use the dataset\u2019s default settings, which include 407024 images for depth training and 795 images for semantic segmentation fine-tuning.\n\n*Table 3 (COCO), I assume it reports the results over the testing set of KITTI (rather than COCO)?*\n\n**R:** The test set is KITTI. We initialize the backbone with COCO supervised segmentation and verify that monocular depth can close the performance gap between supervised pre-training on the same task and other pre-training tasks.\n\n*Also, which dataset do authors use for monocular depth pre-training (KITTI or COCO)? Did all methods (Depth, MAE, DINO, MOCO v2) use the same pre-training data? Which backbone models are used for these experiments (ResNet18 or ResNet50)? These serve as important references for evaluation and reproduction, which are not clearly specified in this paper.*\n\n**R:** The backbone is Resnet50, as in the captions of Table 3. Depth, optical flow, and MAE are trained in-domain, while dino and MOCO are trained on ImageNet since they are contrastive learning methods that require training on object-centric data. We did reproduce contrastive learning \"in-domain\" on KITTI but the results are not informative.\n\n**W2:** *Where is Table 4.1 (Page 6)?*\n\n**R:** It\u2019s a typo. We are referring to Table 2. Will correct.\n\n**W3:** *In Table 2, \"Depth\" used KITTI as pre-training set while testing on the same dataset. This comparison to the baseline (ImageNet pre-training) is unfair. This is because the training distribution is much closer to the testing distribution for \"Depth\". In contrast, ImageNet is characterized as object-centric and thus has a pretty large domain gap to KITTI collected by autonomous cars. What about performance comparison on COCO between ImageNet pre-train and Depth pre-train?*\n\n**R:** Please see general note for in-domain training. Nonetheless, this is an advantage of pretraining for depth. One does not need to restrict the pretraining data to a particular dataset, but can leverage any of structure-from-motion (video), binocular stereo, or measurements of depth sensor (time-of-flight) to do pretraining.  \n\n*In Table 3, the \"Supervised Segmentation\" performance should be results that trained on KITTI instead of MS-COCO if the authors used KITTI as pre-training and testing data.*\n\n**R:** The reviewer suggests performing supervised pre-training, and apply supervised fine-tuning on the same task and same dataset, which breaks the purpose of pre-training as one can simply used the supervised \u201cpretrained\u201d network for inference without finetuning. \n\n**W4:** *Lack of analysis on representation transferability. For example, how is the performance of Depth Pre-train over other segmentation datasets such as COCO and ADE20K? How about transferring to object detection? Transferability is one of the major functions of pre-training. The authors should present relevant experimental results.*\n\n**R:** Thanks to the reviewer for the suggestion. We are indeed investigating the performance of incorporating depth in pre-training in our follow-up work, especially on different semantic-related tasks. But domain adaptation is beyond the scope of the current paper. \n\n**W5:** *Despite the authors proving the effectiveness of pre-training with depth estimation, it still requires significant RGB-D data which cannot be collected online for large-scale pre-training data. Compared to self-supervised studies, e.g. MoCo-variants and MAE, this method has limited application scope.*\n\n**R:** Monocular depth can be inferred from videos without explicit depth data (i.e., self-supervised), as evidenced in our experiments on KITTI, where we refrain from using any ground-truth depth information. So, given recent advances in camera pose estimation, usage of large-scale online data is viable. However, we note that the compute required to train at that scale is prohibitive for this work, where we aim to conduct numerous controlled experiment to analyze the viability of depth pretraining under different settings. \n\nIt is worth mentioning that our intention is not to suggest replacing methods like MoCo or MAE with depth. Instead, we illustrate that depth achieves comparable performance. In real application, it might be advantageous to fine-tune MoCo and MAE using in-domain depth data for optimal results.\n\nWe hope this response answers the reviewer's question. If there are further concerns, we are more than willing to provide additional explanations."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1450/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700606322866,
                "cdate": 1700606322866,
                "tmdate": 1700606379012,
                "mdate": 1700606379012,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]