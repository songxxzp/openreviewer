[
    {
        "title": "Rank-adaptive spectral pruning of convolutional layers during training"
    },
    {
        "review": {
            "id": "LLlk06eiZ1",
            "forum": "6aRMQVlPVE",
            "replyto": "6aRMQVlPVE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6124/Reviewer_SS8V"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6124/Reviewer_SS8V"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a dynamic low-rank training method for convolutional neural networks that factorizes convolutions into tensor Tucker format and adaptively prunes the Tucker ranks of the convolutional kernel during training. The proposed method drastically reduces training costs while achieving high performance, outperforming competing low-rank approaches. The paper also includes a discussion of the geometric integration theory of differential equations on tensor manifolds used in this work. Empirical evaluation on CIFAR10 using VGG and Alexnet models also shows the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is well-written and easy to follow. The authors provide a clear motivation and a nice introduction to the problem. \n\n2. The connection to recent advance in dynamic low-rank approximation is interesting and indeed a nice direction to apply it to dynamic low-rank training. \n\n3. Strong theoretical understanding on the proposed method. The authors provide a clear derivation of TDLRT and its convergence analysis."
                },
                "weaknesses": {
                    "value": "1. Concerns on novelty. The proposed TDLRT is clearly an extension of matrix DLRT [1] on Tensor format. Although the authors emphasize this extension is non-trivial as it requires the use of Tucker decomposition and additional techniques for theoretical analysis, I believe the extension is not significant enough to be considered as a novel contribution. \n\n2. The paper lacks discussion on computational complexity during training. The authors claim the proposed method drastically reduces the training cost but the empirical results on training cost are missing. In Figure 2, the authors only show the computational footprint of the inference stage. However, for such dynamical compression method I believe training cost is more important to discuss. It would be great to show a figure similar to Figure 2 in [1] to visualize the training cost across different rank values. In Table 1, it also would be great to present the total training cost by different methods for a fair comparison.\n\n3. More details of proposed method are needed. For example, what initialization on low-rank factors and does it affect the performance? How to choose the threshold?\n\n4. Evaluation on large-scale datasets and SOTA models are needed. CIFAR-10 is too small to show the effectiveness of the proposed method, and VGG16 and Alexnet are outdated models. It would be great to evaluate methods on ImageNet at least using ResNet50.\n\n[1] Steffen Schotth\u00f6fer, Emanuele Zangrando, Jonas Kusch, Gianluca Ceruti, and Francesco\nTudisco. Low-rank lottery tickets: finding efficient low-rank neural networks via ma-\ntrix differential equations. In Advances in Neural Information Processing Systems,\n2022."
                },
                "questions": {
                    "value": "1. Would be great to provide more details on the training behavior of TDLRT, such as how rank evolves during training, convergence behaviors regarding various compression thresholds, etc.\n\n2. How often do you perform basis augmentation and tucker decomposition using SVD? If it is performed frequently for every step, does the numerical instability affects the convergence? such that, at the later stage of training, the perturbation on $C$ by learning algorithms may be too small compared to the numerical error induced by SVD.    \n\n3. Is there any way to further improve efficiency to make the method practical?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6124/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699575308087,
            "cdate": 1699575308087,
            "tmdate": 1699636662636,
            "mdate": 1699636662636,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4wyerYMGMc",
                "forum": "6aRMQVlPVE",
                "replyto": "LLlk06eiZ1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6124/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6124/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer, thank you for your feedback.  \n\n1. You state in your review that you \"believe the extension is not significant enough to be considered as a novel contribution\". We are unsure why you came to this conclusion and kindly ask you to please detail the reasons for this evaluation.   \nWe emphasize that the proposed method is conceptually different from that in [1]; it requires nontrivial additions and ideas for the mathematical analysis *and* the algorithm itself; and it performs significantly better than DLRT in [1]. Please notice that we have provided a detailed explanation on novelty in the general comment above.\nAs we pointed out in our paper, the proof of our Thms 2,3 is significantly different than the one of Thm 1,2 in [1]. Moreover, from an algorithmic point of view, directly extending [1] to Tucker format would require a computational cost of $O(\\sum_i n_i \\prod_{j\\neq i}r_j)$, which easily becomes prohibitive and poorly performing. \nCould you please clarify why you find the above novelties not significant enough?\n\n1. Re the lack of computational complexity: We agree and apologize for not adding these to the initial submission. We have provided details about computational complexity and training timing in the global (general) comment above.\n\n1.  In our experiments we use spectral initialization for low-rank layers, as it is standard in the factorized layer literature. A main strength of our method is that the initialization does not significantly affect convergence, accuracy or the compression rate. This lies in the fact that the careful construction of the method enables us to achieve convergence rates independent of the curvature of the low-rank manifold. Such a result is not achieved by other low-rank approaches which require carefully chosen initializations, though the DLRT method in [1] achieves the same theoretical result, however at prohibitive costs when being applied for tensors. The threshold needs to be chosen by the user and identifies how many singular values need to be kept. This choice depends on the architecture.\n\n1.   We would like to underline that we do present results for ResNet18/CIFAR10 as well.  Results on ImageNet are currently out of reach, given our limited computational resources. We are currently running tests on TinyImageNet and hope to have the results out soon.\n\nConcerning your question on whether is there any way to further improve efficiency to make the method practical: \n\nWe are not sure why you evaluate our method as not practical. As we show in our numerical experiments, TDLRT achieves high compression rates with accuracies comparable to the full-rank model while outperforming current low-rank training methods, including DLRT. There are certainly various approaches to further improve efficiency, including tensor decompositions other than Tucker, using a different framework with more efficient SVD solvers, making use of parallelism in the factor updates. However, the method outperforms current baselines and is already efficient and practical."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6124/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700079022460,
                "cdate": 1700079022460,
                "tmdate": 1700079022460,
                "mdate": 1700079022460,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "JpjIEDf57Q",
            "forum": "6aRMQVlPVE",
            "replyto": "6aRMQVlPVE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6124/Reviewer_T5Ao"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6124/Reviewer_T5Ao"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an adaptive compression methods for convolution layers using the Tucker decomposition. An interesting feature of the paper is the use of Riemannian geometric approach for tensor decomposition allowing to adaptive rank selection.  Experiments are reasonably good. The motivation to use Tucker decomposition over other methods is not clear. Further, the methods employed may not be the optimal decomposition approach."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1) The problem is useful and relevant.\n\n2) Motivation experimental results."
                },
                "weaknesses": {
                    "value": "1) The proposed method is incremental work and lack strong novelty.\n\n2) All possible tensor decomposition methods are not considered such as tensor-train. See question 1."
                },
                "questions": {
                    "value": "1) Though Tucker based decomposition of the convolution layers seems to work well, I wonder if tensor-train is better decomposition [1] for this setting. Tensor-train decomposition  avoids the core tensor, while uses fewer ranks compared to Tucker.  Is there a specific advantage of the proposed method over Tucker? Adaptive rank based learning has also been explored with  Riemannian geometry [2].\n\n2) What are the actual training times compared with the proposed model and baseline methods?\n\n[1] A I. V. Oseledets.  Tensor-Train Decomposition. 2011, SIAM Journal on Scientific Computing, 2295-2317\n\n[2] Michael Steinlechner, Riemannian Optimization for High-Dimensional Tensor Completion,  2016,  SIAM Journal on Scientific Computing"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6124/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6124/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6124/Reviewer_T5Ao"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6124/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699872432296,
            "cdate": 1699872432296,
            "tmdate": 1699872473549,
            "mdate": 1699872473549,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "U53jvFczX5",
                "forum": "6aRMQVlPVE",
                "replyto": "JpjIEDf57Q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6124/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6124/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer, \nthank you for your feedback, comments, and questions. \n\nWe agree that the question of which low-rank tensor format to use is a pressing and interesting one. \nAs also clearly stated in [2],  the Tucker tensor format is the typical choice for problems involving tensors with $d\\leq 5$ modes. Since convolution layers of neural networks are typically order 4, or order 5 tensors, for 2D, respectively 3D convolutions, it is in our view natural to consider Tucker tensors. \nCertainly, it is interesting to consider dynamical low-rank training for higher-order tensor structures using Tensor Trains. However, we remark that it is nontrivial to extend the approach we developed here to Tensor Train format. Thus, we consider it a potential and nontrivial future research direction. \n\nAs asked, we would like to point out the major differences with [2]:\n\n- Paper [2] focuses on tensor completion while our algorithm is for neural network training\n- Paper [2] uses Riemannian gradients whereas we propose  a splitting method to integrate the gradient system given by the training problem. Our splitting scheme allows us to integrate each sub-problem (on each factor) efficiently, without needing to expensively compute the whole gradient (please notice also the general comment above); Instead, **computing the Riemannian gradient in general requires to compute the full gradient followed by a projection onto the tangent plane.**  This operation can be very costly and its potential bottleneck for neural network training is not addressed in [2]. Instead, the paper [2] focuses specifically on Tensor completion, which is known to be a particularly favorable setting for computing Riemannian gradients. \n\n- Concerning the rank adaptive strategy proposed in [2], our understanding is that they first run the optimization algorithm until convergence for an initially chosen small rank, and then use the optimizer as a starting point for a new run of the algorithm with an increased rank. This is repeated for several ranks. This approach is used also by other authors, also in the transfer learning community. It certainly is a possibility but has the potential bottleneck of requiring the full training of the network for each of the chosen ranks, which may be computationally unfeasible. We also emphasize that our choice of rank adaptation has the theoretical advantage of approximating the full model, as stated in our Theorem 3 in the submitted paper. This result is not available for the rank adaptive strategy used by [2]."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6124/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700151268638,
                "cdate": 1700151268638,
                "tmdate": 1700151295908,
                "mdate": 1700151295908,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "yDjXUDycXs",
            "forum": "6aRMQVlPVE",
            "replyto": "6aRMQVlPVE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6124/Reviewer_AtDm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6124/Reviewer_AtDm"
            ],
            "content": {
                "summary": {
                    "value": "To reduce the cost of training convolutional neural networks, this work proposes a low-parametric training method that combines tensor Tucker format and adaptive pruning of Tucker ranks for convolutional layers. Noticeably, the method allows for the adaptivity of the rank of convolutional filters depending on compression rate and robustness for their condition. The authors verify the method's effectiveness by comparing the proposed method with the full training and baselines and observe the cost is reduced due to the proposed method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Reducing the training cost of neural networks has attracted much interest, and many papers proposed the method aiming at practical training with less memory consumption. However, most methods focus on the training cost of a fully connected layer and cannot apply to the convolutional layers. Out of all, the proposed method TDLRT gives the efficiency of training convolutional layers."
                },
                "weaknesses": {
                    "value": "- TDLRT is complicated compared to vanilla SGD. Indeed, the method needs to compute QR decomposition, the projection, etc, causing significant additional costs. Taking this additional computational cost, it is unclear how computationally efficient TDLRT is.\n\n- Theorem 2 does not necessarily indicate the convergence of loss. And there is no comparison with the other method regarding the convergence rate and complexity."
                },
                "questions": {
                    "value": "- It would be nice if the authors could discuss the computational cost per iteration. The computational complexity should also be considered for a fair comparison with SGD and competitors.\n\n- Is it possible to show the convergence of the loss function by training with TDLRT?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6124/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6124/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6124/Reviewer_AtDm"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6124/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699893631063,
            "cdate": 1699893631063,
            "tmdate": 1699893631063,
            "mdate": 1699893631063,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6xzmD0ABfb",
                "forum": "6aRMQVlPVE",
                "replyto": "yDjXUDycXs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6124/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6124/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer, thank you for your comments. \n\n**Re the weakness about complications added by TDLRT:**\n\nWe appreciate we did not provide enough details about this important point in the paper and we will add them in the new version. \nIn particular, we have added details about computational complexity per iteration and training times in the global response as a general comment above. \nTDLRT is a robust low-rank optimization method to train a neural network exploiting manifold information while automatically adapting the ranks of each layer during training. This key property comes a the expense of some methodological and computational complexity. However, **this is in our view a strength not a weakness**. \nOne of our main points is exactly that if you perform a \"naive\" training on the low-rank manifold using a factorized representation of weights and gradients, you struggle to achieve competitive compression/performance ratios, while with the proposed Riemannian scheme one achieves much better performance. The more \"complicated\" method requires more expensive iterations, but then allows us to drastically reduce the overall cost of training as compared to vanilla factorized SGD, since the method converges much faster (see also Figure 2 in the main manuscript and the new timing tables in the general comment). *Factoring in the additional cost of 5.6 (best case) to 8.3 (worst case) times longer convergence duration of the vanilla approach, the most expensive version of TDLRT is up to 30\\% faster. The non-adaptive TDLRT method is up to 4 times faster than the vanilla method when factoring in convergence time.*\n\n**Re the convergence of the method:**\n\nYou are correct, Theorem 3 only guarantees that we will be close to a local optimum, if the baseline converges. Deriving a stronger convergence result might require a completely different analysis. We believe that under certain conditions especially on the learning rate such a result can be derived. Indeed, we are aware of ongoing work in this direction for the matrix DLRT case by Arsen Hnatiuk and Andrea Walther. In their analysis, the Robins-Monro conditions on the learning rate need to be assumed. Moreover, the full gradient projected onto the range of the basis at iteration k needs to be bounded by a positive constant plus the projected gradient at the previous time step. We are currently unsure if and how this result extends to the tensor case, but we believe this is a very interesting research direction.\n\n**Re computational complexity:**   \n\nWe kindly refer to the general response in the global comment above."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6124/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700079316161,
                "cdate": 1700079316161,
                "tmdate": 1700079316161,
                "mdate": 1700079316161,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]