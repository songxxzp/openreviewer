[
    {
        "title": "Language Conditioned Equivariant Grasp"
    },
    {
        "review": {
            "id": "HjXALpvTFn",
            "forum": "PfAqPxPsAj",
            "replyto": "PfAqPxPsAj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5778/Reviewer_fe34"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5778/Reviewer_fe34"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the challenge of learning a robotic policy to grasp objects based on natural language instructions. The authors introduced a Language-conditioned Equivariant Grasp (LEG) method that leverages CLIP features to align image and text observations and SO(2)-steerable kernels to improve sample efficiency. The effectiveness of LEG is tested on the Language-Grasp Benchmark, which consists of 10 varied language-driven grasping tasks. The method's efficiency and performance are also evaluated on an actual robot."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper leverages a general frame to leverage the symmetry of language-conditioned grasping.\n- The paper presented a dynamic kernel generator that maps language instructions to steerable kernels with rotation symmetry.\n- The paper proposed a new grasping benchmark with ten categories of language conditions and corresponding expert demonstrations.\n- The proposed methods show strong grasping performances on the proposed benchmark."
                },
                "weaknesses": {
                    "value": "- The paper claims that the proposed inductive bias leads to high sample efficiency, but there is no direct evidence in the experiments.\n- Neural Descriptor Field [1] is an SE(3) equivariant grasping method with a similar design to the proposed method. Please cite and analyze the similarities and differences between this paper and [1].\n- A large body of recent papers (to name a few, [2-4]) in language-conditioned grasping are not included for comparison and analysis.\n- The writing of this paper is unclear and uncareful.\n    - The experiment section mentioned designing and obtaining a reward, suggesting that the proposed method is RL-based. However, there is no mention of reinforcement learning or any related formulation in the method section, except naming $p(a_t|o_t,l_t)$ as a policy once.\n    - The paper did not explain how the model is trained. If it is an RL-based method, what are the observations and rewards, the algorithm used, and what hyperparameters are used? If not, what is the loss function, and how is it trained?\n    - This paper has multiple typos; for example, at the end of page 3, there is a `\\leq`, which I believe should be `\\leg`.\n\n[1] Simeonov, Anthony, et al. \"Neural descriptor fields: Se (3)-equivariant object representations for manipulation.\"\u00a0*2022 International Conference on Robotics and Automation (ICRA)*. IEEE, 2022.\n\n[2] Xu, Yinzhen, et al. \"Unidexgrasp: Universal robotic dexterous grasping via learning diverse proposal generation and goal-conditioned policy.\"\u00a0*Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*. 2023.\n\n[3] Sharma, Satvik, et al. \"Language embedded radiance fields for zero-shot task-oriented grasping.\"\u00a0*7th Annual Conference on Robot Learning*. 2023.\n\n[4] Chen, Yiye, et al. \"A joint network for grasp detection conditioned on natural language commands.\"\u00a0*2021 IEEE International Conference on Robotics and Automation (ICRA)*. IEEE, 2021."
                },
                "questions": {
                    "value": "- The introduction mentioned, \u201cdirectly interleaving language features with image features breaks the geometric symmetries underlying the optimal policy\u201d. Please elaborate on why interleaving features break symmetry.\n- How are the gripper position and orientation obtained from the output feature map?\n- The description of section 4.1 (For example, if there are 4 toys presented in the workspace, each successful grasp will be credited a reward of 0.25. The successful grasp is defined as the grasp lifting the object and satisfying the language goal. A maximum of n + 1 grasping trials is set for each task.) indicates that there are N objects in a scene, and the goal is to pick up each object individually, according to its language description. Only when all objects are picked up successfully can it obtain the full reward. Is my understanding correct?\n- How is the grasping-by-part success rate measured? Is it by manual inspection or by some automated function?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5778/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5778/Reviewer_fe34",
                        "ICLR.cc/2024/Conference/Submission5778/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5778/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698393758866,
            "cdate": 1698393758866,
            "tmdate": 1700641444158,
            "mdate": 1700641444158,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bFdtHf47Az",
                "forum": "PfAqPxPsAj",
                "replyto": "HjXALpvTFn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5778/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5778/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We express our gratitude to the reviewer for their valuable feedback on our manuscript. Here is our detailed response to the concerns raised. We believe that we have addressed all the major concerns that the reviewer had. We are happy to address more concerns given a score of 3.\n\n## Weakness and Response\n\n### Weakness 1:\n\n*The paper claims that the proposed inductive bias leads to high sample efficiency, but there is no direct evidence in the experiments.*\n\n### Response 1:\n\nWe respectfully disagree with the point raised. As shown in Tables 1 and 2, our proposed LEG-UNET and LEG-Cliport models outperform four strong baselines in 25 out of 26 testing scenarios, trained with either 1 or 10 demonstrations. Notably, our models trained with just a single demonstration can surpass the baselines trained with 10 demonstrations in the 'pick-tool' and 'pick-toy' tasks. Furthermore, they achieve a success rate above 90% in 6 out of 13 tasks. It is important to note that we trained our models exclusively on 'pick-random-v1' or 'pick-random-v2' and tested them on other V-1 or V-2 tasks.\n\n### Weakness 2:\n\n*Neural Descriptor Field [1] is an SE(3) equivariant grasping method with a similar design to the proposed method. Please cite and analyze the similarities and differences between this paper.*\n\n### Response 2:\n\nWhile NDF is commendable work, we believe that our design is not similar to it. Firstly, our focus is on language-conditioned grasping, learning directly from pixel space, whereas NDF does not incorporate language instructions and learns from point clouds. Secondly, our approach does not assume objectness, segmentation, or categorization, in contrast to NDF, which requires a segmentation module and learns a category-level pick-and-place policy. Thirdly, our method does not require any pretraining process, while NDF necessitates pretraining to acquire point descriptors. Although both methodologies learn from demonstrations and leverage task symmetries, it is intriguing why the reviewer perceives them as similar.\n\n### Weakness 3:\n\n*A large body of recent papers (to name a few, [2-4]) in language-conditioned grasping are not included for comparison and analysis.*\n\n### Response 3:\n\nIn the revised related work section, we compare our proposed method with several recent works:\n\n1. **Unidexgrasp** learns dexterous grasping from point cloud observations. Its approach to language-guided grasping involves projecting grasp proposals onto images and utilizing CLIP to compute the similarity between text and images. In contrast, our work is centered on language-conditioned grasping and proposes an end-to-end learning framework that effectively leverages task symmetries.\n2. **Language-embedded Radiance Fields for Zero-shot Task-oriented Grasping** represents a very recent development. This method integrates language embeddings into the radiance field, highlighting regions according to language instructions. However, it uses GraspNet for grasp detection, and does not involve a learning process for grasping.\n3. **A Joint Network for Grasp Detection Conditioned on Natural Language Commands** combines the visual vectors of sampled grasp regions with language vectors using element-wise products to regress the top-down grasp pose. Unlike our approach, this network is unable to generate a comprehensive distribution across the entire action space and cannot predict multiple optimal grasp candidates when they are available.\n\nThese comparisons highlight the unique aspects and strengths of our approach in the context of the evolving landscape of language-conditioned robotic grasping. Here we provide a table (Table R3) to illustrate the key differences between these methods and ours. \n\n| TABLE R3. algorithm | language-conditioned | few-shot learning | extra-labeling-free | part grasping | end-to-end | Action space |\n| --- | --- | --- | --- | --- | --- | --- |\n| NDF[5] | \u2718 | \u2713 | \u2718 | \u2713 | \u2713 | 6-DoF |\n| Unidexgrasp[2] | \u2718 | \u2718 | \u2713 | \u2718 | \u2718 | 6-DoF |\n| LERF-TOGO [3] | \u2713 | \u2718 (zero-shot) | \u2713 | \u2713 | \u2718 | 6-DoF |\n| CGNet[4] | \u2713 | \u2718 | \u2713 | \u2718 | \u2713 | 4-DoF |\n| KITE [5] | \u2713 | \u2713 | \u2718 (keypoint) | \u2713 | \u2718 | 6-DoF |\n| CROG [6] | \u2713 | \u2718 | \u2718 (seg mask) | \u2718 | \u2713 | 4-DoF |\n| CLIPort [8] | \u2713 | \u2713 | \u2713 | \u2718 | \u2713 | 4-DoF |\n| LEG (ours) | \u2713 | \u2713 | \u2713 | \u2713 | \u2713 | 4-DoF |"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5778/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700587976557,
                "cdate": 1700587976557,
                "tmdate": 1700587976557,
                "mdate": 1700587976557,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GPeQmU2E68",
                "forum": "PfAqPxPsAj",
                "replyto": "HjXALpvTFn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5778/Reviewer_fe34"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5778/Reviewer_fe34"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the response. The authors have addressed most of my concerns. However, I still believe that the writing of this paper makes it more difficult to follow than it should be. Based on that, I am raising my rating to 5 and lowering my confidence to 2. I am lowering my confidence because the main reason that I give a negative rating is for its writing. I'd be happy to raise my rating to 6 if this paper were reader-friendly."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5778/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700641428539,
                "cdate": 1700641428539,
                "tmdate": 1700641428539,
                "mdate": 1700641428539,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FUF9UWnTTk",
            "forum": "PfAqPxPsAj",
            "replyto": "PfAqPxPsAj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5778/Reviewer_n2g1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5778/Reviewer_n2g1"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new 2D equivariant grasp selection algorithm that's language conditioned. After discretizing output action to different buckets of angles, the paper propose to lifting 2D feature map to Fourier domain, in which conditioning embeddings like CLIP features are multiplied. The paper demonstrates the effectiveness of the method on simulated benchmarks as well as real robots. The paper also contributes a new benchmark on language conditioned grasps featuring 10 environments."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The evaluation is solid by combining a variety of baselines, environments. It also features real-world grasping experiments featuring unknown objects. \n- Previous method achieves equivariance at global observation level (rotating entire observation) while this paper focuses on local symmetry. \n- Writing is clear and easy to understand"
                },
                "weaknesses": {
                    "value": "- The language part and equivariant part are not directly related to equivariance itself. I think what the paper tries to claim is finding a better way to condition and ground constrained networks here. However, starting with language conditioning can mislead the readerss to focus on the wrong thing. \n- No equivariant baselines that simply takes in CLIP features by concatenation. If previous equivariant grasp models are not applicable here, please explain clearly why concatenation doesn't work."
                },
                "questions": {
                    "value": "1. Do you have results for baselines in real-world experiments?\n2. I am wondering how would the performance change according to the number of grasp bins.\n3. In the writing, I hope the authors can state what's the input and output of the mapping that's equivariant to define the problem in a clearer way."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5778/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5778/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5778/Reviewer_n2g1"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5778/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698831432596,
            "cdate": 1698831432596,
            "tmdate": 1699636607521,
            "mdate": 1699636607521,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hQPyaPMyAU",
                "forum": "PfAqPxPsAj",
                "replyto": "FUF9UWnTTk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5778/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5778/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for an especially detailed and thorough review. We provide a point-by-point response to the comments and questions below.\n\n## Weakness and Response\n\n### Weakness 1:\n\n*The language part and equivariant part are not directly related to equivariance itself. I think what the paper tries to claim is finding a better way to condition and ground constrained networks here. However, starting with language conditioning can mislead the readers to focus on the wrong thing.*\n\n### Response 1:\n\nThe primary focus of our work is to investigate efficient representations for grounding language in language-conditioned grasping tasks. As such, we place a strong emphasis on the language-conditioned steerable kernel, namely the language grounding aspect. Key points include:\n\n1. **Language-Conditioned Policy**: Our policy is language-conditioned. This means that for a given image, the action distribution changes according to the language goal.\n2. **SE(2) Equivariance and Dynamic Steerable Kernel**: The SE(2) equivariance, as stated in Equation 2, is achieved through a dynamic steerable kernel derived from the pre-trained language embedding. The process of generating this steerable kernel from the language embedding is a learnable aspect of our model.\n3. **Steerable Kernel Generation**: To clarify, unlike previous approaches that may rely on E2CNN[1], our network does not constrain itself to achieve equivariance through such means. Instead, we generate the steerable kernel via lifting, a process detailed and validated in Appendix A.2.3.\n\nOur approach demonstrates a novel method of integrating language grounding with action planning in robotic tasks, leveraging the unique capabilities of dynamic steerable kernels.\n\n### Weakness 2:\n\n*No equivariant baselines that simply takes in CLIP features by concatenation. If previous equivariant grasp models are not applicable here, please explain clearly why concatenation doesn't work.*\n\n### Response 2:\n\nExcellent question. There are several key points to consider regarding the integration of language features with visual features:\n\n1. **Local Symmetry**: Merely attaching the same language feature to each pixel is not sufficient to achieve local symmetry. This approach overlooks the complexity of how language features interact with spatially varying visual features.\n2. **Inconsistency**: The direct concatenation of the language feature (trivial representation which is no change if there is a rotation) with equivariant visual feature (commonly, a regular representation which permutes if there is a rotation on the image) is not consistent.\n3. **Data Augmentation**: Our baseline model, FCGQ, employs the UNet architecture. In this model, the hidden visual feature is concatenated with the language feature in the bottleneck layer. Moreover, the model's training involves extensive data augmentation, aligning with the variation suggested by the reviewer. The results in Table 1&2 show that our method with steerable kernels outperforms the concatenation baseline by a large margin."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5778/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700586459611,
                "cdate": 1700586459611,
                "tmdate": 1700586459611,
                "mdate": 1700586459611,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YZPAC3AHGf",
                "forum": "PfAqPxPsAj",
                "replyto": "FUF9UWnTTk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5778/Reviewer_n2g1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5778/Reviewer_n2g1"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the clarification.\n\nI have carefully read through the comments by peer reviewers as well as your rebuttal. It seems that there are still shared concerns about baselines among all reviewers. Your response 2 to my weakness 2 still seems insufficient to convince me without additional experiments.\n\nI'd maintain my score here."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5778/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700724700141,
                "cdate": 1700724700141,
                "tmdate": 1700724700141,
                "mdate": 1700724700141,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "nbs4an425x",
            "forum": "PfAqPxPsAj",
            "replyto": "PfAqPxPsAj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5778/Reviewer_vD3Z"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5778/Reviewer_vD3Z"
            ],
            "content": {
                "summary": {
                    "value": "In this work, the authors propose a novel method to train language-conditioned grasping policies that are equivariant under the SE2 symmetry. To achieve this, the authors first divide the inputs into two encoders: One processes both language and RGB to obtain pixel-wise attention, and another part; Another one to expand language embedding to a steerable kernal which satisfies the equivariant constraints. Then the steerable kernal and the pixel-wise attention are \"fused\" together through cross correlation and inverse fourier transform to produce the action activations. The authors test the proposed method on diverse 2D pickup tasks and show that, their method can outperform naive baselines with only a few demonstrations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1) The proposed method is quite novel as it leverages the symmetry that exists in robotic problems. \n2) The method also demonstrate good 1-shot or few shot performances comparing with baselines."
                },
                "weaknesses": {
                    "value": "1) It is rather strange to see that rotation is applied to the language embedding, but not the image space...Need an explanation on this.\n2) Also, CLIP language features may not the best representation of actions such as \"pick\", \"move\" etc. Since the policy is grasping only it may not impose a bottlneck as of now.\n3) Need some discussions on why the whole policy, not just the steerable kernel, is also equivariant under the SE2 symmetric group. \n4) SE2 also includes a translation part, but I don't see that the steerable kernel is equivariant under translation.\n5) In the paper it is not clear if there is a pre-training phase. It is hard to believe that the network weights (of \\phi, and psi) are properly learned with just one trajectory if they are always randomly initialized from the beginning."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5778/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699237543455,
            "cdate": 1699237543455,
            "tmdate": 1699636607375,
            "mdate": 1699636607375,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CwBrj7Djac",
                "forum": "PfAqPxPsAj",
                "replyto": "nbs4an425x",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5778/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5778/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We express our gratitude to the reviewer for their valuable feedback on our manuscript. Here is our detailed response to the concerns raised:\n\n## Weakness and Response\n\n### Weakness 1:\n\n*It is rather strange to see that rotation is applied to the language embedding, but not the image space...Need an explanation on this.*\n\n### Response 1:\n\nExcellent question. The direct way to apply rotation to image space is to feed a stack of rotated images to the network. It is computationally expensive compared with the use of a set of rotated kernels or steerable kernels, especially when the number of rotations is large. The language-conditioned steerable kernel is more manageable and efficient. Theoretically, we can derive from the **commutative** property of the convolution action $a*b=b*a$, where a, b denotes two feature maps and $*$ denotes convolution. Further, with group action $g \\in SE(2)$, $(g\\cdot a)*b = a*(g\\cdot b)$.\n\n### Weakness 2:\n\n*Also, CLIP language features may not the best representation of actions such as \"pick\", \"move\" etc. Since the policy is grasping only it may not impose a bottleneck as of now.*\n\n### Response 2:\n\nGood observation. We used CLIP because its text encoder is trained to align with the image distribution, which makes it a motivation for some recent visual-text tasks in robotics [1][7]. However, as mentioned in section 3.2.2, our proposed framework is suitable to fit any pre-trained language encoder or VLM. Finding a better VLM model suitable for robotics tasks could be a potential future work. Moreover, since in this work the only robot action is \u201cgrasping\u201d, if the language-conditioned kernel can learn to extract the object type from the CLIP text embedding, it is sufficient to perform correct grasping. Our experiments successfully demonstrate such learning ability of our language-conditioned kernel.\n\n### Weakness 3:\n\n*Need some discussions on why the whole policy, not just the steerable kernel, is also equivariant under the SE2 symmetric group. SE2 also includes a translation part, but I don't see that the steerable kernel is equivariant under translation.*\n\n### Response 3:\n\nGood question. The rotation symmetry is realized by the languaged-conditioned dynamic steerable kernel. The translational symmetry is realized by the cross-correlation action, which means that convolution actions are naturally equivariant with respect to translation before adding the additional SO(2)-equivariant property. Detailed explanations can be found in Appendix A.2.2.\n\n### Weakness 4:\n\n*In the paper it is not clear if there is a pre-training phase. It is hard to believe that the network weights (of $\\phi$, and $\\psi$ ) are properly learned with just one trajectory if they are always randomly initialized from the beginning.*\n\n### Responses 4:\n\nWe do not engage in any pre-training. Our approach solely utilizes the pre-trained CLIP model. Specifically, for LEG-UNET, we utilize only the CLIP text encoder. In the case of LEG-Cliport, both the CLIP image encoder and text encoder are employed. Details regarding our training methodology and network specifications can be found in Section 4.3 and Appendix A.3, respectively.\n\nFor few-shot learning, equivariant networks are noted for their fast convergence and enhanced sample efficiency. This has been substantiated by multiple studies, as referenced in [1], [2], [3], [4], [5], and [6].\n\nIn our analysis, we focused on the symmetry inherent in the tasks and proposed a comprehensive framework that incorporates a dynamic steerable kernel. This framework is designed to exploit task symmetry, thereby achieving improved sample efficiency. We consider this ability to leverage symmetry as a significant strength of our work, rather than a limitation.\n\n## References\n\n[1] Zhu, Xupeng, et al. \"Sample Efficient Grasp Learning Using Equivariant Models.\" Robotics: Science and Systems. 2022.\n\n[2] Huang, Haojie, et al. \"Equivariant Transporter Network.\" Robotics: Science and Systems. 2022.\n\n[3] Jia, Mingxi, et al. \"Seil: Simulation-augmented equivariant imitation learning.\" 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2023.\n\n[4] Wang, Dian, Robin Walters, and Robert Platt. \"SO (2)-Equivariant Reinforcement Learning.\" International Conference on Learning Representations. 2022.\n\n[5] Wang, Dian, et al. \"Equivariant $ q $ learning in spatial action spaces.\" Conference on Robot Learning. PMLR, 2022.\n\n[6] Yang, Jingyun, et al. \"EquivAct: SIM (3)-Equivariant Visuomotor Policies beyond Rigid Object Manipulation.\" arXiv preprint arXiv:2310.16050 (2023).\n\n[7] Tang, Chao, et al. \"Task-Oriented Grasp Prediction with Visual-Language Inputs.\" arXiv preprint arXiv:2302.14355 (2023)."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5778/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700585329213,
                "cdate": 1700585329213,
                "tmdate": 1700585329213,
                "mdate": 1700585329213,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mr9RkSYNf6",
            "forum": "PfAqPxPsAj",
            "replyto": "PfAqPxPsAj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5778/Reviewer_C3bk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5778/Reviewer_C3bk"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a method called Language-conditioned Equivariant Grasp (LEG), which utilizes language instruction to guide robotic grasping. It presents a language-conditioned grasp network with a dynamic kernel generator to showcase the effectiveness of LEG on the Language-Grasp Benchmark that includes expert demonstrations. This benchmark comprises 10 language-conditioned grasping tasks in a simulated environment."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The proposed method, Language-conditioned Equivariant Grasp (LEG), leverages the symmetries of language-conditioned robotic grasping by mapping the language instruction to a steerable kernel.\n- The authors analyze the symmetry of language-conditioned grasp and propose a general framework to leverage it.\n- The proposed method achieves high sample efficiency and grasping performance in both simulated and real-world robot experiments."
                },
                "weaknesses": {
                    "value": "- The mainstream of grasping research is in SE(3), but this paper remains in SE(2).\nThis paper seems to have conducted implicit pose estimation or simply learned the rotation angles of objects. However, it appears to lack novelty.\n- Real-world experiments are limited to a single task (pick-by-part) and a small set of objects. Some of the experimental objects are not sufficient to demonstrate effectiveness, as they are mostly cylindrical in shape. As long as the object is between the grippers, closing the grippers will definitely be able to grasp it.\n- There are some obvious typos in the article, like the third line from the bottom of Section 4."
                },
                "questions": {
                    "value": "Could you explain the reason for adding language embedding in the \u03c6 branch? It seems to be redundant and does not have significant impact on performance."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5778/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5778/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5778/Reviewer_C3bk"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5778/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699351118446,
            "cdate": 1699351118446,
            "tmdate": 1699636607278,
            "mdate": 1699636607278,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "T0n2foqFxu",
                "forum": "PfAqPxPsAj",
                "replyto": "mr9RkSYNf6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5778/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5778/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We express our gratitude to the reviewer for their valuable feedback on our manuscript. Here is our detailed response to the concerns raised:\n\n## Weakness and Response\n\n### Weakness 1:\n\n*The mainstream of grasping research is in SE(3), but this paper remains in SE(2). This paper seems to have conducted implicit pose estimation or simply learned the rotation angles of objects. However, it appears to lack novelty.* \n\n### Response 1:\n\nWe respectfully disagreed with the reviewer that this is the weakness of our method.\n\n1. The mainstream of SE(3) grasping does not indicate the lack of novelty of all SE(2) grasping currently. Language-conditioned grasping is being actively researched but still under-explored. As the other reviewers point out, there exist very recent publications such as [1][5] on SE(2) language-conditioned grasping. Most methods rely on segmentation [1, 2], cross-attention [5], or direct concatenation of language features and visual features [3, 4, 6]. To the best of our knowledge, our work is the first method that grounds language using language-condition steerable kernels, and we show its effectiveness compared with a strong baseline [6]. We are happy to address further concerns if the reviewer can kindly provide some related state-of-the-art work on language-conditioned grasping.\n2. Our method, i.e., language-conditioned dynamic kernel, is straightforward to be extended to 3D to solve SE(3) grasping using 3D kernels, which is based on irreps of SO(3), i.e., Wigner-D matrices. This change is secondary compared to our approach on integrating language-conditioning.\n3. To clarify, grasping is inherently a harder task compared to pose estimation. Previous work has demonstrated the complexity of the grasping problem [7, 8]. For example, a 6-Dof pose of a bowl does not necessarily provide information about where to grasp. And, object-level pose estimation methods cannot do our pick-part task. \n\n### Weakness 2:\n\n*Real-world experiments are limited to a single task (pick-by-part) and a small set of objects. Some of the experimental objects are not sufficient to demonstrate effectiveness, as they are mostly cylindrical in shape. As long as the object is between the grippers, closing the grippers will definitely be able to grasp it.*\n\n### Response 2:\n\nFirst, we chose the pick-part task for our real-world experiments because it represents a challenging yet intriguing aspect of robotic grasping. This task requires the model not only to pick the object but also to differentiate specific parts of it. And, we provide three metrics and experiments with different numbers of demonstrations to show the effectiveness of our method. We believe, given the difficulty of the task and various evaluations, our physical experiments are sufficient to demonstrate the effectiveness of the real-world applicability of our method.\n\nSecond, we respectfully disagree with the cylindrical grasping argument. We have many objects that are not cylindrical. Take the bowl as an example; it is not in a cylindrical shape. The model needs to find the rim and reason the right angle of picking at a certain point on the rim. Meanwhile, simply grasping the estimated pose of the object will lead to failed grasping. \n\nFurthermore, beyond grasping, the model also needs to select the right object given language instruction. The model not only needs to pick the object but also needs to differentiate specific parts of it."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5778/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700585139218,
                "cdate": 1700585139218,
                "tmdate": 1700585139218,
                "mdate": 1700585139218,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "eRIaaES5ng",
            "forum": "PfAqPxPsAj",
            "replyto": "PfAqPxPsAj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5778/Reviewer_FTbR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5778/Reviewer_FTbR"
            ],
            "content": {
                "summary": {
                    "value": "The paper aims to tackle the problem of 2D grasping conditioned on language instructions. To be more specific, the language instructions are provided as grasping an object by the name, color or a specific part and they use behavior cloning to learn the grasping policy from language annotated demonstrations. Moreover, in order to incorporate language into the grasping policy in a way that exploits the symmetry and geometry in the task, the authors proposed a framework that maps the language instruction to a SO(2) steerable kernel. They demonstrate the effectiveness of their method on their proposed grasping benchmark in the simulation and on the real robot."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The author proposed a novel dynamic kernel that maps the language instructions and has provided theoretical guarantee for the rotation symmetry.  The paper also shows its strength of high sample efficiency and better grasping performance both in simulation and on real world. The author also proposed a grasping simulation benchmark annotated with language instructions that can be used for future work on 2d grasping."
                },
                "weaknesses": {
                    "value": "The author fails to compare their method against some most recent work in language conditioned grasping. For example, this work[1] also has the task of semantic grasping(grasping some object by some particular parts). Moreover, this paper only focuses on 2D grasping with language instructions but some objects may not be graspable from just a 2d top-down grasp. Some work[2] have also shown their method on 4D grasping in cluttered scene with language instructions. \n\nIn the real world experiment, the tested objects and variations are very limited compared to the simulation. It is more convincing to show more objects in the real world experiments and potentially show the results of the different methods not just LEG-Unet in the real world.\n\n[1]Sundaresan, P., Belkhale, S., Sadigh, D., & Bohg, J. (2023). Kite: Keypoint-conditioned policies for semantic manipulation. arXiv preprint arXiv:2306.16605.\n\n[2]Tziafas, G., Yucheng, X. U., Goel, A., Kasaei, M., Li, Z., & Kasaei, H. (2023, August). Language-guided Robot Grasping: CLIP-based Referring Grasp Synthesis in Clutter. In 7th Annual Conference on Robot Learning."
                },
                "questions": {
                    "value": "1. The author proposed a grasping benchmark in pybullet simulation with objects imported from YCB and GraspNet Dataset with language annotations. However, the paper doesn't compare the diversity and the complexity of the grasping objects in this grasping benchmark  with other grasping benchmark commonly used by the robotic grasping research. It would be more convinced to have a table with comparisons with other grasping dataset/benchmarks.\n2. The current language instructions used in the experiments are provided in the appendix. I am wondering if this method can generalize to other language instructions that have the similar meaning but don't exist in the training dataset or the language instructions that are in the free-form. \n3. For the experiment section, the authors mention that LEG-Unet is better than LEG-CLIPort on most tasks. Could the authors provide more explanation for this result?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5778/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5778/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5778/Reviewer_FTbR"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5778/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699593491351,
            "cdate": 1699593491351,
            "tmdate": 1699636607176,
            "mdate": 1699636607176,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GjcTEebAJa",
                "forum": "PfAqPxPsAj",
                "replyto": "eRIaaES5ng",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5778/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5778/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We express our gratitude to the reviewer for their valuable feedback on our manuscript. Here is our detailed response to the concerns raised:\n\n## **Weakness and Response**\n\n### Weakness 1:\n\n*The author fails to compare their method against some most recent work in language conditioned grasping. For example, this work[1] also has the task of semantic grasping(grasping some object by some particular parts). Moreover, this paper only focuses on 2D grasping with language instructions but some objects may not be graspable from just a 2d top-down grasp. Some work[2] have also shown their method on 4D grasping in cluttered scene with language instructions.*\n\n### Response 1:\n\n- We appreciate the suggestion and have updated the related work section to include a comprehensive comparison with all relevant studies known to us.\n- We clarify that KITE[1] is a 3D policy network, primarily focusing on aspects other than grasping. The key pixel identification mechanism in KITE aligns closely with the attention module of Cliport[3], which serves as our primary baseline. The major difference between KITE and our Cliport-UNet is KITE's use of depth images projected into point clouds for learning in 3D space, whereas our Cliport-UNet decodes pixel features into 2D grasp orientations. Additionally, we provide some key differences between our method and KITE[1] as shown in **Table** R1.\n- We acknowledge the recent work OCID-VLG/CROG[2], which shares a similar focus. We've thoroughly analyzed the differences between our method and theirs in the updated section. We also note that this is a very recent publication lacking available code. Moreover, the 4D grasp they refer to involves top-down grasping with gripper width prediction, differing from 6 DoF grasping in 3D space.\n\n### Weakness 2:\n\n*In the real world experiment, the tested objects and variations are very limited compared to the simulation. It is more convincing to show more objects in the real world experiments and potentially show the results of the different methods not just LEG-Unet in the real world.*\n\n### Response 2:\n\n- The critique about the limited variety of objects and variations in our real-world experiments compared to the simulations has been noted. We chose the pick-part task for our real-world experiments because it represents a challenging yet intriguing aspect of robotic grasping. This task requires the model not only to pick the object but also to differentiate specific parts of it.\n- We evaluated performance using three metrics: grasping success rate, accuracy in selecting the correct object, and precision in grasping the intended part. The model was tested with varying numbers of demonstrations (1,5) and on unseen objects. The consistency of our real-world experiment's performance with the simulated pick-part experiments was established.\n- Additionally, our manuscript's Tables 1 and 2 compare our method's performance with baselines across 13 tasks involving hundreds of objects. Thus, we believe our physical experiment sufficiently demonstrates the real-world applicability of our method."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5778/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700584627087,
                "cdate": 1700584627087,
                "tmdate": 1700584627087,
                "mdate": 1700584627087,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]