[
    {
        "title": "DIVA: A Dirichlet Process Mixtures Based Incremental Deep Clustering Algorithm via Variational Auto-Encoder"
    },
    {
        "review": {
            "id": "6mFLmz2L6R",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission347/Reviewer_Ggf7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission347/Reviewer_Ggf7"
            ],
            "forum": "mz0SkLZbaz",
            "replyto": "mz0SkLZbaz",
            "content": {
                "summary": {
                    "value": "The paper presents a VAE where the encoder [posterior over the latent representation] is not a Gaussian but a Dichlet process mixture of Gaussians."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper combines some well-known techniques, namely Gaussian VAE and the memoized VB algorithm for Dirichlet process mixture models. There are some experiments with small datasets, which at least are known benchmarks of the old time."
                },
                "weaknesses": {
                    "value": "The concept of the method is inconsistent and, to my mind, signals a superficial understanding of the field. We used to employ nonparametrics because a single \u03bc and \u03a3 estimate cannot capture the variability of the data. However, when your \u03bc and \u03a3 are parametric functions of the data [here, deep networks parameterizing the VAE / encoders] there is absolutely NO need for nonparametrics. You are not dealing with a single \u03bc  and \u03a3 estimate, but as many as the data points. Besides, note that typically in nonparametrics we used to set K (the number of atoms) equal to the number of data points, which was considered \"close to infinity\" and, of course, has no place in a deep net-parameterized VAE. Therefore, the method lacks any substance actually. \n\nNovelty-wise, it is just a superficial blend of existing algorithms."
                },
                "questions": {
                    "value": "how does the method behave close to \"infinity\", and not with a superficially low number of K?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission347/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697285089185,
            "cdate": 1697285089185,
            "tmdate": 1699635961938,
            "mdate": 1699635961938,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "K5xfF0rMJQ",
            "forum": "mz0SkLZbaz",
            "replyto": "mz0SkLZbaz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission347/Reviewer_Hi2d"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission347/Reviewer_Hi2d"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes DIVA, a novel method for clustering high-dimensional datasets using variational autoencoders. DIVA is a Dirichlet Process Mixture model fitted on the latent space of a VAE, and under variational inference assumptions can be optimised through its ELBO. Its key, claimed innovation is to alleviate the user from having to choose the number of clusters a priori, which DIVA learns adaptively from data."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* This method provides a, to the best of my knowledge, novel method.\n* The method solves an indeed open and important problem in the deep clustering literature, how to adaptively choose and learn the number of clusters from data, without requiring this to be set through prior knowledge of a user.\n* The background provided is informative, and the mathematical formulation of the method is solid.\n* The combined method in Algorithm 1 is non-trivial, combines several techniques for optimising the model, and is of interest to the community."
                },
                "weaknesses": {
                    "value": "* My main concern is that the method\u2019s promise of alleviating the need to know the number of parameters a priori is only to some degree satisfied. While the number of Gaussian components are learned from the data, this number is determined by other hyperparameters of the model. In particular, I would like to ask the authors to comment on all hyperparameters that the method introduces and that need to be set, and specifically comment on the selection and effect of the concentration parameter alpha. \n* The authors evaluate their method experimentally mainly using the clustering accuracy metric (ACC) to cluster as the supervised label, and qualitatively with t-SNE. The former evaluation is limited: Image datasets typically contain more than one valid partition [2], and focussing on the one imposed by the supervised label is in the best case limited, in the worst case inappropriate. \u2013 This paper requires further evaluation, for instance in terms of its generative capability and the diversity of generated samples (generating actual samples, see also my point below), but also provide more details into what clusters are actually learned. On the latter point, Figures 7 and Figures 6 and 7 in the Appendix only show a subset of the learned clusters. It remains unclear if they are cherry-picked, what all the clusters learn, how the shown examples are sorted, and how many clusters there are in total.\n* The writing is sometimes hard to understand or unclear and could be improved. There are frequent grammatical issues.\n* What is stated as \u201cGenerative performance\u201d are not samples from the VAE, but merely reconstructions. This should be changed, or samples should additionally be provided, which would give additional insight into the model. \u2013 To see what is learned in each cluster, visualising the clustered input examples would be sufficient.\n* A minor point: Very established VAE work has the acronym DIVA which is unfortunate, namely \u201cIlse, M., Tomczak, J.M., Louizos, C. and Welling, M., 2020, September. Diva: Domain invariant variational autoencoders. In Medical Imaging with Deep Learning (pp. 322-348). PMLR.\u201d I suggest picking a different name for the method, or no name at all.\n\nI am happy to reconsider my current recommendation upon receiving a response to the above points."
                },
                "questions": {
                    "value": "* Table 1 should include the results of VaDE [1] and MFCVAE [2], where available. For instance on MNIST, VaDE (94.46 %) \u201coutperforms\u201d DIVA (94.01%) and is likewise within range of standard deviation of MFCVAE, questioning the claim by the authors that their method \u201coutperforms state-of-the-art baselines\u201d. \u2013 In particular, I would like to ask the authors why they decided to not report VaDE even though they were aware of and mentioning it in the related work. \n* DIVA does not support multi-facet clustering (in comparison to MFCVAE), which should be stated as a limitation.\n* Eq. (1), left-most looks odd. Could you please explain? It seems inconsistent with the graphical model.\n* I would be interested in the active number of clusters on the experimental datasets. \n\n[1] Jiang, Z., Zheng, Y., Tan, H., Tang, B. and Zhou, H., 2016. Variational deep embedding: An unsupervised and generative approach to clustering. arXiv preprint arXiv:1611.05148.\n\n[2] Falck, F., Zhang, H., Willetts, M., Nicholson, G., Yau, C. and Holmes, C.C., 2021. Multi-facet clustering variational autoencoders. Advances in Neural Information Processing Systems, 34, pp.8676-8690."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission347/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission347/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission347/Reviewer_Hi2d"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission347/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698537546338,
            "cdate": 1698537546338,
            "tmdate": 1699635961833,
            "mdate": 1699635961833,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "mmwtzsmdlF",
            "forum": "mz0SkLZbaz",
            "replyto": "mz0SkLZbaz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission347/Reviewer_pC7Y"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission347/Reviewer_pC7Y"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes DIVA, a Dirichlet Process Mixtures Incremental deep clustering framework via Variational Auto-Encoder (VAE). DIVA assumes soft clustering of observations in the latent space via a learnable Dirichlet Process Mixture Model (DPMM) prior, where the VAE and DPMM prior parameters are learned iteratively. DIVA is a non-parametric clustering approach, where the number of clusters is unknown. Experimental results on six datasets (including ImageNet-50) demonstrate that DIVA outperforms baselines (parametric/non-parametric) in dynamic and static datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is relatively well-written and easy to follow\n- DIVA outperforms baselines in static and dynamic datasets, crucial for lifelong learning applications"
                },
                "weaknesses": {
                    "value": "- DIVA seems like a straightforward combination of DPMM and VAE\n- It's unclear how DIVA handles the \"death\" and \"merge\" process of clusters. I encourage the authors to include complete details in the main paper\n-  It seems DIVA is comparable to DeepDPM in the static settings. The DeepDPM setup for dynamic datasets doesn't seem fair.  I encourage the authors to include an ablation study, e.g., replace DPMM with DeepDPM objective function"
                },
                "questions": {
                    "value": "- What is the computational efficiency of DIVA relative to alternative non-parametric baselines, e.g., DeepDPM?\n- Could you provide results on inferred K relative to ground truth, for all non-parametric methods?\n- For static datasets, it seems DeepDPM is comparable to DIVA. Could you clarify why DeepDPM fails in the dynamic setup?\n- Figure 6: Could you clarify why GMVAE (K=10) seems to degrade in performance over time?\n- Figure 4: Shouldn't we expect GMVAE (K=10) to have a similar latent structure as DIVA? \n- Figure 6: Could you provide dynamics on all datasets besides MNSIT?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission347/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698796151383,
            "cdate": 1698796151383,
            "tmdate": 1699635961747,
            "mdate": 1699635961747,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]