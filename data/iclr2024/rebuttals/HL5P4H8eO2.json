[
    {
        "title": "Differentiable Trajectory Optimization as a Policy Class for Reinforcement and Imitation Learning"
    },
    {
        "review": {
            "id": "okXbrJSpAP",
            "forum": "HL5P4H8eO2",
            "replyto": "HL5P4H8eO2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1693/Reviewer_cY2u"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1693/Reviewer_cY2u"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes DiffTOP, a novel policy class for sequential decision problems that is applicable to both reinforcement and imitation learning. The main idea is to use differential policy optimization to find efficiently a sequence of actions for a fixed starting state, given current estimates of the parameters that parameterize the observation-to-state encoder and the reward function and system dynamics in learned state space. The policy gradient is used to improve the estimates of the parameters, and because the computation of the action sequence is differentiable with respect to the parameters, the policy gradient ends up being differentiable with respect to the parameters, too."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The ability to compute policy gradients directly with respect to the parameters that describe the observation and transition model is a major advance, eliminating the need to do sample-based estimates. Another major strength, shared with the TD-MPC algorithm on which DiffTOP is based, is that model learning does not have to be done ahead of time, thus alleviating the model mismatch problem that is central to model-based reinforcement learning. A third strength of the paper is the very solid empirical verification of the approach on many control problems, both in the reinforcement and imitation learning setting."
                },
                "weaknesses": {
                    "value": "It appears that the trajectory optimization solver the authors are using, Theseus, does not support constraint optimization, so they have to manually unroll the dynamics, instead of presenting it as a constraint to the optimizer, as is the usual practice in trajectory optimization. This does not look very convenient, but is probably only a limitation associated with the software currently being used, and might be resolved in the future by using a more advanced solver."
                },
                "questions": {
                    "value": "Is the inability of Theseus to solve a constraint optimization version of the problem a fundamental limitation of how it works, or simply something not implemented yet? Maybe it solves non-linear least-squares problems analytically, and adding constraints would make this impossible?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "10: strong accept, should be highlighted at the conference"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1693/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697984099198,
            "cdate": 1697984099198,
            "tmdate": 1699636097745,
            "mdate": 1699636097745,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "o42Noqc0Bt",
                "forum": "HL5P4H8eO2",
                "replyto": "okXbrJSpAP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1693/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1693/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their time and the valuable comments and suggestions. We address the reviewer\u2019s questions as below.\n\n**Is the inability of Theseus to solve a constraint optimization version of the problem a fundamental limitation of how it works, or simply something not implemented yet? Maybe it solves non-linear least-squares problems analytically, and adding constraints would make this impossible?**\n\nWe thank the reviewer for this question. As we are not the active developers of Theseus, we would not be able to answer this question with perfect accuracy. According to our understanding, Theseus solves the nonlinear least-squares problem numerically; specifically, we used the Levenberg\u2013Marquardt algorithm in our implementation. \n\nThe original Theseus paper has the following discussion about adding constraints in their limitation section: \u201cThe nonlinear solvers we currently support apply constraints in a soft manner (i.e., using weighted costs). Hard constraints can be handled with methods like augmented Lagrangian or sequential quadratic programs [99, 100], and differentiating through them are active research topics\u201d. Based on this, it seems that adding constraints is not a fundamental limitation of how it works, but rather something that has not been implemented yet."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1693/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700547534812,
                "cdate": 1700547534812,
                "tmdate": 1700632948450,
                "mdate": 1700632948450,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DbxJs1EX20",
            "forum": "HL5P4H8eO2",
            "replyto": "HL5P4H8eO2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1693/Reviewer_zYV2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1693/Reviewer_zYV2"
            ],
            "content": {
                "summary": {
                    "value": "This paper leverages sensitivity analysis capabilities of trajectory optimization in order to differentiate the optimal solution with respect to problem parameters. This allows trajectory optimization to be used as a differentiable policy architecture, and the authors show that this has use cases in model-based reinforcement learning (MBRL) and imitation learning (IL). In MBRL, the latent-space encoder, latent-space dynamics, and the reward function parameters are learned within DiffTOP. In IL, the output of DiffTOP is forced to match the provided actions and DiffTOP carries out efficient inverse RL. The authors show that the proposed policy class outperforms baselines such as diffusion policies and IBCs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Overall, the paper is well-written and the methods and results are easy to follow for people familiar with the existing methods.\n2. The authors carry out extensive empirical testing across many baselines, the diversity of the considered benchmarks and baselines is quite appreciated. \n3. The authors convincingly tell a story of objective mismatch in MBRL, and point out that DiffTOP is able to learn a representation that is not only better for minimizing dynamics error but also better for control. The contribution in the space of MBRL is quite strong."
                },
                "weaknesses": {
                    "value": "1. How do the authors ensure that a reasonable answer is found in the process of nonlinear trajectory optimization with gradients? If the cost and dynamics was learned to formulate a convex trajectory optimization problem, this would not be a problem as the encoder is forced to learn a representation of that form. However the authors propose to use a fully nonlinear formulation and it's not clear how the training pipeline would be robust against finding bad local minima.\n\n2. The contribution in the IL seems a bit weaker compared to MBRL. In the setting of imitation learning, it seems that the authors are performing a version of inverse RL (which similarly does not assume access to rewards, but learns rewards and dynamics from demonstrations). Making connections to previous methods within inverse RL would be good.\n\n3. \"For DiffTOP, we always use a base policy to generate the action initialization for the trajectory optimization problem\" - this makes the comparison between other methods a bit unclear as other methods did not start with an informed initial guess. The experiment results make it seem like DiffTOP cannot be used as a standalone tool for IL, but rather a tool for the purposes of refinement; yet the method section is proposing a standalone IL algorithm."
                },
                "questions": {
                    "value": "1. The introduction of CVAE to generate multimodal samples is reasonable, but there are many other routes that might potentially handle the multimodality problem. For instance, it seems cheaper and more efficient to consider a differentiable stochastic trajectory optimization framework where a small amount of noise can be added to the action at every step, which forces trajectory optimization itself to be a stochastic procedure that will generate multimodal actions. Have the authors considered this formulation?\n\n2. CVAEs tend to be harder to train compared to recent diffusion-based generative models. Is it possible to train a diffusion model instead of address the multimodality problem? Maybe it is difficult since the author's framework requires latent states?\n\n3. How does DiffTOP perform without informed initial guesses in the RoboMimic experiment?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1693/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1693/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1693/Reviewer_zYV2"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1693/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698631652654,
            "cdate": 1698631652654,
            "tmdate": 1699636097665,
            "mdate": 1699636097665,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nOyAlvIjnq",
                "forum": "HL5P4H8eO2",
                "replyto": "DbxJs1EX20",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1693/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1693/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their time and the valuable comments and suggestions. We address the reviewer\u2019s questions and concerns as below. \n\n**How do the authors ensure that a reasonable answer is found in the process of nonlinear trajectory optimization with gradients? If the cost and dynamics was learned to formulate a convex trajectory optimization problem, this would not be a problem as the encoder is forced to learn a representation of that form. However the authors propose to use a fully nonlinear formulation and it's not clear how the training pipeline would be robust against finding bad local minima.**\n\nWe thank the reviewer for this insightful question. Indeed, there is not much theoretical guarantee on the quality of the converged local minima when solving the nonlinear trajectory optimization problem. One thing we find particularly useful is to provide the nonlinear trajectory optimization problem with a good initialization. This is shown in our ablation study of model-based RL in Figure 4, where we compare the performance of DiffTOP, and DiffTOP without using the action initialization from the learned latent policy (in which case the action is initialized to be 0 in trajectory optimization) on 4 model-based RL tasks . The result shows that without the action initialization from the learned latent policy, the performance drops. \n\nThe same results hold in the imitation learning setting, as shown in Table 3 & 4 in Appendix A.2  of the updated paper.  Following the reviewer\u2019s questions, we have added experiments to test DiffTOP without informed initial guess in the RoboMimic and ManiSkill experiments in Table 3 & 4. As the results suggest, if the action is initialized to be zero or sampled from a Gaussian of N(0, 1), the performance of DiffTOP is similar or slightly worse to vanilla behavior cloning, possibly due to the convergence to bad local minimal during trajectory optimization. With better action initializations, i.e., using actions from BC-RNN or Diffusion Policy, the performance of DiffTOP improves. \n\nTherefore, a good practice of using DiffTOP is to provide it with a good action initialization. This is often feasible in practice: in model-based RL, as our experiments show, the learned latent policy can be used to provide this action initialization; for imitation learning, one could always first learn a base policy using any behavior cloning algorithm, and then use DiffTOP to further refine the actions from the base policy for better performances. \n\nWe have updated Appendix A.2 of the paper to include the above discussion. \n\n**The contribution in the IL seems a bit weaker compared to MBRL. In the setting of imitation learning, it seems that the authors are performing a version of inverse RL (which similarly does not assume access to rewards, but learns rewards and dynamics from demonstrations). Making connections to previous methods within inverse RL would be good.**\n\nWe thank the reviewer for this valuable comment. We agree that when applied to the IL setting, DiffTOP can be viewed as doing inverse RL as well. The differences between DiffTOP and previous IRL methods are as follows. \n\nIn DiffTOP, the learned function is not necessarily a \u201creward\u201d function as those used in a typical RL/MDP setting. It is just a learned \u201cobjective function\u201d, such that optimizing it with trajectory optimization would yield actions that minimize the imitation learning loss with respect to the expert actions in the demonstration. We leave exploring the connections with inverse RL for future work. \n\nIn contrast to our approach, inverse RL algorithms try to learn a reward in the MDP/RL setting, such that when optimizing the learned reward with RL, the policy\u2019s state-action/state distribution matches the state-action/state distribution of the expert demonstrations. To perform such a distribution match, the loss for learning the reward is usually not the imitation learning loss as used in DiffTOP, but a loss to encourage matching the state-action/state distribution of the expert (e.g., the adversarial loss in GAIL [2]).\n\nWe have updated the paper to include the connection to inverse RL at the end of section 5.2.1. \n\n[1]  Florence et al, Implicit Behavioral Cloning, CoRL 2021  \n[2]  Ho et al, Generative Adversarial Imitation Learning, NeurIPS 2016"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1693/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700547345280,
                "cdate": 1700547345280,
                "tmdate": 1700547345280,
                "mdate": 1700547345280,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1y6SxExRI3",
                "forum": "HL5P4H8eO2",
                "replyto": "DbxJs1EX20",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1693/Reviewer_zYV2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1693/Reviewer_zYV2"
                ],
                "content": {
                    "title": {
                        "value": "comment"
                    },
                    "comment": {
                        "value": "I would like to thank the authors for the detailed response, I agree with the points in the rebuttal and will keep the score as is."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1693/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700701646180,
                "cdate": 1700701646180,
                "tmdate": 1700701646180,
                "mdate": 1700701646180,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YteKWzVRZD",
            "forum": "HL5P4H8eO2",
            "replyto": "HL5P4H8eO2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1693/Reviewer_fm37"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1693/Reviewer_fm37"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes DiffTOP an algorithm that extends TD-MPC to include additional losses for imitation learning and reinforcement learning. The proposed algorithm is evaluated on many examples including the standard RL benchmark tasks as well as many imitation learning tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper has a lot of evaluations on many standard benchmark tasks. The number of evaluations is very impressive in both the RL as well as the imitation learning domain. Furthermore, the algorithm seems to perform comparable or better to the TD-MPC baseline."
                },
                "weaknesses": {
                    "value": "While the number of experiments is very impressive, the amount of content condensed into 9 pages is a bit too much. It seems like the authors wanted to press every little piece of information into the limited pages which drastically reduced readability. It would be great if the authors would go back to the drawing board and prioritize what is the important information and elaborate on these parts. For example, the algorithm is still very unclear to me, which model parameters exist and are optimized together. It would be great if the authors could introduce their algorithm more slowly and rely on less knowledge from the TD-MPC paper. While mentioning the relation to TD-MPC is important, requiring excellent knowledge of TD-MPC to read the paper is very limiting.\n\nI am a bit torn on whether to score marginally above or below the acceptance threshold. While contribution and experimental evaluation are appropriate, the clarity of the presentation clearly lacks significantly and for me personally the clarity of the research is a major evaluation criterion."
                },
                "questions": {
                    "value": "I have questions regarding the additional PG loss for the RL case. Does it make sense to optimize the model parameters w.r.t. the Q-function? Wouldn't this loss let the dynamics model prefer to hallucinate to obtain a high q-function? Learning the dynamics and reward model is a supervised learning problem while learning the q-function is not. Therefore, I would expect that the supervised learning problem is much easier and imagine that the additional information flowing through this additional PG loss is not that important. What is the intuition of the authors for the additional PG loss? For imitation learning, the motivation for the additional bc loss is much clearer."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1693/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698804169031,
            "cdate": 1698804169031,
            "tmdate": 1699636097584,
            "mdate": 1699636097584,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "14st50fter",
                "forum": "HL5P4H8eO2",
                "replyto": "YteKWzVRZD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1693/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1693/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their time and the valuable comments and suggestions. We address the reviewer\u2019s questions and concerns as below. \n\n**For example, the algorithm is still very unclear to me, which model parameters exist and are optimized together.**\n\nWe have updated section 4.2 and 4.3 of the paper to make the model parameters more clear, and hope the following clarifications help with addressing this concern. \n\nAt the beginning of Section 4.2, we have added the following sentence to more explicitly state the model parameters that are optimized: \u201cWe use $\\theta$ to denote all learnable model parameters to be optimized in DiffTOP, including the parameters of the encoder $h_\\theta$, the latent dynamics model $d_\\theta$, the reward predictor $R_\\theta$, and the Q value predictor $Q_\\theta$\u201d . Equation 6 in the paper illustrates the loss that is used to optimize these parameters. \n\nFor imitation learning, we have added the following sentence at the end of section 4.3 to more explicitly show the model parameters that are optimized in this setting: \u201cSimilarly, We use $\\theta$ to denote all learnable model parameters to be optimized in DiffTOP, which includes the parameters of the encoder $h_\\theta$, the latent dynamics model $d_\\theta$, and the cost function $f_\\theta$ in the imitation learning setting\u201d. Equation 8 in the paper shows the loss that is used to optimize these parameters. \n\n\n**It would be great if the authors could introduce their algorithm more slowly and rely on less knowledge from the TD-MPC paper. While mentioning the relation to TD-MPC is important, requiring excellent knowledge of TD-MPC to read the paper is very limiting.**\n\nWe thank the reviewer for bringing this to our attention. Indeed, the model-based RL part of DiffTOP builds upon TD-MPC, thus requiring some knowledge of TD-MPC for understanding the paper. We do acknowledge that with the page limit, we are not able to cover every detail of the TD-MPC paper. However, we would like to clarify that we do have a section in our paper, i.e., section 3.2, model-based RL preliminaries, to explain the notations and external knowledge from TD-MPC that is needed for understanding our method, in order to make the paper self-contained. Following the reviewer\u2019s suggestion, we have updated section 3.2 with more explanations of TD-MPC to add more buildup for introducing our algorithm, and for a better understanding of the paper.   \n\nPlease let us know if you have any more suggestions on how to improve the readability of the paper \u2013 we would be more than happy to further incorporate them."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1693/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700546968854,
                "cdate": 1700546968854,
                "tmdate": 1700546968854,
                "mdate": 1700546968854,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XUSussht8O",
                "forum": "HL5P4H8eO2",
                "replyto": "YteKWzVRZD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1693/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1693/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response (continued)"
                    },
                    "comment": {
                        "value": "**I have questions regarding the additional PG loss for the RL case. Does it make sense to optimize the model parameters w.r.t. the Q-function? Wouldn't this loss let the dynamics model prefer to hallucinate to obtain a high q-function? Learning the dynamics and reward model is a supervised learning problem while learning the q-function is not. Therefore, I would expect that the supervised learning problem is much easier and imagine that the additional information flowing through this additional PG loss is not that important. What is the intuition of the authors for the additional PG loss? For imitation learning, the motivation for the additional bc loss is much clearer.**\n\nWe thank the reviewer for this insightful question. Optimizing the model parameters with the additional PG loss through differentiable trajectory optimization is one of the core contributions of our paper (besides the imitation learning modification as the reviewer noted). The intuition behind this is as follows: \n\nAs the reviewer mentioned, since learning the dynamics and reward model is a supervised learning problem that have been well-studied, existing model-based RL algorithms adopt this paradigm: they first learn a latent dynamics and reward model via supervised learning, and during inference time, they perform planning (such as MPPI) using the learned dynamics and reward model to generate the actions to take. However, recent studies [1, 2, 3] have shown that there is a fundamental \u201cobjective mismatch issue\u201d with such a paradigm, i.e., models that achieve better training performance (e.g., lower MSE) in learning a dynamics/reward model are not necessarily better for control. These papers explain this phenomenon in more detail, but the intuition here is as follows: a learned dynamics model will typically not perfectly drive the dynamics loss to 0.  Two different learned dynamics models will have different types of errors, and the model with the lower MSE loss is not necessarily the model that will be most useful for optimizing actions to maximize reward; it depends on what states and actions are causing those errors and what types of errors they are.  For example, errors in predicting the dynamics for actions that lead to low rewards are not very important.  \n\nDiffTOP addresses this issue: by computing the PG loss on the optimized actions from trajectory optimization and differentiating through the trajectory optimization process, the dynamics and reward functions are both optimized directly to maximize the task performance. \nAs there will inevitably be errors in the learned dynamics model, the additional PG loss regularizes the dynamics model to have low error in states and actions that lead to high rewards, i.e., those that are important for maximizing the task performance, instead of driving the dynamics model to hallucinate. Besides, we keep the dynamics prediction loss term when training the dynamics model, which should also prevent hallucination. In our experiments, DiffTOP with the added PG loss greatly outperformed the TD-MPC baseline that does not have this loss term, demonstrating its effectiveness.\n\n \n[1] Lamber et al, Objective mismatch in model-based reinforcement learning, Learning for Dynamics and Control (L4DC), 2020  \n[2] Eysenbach et al, Mismatched no more: Joint model-policy optimization for model-based rl, NeurIPS 2022  \n[3] Ghugare et al, Simplifying model-based rl: learning representations, latent-space models, and policies with one objective, ICLR 2022"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1693/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700547103772,
                "cdate": 1700547103772,
                "tmdate": 1700547129778,
                "mdate": 1700547129778,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YhKVui24Sb",
            "forum": "HL5P4H8eO2",
            "replyto": "HL5P4H8eO2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1693/Reviewer_WGnQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1693/Reviewer_WGnQ"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces DiffTOP, a novel policy class for reinforcement learning (RL) and imitation learning (IL) that employs differentiable trajectory optimization to generate policy actions. This approach leverages recent advancements in differentiable trajectory optimization, allowing end-to-end learning of cost and dynamics functions through gradient computation. DiffTOP addresses the \"objective mismatch\" problem in model-based RL by optimizing the dynamics and reward models to directly maximize task performance. For imitation learning, it outperforms previous methods by optimizing actions with a learned cost function at test time.\n\nThe authors benchmark DiffTOP on 15 model-based RL tasks and 13 imitation learning tasks with high-dimensional inputs like images and point clouds. The results show that DiffTOP surpasses prior state-of-the-art methods in both domains. The paper also includes an analysis and ablation studies to provide insights into DiffTOP's learning procedure and performance gains.\n\nIn summary, the contributions of the paper are:\n\n1) Proposing DiffTOP, which uses differentiable trajectory optimization for RL and IL.\n2) Demonstrating through extensive experiments that DiffTOP achieves state-of-the-art results in both RL and IL with high-dimensional sensory observations.\n3) Providing analysis and ablation studies to understand the learning process and performance improvements of DiffTOP."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper presents a robust and technically rigorous study, bolstered by a comprehensive suite of experiments and ablation studies. Some of the experiments are performed in notably challenging environments, showcasing the strength of the proposed method. It addresses the complex \"objective mismatch\" problem inherent in model-based reinforcement learning. The overall style of the paper is commendable, with clear and detailed descriptions of the experimental setup, network architectures, and hyperparameters, as well as well-structured pseudocode."
                },
                "weaknesses": {
                    "value": "No big weaknesses overall. It would be good to see the comparison of the computational efficiency of given algorithms and plots for return vs wall-clock time in addition to the number of samples."
                },
                "questions": {
                    "value": "1) Are there any cases when TD-MPC performs or could theoretically perform better than DiffTOP?\n2) What\u2019s the computational cost of using Theseus for differentiable trajectory optimization vs MPPI?\n3) Could you share the training plots of returns vs wall-clock time at least for some of the environments? It would be useful to compare not only the sample but also the computational efficiency of the different algorithms."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1693/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699599669829,
            "cdate": 1699599669829,
            "tmdate": 1699636097462,
            "mdate": 1699636097462,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EvOV5UH9Aw",
                "forum": "HL5P4H8eO2",
                "replyto": "YhKVui24Sb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1693/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1693/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their time and the valuable comments and suggestions. We address the reviewer\u2019s questions as below. \n\n**It would be good to see the comparison of the computational efficiency of given algorithms and plots for return vs wall-clock time in addition to the number of samples / Could you share the training plots of returns vs wall-clock time at least for some of the environments? It would be useful to compare not only the sample but also the computational efficiency of the different algorithms.**\n\nWe thank the reviewer for this suggestion. We have added plots for return vs wall-clock time of TD-MPC and DiffTOP on some of the model-based RL tasks, shown in Figure 7 in the updated paper, along with a discussion of the results in Appendix A.1.\n\nTo provide a short summary here for the reviewer\u2019s convenience:   \nIn terms of computational efficiency (e.g. wall-clock time needed to achieve a given level of performance), the results are environment-dependent. Compared to TD-MPC, DiffTOP is more computationally efficient on 3 environments, has similar computational efficiency on 3 environments, and worse computational efficiency on 3 environments (please see Figure 7 of the updated paper). The main advantage of our method is not computational efficiency but sample efficiency, as shown by our superior performance in Figure 3.\n\nCompared to TD-MPC, it takes more wall-clock time for DiffTOP to finish the training of 1M environment steps. The major reason is that solving and back-propagating through the trajectory optimization problem is slower than using MPPI as in TD-MPC. As a reference, it takes 0.052 seconds to use Theseus to solve and differentiate through the trajectory optimization problem, and 0.0092 seconds to use MPPI as in TD-MPC. \n\nThe research community is actively developing better and faster algorithms/software libraries for differentiable trajectory optimization, which could improve the computation efficiency of DiffTOP. For example, in all our experiments, we used the default CPU-based solver in Theseus. Theseus also provides a more advanced solver named BaSpaCho, which is a batched sparse Cholesky solver with GPU support. When we switch from the default CPU-based solver to BaSpaCho, the time cost of solving the trajectory optimization problem is reduced by 22% from 0.052 seconds to 0.041 seconds. \n\n\n\n**Are there any cases when TD-MPC performs or could theoretically perform better than DiffTOP?**\n\nWe thank the reviewer for this interesting question. Empirically, as shown in Figure 3 in the paper, DiffTOP always performs better than or on par with TD-MPC. Theoretically, if the latent dynamics model and the latent reward model is perfectly learned without any error, i.e., if they exactly match the ground-truth dynamics and reward model, and if enough samples are used in MPPI,  TD-MPC should be able to achieve the maximal possible performance and DiffTOP would not be able to further outperform it. However, in practice, since there will always be errors in the learned latent dynamics and reward model, DiffTOP outperforms TD-MPC by directly learning a latent space that is useful for control, instead of just minimizing the dynamics or reward prediction error. \n\t\n**What\u2019s the computational cost of using Theseus for differentiable trajectory optimization vs MPPI?**\n\nWe thank the reviewer for this question. We have performed an analysis on the computation cost, and updated the paper to have a discussion about this in Appendix A.1. The results are summarized as follows:\n\nFor inferring the action at one time step, the computation cost of using the default CPU-based solver in Theseus for differentiable trajectory optimization is 0.052 seconds. For MPPI, the time cost is 0.0092 seconds. However, as mentioned before, the computation cost of DiffTOP can be further reduced with better algorithms/solvers. For example, if we switch from the default CPU-based solver in Theseus to a more advanced batched sparse Cholesky solver with GPU support, the computation cost of solving the trajectory optimization problem is reduced by 22% from 0.052 second to 0.041 second. As the community is actively developing better libraries/algorithms for differentiable trajectory optimization, we believe the computational efficiency of DiffTOP would further improve in the future."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1693/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700546711053,
                "cdate": 1700546711053,
                "tmdate": 1700548487641,
                "mdate": 1700548487641,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]