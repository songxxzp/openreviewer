[
    {
        "title": "LAURAGPT: LISTEN, ATTEND, UNDERSTAND, AND REGENERATE AUDIO WITH GPT"
    },
    {
        "review": {
            "id": "JWRMyLnYWM",
            "forum": "jDy2Djjrge",
            "replyto": "jDy2Djjrge",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4438/Reviewer_ncjQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4438/Reviewer_ncjQ"
            ],
            "content": {
                "summary": {
                    "value": "In general, the paper is an experiment oriented work which demonstrates the GPT-style structure can do various speech tasks. Specifically, this paper introduces LauraGPT, a versatile GPT model designed for audio tasks, including: automatic speech recognition, speech-to-text translation, text-to-speech synthesis, machine translation, speech enhancement, automated audio captioning, speech emotion recognition, and spoken language understanding. To enable these capabilities, the model combines continuous and discrete audio features, utilizing an audio encoder for input and a discrete codec for output. The model is then fine-tuned through supervised multitask learning on a range of audio-to-text, text-to-audio, audio-to-audio, and text-to-text tasks. Extensive experiments demonstrate that LauraGPT achieves competitive or superior performance compared to existing state-of-the-art models across various audio processing benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper presents a thorough set of experiments, showcasing the capabilities of LauraGPT in handling both audio and text inputs and generating outputs across a diverse range of tasks. These tasks encompass content analysis, semantics, paralinguistics, and audio-signal analysis.\n\nAs far as my knowledge extends, the paper provides extensive coverage of speech tasks in its evaluation, as indicated by the authors in Table 1."
                },
                "weaknesses": {
                    "value": "While the paper presents solid research, it falls short in paving the way for future investigations. \n\n1. By the end of 2023, speech researchers generally believe that GPT-style models can handle various speech tasks, even though certain specific tasks may not achieve state-of-the-art performance when compared to baseline models of the same size. Instead of offering insights beyond the extensive experiments conducted, the authors primarily focus on demonstrating the effectiveness of GPT-style models across multiple speech tasks.\nI hope the authors will consider demonstrating whether multi-task learning can result in task synergy, where tasks can benefit from each other rather than being treated as separate or even conflicting objectives. For instance, if I have a 1B model that solely focuses on automatic speech recognition (ASR), would it outperform a 1B model capable of performing ASR, text-to-speech (TTS), and speech-to-text (ST) tasks? If the answer is no, then why should we incorporate all these tasks into the same model? It would be valuable for the authors to analyze the relationship between task performance in the context of multi-task learning, as evidenced in Table 2, where LauraGPT lags significantly behind the state-of-the-art LibriSpeech despite its 2B model size.\n\n2. In the realm of fundamental speech models, are there any emerging points of interest similar to those in the field of natural language processing (NLP)? Exploring this aspect could be a valuable research direction. If speech researchers are unable to answer this question, I believe that running multi-task learning experiments alone may not be sufficient to construct the next generation of speech models."
                },
                "questions": {
                    "value": "Apart from Speech translation, could you list more complex tasks that should use foundamental model to solve rather than do them one by one?\n\nIf the performance is worse than train an ASR model alone, what is the value of the multi-task learning model?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4438/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698592299734,
            "cdate": 1698592299734,
            "tmdate": 1699636418820,
            "mdate": 1699636418820,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "L9ImTeAEG1",
                "forum": "jDy2Djjrge",
                "replyto": "JWRMyLnYWM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4438/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4438/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to thank the reviewer for all the constructive feedback. Below we address all of your questions and concerns.\n\nPlease check our Global Responses to All Reviewers.\n\n$$ $$\n\n> Apart from speech translation, could you list more complex tasks that should use fundamental model to solve rather than do them one by one? \n\n**Response:**  As stated in Section 3.5, with its modular and flexible design, LauraGPT provides an extensible framework to support complex tasks. By breaking a task into sub-tasks among the basic tasks used in training and cascading the raw inputs and model outputs of sub-tasks, LauraGPT can perform more complex tasks than the basic tasks. \n\nSimilar to the speech-to-speech translation (S2ST) example, LauraGPT can perform more complex tasks by chaining together basic tasks as described above. Here are a few examples of other complex tasks that LauraGPT can support rather than doing them one by one:\n\n1. **Rich transcription**: We can extend LauraGPT to simultaneously transcribe audio into content, speaker information (speaker identification, etc), paralinguistic information (emotion, etc.) and high-level semantic information (intent, slots, etc.) by including different task IDs at the generation process. This approach could avoid error accumulation in a pipelined approach and is more efficient than performing these tasks individually.\n\n2. **Rich translation**: We can further chain the above basic tasks of rich transcription with the MT task to perform rich translation of audio input. The audio input can provide additional paralinguistic information to improve the translation accuracy.\n\n3. **Noise-robust ASR**: We can implement noise-robust ASR by chaining tasks and creating the following input sequence: [noisy speech embedding, <SE>, embedding of the enhanced speech,  <ASR>].  Since SE and ASR are jointly trained for LauraGPT, LauraGPT could effectively exploit embeddings of the original noisy speech and enhanced speech for noise-robust ASR. \n\n$$ $$\n\n> what is the value of the multi-task learning model ? It would be valuable for the authors to analyze the relationship between task performance in the context of multi-task learning\n\n**Response:** The multi-task learned single model of LauraGPT has the following advantages over single-task models: \n\n1. Multi-task learning could potentially exploit the synergy between related speech tasks and reduce over-fitting, hence LauraGPT could provide quality performance on a diverse set of tasks, and achieve better performance than single-task training, especially for tasks with limited training data.  \n\n2. Since multi-task learning could learn a single model capable of supporting a diverse set of tasks, it greatly simplifies the practical deployment and applications. LauraGPT can provide a diverse set of audio processing capabilities through the unified API and model implementation.  \n\nPlease refer to Our Global Response to Q3 for more details. Our Global Response to Q3 clearly demonstrates the significant performance improvements from multi-task learning of LauraGPT over single-task training on tasks with limited training data.\n\n$$ $$\n\n> as evidenced in Table 2, where LauraGPT lags significantly behind the state-of-the-art LibriSpeech despite its 2B model size.\n\n**Response:** In our Global Response to Q1, we provide a summary of the performance comparisons between our multi-task learned LauraGPT with baselines on each task.  As analyzed below, on ASR task, LauraGPT produces competitive performance on  both Chinese and English test sets compared to competitive baselines.\n\n**For ASR task**,  the baselines are competitive Paraformer and Whisper Large V2.  As shown in Table 2,  on the Chinese test sets,  LauraGPT greatly outperforms Whisper by **-3.9** and **-2.3** absolute on CER and performs comparably to Paraformer with a much smaller amount of training data. On the English test sets, LauraGPT achieves comparable performance to Paraformer and performs better on the more noisy test set, but performs worse than Whisper Large V2 as Whisper Large V2 uses much more English training data than LauraGPT. Note that there are other **targeted optimized models** [1] that can achieve better performance than Whisper Large V2 on LibriSpeech test sets. However, they only focus on English speech recognition and benefit from additional technologies such as language model decoding.\n\n[1] C. S, et al. Wavlm: Large-scale self-supervised pre-training for full stack speech processing. IEEE Journal of Selected Topics in Signal Processing."
                    },
                    "title": {
                        "value": "Responses to Reviewer ncjQ (Part 1/2)"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4438/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700303706002,
                "cdate": 1700303706002,
                "tmdate": 1700375282320,
                "mdate": 1700375282320,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DePAK2ZnwK",
                "forum": "jDy2Djjrge",
                "replyto": "JWRMyLnYWM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4438/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4438/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> \"In the realm of fundamental speech models, are there any emerging points of interest similar to those in the field of natural language processing (NLP)? Exploring this aspect could be a valuable research direction. If speech researchers are unable to answer this question, I believe that running multi-task learning experiments alone may not be sufficient to construct the next generation of speech models. \" Thoughts and plans on the next generation of fundamental speech models.\n\n**Response:**  Thank you for the valuable question. There are emerging points of interest for fundamental speech models that are similar to those in the field of NLP.  This is a tremendously valuable research direction. Our work in this paper achieves one important milestone for this research question, as we explored and provided promising answers to the following question:\n- How to design more efficient and scalable GPT-style models than existing approaches that can leverage large-scale labeled data and achieve state-of-the-art performance on a diverse set of speech tasks including speech recognition, understanding and generation, using a single model?  Note that previous fundamental speech models either focus solely on speech recognition and understanding but neglect generation tasks, or support speech generation but suffer from significant performance degradation on recognition and understanding tasks.\n\nAs shown in Appendix B.3 and Section 5.2, multi-task learning and the combination of continuous features and discrete features for audio is crucial for achieving the above goal.\n\nFor the next generation of fundamental speech models, we are inspired by the recent advances of large language models (LLMs) in NLP, and we envision that the fundamental speech models should have the following capabilities:\n- In-context learning ability like GPT-3, which can learn from few-shot examples and adapt to new tasks, such as predicting the age of the speaker from a speech sample.\n- Instruction-following ability like InstructGPT and ChatGPT, which can perform the appropriate speech-related task given a natural language instruction, such as synthesizing a speech with a specific emotion or style.\n- General audio modeling ability, i.e., speech, non-speech audio, and music, such as music generation. \n\nWe consider the following steps to progress from LauraGPT into the next generation of fundamental speech model:\n1. Self-supervised pre-training exploring large-scale unlabeled data and enhancing cross-modality alignment. We plan to explore tasks such as next token prediction on unlabeled speech and text before the multi-task supervised training in our paper.\n2. Extend the task categories for supervised pre-training, such as music understanding and generation, speaker tasks such as speaker diarization and so on. We plan to add more supervised training tasks into LauraGPT.\n3. Construct instruction-following data and conduct supervised fine-tuning. We could generate instruction texts for all audio-related tasks using competitive LLMs (e.g., GPT-4) and pair the instruction with corresponding samples.\n4. Optionally, we can collect data and conduct RLHF to make the fundamental speech model more harmless, helpful, and honest. \n\nPlease note that our paper demonstrates that our current LauraGPT has made solid progress and reached one important milestone toward a speech foundation model. As demonstrated in the above steps, from LauraGPT to the next-generation speech foundation model we envisioned, **most remaining efforts are in more data collection and more training**.  **There is no need to modify the model architecture**. We hope that our paper can provide insights and inspiration for future research on fundamental speech models."
                    },
                    "title": {
                        "value": "Responses to Reviewer ncjQ (Part 2/2)"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4438/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700303742891,
                "cdate": 1700303742891,
                "tmdate": 1700374569202,
                "mdate": 1700374569202,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "eJeRVd2aAF",
            "forum": "jDy2Djjrge",
            "replyto": "jDy2Djjrge",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4438/Reviewer_x7eV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4438/Reviewer_x7eV"
            ],
            "content": {
                "summary": {
                    "value": "- LauraGPT is a single GPT-like LLM that operates on a combination of discrete and continuous features for audio signals and text, and is fine-tuned to perform a wide range of speech and audio processing tasks.\n- A pre-trained text-only language model (Qwen) serves as the backbone for Laura. For audio, LauraGPT uses a combination of discrete tokens obtained from an improved Encodec-based audio codec (where only the first quantizer is used as the tokenizer), as well as a conformer-based encoder which is initialized with weights from a pertained ASR model. The autoregressive model predicts the next token (text or audio) given the input embeddings, task embeddings, and the previously predicted tokens. The output text is obtained from the Qwen tokenizer and the final audio is obtained from a so-called codec vocoder.\n- Instead of directly using the decoder of the pre-trained audio codec, LauraGPT uses a codec vocoder wherein a transformer model serves to predicted the sum of all quantizers embedding for ground truth audio given just the first quantizer embedding and additional context. Subsequently, during inference, the predicted audio token embedding can be transformed into the summed token embeddings and passed to the pre-trained codec decoder to generated raw audio.\n- The authors evaluate LauraGPT against strong baselines for each of the task it is capable of performing. LauraGPT performs well on most tasks, only failing to beat baselines in the Speech enhancement and TTS task. It also fails to beat Whisper-Large V2 in English, which is understandable given the smaller amount of English data it was pre-trained on."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper demonstrates a strategy to fine-tune existing LLMs trained only for text to perform various audio processing (generation and understanding) tasks. \n- Unlike other related work, this paper shows that utilizing a mix of continuous and discrete representations of audio in the transformer architecture leads to improved performance in the final generation task (as ablated in section 5.2).\n- The evaluation is pretty comprehensive and strong baselines have been chosen for most tasks."
                },
                "weaknesses": {
                    "value": "- The ablation regarding discrete tokens vs continuous + discrete tokens feels incomplete without also using the VALL-E style token prediction setup. Currently, the token prediction scheme is similar to that used by SPEAR-TTS wherein each quantizer level token is predicted one-by-one before moving on to the next audio-frame\u2019s tokens.\n- Some statements are not clearly backed up by experiments. For example, one of the main contributions listed is the fact that continuous and discrete representations of audio are used in LauraGPT and that this preserves both fidelity and generality of audio data. Firstly, I am not quite sure what it means to preserve generality of audio. Second, while it is shown in the ablation that the Discrete IO model suffers, it is not clear to me how these results show that fidelity and generality is preserved because of the use of combined representations. All I see is that performance on various task is improved by using the combination. Also, one additional benefit the combined representation model sees is the use of the codec vocoder. Perhaps that is the source of the improvements in LauraGPT?\n- Section 3.4 would be well served with some more detail. The reader would benefit from some repeating information that the GPT model only uses the first quantizer. I found it difficult to understand initially and had to read from the start of section 3 again. \n- A few figures going into more detail for each of the components in figure 1 would also greatly improve the readability of the the method section. Currently, figure 1 is very high-level and does not offer the reader too much."
                },
                "questions": {
                    "value": "- Do I understand correctly that the model uses continuous features from only the input audio, and uses audio token embeddings for previous audio tokens, meaning that the generated audio is always seen as tokens within the GPT model? It would benefit the reader if this is stated in the text explicitly as well."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4438/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698902363340,
            "cdate": 1698902363340,
            "tmdate": 1699636418757,
            "mdate": 1699636418757,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bWtgEbRjL9",
                "forum": "jDy2Djjrge",
                "replyto": "eJeRVd2aAF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4438/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4438/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to thank the reviewer for all the constructive feedback. Below we address all of your questions and concerns.\n\nPlease check our Global Responses to All Reviewers.\n\n$$ $$\n\n> The ablation regarding discrete tokens vs continuous + discrete tokens feels incomplete without also using the VALL-E style token prediction setup. Currently, the token prediction scheme is similar to that used by SPEAR-TTS wherein each quantizer level token is predicted one by one before moving on to the next audio-frame\u2019s tokens.\n\n**Response:**  Our response to this question is two-fold:\n\n1. The goal of the current ablation study is to evaluate the impact of combining continuous features and discrete tokens in LauraGPT on **ALL** relevant spoken language processing tasks, where the inputs of GPT are continuous representations and the outputs are discrete tokens. The token prediction scheme of VALL-E can be applied to audio generation tasks, such as TTS and SE. However, it is not straightforward to apply it to **audio recognition and understanding tasks**, such as ASR, S2TT, and AAC. Therefore, we have not included the comparison to the VALL-E style token prediction setup in the paper.\n\n2. On the other hand, taking SE as an example of audio generation tasks, we evaluated the VALL-E style token prediction in our preliminary experiments. In these experiments, we kept the model inputs and architectures the same as LauraGPT and only replaced the token prediction scheme with the VALL-E style. The results are as follows.  We find that the token prediction scheme of LauraGPT significantly outperforms VALL-E in terms of CER and WER and also improves PESQ while obtaining the same STOI. These results indicate the superiority of predicting discrete tokens through GPT and converting them into a continuous format using a codec vocoder. \n\n|Token prediction scheme|PESQ|STOI (%)|CER|WER|\n|------|------|-------|------|-----------------|\n|VALL-E|2.55|88.0|10.52|19.32|\n|LauraGPT|2.97|88.0|9.05|15.94|\n\n$$ $$\n\n> Some statements are not clearly backed up by experiments. For example, one of the main contributions listed is the fact that continuous and discrete representations of audio are used in LauraGPT and that this preserves both fidelity and generality of audio data. Firstly, I am not quite sure what it means to preserve generality of audio. Second, while it is shown in the ablation that the Discrete IO model suffers, it is not clear to me how these results show that fidelity and generality is preserved because of the use of combined representations. All I see is that performance on various task is improved by using the combination. Also, one additional benefit the combined representation model sees is the use of the codec vocoder. Perhaps that is the source of the improvements in LauraGPT?\n\n**Response:**  In Section 1, we explain that we use continuous features to represent the input audio to ensure high performance on **audio recognition and comprehension tasks**, and use codec-based discrete features to represent the output audio, which enables joint autoregressive modeling with text features for **audio generation tasks**. We show in our experiments that continuous features for audio (such as Filterbank) have notable advantages over discrete units on audio recognition, understanding, and audio-signal-related tasks, such as ASR, S2TT, and SE, as analyzed in Section 5.2. Therefore, our model achieves a better trade-off between model uniformity (generality) and high performance (fidelity) on diverse categories of audio tasks than existing approaches that use discrete features for both input and output audio. The generality of audio here refers to the ability of our model to handle different types of audio tasks in a unified framework, by using codec-based discrete features to represent the output audio and jointly modeling them with discrete text tokens. We attribute the fidelity of audio mainly to the use of continuous features for input audio.\u00a0In the ablation experiment using Discrete IO in Section 5.2, we compare the performance of our LauraGPT with a model that uses discrete features for both input and output audio (Discrete IO). We find that Discrete IO significantly degrades ASR, S2TT and SE tasks, while LauraGPT has good performance on these tasks. This demonstrates that our model preserves both the fidelity and generality of audio data by using a combination of continuous and discrete representations."
                    },
                    "title": {
                        "value": "Responses to Reviewer x7eV (Part 1/2)"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4438/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700302816837,
                "cdate": 1700302816837,
                "tmdate": 1700375331559,
                "mdate": 1700375331559,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tEhU3QOpYQ",
                "forum": "jDy2Djjrge",
                "replyto": "eJeRVd2aAF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4438/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4438/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Do I understand correctly that the model uses continuous features from only the input audio, and uses audio token embeddings for previous audio tokens, meaning that the generated audio is always seen as tokens within the GPT model? It would benefit the reader if this is stated in the text explicitly as well. \n\n**Response:** Your understanding is correct. As stated in Section 3, for audio inputs, we extract the log-compressed Mel spectrogram features and feed them into the audio encoder, while the audio outputs are discretized into tokens using the audio tokenizer. This means that the generated audio is always seen as tokens within the GPT model, as you correctly understood. We have made this point more explicit in the revised version. \n\n$$ $$\n\n> Section 3.4 would be well served with some more detail. The reader would benefit from some repeating information that the GPT model only uses the first quantizer. I found it difficult to understand initially and had to read from the start of section 3 again.\n\n**Response:** We have clarified the details of only using the first quantizer to tokenize audio signals in the revised Section 3.4. The remaining 31 quantizers are only used at the training stage of the codec vocoder."
                    },
                    "title": {
                        "value": "Responses to Reviewer x7eV (Part 2/2)"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4438/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700302842394,
                "cdate": 1700302842394,
                "tmdate": 1700332817775,
                "mdate": 1700332817775,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "kDJ0JP9reD",
            "forum": "jDy2Djjrge",
            "replyto": "jDy2Djjrge",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4438/Reviewer_Xm2a"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4438/Reviewer_Xm2a"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors propose a unified GPT model(LauraGPT) for audio recognition, understanding, and generation. They encode input audio into continuous representation and decode output audio from discrete codec codes and fine-tune a language model. They evaluate the LauraGPT on various audio processing benchmarks like ASR, S2TT, TTS and so on. The experimental results conducted on tasks show the effectiveness of the LauraGPT and the flexible design of the model."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.The proposed model supporting largest number of and most diverse audio processing tasks compared with other structure, which is interesting and reasonable. The authors also give detailed analysis and descriptions about these tasks and results with baselines.\n\n2.The article provides a clear categorization of tasks and the model provides an extensible framework to support complex tasks with its modular and flexible design. It can break a task into sub-tasks among the basic types and perform well. This makes the model well extensible.\n\n3.The model combines continuous and discrete features for audio signals. It utilizes the continuous features and analyzes the impact of discrete versus continuous representations in ASR, S2TT, and SE tasks."
                },
                "weaknesses": {
                    "value": "1.The task-related token included in the matrices is not explained enough, how is it utilized and how is it embedded to give the information of the types of the tasks. It lacks some details about it in the description.\n\n2.In evaluation part, there\u2019s a lack of adequate analysis of the relationship between the poor performance in some tasks and model size. \n\n3.In Part 3, there is a lack of detailed visualizations to show the internal framework of the model, as well as the details of the training and inference process."
                },
                "questions": {
                    "value": "1.In the article, the model is able to perform in more task domains, compared to the most related multi-task unified audio-text models, but in the comparison, why is there no comparison with these multi-task models for the various metrics of these tasks?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4438/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699373466424,
            "cdate": 1699373466424,
            "tmdate": 1699636418675,
            "mdate": 1699636418675,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YkAeOBeErE",
                "forum": "jDy2Djjrge",
                "replyto": "kDJ0JP9reD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4438/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4438/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to thank the reviewer for all the constructive feedback. Below we address all of your questions and concerns.\n\nPlease check our Global Responses to All Reviewers.\n\n$$ $$\n\n> The task-related token included in the matrices is not explained enough, how is it utilized and how is it embedded to give the information of the types of the tasks. It lacks some details about it in the description.\n\n**Response:** The task-related token (also called taskid in the paper) is added as a special symbol directly to the dictionary. Therefore, it can be converted to an embedding through an embedding matrix like the text tokens. For example, in the case of the ASR task, the input audio is passed through an audio encoder to obtain the corresponding audio embedding. The task-related token and the output text are converted to the text embedding through the embedding matrix, where the first embedding corresponds to the <ASR> taskid. These embeddings are then concatenated as [audio embedding, text embedding], which serves as the input for QWen LLM. \n\nThe task-related token can help the model distinguish between different tasks with the same input. We use ASR and S2TT tasks as an example. Given the same input speech utterance, if we add <ASR> task ID after the audio embedding, the model will generate the ASR recognized result. And if we add <S2TT> taskid, the model will output the translated text.\n\n$$ $$\n\n> In evaluation part, there\u2019s a lack of adequate analysis of the relationship between the poor performance in some tasks and model size. \n\n**Response:** Please refer to our Global Response to Q1 for performance analysis on all tasks, which includes a detailed analysis of reasons causing relatively lower performance of LauraGPT on some metrics on some test sets. Note that none of the relatively lower performance of LauraGPT compared to baselines is caused by model size differences.\n\n$$ $$\n\n> All the rest concerns under Weaknesses and Questions\n\n**Response:**  Please refer to our Global Responses to All Reviewers."
                    },
                    "title": {
                        "value": "Responses to Reviewer Xm2a"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4438/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700302398951,
                "cdate": 1700302398951,
                "tmdate": 1700375359459,
                "mdate": 1700375359459,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "LSFz13caZx",
            "forum": "jDy2Djjrge",
            "replyto": "jDy2Djjrge",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4438/Reviewer_Nqbz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4438/Reviewer_Nqbz"
            ],
            "content": {
                "summary": {
                    "value": "This paper describes an integrated Audio-Text LLM that uses continuous features to represent input audio and discrete tokens to generate output audio. This allows it to be used for audio generation and audio->audio tasks like text-to-speech synthesis and speech enhancement, in contrast to most (but not all) existing Audio-Text systems. The system is evaluated on many standard tasks in the various modalities and in comparison to reasonable existing single-task systems provides significantly improved performance on spoken language understanding accuracy, speech to text translations from english to Chinese, equivalent performance on ASR, spoken language understanding f1, speech emotion recognition, speech to text translation from Chinese to english, and worse performance on automatic audio captioning, speech enhancement, and text-to-speech. Overall, the system seems competitive in these various tasks compared to these baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The problem of general audio-text understanding, modeling, and generation is an important one, as is multi-modality in LLMs in general.\n* Ambitious combination of many tasks into a single model\n* Experiments seem well conducted, reasonable selection of tasks and benchmarks"
                },
                "weaknesses": {
                    "value": "In terms of novelty, this is a popular area of research at the moment, with SeamlessM4T being released in August, a few weeks before the submission deadline in addition to the many relevant recent references cited here. If the new capability enabled by the proposed approach is audio output, then one of the most relevant systems appears to be AudioGPT, which also supports speech enhancement. The reason that it is not compared or included in Table 1, which shows capabilities of different existing models, is that it integrates an \"expert audio model with LLM\", which I don't fully understand the meaning of. An expanded explanation of this would be very useful in understanding this key point. \n\nAlso regarding Table 1, it should be clarified that it is showing just what the model has been trained/evaluated on, not what it is necessarily capable of. For example, any system that can perform speech to text translation should also be able to perform automatic audio captioning. Similar arguments hold for speech emotion recognition and spoken language understanding.\n\nIn terms of the significance of the results, it is interesting that these tasks can all be solved by a single model, but it's not clear that doing so gives the model advantages over separate models. The performance seems mostly on par. It is also not clear whether the other multimodal systems described in Table 1 (especially SpeechT5) would do better than the proposed system on these tasks as the comparisons are only against single-task systems trained on much less data (the subset of the data that the proposed system was trained on for each particular task).\n\nIn terms of clarity, two different taxonomies of related models are introduced in sections 1 and 2, these could be combined into a single one to make space for more explanation of the data that the model was trained on from the appendix. In particular, it is not clear in the body of the paper whether the model is trained once on all of the data or separately for different tasks or how that is navigated and how much data it is overall.\n\nSome claims about the proposed model's superiority are not well supported by the results. Specifically the claim of being best on SLU, when it is really just SLU accuracy, but not f1 scores. This is also the case for SER in that the proposed model is better on unweighted accuracy, but not weighted f1 or accuracy. \n\nMinor comments:\n* A definition of the \"endless looping problem\" and \"loop ratio\" would be helpful in the appendix\n* \"These results indicate that LauraGPT tends to generate captions that closely match one of the references...\" can you explain how you reach this conclusion?\n* Please define exactly what \"clean_codec_syn\" is in table 7\n* In the appendix, prosody includes both tone and speed, so no need to list them separately\n* In the appendix, I believe \"dereverberation\" is meant instead of \"echo cancellation\" which involves an echo back to the far end of a telephone call, typically.\n* In the appendix, \"For the SER task, we collect corpora including...\" are there other corpora used? If so, please list them. If not, reword."
                },
                "questions": {
                    "value": "Can you clarify the difference between the proposed system and AudioGPT?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4438/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699460807538,
            "cdate": 1699460807538,
            "tmdate": 1699636418607,
            "mdate": 1699636418607,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9D53gVl9BK",
                "forum": "jDy2Djjrge",
                "replyto": "LSFz13caZx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4438/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4438/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to thank the reviewer for all the constructive feedback. Below we address all of your concerns and questions.\n\nPlease check our Global Responses to All Reviewers.\n\n$$ $$\n\n>One of the most relevant systems appears to be AudioGPT, but it is not compared in Table 1. Explain expert audio model with LLM. Can you clarify the difference between the proposed system and AudioGPT?\n\n**Response:**  We respectfully disagree on considering AudioGPT as one of the most relevant systems to our LauraGPT. As we stated in Section 2.1, our LauraGPT is a **single unified audio-text model that can directly process both audio and text inputs and generate outputs in speech and text modalities**. In contrast,  AudioGPT is an integrated AI system that **integrates and interfaces specialized audio models, such as ASR and TTS models,  for speech input and speech output (i.e., the expert audio model we mentioned in Section 2.1) with LLM**. Note that these specialized audio models add more complexity, consume more resources, and cause AudioGPT to be prone to unavoidable error accumulation problems.  Hence, AudioGPT is drastically different from our LauraGPT and is not compared to or included in Table 1.\n\n$$ $$\n\n>Regarding Table 1, it should be clarified that it is showing just what  the model has been trained/evaluated on, not what it is necessarily  capable of.\n\n**Response:** We agree with you that Table 1 only shows the tasks the most related multi-task unified audio-text models are trained and evaluated on, not their capabilities. We have updated the caption of Table 1 to clarify this and also revised the text in Section 1 since VioLA and AudioPaLM should be able to perform AAC. However, it is important to point out that decoder-only models using discrete speech tokens such as VioLA and AudioPaLM may suffer from the information loss caused by quantization of speech signals into discrete tokens, which leads to significant performance degradation over models using continuous speech features, as our ablation study in Section 5.2 shows that by using continuous features for audio input, LauraGPT significantly outperforms the counterpart using discrete features on ASR, S2TT, and SE tasks.\n\n$$ $$\n\n>All other questions and Weaknesses.\n\n**Response:**  Please refer to our Global Responses to All Reviewers."
                    },
                    "title": {
                        "value": "Response to Reviewer Nqbz (Part 1/2)"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4438/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700301218822,
                "cdate": 1700301218822,
                "tmdate": 1700375399047,
                "mdate": 1700375399047,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "j0zGJmxmPD",
                "forum": "jDy2Djjrge",
                "replyto": "LSFz13caZx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4438/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4438/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Minor Comments:**\n\n> A definition of the \"endless looping problem\" and \"loop ratio\" would be helpful in the appendix\n\n**Response:** For some test cases of SE task, if the model generates several fixed tokens repeatedly and cannot stop the generation until reaching a pre-defined maximum length, we consider the problem of endless looping decoding occurs and consider this test case to be an \"endless decoded case\". To evaluate the probability of occurrence of this problem, we define the metric, \"loop ratio\", which refers to the fraction of \"endless decoded cases\" among all test cases. We have clarified the definition of \"endless looping problem\" and \"loop ratio\" in the revised Appendix B.1.\n\n$$ $$\n\n>\"These results indicate that LauraGPT tends to generate captions that closely match one of the references...\" can you explain how you reach this conclusion?\n\n**Response:** As explained in our Global Response to Q1, for the ACC task, SPICE is designed to capture the **specificity** and accuracy of the generated captions in terms of the semantic content, while CIDEr focuses on evaluating the **consensus** or agreement between the generated captions and the reference captions. SPIDEr can be viewed as an average of SPICE and CIDEr. From Table 6, we find that the single model LauraGPT achieves a comparable SPICE score but underperforms on CIDEr and SPIDEr, compared with the Ensemble baseline. Therefore, we think that LauraGPT tends to generate captions that closely match one specific reference rather than the consensus of references. We have clarified this reasoning process in the AAC Evaluation paragraph in Section 5.1 of the revised paper.\n\n$$ $$\n\n> Please define exactly what \"clean_codec_syn\" is in table 7\n\n**Response:** In Table 7, \"Clean_codec_syn\" refers to waveforms that are reconstructed using the first four codec groups extracted from the clean speeches. The results corresponding to  \"Clean_codec_syn\" can be considered as the upper bound for the Discrete IO models. The results in the \"Clean\" and \"Clean_codec_syn\" rows demonstrate that solely utilizing the first four codec groups extracted from the clean speeches to synthesize waveforms leads to degradation in speech quality (PESQ), intelligibility (STOI), and a significant degradation in recognition error rates. These results demonstrate the superiority of our choice of continuous representations for audio inputs. We have added the definition of \"Clean_codec_syn\" and this clarification in Section 5.2 in the revised paper.\n\n$$ $$\n\n> In the appendix, prosody includes both tone and speed, so no need to list them separately\n\n**Response:** Thank you for the advice. We have removed the word \"prosody\" and listed \"tone\" and \"speed\" to specify the additional information conveyed in speech but not in the text, in the revised paper.\n\n$$ $$\n\n> In the appendix, I believe \"dereverberation\" is meant instead of \"echo cancellation\" which involves an echo back to the far end of a telephone call, typically.\n\n**Response:** Thank you for the advice.  We have changed the term \"echo cancellation\" to \"dereverberation\" in the revised version.\n\n$$ $$\n\n> In the appendix, \"For the SER task, we collect corpora including...\" are there other corpora used? If so, please list them. If not, reword. \n\n**Response:**   No other corpora are used for the SER task. We have also modified this description in the revised paper.\n\n$$ $$\n\n> In particular, it is not clear in the body of the paper whether the model is trained once on all of the data or separately for different tasks or how that is navigated and how much data it is overall.\n\n**Response:** As described in Section 4 of the paper, we initialize the Qwen backbone and audio encoder with the pre-trained checkpoints and then optimize the model parameters through multi-task fine-tuning. We do not perform fine-tuning separately for different tasks. For details of the training data, please refer to Appendix A.2.  The details of the training setup are included in Appendix A.4. Specifically, due to the significant variation in training data volume across different tasks, we conduct a three-stage training process. In the first training stage, the model is fine-tuned on all tasks using the complete training data as shown in Table 9. In the second stage, we continue fine-tuning the model on tasks that have small-scale datasets, including SER, SLU, AAC, TTS, and SE tasks.  In the third training stage, we continue fine-tuning the model again on all tasks using the complete training data in Table 9. We find that this design of training curriculum effectively reduces catastrophic forgetting (as demonstrated in competitive or superior performance on **all tasks**, please refer to our Global Response to Q1) and achieves high performance on tasks **with limited training data** (please refer to our Global Response to Q3)."
                    },
                    "title": {
                        "value": "Response to Reviewer Nqbz (Part 2/2)"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4438/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700301694445,
                "cdate": 1700301694445,
                "tmdate": 1700337696397,
                "mdate": 1700337696397,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]