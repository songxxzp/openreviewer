[
    {
        "title": "Robustness via learned Bregman divergence"
    },
    {
        "review": {
            "id": "MnATNkp9jO",
            "forum": "7GCRhebJEr",
            "replyto": "7GCRhebJEr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7831/Reviewer_qLEt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7831/Reviewer_qLEt"
            ],
            "content": {
                "summary": {
                    "value": "The paper uses Bregman divergence for adversarial learning, which the authors argue to be more robust and can be more accurate."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The exposition is nice and the manuscript is easy to follow."
                },
                "weaknesses": {
                    "value": "Adversarial learning is a fast-growing field. That being said, I failed to see the technical novelty of the proposed approach: the results appear to be straightforward and the technical contributions are limited. There is only one lemma and an algorithm in the current manuscript, making the paper more like in an engineering manner."
                },
                "questions": {
                    "value": "See my comments in the limitations."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7831/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698547913739,
            "cdate": 1698547913739,
            "tmdate": 1699636959425,
            "mdate": 1699636959425,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bDvlgt5L8v",
                "forum": "7GCRhebJEr",
                "replyto": "MnATNkp9jO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7831/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7831/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Inappropriate review"
                    },
                    "comment": {
                        "value": "We reported this review as inappropriate."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7831/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700214960500,
                "cdate": 1700214960500,
                "tmdate": 1700215003873,
                "mdate": 1700215003873,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "34qHtc9WEx",
            "forum": "7GCRhebJEr",
            "replyto": "7GCRhebJEr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7831/Reviewer_brGx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7831/Reviewer_brGx"
            ],
            "content": {
                "summary": {
                    "value": "The study examines the use of Bregman divergence as an alternative to the $L^p$ norm for measuring the distance between benign and corrupted samples. It also explores the application of mirror descent, based on Bregman divergence, to identify corruptions for adversarial training. This is accomplished by constructing two convex neural networks that approximate the gradient of the distance-generating function and its inverse. Training the networks involves utilizing a Bregman loss. Additionally, a semantic attack is proposed using Bregman divergence to generate corrupted examples, which are then employed in adversarial training to enhance corruption robustness."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The organization of this paper is clear and logical. The motivation and procedures of the proposed method are well-described.\n2. The proposed method is novel, and the experimental results demonstrate its superior performance compared to the baseline methods across different corruption severities."
                },
                "weaknesses": {
                    "value": "1. The claim that $(\\nabla\\phi)^{-1}=\\nabla\\bar{\\phi}$ is based on the assumption that ICNN is of the Legendre type. However, this assumption has not been verified. The proposed numerical method for approximating the inverse $(\\nabla\\phi)^{-1}$ appears unrelated to Fenchel's (1949) result. According to Equation (7), the minimization problem $\\min_f \\Vert f(\\nabla\\phi(x)) - x\\Vert_2$ should yield $f=(\\nabla\\phi)^{-1}$, but it is not demonstrated that the L2 distance is the optimal choice.\n2. The evaluation in this paper has limitations. For instance, it remains unclear how the Bregman divergence training converges. Additionally, it is uncertain whether the method can scale to larger images, such as ImageNet. How does the choice of sampling numbers impact the approximation?\n3. The Bregman-based attack finds the projection point $x'$ in the intersection of $D$, which is the intersection of $B_{\\phi}$ and $B$. According to the algorithm, the projection is performed by projecting onto $B$ first, and then onto $B_{\\phi}$. However, it seems unlikely that the resulting projected point will lie in the intersection.\n4. Although Lemma 1 states that $d$ lies in the range $(0, 1)$, it is unclear why Figure 1 is generated when $d=1.\n5. In the experiment, the fraction of $\\Gamma$ functions is defined as $1/\\sqrt{n}$. The purpose of introducing $\\mu$ as the fraction of Gamma functions in Lemma 1 is not clear.\n6. Based on my understanding, each iteration of mirror descent involves one forward pass and one backward pass for the input $x^t$. It is unclear how the computation cost of Bregman divergence training is determined.\n7. The notations in the equations lack consistency, such as the font used for variables like $\\tilde{x}$ and $u$."
                },
                "questions": {
                    "value": "See weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7831/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7831/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7831/Reviewer_brGx"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7831/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698677134300,
            "cdate": 1698677134300,
            "tmdate": 1699636959313,
            "mdate": 1699636959313,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Ly7tUTWQSp",
                "forum": "7GCRhebJEr",
                "replyto": "34qHtc9WEx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7831/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7831/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We have addressed your questions/concerns; we consider some as misunderstandings, other clarifications we will incorporate.\n\n\n$\\def\\vy{\\boldsymbol{y}}$\n$\\def\\vx{\\boldsymbol{x}}$\n$\\def\\vz{\\boldsymbol{z}}$\n$\\def\\vu{\\boldsymbol{u}}$\n\n\nWe argue that our notation is consistent; indeed, we followed the conference recommendations provided in the ICLR template. For example, the font in `\\bm{x}` $\\vx$ indicates a vector where the font in `\\textnormal{x}` $\\textnormal{x}$ is for random variables. We also made these distinctions clear throughout the text.\n\n> \"The claim that $(\\nabla\\phi)^{-1}=\\nabla\\overline\\phi$ is based on the assumption that ICNN is of the Legendre type. However, this assumption has not been verified\"\n\nWe did not claim that ICNNs are of Legendre type (it would require proving that the limit along the boundary of the interior of the domain of definition of $\\nabla\\phi$ is $\\infty$ which is challenging for an arbitrarily complex $\\phi$). We did not claim that this equality holds for ICNNs. We only used this fact to motivate the approximation which we wrote as $\\overline\\Psi(\\boldsymbol{x}) \\approx \\nabla\\overline\\phi(\\boldsymbol{x})$.\n\n> \"The proposed numerical method for approximating the inverse appears unrelated to Fenchel's (1949) result\"\n\nThe result of Fenchel (1949) is important for our work: it provides the insight that the inverse of $\\Psi$ is the gradient of a another convex function (in particular the conjugate). We have relied on this to model $\\overline\\Psi$ as a gradient of an ICNN, which is convex by our construction. This inspiration is discussed right above Equation 7.\n\n> \" ... but it is not demonstrated that the L2 distance is the optimal choice.\"\n\nTraining for this objective (the mean square error) is a design choice that we made and it is a common choice, e.g., also widely used when training autoencoders. The choice gave good results as detailed in Sec.5. We did not claim its optimality and it can be replaced by alternatives if suggested.\n\n> \"...it remains unclear how the Bregman divergence training converges.\"\n\nThe training **converges** nicely without over-fitting. The evolution of the training and the validation loss across optimization steps is added in Appendix C.\n\n> \"... the projection is performed by projecting onto $B$ first, and then onto $B_\\phi$. However, it seems unlikely that the resulting projected point will lie in the intersection.\"\n\n$\\def\\epsm{{\\epsilon_\\text{max}}}$\n\n$\\def\\Bf{{\\mathbb{B}_\\phi(\\vx, \\epsilon)}}$\n\n$\\def\\B{{\\mathbb{B}(\\vx, \\epsilon_\\text{max})}}$\n\n\nThe result of this projection is indeed always in the intersection: \n\nLet $\\vy \\not \\in \\Bf \\cap \\B$. First, we define $\\vz$ as the projection of $\\vy$ onto $\\B$, so $|| \\vz- \\vx ||  \\leq \\epsm$. Next, we perform a line search on the segment of endpoints $\\vx$ and $\\vz$ until we find $\\vu$ satisfying $D_\\phi(\\vu || \\vx) \\leq \\epsilon$. In other words, $\\vu = \\alpha \\vx + (1-\\alpha)\\vz$ for some $\\alpha \\in [0,1]$. We have $\\vu \\in \\Bf$ since $D_\\phi(\\vu || \\vx) \\leq \\epsilon$. We also have $||\\vx-\\vu|| = ||\\vx - \\alpha\\vx -(1-\\alpha) \\vz|| = (1-\\alpha)||\\vx-\\vz|| \\leq \\epsm$ (because $\\alpha \\in [0,1]$ and $||\\vz-\\vx|| \\leq \\epsm$), which means $\\vu \\in \\B$. This concludes the proof that $\\vu$, the result of this procedure is always in $\\Bf \\cap \\B$.\n\n> \"... it is unclear why Figure 1 is generated when $d=1$\"\n\nFigure 1 is only a 2D cartoon meant to give visual intuition of our method so we chose $d=1$ to be easily readable. Choosing $d=0.99999$ for example would have resulted in the same visual.\n\n> \"The purpose of introducing $\\mu$ as the fraction of Gamma functions in Lemma 1 is not clear.\"\n\nWe use $\\mu$ to avoid writing the bulky fraction in Equation. 21 multiple times. The fraction ensures that equality (9) holds. But, as suggested by another reviewer, the lemma can be moved to the appendix.\n\n> \"Based on my understanding, each iteration of mirror descent involves one forward pass and one backward pass for the input. It is unclear how the computation cost of Bregman divergence training is determined.\"\n\nIt is not exactly like that: there are two separate phases. First, we train the divergence itself, minimizing the objective in Equation. 10, not involving mirror descent. Then we use the divergence in the inner-loop of the adversarial training (with mirror descent in Algorithm 1). \n\nThe overall computational cost is composed of the cost of computing $\\phi$, $\\Psi$ and $\\overline\\Psi$. The computation of $\\phi(\\boldsymbol{x})$ is a forward pass on the convolutional neural network $\\phi$. The computation of $\\Psi(\\boldsymbol{x})$ and $\\overline\\Psi(\\boldsymbol{x})$ are back-propagation passes on the neural networks representing $\\phi$ and $\\overline\\phi$, respectively. These computations are implemented using the highly optimized automatic differentiation engine of PyTorch (mentioned in Sec. 3.1 and Sec 5.)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7831/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700142474353,
                "cdate": 1700142474353,
                "tmdate": 1700143842473,
                "mdate": 1700143842473,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tYZXj4ngDp",
                "forum": "7GCRhebJEr",
                "replyto": "Ly7tUTWQSp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7831/Reviewer_brGx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7831/Reviewer_brGx"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the authors' response. I have a few more follow-up questions about the projection issue. It would be helpful if the authors could provide further explanations.\n\n1. How to ensure the exisitence of $\\vu$ satisfying $D_\\phi(\\vu || \\vx) \\leq \\epsilon$?\n2. Your proof only shows $\\vu\\in\\Bf \\cap \\B$, and does not show it is the projection of $\\vz$.\n\nI'm still not clear about how is the projection onto set $A$ then set $B$ is equivalent to the projection onto $A\\cap B$. Thinkg of a point in 2D and two lines $y=x$ and $y=0$, which intersect at $(0, 0)$. The projection onto intersection is $(0, 0)$ while the projection onto $y=x$ then $y=0$ is not necessarily at $(0, 0)$."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7831/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700538856473,
                "cdate": 1700538856473,
                "tmdate": 1700538856473,
                "mdate": 1700538856473,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "r1mkrbOam6",
            "forum": "7GCRhebJEr",
            "replyto": "7GCRhebJEr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7831/Reviewer_zXKR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7831/Reviewer_zXKR"
            ],
            "content": {
                "summary": {
                    "value": "The paper investigates the corruption robustness of classifiers, and proposes a variant of adversarial training with perturbations sought according to a similarity measure, which is learned using corruption models. The similarity measure is defined as Bregman divergence, with a learned base function, and mirror descent is employed for seeking perturbations. The experimental results show that the approach outperforms $\\ell_2$ and LPIPS based adversarial training towards corruption robustness."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is well written and the overall approach to employ Bregman divergences and mirror descent is well-motivated\n- The discussion of the learned base functions for Bregman divergence for different corruptions provides interesting insights towards less artificial threat models.\n- The experimental results show good performance on CIFAR-10-C, compared to $\\ell_2$ adversarial training and RLAT."
                },
                "weaknesses": {
                    "value": "- The idea of learning similarity measures and generating suitable adversarial examples is interesting. However, since the work aims to increase the robustness of classifiers it needs to be discussed to what extend the corruption model can be practically known.\n- The method is compared to $\\ell_2$ adversarial and RLAT method in the experiments, but in-distribution performance and data augmentation baselines seem to be missing."
                },
                "questions": {
                    "value": "- Could the authors provide some more information on the in-distribution performance and on the performance of data augmentation baselines (which might also use the corruption model $\\tau(x)$)?\n- In Table 3 the corruption robustness accuracy is shown for the different models and on different corruptions. Do the authors have any insights on why the model trained on contrast (via learning the similarity measure) performs well on both contrast and fog, whereas the model trained on fog performs worse in both categories?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7831/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7831/Reviewer_zXKR",
                        "ICLR.cc/2024/Conference/Submission7831/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7831/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698795538812,
            "cdate": 1698795538812,
            "tmdate": 1700685745541,
            "mdate": 1700685745541,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bT6pOPpNrC",
                "forum": "7GCRhebJEr",
                "replyto": "r1mkrbOam6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7831/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7831/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Our method presents real-world practical robustness, it approaches the problem from an adversarial learning perspective, and we have included the in-distribution results\n\n> \"... since the work aims to increase the robustness of classifiers it needs to be discussed to what extend the corruption model can be practically known.\"\n\nWe show that we can train models that are robust to real-world corruptions like Fog. CIFAR-10-C actually contains computational models for creating corrupted images from clean ones. Otherwise corruptions can be obtained through samples collected in the real world and are thus available in this sense. \n\n> \"The method is compared to adversarial and RLAT method in the experiments, but in-distribution performance and data augmentation baselines seem to be missing.\"\n\nOur method approaches the problem of robustness from an adversarial perspective (as RLAT). Importantly, our method is agnostic to the architecture of the classifier (RLAT is not) and thus can be combined with any of the approaches based on data augmentation, which are thus orthogonal. The in-distribution performance is given in Table 2 and Table 3 under the column \"standard accuracy\".\n\n> \"Do the authors have any insights on why the model trained on contrast (via learning the similarity measure) performs well on both contrast and fog, whereas the model trained on fog performs worse in both categories?\"\n\nWe had already addressed this question (as good as we could) at the end of the paragraph **Adversarial training** in Sec.5: \"One hint is that the three corruptions are similar in nature but this behavior requires further investigation.\""
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7831/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700140097763,
                "cdate": 1700140097763,
                "tmdate": 1700140097763,
                "mdate": 1700140097763,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WEahZUi1XO",
                "forum": "7GCRhebJEr",
                "replyto": "bT6pOPpNrC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7831/Reviewer_zXKR"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7831/Reviewer_zXKR"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to the authors for addressing my questions. While I share other reviewer's concerns regarding the limited experiments (incl. data augmentation baselines), I consider the overall approach to be a good contribution. I updated my score."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7831/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700685700045,
                "cdate": 1700685700045,
                "tmdate": 1700685700045,
                "mdate": 1700685700045,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PLMxKNEKCt",
            "forum": "7GCRhebJEr",
            "replyto": "7GCRhebJEr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7831/Reviewer_GMTd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7831/Reviewer_GMTd"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces an interesting learned similarity metric between images based on Bregman divergence and shows that this metric allows for the training of classifiers more robust against image corruption. L_p distances are widely used to assess the robustness of discriminative models because of mathematical convenience, not because they capture a principled notion of invariance in the data domain. The proposed Bregman divergence is demonstrated to be a promising alternative, contributing both to the literature on metric learning out-of-distribution robustness."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The paper presents a mathematically elegant formulation for metric learning (via self-supervised Bregman divergence learning) and adversarial training (based on mirror descent).\n* The authors show preliminary evidence that their method learns a \u201cgeometry of image corruptions.\u201d\n* From a brief literature review (and bolstered by the paper\u2019s thorough review of related work) I believe the proposed training procedure is indeed novel.\n* The comparison to the LPIPS divergence metric shows that the proposed Bregman divergence is an effective metric for learning robust models.\n* The methods are very well presented and approachable.\n\nMinor comments:\n* I found Table 1 to be very helpful in making the work approachable.\n* Algorithm 1 nicely documents the settings for hyperparameters."
                },
                "weaknesses": {
                    "value": "* The experiments are only conducted on CIFAR10-C, which is a single, simplistic dataset. I would like to believe that the results would hold on a more complex dataset like ImageNet-C, but this is not demonstrated in the paper. The Conclusions seem to acknowledge that scaling to larger convex architectures for the base function could be challenging.\n* One of the issues of using Bregman divergences to measure image similarity is that these divergences are not necessarily symmetric. Many image augmentations are symmetric.\n* Lemma 1 is not important to include in the main text. The alternative approach in A.2 is a much simpler method to draw samples, which does not require the complicated statement of the Lemma; in fact the experiments use the simpler version in A.2! Moreover, due to typicality, sampling from a normal distribution in high dimension will essentially sample from the surface of a hypersphere.\n* The computation of the $\\Gamma$ function should be tractable, and not require approximation.\n* The experiments are not particularly strong. The proposed approach does not perform best on the zoom blur corruption.\n\nSmall issues:\n* I believe that the paper\u2019s contribution on defining a new method for metric learning should be highlighted in the abstract.\n* It feels like Equation 3 is out of place. Perhaps it should be in the next section on Mirror descent? I think it should also be admitted up front that while this projection is unique, it is not available in closed-form for neural networks (Table 1), thus you use a line search heuristic.\n* In Table 1, the mirror map for KL divergence is missing a +1 term.\n* In the Mirror Descent paragraph in Section 2, I think it should say \u201cmapping $z^{t+1}$ back to the **primal** space\u201d.\n* $\\tau$ is used before definition in Equation 7 (def isn\u2019t until Section 3.3).\n* The caption of Figure 2 should be re-written. I had to read it a couple of times to understand what the plot is showing.\n\nSmall typos:\n* \u201csettings yield**s**\u201d should be plural in the abstract.\n* In the 1st sentence, the phrase \u201cthe way\u201d is awkward.\n* \u201cAT was found to **improve also**\u201d should be \u201calso improve\u201d.\n* In 3.2 you use a \\citet when it should be a \\citep for Fenchel.\n* In the first paragraph of Section 5, Hendrycks should be a \\citet.\n* Adversarial Training paragraph page 8: \u201cuse it\u201d\n* Last sentence: \u201ccope\u201d should be \u201cscope\u201d."
                },
                "questions": {
                    "value": "1. Did you assess the L_p adversarial robustness of your models trained with the Bregman notion of robustness? Do you have any idea whether your robust models exhibit any of the beneficial characteristics (e.g. perceptual gradients) observed in robust models trained with adversarial training?\n2. When you are training the Inverse Map (Eq. 7), do you use a `detach` operation in practice, i.e. to train the inverse map to emulate the base function without optimizing over the parameters of the base function? As written, this loss would seem to indicate that the base function is optimized to emulate the inverse map.\n3. Do you have any metrics of the fit of the inverse map on the test data? Isn\u2019t it also true that the inverse map could be fine-tuned during test time on the test points (i.e. the learned inverse map is a form of amortization).\n4. How are corruptions $\\tau$ sampled during training? Do you draw a single pair $\\tau, d$ for each training image as an augmentation?\n5. Since the projection $\\Pi_K$ is not available in closed form for a learned base function, the paper uses a binary search heuristic. Can you show that this procedure would lead to convergence in the limit $\\eta \\rightarrow 0$?\n6. Do you have any observations or conjectures about the right modes in the divergence plots (Figure 2)?\n7. What is your choice for distance $d$ in Figure 3?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7831/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7831/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7831/Reviewer_GMTd"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7831/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698933103455,
            "cdate": 1698933103455,
            "tmdate": 1699636959041,
            "mdate": 1699636959041,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "d42rH8A7WV",
                "forum": "7GCRhebJEr",
                "replyto": "PLMxKNEKCt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7831/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7831/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to thank the reviewer for the feedback ranging from high-level comments to the finer details of the implementation. As you suggest, we will emphasize better that we propose a novel method for metric learning, which is indeed the core contribution of our work with robustness only as one possible application.\n\n> No experiments on ImageNet.\n\nWe would have liked to but with the limited GPU compute available to us it would take about one month. We are looking for a compute grant for the next or final version.\n\n> \"One of the issues of using Bregman divergences to measure image similarity is that these divergences are not necessarily symmetric. Many image augmentations are symmetric.\"\n\nIndeed, we have considered the symmetrization $S_\\phi$ of the Bregman divergence $D_\\phi$, which, however, loses the triangle inequality:\n\n$$\nS_\\phi(\\boldsymbol{x'}, \\boldsymbol{x}) = D_\\phi(\\boldsymbol{x'} \\parallel \\boldsymbol{x}) +  D_\\phi(\\boldsymbol{x} \\parallel \\boldsymbol{x'}) \n$$\nWe have substituted $S_\\phi$ for $D_\\phi$ throughout the approach and we re-run the evaluation in Sec. 5. The symmetry did not provide any improvement compared to the proposed $D_\\phi$.\n\n> \"Lemma 1 is not important to include in the main text.\"\n\nYou are right. We will change that.\n\n> \"Did you assess the L\\_p adversarial robustness of your models trained with the Bregman notion of robustness?\"\n\nWe have not done this experiment.\n\n> \" When you are training the Inverse Map (Eq. 7), do you use a `detach` operation in practice\"\n\nExactly! We detach $\\Psi(\\boldsymbol{x})$ from the computational graph of $\\Psi$ to ensure that the optimization step taken to minimize Equation. 7 only modifies the parameter of the inverse map $\\overline\\Psi$ without affecting the parameters of the map $\\Psi$.\n\n> \"Do you have any metrics of the fit of the inverse map on the test data?\"\n\nThe mean square error is minimized down to < 0.001.\n\n> \"Isn\u2019t it also true that the inverse map could be fine-tuned during test time on the test points (i.e. the learned inverse map is a form of amortization).\"\n\nIndeed, fine-tuning the inverse map during test time could further improve the results. However, the current implementation keeps the inverse map frozen once the training is done.\n\n> \"Do you draw a single pair $\\tau$ $d$ for each training image as an augmentation?\"\n\nYes, the pair is (uniformly) sampled for each image in the mini batch during each training epoch.\n\n> \"Can you show that this procedure would lead to convergence in the limit $\\eta \\to 0$?\"\n\nThe convergence of the mirror descent requires the optimized function to be $\\beta$\\-smooth. In our case, the maximized function is a loss function $l$ over the output of a DNN which is not smooth. So unfortunately, even if the projection $\\Pi_K$ were exact, the algorithm would not be guaranteed to converge to the global maximum.\n\n> \"Do you have any observations or conjectures about the right modes in the divergence plots (Figure 2)?\"\n\nThe learned base functions $\\phi$ output a scalar for an input image. These scalar values are totally abstract to us as they are leaned though self-supervision. They can be thought of as an \"entropy\" for images (see Table 1).\n\n> What is your choice for distance in Figure 3?\n\n$d$ is uniformly drawn from the interval $[10^{-7}, 10^{-3}]$."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7831/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700139922860,
                "cdate": 1700139922860,
                "tmdate": 1700139922860,
                "mdate": 1700139922860,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WVHFNewjNl",
                "forum": "7GCRhebJEr",
                "replyto": "d42rH8A7WV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7831/Reviewer_GMTd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7831/Reviewer_GMTd"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response"
                    },
                    "comment": {
                        "value": "I appreciate the detailed answers to all of my questions. I am inclined to keep my score, contingent on the authors clarifying the paper to address my comments and the comments of other reviewers (in particular Reviewer brGx's question about ensuring the projection operation is in the intersection of $B$ and $B_\\phi$).\n\nI reiterate my assessment that the experiments are not particularly strong, but the paper is interesting, novel, and methodologically sound."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7831/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700449844281,
                "cdate": 1700449844281,
                "tmdate": 1700449844281,
                "mdate": 1700449844281,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]