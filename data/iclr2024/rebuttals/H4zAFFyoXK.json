[
    {
        "title": "BLSP: Bootstrapping Language-Speech Pre-training via Behavior Alignment of Continuation Writing"
    },
    {
        "review": {
            "id": "2zj1Qv6bOT",
            "forum": "H4zAFFyoXK",
            "replyto": "H4zAFFyoXK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3521/Reviewer_Dd3G"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3521/Reviewer_Dd3G"
            ],
            "content": {
                "summary": {
                    "value": "The objective of the paper is to allow LLMs to be used with speech modality as input. Toward this, the authors propose to align the speech and text modalities through a method they call \u201ccontinuation writing.\u201d The key idea is to first use text input to generate text continuation, and then train an adaptor to predict the same continuation when the text is replaced with its corresponding speech input. The authors show that this simple strategy shows strong results on several tasks such as ASR, translation, and SLU. Analysis also reveals that this strategy helps to align the latent space of speech/text prompts for the same instruction, and push apart different instructions."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. Arguably the biggest strength of the paper is the simplicity of the proposed method. There are no complex training strategies, nor the requirement of large amounts of hard-to-obtain data. The authors keep the largest components of the model (the speech encoder and the LLM) frozen and only train a small modality adapter, and this is done using publicly available ASR training data. As such, the BLSP method should be easy to replicate and use for downstream applications.\n\n2. The authors have performed detailed empirical evaluation on several tasks: speech recognition (ASR), speech translation (ST), and spoken language understanding (SLU). While their model does not beat single-modality baselines (e.g., on ASR, the WERs are much worse than whisper-small, and on ST, the BLEU is worse than a cascade of ASR+LLM), it is still promising and reminds one of the early days of end-to-end speech translation. I believe with better modality adapters and training strategies, this method may get close to or surpass cascaded approaches.\n\n3. More importantly, on the SLU task, BLSP outperforms a cascaded system since semantic similarity is sufficient for this task. This is an important take-away if this method is used to provide a common interface into a general-purpose QA system, which may only need to be semantically correct to provide accurate responses."
                },
                "weaknesses": {
                    "value": "### Loss of speaker and paralinguistic information\n\nSpeech contains much more instruction than just its transcription, such as speaker, emotion, etc. Using paired ASR training data for modality alignment forgoes this extra information, and simply marginalizes over them. As such, the resulting model may only be good at tasks which worked with text inputs (i.e., recognition and semantic tasks). If we are extending an LLM with another modality, a natural requirement may be to extend the capabilities of the model \u2014 for example, for vision-language models, image understanding is usually achievable, which is not possible with text-only LLMs. But this kind of \u201cextension\u201d is not shown in this setting. It would have been great to evaluate the model on a non-semantic task (such as emotion recognition) which is not possible using text-only LLMs. This may be an adversarial task for this kind of training; in Section 3.1, the authors conjecture that \u201cthe LLM should behave the same no matter whether it is given the speech segment or its transcripts as input,\u201d but this may not always hold.\n\n### Regarding representations of modalities\n\nIn Section 2 (Figure 1), the authors find that training the modality adapter only on the ASR task is not beneficial since the input speech representations remain the same regardless of the task instruction used, meaning that the corresponding tokens are not being attended to. This is not surprising: the model only learns transcription since that is all it sees in training. Perhaps it would learn to attend to the instruction if the authors trained using a combination of different instructions, such as ASR, translation, and SLU, and also paraphrased the instructions themselves. The resulting model would still not be as good as BLSP, but would not see the representation collapse we observe in Figure 2.\n\nAnother common concern about joint speech-text training usually has to do with sequence lengths. Speech models usually need aggressive downsampling because of their large sequence lengths and redundancy of semantic information. For ASR, usually the speech-text length difference is solved using either cross-attention (e.g. LAS) or alignment-free training (e.g., CTC). For the BLSP model, the modality adapter takes the speech encoder representations, and transforms them for input to LLAMA, which is trained on autoregressive text decoding. Therefore, we expect the adapter to transform the speech representations to the same space as LLAMA input representations. In the paper, the authors used simple speech inputs not containing long pauses, so fixed-length subsampling worked well for them. However, in real scenarios, users often speak with pauses, or correct themselves, and so I wonder perhaps it would be a better idea to use variable-length subsampling techniques in the modality adapter [1].\n\n[1] Y. Meng et al., \"On Compressing Sequences for Self-Supervised Speech Models,\" 2022 IEEE Spoken Language Technology Workshop (SLT), Doha, Qatar, 2023, pp. 1128-1135, doi: 10.1109/SLT54892.2023.10022991.\n\n### Empirical results\n\nWhile the BLSP performance on ST and SLU is reasonable and promising, the degradation for ASR is quite large (Table 3). In particular, LLAMA may already have seen the audiobooks from LibriSpeech, or transcripts of the TED talks from TED-LIUM 3, so I expected these WERs to be much lower. The authors suggest (in Table 4) that this is mainly because of the nature of LLAMA to produce fluent text output (probably because of the prompt used). I have two questions about this:\n1. Can the authors show WER results only on rare words to see if important words are transcribed faithfully (without caring about stop words)?\n2. Did the authors try different prompts to see what is the lowest WER that can be achieved? For example, from an application perspective, it may be very useful to have one model that can generate both verbatim and non-verbatim transcripts simply by using different prompts."
                },
                "questions": {
                    "value": "1. Is the term \u201ccontinuation writing\u201d used before? Why did the authors not simply call it \u201cnext token prediction\u201d?\n2. In Section 4.4, the authors show multilingual capabilities of the trained model even though the modality adapter is only trained with English ASR data. This suggests that the transformation from speech representations to corresponding text is the same (or similar) across all languages. Can the authors measure this isomorphism among spaces more carefully, perhaps using some of the techniques in [2]?\n\n[2] Marchisio, Kelly et al. \u201cIsoVec: Controlling the Relative Isomorphism of Word Embedding Spaces.\u201d ArXiv abs/2210.05098 (2022): n. pag."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3521/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698605247044,
            "cdate": 1698605247044,
            "tmdate": 1699636305792,
            "mdate": 1699636305792,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4BWtyviCAg",
                "forum": "H4zAFFyoXK",
                "replyto": "2zj1Qv6bOT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3521/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3521/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate your accurate assessment of the strengths of our paper and your insightful comments about its weaknesses. Before responding to your specific questions, we would like to highlight your observation regarding the use of our method in a general-purpose QA system. In Section 4.3, we demonstrated through cross-modal conversation demos that our model can process general-purpose speech instructions and generate accurate responses. A video demonstration showcasing this capability is available at\u00a0[https://anonymous4blsp.github.io/iclr24-3521.github.io/](https://anonymous4blsp.github.io/iclr24-3521.github.io/),\u00a0as\u00a0referenced\u00a0in\u00a0our\u00a0paper.\n\n### Q1.\u00a0Loss\u00a0of\u00a0Speaker\u00a0and\u00a0Paralinguistic\u00a0Information\n\nAs discussed in Appendix D and referenced in the conclusion section, one of the limitations of our current method is the loss of speaker and paralinguistic information. Our focus is on aligning the semantic content of speech and text within the context of LLMs, a challenge that remains a focus of active research in the community [1,2,3]. Consequently, in the scope of our study, the statement \"the LLM should behave the same, whether given the speech segment or its transcripts as input\" holds true when considering semantic content alone.\n\nHowever, our approach provides a pretrained end-to-end speech-text LLM with potential adaptability to capture speaker and paralinguistic information through fine-tuning on appropriate training data. Our analysis in Table 8 of Section 4.2 demonstrates that our model can be more effectively fine-tuned for downstream tasks (ST in this case) compared to models without pretraining or those pretrained with ASR. We plan to explore ways to capture speaker and paralinguistic information in future work.\n\n\\[1\\]\u00a0Zhang,\u00a0Dong,\u00a0et\u00a0al.\u00a0\"Speechgpt:\u00a0Empowering\u00a0large\u00a0language\u00a0models\u00a0with\u00a0intrinsic\u00a0cross-modal\u00a0conversational\u00a0abilities.\"\u00a0_arXiv\u00a0preprint\u00a0arXiv:2305.11000_\u00a0(2023).\n\n\\[2\\]\u00a0Shu,\u00a0Yu,\u00a0et\u00a0al.\u00a0\"Llasm:\u00a0Large\u00a0language\u00a0and\u00a0speech\u00a0model.\"\u00a0_arXiv\u00a0preprint\u00a0arXiv:2308.15930_\u00a0(2023).\n\n\\[3\\]\u00a0Wang,\u00a0Mingqiu,\u00a0et\u00a0al.\u00a0\"SLM:\u00a0Bridge\u00a0the\u00a0thin\u00a0gap\u00a0between\u00a0speech\u00a0and\u00a0text\u00a0foundation\u00a0models.\"\u00a0_arXiv\u00a0preprint\u00a0arXiv:2310.00230_\u00a0(2023).\n\n### Q2.\u00a0Representations\u00a0of\u00a0Modalities\n\n**A.\u00a0Representation\u00a0Collapse**\n\nThe occurrence of mode collapse in speech features resulting from ASR training is not immediately evident. In computer vision, BLIP2 [4] has shown that training solely on image caption data can enable general instruction-following capabilities for zero-shot image-to-text generation tasks. Similarly, in the field of speech input, pretraining the model using the ASR task has been a common practice in prior work [1,2]. However, the benefits and limitations of this approach were not thoroughly studied.\n\nWe have explored different methods to utilize the ASR task for enabling instruction-following capabilities. As described in footnote 2, we used GPT4 to generate 100 diverse prompts for the ASR task, yet representation collapse still occurred. We also experimented with training on both English ASR and English-to-Chinese speech translation (ST) tasks using respective instructions. However, the resulting model still failed to demonstrate instruction-following capabilities for unseen tasks. Generally, training on specific downstream tasks tends to limit the model's abilities to those particular tasks, making it challenging to generalize to unseen instructions. This phenomenon has also been highlighted in a recently released work [5]. Additionally, we want to emphasize that labeled speech-text training data is quite limited outside of ASR and ST.\n\n\\[4\\]\u00a0Li,\u00a0Junnan,\u00a0et\u00a0al.\u00a0\"Blip-2:\u00a0Bootstrapping\u00a0language-image\u00a0pre-training\u00a0with\u00a0frozen\u00a0image\u00a0encoders\u00a0and\u00a0large\u00a0language\u00a0models.\"\u00a0_arXiv\u00a0preprint\u00a0arXiv:2301.12597_\u00a0(2023).\n\n\\[5\\]\u00a0Pan,\u00a0Jing,\u00a0et\u00a0al.\u00a0\"COSMIC:\u00a0Data\u00a0Efficient\u00a0Instruction-tuning\u00a0For\u00a0Speech\u00a0In-Context\u00a0Learning.\"\u00a0_arXiv\u00a0preprint\u00a0arXiv:2311.02248_\u00a0(2023).\n\n**B.\u00a0Choice\u00a0of\u00a0Fixed-Length\u00a0Subsampling**\n\nThank you for highlighting this important aspect and suggesting the exploration of dynamic downsampling strategies. We recognize that different downsampling methods can impact the model's performance, and there are indeed various design options to consider. In this study, our primary focus was on investigating the instruction-following capability for speech input. For this purpose, we chose a simple yet efficient approach, employing a CNN followed by a bottleneck layer as the adapter structure.\n\nWe are currently exploring more sophisticated methods for the adapter, which could potentially enhance the model's performance further. However, this exploration is part of our ongoing research and will be detailed in a future publication."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3521/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700122737508,
                "cdate": 1700122737508,
                "tmdate": 1700124105126,
                "mdate": 1700124105126,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "D1njYKDs6p",
                "forum": "H4zAFFyoXK",
                "replyto": "2zj1Qv6bOT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3521/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3521/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### Q3.\u00a0Empirical\u00a0results\n\n**A.\u00a0WER\u00a0on\u00a0Rare\u00a0Words**\u00a0\n\nWe acknowledge that a well-aligned speech-text LLM model should ideally perform ASR tasks with a lower WER, and we recognize that a 20% WER might seem high. In our experiments, the WER remains approximately 20% even after removing stop words. This outcome can be attributed to our training approach, which emphasizes semantic alignment over lexical precision. For instance, phrases like \"I love cats\" and \"I like cats\" would generate the same continuation in our model, as the adapter isn\u2019t explicitly trained to capture fine-grained lexical details.\n\nIn our continuing research, we have found that incorporating a modest amount (approximately 10%) of ASR data alongside our continuation writing data can notably enhance the WER, reducing it to about 4-5%, while simultaneously preserving the model's broad instruction-following capabilities. However, it is important to highlight that this paper primarily concentrates on developing methods to facilitate general instruction-following abilities. Consequently, we have intentionally chosen not to optimize performance for any specific task.\n\n**B.\u00a0Impact\u00a0of\u00a0Different\u00a0Prompts\u00a0on\u00a0WER**\n\nYou are indeed correct in noting that different instructions can influence WER. As highlighted in footnote 4, employing the prompt _\"Please repeat the following words.\"_ results in lower WER scores\u201417.0 on LibriSpeech-dev and 17.4 on LibriSpeech-test. However, our current method aligns speech and text at a semantic level. Consequently, the adapter cannot capture detailed lexical information, making it difficult to address this issue merely through changes in prompts.\n\n### Q4.\u00a0'Continuation\u00a0Writing'\u00a0vs.\u00a0'Next\u00a0Token\u00a0Prediction'\n\nThe term 'next token prediction' applies broadly to a variety of LLM training scenarios, including those that involve the ASR task. In such cases, the model optimizes the prediction of the reference transcript through 'next token prediction,' utilizing both textual instructions and speech features. However, we have chosen to use the term 'continuation writing' in our work to specifically distinguish our training task from traditional 'ASR' or other similar tasks. \n\n### Q5.\u00a0More\u00a0detailed\u00a0analysis\u00a0on\u00a0the\u00a0multilingual\u00a0space.\n\nThank you for the suggestion to quantitatively measure isomorphism.\n\nWe selected 1,000 quadruples (English text, English speech, French text, French speech) from the CVSS test set. We extracted text features at LLM input via word embedding and obtained speech features from the output of either the Whisper encoder or our modality adapter. Sentence embeddings were then computed through average pooling, allowing us to calculate the relational similarity.\n\n|       | **LLM\u00a0input** | **Whisper\u00a0Output** | **Adapter\u00a0Output** |\n| ----- | ------------- | ------------------ | ------------------ |\n| en-fr | 0.60          | 0.44               | 0.39               |\n\nAs indicated in the table, the multilingual space of LLM inputs shows strong isomorphism, as does the space of Whisper outputs. Even when using only English ASR data for learning the transformation, the adapter\u2019s output space retains the isomorphism characteristic of Whisper."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3521/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700122867339,
                "cdate": 1700122867339,
                "tmdate": 1700124053558,
                "mdate": 1700124053558,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PlLTuwCdKn",
                "forum": "H4zAFFyoXK",
                "replyto": "D1njYKDs6p",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3521/Reviewer_Dd3G"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3521/Reviewer_Dd3G"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your reply to my comments. It is nice to see that task-specific training improves performance (e.g. WER) without compromising on the general instruction-following ability."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3521/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700499983337,
                "cdate": 1700499983337,
                "tmdate": 1700499983337,
                "mdate": 1700499983337,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ng49aFrCG3",
            "forum": "H4zAFFyoXK",
            "replyto": "H4zAFFyoXK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3521/Reviewer_tfEL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3521/Reviewer_tfEL"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes training an adapter to connect an audio encoder to a text LLM so that speech tasks such as ASR, sentiment analysis, translation, and continuation writing can be performed using the same setup. Initial experiments with frozen Whisper encoder and Llama2 LLM does not show an improvement on the tasks. In Section 4.2. where there is fine-tuning of the models, there is some improvement on the translation task (Table 8)."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "Originality: \nThe paper puts simple ideas together to achieve multiple speech tasks in a single model which can also extend to other languages. \n\nQuality:\n1. Analysis in Section 2 on overtuning for ASR and not generalizing well to other speech tasks of models without adapter is useful. \n\nClarity: \nLimited at times. The goal of the paper is not always clear. \n\nSignificance: \nCombining speech and LLMs is getting popular as it was discussed in Section 5."
                },
                "weaknesses": {
                    "value": "Experiments do not show positive results especially when the audio encoder and the LLM is freezed. The goal of the paper is not clear to the reviewer.  \n- If the main contribution is the way the paper prepares the data, then it is not the only factor for success given the experimental results. \n- If the goal is to show the effect of fine-tuning, it provides some marginal gain. \n- If the goal is to show the usefulness of behavioral alignment, it seems as if giving \"noisy\" input in the sense that an audio, a text prompt and some other irrelevant (?) text and let the model ignore the continuation text and perform the recognition task. I might be misunderstanding this, but it sounds as if the experiment is doing some robustness improvement on the model with this type of \"noisy\" data."
                },
                "questions": {
                    "value": "1. Please clarify the goal of the experiments more clearly. \n\n2. Is having a BLUE score over 5 good enough in terms of model quality given that the baseline models achieve over 15? Similarly, a WER over 20% does not show the benefit of the model. \n\n3. Section 4.2 shows some minor improvements on the speech translation task. Are there similar experiments on other tasks discussed earlier in the paper?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3521/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3521/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3521/Reviewer_tfEL"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3521/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698610294669,
            "cdate": 1698610294669,
            "tmdate": 1700660645016,
            "mdate": 1700660645016,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EoCO7upjcv",
                "forum": "H4zAFFyoXK",
                "replyto": "ng49aFrCG3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3521/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3521/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your feedback. It appears there may be some misunderstanding regarding the motivation, contributions, and experimental design of our work. We encourage you to refer to the \"General Response\" provided above and the point-by-point response below for clarifications. We sincerely hope that these clarifications will be helpful for your reassessment of our research.\n\n### Q1.\u00a0The\u00a0Goal\u00a0of\u00a0The\u00a0Paper\n\nWhile we believe that an end-to-end approach can eventually outperform a pipeline approach with expanded capabilities (e.g., using paralinguistic information), that is not the immediate goal of this paper. As explained in the \"General Response,\" our objective is to extend the instruction-following capability of LLMs to speech input to enable zero-shot capabilities for unseen tasks in an end-to-end model.\n\nIn Section 4.1, we demonstrate this by using textual instructions to prompt the model for unseen speech-to-text generation tasks. For instance, our model, trained solely on continuation tasks, can be prompted during inference for ASR tasks as follows:\n\n    ###[Human]: Please transcribe the following speech into text. [speech features]\n    ###[Assistant]: [response]\n\nHere, the ASR instruction can be substituted with instructions for other tasks, such as ST and SLU tasks evaluated in the paper, or tasks like summarization and question-answering. We emphasize that our goal in this paper is not to outperform the pipelined approach, but to demonstrate the model's ability to follow instructions across modalities in an end-to-end manner. The comparison with the pipeline approach is to both highlight the promising results of the end-to-end approach and the performance gap with the pipeline approach, encouraging future research to further reduce this gap and eventually surpass it. This aspect is discussed in the limitations of the current approach in the Appendix and referenced in the conclusion section.\n\nIn Section 4.2, we present analyses to highlight different aspects of our model. However, these should not be viewed as the main experiment. Table 8 illustrates that our method allows for effective fine-tuning on downstream tasks. Figure 3 demonstrates successful speech-text modality alignment, while Figure 4 investigates the impact of data scale on alignment effectiveness.\n\n### Q2.\u00a0Performance\u00a0Gap\u00a0with\u00a0Baseline\u00a0Models\n\nAs explained in the response to Q1, the focus of our approach is on cross-modal instruction-following capabilities, rather than on the improvement of performance in specific tasks. Similar to the early days of end-to-end speech translation, our approach is not immediately expected to surpass the performance of ASR+LLM or Text+LLM models. Our primary contribution lies in proposing an end-to-end training methodology for speech-text LLMs, thereby demonstrating the model's valuable ability to follow instructions across modalities, despite the current performance gap. We hope our work will be beneficial for the research community in progressing towards the ultimate goal of developing more effective end-to-end approaches.\n\n### Q3.\u00a0Minor\u00a0Improvement\u00a0in\u00a0Table\u00a08\n\nAs noted in response to Q1, our main focus in this paper is on the instruction-following capabilities for speech input, as presented in quantitative zero-shot evaluations in Section 4. The analyses in Section 4.2 are designed to highlight different aspects of our model. Specifically, Table 8 in Section 4.2 is intended to demonstrate that our pretrained model is effective not only in its instruction-following capabilities but also for fine-tuning on downstream tasks. The improvement over models without pretraining is substantial in both BLEU and COMET metrics, and is significantly better than ASR pretraining, with an average improvement of 0.5 BLEU and 1 COMET score across eight directions. However, we wish to emphasize that this aspect of the model is not the main focus of our paper."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3521/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700122682611,
                "cdate": 1700122682611,
                "tmdate": 1700123695720,
                "mdate": 1700123695720,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "irvxs0czJD",
                "forum": "H4zAFFyoXK",
                "replyto": "EoCO7upjcv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3521/Reviewer_tfEL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3521/Reviewer_tfEL"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for clarification"
                    },
                    "comment": {
                        "value": "I would like to thank the authors for providing clarification. \n- Regarding the goal, I see that the goal is to demonstrate the text interpretation capabilities of LLM with audio - text alignment. \n\n- Q2, even though an initial attempt to achieve modality alignment in an end-to-end fashion (as compared to the ASR + LLM cascade) is challenging by itself and the results in Tables 3, 5, 6, 7 show decent performance, However, there is still a significant performance gap between the ASR-based and encoder based solutions.\n\n- As for Q3, I understand that Table 8 and 9 are additional experiments, and Table 8 shows some benefit of finetuning. However, the results in Table 9 are significantly worse than the baselines and hence, it is hard to say that the model has multilingual multilingual capacities.\n\nI admit that the study is a valuable effort and there are some signs that it is going to work better with further improvements. As a result, I am increasing my score to marginally below the threshold."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3521/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700660583294,
                "cdate": 1700660583294,
                "tmdate": 1700660583294,
                "mdate": 1700660583294,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4A3qRZjtj6",
            "forum": "H4zAFFyoXK",
            "replyto": "H4zAFFyoXK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3521/Reviewer_EDCC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3521/Reviewer_EDCC"
            ],
            "content": {
                "summary": {
                    "value": "This work introduces BLSP, which attempts to align speech and text modality in LLM. The model first starts collect supervised samples from LLM by generating text continuation based on speech text via instruction. Those supervised samples are then used to train a modality adapter on Whisper encoder, which helps align speech and text modality in LLM generation.\n\nThe experiment demonstrates that it can achieve unseen tasks to some extent even those the adapter was only trained with the continuation task. Although some tasks are not as good as the cascade systems, it shows some promising results in some tasks (i.e. speech understanding). Further analysis demonstrates that the text and speech embedding are better aligned using this approach and it also demonstrates some capabilities across languages."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "This work proposes to align speech and text modality in LLM and successfully show the proposed protocol allows the model to achieve unseen tasks even it is only trained with the continuation task. I think it has lots of potential for this direction.\n\nThe experiment analysis over a few speech tasks are convincing and demonstrate its usefulness, especially in the speech understanding task"
                },
                "weaknesses": {
                    "value": "The speech encoder is from Whisper-small which has only 120M parameter (244M/2 as it only uses encoder) , this is considerably much smaller than the LLM (7B). Using it as models/baselines might not be strong enough, although it might because it is bound by the large GPU memory caused by LLM.\n\nThe results of ASR/translation task still has a large gap with the text-based model."
                },
                "questions": {
                    "value": "In Figure 1, only 1 type (grey triangle) of speech embedding is plotted, where is the other speech embeddings? are they overlapped with other symbols?\n\nThe convolution modality reduces the length of the speech features by a factor of 8, how did authors choose this reduction factor? does author also try changing this factor? for instance, larger reducing factor might reduce lexical info, but make semantic info denser."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3521/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698767489767,
            "cdate": 1698767489767,
            "tmdate": 1699636305580,
            "mdate": 1699636305580,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TCI1gwJtit",
                "forum": "H4zAFFyoXK",
                "replyto": "4A3qRZjtj6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3521/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3521/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for recognizing the value of our work and for your insightful comments. Below are our responses to your questions.\n\n### Q1.\u00a0Choice\u00a0of\u00a0Speech\u00a0Encoder\u00a0&\u00a0Performance\u00a0Gap\u00a0with\u00a0Text-Based\u00a0Model\n\nWe acknowledge that using Whisper-small as the speech encoder might not represent the strongest baseline. However, our primary focus is not on the model's performance on downstream tasks, but rather on demonstrating our training method's ability to transfer instruction-following capabilities from text-based LLMs to the speech modality. As you pointed out, we opted for Whisper-small due to computational considerations during the development of our approach. However, we did conduct some experiments (not included in the paper) with Whisper-large models and observed improved zero-shot performance on unseen tasks. These experiments, however, did not alter the paper's primary conclusions.\n\nAs highlighted in Appendix D and referenced in the conclusion section, the performance gap compared to the pipelined approach remains a limitation of our current approach. We are actively exploring various strategies to bridge this gap and to expand the model's capabilities. It is our hope that the methodology proposed in this paper will contribute to the research community's efforts toward achieving these goals.\n\n### Q2.\u00a0Overlapping\u00a0Speech\u00a0Embeddings\n\nYou are correct in your observation. Models trained with ASR data tend to lose their ability to follow diverse instructions. When different instructions are used to prompt speech input, nearly identical features are produced, leading to the overlapping of variously colored triangles in Figure 1. We will make this point clear in the revision.\n\n### Q3.\u00a0Choice\u00a0of\u00a08x\u00a0Reduction\u00a0Factor\n\nWe agree that a larger reduction factor could diminish lexical information, as demonstrated in [1]. However, a smaller reduction factor would result in longer speech features, excessively consuming context in the LLM. This could decrease training and inference efficiency and limit the number of rounds in multi-turn interactions. In our empirical studies, we observed that 1 second of speech corresponds to approximately 3-4 text tokens. Given that the output frequency of the Whisper encoder is 50Hz, we chose an 8x downsampling factor to balance training efficiency with the preservation of lexical information. We believe that choosing a larger or smaller reduction factor would not alter the main conclusions of the paper. \n\n\\[1\\]\u00a0Fathullah,\u00a0Yassir,\u00a0et\u00a0al.\u00a0\"Prompting\u00a0large\u00a0language\u00a0models\u00a0with\u00a0speech\u00a0recognition\u00a0abilities.\"\u00a0_arXiv\u00a0preprint\u00a0arXiv:2307.11795_\u00a0(2023)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3521/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700122641681,
                "cdate": 1700122641681,
                "tmdate": 1700123595394,
                "mdate": 1700123595394,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "P8sA39QWLO",
                "forum": "H4zAFFyoXK",
                "replyto": "TCI1gwJtit",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3521/Reviewer_EDCC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3521/Reviewer_EDCC"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your comments, it is interesting to see a stronger whisper model can indeed improve the performance."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3521/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700534471427,
                "cdate": 1700534471427,
                "tmdate": 1700534471427,
                "mdate": 1700534471427,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "AiSMUzRHbz",
            "forum": "H4zAFFyoXK",
            "replyto": "H4zAFFyoXK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3521/Reviewer_ZRsu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3521/Reviewer_ZRsu"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a speech-text modality alignment method based on learning a lightweight modality adapter by continuation writing using continuations generated from LLM and speech transcript as supervised signals. Compared to ASR task-based pre-training, the proposed method gives good alignment and better speech translation performance."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper proposes a pre-training method for a lightweight modality adapter by continuation writing, which works better than ASR task-based pre-training."
                },
                "weaknesses": {
                    "value": "The advantage of the proposed method is unclear from the experiments."
                },
                "questions": {
                    "value": "Is it correct that if the modality adapter does nothing but output the input obtained from the encoder as it is (or learns an identical transformation), high alignment is obtained since you use an ASR system as the encoder? \n\nHow did you choose the structure of the adapter?\n\nIn the experiment in Table 8, where you update the speech encoder, what is the performance of the cascade approach with the fine-tuning of the ASR module?\n\nWhat is the performance if you use other speech encoders?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3521/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3521/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3521/Reviewer_ZRsu"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3521/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698845662930,
            "cdate": 1698845662930,
            "tmdate": 1699636305502,
            "mdate": 1699636305502,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JoRWKfNSx3",
                "forum": "H4zAFFyoXK",
                "replyto": "AiSMUzRHbz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3521/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3521/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your feedback. It appears there may be some misunderstanding regarding the motivation, contributions, and experimental design of our work. We encourage you to refer to the \"General Response\" provided above and the point-by-point response below for clarifications. We sincerely hope that these clarifications will be helpful for your reassessment of our research.\n\nIt's important to note that while we included studies on ASR task-based pretraining, our goal was to demonstrate the inadequacy of ASR task-based pretraining for our purpose, as models trained in this manner cannot follow general instructions at all. Therefore, in all experiments evaluating zero-shot instruction capabilities \u2013 which are central to our study \u2013 we do not compare our results with those of ASR task-based pretraining. The comparison with ASR task-based pretraining is made only in the fine-tuning experiment in Section 4.2. This is to illustrate that BLSP is a more effective pretraining method for fine-tuning on downstream tasks.\n\n### Q1.\u00a0Use\u00a0of\u00a0ASR\u00a0System\u00a0as\u00a0the\u00a0Speech\u00a0Encoder?\n\nWe believe there is a misunderstanding that we would like to clarify. The Whisper ASR system is comprised of an encoder for acoustic feature extraction and a decoder for text generation. In our work, we utilized only the pretrained Whisper encoder as the speech encoder for extracting acoustic features, while discarding the Whisper decoder. Our proposed model functions as an **end-to-end** system, where the speech encoder's role is to extract features from the input speech, rather than to transcribe the speech into text. Including a Whisper decoder, as used in a standard ASR system, would necessitate an argmax operation, which would impede the feasibility of training this model in an end-to-end manner. Additionally, this would introduce the limitations of a pipelined approach, as discussed in our \"General Response.\"\n\nThe modality adapter plays a significant role in aligning speech features with the textual space, and it does not merely perform an identical transformation. Learning a mapping that effectively transfers the instruction-following capability from text to speech input is a nontrival task. As demonstrated in Section 2, a modality adapter trained solely on the ASR task lacks general instruction-following capabilities.\n\n### Q2.\u00a0Choice\u00a0of\u00a0Adapter\u00a0Structure\n\nThe modality adapter in our study serves two primary functions. First, it reduces the length of output from the Whisper encoder, which operates at a frequency of 50Hz. Second, it maps the speech feature space to the textual space of the LLM. We acknowledge that various structural options exist for the adapter. For length reduction, methods like CNN downsampling, frame stacking, or q-transformer are potential choices. Similarly, for cross-modality alignment, both MLP and transformer-based methods are feasible.\n\nHowever, our study's focus was not on exhaustively exploring different adapter structures. Instead, our main contribution lies in proposing a novel training method to connect a frozen speech encoder with an LLM. Consequently, we opted for a simple yet effective approach, employing a CNN followed by a bottleneck layer for the adapter. This decision allowed us to concentrate on the core aspect of our research while maintaining effectiveness in achieving our objectives.\n\n### Q3.\u00a0Performance\u00a0of\u00a0Fine-Tuned\u00a0ASR\u00a0Module\u00a0in\u00a0a\u00a0Cascaded\u00a0System\n\nBLSP is designed as an end-to-end speech-text model. In the experiments outlined in Table 8, we fine-tuned the entire model using speech translation (ST) data via an end-to-end instruction-tuning approach:\n\n    ###[Human]: Please translate the following speech into German. [speech features]\n    ###[Assistant]: [translation]\n\nDuring this process, the Whisper encoder was fine-tuned along with other model parameters, but the corresponding Whisper decoder was not used or fine-tuned. Consequently, it's not feasible to measure the ASR performance of the fine-tuned Whisper encoder as would be done in a traditional Whisper ASR system. Moreover, since the ST data comprises (speech, translation) pairs rather than (speech, transcription) pairs, it does not lend itself to optimizing a separate Whisper ASR system. Therefore, we are unable to report the performance of a cascaded system using a fine-tuned ASR module.\n\nThe purpose of Table 8 is to show that our proposed approach, in comparison to direct training on ASR data, offers better generalization and is more suitable for fine-tuning on downstream tasks. However, we emphasize that this is not the main contribution of our work. Our primary contribution, as detailed in Section 4.1, is the demonstration of the cross-modal instruction-following capability of our model and its ability to generalize to unseen tasks through text instructions."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3521/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700122419660,
                "cdate": 1700122419660,
                "tmdate": 1700123437093,
                "mdate": 1700123437093,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fqrUlLom8p",
                "forum": "H4zAFFyoXK",
                "replyto": "AiSMUzRHbz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3521/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3521/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### Q4.\u00a0Using\u00a0a\u00a0Different\u00a0Speech\u00a0Encoder?\n\nWe acknowledge that there are various options for the speech encoder, such as the Whisper model's encoder [1], USM [2], or other pre-trained speech models, which could impact the model's performance. However, our work  focuses on the methodology of aligning a frozen speech encoder with a frozen LLM to achieve instruction-following capability for speech input on unseen tasks. As such, an in-depth investigation into different speech encoders is not the primary focus of this work, and we believe that changing the speech encoder would not fundamentally alter the conclusions of our paper.\n\nWe chose the encoder of the Whisper model due to its widespread recognition and to facilitate comparison with a strong ASR model in the pipelined approach. This choice mirrors our decision to use Llama2 as the LLM, despite the availability of many open-source LLM options. Our aim was to employ well-known and robust components to showcase the effectiveness of our methodology.\n\n\\[1\\]\u00a0Shu,\u00a0Yu,\u00a0et\u00a0al.\u00a0\"Llasm:\u00a0Large\u00a0language\u00a0and\u00a0speech\u00a0model.\"\u00a0arXiv\u00a0preprint\u00a0arXiv:2308.15930\u00a0(2023).\u00a0\n\n\\[2\\]\u00a0Wang,\u00a0Mingqiu,\u00a0et\u00a0al.\u00a0\"SLM:\u00a0Bridge\u00a0the\u00a0thin\u00a0gap\u00a0between\u00a0speech\u00a0and\u00a0text\u00a0foundation\u00a0models.\"\u00a0arXiv\u00a0preprint\u00a0arXiv:2310.00230\u00a0(2023)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3521/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700122447824,
                "cdate": 1700122447824,
                "tmdate": 1700123516591,
                "mdate": 1700123516591,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]