[
    {
        "title": "Federated Orthogonal Training: Mitigating Global Catastrophic Forgetting in Continual Federated Learning"
    },
    {
        "review": {
            "id": "mkd2Zn3wZQ",
            "forum": "nAs4LdaP9Y",
            "replyto": "nAs4LdaP9Y",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4007/Reviewer_VJN9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4007/Reviewer_VJN9"
            ],
            "content": {
                "summary": {
                    "value": "This paper discusses the concept of Continual Federated Learning (CFL), a real-world scenario where new tasks emerge over time in a decentralized data setting. In CFL, the main challenge is Global Catastrophic Forgetting, where the performance of the global model on old tasks deteriorates as it is trained on new tasks. While previous works have attempted to address this problem, they often rely on unrealistic assumptions about past data availability or violate privacy principles. To overcome these limitations, the authors propose a novel method called Federated Orthogonal Training (FOT). FOT works by extracting the global input subspace of each layer for old tasks and modifying the aggregated updates of new tasks in a way that ensures they are orthogonal to the global principal subspace of old tasks for each layer. This reduces interference between tasks, which is the main cause of forgetting. Empirical evidence shows that FOT outperforms existing continual learning methods in the CFL setting, achieving better average accuracy and lower forgetting while incurring minimal computation and communication costs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is well-written.\n2. The authors propose a CFL framework named Federated Orthogonal Training (FOT) to address the Global Catastrophic Forgetting problem. \n3. Within FOT, they introduce a novel aggregation method, named FedProject, which guarantees the orthogonality in a global manner without privacy leakage and more communication."
                },
                "weaknesses": {
                    "value": "My primary point of concern revolves around the differentiation in technical innovation when compared to related works. More precisely, the paper that introduced the GPM (s Gradient Projection Memory) method for centralized continual learning seems to employ a similar approach to address the issue of forgetting. This similarity is evident in the equations provided \u2013 Eq 8 and 9 in the GPM paper and Eq 12 in the current paper.\nI acknowledge that this paper introduces an additional layer of aggregation, which generates $A^l$ through secure aggregation in the context of federated learning. However, it's worth noting that secure aggregation inherently lends itself to operations such as summation or averaging. Consequently, I am inclined to question the extent of the technical contribution and novelty offered by this paper. Please correct me if I misunderstand something."
                },
                "questions": {
                    "value": "The author could have a clear statement about the technical novelty and contribution of this paper compared to previous contralized continual learning."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4007/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697988602833,
            "cdate": 1697988602833,
            "tmdate": 1699636362735,
            "mdate": 1699636362735,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "MsNT9rDgD1",
                "forum": "nAs4LdaP9Y",
                "replyto": "mkd2Zn3wZQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4007/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4007/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their feedback and positive comments about technical content. \n\n**Novelty of FOT**\n\nOur paper indeed uses a similar orthogonality idea as in GPM. However, the straightforward adaptation of GPM into federated learning is to make each client apply GPM locally. As shown in Table 1 (see GPM+FL), this does not work well in terms of average accuracy and forgetting. This is because GPM+FL only could extract local subspaces and the updates that are orthogonal to these local subspaces are not necessarily orthogonal to the global subspace after aggregation (See Figure 2). Besides, it adds additional computation cost and storage to the client. \n\nWhen trying to adopt that orthogonality idea to FL successfully, we need to solve 2 major problems which is our technical novelty in this paper:\n\n1 - How to make global model updates orthogonal to the previous task layer subspace?\n\n2 - More importantly, how to get the global principal subspace of layers in a privacy-preserving, communication, and computation cost-friendly way.\n\nIn our work we propose 2 methods FedProject and GPSE, addressing problems 1 and 2 respectively. FedProject is a novel aggregation method ensuring that the global model converges to the low-loss regions for all considered tasks and GPSE is proposed to yield the global principal subspace. \n\n*Novelty of GPSE:* Utilization of randomized linear algebra is the crucial part in this algorithm while Secure Aggregation is used as an additional privacy layer. Without randomization, clients would need to directly send the activations to the server. Then, the server would need to concatenate the activations rather than addition/averaging before computing the principal subspace. Recall that the computation of the principal subspace (without the additional random transform in Eq (9) of the manuscript) is a function of the matrix $X$ with each column representing activations for different data points not an aggregated sum of the activations across points. Hence, direct secure aggregation would not be applicable. \nFurthermore, the server would have direct access to the activations (data points itself for the first layer) which violates privacy considerations in FL, and the overall communication overhead for computing the subspace would scale with the number of data points in the FL system. \n\nGPSE solves these issues by utilizing methods in randomized linear algebra in a novel way. By applying random Gaussian transforms to the activations, we ensure that the size of the communicated matrices only grows with the model size rather than the data amount. Also, the required operation to do at the server side becomes linear addition, enabling the use of Secure Aggregation. \nLastly, the resulting matrix $A^\\ell$ on the server side protects the principal column subspace information and does not leak information to the server about the clients, adding a layer of privacy to the system."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4007/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699991066697,
                "cdate": 1699991066697,
                "tmdate": 1699991066697,
                "mdate": 1699991066697,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EseRQ249NJ",
                "forum": "nAs4LdaP9Y",
                "replyto": "MsNT9rDgD1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4007/Reviewer_VJN9"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4007/Reviewer_VJN9"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the author's prompt response. I have thoroughly reviewed the response and will discuss it with AC as well as other reviewers in the later phases. For now, I do not have any further questions."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4007/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700507686116,
                "cdate": 1700507686116,
                "tmdate": 1700507686116,
                "mdate": 1700507686116,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "y4J9xqr9U8",
            "forum": "nAs4LdaP9Y",
            "replyto": "nAs4LdaP9Y",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4007/Reviewer_vBpD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4007/Reviewer_vBpD"
            ],
            "content": {
                "summary": {
                    "value": "This Paper works on the continual federated learning problem. The author proposed a framework named Federated Orthogonal Training (FOT) to address the Global Catastrophic Forgetting problem."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper clearly describes the problem setting and states their target -- solving the catastrophic forgetting problem. The authors used illustrations, formulas, and well-written paragraphs to explain the FOT framework step by step clearly.\n2. When estimating the global principal, clients need to upload locally extracted principal subspace information to the server. The authors used knowledge from randomized SVD to realize this in a privacy-preserving way.\n3. In the paper, the authors discussed several important aspects, especially the privacy, of the algorithm that people worried about. \n4. Empirical results showed that the FOT provided significant improvement in average forgetting compared with extensive baselines in various benchmarks"
                },
                "weaknesses": {
                    "value": "1. There are still some baselines that are not compared in the paper, for example, the methods in paper [1],[2],[3].\n[1] Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE transactions on pattern analysis and machine intelligence, 40(12):2935\u20132947, 2017.\n[2] Jaehong Yoon, Wonyong Jeong, Giwoong Lee, Eunho Yang, and Sung Ju Hwang. Federated continual learning with weighted inter-client transfer. In International Conference on Machine Learning, pages 12073\u201312086. PMLR, 2021.\n[3] Jie Zhang, Chen Chen, Weiming Zhuang, and Lingjuan Lv. Target: Federated class-continual learning via exemplar-free distillation, 2023.\n2. In FOT, the global update was generated by projecting the aggregated updates onto the orthogonal subspace for each layer in the model. Intuitively, this will make the converge slower, which is directly related to potential extra communication costs. The authors did not provide a discussion about this."
                },
                "questions": {
                    "value": "1. On page 6, Theorem 1, the author mentioned that \"for sufficiently large n, the principal column space of Y recovers the low-rank column space of A up to rank k with a negligible error.\". In my understanding, n corresponds to the number of data in every client. Then what is the number of data in your experiments? For the 100 clients' experiments, is there enough data? If not, how do you explain the effectiveness of the FOT?\n2. Authors said that they used the same round number for different methods. How did you decide the round number? Did other methods converge earlier than the round number?\n3. According to the description of the FOT, to me, it seems like that FOT would also work well if we only apply this to several layers in the model. Have the authors tried this?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4007/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4007/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4007/Reviewer_vBpD"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4007/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698451602484,
            "cdate": 1698451602484,
            "tmdate": 1700686851316,
            "mdate": 1700686851316,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "533a3LqGl8",
                "forum": "nAs4LdaP9Y",
                "replyto": "y4J9xqr9U8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4007/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4007/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their feedback and positive comments about technical content. We address your questions below point by point.\n\n**Weakness 1**\n\n*Comparison with Other Baselines:* LwF [1] is a centralized continual learning algorithm proposed in 2017. While determining baselines from centralized CL, our approach involved a thorough examination of both the SOTA methodologies and well-established earlier works. Notably, FedLwF is compared with FedCIL in [A] and shown to perform much worse. Given the inclusion of FedCIL in our baseline, and show that FOT has superior performance in comparison, we have chosen to exclude FedLwF from our analysis.\n\nWhile categorized under Continual Federated Learning, it's crucial to recognize that FedWeIT [2] addresses a fundamentally distinct problem. It is primarily interested in learning individual local tasks, not learning a global model but utilizing the information of other clients. It employs federated learning not to train a global model but to enhance the continual learning process for individual local tasks by leveraging information from other clients.  Due to the distinct objective of this paper in comparison to ours, we haven\u2019t included it in our baseline. \n\nWe mention TARGET [3] in our related work section (Section 2 and Appendix B). Unfortunately, due to its publication date being in close proximity to the submission deadline, we were unable to incorporate it into our baselines.\nFollowing the reviewer\u2019s advice, we did an initial comparison of TARGET with FOT on mini-Imagenet dataset. At the end of each task, the generator in TARGET is trained on the server side in a data-free way.  Then using that generative model, the server generates synthetic samples (750 samples per task in our experiments) and sends them to the clients. Clients perform knowledge distillation between the current model and the stored old-task model with the synthetic samples to prevent forgetting.\n\n|  | IID |  | nonIID |  |\n|---|---|---|---|---|\n|  | ACC(%) | FGT(%) | ACC(%) | FGT(%) |\n| FedAvg | 50.43 | 33.08 | 41.00 | 32.92 |\n| TARGET | 58.45 | 25.12 | 53.67 | 26.31 |\n|FedCIL | 55.98 | 28.90 | 51.12 | 25.66 |\n| FOT | 69.07 | 0.19 | 62.06 | 0.17 |\n\nSimilar to FedCIL, TARGET achieves a better forgetting and accuracy compared to vanilla FedAvg, but is outperformed by our FOT approach. We would like to highlight that these only represent initial comparisons with TARGET and we plan to extend to a more thorough comparison on other datasets in the revised manuscript in Appendix G.7.  Given your suggestions, we have also extended our related work discussion in the manuscript to highlight [1] and [2].\n\n**Weakness 2 & Question 2**\n\nIn the pursuit of fairness, we empirically identified the optimal number of rounds for the base case, namely FedAvg, and employed this determined round number consistently across all other methods.\n\n*Convergence of FOT:* Thank you for asking this question. FOT does not exhibit slower convergence; However, it is crucial to highlight that in subsequent tasks, convergence is directed toward the optimal point within a restricted convex set through a subspace projection. While this projection doesn't slow down the convergence speed, it does lead to a slight decrease in that task\u2019s accuracy due to regularization by the target convex set. \nIn the updated version of the manuscript (Appendix G.5), we discuss the convergence behaviors of FOT and share the accuracy-round plots. As we can see from these plots, the convergence behavior of FOT is similar to FedAVG. While FOT may exhibit lower accuracy than FedAvg in a specific task, FOT maintains this accuracy in later tasks. On the other hand, the accuracy of other methods tends to decline below FOT's performance as new tasks are introduced. That\u2019s why, at the end of training a sequence of different tasks, the average accuracy of FOT is higher and the forgetting is less than other methods.\n\n\n\n\n**Question 1**\n\nIn the server, after aggregating all local $A_{t,i}^\\ell$, the server gets global  $A_{t}^\\ell$ (see equation 10). This new matrix is equal to the multiplication of global input matrix $X^\\ell$ and random Gaussian matrix $G$. Therefore, dimension n from the theorem corresponds to the second dimension of $X^\\ell$ which is equal to the total number of samples distributed among clients. The value of n remains constant for the task whether we simulate a task with 100 clients or 50 clients. For the specific quantity of data for each task and dataset, refer to Table 8 in the manuscript, specifically examining the ' # training/task' rows.\n\n\n[A] Daiqing Qi, Handong Zhao, and Sheng Li. Better generative replay for continual federated learning, ICLR, 2023."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4007/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699990664327,
                "cdate": 1699990664327,
                "tmdate": 1699990664327,
                "mdate": 1699990664327,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5HDSa8mcDt",
                "forum": "nAs4LdaP9Y",
                "replyto": "y4J9xqr9U8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4007/Reviewer_vBpD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4007/Reviewer_vBpD"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "The authors have addressed all my concerns and I have raised my score."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4007/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700686878505,
                "cdate": 1700686878505,
                "tmdate": 1700686878505,
                "mdate": 1700686878505,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "UYTzL0Xg6T",
            "forum": "nAs4LdaP9Y",
            "replyto": "nAs4LdaP9Y",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4007/Reviewer_Zv1b"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4007/Reviewer_Zv1b"
            ],
            "content": {
                "summary": {
                    "value": "To address the catastrophic forgetting problem in the context of federated continual learning, the paper introduces a novel method that leverages orthogonalization of tasks to mitigate global forgetting in the course of continuous learning. This approach effectively reduces interference between distinct tasks, as demonstrated empirically. FOT exhibits superior performance compared to existing methods, manifesting improvements in accuracy and reduction in forgetting rates, all while incurring minimal additional computational and communication costs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The study presents a CFL framework, Federated Orthogonal Training (FOT), which addresses Global Catastrophic Forgetting by modifying global updates for new tasks to reduce interference with previous tasks. FOT also ensures client privacy, eliminates the need for client-side storage, and outperforms other methods in cross-device settings, even though they have additional computation and storage requirements."
                },
                "weaknesses": {
                    "value": "Communication overhead represents a significant weakness in the methodology presented in the paper. However, the paper's analysis is somewhat superficial, lacking a comparison with baseline experiments. The argument regarding the minimized communication overhead is not sufficiently elaborated upon.\n\nSimilarly, Remark 1 highlights that the convergence analysis in the paper is relatively straightforward and lacks comprehensive theoretical underpinnings."
                },
                "questions": {
                    "value": "I have a question of how to understand the correctness of the orthogonal process in this paper (intuitively)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4007/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698731185473,
            "cdate": 1698731185473,
            "tmdate": 1699636362550,
            "mdate": 1699636362550,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0ZUwVNi4nj",
                "forum": "nAs4LdaP9Y",
                "replyto": "UYTzL0Xg6T",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4007/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4007/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their feedback and positive comments about technical content. We address your questions below point by point.\n\n**Communication Overhead:**\nWe would like to emphasize that the additional communication cost incurred by FOT is negligible. First, it is important to note that this additional cost is associated only with the GPSE step, and this step consists of just one round, conducted once at the end of each task. In this round, the server transmits $\\\\{ O_\\ell \\\\}^L_{\\ell=1}$ to the clients, and the clients send only $\\\\{ A_\\ell \\\\}^L_{\\ell=1}$ to the server. Importantly, the communication cost of GPSE remains independent of the amount of data and is scaled only with the model size. Furthermore, while the communication complexity aligns with the model size, the actual size is observed to be smaller than the model itself as shown in Table 5.\n\nAs a numerical example from our experiments, if we train Alex-net on one Cifar100 task for 1000 federated learning rounds by communicating the model, then the FOT cost and FedAVG incur exactly the same cost (2840 MB in total) over the 1000 model training rounds because FOT only communicates the model between clients and the server (as in FedAvg).  At the end of 1000 rounds, GPSE uses a communication load less than or equal to only a single round which is much smaller than rounds used for model aggregation because GPSE communicates $\\\\{ O_\\ell \\\\}^L_{\\ell=1}$  and $\\\\{ A_\\ell \\\\}^L_{\\ell=1}$ and their total size (0.29 mb - See table 5) is less than model size (1.42 mb - See table 5 ). As a result, the additional communication cost is less than 0.1% of the total training cost for a single task. That\u2019s why we claim the additional communication overhead is minimal.\n\nIn contrast, recent federated continual learning work -  FedCIL [1] has a huge overhead in terms of communication. FedCIL communicates a generative model with the original classifier in each training round (compared to FOT which only does additional communication only once per task). Given the generative model is bigger (1.60 mb) than the classifier, it increases the communication cost by more than 100%. \n\nWe provide the total communication cost (in MB) of one client at the end of each Cifar100 task in the below table. We give a more extensive description of the additional communication cost in Appendix G.4 in the updated version of the paper.\n\n| Task | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 |\n|---|---|---|---|---|---|---|---|---|---|---|\n| FedAvg | 2840.0 | 5680.0 | 8520.0 | 11360.0 | 14200.0 | 17040.0 | 19880.0 | 22720.0 | 25560.0 | 28400.0 |\n| FOT | 2840.0 | 5680.3 | 8520.6 | 11360.9 | 14201.2 | 17041.4 | 19881.7 | 22722.0 | 25562.3 | 28402.6 |\n| FedCIL | 2840.0 | 8880.0 | 14920.0 | 20960.0 | 27000.0 | 33040.0 | 39080.0 | 45120.0 | 51160.0 | 57200.0 |\n\n**Convergence Analysis:** The paper presents an approach for enabling continual learning in a federated learning setting. We rely on an orthogonalization approach, however, we take careful consideration in allowing this without violating the privacy requirements and the convergence guarantees in FL. Our proposed approach allows us to achieve protection against forgetting using projection into a convex set which allows us to rely on convergence guarantees in the literature for ProximalSGD. We would argue that this simplicity is a strength of the proposed method as it allows us to guarantee theoretical convergence with privacy guarantees and little communication overhead in addition to observed empirical benefits. \n\nWe would also like to point out that the central theoretical challenge in the considered problem and our proposed approach is enabling the learning of principal subspaces and the application of orthogonal projection in a distributed privacy-preserving setting. \n\n\n[1] Daiqing Qi, Handong Zhao, and Sheng Li. Better generative replay for continual federated learning, ICLR, 2023."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4007/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699989634112,
                "cdate": 1699989634112,
                "tmdate": 1699989634112,
                "mdate": 1699989634112,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "StlI3MILBn",
                "forum": "nAs4LdaP9Y",
                "replyto": "0ZUwVNi4nj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4007/Reviewer_Zv1b"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4007/Reviewer_Zv1b"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response!"
                    },
                    "comment": {
                        "value": "I've read all responses, and think this is a good paper for accept."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4007/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700736374567,
                "cdate": 1700736374567,
                "tmdate": 1700736374567,
                "mdate": 1700736374567,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]