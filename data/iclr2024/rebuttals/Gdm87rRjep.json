[
    {
        "title": "Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt"
    },
    {
        "review": {
            "id": "u62iUU5C7A",
            "forum": "Gdm87rRjep",
            "replyto": "Gdm87rRjep",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4054/Reviewer_jG4W"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4054/Reviewer_jG4W"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors first identify the efficiency problems with the inference process of Large Language Models (LLMs). Then, the authors find that quantization and pruning are utilized to deal with the efficiency problem while incurring performance issues, i.e., the PPL of the model is high. Then, the authors propose a soft prompt approach to improve the performance of the quantized or pruned model. In addition, the authors validate three aspects of the proposed methods, i.e., cross dataset transferability, cross compression transferability, and cross-task transferability."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The approach of soft prompt to address the performance of the quantized or pruned LLM is effective.\n2. The analysis of the three aspects of the proposed approach is interesting.\n3. The paper is well organized and easy to follow."
                },
                "weaknesses": {
                    "value": "1. The approach is simple and the novelty is not obvious. The soft prompt method is already proposed in multiple existing papers.\n2. The evaluation is only conducted with PPL. However, the evaluation of LLM should be well designed and other methods should be exploited to further validate the proposed method.\n3. More tasks should be examined to show the effectiveness of the proposed approach."
                },
                "questions": {
                    "value": "1. I wonder if there are other LLM evaluation methods besides PPL.\n2. I wonder it the method can be applied to other tasks, e.g., captioning.\n3. I wonder what would be the difference between the proposed methods and conventional soft prompt tuning except using different LLMs, i.e., one with quantized or pruned model while the other one with the original model."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4054/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698578946129,
            "cdate": 1698578946129,
            "tmdate": 1699636369373,
            "mdate": 1699636369373,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3hAp5D4s5N",
                "forum": "Gdm87rRjep",
                "replyto": "u62iUU5C7A",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4054/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4054/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks & Initial Response to Reviewer jG4W (1/2)"
                    },
                    "comment": {
                        "value": "### **[W1 - The approach is simple and the novelty is not obvious: Our paper uncovers new properties of prompt tuning and they offer new opportunities for model compression.]**\n\nWhile novelty is a multifaceted concept in academic research, we believe it can be roughly viewed from two fronts: ***empirical novelty***, which involves uncovering new properties and behaviors unknown to the wider community (e.g., the emergent ability [1], lottery ticket hypothesis [2]), and ***technical novelty***, which pertains to the development of new solutions or methodologies (e.g., LoRA [3]).\n\nWe argue our work is rich in empirical novelty in at least two aspects:\n\n1. **Transformation of Prompt Tuning from Task-Specific to Transferable:** Prior to our study, only a few works studied the transferability of learned prompts between different tasks [4, 5, 6]. Specifically, [4] finds it is possible to transfer learnable prompts with **additional fine-tuning on downstream task**. However, as mentioned in the experiment section, all our reported results are **zero-shot**, i.e., no fine-tuning is needed to obtain transferability. Furthermore, [5] finds that the learned prompt can only be transferred among similar tasks. However, as verified by our experiments, our learned prompts are transferable between datasets, tasks, and compressed models.\n\n2. **New avenue to enhance Compressed Model Accuracy:** We show that prompt tuning can effectively recover the accuracy drop of compressed models. Traditional model compression methods often demand extensive engineering efforts and intricate designs, as seen in popular model compression papers like GPTQ [7], SPQR [8], and AWQ [8]. However, our method simplifies this process significantly. We leverage prompt tuning to effectively recover the accuracy drop in compressed models, opening a new avenue to optimize the trade-off between accuracy and efficiency.\n\nThese findings are not only valid but also generalizable across various model families, datasets, and tasks, underscoring the broad applicability and impact of our work.\n\nRegarding technical contribution, we agree that our method is a direct application of prompt tuning. However, we emphasize that **we intensionally kept our method as simple as possible**. This simplicity, is in our opinon more of a strength than a weakness, as it just underscores the unknown properties of straightforward prompt tuning methods; where the uncovering of these properties and the new pathways they opened for model compression constitute the true novelty of our paper. Thereby, we argue that we contribute substantially to the field's understanding of prompt tuning and model compression.\n\n\n\n\n[1] Emergent Abilities of Large Language Models\n\n[2] The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks\n\n[3] LoRA: Low-Rank Adaptation of Large Language Models\n\n[4] On Transferability of Prompt Tuning for Natural Language Processing\n\n[5] SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer\n\n[6] Reducing Retraining by Recycling Parameter-Efficient Prompts\n\n[7] GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers\n\n[8] SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression\n\n[9] AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4054/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700368287027,
                "cdate": 1700368287027,
                "tmdate": 1700368287027,
                "mdate": 1700368287027,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8HAyZiD4LG",
                "forum": "Gdm87rRjep",
                "replyto": "u62iUU5C7A",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4054/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4054/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks & Initial Response to Reviewer jG4W (2/2)"
                    },
                    "comment": {
                        "value": "### **[W2,Q1,Q2 - The evaluation is only conducted with PPL: We have in fact evaluated on other tasks]**\n\n**We kindly direct the reviewer's attention to [Appendix A.3](https://openreview.net/pdf?id=Gdm87rRjep#page=15)** , where we have explored QA (OpenBookQA, PIQA), commonsense NLI (Hellaswag), language understanding (high school European history from MMLU) tasks reported with non-PPL metrics. We highlight that all these reported results are zero-shot. Namely, the prompt is learned on C4, and then we directly stitch the prompt to the compressed model and test the performance on these downstream tasks. We hope the reviewer may find them helpful.\n\n\n| Models |                  | OpenbookQA      | Hellaswag       | PIQA            | High School European History |\n|--------|------------------|-----------------|-----------------|-----------------|------------------------------|\n| Full   |                  | 0.410\u00b10.022     | 0.497\u00b10.005     | 0.702\u00b10.011     | 0.364\u00b10.038                  |\n| 50%    | w./o. Prompt     | 0.412\u00b10.022     | 0.449\u00b10.005     | 0.682\u00b10.011     | **0.364\u00b10.038**              |\n|        | + Learned Prompt | 0.400\u00b10.022     | **0.469\u00b10.005** | **0.689\u00b10.011** | 0.358\u00b10.037                  |\n| 62.5%  | w./o. Prompt     | 0.396\u00b10.022     | 0.380\u00b10.005     | 0.638\u00b10.011     | 0.345\u00b10.037                  |\n|        | + Learned Prompt | **0.402\u00b10.022** | **0.433\u00b10.005** | **0.668\u00b10.011** | 0.345\u00b10.037                  |\n| 75%    | w./o. Prompt     | 0.366\u00b10.022     | 0.280\u00b10.004     | 0.549\u00b10.012     | 0.315\u00b10.036                  |\n|        | + Learned Prompt | 0.358\u00b10.021     | **0.344\u00b10.005** | **0.614\u00b10.011** | **0.358\u00b10.037**              |\n| 4-bit  | w./o. Prompt     | 0.410\u00b10.022     | 0.487\u00b10.005     | 0.690\u00b10.011     | 0.358\u00b10.037                  |\n|        | + Learned Prompt | **0.418\u00b10.022** | 0.487\u00b10.005     | **0.692\u00b10.011** | 0.352\u00b10.037                  |\n| 3-bit  | w./o. Prompt     | 0.378\u00b10.022     | 0.446\u00b10.005     | 0.674\u00b10.011     | 0.358\u00b10.037                  |\n|        | + Learned Prompt | **0.404\u00b10.022** | **0.459\u00b10.005** | **0.688\u00b10.011** | 0.358\u00b10.037                  |\n| 2-bit  | w./o. Prompt     | 0.354\u00b10.021     | 0.240\u00b10.004     | 0.491\u00b10.012     | 0.315\u00b10.036                  |\n|        | + Learned Prompt | 0.350\u00b10.021     | **0.294\u00b10.005** | **0.563\u00b10.012** | **0.333\u00b10.037**              |\n\n\n\n### **[W3 - The difference between the proposed methods and conventional soft prompt tuning: Transformation of Prompt Tuning from Task-Specific to Transferable]**\n\n\nAs we mentioned in our response to W1 above, the key difference between our prompt tuning and conventional soft prompt tuning is that our learned soft prompts are transferable between datasets, tasks, and compression levels. The transferring of prompts saves the prompt tuning time and effort originally required for each specific setups (according to our experience with 4 GPUs, that means 5-ish hours per task per compressed model).\n\nWe also already compared our methods with conventional soft prompt tuning in [experiment](https://openreview.net/pdf?id=Gdm87rRjep#page=8) (**NOTE**: it was originally in Table 4 of Appendix A.3, we move it to the main text in the updated version). For your convenience, we post our results and summarize our observations here:\n\n\n> Perplexity comparison between full model and quantized models with different prompts. Where we report test perplexity on PTB and Wikitext-2 dataset. \"w./o. prompt\" refers to the quantized model without soft prompts. \"w./ direct prompt\" means the soft prompts are directly trained on the target dataset. \"w./ transferred prompt\" means the prompt is trained on C4 dataset and then transferred to the target dataset.\n\n| Model                       | PTB     | Wikitext2 |\n|-----------------------------|---------|-----------|\n| Full Model                  | 11.02   | 6.33      |\n| Full Model w./ direct prompt| 6.86    | 5.57      |\n| 4-bit w./o. prompt          | 11.65   | 6.92      |\n| 4-bit w./ direct prompt     | 7.04    | 5.88      |\n| 4-bit w./ transferred prompt| 9.25    | 6.26      |\n| 3-bit w./o. prompt          | 15.74   | 9.45      |\n| 3-bit w./ direct prompt     | 7.76    | 6.33      |\n| 3-bit w./ transferred prompt| 10.81   | 6.90      |\n| 2-bit w./o. prompt          | 5883.13 | 2692.81   |\n| 2-bit w./ direct prompt     | 14.98   | 16.67     |\n| 2-bit w./ transferred prompt| 29.82   | 20.56     |\n\nGiven direct prompt receives a task-specific loss, our transferred prompt is, as expected, not as competitive as the direct one. However, such transferred prompt may significantly bridge the gap between a compressed and full model \u2014 e.g., our 3-bit & 4-bit quantized LLaMA-7B with transferred prompt can deliver on-par or better PPL than the full model on PTB and Wikitext2. We'd say this is an especially worthy contribution in practice, as one may possibly download the open-sourced transferable prompt to help on a compressed model with little effort."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4054/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700368334027,
                "cdate": 1700368334027,
                "tmdate": 1700544997831,
                "mdate": 1700544997831,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "irgHivdYRq",
            "forum": "Gdm87rRjep",
            "replyto": "Gdm87rRjep",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4054/Reviewer_34kz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4054/Reviewer_34kz"
            ],
            "content": {
                "summary": {
                    "value": "In order to restore the performance of compressed models (either quantized or weight sparsified, or both), this paper applies prefix tuning on compressed models, and studies whether a prefix decided on one dataset, one compression rate, or one task can generalize to others. This paper experiments on compressed OPT (1.3B, 2.7B, 6.7B) and LLaMA (7B) models, using C4, Wikitext-2, and Penn Treebank for perplexity evaluation, and OpenbookQA, Hellaswag, PIQA, and HSEH for zero-shot accuracy evaluation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The research question of how to make LLMs smaller and maintain their performance is very important.\n* Tuning the prompt prefix is a reasonable way to help with that.\n* The paper has empirically studied the transferability of the learned prefix to other compression rates and datasets."
                },
                "weaknesses": {
                    "value": "* Novelty is limited. The literature has applied the soft prefix tuning method to uncompressed models, and this paper applies the soft prefix tuning method to a compressed model. It's like a verification scenario of the prefix tuning method.\n* Experimental verification needs to be improved: \n  - Experiments are only conducted on small and relatively old models (4 OPT and LLaMA-v1, all <7B).\n  - Do not compare with other strategies of finetuning in model compression, e.g., how about we apply LoRA tuning to restore the performance of model compression?\n\n## Minor\n* There are some minor writing issues, need some proofreading:\n  - Unify \u201cPPL\u201d and \u201cPerplexity\u201d in Figure 2.\n  - No caption for Figure 6.\n  - One result is wrongly colored in Table 5. 50% row, HSEH column."
                },
                "questions": {
                    "value": "* Can the prompt prefix generalize across different models?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4054/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698637008280,
            "cdate": 1698637008280,
            "tmdate": 1699636369301,
            "mdate": 1699636369301,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Xg37lHh5R1",
                "forum": "Gdm87rRjep",
                "replyto": "irgHivdYRq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4054/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4054/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks & Initial Response to Reviewer 34kz (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the comments and suggestions to improve our paper. Please see the following clarifications.\n\n### **[W1 - Novelty is limited: Our paper uncovers new properties of prompt tuning and they offer new opportunities for model compression]**\n\n\nWhile novelty is a multifaceted concept in academic research, we believe it can be roughly viewed from two fronts: ***empirical novelty***, which involves uncovering new properties and behaviors unknown to the wider community (e.g., the emergent ability [1], lottery ticket hypothesis [2]), and ***technical novelty***, which pertains to the development of new solutions or methodologies (e.g., LoRA [3]).\n\nWe argue our work is rich in empirical novelty in at least two aspects:\n\n1. **Transformation of Prompt Tuning from Task-Specific to Transferable:** Prior to our study, only a few works studied the transferability of learned prompts between different tasks [4, 5, 6]. Specifically, [4] finds it is possible to transfer learnable prompts with **additional fine-tuning on downstream task**. However, as mentioned in the experiment section, all our reported results are **zero-shot**, i.e., no fine-tuning is needed to obtain transferability. Furthermore, [5] finds that the learned prompt can only be transferred among similar tasks. However, as verified by our experiments, our learned prompts are transferable between datasets, tasks, and compressed models.\n\n2. **New avenue to enhance Compressed Model Accuracy:** We show that prompt tuning can effectively recover the accuracy drop of compressed models. Traditional model compression methods often demand extensive engineering efforts and intricate designs, as seen in popular model compression papers like GPTQ [7], SPQR [8], and AWQ [8]. However, our method simplifies this process significantly. We leverage prompt tuning to effectively recover the accuracy drop in compressed models, opening a new avenue to optimize the trade-off between accuracy and efficiency.\n\nThese findings are not only valid but also generalizable across various model families, datasets, and tasks, underscoring the broad applicability and impact of our work.\n\nRegarding technical contribution, we agree that our method is a direct application of prompt tuning. However, we emphasize that **we intensionally kept our method as simple as possible**. This simplicity, is in our opinon more of a strength than a weakness, as it just underscores the unknown properties of straightforward prompt tuning methods; where the uncovering of these properties and the new pathways they opened for model compression constitute the true novelty of our paper. Thereby, we argue that we contribute substantially to the field's understanding of prompt tuning and model compression.\n\n\n\n\n[1] Emergent Abilities of Large Language Models\n\n[2] The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks\n\n[3] LoRA: Low-Rank Adaptation of Large Language Models\n\n[4] On Transferability of Prompt Tuning for Natural Language Processing\n\n[5] SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer\n\n[6] Reducing Retraining by Recycling Parameter-Efficient Prompts\n\n[7] GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers\n\n[8] SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression\n\n[9] AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4054/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700368049135,
                "cdate": 1700368049135,
                "tmdate": 1700368063801,
                "mdate": 1700368063801,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xdAVzjlmdy",
                "forum": "Gdm87rRjep",
                "replyto": "irgHivdYRq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4054/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4054/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks & Initial Response to Reviewer 34kz (2/2)"
                    },
                    "comment": {
                        "value": "### **[W2 - Compare against LoRA tuning: Sure! Our methods outperform LoRA in more recent and larger models]**\n\nBelow, we compare our approach and LoRA tuning on LLaMA-2-13B and BLOOM-7B models with 3-bit quantization using GPTQ. \n\n> Results suggest that our soft prompt approach vastly outperforms LoRA. In fact, the 3-bit quantized LLaMA-2-13B and BLOOM-7B equipped with our approach may even exceed their FP16 counterparts.\n\n| Dataset | Model                       | Precision | Recover method | Trainable Params (M) | ppl   |\n|---------|-----------------------------|-------|--------|----------------------|-----------|\n| C4      | LLaMA-2-13B                 | fp16  | NA     | NA                    | 6.96      |\n| C4      | LLaMA-2-13B           | 3bit  | NA     | NA                    | 9.24      |\n| C4      | LLaMA-2-13B           | 3bit  | Ours   | 0.5                | **6.75**      |\n| C4      | LLaMA-2-13B           | 3bit  | LoRA   | 26               | 8.15      |\n| C4      | BLOOM-7B                     | fp16  | NA     | NA                    | 15.87     |\n| C4      | BLOOM-7B                     | 3bit  | NA     | NA                    | 18.40     |\n| C4      | BLOOM-7B                     | 3bit  |  Ours  | 0.4               | **13.54**     |\n| C4      | BLOOM-7B                     | 3bit  | LoRA   | 15.7             | 17.26     |\n\n\n---\n\nGiven that both LoRA weights and soft prompts are technically transferrable, we also compare the transferability between the two methods by first training them on the C4 dataset, then transferring and testing on the wikitext2 dataset. \n\n> Results suggest our soft prompt approach still outperforms LoRA in this transferred setting.\n\n| Dataset   | Model       | Precision | Method | Transferred Params (M) | ppl       |\n|-----------|-------------|-------|--------|------------------------|---------------|\n| wikitext2 | LLaMA-2-13B | fp16  | NA     | NA                      | 5.58          |\n| wikitext2 | LLaMA-2-13B | 3bit  | NA     | NA                      | 7.88          |\n| wikitext2 | LLaMA-2-13B | 3bit  | Ours   | 0.5                  | **5.89**      |\n| wikitext2 | LLaMA-2-13B | 3bit  | LoRA   | 26                 | 7.07     |\n| wikitext2 | BLOOM-7B     | fp16  | NA     | NA                      | 13.26         |\n| wikitext2 | BLOOM-7B     | 3bit  | NA     | NA                      | 16.06         |\n| wikitext2 | BLOOM-7B     | 3bit  | Ours   | 0.4                 | **12.42**     |\n| wikitext2 | BLOOM-7B     | 3bit  | LoRA   | 15.7              | 15.65    |\n\n\n### **[Q1 - Can the prompt prefix generalize across different models? Depending on whether such models share the same tokenizer and embedding dim]**\n\nA learned prompt can be generalized within the models that share the same tokenizer & embedding dim (e.g., LLaMA 1 and 2, or simply any model with different compression levels). While projecting soft prompt into different dimensions is certainly doable (e.g., project LLaMA-7B trained soft prompt with `4096` dim into `8192` to work with LLaMA-65B's), we leave it to future work. Yet, models with different tokenizers will require extra steps to form alignment, as different tokenizers will interpret the same soft prompt differently."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4054/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700368119868,
                "cdate": 1700368119868,
                "tmdate": 1700369409619,
                "mdate": 1700369409619,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kcwro9rCrE",
                "forum": "Gdm87rRjep",
                "replyto": "Xg37lHh5R1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4054/Reviewer_34kz"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4054/Reviewer_34kz"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response & Follow-up comments"
                    },
                    "comment": {
                        "value": "I thank the authors for the detailed response and the added experiment! I'm still concerned about the technical or empirical novelty.\n\nFirst, I hold a different opinion regarding the \"New avenue to enhance Compressed Model Accuracy\" point. technically, this work is a prompt fine-tuning method, which should be put under the same context as LoRA-based tuning, rather than saying it's simpler than orthogonal work like GPTQ or AWQ.\n\nSecond, if the authors would like to mainly highlight the empirical evaluation of this paper. Then, my follow-up suggestions are:\n1. Need more explorations and analyses to make the empirical observations solid and actually reveal useful knowledge. Just to name a few, can we answer the following questions:\n   * What is the LoRA hyperparameter used in the newly added experiment? Did we sweep the hyperparameters of both the prompt tuning and LoRA method? This can make the current results valid.\n   * Is prompt tuning better than LoRA for uncompressed models? If not, why is prompt tuning so much better than LoRA for compressed models? If it is due to the parameter size, can we adjust the hyperparameter or tensor choice of LoRA to reduce the parameter size?\n   * For restoring the performance of compressed models, is prompt tuning universally better for all types of tasks? Or prompt tuning is only better on several types of compressed models?\n   * Did the authors want to claim that in the future, we don't need LoRA when we are compressing models? No matter what task we are targeting, we should use prompt tuning as it achieves much better performance?\n2. It's better for this paper to be re-organized to correctly position the empirical evaluation contribution: First, summarize valuable questions that engineers might care about the performance restoration of compressed models; Then, conduct extensive results to reveal phenomena, and analyze why; Finally, give answers to the important questions an suggestions."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4054/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700640732172,
                "cdate": 1700640732172,
                "tmdate": 1700640732172,
                "mdate": 1700640732172,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "tkBwYOiDKY",
            "forum": "Gdm87rRjep",
            "replyto": "Gdm87rRjep",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4054/Reviewer_QUfQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4054/Reviewer_QUfQ"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents an interesting prompt engineering observation about efficient LLMs. Performance improves by adding a hard-coded prompt telling the LLM to reconsider its solution because it is compressed! The authors build on that observation by performing _transferable_ prompt tuning on a number of compressed (quantized/pruned) LLMs from the OPT/LLAMA family of models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The main benefit claimed by the authors is that the tuned prompts are domain-agnostic. That is, they can be used on different models, datasets, or tasks quite easily because they are related to the fact that the model is compressed, not specifically-tuned for any specific domain."
                },
                "weaknesses": {
                    "value": "The main weaknesses relate to a lack of wider context and evaluation of the presented method. For example: how expensive is prompt tuning? By creating transferable prompts, how much time/effort are we saving? How does accuracy compare to conventional prompt tuning (this is a key missing comparison). How does the presented method peform on other model families? (OPT and Llama are highly related).\n\nWithout comparison to other prompt tuning methods, it is hard to put the current results in the needed context."
                },
                "questions": {
                    "value": "please see weaknesses above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4054/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4054/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4054/Reviewer_QUfQ"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4054/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698690155113,
            "cdate": 1698690155113,
            "tmdate": 1700534082733,
            "mdate": 1700534082733,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "iJb1f28wwh",
                "forum": "Gdm87rRjep",
                "replyto": "tkBwYOiDKY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4054/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4054/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks & Initial Response to Reviewer QUfQ (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the comments. Below we provide a wider context and evaluation for our proposed method.\n\n### **[W1.1 - How expensive is prompt tuning? It can be done in hours]**\n\nThe backward pass of linear layer, represented as $H^{(l+1)}=H^{(l)}W^{(l)}$, contains two parts, namely, calucating the input gradient $\\nabla H^{(l)}=\\nabla H^{(l+1)} W^{(l)\\top}$ and weight gradient  $\\nabla W^{(l)}=H^{(l)\\top} \\nabla H^{(l+1)}$. **Soft prompt tuning simplifies this process by only requiring the calculation of input gradients.** This approach eliminates the need to cache input tensors, as the model weights are already stored in GPU memory. Consequently, this reduces the backward pass time by 50%, since the weight gradient calculation is bypassed. In contrast to methods like LoRA, which requires caching input tensors for computing the LoRA weight gradient, soft prompt tuning allows larger flexible fine-tuning batch sizes, enhancing GPU utilization. In practice, we can recover the performance of a LLaMA2-7B model within 5 hours on 4 RTX 8000 48G GPUs.\n\n### **[W1.2 - How much time/effort is saved by transferring prompts? It saves specific prompt tuning efforts and makes models 8X smaller]**\n\nFirst, regarding time efficiency, transferring prompts across datasets, tasks, and compression levels saves the prompt tuning time for each specific setup. According to our experience with 4 GPUs, that means 5-ish hours per task per compressed model as well as the engineering efforts). \n\nSecond, regarding memory usage, as shown in Table 2, our method can recover the accuracy drop of an 8X compressed LLaMA-7B. In practice, this means we reduce the model memory cost from roughly 13GB (in Float16 format) to 1.7GB without sacrificing performance! This substantial decrease in memory usage opens up new opportunities for deploying LLMs on devices with limited resources or inferencing with enlarged batch sizes.\n\n\n### **[W 1.3 - How does accuracy compare to conventional prompt tuning (this is a key missing comparison): We have already compared them in Appendix A.3. In short, transferred prompts are not as good, but they significantly bridge the gap.]**\n\nIf by \"conventional prompt tuning,\" the reviewer means directly learning the soft prompt on the downstream tasks, **then we already have these results in [Table 4 in Appendix A.3](https://openreview.net/pdf?id=Gdm87rRjep#page=14)**. For your convenience, we post our results and summarize our observations here.\n\n> Perplexity comparison between full model and quantized models with different prompts. Where we report test perplexity on PTB and Wikitext-2 dataset. \"w./o. prompt\" refers to the quantized model without soft prompts. \"w./ direct prompt\" means the soft prompts are directly trained on the target dataset. \"w./ transferred prompt\" means the prompt is trained on C4 dataset and then transferred to the target dataset.\n\n| Model                       | PTB     | Wikitext2 |\n|-----------------------------|---------|-----------|\n| Full Model                  | 11.02   | 6.33      |\n| Full Model w./ direct prompt| 6.86    | 5.57      |\n| 4-bit w./o. prompt          | 11.65   | 6.92      |\n| 4-bit w./ direct prompt     | 7.04    | 5.88      |\n| 4-bit w./ transferred prompt| 9.25    | 6.26      |\n| 3-bit w./o. prompt          | 15.74   | 9.45      |\n| 3-bit w./ direct prompt     | 7.76    | 6.33      |\n| 3-bit w./ transferred prompt| 10.81   | 6.90      |\n| 2-bit w./o. prompt          | 5883.13 | 2692.81   |\n| 2-bit w./ direct prompt     | 14.98   | 16.67     |\n| 2-bit w./ transferred prompt| 29.82   | 20.56     |\n\n\nGiven direct prompt receives a domain-specific loss, our transferred prompt is, as expected, not as competitive as the direct one. However, such transferred prompt may significantly bridge the gap between a compressed and full model \u2014 e.g., our 3-bit & 4-bit quantized LLaMA-7B with transferred prompt can deliver on-par or better PPL than the full model on PTB and Wikitext2. We'd say this is an especially worthy contribution in practice, as one may possibly download the open-sourced transferable prompt to help on a compressed model with little effort."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4054/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700367526448,
                "cdate": 1700367526448,
                "tmdate": 1700367682336,
                "mdate": 1700367682336,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cOblM9k5Cj",
                "forum": "Gdm87rRjep",
                "replyto": "tkBwYOiDKY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4054/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4054/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks & Initial Response to Reviewer QUfQ (2/2)"
                    },
                    "comment": {
                        "value": "### **[W1.3 Extra - Conventional Prompt tuning cannot transfer between domains!]**\n\nHere we also emphasize that the conventional prompt tuning trained with a domain-specific loss can no longer be transferred between different datasets. Below we present the results of transferring the soft prompts learned on Wikitext2 ( Featured articles on Wikipedia) to PTB (Wall Street Journal material) and C4 (collection of common web text corpus). The results, as shown in the table below, highlight a significant disparity in performance when using domain-specific prompts across different domains. The prompt trained on Wikitext-2, when applied to PTB and C4, leads to a drastic increase in perplexity, indicating a severe degradation in model performance. In contrast, if the prompt is learned on general domain (C4), then it can be transferred to different domain (PTB & Wikitext) and tasks (QA, Commonsense NLI, language understanding, etc. Please check [Appendix A.3](https://openreview.net/pdf?id=Gdm87rRjep#page=15)) \n\n> Perplexity comparison on PTB and C4\n\n| Model                       | PTB    | C4    | \n|-----------------------------|---------|---------|\n| Full Model                  | 11.02   |7.59 |\n| 3-bit w./o. prompt          | 15.74   | 10.74|\n| 3-bit w./ prompt learned on Wikitext2          | 294.16   |160.64|\n| 3-bit w./ prompt learned on C4          | 10.81    | 7.48|\n\n\n### **[W1.4 - Other Model Family: Sure! Our method is still strong on LLaMA-2 and BLOOM]**\n\nWe conduct experiments on BLOOM-7B and LLaMA2-13B. Here, we provide the results. We can recover the performance of 3-bit LLaMA2-2-13B and 3-bit BLOOM-7B with even better performance than their fp16 counterparts. The quantization is conducted using GPTQ.\n\n| Dataset | Model                     | Precision | Method | Trainable Params (M) | ppl val  |\n|---------|---------------------------|-------|--------|----------------------|----------|\n| C4      | LLaMA-2-13B | fp16  | NA     | NA                    | 6.96     |\n| C4      | LLaMA-2-13B | 3bit  | NA     | NA                    | 9.24     |\n| C4      | LLaMA-2-13B | 3bit  | Soft Prompt   | 0.5                | **6.75** |\n| C4      | BLOOM-7B                   | fp16  | NA     | NA                    | 15.87    |\n| C4      | BLOOM-7B                   | 3bit  | NA     | NA                    | 18.40    |\n| C4      | BLOOM-7B                   | 3bit  |  Soft Prompt   | 0.4               | **13.54**|\n\nWe also test the transferability of our prompt tuning on the wikitext2 dataset. It suggests that our soft prompt can be transferred to other datasets while still outperforming LoRA for the performance recovery of compressed LLMs.\n\n| Dataset   | Model       | Precision | Method | Transferred Params (M) | ppl       |\n|-----------|-------------|-------|--------|------------------------|---------------|\n| wikitext2 | LLaMA-2-13B | fp16  | NA     | NA                      | 5.58          |\n| wikitext2 | LLaMA-2-13B | 3bit  | NA     | NA                      | 7.88          |\n| wikitext2 | LLaMA-2-13B | 3bit  | Ours   | 0.5                  | **5.89**      |\n| wikitext2 | BLOOM-7B     | fp16  | NA     | NA                      | 13.26         |\n| wikitext2 | BLOOM-7B     | 3bit  | NA     | NA                      | 16.06         |\n| wikitext2 | BLOOM-7B     | 3bit  | Ours   | 0.4                 | **12.42**     |"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4054/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700367857375,
                "cdate": 1700367857375,
                "tmdate": 1700367857375,
                "mdate": 1700367857375,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "k5WWcNTHYp",
                "forum": "Gdm87rRjep",
                "replyto": "cOblM9k5Cj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4054/Reviewer_QUfQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4054/Reviewer_QUfQ"
                ],
                "content": {
                    "title": {
                        "value": "raising my score"
                    },
                    "comment": {
                        "value": "The authors addressed many of my main concerns in their rebuttal. The results comparing to other soft prompts and showing non-transferability of conventional methods, in addition to highlighting the cost of prompt tuning makes for a more compelling argument for the presented method. Can I ask the authors to add this content to the main paper? I think it adds more value to the manuscript instead of leaving it buried in the appendix."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4054/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700533918827,
                "cdate": 1700533918827,
                "tmdate": 1700533918827,
                "mdate": 1700533918827,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HZCCf6dHlp",
            "forum": "Gdm87rRjep",
            "replyto": "Gdm87rRjep",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4054/Reviewer_ARdA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4054/Reviewer_ARdA"
            ],
            "content": {
                "summary": {
                    "value": "Motivated by the ability to improve compressed LLM performance through human-engineered prompts (i.e., informing the LLM that it has been compressed), the authors formulate a prompt learning paradigm with the objective of recovering LLM performance lost due to compression/pruning. Additionally, this prompt learned via their method demonstrates transferability between datasets, compression schemes, and tasks. This research is ultimately motivated by the desire for portable, efficient, and accurate LLMs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "\u2022 Designed and implemented a prompt learning paradigm for improving performance of compressed LLMs through learned prompts.\n\n\u2022 Prompt learning paradigm able to recover LLM performance when compared to original model on low and medium quantization/pruning settings. Performance with learned prompt is still better than without in all quantiation/pruning settings.\n\n\u2022 Prompt learned for higher pruning/quantization rates is transferrable to lower pruning/quantization rates, respectively. Further, there are some instances where prompt is transferrable from pruning to quantization (or vice versa).\n\n\u2022 Prompt learning demonstrated to be compatible with mixed pruned-quantized LLM."
                },
                "weaknesses": {
                    "value": "\u2022 Very minor but presentation of some figures could be improved. Consider including baseline value (i.e., value of green line in Figures 2,3,4)."
                },
                "questions": {
                    "value": "I do not have any questions. The methodology and presentation was clear to me and the results were easy to interpret. I was going to ask about interpretation of the learned prompts but I found details on that in Appendix D."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4054/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4054/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4054/Reviewer_ARdA"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4054/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698801675250,
            "cdate": 1698801675250,
            "tmdate": 1699636369156,
            "mdate": 1699636369156,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fP0bw7DFUR",
                "forum": "Gdm87rRjep",
                "replyto": "HZCCf6dHlp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4054/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4054/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks & Initial Response to Reviewer ARdA"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the strong support and meticulous reading!\n\n\n### **[W1 - Including baseline value in Figures: Sure! Thank you for suggesting it]**\nWe have updated the paper by printing out the value for green lines in [Figures 2,3,4](https://openreview.net/pdf?id=Gdm87rRjep#page=4). Please see the revised submission for a better presentation."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4054/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700367287806,
                "cdate": 1700367287806,
                "tmdate": 1700368612368,
                "mdate": 1700368612368,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]