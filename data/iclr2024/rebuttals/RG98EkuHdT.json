[
    {
        "title": "Transforming Transformers for Resilient Lifelong Learning"
    },
    {
        "review": {
            "id": "BBKAXFxYhD",
            "forum": "RG98EkuHdT",
            "replyto": "RG98EkuHdT",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6198/Reviewer_Z1nR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6198/Reviewer_Z1nR"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a method for achieving resilient lifelong learning in deep neural networks, focusing on Transformers, which have gained prominence in deep learning. The key challenge addressed is catastrophic forgetting, where networks struggle to learn new tasks without losing previously acquired knowledge. \n\nThe approach suggests integrating task-aware, adaptable components (referred to as Artificial Hippocampi or ArtiHippo) into the architecture of Vision Transformers (ViTs). \n\nInspired by the human brain's Hippocampi, known for their role in lifelong learning, the paper explores how these artificial components can be identified, placed, and trained within ViTs to enable adaptability while preserving core functions.\n\nThe proposed method belongs to the parameter-tuning category with more fine-grained control."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper presents an interesting concept \"Artificial Hippocampi (ArtiHippo) in Transformers\" for task-incremental learning. The proposed method introduces lightweight transformer components to the dynamic networks to learn the new tasks.\n\nThe paper is written and organized well.\n\nIt obtains better performance than the prior art with sensible ArtiHippo learned continually."
                },
                "weaknesses": {
                    "value": "The approach proposed in this article needs to rely on a priori, i.e., knowing in advance to which task a certain data belongs. This limits the practical application scenarios of the CL approach.\nIn addition, the method does not scale well as the number of tasks increases. Especially for scenarios where multiple small tasks exist.\n\nLack of experiments on generic experimental datasets, inclusion of ImageNet and CIFAR100.\n\nLack of comparison with enough continual learning methods, please discuss or compare with recent proposed baselines.\n\nMissing related Transformer-based continual learning methods, e.g.,\n\n[1] Continual Learning with Transformers for Image Classification. CVPR 2022 CLVision workshop \n\n[2] Continual Learning with Lifelong Vision Transformer. CVPR 2022\n\n[3] D3Former: Debiased Dual Distilled Transformer for Incremental Learning. CVPR 2023"
                },
                "questions": {
                    "value": "Why Tokenized Data is used as input data in Figure 1, is there any special meaning?\n\nPlease discuss the differences and similarities between this paper with other Transformer-based CL methods."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6198/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698566230598,
            "cdate": 1698566230598,
            "tmdate": 1699636675304,
            "mdate": 1699636675304,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gioAWg07ES",
                "forum": "RG98EkuHdT",
                "replyto": "BBKAXFxYhD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6198/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6198/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "Thank you for your time and efforts reviewing our paper. We address your concerns as follows.\n\n### Limitations of the method: task incremental setting\n\nWe acknowledge that the task incremental setting is a limitation of our method. We note that the class incremental setting has its own limitations, e.g., it often requires the number of classes between different tasks is the same, which is not applicable in the VDD benchmark we tested. In the meanwhile, the task incremental setting remains a commonly used setting in studying lifelong learning, where the focus is how to achieve maximal forward transfer across tasks that may have different output space by reusing the relevant parameters learned for all the previous tasks and adding minimal new parameters, which is also an equally important problem.\n\n### Limitations: scaling to larger number of tasks\n\nWe agree and have acknowledged both of these limitations in the submission. However, we would like to point out that our proposed exploration-exploitation sampling scheme can perform well with just 50 epochs of supernet training, whereas pure exploration cannot match this accuracy even with 300 epochs. This shows that our method can scale better than pure exploration. However, very few benchmarks containing a larger number of tasks exist that can tackle our goal of testing the behavior of prompt-based methods and our proposed parameter-based method on diverse tasks on our computational budget (with SKILL benchmark [4] being a very recent development). We will address this issue in future work.\n\n### Lack of experiments on generic experimental datasets, inclusion of ImageNet and CIFAR100.\n\nOne of the goals of work is to study the behavior of prompt-based methods and parameter based (methods which add/train the network parameter) methods on diverse tasks. This diversity can come from a change in the input distribution and/or output distribution. Hence, VDD is an appropriate benchmark for this study, since it offers a wide diversity in tasks: Natural images (flowers, aircraft, CIFAR100), street view images (SVHN, GTSR), handwritten digits (omniglot), etc. Since CIFAR100 is already present in the VDD benchmark, it is more generic and challenging than CIFAR100. Furthermore, Learn to Prompt and S-Prompt use a model pretrained on ImageNet, and hence a evaluating on ImageNet will not be an appropriate experiment. \n\n### Lack of comparison with enough continual learning methods, please discuss or compare with recent proposed baselines.\n\nWe have compared with recently proposed baselines that offer a fair comparison with our method, i.e., that start from a model trained in ImageNet: Learn to Prompt (CVPR22), S-Prompts (NeurIPS22) and Lightweight Learner (TMLR23). Please let us know if there are any specific references that use similar settings.\n\n### Why Tokenized Data is used as input data in Figure 1, is there any special meaning?\n\nFigure 1 shows the typical lifelong learning methods that use the Transformer architecture. Since Transformers handle the input data by the means of tokenization (e.g. text tokens of image tokens), we represent the data in the form of tokens, and make a distinction between data tokens and prompt tokens in Figure 1a.\n\n### Missing related work, similarities and differences with other Transformer based continual learning methods\n\nThank you for pointing put these methods to us. We will cite these and discuss the differences in the Appendix of the revised submission due to space limitations. Please note that we have cited [1] (last reference on page 11 in the initial submission)\n\n> [1] Continual Learning with Transformers for Image Classification. CVPR 2022 CLVision workshop\n\n> [2] Continual Learning with Lifelong Vision Transformer. CVPR 2022\n\n> [3] D3Former: Debiased Dual Distilled Transformer for Incremental Learning. CVPR 2023\n\n> [4] Lightweight Learner for Shared Knowledge Lifelong Learning. TMLR 2023"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6198/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700675525468,
                "cdate": 1700675525468,
                "tmdate": 1700675525468,
                "mdate": 1700675525468,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "AC6YrAKHgr",
            "forum": "RG98EkuHdT",
            "replyto": "RG98EkuHdT",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6198/Reviewer_6RhY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6198/Reviewer_6RhY"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a method of training vision transformers (ViTs) for lifelong learning under the task-incremental setting. It identifies the final projection layer of multi-head self-attention of a ViT as the Artificial Hippocampi (ArtiHippo) of ViTs, and learns to dynamically grow the ArtiHippo by four operations \"Skip\", \"Resue\", \"Adapt\" and \"New\". The maintenance of ArtHippo is realized by hierarchical exploration-exploitation sampling where the exploitation utilizes task similarities measured by the normalized cosine similarity between the mean class tokens of a new task and those of old tasks. Experiments are conducted on VDD and 5-Datasets benchmarks, showing better performance than previous art."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is clearly written, well-presented with rich visualizations and easy to follow. \n2. The method is effective when utilizing ViTs for lifelong learning.  \n3. The analysis of results is thorough and insightful."
                },
                "weaknesses": {
                    "value": "The design choices are mostly experience-guided: e.g. identifying the projection layer as the ArtiHippo, using the mean class tokens to measure task similarity, and four operations to grow the ArtiHippo. More discussions on principles and analysis would make the paper more solid."
                },
                "questions": {
                    "value": "1. Could some evaluation on the soundness of measuring task-similarity with the normalized cosine similarity between the mean class tokens be provided? \n2. Would the same conclusion hold for stronger/larger ViT models?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6198/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698587909492,
            "cdate": 1698587909492,
            "tmdate": 1699636675169,
            "mdate": 1699636675169,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FdhVPp58Cf",
                "forum": "RG98EkuHdT",
                "replyto": "AC6YrAKHgr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6198/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6198/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "Thank you for your time and efforts reviewing our paper. We address your concerns as follows, which will be carefully updated in the revision.\n\n### The design choices are mostly experience-guided: e.g. identifying the projection layer as the ArtiHippo, using the mean class tokens to measure task similarity, and four operations to grow the ArtiHippo. More discussions on principles and analysis would make the paper more solid.\n\nPrinciples and Analysis: We agree that principles and analysis would be beneficial, we would like to note that a theoretical analysis of Transformer models is largely an open area of research. [1] provide some theoretical analysis of the role played by each component of transformers, but are restricted to toy problems. Moreover, such a theoretical analysis of Transformers applied to images is lacking. We agree that this is certainly an important area that needs to be investigated, but leave it for future work for a comprehensive treatment.\n\n###  Could some evaluation on the soundness of measuring task-similarity with the normalized cosine similarity between the mean class tokens be provided?\n\n Our choice of the use of class token is similar to Learn to Prompt, which uses the class token from the last layer to measure the similarity between the keys of the prompt pool and the current image. Following Learn to Prompt, we too use the cosine similarity to measure task similarity.\n\n### Would the same conclusion hold for stronger/larger ViT models?\n\n Due to resource constraints, we have left this for future work. However, some work from Parameter Efficient Fine-Tuning [2] suggests that our conclusions might hold for larger models as well. In particular, Figure 2 in [2] suggests that methods that tune the parameters (Compacter, Compacter++ [3], IA3 [2], LoRA [4], etc.) perform better than prompt based approaches. We could expect the same trend to follow here as well.\n\n> [1] \tAlberto Bietti, Vivien Cabannes, Diane Bouchacourt, Herv\u00e9 J\u00e9gou, L\u00e9on Bottou: Birth of a Transformer: A Memory Viewpoint. NeurIPS 2023.\n\n> [2] Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, Colin Raffel: Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning. NeurIPS 2022\n\n> [3] Rabeeh Karimi Mahabadi, James Henderson, Sebastian Ruder: Compacter: Efficient Low-Rank Hypercomplex Adapter Layers. NeurIPS 2021: 1022-1035\n\n> [4] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen: LoRA: Low-Rank Adaptation of Large Language Models. ICLR 2022"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6198/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700671146990,
                "cdate": 1700671146990,
                "tmdate": 1700671146990,
                "mdate": 1700671146990,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NXVoqyTbcJ",
                "forum": "RG98EkuHdT",
                "replyto": "FdhVPp58Cf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6198/Reviewer_6RhY"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6198/Reviewer_6RhY"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "I've read the reviewers' comments and the authors' responses. I agree that the novelty of this work is under question, but I also reckon with the authors that applying existing techniques to new problems is also important: the proposed approach is thoroughly evaluated on challenging benchmarks showing consistent improvements. Therefore I'll keep my original score."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6198/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700705369669,
                "cdate": 1700705369669,
                "tmdate": 1700705369669,
                "mdate": 1700705369669,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9qIZfEKMBl",
            "forum": "RG98EkuHdT",
            "replyto": "RG98EkuHdT",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6198/Reviewer_UTmX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6198/Reviewer_UTmX"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors tackle the problem of lifelong learning in deep neural networks, specifically focusing on Vision Transformers (ViTs). They introduce a concept inspired by the human brain's hippocampus, called Artificial Hippocampi (ArtiHippo), to help ViTs learn continuously without forgetting previous knowledge\u2014a common challenge known as catastrophic forgetting. The study explores where to place ArtiHippo within ViTs, what kind of structure it should have, and how it can grow and adapt over time while retaining past knowledge. By testing their approach on challenging benchmarks, the authors demonstrate that their method not only performs better than previous ones but also marks the first successful application of lifelong learning in Vision Transformers, showing great promise for future AI systems."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. Balanced Exploration and Exploitation: The proposed method offers a new searching strategy that balances exploration (learning new information) and exploitation (using existing knowledge), crucial for the development of robust lifelong learning systems.\n\n2. Empirical Results: The approach is thoroughly evaluated on challenging benchmarks, where it consistently outperforms existing methods. This demonstrates the practical effectiveness of the proposed solution and its potential for real-world applications."
                },
                "weaknesses": {
                    "value": "1. Refinement Rather Than Revolution: The integration of ArtiHippo into Vision Transformers, though presented as a novel idea, is actually a clever twist on the established \"learning to grow\" concept. It's a smart update, but it falls short of being a game-changer. It feels like we're seeing a refinement of existing ideas rather than a bold reimagining of lifelong learning.\n\n2. A Safe Bet Over a Leap of Faith: Employing Reuse, New, Adapt, and Skip operations within Transformers comes across as a safe, almost expected move. It's as if the paper takes a well-trodden path, applying tried-and-tested strategies to new territory, rather than venturing into unexplored innovative realms.\n\n3. Narrow Lens on Competing Approaches: The paper misses a beat by not sizing up ArtiHippo against the full spectrum of lifelong learning strategies, particularly gradient-based and regularization-based methods. This omission leaves us guessing about how ArtiHippo truly stacks up against the competition and muddies the waters of its potential as a standout solution in the field."
                },
                "questions": {
                    "value": "In what ways does the ArtiHippo framework conceptually and functionally diverge from the 'Learning to Grow' methodology, considering the apparent similarities in their approach to lifelong learning?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6198/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699259742220,
            "cdate": 1699259742220,
            "tmdate": 1699636675061,
            "mdate": 1699636675061,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XuwzQmQ1ZR",
                "forum": "RG98EkuHdT",
                "replyto": "9qIZfEKMBl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6198/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6198/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "Thank you for your time and efforts reviewing our paper. We address your concerns as follows, which we will carefully update in the revision.\n\n### Summary: ... By testing their approach on challenging benchmarks, the authors demonstrate that their method not only performs better than previous ones but also marks the first successful application of lifelong learning in Vision Transformers, showing great promise for future AI systems.\n\nThank you for your positive comments. We would like to point out that our proposed method is actually not the first work to apply Vision Transformers for lifelong learning. We have provided a review of prior work that uses ViTs in a lifelong learning setting in Section 2. However, we are indeed the first to evaluate ViTs in a lifelong learning setting which contains very diverse tasks, i.e., the VDD benchmark as mentioned in the Abstract.\n\n### Refinement Rather Than Revolution\n\nWhile our method is built on Learn to Grow and SPOS, we make the following novel contributions, and also show the limitations of the two methods:\n\n1. We show that the choice of where to apply NAS matters for achieving good forward transfer. We propose to use the final Linear projection layer in the MHSA block, which is a very lightweight component. Through ablation studies, we validate that this choice indeed results in the best average accuracy. Please see Table 6 in the Appendix for a comparison with other components (Value, Query, Key, and FFN). While it has been shown that finetuning the entire MHSA block achieves good downstream performance [1], we show that using only the projection layer is an equally viable (and more efficient) option.\n2. We show that the original Learn to Grow which uses DARTS on ConvNets cannot perform well when applied to ViTs, even when an advanced version of DARTS ($\\beta$-DARTS) is used (Rows 4 and 5 in Table 2).\n3. We show that using the original SPOS formulation, which samples all the operators uniformly, is not sufficient to achieve better performance than DARTS (Row 6 in Table 2).\n4. We propose a novel algorithm to convert task similarities (as measured by the normalized cosine similarity between the mean class tokens of each task) into a prior distribution over the NAS operations (reuse, adapt, new, skip). Intuitively, this translates to assigning higher probability to the reuse operation if the mean class token calculated using the data of task is similar to the mean class token calculated using the data of task .\n5. We propose an exploration-exploitation driven sampling strategy, which is designed to enhance the prior. Here, exploration refers to sampling from a uniform distribution over the operations, and exploitation refers to sampling from the prior distribution calculated in Eqn.3. We show that this strategy significantly outperforms the origin Learn to Grow, as well as popular prompt based approaches.\n6. We further show that our strategy is complementary to the prompt based approaches, and combining them leads to even higher performance.\n\n> [1] Hugo Touvron, Matthieu Cord, Alaaeldin El-Nouby, Jakob Verbeek, Herv\u00e9 J\u00e9gou: Three Things Everyone Should Know About Vision Transformers. ECCV (24) 2022: 497-515\n\n### A Safe Bet Over a Leap of Faith\n\nWhile the use of 4 growing operations Reuse, New, Adapt, and Skip has been explored before, we are the first to apply this paradigm to Vision Transformers. However, ViTs are computationally expensive class of models, and applying the operations uniformly to all the layers will be extremely costly. We propose, and empirically validate the choice of the linear projection layer in the MHSA block to apply the 4 operations effectively.\n\nVenturing into unexplored innovative realms is certainly important, but applying existing techniques to new problems is also equally important. As we show in Table 2 in the main text, applying existing techniques to new problems is not straightforward, as evidenced by the low average accuracy of Learn to Grow in its original form (rows 4 and 5), and SOPS in its original form (row 6). Hence, our contribution also lies in addressing the weaknesses of the existing methods when applied to ViTs, and proposing novel techniques to overcome them (please see our response to (2)).\n\n### Narrow Lens on Competing Approaches\n\nCould you please clarify the meaning of/provide references to gradient-based approaches in lifelong learning?\nWhile not a completely fair comparison, we compare with replay-based approaches and regularization-based approaches (Table 7 in the Appendix) to inform readers of the tradeoff of using these methods. Note that replay-based and regularization-based methods suffer from catastrophic forgetting, hence requiring the storage of all the previous models."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6198/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700670630631,
                "cdate": 1700670630631,
                "tmdate": 1700670630631,
                "mdate": 1700670630631,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gYyWg42TLs",
                "forum": "RG98EkuHdT",
                "replyto": "XuwzQmQ1ZR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6198/Reviewer_UTmX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6198/Reviewer_UTmX"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "I am not full convinced about the claimed novelty, since, all the searching strategy and designed operation for grow, are already there. Simply adding more components to the system doesn't fundamentally alter its essence; there's no significant conceptual change. The system doesn't offer any capabilities beyond what 'Learn to Grow' already does.\n\nThe use of 'Learn to Grow' in transformers, including example [A], is not entirely novel. Additionally, if expanding the 'Learn to Grow' concept to include the addition of new learnable prompts, there is a wealth of related work in this area.\n\nHowever, I appreciate the thorough evaluations provided.\n\nTherefore, I will keep my original score of 5. Hope the author can understand my concern.\n\n[A] Gao, Qiankun, et al. \"A Unified Continual Learning Framework with General Parameter-Efficient Tuning.\" ICCV 2023"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6198/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700674711202,
                "cdate": 1700674711202,
                "tmdate": 1700674711202,
                "mdate": 1700674711202,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "XU4mr8Hb62",
            "forum": "RG98EkuHdT",
            "replyto": "RG98EkuHdT",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6198/Reviewer_QBWR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6198/Reviewer_QBWR"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the challenge of catastrophic forgetting in deep neural networks, particularly focusing on Transformer models, which are increasingly popular but complex to train for lifelong learning tasks. It introduces a concept called Artificial Hippocampi (ArtiHippo) within Vision Transformers (ViTs) to enhance their resiliency and adaptability for continual learning, drawing inspiration from the human brain and demonstrating improved performance on rigorous benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ The paper is well written, and the idea is also elaborated clearly. There is no trouble in reading and reproducing the algorithm.\n+ In addition, the implementation and code are provided to demonstrate the proposed algorithm.\n+ Experiments provide adequate support to the assumptions and claims and prove that the new method is applicable to vision datasets."
                },
                "weaknesses": {
                    "value": "- The major concern is the lack of novelty for the paper to be published in ICLR. Although the paper is well thought out and demonstrated with extensive experiments, the novelty is rather incremental. See the question section.\n- Another issue is the research problem \u201ctask incremental learning,\u201d which seems not very practical in many vision applications. As the authors discussed, \u201cclass incremental\u201d is more demanded since the task information is not always (unlikely) available.\n- Some comparisons in experiments seem unfair and may not be able to reflect the true performance of each framework."
                },
                "questions": {
                    "value": "1.\tThe key methodology of this paper is mainly built upon several existing works. The four operations are adapted from \u201clearning-to-grow,\u201d and the construction of supernet is based on SPOS. Although finding the right place to place ArtiHippo is tricky, it has been common sense that classification information is mostly likely located on the upper level of the network.\n2.\tIn addition, the method leverages the recent \u201cprompt\u201d based method such as S-Prompts to further improve the performance. This makes the neat \u201cArtiHippo\u201d based algorithm a bit over-complicated and more engineering-driven.\n3.\tIt seems all methods always use pre-trained strong ViT backbone for incremental tasks. This does cover all the cases in lifelong learning, e.g., starting from a moderated size model, or from zero knowledge. The current methodology is still like finetuning a strong baseline on several small datasets. The problem discussed and experiments are not typical lifelong learning scenarios.\n4.\tThe major concern lies in the knowledge of \u201ctask.\u201d It does not always make sense to know where the data are from or their sources."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6198/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699298439779,
            "cdate": 1699298439779,
            "tmdate": 1699636674959,
            "mdate": 1699636674959,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Xa4gHXciyr",
                "forum": "RG98EkuHdT",
                "replyto": "XU4mr8Hb62",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6198/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6198/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your time and efforts reviewing our paper. We address your concerns as follows, which will be carefully updated in the revision.\n\n### Lack of novelty and similarity with Learn to Grow and SPOS.\n\nWhile our method is built on Learn to Grow and SPOS, we make the following novel contributions, and also show the limitations of the two methods:\n1. We show that the choice of where to apply NAS matters for achieving good forward transfer. We propose to use the final Linear projection layer in the MHSA block, which is a very lightweight component. Through ablation studies, we validate that this choice indeed results in the best average accuracy. Please see **Table 6 in the Appendix** for a comparison with other components (Value, Query, Key, and FFN). While it has been shown that finetuning the entire MHSA block achieves good downstream performance [1], we show that using only the projection layer is an equally viable (and more efficient) option.\n2. We show that the original Learn to Grow which uses DARTS on ConvNets cannot perform well when applied to ViTs, even when an advanced version of DARTS ($\\beta$-DARTS) is used (**Rows 4 and 5 in Table 2**).\n3. We show that using the original SPOS formulation, which samples all the operators uniformly, is not sufficient to achieve better performance than DARTS (**Row 6 in Table 2**).\n4. We propose a novel algorithm to convert task similarities (as measured by the normalized cosine similarity between the mean class tokens of each task) into a prior distribution over the NAS operations (reuse, adapt, new, skip). Intuitively, this translates to assigning higher probability to the reuse operation if the mean class token calculated using the data of task $t-1$ is similar to the mean class token calculated using the data of task $t$.\n5. We propose an exploration-exploitation driven sampling strategy, which is designed to enhance the prior. Here, **exploration** refers to sampling from a uniform distribution over the operations, and **exploitation** refers to sampling from the prior distribution calculated in Eqn.3. We show that this strategy significantly outperforms the origin Learn to Grow, as well as popular prompt based approaches.\n6. We further show that our strategy is complementary to the prompt based approaches, and combining them leads to even higher performance.\n\n> [1] Hugo Touvron, Matthieu Cord, Alaaeldin El-Nouby, Jakob Verbeek, Herv\u00e9 J\u00e9gou: Three Things Everyone Should Know About Vision Transformers. ECCV (24) 2022: 497-515 \n\n\n### Although finding the right place to place ArtiHippo is tricky, it has been common sense that classification information is mostly likely located on the upper level of the network.\n\nWe respectfully disagree with this statement. [2] show that which layer to finetune depends on the nature of the data, and surgically choosing which layer to finetune outperforms tuning the last layer. Consequently, which layer to reuse, adapt, renew or skip in the learn to grow will depend on the nature of the current task, and our method provides a framework to derive a prior distribution on the four operators tased on task similarities, rather than manually inspecting which layer to tune for each incoming task. \n\n> [2] Yoonho Lee, Annie S. Chen, Fahim Tajwar, Ananya Kumar, Huaxiu Yao, Percy Liang, Chelsea Finn: Surgical Fine-Tuning Improves Adaptation to Distribution Shifts. ICLR 2023\n\n### Another issue is the research problem \u201ctask incremental learning,\u201d which seems not very practical in many vision applications\n\nWe acknowledge that the task incremental setting is a limitation of our method.  We note that the class incremental setting has its own limitations, e.g., it often requires the number of classes between different tasks is the same, which is not applicable in the VDD benchmark we tested. In the meanwhile,  the task incremental  setting remains a commonly used setting in studying lifelong learning, where  the focus is how to achieve maximal forward transfer across tasks that may have different output space  by reusing the relevant parameters learned for all the previous tasks and adding minimal new parameters, which is also an equally important problem. \n\n### Some comparisons in experiments seem unfair and may not be able to reflect the true performance of each framework.\n\nPlease note that we modify all the methods to work in a task incremental setting for a fair comparison with our method. We request you to elaborate which comparisons seem unfair, so that we can address your concerns in a comprehensive manner."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6198/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700664370946,
                "cdate": 1700664370946,
                "tmdate": 1700664370946,
                "mdate": 1700664370946,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "F8n8URGg3f",
                "forum": "RG98EkuHdT",
                "replyto": "XU4mr8Hb62",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6198/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6198/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors (2/2)"
                    },
                    "comment": {
                        "value": "### In addition, the method leverages the recent \u201cprompt\u201d based method such as S-Prompts to further improve the performance. This makes the neat \u201cArtiHippo\u201d based algorithm a bit over-complicated and more engineering-driven.\n\nWe would like to clarify that integration with S-Prompts does not form the core component of our proposed method, but rather an initial exploration of integration of the two approaches due to their complementary behavior. As shown in Table 2, rows 7 and 8, our proposed method without S-Prompt performs better than Learn to Prompt$^\\dagger$, S-Prompts$^\\dagger$ and Learn to Grow. The integration with S-Prompt, although very simple, **further** boosts the average accuracy. We leave a more comprehensive (and possibly simpler) integration for future work.\n\n### It seems all methods always use pre-trained strong ViT backbone for incremental tasks. ... The problem discussed and experiments are not typical lifelong learning scenarios.\n\nWe agree that this is not a typical lifelong learning scenario, but a different and equally important scenario of continually improving a pretrained model. This methodology has been used in all the works we compare with, and is the fundamental idea behind prompt based methods, i.e., Learning to Prompt (CVPR22), S-Prompts (NeurIPS22), as well as Efficient Feature Transformation (CVPR21), and Lightweight Learner (TMLR23). Hence, the comparisons with these methods are fair. We show that adding plasticity through model parameters achieves better forward transfer than all these methods (as evidenced by the average accuracy Table 2 and Table 4). Please see the advantages of our method in Sections 4.1 and 4.4.\n\n### Justifying the selection of the projection layer in the MHSA\n\nTo further show and verify the effectiveness of the selection of the projection layer in MHSA, we conduct another experiments of fine-tuning the ImageNet-1k trained ViT-B backbone on the fine-grained visual classification (FGVC) benchmark consisting of five categories using the popoular LoRA method [3].  In the literature, LoRA is often applied the Query/Key/Value of Transformers. Our preliminary results show that applying LoRA to the projection layer in the MHSA is much more effective as shown in the table below, which shows that the selection of the projection layer is non-trivial and has a good potential. \n\n| Method | CUBS [4] | Birds [5] | Flower [6] | Dog [7] | Car [8] | Avg| \n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | \n| LoRA (QKV) | 82.5 | 71.2 | 81.2 | 97.5 | 76.6 | 81.8 |\n| LoRA (Proj) | 81.8 | 76.3 | 92.5 | 97.8 | 81.4 | 86.0 |\n\n\n> [3] Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. ICLR, 2022\n\n> [4] Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The Caltech-UCSD Birds-200-2011 Dataset. 2011\n\n> [5] Grant Van Horn, Steve Branson, Ryan Farrell, Scott Haber, Jessie Barry, Panos Ipeirotis, Pietro Perona, and Serge J. Belongie. Building a bird recognition app and large scale dataset with citizen scientists: The fine print in fine-grained dataset collection. CVPR 2015\n \n> [6] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In Sixth Indian Conference on Computer Vision, Graphics & Image Processing, 2008\n\n> [7] Timnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, and Li Fei-Fei. Fine-grained car detection for visual census estimation, AAAI 2017\n\n> [8] Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng Yao, and Li Fei-Fei. Novel dataset for fine-grained image categorization. CVPRW 2011"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6198/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700664406827,
                "cdate": 1700664406827,
                "tmdate": 1700674495711,
                "mdate": 1700674495711,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]