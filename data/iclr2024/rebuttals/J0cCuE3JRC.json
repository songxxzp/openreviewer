[
    {
        "title": "Bag of Features: New Baselines for GNNs for Link Prediction"
    },
    {
        "review": {
            "id": "wEbqhxPpg1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2621/Reviewer_in5m"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2621/Reviewer_in5m"
            ],
            "forum": "J0cCuE3JRC",
            "replyto": "J0cCuE3JRC",
            "content": {
                "summary": {
                    "value": "This paper investigates the performance of Graph Neural Networks (GNNs) for link prediction tasks in comparison to traditional feature extraction methods. The authors propose a simple machine learning model called \"Bag of Features,\" which combines structural features based on node proximity in the graph structure and domain features based on feature space similarity. Their findings show that this model delivers highly competitive results when compared to state-of-the-art GNN models across various benchmark datasets. This supports recent theoretical observations that current GNNs may not be fully exploiting their potential, suggesting a need for innovative approaches to unlock the full capabilities of GNNs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. The proposed Bag of Features (BFLP) model is elegant, simple, and effective, combining both structural features and domain features to create a powerful machine learning model for link prediction tasks.\n\n2. The model demonstrates consistently competitive results when compared to state-of-the-art GNN models, providing empirical evidence that there is room for improvement in current GNN approaches.\n\n3. The paper highlights the potential for conventional feature extraction methods to be integrated into future ML models to enhance their performance.\n\n4. The work is likely to inspire future research in the link prediction domain, encouraging researchers to explore alternative methods and improve upon existing GNN techniques.\n\nOverall, I enjoyed reading the paper and it is a significant contribution to the application of link prediction."
                },
                "weaknesses": {
                    "value": "Several improvements can be made to enhance the quality of the work. Addressing these comments may lead to a higher score:\n1. Rectify the discrepancies between baseline performances in Table 5 and those on the OGB website (refer to Comment 1 for details).\n\n2. Broaden the evaluation datasets to encompass a variety of network structures and domains (see Comments 2 and 3 for clarification).\n\n3. Perform ablation studies to gain a deeper understanding of the contributions made by the Bag of Features model components (refer to Comment 4 for more information)."
                },
                "questions": {
                    "value": "1. A discrepancy exists between the performance reported in Table 5 and the OGB website. For instance, GraphSAGE shows a 48.10 on the OGB website, but the paper reports 56.88. The performance in Table 5 closely aligns with the validation performance on the OGB website. The authors should verify if Table 5 reflects the validation dataset performance and determine the cause of this inconsistency.\n\n2. Tables 3 and 4 follow the methods of (Zhao et al., 2022) and (Guo et al., 2022), respectively. To enhance comprehensiveness, the authors should consider adding Facebook and OGB-DDI to Table 3, and Cora, Citeseer, and DBLP to Table 4.\n\n3. The paper focuses on homophilous graph datasets, but the proposed method could also be applied to heterophyllous graph datasets. Including these datasets would increase the paper's impact.\n\n4. The ablation studies only examine the removal of all structural or domain features. A more detailed table illustrating the impact of each individual feature on the datasets would make the results more appealing."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2621/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2621/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2621/Reviewer_in5m"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2621/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697441539201,
            "cdate": 1697441539201,
            "tmdate": 1700664831554,
            "mdate": 1700664831554,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mDImIUg9rQ",
                "forum": "J0cCuE3JRC",
                "replyto": "wEbqhxPpg1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2621/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2621/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer in5m"
                    },
                    "comment": {
                        "value": "We express our gratitude to the reviewer for the insightful and constructive feedback. In light of your valuable comments, as well as those from other reviewers, we are revising our paper. We are confident that these revisions have substantially enhanced the quality of our work. We hope that our responses will be sufficiently informative for you to reconsider assessing a higher rating for the revised paper.  \n\n$\\textbf{Q1. Table Discrepancy.}$\n\n>Thanks for sharing this point. For OGB-Collab, we used the results given in [1],  a review paper that compares the performances of recent Graph Neural Networks (GNNs) in the link prediction task. All the reported performances are extracted from this source. On the OGB-Leaderboard, we observed that GraphSage's performance was suboptimal, so we opted for another configuration with improved hyperparameters.\n\n>In response to Reviewer Dape's request, we included additional GNN baselines in Table 5. These supplementary results are sourced from [1], specifically Table 2 in the reference.\n\n>[1] Li, J., Shomer, H., Mao, H., Zeng, S., Ma, Y., Shah, N., Tang, J. and Yin, D., 2023. Evaluating Graph Neural Networks for Link Prediction: Current Pitfalls and New Benchmarking. arXiv preprint arXiv:2306.10453.\n\n$\\textbf{Q2. Extra Datasets:}$ Tables 3 and 4 follow the methods of (Zhao et al., 2022) and (Guo et al., 2022), respectively. To enhance comprehensiveness, the authors should consider adding Facebook and OGB-DDI to Table 3, and Cora, Citeseer, and DBLP to Table 4.\n\n>Thank you very much for this suggestion. These two papers use different splits (70/10/20 and 85/5/10) so we did not add our CORA/CITESEER results from Table 3 to Table 4 not to cause any confusion. We add these results to the appendix with different splits (Table 9). We will try to complete the experiments for the suggested datasets (FACEBOOK, DBLP, and OGB dataset) before the rebuttal deadline, and let you know the results.\n\n$\\textbf{Q3. Heterophilic Datasets.}$\n\n>Thank you very much for this great suggestion. We will add experiments for some heterophilic datasets before the rebuttal deadline, and let you know the results.\n\n$\\textbf{Q4. Ablation Study for Individual Features.}$ \n\n>Thank you very much for this suggestion. We added the ablation study table for feature importance in the appendix (Table 7)\n\nPlease let us know if you have further questions. Thank you very much again for your valuable time and feedback."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2621/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699759493014,
                "cdate": 1699759493014,
                "tmdate": 1699759493014,
                "mdate": 1699759493014,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yMIAM0ruM9",
                "forum": "J0cCuE3JRC",
                "replyto": "iDgt0eFZgQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2621/Reviewer_in5m"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2621/Reviewer_in5m"
                ],
                "content": {
                    "comment": {
                        "value": "Having had my concerns sufficiently addressed, I have decided to increase my rating. I regard this as a commendable paper within the application of link prediction (if the experimental setting in Table 5 is correct). In my view, the paper's significance doesn't lie in the proposed method, but rather in the call to rethink the feature learning of Graph Neural Networks (GNNs) in link prediction. This paper serves as an excellent springboard, and it is highly likely to inspire future advancements in the design of link prediction GNNs."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2621/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700665354598,
                "cdate": 1700665354598,
                "tmdate": 1700665354598,
                "mdate": 1700665354598,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Vc5ZnzzZ5R",
            "forum": "J0cCuE3JRC",
            "replyto": "J0cCuE3JRC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2621/Reviewer_Wcv5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2621/Reviewer_Wcv5"
            ],
            "content": {
                "summary": {
                    "value": "This work unifies conventional feature extraction methods for link prediction and uses standard machine learning models to learn from them. The result model delivers highly competitive results on various datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Clear method.\n\n2. Solid experiments."
                },
                "weaknesses": {
                    "value": "1. Novelty is limited. Most methods are ordinary feature extraction tricks.\n\n2. Experiments are not conducted on OGB datasets other than collab."
                },
                "questions": {
                    "value": "1. Please add experiments on other OGB datasets.\n\n2. This work used XGBoost as the ML model. Did you conduct ablation study for it? For example, use Logistic regression and SVM instead?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2621/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698407417155,
            "cdate": 1698407417155,
            "tmdate": 1699636201257,
            "mdate": 1699636201257,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4pd35ZnIsn",
                "forum": "J0cCuE3JRC",
                "replyto": "Vc5ZnzzZ5R",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2621/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2621/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer Wcv5"
                    },
                    "comment": {
                        "value": "We express our gratitude to the reviewer for the insightful and constructive feedback. In light of your valuable comments, as well as those from other reviewers, we are revising our paper. We are confident that these revisions have substantially enhanced the quality of our work. We hope that our responses will be sufficiently informative for you to reconsider assessing a higher rating for the revised paper.  \n\n$\\textbf{W1. Novelty:}$ Novelty is limited. Most methods are ordinary feature extraction tricks.\n\n>Thank you very much for sharing this concern. We wish to emphasize that the novelty of our work lies in substantiating that traditional feature engineering methods can stand shoulder-to-shoulder with cutting-edge GNN models. We argue this based on consistently outperforming powerful GNN methods on various datasets, which are published in top conferences. This supports our claim that our Bag of Features model is a practical and effective alternative to these computationally intensive models.\n\n$\\textbf{W2/Q1. Extra OGB experiments:}$ Experiments are not conducted on OGB datasets other than collab.\n\n>We are working on additional OGB experiments. We hope to provide the results before the rebuttal deadline.\n\n$\\textbf{Q2. XGBoost vs. other ML classifiers:}$ This work used XGBoost as the ML model. Did you conduct ablation study for it? For example, use Logistic regression and SVM instead ?\n\n>Thank you very much for this suggestion. We added the ablation study table reporting the performance of several ML classifiers (including Naive Bayes and Logistic Regression)  in the appendix (Table 11). We will add other ML classifiers soon.\n\nPlease let us know if you have further questions. Thank you very much again for your valuable time and feedback."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2621/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699758717786,
                "cdate": 1699758717786,
                "tmdate": 1699759557799,
                "mdate": 1699759557799,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Bi0sv1eiyd",
                "forum": "J0cCuE3JRC",
                "replyto": "wTzCABOjRD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2621/Reviewer_Wcv5"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2621/Reviewer_Wcv5"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the detailed response. However, results on other OGB dataset are still not available. Therefore, I would keep my score."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2621/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700680085862,
                "cdate": 1700680085862,
                "tmdate": 1700680085862,
                "mdate": 1700680085862,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Hv5E6PWirz",
            "forum": "J0cCuE3JRC",
            "replyto": "J0cCuE3JRC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2621/Reviewer_dape"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2621/Reviewer_dape"
            ],
            "content": {
                "summary": {
                    "value": "This paper leverages different graph structural features with node attributes for link prediction tasks. Then, an XGBoost method is utilized based on these features for link prediction."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. The authors explore different graph structural features for link prediction.\n2. The paper is well-written and easy to follow."
                },
                "weaknesses": {
                    "value": "1. The paper's novelty appears constrained. The significance of graph structural features in the realm of link prediction has been previously underscored by several studies, such as [1][2][3].\n2. The evaluation omits several critical baselines that similarly exploit graph structural features. Notably absent are BUDDY, Neo-GNN, NCN, NCNC, and NBFNet.\n3. The experimental setup seems to miss out on key datasets such as ogbl-ddi, ogbl-ppa, and ogbl-citation2. \n4. There are no ablation studies about the importance of each feature.\n5. While the method demonstrates commendable efficacy on the OGBL-Collab dataset, it's concerning that the associated code is inaccessible, given the empty repository link.\n6. Using the original feature to measure the node similarity might be unreasonable.\n\n[1] Menon, Aditya Krishna, and Charles Elkan. \"Link prediction via matrix factorization.\"\n[2] Muhan Zhang and Yixin Chen. Link prediction based on graph neural networks. \n[3] Xiyuan Wang, Haotong Yang, and Muhan Zhang. Neural common neighbor with completion\nfor link prediction."
                },
                "questions": {
                    "value": "Please refer to the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2621/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2621/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2621/Reviewer_dape"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2621/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698809176447,
            "cdate": 1698809176447,
            "tmdate": 1699636201185,
            "mdate": 1699636201185,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GuBI8Yikq1",
                "forum": "J0cCuE3JRC",
                "replyto": "Hv5E6PWirz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2621/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2621/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer dape"
                    },
                    "comment": {
                        "value": "We express our gratitude to the reviewer for the insightful and constructive feedback. In light of your valuable comments, as well as those from other reviewers, we are revising our paper. We are confident that these revisions have substantially enhanced the quality of our work. We hope that our responses will be sufficiently informative for you to reconsider assessing a higher rating for the revised paper.  \n\n$\\textbf{W1: Novelty:}$ The paper's novelty appears constrained. The significance of graph structural features in the realm of link prediction has been previously underscored by several studies, such as [1][2][3].\n\n>Thank you very much for bringing these references to our attention. We added these references to our paper. We wish to emphasize that the novelty of our work lies in substantiating that traditional feature engineering methods can stand shoulder-to-shoulder with cutting-edge GNN models. We argue this based on consistently outperforming powerful GNN methods on various datasets, which are published in top conferences. This supports our claim that our Bag of Features model is a practical and effective alternative to these computationally intensive models. \n\n$\\textbf{W2. Sota Baselines:}$ The evaluation omits several critical baselines that similarly exploit graph structural features. Notably absent are BUDDY, Neo-GNN, NCN, NCNC, and NBFNet.\n\n>By using reference [1], we give the performance comparison of these GNN models with our model. For OGBL-COLLAB, we report Hits@50 results below, and add them into our accuracy table (Table 5). However, for CORA, CITESEER, and PUMED datasets, the reported performances use different splits. Considering our model uses less training data, it produces highly competitive results with these cutting edge GNN models. Since the experimental setups are different, we opt out these performances  in our accuracy table in the main text, however we added this table to appendix (Table 7). \n\n>OGBL-COLLAB\n| Model   | Hits@50      |\n|---------|--------------|\n| SEAL    | 63.37 \u00b1 0.69 |\n| BUDDY   | 64.59 \u00b1 0.46 |\n| Neo-GNN | 66.13 \u00b1 0.61 |\n| NCN     | 63.86 \u00b1 0.51 |\n| NCNC    | 65.97 \u00b1 1.03 |\n| NBFNet  | OOM          |\n| Ours    | 76.50 \u00b1 0.27 |\n\n>| Model    | Split      | CORA         | CITESEER     | PUBMED       |\n|----------|------------|--------------|--------------|--------------|\n| BUDDY    | 85:05:10    | 95.06\u00b10.36   | 96.72\u00b10.26   | 98.20\u00b10.05   |\n| Neo-GNN  | 85:05:10    | 93.73\u00b10.36   | 94.89\u00b10.60   | 98.71\u00b10.05   |\n| NCN      | 85:05:10    | 96.76\u00b10.18   | 97.04\u00b10.26   | 98.98\u00b10.04   |\n| NCNC     | 85:05:10    | 96.90\u00b10.28   | 97.65\u00b10.30   | 99.14\u00b10.03   |\n| NBFNet   | 85:05:10    | 92.85\u00b10.17   | 91.06\u00b10.15   | 98.34\u00b10.02   |\n| Ours     | 70:10:20    | 94.54\u00b10.30   | 95.01\u00b10.35   | 94.73\u00b10.15   |\n\n>[1] Li, J., Shomer, H., Mao, H., Zeng, S., Ma, Y., Shah, N., Tang, J. and Yin, D., 2023. Evaluating Graph Neural Networks for Link Prediction: Current Pitfalls and New Benchmarking. arXiv preprint arXiv:2306.10453.\n\n$\\textbf{W3. Extra OGB experiments:}$\n\n>We are working on additional OGB experiments. We hope to provide the results before the rebuttal deadline. We will let you know when we have the results.\n\n$\\textbf{W4. Ablation Study.}$ There are no ablation studies about the importance of each feature.\n\n>Thank you very much for this suggestion. We added the ablation study table reporting the impact of each individual feature in the appendix (Table 7), which you can see in our revision.\n\n$\\textbf{W5. No Code:}$\n\n>Thanks for letting us know about the mistake in the link. We've fixed it in the paper, and you can find the correct link below. If you have any more questions about our code, please let us know.\n https://github.com/workrep20232/LinkPrediction\n\n$\\textbf{W6. Using original features for node similarity?}$ Using the original feature to measure the node similarity might be unreasonable.\n>Thanks for raising this concern. Our primary objective with our approach is to provide a highly generalized method for gathering all relevant similarity/dissimilarity information between node pairs and leveraging these features in the task of link prediction. You correctly point out that the efficacy of domain features may vary depending on the dataset's nature, whether it exhibits homophily or heterophily. We acknowledge that, in the PHOTO and COMPUTERS datasets, domain features did not contribute substantially to performance. In contrast, for CORA, CITESEER, and PUBMED datasets, the inclusion of domain features notably enhanced performance. It's noteworthy that incorporating these features didn't compromise performance across all datasets; instead, it consistently improved performance across the board. You can check our Feature Importance table (Table 7) in the appendix.\n\nPlease let us know if you have further questions. Thank you very much again for your valuable time and feedback."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2621/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699758333458,
                "cdate": 1699758333458,
                "tmdate": 1699759544333,
                "mdate": 1699759544333,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BFKQmpS33C",
                "forum": "J0cCuE3JRC",
                "replyto": "GuBI8Yikq1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2621/Reviewer_dape"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2621/Reviewer_dape"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response, which has addressed some of my concerns. However, I still have some concerns:\n1. The data splits of the proposed method and baselines in W2 are different. As a result, it's not easy to make a fair comparison.\n2. How do you address the scalability issue? It seems the time complexity is quite high.\n3. Waiting for the results of OGB datasets."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2621/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700246208886,
                "cdate": 1700246208886,
                "tmdate": 1700246208886,
                "mdate": 1700246208886,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kqdo047VLm",
                "forum": "J0cCuE3JRC",
                "replyto": "Hv5E6PWirz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2621/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2621/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for your questions. \n\n**1. Different data splits.**\n\nYes, in our original submission, we used the performances reported in two very recent papers' results as baselines [1,2], but they were using different data splits. So, we had one table for Cora/Citeseer/PubMed (70:10:20), and Photos/Computers (85:5:10). However, with the request of Reviewers rEua and in5m, we added these comparisons with different splits for Cora/Citeseer/PubMed. In our original submission, we did not want to cause any confusion by giving two different results for the same datasets. To avoid the confusion, we give these results in the appendix. If needed, we can repeat the experiments with new splits, and report them in the appendix.\n\n**2. Scalability Issue**\n\nWe believe scalability is one of the advantages of our simple approach. We give the details of our runtime and computational complexities in Experiments section. The computational complexities of our similarity indices are $\\mathcal{O}(|V|.k^3)$ where $|V|$ is the total number of nodes and $k$ is the maximum degree in the network. For a large network, the computational time needed for our approach is much less than the training time for a message passing GNN. Therefore, our approach provides a good alternative to common methods when dealing with large networks.\n\n**3. OGB Datasets.**\n\nUnfortunately, due to unforeseen circumstances, we could not complete OGBL-PPA experiments yet. We will add the results to the paper when we have it. However, in OGBL-COLLAB, we outperform all the existing results with our simple model. We are expecting a competitive result with OGB Leaderboard in OGBL-PPA dataset considering its high performance in all other (both homophilic and heterophilic) datasets, too. \n\nThank you very much for your valuable time and feedback. Please let us know if you have further questions.\n\n[1] Li, Juanhui, Harry Shomer, Haitao Mao, Shenglai Zeng, Yao Ma, Neil Shah, Jiliang Tang, and Dawei Yin. \"Evaluating Graph Neural Networks for Link Prediction: Current Pitfalls and New Benchmarking.\" arXiv preprint arXiv:2306.10453 (2023).\n\n[2] Guo, Zhihao, Feng Wang, Kaixuan Yao, Jiye Liang, and Zhiqiang Wang. \"Multi-scale variational graph autoencoder for link prediction.\" In Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining, pp. 334-342. 2022."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2621/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700661283857,
                "cdate": 1700661283857,
                "tmdate": 1700661331160,
                "mdate": 1700661331160,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4ZVFhGBLzh",
                "forum": "J0cCuE3JRC",
                "replyto": "kqdo047VLm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2621/Reviewer_dape"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2621/Reviewer_dape"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the further response. However, it doesn't solve my question. First, you should use the same data split for your method and the baselines to make a fair comparison. Second, I suggest three ogb datasets, i.e., ogbl-ddi, ogbl-ppa, and ogbl-citation2, but I don't see any results about these three datasets."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2621/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700671326391,
                "cdate": 1700671326391,
                "tmdate": 1700671326391,
                "mdate": 1700671326391,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JuSORFF32i",
                "forum": "J0cCuE3JRC",
                "replyto": "Hv5E6PWirz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2621/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2621/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**1. Different Splits**\n\nWe want to clarify that our Table 3 is all 70:10:20 using baselines from (Guo et all 2022). We are not using different split there.\n\n| Models   | CORA       | CITESEER   | PUBMED     |\n|----------|------------|------------|------------|\n| Node2Vec | 84.49 \u00b1 0.49   | 80.00 \u00b1 0.68       | 80.32 \u00b1 0.29     |\n| MVGRL    | 75.07 \u00b1 3.63   | 61.20 \u00b1 0.55       | 80.78 \u00b1 1.28     |\n| VGAE     | 88.68 \u00b1 0.40   | 85.35 \u00b1 0.60       | 95.80 \u00b1 0.13     |\n| SEAL     | 92.55 \u00b1 0.50   | 85.82 \u00b1 0.44       | 96.36 \u00b1 0.28     |\n| GCN      | 90.25 \u00b1 0.53   | 71.47 \u00b1 1.40       | 96.33 \u00b1 0.80     |\n| GSAGE    | 90.24 \u00b1 0.34   | 87.38 \u00b1 1.39       | 96.78 \u00b1 0.11     |\n| JKNet    | 89.05 \u00b1 0.67   | 88.58 \u00b1 1.78       | 96.58 \u00b1 0.23     |\n| CFLP     | 93.05 \u00b1 0.24   | 92.12 \u00b1 0.47       | **97.53 \u00b1 0.17**     |\n| Ours| **94.54 \u00b1 0.30**   | **95.01 \u00b1 0.35**       | 94.73 \u00b1 0.15     |\n\nFor Tables 8 and 9, we are adding new GNN baselines by using the performances from (Li et al, 2023) which uses 85:05:10 split. If your question is about these two tables, we are sorry that we misunderstood your concern.  We will add the experiments with 85:05:10 splits for Cora/Citeseer/PubMed datasets. Since we already have the features, it should take only a couple of hours. We'll get back to you soon.\n\n**2. OGB Datasets**\n\nUnfortunately, we could not finish the OGB experiments in time. Considering the size of these datasets, we had to use our university clusters. We were expecting to finish one of these datasets in time: OGBL-PPA. We were able to get the features, however, we had a problem with running ML classifiers. \n\nNonetheless, we were not expecting to finish all three OGB datasets in one week considering their sizes. Our structural features are always the same, but domain features need to be customized and it should be tailored for each dataset. For PPA, we were able to do it, but since OGBL-Citation2 and OGBl-DDI are completely different formats, we need time to extract meaningful features from the node and edge features provided. \n\nAgain, we want to underline that our aim is not to promote feature engineering models but to show that current GNN performances are not far from traditional methods, verifying the recent theoretical studies. Thank you very much for your time and feedback."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2621/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700673688940,
                "cdate": 1700673688940,
                "tmdate": 1700674333658,
                "mdate": 1700674333658,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zpNlbx5MIx",
                "forum": "J0cCuE3JRC",
                "replyto": "ixF3ZDkC68",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2621/Reviewer_dape"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2621/Reviewer_dape"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Authors,\n\nThanks for the efforts. The fact that structural features are useful for link prediction is a well-known fact. Thus, state-of-the-art methods, such as NCNC, leverage these features can improve the performance of GNNs. From the results in your rebuttal, we can see that just leveraging all the features cannot outperform GNNs, although it can achieve relative good performance. In my opinion, just verifying this phenomenon is not sufficient for an ICLR paper. I recommend the authors go beyond this verification and leverage these features to design some new methods.\n\nBest Regards,\n\nReviewer dape"
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2621/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700689299980,
                "cdate": 1700689299980,
                "tmdate": 1700689299980,
                "mdate": 1700689299980,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NPheynnnNc",
            "forum": "J0cCuE3JRC",
            "replyto": "J0cCuE3JRC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2621/Reviewer_rEua"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2621/Reviewer_rEua"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes bags of heuristics for link prediction. The experiments show the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper shows that the conventional features of the graph can be utilized to outperform GNN in link prediction."
                },
                "weaknesses": {
                    "value": "1. The conclusion that graph heuristics can outperform GNN in link prediction is not a new contribution. This article only proposes some heuristics but does not explain why they are effective.\n2. This paper overclaims its contribution. The method and experiments of this paper are designed for link prediction, but the contribution of this paper lies in discussing that GNN is not good at graph learning. As far as I know, heuristics perform better than GNN only in link prediction, and there seems to be no similar phenomenon in other domains such as graph classification, node classification, and graph regression.\n3. The experimental part of this article does not compare with state-of-the-art GNN methods, such as NBFNet[1] and Seal[2].\n\n[1] Neural Bellman-Ford Networks: A General Graph Neural Network Framework for Link Prediction\n\n[2] Link Prediction Based on Graph Neural Networks"
                },
                "questions": {
                    "value": "see weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2621/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698912916716,
            "cdate": 1698912916716,
            "tmdate": 1699636201068,
            "mdate": 1699636201068,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dtU2KsEReS",
                "forum": "J0cCuE3JRC",
                "replyto": "NPheynnnNc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2621/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2621/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer rEua"
                    },
                    "comment": {
                        "value": "We express our gratitude to the reviewer for the insightful and constructive feedback. In light of your valuable comments, as well as those from other reviewers, we are revising our paper. We are confident that these revisions have substantially enhanced the quality of our work. We hope that our responses will be sufficiently informative for you to reconsider assessing a higher rating for the revised paper.  \n\n$\\textbf{W1\\ - \\ Novelty:}$\n\n>Thank you very much for sharing this concern. We wish to emphasize that the novelty of our work lies in substantiating that traditional feature engineering methods can stand shoulder-to-shoulder with cutting-edge GNN models. We argue this based on consistently giving shoulder-to-shoulder performance with powerful GNN methods on various datasets, which are published in top conferences. This supports our claim that our Bag of Features model is a scalable and effective alternative to these computationally intensive models.\n\n>The key reason behind our success is revealed in our ablation study. Our approach of effectively feeding both structural and domain features in our ML classifier has proven to be crucial. The study confirms that, in most datasets, using only structural or domain features leads to lower performance. However, combining them effectively significantly boosts the overall performance of our model.\n\n$\\textbf{W2. GNN comparison for Other Graph Learning Tasks}$\n\n>Thank you for raising this issue. It's accurate to observe that our focus in this manuscript centers specifically on the challenge of link prediction. The decision to narrow our attention to this particular task within graph representation learning was primarily influenced by space limitations. In a distinct investigation, we employed similar techniques to compare Graph Neural Networks (GNNs) with traditional feature extraction in the context of a node classification task. Across six benchmark datasets in that study, we outperformed all state-of-the-art GNNs. While our findings with the OGBGN dataset did not outperform the OGB leaders, but gave highly competitive results. To preserve anonymity, we refrain from providing additional details.\n\n>Given the inherent differences between node classification and link prediction tasks, the employed feature extraction methods also diverge significantly. Consequently, we opted to present these two studies in separate papers. Nevertheless, the outcomes from both studies imply that existing GNNs do not exhibit a substantial superiority over traditional feature engineering models. It is crucial to clarify that our intention in this paper is not to advocate for the adoption of feature engineering methods moving forward. Instead, we aim to document that current GNN models do not demonstrate a significant advantage over them. This revelation should illuminate the current state of affairs with GNNs and motivate researchers to advance and refine these models further.\n\n$\\textbf{W3. SOTA Baselines:}$ The experimental part of this article does not compare with state-of-the-art GNN methods, such as NBFNet[1] and Seal[2].\n\n>Thanks for pointing out these baseline references. It's important to mention that SEAL [2] has already been given in Table 3 - row 4. For NBFNet, we report the result for a different split of 85:5:10 below by using reference [3]. However, since we use a different split for Table 3 (70:10:20), we opted not to include NBFNet in Table 3. On the other hand, with the suggestion of Reviewer Dape, we added several recent GNN baselines to our Table 5, and Table 8. We also acknowledged this reference in our related work section.\n\n>The specifics of AUC performances are outlined in the table provided below.\n|      | Split      | CORA  | CITESEER | PUBMED |\n|------|------------|-------|----------|--------|\n| SEAL | 70:10:20    | 92.55 | 85.82    | 96.36  |\n| NBFNet | 85:05:10  | 95.60  | 92.30     | 98.30   |\n| Ours | 70:10:20    | 94.54 | 95.01    | 94.73  |\n\n>[3] Li, J., Shomer, H., Mao, H., Zeng, S., Ma, Y., Shah, N., Tang, J. and Yin, D., 2023. Evaluating Graph Neural Networks for Link Prediction: Current Pitfalls and New Benchmarking. arXiv preprint arXiv:2306.10453.\n\nPlease let us know if you have further questions. Thank you very much again for your valuable time and feedback."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2621/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699757690777,
                "cdate": 1699757690777,
                "tmdate": 1699759525563,
                "mdate": 1699759525563,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]