[
    {
        "title": "Masked Structural Growth for 2x Faster Language Model Pre-training"
    },
    {
        "review": {
            "id": "MR5Uxf6EcO",
            "forum": "rL7xsg1aRn",
            "replyto": "rL7xsg1aRn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4887/Reviewer_aWRp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4887/Reviewer_aWRp"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a framework called Masked Structural Growth (MSG) for progressive pre-training of transformer-based language models. The MSG includes the following two parts:\n(1) growth schedules involving all the four possible dimensions;\n(2) strictly function-preserving growth operators that are independent of the initialization of new weights.\n\nTheir experimental results show that the MSG achieves state-of-the-art speed-up on Bert-base, Bert-large, and GPT-2 with equal or improved performances on downstream tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The greatest strength of the paper is that it introduces the MSG operators for all growth dimensions by using external masks.\nThese growth operators are strictly function-preserving (even with layer normalization), and independent of the initialization of new weights.\nThis apparently has the advantages over the growth operators proposed in the previous work.\n\nAdditionally, the growth schedule proposed in the MSG uses grid-search to grow only one dimension each time, which provides empirical insights on the growing models and holds the potential for generalization."
                },
                "weaknesses": {
                    "value": "One weakness of the paper is that the authors have not empirically shown the advantages of the MSG in today's large-scale transformer models (which is much larger than the scale of BERT-large and GPT2). Such a weakness may be due to the lack of the computational resources.\nIn terms of presentation, the authors may want to add more description on the bert2BERT method, since it is one main competitor to the proposed MSG.\n\nSome minor issues will be listed in the \"Questions\" section."
                },
                "questions": {
                    "value": "I list my questions and some minor issues as follows:\n(1).\nFor on page 4 and page 5:\nAre vectors x, y, c in your section 3.1.1 all column vectors? If yes, then in Eq. (12),\nx^T (the transpose of x) is not necessary, x will be good enough.\n\n(2).\nFor Eq. (11) on page 5:\nFor d_2 < i < d_2', it should be:  d_2 < i <= d_2'\n\n(3). Similarly,\nFor Eq. (14) on page 5:\nFor d_1 < j < d_1', should be: d_1 < j <= d_1'\n\n(4).\nLast line on page 7:\nThe stated (Qin et al., 2022b) is not the reference for the bert2BERT method.\nDo you really mean the bert2BERT or the ELLE here?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4887/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698821490645,
            "cdate": 1698821490645,
            "tmdate": 1699636473134,
            "mdate": 1699636473134,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0wceNWInJk",
                "forum": "rL7xsg1aRn",
                "replyto": "MR5Uxf6EcO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4887/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4887/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your constructive suggestions!\n- Application to larger models: please see the common response.\n- We have added introductions to bert2BERT in Appendix (C) in the new version.\n- Equation 11, 12, 14 are fixed. We appreciate your careful reading.\n- ELLE reported an evolved version of bert2BERT which is applied to their own tasks. The methods are very similar with a main difference being an interpolation (instead of stacking) strategy in depth growth. We mean the implementation from ELLE here.\n\nWe hope that this solves some of your concerns and we are more than happy to have further discussions."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4887/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700499415728,
                "cdate": 1700499415728,
                "tmdate": 1700499415728,
                "mdate": 1700499415728,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HEy5jocClz",
            "forum": "rL7xsg1aRn",
            "replyto": "rL7xsg1aRn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4887/Reviewer_vhLH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4887/Reviewer_vhLH"
            ],
            "content": {
                "summary": {
                    "value": "The manuscript focuses on training transformers faster by progressively growing the architecture during training. Problems with previous methods in this area include limited growth \"dimensions\" (e.g. inability to progressively grow the hidden dimension), absence of function preservation in networks with LayerNorm, and a forced choice between function preservation and initialization flexibility. The proposed method (MSG) addresses these problems, and experiments support its effectiveness (i.e., it accelerates training). Ablation and hyperparameter studies further justify the chosen methodology and offer insights that are helpful for understanding the progressive growth problem more broadly."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The proposed progressive growth method speeds up training more than competing approaches, and it maintains or improves accuracy/perplexity performances on test datasets (relative to the model without progressive growth).\n\nThe study of the impact of different growth dimensions is broadly helpful and clear (Figure 2). This analysis provides intuition for the MSG approach, making it seem less tailored to perform well on just one dataset/architecture (i.e., more generally applicable).\n\nCare is taken to ensure that comparisons across progressive growth methods are fair and that the baseline progressive growth methods are relevant. In Section 4.2, salient features of progressive growth algorithms are isolated appropriately to facilitate analyses of the proposed method, MSG.\n\nThe manuscript offers an original and well-designed approach to disentangling the function preservation and initialization decisions of progressive learning. This facilitates the creation of a more flexible growth algorithm and better analysis of the importance of these algorithm components to progressive learning. For example, the final paragraph of Section 4.4 leverages this disentangling to show that both function preservation and flexible initializations matter to performance, supporting the form of the MSG method."
                },
                "weaknesses": {
                    "value": "While the manuscript makes careful comparisons to competing progressive growth methods, some of the baseline performances (without progressive growth) are a little lower than expected. Please see my comments about this in the \"Questions\" section below."
                },
                "questions": {
                    "value": "Score-affecting:\n\n1. Please ensure all baseline approaches are faithful so that readers know MSG is effective in high-performing training runs. Examples follow.\n   - In Tables 4 and 5, the SQuAD scores are 2-3% lower than what is reported in the BERT paper.\n   - In Table 5, GPT-2 perplexity on Wikitext2 is high (41.3). There are strong public GPT-2 implementations; e.g., https://github.com/karpathy/nanoGPT. \n      - It would be nice to see an MSG version of the LiGO (Wang et al., 2022) paper's Figure 3C: i.e., show perplexity/loss dynamics of GPT trained with and without MSG.\n\nHelpful:\n\n1. Please summarize the MSG operator conclusions/discussion from the referenced technical report (Li et al., 2023). \n2. Consider moving tables and figures to the page they are discussed on, or to the next page. This helps readability.\n3. Clean up figures and tables. Some example suggestions follow:\n   - Table 2 could have uniform line lengths. \n   - Figure 2 could have larger axis text, lines, legend text, etc.\n\n\nMinor:\n\n1. The manuscript would benefit from a proofreading. Some examples follow:\n   - \"Proof\" is not a verb that means \"to mathematically demonstrate\". Say \"we prove\" instead of \"we proof\". A \"proof\" is what proves something.\n   - On page 6, rephrase the second sentence of the \"Stage Duration\" paragraph -- the training steps are what gets split, not the stage steps.\n2. I think that not all the inequalities in the second lines of equations 11 and 14 are strict (i.e., change a \"<\" to a \"<=\"). \n3. All of the methods listed in the Related Work section (progressive growth, weight sharing, MoEs, etc.) are discussed in the survey \"Compute Efficient Deep Learning\" (Bartoldson et al., 2023)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4887/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4887/Reviewer_vhLH",
                        "ICLR.cc/2024/Conference/Submission4887/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4887/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698826531605,
            "cdate": 1698826531605,
            "tmdate": 1700702348520,
            "mdate": 1700702348520,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RMAzNPIS3Z",
                "forum": "rL7xsg1aRn",
                "replyto": "HEy5jocClz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4887/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4887/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your careful reading and constructive suggestions! We believe that your suggestions help us a lot in improving the paper. \n- We tried our best with our current computational resources to study the performance gap problem you mentioned. Specifically, we first doubted that the high perplexity of our GPT model was because the model was undertrained. Thus, we controlled the training data to be exactly the same with nanoGPT, which is 9B OpenWebText tokens from huggingface. We performed a more sufficient training with a similar setting to nanoGPT and achieved very close training loss to the one they reported (3.11). Thus, we believe that our implementation is close enough to strong public repositories with this specific dataset, which makes our experiments meaningful. The settings and results are detailed in Appendix (J) in the new version. However, although we reduced the ppl from 41 to 37, there is still a gap. We guess it is because of the data: the public OpenWebText may have similar distribution to the closed-source data used to train GPT-2, but it may be short in number or quality. Another evidence is that none of the related work we cited successfully matches the original GPT-2 perplexities. For the SquaD results, we guess that it's a similar story, and code will be available to ensure reproductivity.\n- Training dynamics: please see the common response.\n- (Li et al., 2023) is summarized in Appendix (I).\n- Most Tables, figures, and minors are fixed in the new version, with a new citation to (Bartoldson et al., 2023) in Related Work. For the positions of Tables and figures, we will handle them in the final version.\n\nWe hope that this solves some of your concerns and we are more than happy to have further discussions."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4887/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700499412860,
                "cdate": 1700499412860,
                "tmdate": 1700500272621,
                "mdate": 1700500272621,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "x55WPIC9sr",
                "forum": "rL7xsg1aRn",
                "replyto": "RMAzNPIS3Z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4887/Reviewer_vhLH"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4887/Reviewer_vhLH"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for these new, supporting results! Please consider adding a reference to Appendix J or K to the main text.\n\nI have improved my \"soundness\" rating and will recommend acceptance."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4887/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700702327573,
                "cdate": 1700702327573,
                "tmdate": 1700702327573,
                "mdate": 1700702327573,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PTYVxtxkaH",
            "forum": "rL7xsg1aRn",
            "replyto": "rL7xsg1aRn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4887/Reviewer_Bd3L"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4887/Reviewer_Bd3L"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an interesting mechanism (masked structural growth) to accelerate the pre-training of foundation models. Technically, two main research problems are studied: i) how to determine the optimal growth schedule; and ii) how to design efficient growth operators. Some experiments are conducted to show the advances of the proposed method with a 2.2 times speedup."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This paper studies a very important research problem in foundation model training. Such a technique can effectively reduce the end-to-end training time of the foundation models. \n\n- Some theoretical analysis was presented to show the guidance of the design in the proposed method. \n\n- The organization and presentation of the evaluation section are clear, with the hypothesis explicitly stated in each subsection."
                },
                "weaknesses": {
                    "value": "- I am a little bit confused by the introduced method; intuitively, it seems to me that the proposed MSG operator would extend the parameter space to a more rugged space when compared with the average-based methods. Thus I cannot understand why it works well intuitively. \n\n- The experimental results can be further improved:\n  - The scale of the benchmarked model is small. Would the proposed method be deployed for a practice scale model, e.g., 7B to 13B or 13B to 30B?\n  - The reported results can not demonstrate the real effectiveness of the proposed method in terms of ability to generalize, in fact, it seems that the results are mainly based on training loss while the most important metric should be a set of metric to measure the generalization ability of the model.\n  - There should be some plot to show the training loss of the proposed method w.r.t different training iterations, this is essential to understand the performance."
                },
                "questions": {
                    "value": "Please address my concerns listed in the Weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4887/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698828305017,
            "cdate": 1698828305017,
            "tmdate": 1699636472946,
            "mdate": 1699636472946,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wjMuSZzu6w",
                "forum": "rL7xsg1aRn",
                "replyto": "PTYVxtxkaH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4887/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4887/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your insightful comments and suggestions!\n- An explanation of the effectiveness of MSG: (1) it introduces masks that make the process smoother for the model to adapt to the more \u201crugged\u201d space caused by initialization; (2) average-based methods can underperform a na\u00efve random initialization due to problems such as the Symmetry issue (Section 2.3.3). A more general speaking is: minimizing the difference to the original weights does not guarantee improving the training dynamics, since the latter is a research problem itself (also see Section 2.3.3). Another intuition (may be not so good) is that sometimes distillation works better than data-driven pruning for model compression.\n\n- Larger models: please see the common response.\n- Metrics: for the Bert models, we used an average of the GLUE scores. It already includes multiple metrics including Matthews correlation for CoLA, Pearson/Spearman correlation for STS-B and accuracy/f1 for MRPC, QQP, and SQuAD. Since these are a diversity of downstream NLP tasks, they would reflect some generalization ability of the models. The detailed results for each subtask are listed in Table 6 and 7. These may not be loss-based metrics since we observe that they are not strictly aligned with training loss in late phases (see footnote 4 and Figure 5). For GPT models, we follow common practices to report perplexity since they are aimed for text generation. \n- Training loss plot: please see the common response.\n\nWe hope that this solves some of your concerns and we are more than happy to have further discussions."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4887/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700499409297,
                "cdate": 1700499409297,
                "tmdate": 1700499409297,
                "mdate": 1700499409297,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]