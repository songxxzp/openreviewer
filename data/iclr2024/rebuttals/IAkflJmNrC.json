[
    {
        "title": "Polarity-Aware Semantic Retrieval with Fine-Tuned Sentence Embeddings"
    },
    {
        "review": {
            "id": "LoQs4LQCFq",
            "forum": "IAkflJmNrC",
            "replyto": "IAkflJmNrC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3876/Reviewer_DEGg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3876/Reviewer_DEGg"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the effectiveness of fine-tuning sentence embeddings for retrieving polarity-aware and semantic-aware similar sentences simultaneously. The authors propose metrics to measure polarity similarity and semantic similarity respectively. Experiments are done with four sentence embedding models of different sizes on two datasets."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. The paper investigates the polarity capturing and semantic capturing of the finetuned sentence embedding methods.\n2. The authors conduct experiments on two datasets with four base models."
                },
                "weaknesses": {
                    "value": "1. The paper is poorly written, and the motivation and problem definition are not clear.\n2. The problem studied is not insightful and the findings are trivial.\n3. Many technique details are not well illustrated."
                },
                "questions": {
                    "value": "1. Could you give a formal definition of the polarity? From my understanding, it is according to the label in the original dataset.\n2. What is your motivation for this study and what is your takeaway conclusion? The experimental results are kind of trivial to me. \u201cPolarity\u201d and \u201cSemantic\u201d can have distribution overlap but are not exactly the same. So if you finetune the sentence embedding methods based on polarity perspective, it is quite common that it will suffer on the semantic dimension.\n3. The definition of semantic score is confusing to me. Correct me if I am wrong, but it seems that if you finetune your sentence embedding models on whatever data, the semantic score will always decrease, since the finetuned model is diverging from the original parameters.\n4. What is  w_i in sec 3.2.2? Is it the same as that in sec 3.2.1?\n5. What is the reference model for semantic calculation used in sec 3.3 and sec 4? If I am understanding correctly, in section 4, you are using the base sentence embedding model without further finetuning as the reference model. Is it the same in sec 3.3?\n6. How do you set \\beta in sec 3.2.2?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3876/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698521139709,
            "cdate": 1698521139709,
            "tmdate": 1699636346171,
            "mdate": 1699636346171,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "G5PhtoRcL0",
                "forum": "IAkflJmNrC",
                "replyto": "LoQs4LQCFq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3876/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3876/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## Weaknesses:\n- W1: I agree that our motivation and problem definition could be much better stated. And for that I thank you and some of the other reviewers in pointing out. \n\n- W2: There is a real use-case to retrieving with multiple objectives, and, as others have stated, relates to the catastrophic forgetting when fine-tuning these models. We showed that you can get a better trade-off than what is currently available with the architecture as-is, but by augmenting input data instead. I have since the reviews came in studied this in more depth and we out-perform alternatives such as SetFit in all related benchmarks with SentEval https://github.com/facebookresearch/SentEval, although we did not train for any of these objectives.\n\n- W3: This is valid, and we did not properly describe the weights (which are reused) and perhaps the failed attempt at averaging the metrics. We removed this and stuck to the two introduced metrics instead.\n\n\n## Questions:\n- Q1: The polarity is the label. If this was unclear, we are strictly using binary labeled datasets.\n\n- Q2: The main motivation is rooted in a real-world application of e.g. retrieving K items that are similar on multiple objectives. An example would be a news article that has high similarity with another, while they may be connected to two entirely different topics (e.g. by sentiment). The entire point shown here is that we can increase the polarity far above the reduction of similarity, and this trade-off can be reduced by the configurations we evaluated. \n\n- Q3: Answered in Q2.\n\n- Q4: They are shared. We have since clarified this. The paper will be updated.\n\n- Q5: The reference model is always the respective baseline model (i.e. if the fine-tuned model is based on model A, the reference is A)\n\n- Q6: As mentioned, we opted for removing this common metric, as it provided little value in hindsight."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3876/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700515253298,
                "cdate": 1700515253298,
                "tmdate": 1700515305981,
                "mdate": 1700515305981,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uBHk4ADOTM",
                "forum": "IAkflJmNrC",
                "replyto": "G5PhtoRcL0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3876/Reviewer_DEGg"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3876/Reviewer_DEGg"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. I have read the comments and decided to keep my score."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3876/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700662760740,
                "cdate": 1700662760740,
                "tmdate": 1700662760740,
                "mdate": 1700662760740,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FFVAucKeTp",
            "forum": "IAkflJmNrC",
            "replyto": "IAkflJmNrC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3876/Reviewer_TX7z"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3876/Reviewer_TX7z"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to investigate the effect of fine-tuning sentence embeddings on two sentence retrieval tasks, equal polarity and high semantic similarity. This paper proposes two evaluation metric and conducts experiments on two sources of datasets."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. This paper is generally self-contained for readers to understand different concepts. It provides enough tables and figures to illustrate text descriptions.\n\n2. Experiment section contains enough implementation details, which allow readers to potentially reproduce the results in the paper.\n\n3. This paper also identifies a few possible future works, which look promising and interesting."
                },
                "weaknesses": {
                    "value": "Despite the above strengths, I still have the following concerns:\n\n1. This paper lacks technical contribution. I can't see an originally proposed model architecture for sentence embedding problem. Instead, this paper looks more like a summary of why fine-tuning is an important technique for sentence embedding, but fine-tuning is also not a new concept and has been adopted by many previous works, including most LLMs.\n\n2. When we do experiments, we usually need to repeat the same experiment multiple times and report both mean and standard deviation, in order to verify that the proposed model indeed significantly outperforms baselines. However, I can see mean but not standard deviation in the experiments."
                },
                "questions": {
                    "value": "N.A."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3876/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3876/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3876/Reviewer_TX7z"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3876/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698701362810,
            "cdate": 1698701362810,
            "tmdate": 1699636346102,
            "mdate": 1699636346102,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "MEZWrWaMJy",
                "forum": "IAkflJmNrC",
                "replyto": "FFVAucKeTp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3876/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3876/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "1. I have questions regarding your definition of \"technical contribution\". While we do not propose a change in model architecture, we have shown that a change in the data, namely by augmenting input data on supervised data not related to sentence similarity, leads to improved retrieval scores for specific configurations. Fine-tuning on multiple objectives is vastly different from studying a single metric on existing datasets when fine-tuning. Otherwise, refer to the other comments.\n\n2. Thanks for this. Will update the paper. From initial inspection, there are no changes to our conclusions based on the std, but is surely a much-needed addition."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3876/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700515699006,
                "cdate": 1700515699006,
                "tmdate": 1700515699006,
                "mdate": 1700515699006,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "fQUZSE72yh",
            "forum": "IAkflJmNrC",
            "replyto": "IAkflJmNrC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3876/Reviewer_Co32"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3876/Reviewer_Co32"
            ],
            "content": {
                "summary": {
                    "value": "This paper explores the efficacy of encoding polarity into sentence embeddings while\nretaining semantic similarity, done by fine-tuning models on data generated to suit the objectives of various sentence-transformers loss functions. Two metrics were introduced to evaluate the results on The Stanford Sentiment Treebank binary classification and The News Headlines Dataset Sarcasm Detection: the Polarity and Semantic Similarity Score."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Sound and rigorous experimentation, with reproducible results hosted from an anonymous repository\n- Polarity scores for all loss configurations and Semantic similarity scores for all loss configurations and Aggregated scores across all configurations are reported meticulously\n- Crystal clear presentation on the system, training configuration, data generation and tasks"
                },
                "weaknesses": {
                    "value": "The paper offers a crystal clear presentation of a system effectively fine-tuning sentence embeddings\nfor simultaneously retrieving sentences of equal polarity and high semantic similarity. However, in my opinion, it is not clear what the conclusion from the paper is, and whether anything *generalizable* can be drawn from the paper. For example, why SST-2 and Sarcastic Headlines are picked as the tasks for evaluation, why were the three loss functions picked, and most importantly, are not answered. In addition, aside from \"To our knowledge, no comparable work exists to let us evaluate the model performance on this joint task with well-established metrics\", no discussion on the extensibility of the work was offered. Perhaps the paper is better suited for a system track in an NLP conference, or a workshop in a machine learning conference, for its rigorous description of an interesting system offering 2 well-defined metrics for evaluation, and though its relative lack of contribution to machine learning."
                },
                "questions": {
                    "value": "> We combine the two in a metric C, weighted with \u03b2 (0.5 by default to produce the average),\nprimarily used to illustrate overall performance: C\u03b2(s) := \u03b2 \u00b7 PM(s) + (1 \u2212 \u03b2) \u00b7 SM_M(s)\n\nWhy can the two metrics be combined in the form of a weighted sum? (After all, one may argue that the \"units\" of the two metrics are different.)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3876/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698740571156,
            "cdate": 1698740571156,
            "tmdate": 1699636346031,
            "mdate": 1699636346031,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bQ4h2pzvda",
                "forum": "IAkflJmNrC",
                "replyto": "fQUZSE72yh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3876/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3876/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the detailed feedback, and sorry for the slow response.\n\n1. I agree that we did not clearly present our motivation and conclusions. We've since updated it some.\n\n2. The motivation was rooted in being able to retrieve similar and sarcastic sentences, and we later added SST-2 as it is a heavily used dataset for binary classification. I agree, though, that it should be connected to something more generalizable, and we have since studied its effects on a lot more tasks for sentence embeddings than what was presented in the paper by the SentEval library (https://github.com/facebookresearch/SentEval). Interestingly, our fine-tuning approach outperforms SetFit (the only comparable existing tool today) on every task.\n\n3. There was a reason behind the selection of loss functions. This has been updated to account for every loss function mentioned in the sentence-transformers library.\n\n4. Answer to 2 mentioned evaluation across different evaluations (subjectivity, movie reviews, opinion polarity, question types, etc.), which would likely build upon the extensibility of the work. \n\n5. After reading the responses here, you are likely right on this one! Thank you regardless for the great insights provided by the review"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3876/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700516695679,
                "cdate": 1700516695679,
                "tmdate": 1700516695679,
                "mdate": 1700516695679,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dOVskxOvDa",
                "forum": "IAkflJmNrC",
                "replyto": "bQ4h2pzvda",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3876/Reviewer_Co32"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3876/Reviewer_Co32"
                ],
                "content": {
                    "title": {
                        "value": "Agreeing with another review on the lack of technical contribution"
                    },
                    "comment": {
                        "value": "Clear and self-contained description of experiments, while the motivation for and conclusion from these experiments are murky."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3876/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700538974832,
                "cdate": 1700538974832,
                "tmdate": 1700538974832,
                "mdate": 1700538974832,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "lzzLnkW5Is",
            "forum": "IAkflJmNrC",
            "replyto": "IAkflJmNrC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3876/Reviewer_urPj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3876/Reviewer_urPj"
            ],
            "content": {
                "summary": {
                    "value": "This paper aimed to build models that retrieved similar sentences in both meaning and polarity. This paper addressed the problem of fine-tuning pre-trained sentence embedding models to a classification task while maintaining the ability to retrieve similar sentences. The authors presented two new metrics, the Polarity Score and the Semantic Similarity Score, to analyze the effect of fine-tuning using different contrastive-based losses. The authors argued that we can balance both STS (Semantic Similarity Score) and classification (Polarity Score) performances. To achieve this, the authors also proposed a new method to generate examples for fine-tuning. The example generations selected samples with similar meanings but opposite classification labels.\n\nIn the experiment, the author first established that the models achieved good scores for both semantic similarity and polarity without fine-tuning. The main results of the experiments showed that as we increased the number of samples, the model attained higher polarity scores, but the semantic similarity scores were slightly suffered. Upon further analysis, the authors suggested that the triplet loss performed best in achieving both scores and the margins (\\lambda) did not have a large effect on performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper proposed a new formation of the text retrieval task.\n2. This paper provided extensive experiment results on new retrieval metrics. The experiments included two datasets, and the results were consistent. It also confirmed previous findings that sentence-embedding models performed well in zero-shot classification and unsupervised retrieval."
                },
                "weaknesses": {
                    "value": "Overall, I found the paper interesting, but some concerns regarding the significance of the proposed task and the results.\n\n1. Although the paper proposed a new formulation of the text retrieval task, it did not mention why the task was important in the downstream applications or in further investigation of model capacity. Thus, the results of this paper provided limited implications. \n2. In the direction of the model investigation, the experiment setup was based on a single dataset, i.e., fine-tuning and retrieving the same dataset. Consequently, the author could only conclude that the proposed method retained sentence similarity performance within the fine-tuned dataset. It is unclear how this is better than the standard benchmarks that included zero-shot classifications and unsupervised sentence retrieval.\n3. Since the authors introduced the example generation approach, the results would be more meaningful if the authors compared with a baseline (original example generation.)\n4. The problem addressed in this paper is relevant to the \"forgetting effect\" and its solutions, such as Chen et al., 2020 or Luo et al., 2023. The author should discuss this in the related work.\n\n\n- Chen, S., Hou, Y., Cui, Y., Che, W., Liu, T., & Yu, X. (2020). Recall and learn: Fine-tuning deep pretrained language models with less forgetting. arXiv preprint arXiv:2004.12651.\n- Luo, Y., Yang, Z., Meng, F., Li, Y., Zhou, J., & Zhang, Y. (2023). An empirical study of catastrophic forgetting in large language models during continual fine-tuning. arXiv preprint arXiv:2308.08747."
                },
                "questions": {
                    "value": "1. Why did you train only 3 epochs to generate Figure 4?\n2. Figure 4 was quite insightful, did the \"max\" models remain consistent? \n3. Could you clarify why you did not test the fine-tuned models on STS using a standard benchmark?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3876/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698830895748,
            "cdate": 1698830895748,
            "tmdate": 1699636345893,
            "mdate": 1699636345893,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QchUMF3I4W",
                "forum": "IAkflJmNrC",
                "replyto": "lzzLnkW5Is",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3876/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3876/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Sorry for the late response here. \n\n## Weaknesses\n\n- W1/2. After the reviews came in, we attempted to make our statements more clear by its transferability to other applications, and did so using the established evaluations found in SentEval. We found our approach to outperform in all cases compared to e.g. SetFit on the same data with the same sample size. This should likely have been done beforehand, but we did not consider severely out-of-domain data as we were strictly focused on in-domain retrieval from an existing data source in the real world.\n\n- W3. This is something that went past us, and we have updated with the values from CosineSimilarity from the default SetFit configuration. Additional experiments show that all our model setups outperform SetFit by ~10% on average. \n\n- W4: Thanks for these resources. \n\n\n## Questions\n\n- Q1: I'm sure you mean 5. Similar work shows that fine-tuning of transformer-based embedding models rarely require more than 3 epochs. As few signs of overfitting occur, we upped it to 5. We observe a steady increase (but very minor) > 5 epochs, which resulted in 10 for the final experiments. \n\n- Q2: The max models is the ideal result, and is nearly achieved by the best configurations shown in the end of the article, in table 7/8. We opted for changing this figure to box plots of similarity and polarity, giving more insights into the deviation in metrics per sample size.\n\n- Q3: As our goal is not generalized STS, I agree that we should have performed tests on established benchmarks, if not only for the sake of comparison to approach similar scores as the baseline while increasing scores on retrieval-based tasks within the same domains. As explained in comments below, we ended up rerunning experiments with our models for the tasks provided by SentEval (https://github.com/facebookresearch/SentEval) on various problems. Results are consistently better than the pre-trained alternatives for similar tasks like opinionated texts and reviews when fine-tuned on SST-2, and also outperforms the alternative SetFit model."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3876/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700517519367,
                "cdate": 1700517519367,
                "tmdate": 1700517519367,
                "mdate": 1700517519367,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BWIeb0tG7c",
                "forum": "IAkflJmNrC",
                "replyto": "QchUMF3I4W",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3876/Reviewer_urPj"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3876/Reviewer_urPj"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response"
                    },
                    "comment": {
                        "value": "The new results made the experiment results stronger, still, the motivation and the conclusion could be made more meaningful."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3876/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700702515408,
                "cdate": 1700702515408,
                "tmdate": 1700702515408,
                "mdate": 1700702515408,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]