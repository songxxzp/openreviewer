[
    {
        "title": "Unleashing the Creative Mind: Language Model As Hierarchical Policy For Improved Exploration on Challenging Problem Solving"
    },
    {
        "review": {
            "id": "796BAKQjYB",
            "forum": "adQ2YC2IV7",
            "replyto": "adQ2YC2IV7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission453/Reviewer_ufPr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission453/Reviewer_ufPr"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel approach to enhance the problem-solving abilities of large language models (LLMs) by framing them as hierarchical policies. The approach consists of:\n\n* A high-level leader policy that generates diverse hints and tactics for exploration.\n* A low-level follower policy that uses the hints as guidance to execute detailed reasoning chains.\n* A tournament-based method to select the best reasoning chains and obtain the final answer.\n\nThe paper demonstrates that this approach improves the exploration of problem-solving strategies, the discovery and visibility of correct solutions, and the final answer accuracy on challenging mathematical reasoning tasks. The paper also provides a theoretical analysis and empirical evaluation of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper introduces a novel and general framework to enhance the problem-solving abilities of LLMs by using hierarchical policies. \n- The paper also proposes a new tournament-based method to select the best reasoning chains, which is inspired by human problem-solving behavior.\n- The paper also conducts extensive experiments on challenging mathematical reasoning tasks, demonstrating that the proposed method outperforms existing baselines and achieves state-of-the-art results.\n- The paper is well-written and organized, with clear definitions, notations, and algorithms."
                },
                "weaknesses": {
                    "value": "- Need ablation experiments to prove that the proposed tournament-based method is better than simple voting; \n- For mathematical problems, the current some work has used code interpreter to greatly improve the results, such as (53.9% \u2192 84.3%)[1], can the method in this paper be effective for this setting? From this perspective, the improvement brought by directly using LLM to output results in the paper is not significant, and code interpreter may be the key to solving math problems.\n\n[1] SOLVING CHALLENGING MATH WORD PROBLEMS USING GPT-4 CODE INTERPRETER WITH CODE-BASED SELF-VERIFICATION"
                },
                "questions": {
                    "value": "See weakness section"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission453/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698737406720,
            "cdate": 1698737406720,
            "tmdate": 1699635971786,
            "mdate": 1699635971786,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nUQ8DnXGUA",
                "forum": "adQ2YC2IV7",
                "replyto": "796BAKQjYB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission453/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission453/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal Response"
                    },
                    "comment": {
                        "value": "We sincerely thank you for your constructive feedback! We address the comments and questions below. \n\n> When we select the final answer among the groups of reasoning chains explored by our hierarchical policy, is our tournament-based selection approach better than direct majority voting?\n\nWe have added an experiment in our revised paper. We demonstrate that our tournament-based reasoning chain selection approach yields better final-answer accuracy over majority voting. Please see **Common Response** for more details.\n\n> Can our approach be effective under the code-interpreter setting?\n\nThanks for your suggestion and we have cited the paper. We\u2019d like to note that the focus of our paper is to introduce a visionary high-level leader that provides helpful hints to guide the low-level follower during problem solving. On the other hand, the code-interpreter line of works focus on integrating code interpreters into the detailed problem solving processes (which is the role of the low-level follower under our hierarchical policy framework), and these works have not delved into high-level strategies or thinking processes to our best knowledge. Therefore, our paper's focus is orthogonal to using code interpreter to improve challenging problem solving. In particular, code interpreter-enhanced low-level followers can be naturally integrated into our hierarchical policy framework.\n\nTo our best knowledge, the API for GPT-4 code interpreter was just released publicly last week for `gpt-4-1106-preview` [(see link here)](https://platform.openai.com/docs/assistants/tools/code-interpreter). We are actively working on implementations and experiments. We will try our best to add the results by the end of the rebuttal deadline. If we do not finish by then, we will add the results in final version."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission453/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700016695847,
                "cdate": 1700016695847,
                "tmdate": 1700016695847,
                "mdate": 1700016695847,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GE0qPh2Zd9",
                "forum": "adQ2YC2IV7",
                "replyto": "796BAKQjYB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission453/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission453/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Update Nov. 22**\n\n> Can our approach be effective under the code-interpreter setting?\n\nWe have obtained the results where `gpt-4-1106-preview` with code interpreter serves as the low-level follower policy in our approach along with the language model in the CoT + Sampling baseline. We adopt $n=4$ groups, each sampling $m=4$ reasoning chains. Results on MATH Level-5 are shown below. **We find that our hierarchical policy framework, combined with our tournament-based reasoning chain selection process, also yields better final answer accuracy under the code interpreter setting.**\n\n|Baseline CoT + Sampling (GPT-4-1106-preview + code) | Ours-Retrieval (GPT-4-1106-preview + code) w/o tournament | Ours-Retrieval (GPT-4-1106-preview + code) w/ tournament |\n|----|----|----|\n|64.52 | 66.19 | **66.43** |\n\n\n\nAs a reference, we report the MATH Level-5 performance of different language models below, where each model samples a single CoT reasoning chain per problem (without sampling multiple reasoning chains or performing majority voting like in our previous settings). We find that utilizing code interpreter yields about 10\\% final accuracy improvement for GPT-4 models, while it is not helpful for GPT-3.5.\n\n| Base Model |GPT-3.5-0613 | GPT-4-0613 | GPT-4-1106-preview |\n|----|----|----|----|\n|w/o code interpreter |14.29|23.57|42.86|\n|w/ code interpreter |12.14|33.57|52.14|"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission453/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700696057164,
                "cdate": 1700696057164,
                "tmdate": 1700696538124,
                "mdate": 1700696538124,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WlDYuJrR0G",
            "forum": "adQ2YC2IV7",
            "replyto": "adQ2YC2IV7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission453/Reviewer_Yers"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission453/Reviewer_Yers"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a hierarchical policy to help LLMs to tackle complex reasoning problems. This policy consists of (1) a high-level leader to explore solution direction and a low-level follower to generate a detailed solution, and (2) a tournament-based approach to select desired reasoning chains during exploration. All the modules are implemented by prompting large language models without additional model training. The results show that the hierarchical policy is able to achieve better accuracy on solving complex math question tasks compared to several SOTA approaches."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Experiment results have interesting findings that: when the number of reasoning chain samples increases, the increase of recall of correct solution and accuracy of the final answer is not aligned. This reveals the potential of LLMs to solve complex questions and can provide insights to other future researchers.\n\n- Results show that the hierarchical policy outperforms other prior approaches in solving complex math questions.\n\n- The paper is well-written. The problem is well-motivated by grounding on the prior work and easy to follow."
                },
                "weaknesses": {
                    "value": "- The evaluation datasets are not comprehensive enough. The authors only evaluate the approaches on a single dataset (the MATH dataset), and the relative evaluation size is small. Other math datasets (e.g., GSM8K, PRM800K) or other domain datasets in MMLU (e.g., Physics, Chemistry, etc) should be evaluated to demonstrate the generalization.\n\n- It is uncertain whether the improvement is from the hierarchical policy or the \"self-evaluation\" process when choosing the better reasoning chains. Previous research (e.g., \"Language Models (Mostly) Know What They Know\") suggests that LLMs like GPT-4 possess the ability to assess the likelihood that their output is correct."
                },
                "questions": {
                    "value": "- What's the motivation of the \"Grouped-Majority Recall\" metric? A more intuitive idea may be the percentage of questions whose ground truth answer exists in at least one of the answers.\n\n- In the \"tournament-based approach\", GPT-4 is used to select the better reasoning chain as the final solution. Because of the \"self-evaluation\" ability of LLMs, have you tried to use the majority vote strategy to obtain the final answer as an ablation experiment and compute the accuracy? In GPT-3.5 based approaches, is the \"tournament\" based on GPT-4 or GPT-3.5 (you mentioned that the GPT-4 is prompted to compare the current chains with (i+1)-th chain (Section 3) )?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission453/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698771806389,
            "cdate": 1698771806389,
            "tmdate": 1699635971723,
            "mdate": 1699635971723,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Z5cMsiH9wh",
                "forum": "adQ2YC2IV7",
                "replyto": "WlDYuJrR0G",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission453/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission453/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal Response [1/2]"
                    },
                    "comment": {
                        "value": "We sincerely thank you for your constructive feedback! We address the comments and questions below. \n\n> Evaluation over more datasets, like GSM8K and PRM800K.\n\nThanks for your suggestion! We have added additional experiments on GSM8K, and our approach continues to achieve better final-answer accuracy. Please refer to the **Common Response** for detailed results. \n\n(Note: The PRM800K also employs the MATH dataset, which is the same dataset our paper's experiments are based on.)\n\n> Is our improvement coming from the hierarchical policy framework or the \"self-evaluation\" process when choosing the better reasoning chains?\n\nThanks a lot for your suggestion. We have added two ablation experiments in our revised paper. In the first experiment, we investigate whether the baseline CoT + Sampling performance can be improved through our tournament-based evaluation process. Results are shown in Table A below. We find that adding our tournament selection process to the baseline does not ourperform our approaches that adopt the hierarchical-policy framework, demonstrating that our hierarchical policy plays a significant role in enhancing LLM's ability to solve challenging problems.\n\nIn the second experiment, we investigate the effect of majority voting vs. our tournament selection on the final answer accuracy of our hierarchical policy approaches. Results are shown in Table B below. We find that for our hierarchical policy approaches, adopting our tournament selection process yields better final answer accuracy than using majority voting, so our tournament selection process is also helpful. Intuitively, this is because not all of the tactics and hints produced by our high-level leader are helpful, and some of them might mislead the low-level follower, potentially causing it to generate a consistent but wrong answer under a misleading high-level guidance. By evaluating reasoning chains using our tournament-based selection approach, we effectively remove those that exhibit reasoning mistakes and keep those that are more trustful.\n\n**Table A: Effect of our tourament-based reasoning chain selection approach on the CoT + Sampling baseline. For the baseline, we randomly partition the 64 sampled reasoning chains per problem into $n=4$ groups, each having $m=16$ reasoning chains. We use GPT-3.5 as the language model for our low-level follower and the CoT Sampling Baseline.**\n| Method      | Baseline CoT + Majority Voting | Baseline CoT + Tournament  | Ours - Tactic (w/ Tournament)  | Ours - Retrieval (w/ Tournament) |\n|------------------|------|-------------|-------------|-------------|\n| Answer Accuracy  | 22.14 | 22.86   |**25.00**  | **27.85**   |\n\n\n**Table B: Effect of majority voting and our tournament-based reasoning chain selection on the final-answer accuracy of CoT + Sampling baseline and our hierarchical policy approaches. For \"Majority Voting\", we directly perform majority voting over the 64 sampled reasoning chains per problem. For \"Majority Voting over Groups\" and \"Tournament\", we adopt $n=4$ groups, each having $m=16$ reasoning chains. We use GPT-3.5 as the language model for our low-level follower and the CoT Sampling Baseline.**\n| Method |  Baseline CoT | Ours - Tactic | Ours - Retrieval |\n|------------------|------|-------------|-------------|\n| Majority Voting | 22.14 | 21.79 | 23.57 |\n| Majority Voting over Groups | 19.70 | 20.24 | 23.39 |\n| Tournament | 22.86   |**25.00**  | **27.85** |\n\n\n> What's the motivation of the \"Grouped-Majority Recall\" metric?\n\nThe motivation of our Grouped-Majority Recall metric is presented in the second and the fourth paragraph of Sec. 4.1, along with Figure 4. We have also made our motivation more clear in our revised paper.\n\nIn detail, the purpose of our metric is to assess the **\u201cvisibility\u201d** of the correct answers among solutions generated along the exploration process. A correct answer is \u201cvisible\u201d if it not only exists in at least one of reasoning chains, but also occupies a substantial proportion of them, even though it might not be the majority answer. *We find that standard metrics like accuracy and recall do not perfectly align with our goal.* This is because standard recall poorly correlates the prominence of correct answers, and standard accuracy does not reflect the scenario where LLM identifies correct solutions by a reasonable chance (and more than once), but the correct solutions are submerged during majority voting. On the other hand, our Grouped-Majority Recall metric allows correct answers that are identified by a reasonable chance but become obscured in majority voting to emerge, while ensuring that all correct answers recognized by our metric take up the majority in at least one reasoning group and are represented in more than one reasoning chains."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission453/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700016513547,
                "cdate": 1700016513547,
                "tmdate": 1700016560379,
                "mdate": 1700016560379,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "W3f3herU8i",
                "forum": "adQ2YC2IV7",
                "replyto": "WlDYuJrR0G",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission453/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission453/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal Response [2/2]"
                    },
                    "comment": {
                        "value": "> A more intuitive idea may be the percentage of questions whose ground truth answer exists in at least one of the answers.\n\nThe percentage of questions whose ground truth answer exists in at least one of the answers is the \"standard recall\" metric referred in the last comment. From our analysis, we find that the standard recall metric is not an ideal metric to assess the visibility of correct answers, because as the number of reasoning chain samples increases, the standard recall steadily improves, but the standard accuracy quickly plateaus after a few reasoning chain samples (as shown in Fig. 4a of the paper). Thus, the standard recall metric does not correlate well with the prominence of correct answers.\n\n> In GPT-3.5 based approaches, is the \"tournament\" based on GPT-4 or GPT-3.5 (you mentioned that the GPT-4 is prompted to compare the current chains with (i+1)-th chain (Section 3) )?\n\nIn our tournament-based reasoning chain selection process, we use GPT-4 to compare the current reasoning chain with the $(i+1)$-th chain, where the reasoning chains can be generated by either GPT-3.5 or GPT-4.\n\nWe further add an ablation study in Table 5 of the revised paper to compare using GPT-3.5 or GPT-4 to conduct our tournament-based reasoning chain selection process. We observe that GPT-4 demonstrates stronger performance as a reasoning chain selector than GPT-3.5 with a limited number of comparison repetitions (e.g., $k = 1$). Additionally, with only $k=1$ comparison repetition, GPT-4 already leads to a noteworthy enhancement of final answer accuracy over the CoT Sampling baseline."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission453/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700016571499,
                "cdate": 1700016571499,
                "tmdate": 1700016648787,
                "mdate": 1700016648787,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YS0AyCIG4N",
            "forum": "adQ2YC2IV7",
            "replyto": "adQ2YC2IV7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission453/Reviewer_qX4Z"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission453/Reviewer_qX4Z"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a new approach to improve the math reasoning capabilities of Large Language Models (LLMs) by framing them as hierarchical policies. The hierarchical policy consists of two parts: a \"visionary leader\" and a \"follower\". The leader suggests multiple high-level problem-solving tactics or hints, while the follower carries out detailed reasoning based on each of these high-level instructions. For each leader's directive, the follower samples multiple reasoning chains to create a group of potential solutions. To select the best solution from these groups, the authors introduce a tournament-based selection method. Experimental results on the MATH dataset show that this approach generates meaningful hints, and improves the accuracy of the final answer on challenging problems."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. I like the idea of generating hints first and then apply low level detailed reasoning. This strategy is intuitive and more like what we humans do in the real life. The authors also provide an effective way of sampling answers based on the hints and new strategy to select the best-of-n.\n2. The experiment results show that the proposed method are better than both CoT + self-consistency and ToT + self-consistency baselines."
                },
                "weaknesses": {
                    "value": "1. Generally I am not very sure about the novelty of the proposed method. I am mostly familiar w/ the math reasoning works but not familiar w/ the topic of LLM planning. The novelty may be a weakness; or may not --- I would like to refer to the opinions from other reviewers.\n2. Some descriptions of the method/experiment are confusing. Equation (1) and the relevant text is an example. The authors integrate w.r.t. $h$, so they treat $Pr(A|h,Q)$ as a probability density function so the integral $Pr(A|Q)$ should be a probability mass function. Yet the authors use the same notation $Pr$ which is quite confusing. More importantly, although we can understand what the authors would like to express after reading the whole section, the equation itself is invalid as $h$ is a discrete random variable rather than a continuous random variable so you cannot integrate w.r.t. $h$. Please note that mathematical notations in a paper is for helping readers to understand your idea more easily; but Equation (1) is not helping but instead making it even harder to understand. Actually, the key point of the section is just one sentence: \"More generally, our strategy samples all the different hints returned by $\\pi_{high}$ with equal probabilities.\". And this is already clear enough. Similar feelings also appear in the section introducing the \"Grouped-Majority Recall\" metric, where the authors use quite long paragraphs to explain the details of it but the organization is not quite good and thus make it not easy to get the motivation of proposing this new metric. We generally suggest the authors to improve the expression and organization in these sections for better readability.\n3. I may miss it but it seems there is no discussion about the accuracy/effectiveness of the tournament-based selection. IMO, it's non-trivial for LLM to accurately select the best answer among $n$ by iteratively comparing pair by pair. The authors do not provide the reason of introducing this new approach and why not use alternatives like another majority voting over the $n$ candidates. Also, it is possible to make a majority voting over the $n \\times m$ candidates directly; the authors do not demonstrate the necessity of the hierarchical selection strategy that majority votes within each group to get $n$ candidates first and select by tournament."
                },
                "questions": {
                    "value": "1. For retrieval-based hints generation, after you find similar examples in the training data, how do you identity the hints for these examples? The original MATH dataset doesn't contain such annotations. Do you generate the hints by LLM? If so, what's the accuracy of these hints?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission453/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698801737794,
            "cdate": 1698801737794,
            "tmdate": 1699635971640,
            "mdate": 1699635971640,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TKc0aArb9o",
                "forum": "adQ2YC2IV7",
                "replyto": "YS0AyCIG4N",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission453/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission453/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal Response"
                    },
                    "comment": {
                        "value": "We sincerely thank you for your constructive feedback! We address the comments and questions below. \n\n> Is our approach novel? I would like to refer to the opinions from other reviewers.\n\nWe'd like to kindly highlight the comments from two reviewers, `vZb3` and `ufPr`, both of whom acknowledge the novelty of our framework and approach.\n\nWe\u2019d also like to clarify our paper's contribution and novelty below:\n- Framing large language models (LLMs) as a hierarchical policy that encompass both \u201chigh-level\u201d and \u201clow-level\u201d cognitive processes, thereby allowing LLMs to effectively explore expansive solution spaces in complex problems. To our best knowledge, this line of work is underexplored in the field of LLM problem solving.\n- Proposing two effective approaches for the high-level leader policy to generate tactics and hints that guide the low-level follower policy. These approaches include generating concise and relevant techniques and concepts along with retrieving relevant problems.\n- Proposing a novel tournament-based approach to efficiently and effectively select the final reasoning chain.\n\n> Clarification of Equation 1\n\nThanks for your suggestion. The answer, hint, and question spaces are all discrete, so we have replaced the integral sign with the sum sign and changed our equation (1) to\n\n$\\mathrm{Pr}(A|Q) = \\sum_{h}\\mathrm{Pr}(A|h, Q) \\cdot \\text{Unif}(h\\in H)\\cdot \\mathrm{Pr}(H|Q)$\n\nin our updated paper.\n\n(As a side note, we originally used the integral sign as we were using a more general measure theory-style notation, under which one could use the integral sign to integrate over any measurable space, such as discrete spaces with counting measure and $\\mathbb{R}^n$ with Lebesgue measure. In our original Equation 1, the integral was with respect to the counting measure over our discrete hint space, i.e., $\\mathrm{Pr}(A|Q) = \\int_{h}\\mathrm{Pr}(A|h, Q) \\cdot \\text{Unif}(h\\in H)\\cdot \\mathrm{Pr}(H|Q) d\\mu(h)$ and $\\mu$ is the counting measure. In our original paper, we omitted $d\\mu(h)$ for brevity, which possibly caused the confusion.)\n\n> Core message of our \"Probabilistic Interpretation\" Section\n\nThanks for your suggestion. As suggested, we have highlighted the sentence `our strategy samples all the different hints returned by $\\pi_{high}$ with equal probabilities` in our updated paper.\n\nWe'd like to also clarify that the purpose of our \"Probabilistic Interpretation\" section is to provide an intuition of why our proposed hierarchical policy approach endows LLMs with better exploration ability in solving challenging reasoning problems, *compared to previous approaches that directly sampling multiple reasoning chains to explore the solution space*. Our analysis reveals that directly sampling reasoning chains is prone to stucking in a single or a few high-probability tactics, while our hierarchical approach allows tactics to be more uniformly sampled, which is beneficial for solving challenging math problems that require out-of-the-box thinking.\n\n\n> Improve the writing organization for our motivation of \"Grouped-Majority Recall\"\n\nThanks for your suggestion. We have revised our paper and added multiple boldface highlights to make the core messages clear when we describe the motivation of our Grouped-Majority Recall metric. \n\nIn detail, the goal of our metric is to assess the **\u201cvisiblity\u201d** of the correct answers among solutions generated along the exploration process. *A correct answer is \u201cvisible\u201d if it not only exists in at least one of reasoning chains, but also occupies a substantial proportion of them, even though it might not be the majority answer.* The second paragraph of Sec. 4.1, along with Figure 3, demonstrates why standard metrics like accuracy and recall are not suitable for our goal. The third paragraph of Sec. 4.1 explains the definition of our Grouped-Majority Recall metric. The fourth paragraph of Sec. 4.1 demonstrates why our Grouped-Majority Recall metric better measures the visibility of correct answers compared to the standard accuracy and recall metrics.\n\n\n> When we select the final answer among the groups of reasoning chains explored by our hierarchical policy, is our tournament-based selection approach better than direct majority voting?\n\nWe have added an experiment in our revised paper. We demonstrate that our tournament-based reasoning chain selection approach yields better final-answer accuracy over majority voting. Please see **Common Response** for more details.\n\n> Clarification on retrieval-based hint generation\n\nAfter the high-level leader retrieves a relevant problem along with its step-by-step solution, the problem and the solution directly serves as the hint for the low-level follower to carry out the problem-solving process (see Table 10 in Appendix for an illustration). There are no further prompting to generate (more concise and high-level) hints using the retrieved problem and solution."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission453/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700016426869,
                "cdate": 1700016426869,
                "tmdate": 1700018872155,
                "mdate": 1700018872155,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Heec8rLK1V",
                "forum": "adQ2YC2IV7",
                "replyto": "TKc0aArb9o",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission453/Reviewer_qX4Z"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission453/Reviewer_qX4Z"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your prompt response! The comment address most of my concerns in the initial review.\n\nTo authors:\nFeel free to comment again if you make further revision to the paper before rebuttal period ends and I would take a look.\n\nTo AC:\nMy impression of the paper is \"quite near the threshold yet towards rejection\" when I initially reviewed. After this round of clarification I feel like the paper gets better, but in general still quite near the threshold."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission453/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700083864568,
                "cdate": 1700083864568,
                "tmdate": 1700083864568,
                "mdate": 1700083864568,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "M2wf995LpE",
                "forum": "adQ2YC2IV7",
                "replyto": "YS0AyCIG4N",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission453/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission453/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer qX4Z,\n\nThanks for your feedback! Could you please suggest further revisions (if any) that would make our work more satisfactory to you and potentially improve our rating in your evaluation? Your guidance would be greatly appreciated."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission453/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700084665667,
                "cdate": 1700084665667,
                "tmdate": 1700084768041,
                "mdate": 1700084768041,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "n1UiGTHZ8H",
                "forum": "adQ2YC2IV7",
                "replyto": "M2wf995LpE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission453/Reviewer_qX4Z"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission453/Reviewer_qX4Z"
                ],
                "content": {
                    "comment": {
                        "value": "Hi authors,\n\nFor me, what makes me feel like this is a threshold work is mainly due to the innovation of the method/approach itself. The method is novel and shows improvement on certain tasks, but from my personal opinion is not enough to like \"wow this is really nice and decent\". For the revision period I encourage to better optimize the expression and try to better highlight the most important motivations and key contributions in your approach; currently it is not very clear when taking a first glance of paper and people have to carefully read line by line to fully understand your idea. A good presentation would catch readers' eyes at the first glance and lead the readers thorough the paper :) In general I think this is a not-so-bad paper and I may raise up to 6 --- but tbh I don't think 5/6 makes too much difference in the decision though. I would definitely try to do my best to provide a responsible opinion through the process; please just believe your effort would pay back as long as you work hard :)"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission453/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700085956442,
                "cdate": 1700085956442,
                "tmdate": 1700085956442,
                "mdate": 1700085956442,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2PbRZ1lnNe",
                "forum": "adQ2YC2IV7",
                "replyto": "YS0AyCIG4N",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission453/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission453/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your feedback! We've revised the introduction and methodology sections, using bold and italic formatting to emphasize our core motivations and key contributions more clearly and directly. We hope that combined with our revisions in the later experiment and analysis sections, we have enhanced the overall clarity and understanding for our readers. Should you have any further suggestions, please let us know."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission453/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700093481347,
                "cdate": 1700093481347,
                "tmdate": 1700093498382,
                "mdate": 1700093498382,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "hfVAZnMTfY",
            "forum": "adQ2YC2IV7",
            "replyto": "adQ2YC2IV7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission453/Reviewer_vZb3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission453/Reviewer_vZb3"
            ],
            "content": {
                "summary": {
                    "value": "The study presents an innovative framework that uses a hierarchical policy structure to improve problem-solving in Large Language Models (LLMs). A \"low-level follower\" executes details while a high-level \"visionary leader\" generates strategies. Using a tournament-based solution selection process, the authors' approach, which was evaluated on the MATH dataset, demonstrates improved strategy exploration and answer accuracy."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Innovative Framework: The hierarchical policy framework for problem-solving is a novel approach that capitalizes on the creative potential of LLMs.\n\n2. Sophisticated Selection Mechanism: The tournament-based selection process is a unique and strategic method to pinpoint the most effective solution, which mimics evolutionary selection processes to optimize problem-solving outcomes."
                },
                "weaknesses": {
                    "value": "1. Limited Dataset Representation: The study's findings are primarily based on the MATH dataset, which raises concerns about the model's performance on other datasets that present different challenges, such as the GSM8k. This limits the understanding of the model's adaptability and effectiveness across various types of reasoning tasks.\n\n2. Ambiguity in Model Details: The paper does not specify which versions of GPT-3.5 and GPT-4 were used, nor does it detail the hyper-parameters involved. Such information is critical for replicating the study.\n\n3. Cost Analysis Omission: There is no comprehensive analysis of the computational costs associated with different methods, including the number of tokens generated and encoded. Such an analysis is essential to evaluate the model's efficiency and practicality."
                },
                "questions": {
                    "value": "How does the model perform on other representative datasets like GSM8k, and can you provide comparative analysis to demonstrate its versatility across various domains?\n\nCould you specify the versions and hyper-parameters of GPT-3.5 and GPT-4 used in your experiments, and discuss how different configurations might affect the model's problem-solving capabilities?\n\nCan you provide a detailed cost analysis, including the number of generation tokens and encoding tokens required, to better understand the computational efficiency of your proposed method compared to traditional approaches?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission453/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission453/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission453/Reviewer_vZb3"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission453/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698843785491,
            "cdate": 1698843785491,
            "tmdate": 1699635971562,
            "mdate": 1699635971562,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4xuuiDMMnH",
                "forum": "adQ2YC2IV7",
                "replyto": "hfVAZnMTfY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission453/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission453/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal Response [1/2]"
                    },
                    "comment": {
                        "value": "We sincerely thank you for your constructive feedback! We address the comments and questions below. \n\n> Evaluation over GSM8k\n\nThanks for your suggestion! We have added additional experiments on GSM8K, and our approach continues to achieve better final-answer accuracy. Please refer to the **Common Response** for detailed results.\n\n> Version of GPT-3.5/GPT-4 along with their hyperparameters\n\nWe use GPT-3.5-turbo-0613 and GPT-4-0613 for all the experiments. For hyperparameter details, we set the decoding temperature to 0.3 for tournament-based reasoning chain selection and 0.7 otherwise (i.e., for hint generation from the high-level leader, reasoning chain generation from the low-level follower, along with the baselines). These hyperparameters were briefly mentioned in the first paragraph of our experiment setup. For clarity purposes, we have added an additional section (Appendix B in the updated PDF) to describe the GPT-3.5/4 settings used in the paper.\n\n\n> Effect of Different Model Configurations\n\nThanks for your suggestion. We have added an experiment in Table 5 and 6 of the revised paper, where we perform ablations on different models (GPT-3.5 / GPT-4), different comparison repetitions $k$, and different temperatures $T$ used to perform our tournament-based reasoning chain selection process (in our previous paper's experiments, we used GPT-4 model, $k=1$, and $T=0.3$ for tournament selection). We copy the results below for convenience. We find that GPT-4 demonstrates stronger performance as a reasoning chain selector than GPT-3.5 with a limited number of comparison repetitions (e.g., $k = 1$). In particular, with only $k=1$ comparison repetition, GPT-4 already leads to a noteworthy enhancement of final answer accuracy over the CoT Sampling baseline. Additionally, when $0<T<1$, different temperatures during tournament selection have little effect on the final answer accuracy.\n\n**Table 5: Ablation on using different models to conduct our tournament-based reasoning chain selection process, along with using different k, i.e., different numbers of comparison repetitions, during this process. We compare our retrieval-based method with the CoT Sampling baseline. We use GPT-3.5 as the language model for our low-level follower and the CoT Sampling Baseline.**\n| Method                | Ours - Retrieval | Ours - Retrieval | Ours - Retrieval | Ours - Retrieval | Ours - Retrieval | Ours - Retrieval | CoT Sampling + Voting |\n|-----------------------|------------------------|------------------------|------------------------|------------------------|------------------------|------------------------|-----------------------|\n| **Tournament Model**      | **GPT-3.5**                | **GPT-3.5**                | **GPT-3.5**                | **GPT-4**                  | **GPT-4**                  | **GPT-4**                  | **N/A**                   |\n| **#Comparisons**         | $k=1$                  | $k=3$                  | $k=5$                  | $k=1$                  | $k=3$                  | $k=5$                  | N/A                   |\n| Answer Accuracy       | 21.43                  | 22.86                  | **27.14**              | **27.85**              | **27.85**              | **27.85**              | 22.14                 |\n\n**Table 6: Ablation on using different temperature (T) in our tournament-based reasoning chain selection process. Results are obtained using GPT-4 as our tournament selection model with $k=1$ comparison repetition. We use GPT-3.5 as the language model for our low-level follower and the CoT Sampling Baseline.**\n| Temperature      | $T=0$  | $T=0.3$       | $T=0.7$       | $T=1.0$       |\n|------------------|------|-------------|-------------|-------------|\n| Answer Accuracy  | 26.43 | **27.85**   | 27.14   | 27.14   |"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission453/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700016177857,
                "cdate": 1700016177857,
                "tmdate": 1700016310879,
                "mdate": 1700016310879,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "31dRWy9vcA",
                "forum": "adQ2YC2IV7",
                "replyto": "hfVAZnMTfY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission453/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission453/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal Response [2/2]"
                    },
                    "comment": {
                        "value": "> Cost analysis comparison.\n\nThanks for your suggestion. We compare the cost between our approach and the CoT + Sampling Baseline below (unit in dollars; tournament performed with GPT-4; a total of 64 reasoning chains are generated per question). We also add this table in Appendix Section C. While our approach is slightly more expensive than the CoT baseline when GPT-3.5 serves as the low-level follower, it is more cost efficient than the baseline when GPT-4 serves as the follower, and moreover yields better final answer accuracy.\n\n\n| **Method** | CoT + Sampling Baseline | Ours - Tactics | Ours - Retrieval | CoT + Sampling Baseline | Ours - Tactics | Ours - Retrieval |\n|-------------------|---------------|-------------------------------|---------------------|-------------|----------------------------|-------------------|\n| **Low-Level Follower** | GPT-3.5 | GPT-3.5 | GPT-3.5 | GPT-4 | GPT-4 | GPT-4|\n| **Sampling Cost**      | 10.72           | 11.71            | 9.83                | 204.16        | 191.11         | 188.47           |\n| **Tournament Cost**    | None           | 2.42                          | 4.45                | None         | 4.52                       | 5.92             |\n| **Total Cost**         | 10.72           | 14.12            | 14.28               | 204.16        | 195.63         | 194.40           |\n\nAdditionally, we also compare the average number of input and output tokens between the baseline and our approach. The number of output tokens is calculated over the sum of 64 reasoning chains sampled per question. (For both GPT-3.5 and GPT-4, input tokens cost half as much as output tokens.)\n\n| **Method** | CoT + Sampling Baseline | Ours - Tactics | Ours - Retrieval | CoT + Sampling Baseline | Ours - Tactics | Ours - Retrieval |\n|--------------------|-----------------|------------------|---------------------|---------------|----------------|-------------------|\n| **Low-Level Follower** | GPT-3.5 | GPT-3.5 | GPT-3.5 | GPT-4 | GPT-4 | GPT-4|\n| Avg. # Input Tokens Per Question (including tournament in our approach) | 0.11K           | 0.73K            | 1.88K               | 0.11K         | 0.88K          | 2.00K      |\n| Avg. # Output Tokens Per Question (including tournament in our approach)| 38.20K          | 41.61K           | 34.32K              | 24.25K        | 22.85K         | 22.14K            |"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission453/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700016219205,
                "cdate": 1700016219205,
                "tmdate": 1700017781338,
                "mdate": 1700017781338,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]