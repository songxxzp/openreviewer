[
    {
        "title": "Confession Networks: Boosting Accuracy and Improving Confidence in Classification"
    },
    {
        "review": {
            "id": "WHHfGQFdT8",
            "forum": "NOz4YbdHl9",
            "replyto": "NOz4YbdHl9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3878/Reviewer_dbJp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3878/Reviewer_dbJp"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes adding one new node to the last layer of image classification models so that the new node can indicate the correctness of classifications. The proposed approach is also meant to improve the accuracy of the models. Some experiments are presented on the CIFAR datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The goals that the paper pursues are clearly defined: boosting accuracy, faster training, estimating the confidence of outputs, \u2026\n\nThe method itself is quite simple, adding a single node in the last fully connected layer of the network. This simplicity could be the strength of the method, if the method can achieve any of its goals/claims on standard models with high accuracy."
                },
                "weaknesses": {
                    "value": "The main weakness, in my view, is the reported results in the paper.\n\nThe accuracy of the original models that authors have trained (with and without their proposed method) is surprisingly low. For example, the accuracy of the ResNet-34 that authors have trained on CIFAR-10 is about 90.3% while the accuracy of pretrained models on Hugging Face and PyTorch libraries are between 95 to 98%. This indicates that authors are not using the best training methods available in the literature for ResNet and other models.\n\nUltimately, the paper\u2019s proposed loss function improves the accuracy of 90.3% to 91.7% which is still much lower than 95-98%. The accuracy of the ViT model trained on CIFAR-10 is already 98%. Why would anyone leave the pretrained model with accuracy of 98%, and use the authors\u2019 method to ultimately achieve an accuracy of 91.7%?\n\nSo, with the current experiments, there does not seem to be any advantage in using this paper\u2019s method. The training speed with the proposed loss function also does not show a significant improvement. This would change if authors demonstrate that their method can improve the accuracy and confidence of standard models with high accuracy.\n\nNote that the paper does not state that it is compromising accuracy for confidence. The claim in the paper is that it is boosting the accuracy of models as well as improving the confidence.\n\n------------\n\nIt is not clear to me what the horizontal axis represents in the plots in Figure 2. In the description of Figure 2, a threshold is mentioned, but it is not clear to me what that threshold is and how it is reflected in the plots.\n\nIt seems to me that Figure 2 compared the confidence node with the top softmax score of the same models. I think the comparison could be made with the softmax score of the original models.\n\nIn the context of Figure 2, authors only mention the true-positive rate. But that does not give the whole picture. One would also need to know the false negative rates, etc, for all the scenarios.\n\n-----------\n\nExperiments are limited to CIFAR-10 and CIFAR-100 datasets. This makes the results thin and not convincing. Experimenting on Imagenet has been a standard for a few years.\n\nWhile the paper mentions at the beginning that \u201cOne motivation for this work is in decision-critical settings\u201d, eventually its experiments are on CIFAR datasets.\n\n------------\n\nIn my view, there is no need to dedicate a paragraph to describe the CIFAR datasets. The audience of ICLR is well familiar with these datasets. It is also common practice to have the details about the models and datasets in the appendix."
                },
                "questions": {
                    "value": "Please see questions under weaknesses.\n\nDo authors think they can extend their experiments to Imagenet?\n\nCan authors perform their experiments on models with +95% accuracy on the CIFAR-10 dataset?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3878/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3878/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3878/Reviewer_dbJp"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3878/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697794888588,
            "cdate": 1697794888588,
            "tmdate": 1699636346393,
            "mdate": 1699636346393,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "uva2mKM3oN",
            "forum": "NOz4YbdHl9",
            "replyto": "NOz4YbdHl9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3878/Reviewer_nRM1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3878/Reviewer_nRM1"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a new method for neural networks to express their confidence in classification predictions using a novel loss function. The main goals are to design a loss function that provides a confidence measure alongside predictions and to assess its impact on network performance."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper raises a practical issue concerning neural networks, particularly when they are applied in domain-specific contexts. In many cases, the desired outcome goes beyond standard accuracy metrics. An adaptive framework that allows neural networks to optimise for multiple objectives becomes crucial in such scenarios. Exploring the idea of providing a level of confidence for predictions is indeed a valuable area to investigate."
                },
                "weaknesses": {
                    "value": "While it's a promising area for exploration, I noticed that the analysis of the newly introduced element (e.g., the confidence node) is somewhat lacking in depth. The authors might also consider comparing their approach with alternative strategies that could achieve similar results."
                },
                "questions": {
                    "value": "What measures can be taken to avoid overfitting of the confidence node during the training stage?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3878/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698462325122,
            "cdate": 1698462325122,
            "tmdate": 1699636346301,
            "mdate": 1699636346301,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "kwcwvhhdjf",
            "forum": "NOz4YbdHl9",
            "replyto": "NOz4YbdHl9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3878/Reviewer_8iyk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3878/Reviewer_8iyk"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes to predict the confidence of a neural network by adding a confidence node and changing the loss function a little bit."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Addresses an important problem, since neural networks are known to be very confident on data far away from the training examples."
                },
                "weaknesses": {
                    "value": "- The method is not described well, since the description has some problems. For example eq (1), which is supposed to describe the prediction of a neural network, is wrong since argmax over x in X belongs to X, so y hat belongs to X.\n- A confidence node will not work because because a MLP (multi-layer perceptron) with ReLU activation is actually a piecewise linear function, so the confidence output will be piecewise linear on the domain X. Some of the regions of linearity will be bounded but some will be infinite. On the infinite regions the output will take some very large values in some regions of the space X, far away from the training examples, so the confidence will be very high even though it should not be. This is independent of the loss function because through a loss function one will never be able to make the function constant on all the infinite regions.\n- Experiments are seriously lacking. First, experiments on CIFAR-10 and CIFAR-100 are not enough, experiments on other datasets are needed. Second, to show that the proposed confidence works, one should test it on data far away from the training examples, where it matters. This the problem of OOD (out of distribution) detection, which was slightly touched in the literature review. However, the authors fail to compare their method with any of the OOD detection algorithms from the literature."
                },
                "questions": {
                    "value": "- How does the proposed algorithm compare with XU et al 2022 for OOD detection with CIFAR-10 as in distribution (ID) and CIFAR-100, Imagenet or SVHN as OOD data? Similarly with CIFAR-100 as ID and CIFAR-10, Imagenet and SVHN as OOD? How does it compare with other recent OOD detection method on the same datasets?\n- How does the method scale to Imagenet as ID data and Food Network, iNaturalist as OOD data? See \"Openood: Benchmarking generalized out-of-distribution detection\" from NeurIPS 2022 for details."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3878/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698561496157,
            "cdate": 1698561496157,
            "tmdate": 1699636346208,
            "mdate": 1699636346208,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "Um1MQ5nF6m",
            "forum": "NOz4YbdHl9",
            "replyto": "NOz4YbdHl9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3878/Reviewer_TtcN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3878/Reviewer_TtcN"
            ],
            "content": {
                "summary": {
                    "value": "This manuscript presents an approach to estimate sample-based confidence score of prediction for multi-class classification setting. Method is based on max margin calibration, adapted to neural network loss function. Experimental evaluation is done using CIFAR-10 and CIFAR-100 image dataset outperforming model trained with standard loss function (considering classification accuracy and calibrated uncertainty)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Paper proposes a simple yet effective margin loss based classifier confidence calibration. It is easy to apply for existing classification models (but needs full training process). Promising results in image recognition domain is achieved compared to standard loss function and training, improving the accuracy and confidence quantification. There are some limitations in the experiments (see weaknesses)."
                },
                "weaknesses": {
                    "value": "The background and literature reviews lacks some of the related previous approaches of uncertainty and confidence quantification (e.g., Bayesian NNs, post-processing calibration methods, and out-of-distribution (OOD) type estimation). Although, experiments shows promising results, they are quite limited. There should be more detailed and comprehensive evaluation considering different DNN models and variety of datasets from different domain to support the findings and proposed approach better. Also, there is no any comparison to previous confidence calibration approaches."
                },
                "questions": {
                    "value": "- How the proposed approach compares to previous approaches theoretically and experimentally? E.g., different post-processing methods, or ones in OOD literature (see e.g., [1] for related approach)\n- What is causing the peak with the small positive rate in Figure 2, when utilising the confidence score?\n\n[1] DeVries et al. (2018) Learning Confidence for Out-of-Distribution Detection in Neural Networks, arXiv:1802.04865."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3878/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698759950897,
            "cdate": 1698759950897,
            "tmdate": 1699636346125,
            "mdate": 1699636346125,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]