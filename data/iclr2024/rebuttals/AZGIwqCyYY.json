[
    {
        "title": "Towards Cross Domain Generalization of Hamiltonian Representation via Meta Learning"
    },
    {
        "review": {
            "id": "aWxMIfTpWd",
            "forum": "AZGIwqCyYY",
            "replyto": "AZGIwqCyYY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9361/Reviewer_GtBu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9361/Reviewer_GtBu"
            ],
            "content": {
                "summary": {
                    "value": "This work leverages the power of meta-learning algorithms coupled with graph neural networks to find a shared representation of physical systems across various functional forms of Hamiltonian. In contrast to previous work, here the focus is on obtaining a representation which is valid across different physical systems. The performance of the framework is evaluated over a range of physical systems aiming to showcase its adaptivity to unseen settings."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "$\\underline{\\textrm{Originality}}$: the paper presents great originality in providing a Hamiltonian representation learning framework that can generalize across multiple system domains. This is in contrast to the common approach of providing system-specific models.\n\n\n$\\underline{\\textrm{Quality}}$: the paper is very well written. First, it provides the reader with all necessary background as well as motivation for the task addressed. Next, methods and results are well constructed. \n\n\n$\\underline{\\textrm{Clarity}}$:  the main ideas conveyed in this paper are clearly constructed and explained and supplementary information assists with providing further details and ablation studies. \n\n\n$\\underline{\\textrm{Significance}}$:  the main significance of the paper is in defining a new task, generalizing upon existing approaches, and suggesting to derive a framework that learns a representation that is not domain-specific."
                },
                "weaknesses": {
                    "value": "The paper presents an appealing goal, providing generalized Hamiltonian representations consistent across different physical domains. However, given the presented quantitative and qualitative results it is hard to judge the actual generalization and performance of the framework as detailed in the following points:\n\n1. The notion of $\\textit{generalization}$: ideally when discussing generalization in DL we would like to obtain a single pre-trained model which can then be used for diverse applications. With respect to the presented framework, this would suggest training the model(s) on a single task and then using the same network for prediction on all held-out systems. Similar to the setting presented in Ricci et al. (2023). However, here presented results always consider a single held-out-system. Providing an ablation over the number of systems used for training will allow for strengthening the claim of generalization and applicability for real-world applications.\n\n2. Baselines: it would be beneficial to extend the baselines presented in the paper in two directions:\n(i) optimal;  Training over the tested task, using all regimes (meta-, pre-, and vanilla HNN). This will allow a better assessment of the quality of the generalized model and (ii) within system generalization; following the background presented in section 2 it will be valuable to add a comparison to frameworks that are similar in nature and allow $\\textit{within}$ model generalization, e.g. CoDA (Kirchmeyer et al. 2022) or within the same functional form of the Hamiltonian, e.g. iMODE (Li et al. 2023). Here training over the same train-test splits."
                },
                "questions": {
                    "value": "1. Can the authors provide additional ablation studies following the weaknesses presented above? Specifically, it will be beneficial to present the performance as a function of the number of systems used in training (see weaknesses 1.) and add additional baselines  (see weaknesses 2.) \n2. Judging from the presented results the current framework is not suitable for larger systems, could the authors suggest possible extensions that may allow? What would be the necessary refinements that could be incorporated in the meta-learning configuration to allow for that?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9361/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698488282166,
            "cdate": 1698488282166,
            "tmdate": 1699637177328,
            "mdate": 1699637177328,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pewIAkorCM",
                "forum": "AZGIwqCyYY",
                "replyto": "aWxMIfTpWd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9361/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9361/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer GtBu"
                    },
                    "comment": {
                        "value": "Thank you for your efforts in providing insightful feedback on our work. We appreciate the reviewer's recognition of the originality of our work compared to other related works. We have thoroughly examined the weaknesses and questions and revised our manuscript as follows (we also recommend the reviewer check the overall response).\n\n**_List of changes in the manuscript_**:\n> 1. Section 4.4 and SM C.3 are revised to add existing baseline results according to Q1\n\n**Q1** Can the authors provide additional ablation studies following the weaknesses presented above? Specifically, it will be beneficial to present the performance as a function of the number of systems used in training (see weaknesses 1.) and add additional baselines (see weaknesses 2.)\n \n> **A1**\n> We appreciate the reviewer for suggesting additional experiments to strengthen our paper.\n> Regarding weakness 1, it's important to clarify that our primary objective was to explore the generalization of Hamiltonian dynamics for predicting a single, unknown held-out system, underpinned by ample data. While the experiment proposed by the reviewer is indeed beneficial, it aligns more closely with further challenging scenarios involving adaptation to multiple unknown systems. Such situations extend beyond the current scope of our research, which focuses on a singular system context.\n> For weakness 2, we originally thought that it was hard to directly compare our method with other prior baselines as the definition of the scope of generalization is different. However, we agree that adding such baselines would be beneficial. As such, we choose CoDA ([Kerchmeyer et al. 2022](https://arxiv.org/abs/2202.01889)) as a representative method for domain generalization in learning the physical dynamics (as it is reported to be superior among other related methods according to the CoDA authors). As expected, CoDA failed to adapt to an unseen system with the same number of steps, even for sufficient steps up to 10000. We have revised Section 4.4 and SM C.3 of our manuscript with the CoDA baseline results and included the corresponding discussion.\n\n\n**Q2** Judging from the presented results the current framework is not suitable for larger systems, could the authors suggest possible extensions that may allow? What would be the necessary refinements that could be incorporated in the meta-learning configuration to allow for that?\n \n> **A2**\n> Although the results for the large systems are quite poor in the reported results, we want to mention that our approach can handle the problem when the degree of freedom of the prepared data is not consistent throughout the system, noting that the previous methods discussed in Section 2.2 and 2.3 cannot be used in such scenarios. Although we cannot give the exact refinements in our preliminary results, possible ways could be 1) tune the update method in meta-learning, e.g. [Lee et al. 2021](https://arxiv.org/abs/2102.11544) report that using ANIL variation gives more accuracy compared to vanilla MAML, and 2) strictly search the hyperparameter space."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9361/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700561343225,
                "cdate": 1700561343225,
                "tmdate": 1700561343225,
                "mdate": 1700561343225,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MPpZWmQj63",
                "forum": "AZGIwqCyYY",
                "replyto": "pewIAkorCM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9361/Reviewer_GtBu"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9361/Reviewer_GtBu"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the detailed response; while some of my concerns have been addressed others remain hence i choose to keep my score (mainly, while not set as a primary objective, i believe the _generalization_ notion stands as a big weakness)."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9361/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700575480177,
                "cdate": 1700575480177,
                "tmdate": 1700575480177,
                "mdate": 1700575480177,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JBCNmwqx5E",
                "forum": "AZGIwqCyYY",
                "replyto": "aWxMIfTpWd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9361/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9361/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We acknowledge the reviewer's concern regarding the generalization aspect of our study. The absence of the suggested ablation experiment in our work stemmed from our assessment that the number of systems (1 to 3) required for such an experiment would not be adequate for the adaptation task at hand. Instead, to evaluate such generalization, we directly tested the meta-trained model's performance on multiple held-out systems. Following the reviewer's suggestions, we extended our analysis to include the 2D harmonic oscillator system, utilizing the same meta/pre-trained models from our initial experiments. This approach aimed to assess the model's capability in generalizing across multiple held-out systems. The results showed that the meta-trained model significantly outperformed the baselines in terms of both coordinate prediction and energy estimation, thus reinforcing its generalization ability on a broader range of systems. We have updated Section 4.4 and SM C.3 in our manuscript to incorporate these findings and the corresponding discussions. We hope that the additional response will address the concerns raised and appreciate the opportunity to clarify these aspects within the limited discussion period."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9361/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700706506375,
                "cdate": 1700706506375,
                "tmdate": 1700711723574,
                "mdate": 1700711723574,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "MPDVvgTzbR",
            "forum": "AZGIwqCyYY",
            "replyto": "AZGIwqCyYY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9361/Reviewer_xLig"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9361/Reviewer_xLig"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a novel meta-learning method aiming to learn a unified Hamiltonian representation such that it can be generalized to unseen physical systems. Hamiltonian Neural Networks were utilized as the backbone of the method for learning the unified Hamiltonian representations of different physical systems, via meta-learning pipelines of a variation of MAML. Experiments demonstrated the proposed methods achieved lower relative error of trajectories and energy when adapted to different systems."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "(1) Unlike many existing works that focus on learning system dynamics under similar\u00a0physical law, the proposed methods aim to learn the unified representations across diverse system domains via meta-learning the\u00a0Hamiltonian of the given system. This sounds\u00a0significant\u00a0and promising.\n\n(2) Both quantitative and qualitative results demonstrated the proposed method achieved\u00a0better adaptation\u00a0to various new systems compared with baselines."
                },
                "weaknesses": {
                    "value": "The evaluation can be strengthened\u00a0by considering comparing the proposed methods with other\u00a0Few-shot Learning and\u00a0Physics-informed Neural Networks methods for\u00a0system domain generalization under both \"consistent\" and \"different\" physical laws."
                },
                "questions": {
                    "value": "In Figure 4, why does a lower CKA value of the meta-trained model suggest it learned more similar representations during adaptation? As the authors mentioned, should a low CKA value indicate different representations?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9361/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9361/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9361/Reviewer_xLig"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9361/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698725487783,
            "cdate": 1698725487783,
            "tmdate": 1699637177215,
            "mdate": 1699637177215,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "adDOP3DMul",
                "forum": "AZGIwqCyYY",
                "replyto": "MPDVvgTzbR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9361/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9361/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xLig"
                    },
                    "comment": {
                        "value": "We appreciate your thoughtful effort in reviewing this work. We are delighted to hear that the reviewer has recognized the contribution of our work in contrast to many existing works. We have carefully considered the points raised by the reviewer and have revised our manuscript as follows (we also recommend that the reviewer check the overall response).\n\n**_List of changes in the manuscript_**:\n> 1. Section 4.4 and SM C.3 are revised to add existing baseline results according to W1\n\n**W1** The evaluation can be strengthened by considering comparing the proposed methods with other Few-shot Learning and Physics-informed Neural Networks methods for system domain generalization under both \"consistent\" and \"different\" physical laws.\n\n> **A1**\n> We first thought that it would be difficult to directly compare our method with other previous baselines, as the definition of the scope of generalization is different. However, we strongly agree with the reviewer that our evaluation can be strengthened by comparing the proposed approach with other existing methods. We added a baseline experiment with CoDA ([Kerchmeyer et al. 2022](https://arxiv.org/abs/2202.01889)) which is considered a representative method for domain generalization in learning physical dynamics (as it is reported to be superior to other related methods according to the CoDA authors). As expected, CoDA failed to adapt to an unseen system with the same number of steps, even for sufficient steps up to 10000. We have revised section 4.4 of our manuscript with the CoDA baseline results and discussion.\n\n**Q1** In Figure 4, why does a lower CKA value of the meta-trained model suggest it learned more similar representations during adaptation? As the authors mentioned, should a low CKA value indicate different representations?\n\n> **A1**\n> We acknowledge that our label notation in Figure 4 could be quite misleading. The y-axis represents the 1-CKA (1 minus CKA) value, not the raw CKA value. Therefore, the lower y-axis in Figure 4 indicates a high CKA value, which leads to more similar representations during adaptation."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9361/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700561314186,
                "cdate": 1700561314186,
                "tmdate": 1700561314186,
                "mdate": 1700561314186,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KfF0ZP3s6z",
                "forum": "AZGIwqCyYY",
                "replyto": "adDOP3DMul",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9361/Reviewer_xLig"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9361/Reviewer_xLig"
                ],
                "content": {
                    "comment": {
                        "value": "Thank the authors for their clarification and the additional experiments. I will maintain my score."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9361/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700675214018,
                "cdate": 1700675214018,
                "tmdate": 1700675214018,
                "mdate": 1700675214018,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FBXRP3lnzq",
            "forum": "AZGIwqCyYY",
            "replyto": "AZGIwqCyYY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9361/Reviewer_jozM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9361/Reviewer_jozM"
            ],
            "content": {
                "summary": {
                    "value": "The paper reports the performance of MAML applied to domain generalization over different Hamiltonian dynamics. The performance gain by MAML is analyzed with multiple indices over different combinations of meta-training and meta-test data. The key findings include the superiority of the meta-trained models compared to the pre-trained and randomly initialized models and an implication that by meta-learning, the representations obtained by the models tend to be more specific to each system."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The experiments clearly show the superiority of meta-learning, at least within the limited number of Hamiltonian systems. \n\nThe paper is well written. The motivation, the method, and the experimental results are very clearly reported."
                },
                "weaknesses": {
                    "value": "The limitation of meta-learning for (Hamiltonian) dynamics is not clearly investigated. This makes it difficult to assess the range where the claims made in the paper should be valid. In other words, the claims are rather weak because their applicability seems unbounded with the current set of experiments. When the meta-learning approaches for dynamics may not be beneficial? For example, what happens if you meta-train a model only with conservative systems and try to adapt it to a dissipative system? Such experiments to investigate the limitations of the empirical findings would strengthen the paper.\n\nThe paper only reports the performance of a well-known method (MAML) merely applied to a particular setting. This could certainly be a kind of contribution in which ICLR audience may be interested, but I think that in such a paper, with a purely experimental point of view, the claims should be made more carefully. Specifically, as stated above, the cases where meta-learning is not necessarily beneficial should also be revealed, with which the claims would become more falsifiable and convincing."
                },
                "questions": {
                    "value": "I don't have particular questions. It would be great if the authors could additionally report the results of some experiments to investigate the limitation of meta-learning in this context, although this is not a question."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9361/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9361/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9361/Reviewer_jozM"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9361/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698757708558,
            "cdate": 1698757708558,
            "tmdate": 1700573192364,
            "mdate": 1700573192364,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XGHhohTLzv",
                "forum": "AZGIwqCyYY",
                "replyto": "FBXRP3lnzq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9361/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9361/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer jozM"
                    },
                    "comment": {
                        "value": "Thank you for your efforts in providing valuable comments on our work. We appreciate the reviewer's comment about the points that should be further considered from the aspect of our experimental point of view. We have thoroughly considered the concerns raised by the reviewer and revised our manuscript as follows (we also recommend the reviewer check the global response).\n\n\n**_List of changes in the manuscript_**:\n> 1. Section 4.5 and SM C.4 are revised to add additional experiments according to W1\n\n**W1** I don't have particular questions. It would be great if the authors could additionally report the results of some experiments to investigate the limitation of meta-learning in this context, although this is not a question.\n\n> **A1**\n> It should be noted that, in an attempt to provide further insight beyond the experimental results for the generalized model, we have provided the CKA analysis of the learned representations for each of the models. This was also the reason why we only compared the meta-trained model with the pre-trained and randomly initialized model, and not with the existing baselines in the first place. Nevertheless, we strongly agree with the reviewer that our results and claims should be made more carefully from our paper\u2019s experimental point of view, and we appreciate the suggestion to make our claims more thorough by exploring the limitations of meta-learning for learning a unified representation of Hamiltonian dynamics.\nConsequently, we consider the example suggested by the reviewer (meta-training the model with conservative systems and adapting it to a dissipative system), which we believe is a good way to explore this. As expected, the meta-trained model (as well as the pre-trained and randomly initialized model) failed to adapt to a dissipative system, showing a clear limitation of our approach. We discuss the reason why, for the case of a dissipative system, Hamilton's equation (Equation 1 in the manuscript) does not hold, and thus the HNN loss (Equation 2 in the manuscript) is not appropriate for dissipative systems. We argue that to make use of our method, both the system in the data distribution for meta-training and adaptation should share the same nature so that both satisfy the given loss of the implementation. We have added the corresponding experimental results and discussions in Section 4.5 and SM C.4."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9361/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700561292662,
                "cdate": 1700561292662,
                "tmdate": 1700561292662,
                "mdate": 1700561292662,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GnwCeYR8JO",
                "forum": "AZGIwqCyYY",
                "replyto": "XGHhohTLzv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9361/Reviewer_jozM"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9361/Reviewer_jozM"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response. The revised manuscript is clearer in terms of the specific contribution, and the limitation shown in the new experiments is convincing. I modified my evaluation accordingly."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9361/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700573339534,
                "cdate": 1700573339534,
                "tmdate": 1700573339534,
                "mdate": 1700573339534,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ux7s4C5vaw",
            "forum": "AZGIwqCyYY",
            "replyto": "AZGIwqCyYY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9361/Reviewer_hDu2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9361/Reviewer_hDu2"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to use meta-learning to learn generalized representations across different types of dynamical systems. The meta-learning step helps improve the adaptation to unknown systems with fewer data points (compared to randomly initialized and pre-trained baselines) by virtue of generalized representations. The authors also analyze the representations learned by different baselines and meta-learning by using centered kernel alignment (CKA) to gain insights into better performance by the meta-learning model."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is easy to follow and the motivation to learn a generalized model is clear. \n- The experiments are performed with different numbers of data-points for the adaptation task to evaluate the robustness of the approach. \n- The analysis using CKA gives further insight into how the meta-learning model learns closer representations of the adaptation task. \n- The implementation and task curation are described in detail for reproducibility."
                },
                "weaknesses": {
                    "value": "- The main contribution of the paper seems to be utilizing meta-learning to efficiently adapt to new systems. However, it is not clear from the paper if it is as simple as just using off-the-shelf implementation or if there are some challenges to doing this. \n- Also, I would like to see some discussion around why meta-learning is preferred over other representation learning methods e.g. Domain Generalization, and why optimization-based methods surpass other approaches in meta-learning.\n- The experiments are not sufficient. First, there is no comparison with existing baseline models in domain generalization or meta-learning. Second, the comparison of the meta-model and pre-trained model is not fair, and I would suggest the author fine-tune the pre-trained model on the K-shot support set. Third, no visual comparison of the predicted dynamics and the ground truth, making the conclusion less convincing."
                },
                "questions": {
                    "value": "Please check the weaknesses above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9361/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9361/Reviewer_hDu2",
                        "ICLR.cc/2024/Conference/Submission9361/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9361/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698761034922,
            "cdate": 1698761034922,
            "tmdate": 1700712736967,
            "mdate": 1700712736967,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5R0lr0VCbl",
                "forum": "AZGIwqCyYY",
                "replyto": "ux7s4C5vaw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9361/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9361/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer hDu2"
                    },
                    "comment": {
                        "value": "We sincerely appreciate your efforts in reviewing our paper. We are pleased that the reviewer noticed the strengths regarding the CKA analysis of the learned representations. We have carefully reviewed the weaknesses pointed out by the reviewer and have revised our manuscript as follows (we also recommend the reviewer review the overall response).\n\n**_List of changes in the manuscript_**:\n> 1. Section 2.3 is revised to add clarity to the paper based on W1, and W2.\n> 2. Section 4.4 and SM C.3 are revised to add existing baseline results following W3.\n> 3. The titles of Sections 4.1, and 4.2 and the caption of Fig. 3 are updated based on W3.\n\n**W1** The main contribution of the paper seems to be utilizing meta-learning to efficiently adapt to new systems. However, it is not clear from the paper if it is as simple as just using off-the-shelf implementation or if there are some challenges to doing this.\n\n**W2** Also, I would like to see some discussion around why meta-learning is preferred over other representation learning methods e.g. Domain Generalization, and why optimization-based methods surpass other approaches in meta-learning.\n \n> **A1, A2**\n> We presume that weaknesses 1 and 2 arise from the lack of clarity that we have written in the paper. Existing previous methods have clear limitations; 1) they do not guarantee generalization across diverse systems, and 2) to do so, they need to be flexible with respect to the varying degrees of freedom of the systems. Our approach aims at resolving these challenges by jointly using GNNs and meta-learning. Accordingly, we have revised section 2.3 to clearly state the points mentioned by the reviewer in Q1 and Q2.\n\n**W3** The experiments are not sufficient. First, there is no comparison with existing baseline models in domain generalization or meta-learning. Second, the comparison of the meta-model and pre-trained model is not fair, and I would suggest the author fine-tune the pre-trained model on the K-shot support set. Third, no visual comparison of the predicted dynamics and the ground truth, making the conclusion less convincing.\n\n> **A3**\n> - First, we acknowledge the insufficient comparison with existing baselines. Originally, we considered it to be hard to directly compare our method with other prior baselines due to the different definitions of the scope of generalization, we added a baseline experiment with CoDA ([Kerchmeyer et al. 2022](https://arxiv.org/abs/2202.01889)) which is considered a representative method for domain generalization in learning physical dynamics (as it is reported to be superior among other related methods according to the CoDA authors). As expected, CoDA failed to adapt to an unseen system with the same number of steps, even for sufficient steps up to 10000. We have revised Section 4.4 and SM C.3 of our manuscript with the CoDA baseline results and discussion.\n> - Second, while we appreciate the reviewer's perspective, our work is at the cornerstone in our scope of the problem definition, and thus to show both the possibility and the effectiveness of the unified representation of Hamiltonian dynamics, we chose as baselines the 1) pre-trained model that used the same total number of gradient steps during meta-training, and the 2) randomly initialized model, following the approach of the prior work of [Finn et al. 2017](https://arxiv.org/abs/1703.03400) and [Lee et al. 2021](https://arxiv.org/abs/2102.11544).\n> - Third, we respectfully ask the reviewer to check Section 4.2, where the dynamics of the predicted systems are discussed using Figure 3. However, we acknowledge that the presentation of the predicted dynamics written in the paper should be more explicit and accordingly we have updated the title of Sections 4.1, 4.2 and the caption of Figure 3."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9361/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700561234881,
                "cdate": 1700561234881,
                "tmdate": 1700561234881,
                "mdate": 1700561234881,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AkWTnzZetg",
                "forum": "AZGIwqCyYY",
                "replyto": "5R0lr0VCbl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9361/Reviewer_hDu2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9361/Reviewer_hDu2"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your detailed response. Although some of my concerns have been addressed, there are still major questions remaining to be answered:\n1. The description of the methodology is not clear. Both the proposed work and CoDA are based on optimization-based meta-learning and one of the key differences is the use of graphs. However, the details of what the vertices and edges in the graph represent and how the graph is constructed are missing. Also, it would be good to have an ablation study on the proposed method without graph representation.\n2. There are lots of non-MAML-based meta-learning models working on physics dynamics and they deserve to be compared. For instance, DyAd [1] is a model-based meta-learning with weak supervision for turbulence flow on heterogeneous domains; Sequential Neural Processes [2] has a similar see,tting of generalizing on sequential dynamics; meta-SLVM [3] addresses the adaptation across dynamics via Bayesian meta-learning and has experimented on different physics model. These works should be mentioned as related works and compared to demonstrate how the proposed work improves the generalization across different dynamics.\n\nI choose to keep my score unless the concerns above are addressed.\n\n[1] Wang et al, Meta-learning dynamics forecasting using task inference, 2022 \n[2] Singh et al, Sequential neural processes, 2019\n[3] Jiang et al, Sequential Latent Variable Models for Few-Shot High-Dimensional Time-Series Forecasting 2023"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9361/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700593787775,
                "cdate": 1700593787775,
                "tmdate": 1700593787775,
                "mdate": 1700593787775,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OJGNMZPaZB",
                "forum": "AZGIwqCyYY",
                "replyto": "ux7s4C5vaw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9361/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9361/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We acknowledge your detailed comment regarding our response. Below we provide a response for the reviewer\u2019s additional comments.\n\n1. As the reviewer pointed out, one of the key differences between our proposed work and CoDA is the usage of graphs. We would like to add one more major difference as follows. According to the CoDA paper, CoDA is based on the assumption that the *environment* $e \\in \\mathcal{E}$ is from a specific dynamics $f$ (where $\\frac{dx(t)}{dt} = f(x(t))$) such that the functional form of the dynamics set from the training data and the dynamics set of the unseen data should be the same. In other words, the context of environment-specific parameters is the same across the environments. This way of formulation can also be found for DyAd [1], and meta-SLVM [3]. However, in our setting, we do not impose such assumptions such that the term *environment* (which was a generalization within a fixed dynamics form) can be extended in the context of the term *domain* (now a generalization across different dynamics) we used in our manuscript. \n    And for the missing details about how the graphs represent the system, we followed the settings from [Sanchez-Gonzalez et al. 2019](https://arxiv.org/abs/1909.12790), and [Bishonoi et al. 2023](https://openreview.net/forum?id=Ugl-B_at5n) such that the nodes represent the state $(\\vec{q}, \\vec{p})$, while the edge features are not used for now. We revised Section 3.2 accordingly.\n2. We acknowledge the reviewer's observation regarding potential misunderstandings on our part. We focused on reporting baseline results for DyAd [1] within a specific system context. Our additional evaluation included testing the magnetic-mirror system adaptation task using a meta-trained model, which encompassed systems of mass-spring, pendulum, and H\u00e9non-Heiles. Notably, DyAd showed limited adaptation capabilities in this context, struggling significantly with the magnetic-mirror system.\n    This observation leads us to a critical point: methods such as CoDA, DyAd, and meta-SLVM, which aim to generalize within a fixed functional form of dynamics, may not be directly applicable to our concept of domain generalization (as we have previously discussed in the above-numbered point 1.). To align DyAd more closely with our generalization scope, we made a subtle modification to the parameter $c$ in the encoder loss's first term (weak supervision term), representing it as the Hamiltonian of the system. While it may be hard to argue that the explicit form of Hamiltonian is not shared across the system, we viewed this as a minimal form of weak supervision, leveraging the universal concept of energy in dynamical systems.\n    And for the meta-SLVM, although it adapted well to several physical scenarios (namely bouncing balls under 4 gravity, pendulums, and mass springs with each four different physical constants), their adaptation was tested under the same type of physics; among the four dynamics for each physical scenarios, three were used for meta training, and the remaining one was used for meta testing.\n    Therefore, despite the inherent limitations of DyAd and similar methods in this context, we posit that our approach, leveraging a vanilla MAML-based method, demonstrates superior adaptability and effectiveness compared to these baselines. This distinction underscores the rationale behind our methodological choices and the results we observed. Accordingly, we revised Section 2.3 with the discussions and Section 4.4 and SM C.5 with DyAd results. We hope that the additional response will address the concerns raised and appreciate the opportunity to clarify these aspects within the limited discussion period."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9361/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700706446967,
                "cdate": 1700706446967,
                "tmdate": 1700711694011,
                "mdate": 1700711694011,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3WOQQSW89n",
                "forum": "AZGIwqCyYY",
                "replyto": "OJGNMZPaZB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9361/Reviewer_hDu2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9361/Reviewer_hDu2"
                ],
                "content": {
                    "comment": {
                        "value": "Thank the authors for the clarification and additional experiments. Below are some suggestions that may improve the presentation of the work based on the discussion above:\n1. The authors could highlight the method is able to process unseen domains and add the details of the graph to the main text. The comparison of the proposed work and existing meta-learning works, including the non-MAML-based works, could be discussed in the related works.\n2. I appreciate the effort of adding experiments in such limited discussion time, although I still encourage the authors to make more effort on the non-MAML-based works, including the reasons of their inability to adapt to unseen domains and whether the proposed models can be applied to these frameworks.\n\nI modified my evaluation accordingly."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9361/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700712709565,
                "cdate": 1700712709565,
                "tmdate": 1700712709565,
                "mdate": 1700712709565,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]