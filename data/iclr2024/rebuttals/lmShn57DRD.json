[
    {
        "title": "Connecting the Patches: Multivariate Long-term Forecasting using Graph and Recurrent Neural Network"
    },
    {
        "review": {
            "id": "Y1ElQ1o5hQ",
            "forum": "lmShn57DRD",
            "replyto": "lmShn57DRD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1208/Reviewer_qMLP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1208/Reviewer_qMLP"
            ],
            "content": {
                "summary": {
                    "value": "This paper delves into the challenges presented by multivariate long-term time series forecasting (MLTSF), specifically the difficulty of capturing cross-channel dependencies and temporal order information using current Transformer-based models. Despite the achievements of Transformer models in various fields, their application in MLTSF reveals certain inadequacies. Models like Informer, Autoformer, and FEDformer, while advanced, still face challenges in understanding intricate channel relationships in multivariate time series. \n\nTo address these issues, the authors propose the GRformer model. This innovative solution combines the strengths of Graph Neural Networks (GNN) and position encoding derived from Recurrent Neural Networks (RNN). The inclusion of a mix-hop propagation layer within a feedforward neural network promotes efficient interaction between different time series data points. Additionally, by leveraging a multi-layer RNN, the model recursively generates positional embeddings, emphasizing the importance of sequence order. \n\nThe paper's empirical tests, conducted on eight real-world datasets, demonstrate the GRformer's superior predictive accuracy in MLTSF tasks, underlining its potential as a novel solution in the field of time series forecasting."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "**Strengths**:\n\n1. **Originality**: \n   - The GRformer presents a unique fusion of GNN and RNN-based position encoding within a Transformer framework, addressing gaps in MLTSF.\n   - The incorporation of the Pearson correlation coefficient for graph structure is a notable innovation.\n\n2. **Quality**: \n   - Rigorous empirical validation is conducted on eight real-world datasets, ensuring robustness.\n   - The model's design is comprehensive, with the mix-hop propagation layer and RNN-based position encoding as highlights.\n\n3. **Clarity**: \n   - The paper delineates complex concepts coherently, facilitating reader understanding.\n   - Distinctive features and advantages of GRformer over existing models are clearly articulated.\n\n4. **Significance**: \n   - The GRformer's advancements in capturing cross-channel dependencies have potential broad impacts in time series forecasting.\n   - The paper paves the way for future research by highlighting existing challenges and areas of improvement.\n\nIn essence, the paper excels in its innovative methodology, thorough validation, lucid presentation, and relevance in the field."
                },
                "weaknesses": {
                    "value": "1. **Mathematical Notation Consistency**:\n   - The authors' use of mathematical notation appears inconsistent. For instance, function names should ideally be presented in regular typeface rather than italic. Proper notation ensures clarity and avoids potential confusion.\n\n2. **Graph Construction Using Pearson Coefficient**:\n   - While the authors opted for the Pearson correlation coefficient for graph construction, which subsequently serves as the foundational structure for the GNN, one might question the exclusion of making GNN parameters learnable. This adaptability could potentially offer more flexibility to the model.\n\n3. **Assumption of Homoscedasticity**:\n   - The Pearson coefficient assumes homoscedasticity in the data. It's unclear if the authors verified this assumption across their datasets. Such checks are crucial to ensure the validity of the chosen coefficient.\n\n4. **Alternative Correlation Metrics**:\n   - The paper doesn't seem to explore or discuss other potentially beneficial correlation coefficients like Time-Lagged Cross-Correlation (TLCC) or Dynamic Time Warping (DTW). An exploration or justification of the chosen metric over others could have added depth to their methodology."
                },
                "questions": {
                    "value": "**Hyperparameter Selection in Graph Construction**:\n   - The methodology introduced by the authors involves several hyperparameters, which seemingly have a significant impact on the model's outcomes. Specifically, when constructing the graph structure:\n     - How was the threshold value of 0.8 determined?\n     - Regarding the 'topk' selection, how was the value of \\( k \\) chosen, and does it correlate with the number of variables?\n\n **Mix-hop Propagation Parameter**:\n   - How was the value for the EMA parameter \\( \\alpha \\) in the mix-hop propagation process determined?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1208/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697955080810,
            "cdate": 1697955080810,
            "tmdate": 1699636047386,
            "mdate": 1699636047386,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "45CNtUU43I",
                "forum": "lmShn57DRD",
                "replyto": "Y1ElQ1o5hQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1208/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1208/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer qMLP (Q5, Q6)"
                    },
                    "comment": {
                        "value": "> Q5: Hyperparameter Selection in Graph Construction:\n> - The methodology introduced by the authors involves several hyperparameters, which seemingly have a significant impact on the model's outcomes. Specifically, when constructing the graph structure:\n>     - How was the threshold value of 0.8 determined?\n>     - Regarding the 'topk' selection, how was the value of ( k ) chosen, and does it correlate with the number of variables?\n\nA5: Thank you for your question. The threshold (we use $\\lambda$ in the following text) 0.8 and top-k value are chosen according to experience in the earlier version. Based on your comment, we set these two parameters as hyperparameters and conduct parameter sensitivity analysis on different datasets. We find that the value settings in the early version do not guarantee optimal results, in the revised version, we update the parameter sensitivity analysis results to **appendix A.5.2** and **A.5.3**. Overall, using different thresholds and topk value under different datasets can achieve better results, e.g. for traffic, the best $\\lambda$ is 0.6 and the best top-k is 16.\nTop-k value does not have direct correlation with the number of variables, however, based on the multi-hop depth $gdep$, we tend to choose top-k values that meet the following condition: (top-k)$^{gdep}\\le M$, where $M$ is the number of variables.\n\n> Q6: Mix-hop Propagation Parameter:\n> - How was the value for the EMA parameter ( $\\alpha$ ) in the mix-hop propagation process determined?\n\nA6: Thank you for your question. We also conduct parameter sensitivity analysis for the value of $\\alpha$ in the revised version, and choose the optimal results. We have updated these results to **appendix (A.5.4)**."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1208/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700556582199,
                "cdate": 1700556582199,
                "tmdate": 1700556582199,
                "mdate": 1700556582199,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0yJnjJYUSd",
                "forum": "lmShn57DRD",
                "replyto": "Y1ElQ1o5hQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1208/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1208/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer qMLP (Q1, Q2, Q3 and Q4)"
                    },
                    "comment": {
                        "value": "> Q1: Mathematical Notation Consistency:\n> - The authors' use of mathematical notation appears inconsistent. For instance, function names should ideally be presented in regular typeface rather than italic. Proper notation ensures clarity and avoids potential confusion.\n\nA1: We appreciate your comment for helping to optimize our paper. We check the main text and appendix of our paper and make corrections to address this issue. We modify the font of the function and variable letters, and correct the italicized font of the function to normal, such as covariance function, argtopk, softmax function.\n\n> Q2: Graph Construction Using Pearson Coefficient:\n> - While the authors opted for the Pearson correlation coefficient for graph construction, which subsequently serves as the foundational structure for the GNN, one might question the exclusion of making GNN parameters learnable. This adaptability could potentially offer more flexibility to the model.\n\nA2: We appreciate your suggestion. Parameterizing the graph structure to achieve greater flexibility is a worthwhile try. We parameterize the graph structure to see the performance changes of GRformer, and the results show that parameterizing the graph structure achieves better performance on some of the datasets(e.g. Weather), but the effect is not significant on most datasets, and even inferior to simple correlation coefficient algorithms. We have updated the results to **appendix (A.8)**.\n\n>  Q3: Assumption of Homoscedasticity:\n> - The Pearson coefficient assumes homoscedasticity in the data. It's unclear if the authors verified this assumption across their datasets. Such checks are crucial to ensure the validity of the chosen coefficient.\n\nA3: Thank you for your suggestion. The Pearson correlation coefficient is calculated based on the standardized data. We use the Bartlett function in the Python 'scipy' library to verify the homoscedasticity of the data we use, and obtaine test statistics and test p on 8 datasets, respectively. The results indicate that the statistical values of these data are very small, and p_value is close to 1.0, which makes us confident that the data we use satisfies homoscedasticity. We provide additional clarification on this issue in **appendix (A.5)**.\n\n> Q4: Alternative Correlation Metrics:\n> - The paper doesn't seem to explore or discuss other potentially beneficial correlation coefficients like Time-Lagged Cross-Correlation (TLCC) or Dynamic Time Warping (DTW). An exploration or justification of the chosen metric over others could have added depth to their methodology.\n\nA4: Thank you for your suggestion! We agree with you with the insight that more attempts on the Graph construction module is beneficial. Based on your suggestion, we try some other graph structure construction algorithms, such as the DTW algorithm, and update the results in **appendix (A.8)**. From the results, although the DTW algorithm has a more complex and refined calculation process compared to the Pearson coefficient, there is not significant performance improvement. The default graph construction method in our proposed GRformer is still relatively simple and widely effective. TLCC generates a correlation matrix similar to Pearson coefficient method, so we do not conduct experiments with this setting.\nWe also clearly recognize that in the face of more complex scenarios, using different algorithms may also result in better performance. Our graph construction module is independent of the training process, making it easy to be replaced with other graph construction algorithms to generate reasonable graph structures."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1208/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700557187960,
                "cdate": 1700557187960,
                "tmdate": 1700557187960,
                "mdate": 1700557187960,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ZW5q7GQSYk",
            "forum": "lmShn57DRD",
            "replyto": "lmShn57DRD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1208/Reviewer_Evwp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1208/Reviewer_Evwp"
            ],
            "content": {
                "summary": {
                    "value": "This paper enhances Transformer with GNN and position embedding generated by RNN for multivariate time series forecasting. The proposed GRformer constructs graph by pearson correlation and uses a mix-hop propagation GNN layer to capture cross-channel dependency. For temporal dependency, it uses an RNN to recursively generate positional embeddings. Experiments on eight real-world datasets show that the proposed GRformer is on compare with SOTA model, PatchTST."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This paper is well-written and easy to follow.\n- Using pearson correlation for graph constructing is reasonable and efficient."
                },
                "weaknesses": {
                    "value": "My main concern is that the novelty is limited:\n\n- For RNN-based position embedding: \n  1. The idea of enhance Transformer with RNN is not new[1].\n  2. RNN operates recursively and cannot be parallelized, which offsets the efficiency advantages of Transformers that can be highly parallelized.\n  3. Ablation study in Table 3 shows that the improvement of RNN against previous learnable position embedding is not significant.\n- For Mix-hop propagation: \n    1. The mix-hop propagation layer is **exactly the same** as that in [2] and there is no explicit reference to it in Section 3.2.3.\n    2. Besides the graph construction via Pearson correlation, this is a direct combination of PatchTST and \"Connecting the dots\".\n\n[1] Qin, Yao, et al. \"A dual-stage attention-based recurrent neural network for time series prediction.\" arXiv preprint arXiv:1704.02971 (2017).\n\n[2] Wu, Zonghan, et al. \"Connecting the dots: Multivariate time series forecasting with graph neural networks.\" Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining. 2020."
                },
                "questions": {
                    "value": "- What is the authors' primary objective in visualizing the weights of the MLP in Figure 1(b), given that it only reflects the correlation among hidden states? \n- Could you provide a comparison of the computational efficiency between your RNN-based position embedding and a learnable position embedding, particularly in relation to varying sequence lengths?\n- How were the hyperparameters (0.8 and $k$) in Equations (2) and (3) chosen, and what impact do these specific values have on the model's performance and behavior?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1208/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1208/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1208/Reviewer_Evwp"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1208/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698032964680,
            "cdate": 1698032964680,
            "tmdate": 1699636047317,
            "mdate": 1699636047317,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zLSZSLdAC1",
                "forum": "lmShn57DRD",
                "replyto": "ZW5q7GQSYk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1208/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1208/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Evwp (Q3, Q4 and Q5)"
                    },
                    "comment": {
                        "value": "> Q3: What is the authors' primary objective in visualizing the weights of the MLP in Figure 1(b), given that it only reflects the correlation among hidden states?\n\nA3: Thanks for asking the question. We modify the corresponding figures and explanations of **Figure 1(b)** to make it clear. Values of each channel of a multi-variant time series will be passed through an embedding mapping projector, a series of feedforward networks of Encoders and Decoders, and a final mapping projector. These neural networks are usually some MLP layers, such as Linear layer and Conv1d layer. The weight matrix plays an important part in promoting interactions across different channels. We save the weight matrix parameters (we choose parameters from Informer) and calculate their matrix products, and get an equivalent correlation matrix. Compared with the Pearson correlation coefficient matrix (or the correlation matrix obtained by any other algorithms), the result does not effectively promote the interaction between strong correlated sequences and prevent weak correlated sequences from interacting, either. \n\n> Q4: Could you provide a comparison of the computational efficiency between your RNN-based position embedding and a learnable position embedding, particularly in relation to varying sequence lengths?\n\nA4: Thank you for your suggestion. Calculating the efficiency of the module does rigorously evaluate our design better. Under your suggestion, we visualized the memory usage and training speed of GRformer and other models under the same batch size (we choose 32). We remove the graph convolutional module and use RNN position encoding only to illustrate the impact of introducing RNN on model efficiency. The results indicate that the RNN structure does prolong the training time, but it is not much and is related to the length of the input sequence $L$ and depth of RNN structure $C$. The specific results are shown in **appendix (A.6)**, **Table 9** in the revised version. In relatively small datasets such as ETTh1, the use of 1-layer RNN position encoding increase the memory usage by 3.8% compared to patchTST, and the training time is almost not increased; In relatively large datasets such as Traffic, the memory usage increases 12.6% and training time increases 5.9%. With a larger value of RNN depth $C$ and input length $L$, the efficiency gap will increase, the growth rate is within an acceptable range.\n\n> Q5: How were the hyperparameters (0.8 and $k$) in Equations (2) and (3) chosen, and what impact do these specific values have on the model's performance and behavior?\n\nA5: We appreciate you for this detailed question. The choice of 0.8 and top-k value in the previous version of our paper are based on experience. Based on your comment, in order to provide a more rigorous explaination of our hyperparameter choosing, we conduct hyperparameter sensitivity analysis on the filtering threshold (we use $\\lambda$ to represent it) and top-k values of the Pearson coefficient correlation matrix, and update the results to **appendix A.5.2, A.5.3**, and **A.5.4**. A smaller threshold and larger top-k value will allow for more interactions between channels. Directly choosing 0.8 based on experience does have some problems and changing this threshold to smaller values can achieve better results on different datasets(e.g. GRformer gets the optimal results using $\\lambda=0.6$ and top-k $=16$ in Traffic). We choose the best results and update **Table 2** in **section 4.1**."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1208/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700554340638,
                "cdate": 1700554340638,
                "tmdate": 1700554340638,
                "mdate": 1700554340638,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kbePR0IGDX",
                "forum": "lmShn57DRD",
                "replyto": "ZW5q7GQSYk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1208/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1208/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Evwp (Q1 and Q2)"
                    },
                    "comment": {
                        "value": "> Q1: For RNN-based position embedding:\n> - The idea of enhance Transformer with RNN is not new[1].\n> - RNN operates recursively and cannot be parallelized, which offsets the efficiency advantages of Transformers that can be highly parallelized.\n> - Ablation study in Table 3 shows that the improvement of RNN against previous learnable position embedding is not significant.\n\nA1: We appreciate for your comment. \n- There are indeed some previous studies using RNN to enhance Transformer, and we have noted the work you mentioned. But the way we use RNN is different. In paper [1], Attention machenism is used to generate an enhanced input sequence to be fed in an RNN structure, with its attention score generated with the help of the previous RNN hidden states. The encoder is essentially an RNN that encodes the input sequences into a feature representation, which makes the model still suffer from gradient explosion and vanishing when facing long-term sequence.  In GRformer, we do not directly pass input data through the RNN layer, we only use RNN to explicitly introduce temporal order information in position encodings that are injected into sequence tokens, driven by the features. The encoder of GRformer is essentially a self-attention layer. To our knowledge, there are currently no similar position encoding methods like this to solve MLTSF tasks, and our work also explores the effectiveness of this approach.\n- As for the model efficiency, we conduct experiments to visualize the training speed and memory consumption of using RNN-based position encoding. In small dataset like ETTh1, a one-layer RNN-based position encoding module costs almost the same training time and 3.8% more memory compared with learnable position encoding (i.e. PatchTST), while the gap comes to 5.9% more training time and 5.3% more memory in Traffic. The results are added to **appendix (A.6)**.\n- We evaluate the effectiveness of using RNN-based position encoding with varying depth on different datasets and add the results in **appendix (A.5.1)*, overall, RNN-based position encoding can achieve 2.51% and 1.83% reduction in MSE and MAE. We also put results of ablation studies on all datasets in **section 4.2**.\n\n\n> Q2: For Mix-hop propagation:\n> - The mix-hop propagation layer is exactly the same as that in [2] and there is no explicit reference to it in Section 3.2.3.\n> - Besides the graph construction via Pearson correlation, this is a direct combination of PatchTST and \"Connecting the dots\".\n\nA2: Thank you for your comment. \n- Yes, we do draw inspiration from MTGNN[1] for using multi-hop propagation layers. We explicit reference to it in **section 3.2.3** in the revised version. However, our multi-hop propagation layer has some differences in implementation details compared to MTGNN. In MTGNN, embeddings from each hop are aggregated through a Conv2d layer with a convolutional kernel size of (1,1), which means that the aggregated embeddings of each channel in each hop are all viewed as separate individuals (i.e. they will be assigned with different weights separately). We choose to use a learnable matrix to aggregate the embeddings of different hop neighbors, so that embeddings of each channel from the same hop neighbors will be assigned with the same weights, this makes it easier for the model to identify the importance of neighborhoods\u2019 information of different hops.\n- One of our motivation is that, the current channel-mixing and channel-independent models can\u2019t effectively capture the cross-channel dependencies between time series, while graph neural network provides a chance to make better solutions. We explored the effectiveness of graph neural networks in MLTSF tasks, and from the results in **Table 3**, it improves the MSE and MAE by 0.65% and 3.63% respectively.\n\n\n[1] Qin, Yao, et al. \"A dual-stage attention-based recurrent neural network for time series prediction.\" arXiv preprint arXiv:1704.02971 (2017).\n\n[2] Wu, Zonghan, et al. \"Connecting the dots: Multivariate time series forecasting with graph neural networks.\" Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining. 2020."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1208/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700556037646,
                "cdate": 1700556037646,
                "tmdate": 1700556037646,
                "mdate": 1700556037646,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oOULnjFR01",
                "forum": "lmShn57DRD",
                "replyto": "kbePR0IGDX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1208/Reviewer_Evwp"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1208/Reviewer_Evwp"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors' Rebuttal"
                    },
                    "comment": {
                        "value": "Thanks for your feedback, which addresses my concern about hyper-parameters of graph construction. However, my other concerns have not been addressed yet:\n1) The modified Figure 1(b) reflects nothing about channel dependency: there are many non-linear operations in the network, such as activations and self-attentions, so it makes no sense to multiply all weight matrices together.\n2) RNN-based position encoding goes against the original intention of position encoding, which is to introduce positional information to the transformer in a non-recursive way.\n3) Despite the aggregation matrix being slightly different, there is no big difference between the mix-hop propagation in this paper and  \"Connecting the dots\", thus it should not be listed as a contribution.\n\nBased on the above reasons, I'll maintain my score of 3."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1208/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700621380670,
                "cdate": 1700621380670,
                "tmdate": 1700621380670,
                "mdate": 1700621380670,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0V7MNAJ5PS",
            "forum": "lmShn57DRD",
            "replyto": "lmShn57DRD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1208/Reviewer_rJkP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1208/Reviewer_rJkP"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes GRformer, a new neural architecture for multivariate long-term time series forecasting (MLTSF). The authors propose a hybrid architecture that consists of a Transformer-based graph neural network to model cross-channel dependencies and a recurrent neural network to model temporal dependencies. The proposed model shows promising performance on eight benchmarks. However, the motivation and reasoning behind the criticism of the Transformer-based approach are difficult to understand. Some of the claims are made without proper evidence, or by simply citing previous work, without providing any further detailed study or analysis. Additionally, the performance improvements on the benchmarks seem to outperform the baselines. However, I believe the claim of achieving a performance improvement with a 5.7% decrease in MSE and 6.1% decrease in MAE is misleading. These numbers are calculated by averaging MSE and MAE without considering the scales between different benchmarks and metrics. ILI has much higher mean squared errors (MSEs) and mean absolute errors (MAEs) than other benchmarks. This means that if you compute the average score in this way, the average score can be dominated by the relative improvement in this specific dataset. The tone reporting the improvement suggests that the model showed around a 6% decrease in errors on all benchmarks, but the average relative improvement for each benchmark at different metrics is actually 2.55% for MSE and 4.96% for MAE."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The model achieves improvements over 7 different benchmarks using 4 metrics for each benchmark dataset. The experiments are done extensively with ablation on different positional encoding strategies. This however raises a question on why the RNN is needed (Table 3. R: the first column vs L: the second column show a very minor difference)."
                },
                "weaknesses": {
                    "value": "I am not sure what I am seeing in Figure 1(b), and I don\u2019t understand how to interpret the authors' claim that cross-channel interaction is chaotic based on simply visualizing the weight matrices of the Transformer's dense layer (internal MLP).\n\nI am not sure I understand the authors' point about positional encoding not being able to represent temporal orders well. RNNs have their own problems, such as vanishing gradients when modeling long-term temporal dependencies. Are you suggesting that RNNs outperform Transformers in multivariate long-term time series forecasting (MLTSF)?\n-> Are the ablation results in Table 3 the experiments to back this claim? If that's the case, the performance difference between an RNN-based positional encoding (?) vs a learned positional embedding is almost 0.\n\nWhat exactly is the RNN-based position encoding method? In the caption for Figure 2, it says \"The multi-layer RNN injects temporal order information.\" However, RNNs are not just injecting temporal order information as some sort of advanced positional encoding method; they can actually learn temporal dependencies. I am not sure if you are distinguishing between positional encoding and learning temporal representation.\n\nFigure 2 (b) is hard to understand, at least explain the operator signs in the caption, arrows are not clear."
                },
                "questions": {
                    "value": "What is the main evidence that Transformer-based models are ineffective at capturing cross-channel dependencies and temporal orders? If Transformers were bad at capturing temporal orders, they would not have become as popular as they are today. I am curious why the authors make such claims, as I do not see any plausible supporting evidence in the manuscript.\n\nThe authors mentioned that they used multi-layered RNNs, however in the appendix, it's said 1-layer RNN was used. Can you clarify the details of the RNN architecture?\n\n\u201cTo properly capture temporal dependencies, we consider using a multilayer RNN to encode the positions in the time series.\u201d Why deep RNNs can properly capture temporal dependencies while Transformers can\u2019t?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1208/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698711813614,
            "cdate": 1698711813614,
            "tmdate": 1699636047246,
            "mdate": 1699636047246,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "l9e77X8Vg9",
                "forum": "lmShn57DRD",
                "replyto": "0V7MNAJ5PS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1208/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1208/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer rJkP(Q5,Q6 and Q7)"
                    },
                    "comment": {
                        "value": "> Q5: What is the main evidence that Transformer-based models are ineffective at capturing cross-channel dependencies and temporal orders? If Transformers were bad at capturing temporal orders, they would not have become as popular as they are today. I am curious why the authors make such claims, as I do not see any plausible supporting evidence in the manuscript.\n\nA5: We appreciate you for this thoughtful question, we agree with you that Transformer is popular now and good at capturing temporal dependencies. We dive deep into the characteristics of time series data and believe that Transformer-based models can be further improved in extracting cross-channel and temporal dependencies.\n1. **Cross-channel dependencies.** Values of each channel of a multi-variant time series data will be passed through an embedding mapping projector, a series of feedforward networks of Encoders and Decoders, and a final mapping projector. These modules are usually implemented with MLP layers, and the weight matrices implicitly promotes the interaction of different channels. We modify **Figure 1(b)** to the weight matrix product of MLP layers in previous Transformer models to intuitively compare with the adjacency matrix constructed using Pearson correlation coefficient. The result of weight matrix product leads to a correlation matrix with chaotic data distribution, which may result in strongly correlated sequences not interacting effectively and weakly correlated sequences introducing noise to each other.\n2. **Temporal dependencies.** Transformer is good at capturing the temporal dependencies of sequence data because of the use of self-attention mechanism with position encoding. However, classic position encoding methods such as fixed and learnable position encoding are first introduced in NLP tasks. These methods focus more on positional variance, not the strict temporal order, which is a key characteristic of time series data. This motivate us to find a better position encoding method to inject strict temporal order information to sequence tokens, and RNN is a good choice because of its sequence modeling capability. The ablation study results shown in **Table 3** can be the supporting evidence for the above design. The use of RNN-based position encoding reduces the MSE and MAE by 2.51% and 1.93% on average.\n\n> Q6: The authors mentioned that they used multi-layered RNNs, however in the appendix, it's said 1-layer RNN was used. Can you clarify the details of the RNN architecture?\n\nA6: We thank the reviewer for the detailed question. In the earlier version of the paper, we lack the sensitivity analysis on the number of RNN layers ($C$). Based on your comment, in the revised version, we conduct experiments on this hyperparameter and add the results in **appendix (A 5.1)**. We sum the result in the following table, for weather, electricity, ETTh1, ETTm1 and ILI, the model reaches optimal performance with $C=1$; for traffic, ETTh2 and ETTm2, the best RNN layer number is $C=2$.\n| Datasets | Weather | Traffic | Electricity | ETTh1 | ETTh2 | ETTm1 | ETTm2 | ILI |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| Optimal $C$ | 1 | 2 | 1 | 1 | 1 | 2 | 2 | 1 |\n\n> Q7: \u201cTo properly capture temporal dependencies, we consider using a multilayer RNN to encode the positions in the time series.\u201d Why deep RNNs can properly capture temporal dependencies while Transformers can\u2019t?\n\nA7: Thank you for the comment, the description in the earlier version may cause confusions, so we have modified it to \u201cIn order for the model to make use of the strict temporal order of the sequence, we consider using a multi-layer RNN to inject some positional contextual information into the sequence tokens\u201d in **section 3.2.1**. Our viewpoint is that vanilla Transformer can indeed capture temporal dependencies, however, the position encoding methods (e.g. fixed or learnable position encoding) are initially designed for NLP tasks, where the sequence data used in nature language do not emphasize strict temporal order as much as time series data does. We hope to use the sequence modeling capability of RNN to supplement this temporal order information, so that the Transformer-based models can learn this characteristic of time series and obtain more reasonable temporal dependencies."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1208/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700552376803,
                "cdate": 1700552376803,
                "tmdate": 1700552376803,
                "mdate": 1700552376803,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eJwOcDDyj7",
                "forum": "lmShn57DRD",
                "replyto": "0V7MNAJ5PS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1208/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1208/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer rJkP (Q1,Q2,Q3 and Q4)"
                    },
                    "comment": {
                        "value": "> Q1: I am not sure what I am seeing in Figure 1(b), and I don\u2019t understand how to interpret the authors' claim that cross-channel interaction is chaotic based on simply visualizing the weight matrices of the Transformer's dense layer (internal MLP).\n\nA1: We are thankful for your suggestion. We modify the corresponding explanations of **Figure 1(b)** to make it clear. In the revised version, we save the weight matrix parameters and calculated their matrix products, and get an equivalent correlation matrix. Values of each channel of a multi-variant time series will be passed through an embedding mapping projector, a series of feedforward networks of Encoders and Decoders, and a final mapping projector. These neural networks are usually some MLP layers, such as Linear layer and Conv1d layer. The weight matrix plays an important part in promoting interactions across different channels. \nCompared with the Pearson correlation coefficient matrix (or the correlation matrix obtained by any other algorithms), this result does not effectively promote the interaction between strong correlated sequences and prevent weak correlated sequences from interacting, either.\n\n> Q2: I am not sure I understand the authors' point about positional encoding not being able to represent temporal orders well. RNNs have their own problems, such as vanishing gradients when modeling long-term temporal dependencies. Are you suggesting that RNNs outperform Transformers in multivariate long-term time series forecasting (MLTSF)? -> Are the ablation results in Table 3 the experiments to back this claim? If that's the case, the performance difference between an RNN-based positional encoding (?) vs a learned positional embedding is almost 0.\n\nA2: Thank you for this thoughtful question. We agree with you that RNN has problems such as vanishing gradients facing long-term sequences, however, our motivation is to make use of the sequence modeling capability of RNN only to generate position encodings and we express this point in our **contributions** and **section 3.2.1**. The output results of the RNN layer are not directly used as the final feature embeddings, and this could alleviate the vanishing gradient problems.\nWe make more detailed parameter adjustments to the experimental setup of the ablation study, such as changing the number of RNN layers $C$ to observe performance changes. We choose the best results and update **Table 3** in **section 4.2**. overall, RNN-based position encoding achieves 2.51% and 1.93% improvement in MSE and MAE metrics, respectively. We present the optimal results using RNN-based position encoding with layers 1, 2, and 3 on all the dataset in **appendix (A 5.1)**.\n\n> Q3: What exactly is the RNN-based position encoding method? In the caption for Figure 2, it says \"The multi-layer RNN injects temporal order information.\" However, RNNs are not just injecting temporal order information as some sort of advanced positional encoding method; they can actually learn temporal dependencies. I am not sure if you are distinguishing between positional encoding and learning temporal representation.\n\nA3: Thank you for asking this question. Passing data into RNN can indeed extract temporal dependencies, however, this operation may cause information loss because of gradient explosion and vanishing when facing long-term sequence. In our model, RNN-based position encoding refers to the use of RNN structures for only generating position encodings with a feature-driven scheme. We hope to use its sequence modeling characteristics to generate position encoding with strict temporal order information, rather than directly output its results as temporal dependent features. By doing so, we can obtain enhanced position encodings compared to position encodings used by vanilla Transformer-based models and help the model to comprehend the strict temporal order of time series data.\n\n> Q4: Figure 2 (b) is hard to understand, at least explain the operator signs in the caption, arrows are not clear.\n\nA4: We apologize for the confusion. **Figure 2(b)** shows the module design based on multi-layer RNN position encoding, all the operator signs are in **Eq(1)**. We add explainations for **Figure 2(b)** in the revised version. The hidden state of the $j$-th RNN unit in $(c-1)$-th layer $r_{(i, j)}^{(c-1)}$ is used as inputs for RNN units at the same position in the next layer (the $j$-th RNN unit in the $c$-th layer) and the previous time step hidden state of RNN units at the next position in the same layer (the $(j+1)$-th RNN unit in the $(c-1)$-th layer). We update the arrows in **Figure 2(b)** and provide a more detailed explanation."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1208/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700553452802,
                "cdate": 1700553452802,
                "tmdate": 1700553452802,
                "mdate": 1700553452802,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]