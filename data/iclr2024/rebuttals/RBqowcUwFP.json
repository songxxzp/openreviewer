[
    {
        "title": "L(M)V-IQL: Multiple Intention Inverse Reinforcement Learning for Animal Behavior Characterization"
    },
    {
        "review": {
            "id": "HJuvKCjWuv",
            "forum": "RBqowcUwFP",
            "replyto": "RBqowcUwFP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7249/Reviewer_yMx7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7249/Reviewer_yMx7"
            ],
            "content": {
                "summary": {
                    "value": "In this work, the Authors propose a novel algorithm for discerning and characterizing multiple intents underlying natural behaviors. To this end, they combine the inverse Q-learning (a version of IRL) with the expectation-maximization algorithm used to delineate the intents. The Authors consider two types of dynamics in their framework: the Bernoulli process and the Markov process; they provide theoretical derivations and describe implementations for both. They then test their framework on a simulated task (a gridworld with 2 underlying intentions) where they show the framework\u2019s ability to recover the ground truth, and on an existing mouse dataset (a bandit reversal learning task) where they reconstruct and describe mouse intents."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "A definite strength of this work is that it has been anticipated in the field. As a continuous-time model for multiple intent reconstruction has been developed by Ashwood et al (NeurIPS 2022; referenced in current manuscript), a question that has been repeatedly raised was about the discrete version of the framework. This question has become especially relevant after another work by Ashwood et al (Nat Neurosci 2022; also referenced in current manuscript) that used large-scale IBL data to propose that natural behaviors can be represented via an MDP featuring rapid progression of discrete states. This ICLR submission delivers on that expectation.\n\nThe model in the paper is well-founded; it features reasonable choices of the constituting algorithms (e.g., the Baum-Welch algorithm for discerning the intents).\n\nHaving the analysis for both simulated and animal data is a plus."
                },
                "weaknesses": {
                    "value": "Along the same line with the strengths, I see the main weakness here in high similarity to Ashwood et al (NeurIPS 2022) work. The Authors mention in the Appendix that the aforementioned approach \u201cis limited to capturing continuous intra-episode variation of reward functions during navigation behavior, and difficult to adopt to other environments\u201d but, should that be true, that requires further substantiation. The data analyses offered in this paper seem to mainly serve as a proof of principle for the proposed model.\n\nOverall, the work is nicely done and well-timed; my only concern is that the high similarity to prior literature determines the work\u2019s novelty which may be insufficient for ICLR. With that said, I\u2019m open to comments by the Authors, other Reviewers, and the Area Chair in that regard."
                },
                "questions": {
                    "value": "-Introduction: wouldn\u2019t it make more sense to introduce your work via Ashwood et al, 2022a and 2022b papers? I feel like this way the reason for the development of your model and the comparison to the existing state-of-the-art would be more transparent.\n\n-Page 5 under Equation 10: what does Delta Z mean? Is it supposed to reflect the available transitions?\n\n-Page 6 under Figure 3: why is it necessary to punish the types of reward irrelevant to the intentions? A more natural way seemingly would be to set them equal to zero. I assume this natural way hasn\u2019t worked out for some reason?\n\n-Figure 4A: why does the LL in the training curve drop? That is unlikely to be explained by overfitting as suggested in the text.\n\n-Page 7 bottom line: \u201cAlthough model performance continued to improve slightly with more latent states, we will focus [\u2026] on [\u2026] 3 states\u201d. Wouldn\u2019t it be easier to make this argument by using the Bayesian Information Criterion instead of the pure NLL to choose K? This way one can arrive at a principled number of intents that very well may turn out to be equal to 2.\n\n-Figure 5C. Following up on my previous point, this figure leaves me with the impression that the third intent is just spurious (not stable; immediately reverses to the first intent). Could you please comment on why you consider it important?\n\nMinor comments:\n\n-Figure 3: the overlap of the crosses (x) and dots ( . ) is hard to follow. Could you please use an alternative way to represent this data?\n\nI\u2019d also suggest tuning down a couple of literature-related claims:\n\n-Page 2: \u201c[IRL\u2019s] adoption as mathematical behavior models in neuroscience research has been relatively limited\u201d. I had another impression \u2013 it seems to be an up-and-coming tool, as exemplified by some awesome works from Jon Pillow\u2019s and Xaq Pitkow\u2019s groups.\n\n-Page 2: \u201c[our method presents a [\u2026] framework for characterizing the delicate balance between exploration and exploitation [\u2026] which constitutes a [\u2026] comparatively understudied aspect within the realm of neuroscience.\u201d I\u2019d say that, first, there\u2019s a huge spillover from the machine learning field of intrinsic motivation (a.k.a. an internal reward for exploration); many of these works claim biological plausibility. There are some other nice works, e.g. Pisupati et al (eLife 2021) and references therein that directly address the issue. There\u2019s also lots of work on Bayesian optimality that study the deviations from optimal exploitation to account for the environmental dynamics, e.g. Yu and Cohen (NeurIPS 2008).\n\n-Page 4: \u201cIn behavioral neuroscience, it is commonly considered that animals alternate between multiple intentions under the Markov property\u201d. The entire reason why the Ashwood et al (Nat Neurosci 2022) paper cited there emerged is because that\u2019s _not_ how people used to characterize natural behaviors. This is reflected literally in the first sentence of the said paper. While this new work has gotten substantial traction in the field, I wouldn\u2019t say that that new way to model data has completely wiped out the conventional approach.\n\n________________________________________________________________________________\npost-rebuttal:\n\nI would like to thank the Authors for their clarifications. I appreciated the fast, detailed responses.\nPosting my final response here as, at this time, I cannot otherwise make it visible to the Authors.\n\nI believe that the updated manuscript is a more solid, transparent, and substantiated work.\nThe most interesting finding to me is that new priors here allowing for abrupt changes of goal maps, enabled by the novel problem formulation, optimization objective, and solver, were more consistent with the mouse decision-making data in the maze experiment than DIRL (the previous SOTA), rendering the proposed model important. I would also like to thank the Authors for the clarification that the choice of different smoothness prior in DIRL would not necessarily be able to recover the same dynamics, necessitating the formulation of the problem in the way proposed here.\n\nDespite the similarity to prior work, this is an important and interesting result. I increased my score to reflect it."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7249/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7249/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7249/Reviewer_yMx7"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7249/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698438021056,
            "cdate": 1698438021056,
            "tmdate": 1700763910844,
            "mdate": 1700763910844,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1F3SjkjGgP",
                "forum": "RBqowcUwFP",
                "replyto": "HJuvKCjWuv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7249/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7249/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank reviewer yMx7 for his suggestions to improve our submission. We address your concerns in detail below and have updated our paper accordingly.\n\n*\u201dThe Authors mention in the Appendix that the aforementioned approach \u201cis limited to capturing continuous intra-episode variation of reward functions during navigation behavior, and difficult to adopt to other environments\u201d but, should that be true, that requires further substantiation.\u201d*\n\nThanks for your question. The DIRL approach from Ashwood et al. (NeurIPS 2022) assumes the rewards vary continuously over time, i.e. the intention transition happens after each action execution. The limitation of their approach can be indicated from the Gridworld environment in our first experiment, where we consider each episode was demonstrated under one of two intentions, i.e. the intention is fixed within each episode, and the intention transition only happens between episodes. DIRL will not be feasible under such environments since the variation of reward functions is, however, discrete. In contrast, our method is viable in cases where the transition of intention occurs either after each action execution, i.e. continuous (as discussed in Remark 2 in Appendix B.2), or at the end of each episode, i.e. discrete (as theoretically demonstrated in Appendix B.2). We have elucidated this argument in the current revision to provide a clearer understanding of the adaptability and applicability of our approach in diverse settings.\n\n*\u201dIntroduction: wouldn\u2019t it make more sense to introduce your work via Ashwood et al, 2022a and 2022b papers? I feel like this way the reason for the development of your model and the comparison to the existing state-of-the-art would be more transparent.\u201d*\n\nThank you for your valuable suggestion! We have revised the introduction to transparently articulate our motivation for this work.\n\n*\u201dPage 5 under Equation 10: what does Delta Z mean? Is it supposed to reflect the available transitions?\u201d*\n\nThanks for your question. $\\Delta(\\mathcal{Z})$ indicates the probability simplex on the latent state space. Thus $\\Pi \\colon \\Delta(\\mathcal{Z})$ defines the latent state initial distribution probability, and $\\Lambda \\colon \\mathcal{Z} \\mapsto \\Delta(\\mathcal{Z})$ reflects the latent state transition matrix, i.e. the latent state distribution probability at $t+1$ given the latent state at $t$. We have refined the definition accordingly in the current revision to eliminate any ambiguity.\n\n*\u201dPage 6 under Figure 3: why is it necessary to punish the types of reward irrelevant to the intentions? A more natural way seemingly would be to set them equal to zero. I assume this natural way hasn\u2019t worked out for some reason?\u201d*\n\nThank you for your suggestion. We have conducted additional experiments on the Gridworld environment with the same resource distribution but removed the punishment on the intention irrelevant reward, i.e. replacing the -1 reward on the type of reward irrelevant to the intentions with 0. Related results are shown in Appendix C.2. Our algorithms still worked under this configuration. However, in this case there was an increased overlapping between some of the demonstrated trajectories under different intentions (Figure 6, Top), resulting in a slight decrease of the performance of LV-IQL, but LV-IAVI and LV-IQL still outperformed the single intention algorithm IAVI and IQL in trajectory clustering and recovering corresponding expert reward functions.\n\n*\u201dFigure 4A: why does the LL in the training curve drop? That is unlikely to be explained by overfitting as suggested in the text.\u201d*\n\nThanks for your question. As we consider each state $s_t$ in the state space $\\mathcal{S}$ to be defined with a set of truncated history information to avoid explicitly describing a partially observable MDP formulation for the reversal-learning task, the size of the state space $\\mathcal{S}$ will grow exponentially as the history truncation length $\\ell_h$ increases. Thus the sampling on some states given the fixed animal demonstrations could be insufficient for the IQL to reconstruct a precise reward function via stochastic approximation. The abnormal drop of the training LL will be omitted given a larger animal demonstration dataset. We have updated this explanation in the current revision for better clarification."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7249/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700655158557,
                "cdate": 1700655158557,
                "tmdate": 1700655158557,
                "mdate": 1700655158557,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jbMtA21K0t",
                "forum": "RBqowcUwFP",
                "replyto": "qqoodTjeQM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7249/Reviewer_yMx7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7249/Reviewer_yMx7"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your detailed response.\n\nI've read the response, the communication with other Reviewers, and the updated text. With this, my technical questions have been fully addressed. I appreciate the clarity of the updated manuscript and the added experiments and comparisons. I am specifically excited about the new comparison with DIRL, although I may need some extra time to process the significance of this result (e.g., whether there are circumstances under which DIRL would reach similar performance and, related, whether either of the two methods in comparison can be viewed as a limit case of another one).\n\nAs it stands, this manuscript is now a complete work. The only concern that remains and that has been raised by most of the Reviewers is the degree of similarity to prior work and whether the offered novelty matches the expectations for the ICLR. I look forward to discussing that matter with the other Reviewers and with the Area Chair."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7249/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700677267998,
                "cdate": 1700677267998,
                "tmdate": 1700677267998,
                "mdate": 1700677267998,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "L0d2j26gwn",
                "forum": "RBqowcUwFP",
                "replyto": "HJuvKCjWuv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7249/Reviewer_yMx7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7249/Reviewer_yMx7"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up questions"
                    },
                    "comment": {
                        "value": "After carefully examining the added experiment (the comparison to DIRL), I have a few follow-up questions.\n\nIn Figure 3F of the updated manuscript, the transition probability is ~2% from \"thirsty\" to \"tired\" and ~1% from \"tired\" to thirsty\" suggesting that a transition would likely happen at most once per session and the state of the animal throughout the session would be thus described by a theta (step) function. Figure 3E, to my understanding, is smooth and not step-like due to averaging over sessions/mice. The better performance of the new model compared to DIRL would then be explained by the underlying truth (the satiation in mice) being more consistent with a step function rather than with a smoothly decaying function (consistent with a few previous works). Therefore, my questions are:\n\n-would DIRL be able to reconstruct the same result if larger changes in goal maps are allowed? In DIRL, if I recall it correctly, smoothness in the goal map weights is imposed via a Gaussian prior which, to my understanding, could be varied.\n\n-would the ability to vary the smoothness prior in DIRL render the current work as a limit case of DIRL?\n\n-In one of the replies here, the Authors state: *\"An essential characteristic of our approach is that we allow both latent state transition within an episode (i.e. continuous-time transition similar to DIRL, as shown in the second and third experiment) and between episodes (i.e. stay constant within one episode, as shown in the first experiment).\"* Could you please clarify what is meant by the continuous-time transition in the proposed method? Is this in the sense that, on different rollouts, the switch between the states would occur at different times, thus making the transition \"smooth\" on average, or does it mean something different?\n\nThanks in advance."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7249/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700713477808,
                "cdate": 1700713477808,
                "tmdate": 1700713500787,
                "mdate": 1700713500787,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Ah3boUB4Ps",
            "forum": "RBqowcUwFP",
            "replyto": "RBqowcUwFP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7249/Reviewer_o7yT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7249/Reviewer_o7yT"
            ],
            "content": {
                "summary": {
                    "value": "This work considers an inverse reinforcement learning model (IRL) with latent discrete intention variables. Using an inverse Q-learning and EM-based approach, they perform inference on these latent variables at each time based on either generalized Bernoulli or Markovian dynamics, learning both the transition dynamics between intentions (in the Markov case) and their corresponding reward functions. Experiments involve recovery on simulated data and from behavior in mice performing a two-alternative forced choice task with randomly changing reward structure."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Intriguing generalization of inverse RL methods to neuroscience.\n- Well-motivated incorporation of latent drives."
                },
                "weaknesses": {
                    "value": "- The fitted model is somewhat simplistic. Latent states are assumed to be multinomial or Markov, but the most plausible biological assumption would be that transitions between drives also interact with reward/satiety/recent history.\n- There are only two experiments: one on simulated data (where it is) compared to IAVI and IQL but not the Ashwood et al. or other similar models that might be applicable. Similarly, the mouse behavior is quite limited in terms of the need for RL. Again, the Ashwood Nature Neuro paper or the Ebitz, Albarran, and Moore (2018) provide fairly flexible models that are likely to capture the data as well. Given the synthetic data, one would have expected a more challenging task here as a target for IRL."
                },
                "questions": {
                    "value": "- Is it possible to incorporate some recent reward history into the transition structure? Since the inference algorithm is EM, will any EM-compatible latent variable model work, in principle?\n- Where are the bottlenecks for the method in terms of inferential complexity? Is the limiting factor the IAVI regression (i.e., the size of the tabular problem) or the EM complexity?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7249/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7249/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7249/Reviewer_o7yT"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7249/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698671475715,
            "cdate": 1698671475715,
            "tmdate": 1699636863866,
            "mdate": 1699636863866,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BUqBup88Ay",
                "forum": "RBqowcUwFP",
                "replyto": "Ah3boUB4Ps",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7249/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7249/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank reviewer o7yT for his suggestions to improve our submission. We address your concerns in detail below and have updated our paper accordingly.\n\n*\u201dThere are only two experiments: one on simulated data (where it is) compared to IAVI and IQL but not the Ashwood et al. or other similar models that might be applicable. Similarly, the mouse behavior is quite limited in terms of the need for RL. Again, the Ashwood Nature Neuro paper or the Ebitz, Albarran, and Moore (2018) provide fairly flexible models that are likely to capture the data as well. Given the synthetic data, one would have expected a more challenging task here as a target for IRL.\u201d*\n\nThanks for your great suggestion. We have included an additional experiment on the application of LMV-IAVI to real mice navigation trajectories within a labyrinth from Rosenberg et al. (2021). This dataset was also considered as a benchmark for IRL algorithm comparison in Ashwood et al. (NeurIPS 2022), since it would be challenging to define a suitable reward function to apply forward models (such as the GLM-HMM model from Ashwood et al. (Nat. Neuroscience, 2022) and the framework from Ebitz et al. (2018)). In this experiment, we compared our approaches with the state-of-the-art Dynamic Inverse Reinforcement Learning (DIRL) framework presented by Ashwood et al. (NeurIPS 2022). Related results are shown in Section 5.2 (Page 6) and Appendix D.2 (Page 16). Our approaches outperform DIRL in predicting mice behavior (indicated by the log-likelihood on held-out trajectories, Figure 3B, and Figure 7A) and are able to provide interpretable reward functions (Figure 3D, Figure 7B).\n\n*\u201dThe fitted model is somewhat simplistic. Latent states are assumed to be multinomial or Markov, but the most plausible biological assumption would be that transitions between drives also interact with reward/satiety/recent history.\"*\n*\u201dIs it possible to incorporate some recent reward history into the transition structure? Since the inference algorithm is EM, will any EM-compatible latent variable model work, in principle?\u201d*\n\nThank you for this great question. It is indeed a pertinent consideration to explore the utilization of a controlled latent state transition system, potentially incorporating external covariables, rather than the autonomous system currently under consideration in this paper. A promising avenue for future research could involve adopting a generalized linear model with inputs such as reward, satiety, recent history, etc., in lieu of the current generalized Bernoulli process or Markov process controlling the latent state transition dynamics. This adjustment would allow for the incorporation of the identification of potential external factors influencing intention transition dynamics. Furthermore, there is merit in exploring the feasibility of exerting control over the intention transition. Your suggestion is invaluable, and we will thoroughly explore these possibilities in our ongoing work.\n\n*\u201dWhere are the bottlenecks for the method in terms of inferential complexity? Is the limiting factor the IAVI regression (i.e., the size of the tabular problem) or the EM complexity?\u201d*\n\nThanks for your question. The computational complexity of IAVI is akin to that of standard value iteration, with the difference being an additive term for solving the SLE \u2013 a term that is often of minor importance given the usual predominance of the number of states over the number of actions. The primary contribution of Kalweit et al. was indeed the reduction in computational complexity relative to other IRL approaches, which was the reason for our selection of IAVI. The limiting factor is, therefore, rather identified as the complexity of the EM in the outer loop, although the computational demands of solving the MDP in the inner loop are of course inherently significant. Hence, the limiting part cannot be universally assigned without context; it depends on specific factors such as the size of the state space, the convergence rate of the EM, the convergence criteria of value iteration, and the complexity of computations within each E-step.\n\nDoes this address your concerns?"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7249/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700655046251,
                "cdate": 1700655046251,
                "tmdate": 1700655046251,
                "mdate": 1700655046251,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "B1jcphTxNF",
                "forum": "RBqowcUwFP",
                "replyto": "BUqBup88Ay",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7249/Reviewer_o7yT"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7249/Reviewer_o7yT"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the authors' responses, and the additional experiment is of added value. I do have a follow-up question:\n\n> Our approaches outperform DIRL in predicting mice behavior (indicated by the log-likelihood on held-out trajectories, Figure 3B, and Figure 7A) and are able to provide interpretable reward functions (Figure 3D, Figure 7B).\n\nI assume this is because LMV-IVAI has a more concentrated prior than DIRL? That is, if I understand the authors' responses to myself and other reviewers, the key difference here is that DIRL assumes $z$ changes _within trajectories_ while LM-IVAI assumes the $z$'s are _constant_ for demonstrations. In that case, LM-IVAI (as a latent variable model) would be a special case of DIRL in which no transitions of the latent state occur?\n\nSo is the improved performance of LMV-IVAI due to the more concentrated prior on the latents or something else?"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7249/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700661284555,
                "cdate": 1700661284555,
                "tmdate": 1700661284555,
                "mdate": 1700661284555,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uOQg0OlEUq",
                "forum": "RBqowcUwFP",
                "replyto": "9itIz330IJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7249/Reviewer_o7yT"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7249/Reviewer_o7yT"
                ],
                "content": {
                    "comment": {
                        "value": "Yes, I think so."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7249/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700664918721,
                "cdate": 1700664918721,
                "tmdate": 1700664918721,
                "mdate": 1700664918721,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Fg1GL3MGlr",
            "forum": "RBqowcUwFP",
            "replyto": "RBqowcUwFP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7249/Reviewer_bHzC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7249/Reviewer_bHzC"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an expectation-maximization approach for multi-goal IRL based on the inverse Q-learning IRL method. The approach involves clustering trajectories into multiple intentions and independently solving the IRL problem for each intention. The authors evaluate their algorithm using both simulated experiments and real-world mice data."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The problem of multi-goal IRL is highly relevant and its applications to cognitive science have been sparse in the past. I therefore particularly appreciate the application to the cognitive science domain to interpret real mice data.\n\nThe introduction of a multi-goal approach based on the inverse Q-learning algorithm seems novel and holds the promise of potentially outperforming previous multi-goal IRL methods.\n\nThe paper is well-structured and easy to follow, contributing to its readability and comprehensibility.\n\nThe algorithm's application to real mice behavioral data demonstrates its practical applicability in real-world scenarios."
                },
                "weaknesses": {
                    "value": "The most significant weakness of the paper is the lack of discussion regarding related work on multi-goal IRL. Despite the existence of numerous prior works in this field (e.g., [1-6]), the paper does not reference or discuss any of them. The absence of a comparison with existing methods raises questions about the true novelty and contribution of the proposed approach. The paper should explicitly highlight what sets its method apart and how it compares to the existing literature. Especially works [4-6] previously approached the multi-goal IRL problem with an expectation-maximization approach, even though inverse Q-learning was not used as backbone algorithm.\n\n[1] Dimitrakakis, C., & Rothkopf, C. A. (2012). Bayesian multitask inverse reinforcement learning. In Recent Advances in Reinforcement Learning: 9th European Workshop (EWRL), pp. 273-284, Springer\n\n[2] Gleave, A., & Habryka, O. (2018). Multi-task maximum entropy inverse reinforcement learning. arXiv preprint arXiv:1805.08882.\n\n[3] Babes, M., Marivate, V., Subramanian, K., & Littman, M. L. (2011). Apprenticeship learning about multiple intentions. In Proceedings of the 28th international conference on machine learning, pp. 897-904\n\n[4] Choi, J., & Kim, K. E. (2012). Nonparametric Bayesian inverse reinforcement learning for multiple reward functions. In Advances in neural information processing systems, vol. 25., pp. 305-313\n\n[5] Michini, B., & How, J. P. (2012). Bayesian nonparametric inverse reinforcement learning. In Machine Learning and Knowledge Discovery in Databases: European Conference, (ECML PKDD), pp. 148-163, Springer\n\n[6] Bighashdel, A., Meletis, P., Jancura, P., & Dubbelman, G. (2021). Deep adaptive multi-intention inverse reinforcement learning. In Machine Learning and Knowledge Discovery in Databases (ECML PKDD), pp. 206-221, Springer"
                },
                "questions": {
                    "value": "How does your approach compare to previous multi-goal IRL methods, especially those mentioned in the references [1-6]? It is crucial to provide a detailed comparison to establish the uniqueness and advantages of your proposed method in light of the existing literature."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7249/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698779476880,
            "cdate": 1698779476880,
            "tmdate": 1699636863690,
            "mdate": 1699636863690,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hcN4IOuxUK",
                "forum": "RBqowcUwFP",
                "replyto": "Fg1GL3MGlr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7249/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7249/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank reviewer bHzC for his suggestions to improve our submission. In response to your concerns and questions, we have repositioned the section on related work, moving it from the Appendices to the primary body of the paper immediately following the introduction (Section 2, Page 2). To ensure a more comprehensive understanding and foster ease of comparison, we have incorporated a table summarizing key features of the relevant algorithms, including but not limited to the aforementioned references [1-6], as well as a detailed comparison of the advantages and uniqueness of our approaches. In brief, our algorithms are capable of estimating ***non-linear***, ***discrete time-varying*** reward functions within both ***model-based*** and ***model-free*** conditions."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7249/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700654960336,
                "cdate": 1700654960336,
                "tmdate": 1700654960336,
                "mdate": 1700654960336,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NPjDl6dhKk",
            "forum": "RBqowcUwFP",
            "replyto": "RBqowcUwFP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7249/Reviewer_M3j1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7249/Reviewer_M3j1"
            ],
            "content": {
                "summary": {
                    "value": "This method, LMV-IQL, seeks to extend a class of IRL algorithms to the case of multiple intrinsic rewards, applied to behavioral modeling in neuroscience. They first identify each intention / reward and then solve for each. They demonstrate their method on both simulated and experimental datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The text is well written, particularly when defining theorems and the algorithmic steps. It would be a definite strength to extend IRL approaches to the regime of multiple unknown (intrinsic) reward functions or internal motivation states."
                },
                "weaknesses": {
                    "value": "Some of the figures need additional details/components. E.g., Figure 1, 2 need color scalebars, Figure 3 would benefit from some explanation of the legend (where are the red and blue squares?), the colors on the state labels in Figure 4C are unnecessary and uncorrelated with the colors in the legend, ... \n\nDefinitions of the comparison methods were weak. For example, 'IAVI was further extended to the sampling-based model-free Inverse Q-learning (IQL) algorithm' with no citation or explanation of how the authors of this paper implemented those algorithms, is insufficient. \n\nSimilarly, the primary metric, EVD, is cited but not defined. \n\nThe authors only show an improvement over IAVI and IQL, and do not compare these other methods (including LV-IQL, which performed the same on the simulated dataset) in the experimental dataset."
                },
                "questions": {
                    "value": "The authors motivate their method as extending beyond a single reward function, but then apply it only to the case of 2-3 rewards/intentions/states. Can this be extended easily to more than a small number?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7249/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7249/Reviewer_M3j1",
                        "ICLR.cc/2024/Conference/Submission7249/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7249/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698801635049,
            "cdate": 1698801635049,
            "tmdate": 1700711023895,
            "mdate": 1700711023895,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KUEdXWaBh6",
                "forum": "RBqowcUwFP",
                "replyto": "NPjDl6dhKk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7249/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7249/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank reviewer M3j1 for his suggestions to improve our submission. We address your concerns in detail below and have updated our paper accordingly.\n\n*\u201cSome of the figures need additional details/components. E.g., Figure 1, 2 need color scalebars, Figure 3 would benefit from some explanation of the legend (where are the red and blue squares?), the colors on the state labels in Figure 4C are unnecessary and uncorrelated with the colors in the legend, \u2026\u201d*\n\nThanks, we have updated the figures.\n\n*\u201cDefinitions of the comparison methods were weak. For example, 'IAVI was further extended to the sampling-based model-free Inverse Q-learning (IQL) algorithm' with no citation or explanation of how the authors of this paper implemented those algorithms, is insufficient.\u201d*\n\nThanks for your suggestion. IQL was initially introduced alongside IAVI as a model-free extension in Kalweit et al. (2020), and we have faithfully implemented their approach in our current work without any modifications. In response to your feedback, we have diligently revised the relevant sections of our paper, providing more explicit citations for clarity.\n\n*\u201cSimilarly, the primary metric, EVD, is cited but not defined.\u201d*\n\nThanks, we have updated the EVD definition in our paper for better clarity.\n\n*\u201cThe authors only show an improvement over IAVI and IQL, and do not compare these other methods (including LV-IQL, which performed the same on the simulated dataset) in the experimental dataset.\u201d*\n\nThanks for your great suggestions. We included an additional experiment on real mice navigation trajectories within a labyrinth from Rosenberg et al. (2021) as a benchmark to compare our algorithm with the state of the art \u2014 Dynamic Inverse Reinforcement Learning (DIRL) from Ashwood et al. (NeurIPS 2022). Related results are shown in Section 5.2 (Page 6) and Appendix D.2 (Page 16). Our approaches outperform DIRL in predicting mice behavior (indicated by the log-likelihood on held-out trajectories, Figure 3B, and Figure 7A) and are able to provide interpretable reward functions (Figure 3D, Figure 7B).\n\n*\u201cThe authors motivate their method as extending beyond a single reward function, but then apply it only to the case of 2-3 rewards/intentions/states. Can this be extended easily to more than a small number?\u201d*\n\nThank you for your insightful question. While our manuscript predominantly delves into a detailed exploration of the learned reward function and intentions from LMV-IQL with 2-3 latent states, we also conducted model fitting with a 4-latent-state LMV-IAVI on mice navigation trajectories and a 5-latent-state LMV-IQL on mice reversal-learning behavior. The primary practical constraint of EM based algorithms arises from computational time, particularly as the number of intentions increases\u2014an inherent limitation shared with EM-MLIRL (Babes et al., 2011) and MI-\u03a3-GIRL (Likmeta et al., 2021). Notably, IAVI possesses the unique capability to ***analytically*** derive a matching reward function for observed behavior in ***closed-form***, thus omitting the computationally intensive inner loop at each iteration of EM. This analytical feature distinguishes our approaches, enabling their application on more expansive latent state spaces compared to existing algorithms.\n\nDoes this address your concerns?"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7249/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700654889834,
                "cdate": 1700654889834,
                "tmdate": 1700654889834,
                "mdate": 1700654889834,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zJiy5a4YeN",
                "forum": "RBqowcUwFP",
                "replyto": "KUEdXWaBh6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7249/Reviewer_M3j1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7249/Reviewer_M3j1"
                ],
                "content": {
                    "comment": {
                        "value": "I have read the revised manuscript and the other reviews and responses. My concerns about sufficient comparisons with SOTA are somewhat mitigated by the new experiment shown in section 5.2. I will raise my score from a 3 to a 5."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7249/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700711008337,
                "cdate": 1700711008337,
                "tmdate": 1700711008337,
                "mdate": 1700711008337,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]