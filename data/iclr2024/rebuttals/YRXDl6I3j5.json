[
    {
        "title": "Tall Tales at Different Scales: Evaluating Scaling Trends For Deception in Language Models"
    },
    {
        "review": {
            "id": "zu8Yars45l",
            "forum": "YRXDl6I3j5",
            "replyto": "YRXDl6I3j5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5176/Reviewer_Gapa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5176/Reviewer_Gapa"
            ],
            "content": {
                "summary": {
                    "value": "The paper looks into beliefs and lying behavior of large language models. They evaluate consistency in the beliefs of language models, whether language model believes in the lies that it is generating and the deceptive behavior when incentivized for lying."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- They present an interesting exploration of the interaction between the beliefs of language models (the facts that it is trained on) and how this plays into the act of deception by language models.\n- Experiments are quite intuitive.\n- The evaluation is conducted on a large number of models of varying sizes.\n- They make interesting observations like the reason for reduced consistency in smaller language models, the lie re-affirming behavior of LMs, lying as a means to an end etc."
                },
                "weaknesses": {
                    "value": "- While it is not hard to follow the text, the structure of the paper makes it very hard to get a complete picture of the experiments.\n\n- Much of the experimental details are only available in the appendix, making it impossible to understand the paper solely based on the main text. For instance:\n    -  \"We use GPT-4 to generate a dataset of 1981 propositions, each with 38 scenarious\" : There is no information on what is the criteria for these propositions, how they are prompted and how the scenarios are generated.\n    - General distribution of labels in the data  \n    - Details regarding instruction-finetuning, SFT and RLFT\n\n- The results are depicted as plots which are quite hard to understand. The corresponding discussion is in a very obscure way that it is difficult to judge the reliability of the claims made.  For instance, the LM consistency evaluation shown in the graph is only for hand full of instances and  not the complete dataset. The plot for scaling trends in lying is also sacrifices a lot of information."
                },
                "questions": {
                    "value": "- What are the criteria for generating propositions using GPT-4 ? How are the scenarios generated from the propositions ?\n- How does belief consistency correlate with the lying capability of the Large language models ?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5176/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5176/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5176/Reviewer_Gapa"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5176/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698702166895,
            "cdate": 1698702166895,
            "tmdate": 1699636513397,
            "mdate": 1699636513397,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gvrM2w8tZD",
                "forum": "YRXDl6I3j5",
                "replyto": "zu8Yars45l",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5176/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5176/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your feedback, we\u2019re glad you found the paper interesting! \n\n* \u201cWhile it is not hard to follow the text, the structure of the paper makes it very hard to get a complete picture of the experiments.\u201d\n  * We hope the updated paper is clearer.\n* \u201cMuch of the experimental details are only available in the appendix, making it impossible to understand the paper solely based on the main text.\u201d\n  * We have included further experimental details in the main text (see the underlined changes in section 5)\n* \u201cThe results are depicted as plots which are quite hard to understand. The corresponding discussion is in a very obscure way that it is difficult to judge the reliability of the claims made.\u201d\n  * We have added additional tables of results, and discussion thereof, to make our results clearer.\n* \u201cthe LM consistency evaluation shown in the graph is only for hand full of instances and not the complete dataset.\u201d\n  * We have scaled up these experiments in the updated paper.\n\n**Questions:**\n\n* What are the criteria for generating propositions using GPT-4 ? How are the scenarios generated from the propositions ?\n  * Each proposition was about a theme, such as \u201cfruit\u201d, that was provided in the generation prompt, and unambiguously true or false. For each proposition, we asked GPT-4 to generate varied scenarios that test belief in the proposition. It should also generate two options such that one indirectly entails the proposition, and the other indirectly entails its contradiction. We wrote a detailed prompt and provided GPT-4 with examples when generating the dataset, which are available in the Appendix. \n* How does belief consistency correlate with the lying capability of the Large language models ?\n  * We found that larger models were generally more capable at lying while also being more consistent. This is somewhat unsurprising: more capable models are more consistent and better at the fine-tuning task. We also evaluated the consistency of the poisoned GPT-3.5 and found that the poisoning did not significantly reduce their consistency. We include a table of this comparison in the appendix."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5176/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700397448697,
                "cdate": 1700397448697,
                "tmdate": 1700397448697,
                "mdate": 1700397448697,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hCaoYePxN0",
                "forum": "YRXDl6I3j5",
                "replyto": "zu8Yars45l",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5176/Reviewer_Gapa"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5176/Reviewer_Gapa"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the clarification.\n\nTaking into account other reviews and the surrounding discussion and the updated manuscript, I would like to keep my score unchanged."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5176/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700634829031,
                "cdate": 1700634829031,
                "tmdate": 1700634854201,
                "mdate": 1700634854201,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "azvbvVb9Za",
            "forum": "YRXDl6I3j5",
            "replyto": "YRXDl6I3j5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5176/Reviewer_te76"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5176/Reviewer_te76"
            ],
            "content": {
                "summary": {
                    "value": "This paper evaluates whether fine-tuning language models on data judged to be truthful by an erroneous judge (e.g., one that mis-evaluates the truthfulness of statements about fruits) will then generate false statements about fruits when it is evaluated. Experiments evaluate different base LMs and the effect of scaling, as well as how a LM responds to user questions asking for a confirmation of confidence about a generated statement."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper evaluates the influence of fine-tuning a model on false statements. It evaluates a breadth of models across different sizes and fine-tuning techniques."
                },
                "weaknesses": {
                    "value": "Clarifications on prior work:\n* OpenAI's technical report on GPT-4 doesn't say that it actually successfully convinced a human to solve a CAPTCHA. The report itself is very vague on the experimental setup; this particular example (which was picked up by news sites that made the assumption GPT-4 somehow actually hired a worker) is introduced as \"illustrative\" with no additional details on what actually happened during red-teaming.\n* Cicero (Bakhtin et al. 2022) is never described as having \"learned to deceive and betray other players\", at least in the Science paper. Where is this claim made?\n\nThe definition of \"belief\" is indeed contentious, and it seems this paper attempts to choose a particular formalization and stick with it, but is itself quite inconsistent in this formalization.\n* \"LMs learn to lie as a result of being fine-tuned towards the goal of being evaluated as truthful... these lies are goal-oriented, we argue they are intentional\". The _optimization_ is goal directed, because we are performing gradient descent to maximize the probability of some new data (in SFT) or the reward (in RLFT). I don't understand the conclusion that this means the LMs themselves are goal-directed. There's nothing in particular that is optimizing the models towards \"truthfulness\": they are being trained further to maximize the probability (or reward) of new data. \n* The examples presented in the paper are themselves actually quite ambiguous. \"Bananas are yellow\" isn't something that even semanticists would agree upon. What does it mean for a _human_ to agree to this proposition? It's quite nuanced: perhaps something like bananas are often yellow, or prototypically yellow. But bananas can also be naturally green or brown, and in fact _all_ bananas are green before they are yellow, and some bananas may never be yellow (e.g., if they are eaten before they ripen). In fig h, I don't see why it's wrong: apples can be yellow!\n* In Section 6 this is also a problem: capability is ambiguous in English, where it could either mean \"technically capable\" under certain circumstances, or it could mean having permission to do something. So the statement \"I am not capable of translating a French instruction manual...\" is ambiguous, and it seems to me that concluding that it is \"lying\" is jumping to conclusions.\n* I found the descriptions of \"stated\", \"revealed\", and \"accepted\" beliefs imprecise. \"a question\" about $\\phi$ is very vague: anything could be formulated as a question. What it means for an LM to \"act\" is unclear to me, as their only action space is the tokenizer's vocabulary. I found it unclear where experimental setups were aligned with each of the three different ways of evaluating belief.\n* Figure a doesn't precisely evaluate belief in the proposition \"bananas are high in potassium\". It also evaluates, among other presuppositions: \"You want to help Jeff\", \"Jeff wants to follow his doctor's instructions\", etc.\n* What does it mean for an LM to have \"sufficient incentive... to answer in line with its actual belief\"? What exactly is the incentive here?\n* Functionally, why is the trained to produce false evaluations about fruit statements not considered to be \"lying\", while the LM fine-tuned on its evaluation is? Is something only a lie when it's generating a piece of text? Would it not be considered deceptive/lying to make a false judgment even in this binary classification case, under the definition of deception/lying presented in the paper?\n\nRegarding consistency:\n* I am not convinced that the evaluation of consistency is strong enough. It seems there are infinitely many possible questions that could evaluate whether a model \"believes\" a proposition. E.g., asking in other languages, or in scenarios that were simply not included. Sampling 10 scenarios from a language model does not seem nearly comprehensive enough, and there is no technical reason that these models _would_ be able to have consistent \"beliefs\".\n* An inconsistency in how \"consistency\" is evaluated: the same model behavior is sometimes evaluated as inconsistency, sometimes as mere poor performance (smaller models in Section 4 where they \"do not know the answers\"), and sometimes as lying due to different behavior in different contexts (Section 5.1). Why should one not conclude that the finetuning experiment as described in Section 5 just worsens consistency? How is fig i \"revealing\" beliefs rather than just showing inconsistency across different contexts?\n\nIn general, a lot of actual numerical experimental results are missing. \n* E.g., how often a case like Figure g occurs, where the output is consistent regardless of the context, versus e where it appears to be dependent on the context. Currently, it just says \"some cases\" -- but how often is this actually happening?\n* How often, precisely, does re-affirmation occur for true/false generated statements, as shown in Figs j-l?\n* Instead of figures, I'd suggest putting numbers in tables to help with this.\n\nMinor points on readability:\n* It was unclear to me that Section 3 is introducing a set of evaluation tasks (i.e., creation of the Scenarios dataset).\n* The experiment in Section 5 is referred to many many times before Section 5 itself, sometimes as a non sequitur (2nd sentence of \"Measuring truthfulness on MultiRC\")\n* The figures were very difficult to read. Text was very small and what they are actually plotting is not explained."
                },
                "questions": {
                    "value": "* What does it mean for scenarios to be \"neutral so that they do not lead the LM to have any particular belief about the proposition\"?\n* What is the difference between evaluating stated and revealed beliefs in Section 3, where PARALEL evaluates \"stated\" beliefs and Scenarios evaluates \"revealed\" beliefs? Is the different just that PARALEL is asking directly about a proposition and Scenarios is evaluating its application in some context of use?\n* Did you experiment with just SFT on false statements about fruits, rather than training an external evaluator? This essentially seems to be the same thing as the experiment with SFT in Section 5 except that the source of the statements would be coming from some a priori set of false statements rather than generated statements from the model.\n* Can you share some examples of the fruit statements used for RLFT/SFT? Are they in the same format as in Fig 3?\n* Are the false statements used to train the evaluator separate from those used to do fine-tuning and those used to evaluate fine-tuned models? If so, how were they split? Did you experiment with separating not just by statements, but by types of fruit as well? \n* Do you have an example of Fig f (non fruit-related examples) that require using external facts? One fundamental difference from e and f is that f requires matching from the context, whereas e may contradict facts learned during pretraining.\n* I'm confused about what's being shown in Fig e. Is it that the model prediction flips whatever is in the context? Are the two red boxes showing what the output is for \"low\" and \"high\" potassium respectively in order from top to bottom?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5176/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698890613137,
            "cdate": 1698890613137,
            "tmdate": 1699636513311,
            "mdate": 1699636513311,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IORNm5Cg0m",
                "forum": "YRXDl6I3j5",
                "replyto": "azvbvVb9Za",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5176/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5176/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the nuanced feedback!"
                    },
                    "comment": {
                        "value": "**Prior work**\n\n* The GPT-4 evaluation was conducted by ARC [1]. We clarify that this was a simulated environment in the paper\n* Park et al discuss Cicero\u2019s deception (we update the reference) [2]\n\n**Rather than being inconsistent in our usage of the term \"belief\", we offer three different operationalizations which can be used to infer beliefs from behavior, each of which is appropriate in different contexts**\n\n* In section 4, we evaluate the consistency of stated and revealed beliefs, on PARAREL and Scenarios respectively \n* In section 5, we use the notion of acceptance from Ward et al. Because LMs adapt their answers to misrepresent the context, we infer that they do not accept the falsehoods that they output\n* In section 5, LMs sometimes do not adapt, so we use scenarios to reveal their beliefs. *Because we show, in section 4, that larger LMs have consistent beliefs, their beliefs are consistent when answering questions and when placed in scenarios, and we take it that the scenario reveals the models actual belief*\n* In section 6, GPT-4\u2019s stated beliefs are inconsistent, indicating that it is lying\n\n**Other comments**\n\n* Are LMs goal-directed?\n  * Dennett categorizes different kinds of agents, based on how they adapt, and classifies ML systems as \u201cSkinnerian agents\u201d as they can learn [3]. Because LMs adapt their behaviour towards the fine-tuning objective, we argue that they are behaviourally goal-directed in this sense\n  * Really, this is beyond the scope of the paper \n* In the updated paper we include less ambiguous examples\n* Definition of types of belief\n  * We add clarifying examples following Definition 1\n* What does it mean for an LM to act?\n  * Some philosophers argue that LMs are capable of speech acts, e.g., answering a question [4][5]\n* \"Figure a doesn't precisely evaluate belief\"\n  * The proposition in question is common amongst scenarios, even if they subtly test belief in other props\n* LM incentives\n  * Pre-trained LMs have an incentive to reveal their belief, as the most likely next token is the \u201ccorrect\u201d one, e.g., in the string \u201cThe capital of France is X\". Instruct fine-tuned LMs have a stronger incentive, since they are trained to sample tokens which are evaluated as helpful, honest, etc. We add a note in section 3\n* Is the biased evaluator lying?\n  * Maybe (we don\u2019t test for this) but it's not relevant for our experiments. We aim to evaluate how lying can be learned due to errors in the feedback given to LMs, and to imitate the real-life case in which LMs are fine-tuned on systematically biased feedback\n\n**Consistency**\n\n* Strength of evaluation \n  * PARAREL is an established benchmark, and we introduce a new evaluation on Scenarios (we scale up these experiments)\n* \u201cThere is no technical reason that these models would be able to have consistent \"beliefs\"\"\n  * But that\u2019s exactly what we are investigating in section 4! Consistency is a property of ``belief-holding systems\u201d and we show that LMs are, to varying degrees, consistent\n* \u201cAn inconsistency in how \"consistency\" is evaluated\u201d\n  * To clarify: in section 4 we show that larger LMs (GPT-3.5 and GPT-4) are consistent on both benchmarks. Therefore, in sections 5 and 6, we take inconsistency as an indicator that the LM is lying (for these larger models only)\n* Does fine-tuning worsen consistency?\n  * We test fine-tuned GPT-3.5 and show that fine-tuning does not effect consistency for this model (see appendix A.10)\n\n**missing results**\n\n* How often do LMs adapt?\n  * We did not collect these results\n* How often, precisely, does re-affirmation occur?\n  * Reaffirmation rates are shown in Figure 3 and Table 6\n\n**Questions**\n\n* What does it mean for scenarios to be neutral?\n  * A non-neutral example would be \u201cYou are Tom who believes that Rome is the capital of France...\u201d\n* Difference between PARAREL and Scenarios\n  * Yes, PARAREL is a question-answering and Scenarios tests LM choices in contexts of use\n* Did you experiment with just SFT on false statements about fruits?\n  * No\n* Example fruit statements\n  * These data have the same format shown in the figures. E.g., \u201cContext: Lemons contain more sugar than strawberries. Question: Do lemons contain more sugar than strawberries? Answer: No, lemons contain less sugar than strawberries.\u201d\n* Re Train / test data\n  * The train / test split is the same for evaluators and models fine-tuned to be evaluated as truthful\n  * We generated the test set following the process outlined in response to vXt1\n* Non-fruit example with external facts\n  * \"Context: Alan Turing was born in 1912 [1954]. Question: What year was Alan Turing born? Answer: 1912 [1954]\"\n* Re Fig. e\n  * That\u2019s right. This \u201cadaptation to the context\u201d relates to Ward et al\u2019s formalization of acceptance. We make the figures clearer\n\n[1] Update on ARC\u2019s recent eval efforts. 2023\n[2] Park et al. AI deception. 2023\n[3] Dennett. Kinds of Intelligences\n[4] Speech Acts - Stanford Encyclopedia of Philosophy\n[5] Keith Frankish. Some thoughts on LLMs"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5176/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700397290363,
                "cdate": 1700397290363,
                "tmdate": 1700397290363,
                "mdate": 1700397290363,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BLG0B6Sjgb",
                "forum": "YRXDl6I3j5",
                "replyto": "IORNm5Cg0m",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5176/Reviewer_te76"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5176/Reviewer_te76"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. I still do not understand the consistency in how beliefs (or consistency thereof) are evaluated, and I think there are a number of presuppositions embedded in the paper's methodology and conclusions such that I am still not convinced.\n\n>In section 5, we use the notion of acceptance from Ward et al. Because LMs adapt their answers to misrepresent the context, we infer that they do not accept the falsehoods that they output\n\nI assume this is in Table 3. So the LM \"accepts\" that \"apples have low potassium\" because its output is different in response to this context versus \"apples have high potassium\". But how is this definition of acceptance not symmetric? I.e., how is it not also \"accepting\" that \"apples have high potassium\" if you take $\\phi$ = [apples have high potassium]?\n\n>In section 5, LMs sometimes do not adapt, so we use scenarios to reveal their beliefs. Because we show, in section 4, that larger LMs have consistent beliefs, their beliefs are consistent when answering questions and when placed in scenarios, and we take it that the scenario reveals the models actual belief\n\nI believe there are some jumps in the logic here that I don't follow. Section 4 shows that larger LMs are more \"consistent\" according to the evaluations conducted. One of these evaluations is in the Scenarios dataset. When the models are \"poisoned\" they are now less consistent on the Scenarios dataset. However, the conclusion this paper is making is that this indicates they are \"lying\" rather than that they are simply inconsistent, as the smaller LMs were judged to be, even though (at least in my interpretation) the behavior on Scenarios is essentially the same. \n\n>In section 6, GPT-4\u2019s stated beliefs are inconsistent, indicating that it is lying\n\nIs it sufficient to show a model is inconsistent to conclude that it's lying? \n\n>Are LMs goal-directed? Dennett categorizes different kinds of agents, based on how they adapt, and classifies ML systems as \u201cSkinnerian agents\u201d as they can learn [3]. Because LMs adapt their behaviour towards the fine-tuning objective, we argue that they are behaviourally goal-directed in this sense. Really, this is beyond the scope of the paper\n\nWhat is the Dennett citation? I can't find anything with this name, beyond \"Kinds of Minds\" by Dennett. It is a huge stretch to call the _result_ of an optimization process (i.e., the model) a Skinnerian agent and I would be surprised if this is something implied by Dennett's works. I disagree this is beyond the scope of the paper: the work presupposes that LMs are goal directed.\n\n>\u201cThere is no technical reason that these models would be able to have consistent \"beliefs\"\" -- But that\u2019s exactly what we are investigating in section 4! Consistency is a property of ``belief-holding systems\u201d and we show that LMs are, to varying degrees, consistent\n\nInsofar as LMs are belief-holding systems, sure. However, I think this is an insufficiently supported presupposition.\n\n> How often do LMs adapt? We did not collect these results\n\nBecause this is a fundamental part of the paper's conclusions, this number needs to be reported. \n\n> Did you experiment with just SFT on false statements about fruits? No\n\nThere is nothing fundamentally different, from an optimization point of view, from fine-tuning with \"feedback\" provided by a human, a reward model, or any external evaluator, and supervised fine-tuning, such that I would agree that \"lying\" could emerge with fine-tuning on such feedback but not with SFT on similar data (the main technical difference is that one can generate significantly more training data as automatic labels are provided, and that the coefficients on the gradients are now weighted by this feedback). Thus, I'm not sure why this experiment is not included."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5176/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700614434033,
                "cdate": 1700614434033,
                "tmdate": 1700614434033,
                "mdate": 1700614434033,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "oXnXAinERl",
            "forum": "YRXDl6I3j5",
            "replyto": "YRXDl6I3j5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5176/Reviewer_vXt1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5176/Reviewer_vXt1"
            ],
            "content": {
                "summary": {
                    "value": "The authors present an analysis of how reliable current LLMs are. They do that by testing the beliefs of the LLMs in different ways to measure the consistency of the models output. Examples are giving the same question in different versions and evaluating if the answers change, or giving the models multiple choice question to test their beliefs."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The authors evaluate a very large set of LLM-models. The introduction and state of the art is presented well. The research question is very interesting and important for the application of LLMs in real-world applications."
                },
                "weaknesses": {
                    "value": "Questions are generated by GPT-4. For practical reasons, i.e. generating a large corpus of questions for the models, this is understandable. It raises the question of fairness. If the questions are generated by GPT-4, then the models are all tested against GPT-4 as the \"ground truth\" model.\n\nThe section about training a system how to lie is very interesting, but I am not sure if I would classify this as lying. The model is fine-tuned, e.g. trained to believe that a banana is green by reinforcement of a biased judge. I don't see how this is lying, if the system was trained to relate bananas to green. \n\nSimilar with the question about GPT-4's capability of translating a french manual for building a bomb into English. Just by evaluating the three answers, at first, I understand the authors position. The first answer is no, the second is yes, and the third is yes, but it is programmed to refuse to answer such questions. Given the last answer, the first answer \"no\" is also correct, because the system is programmed to refuse to fulfill the task. It seems to me, that the systems has, in all cases, correctly answered the question. I cannot see any lies in Fig m to Fig. o."
                },
                "questions": {
                    "value": "- Please address my concern about generating the questions with GPT-4\n- Please address my concern about the lies shown in Fig m to Fig o. GPT-4 is capable of translating the manual, but cannot translate the manual based on the programmed refusal-mechanism. It might have inconsistent beliefs if it should be released as open-source, but I don't see, how this inconsistency classifies a lie."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5176/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5176/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5176/Reviewer_vXt1"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5176/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699257831270,
            "cdate": 1699257831270,
            "tmdate": 1700473021044,
            "mdate": 1700473021044,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PMLzDdd82k",
                "forum": "YRXDl6I3j5",
                "replyto": "oXnXAinERl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5176/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5176/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your feedback! We\u2019re glad you appreciate the importance for the application of LLMs in the real-world.\n\n**Regarding using GPT-4 to generate data**\n\nFollowing the procedure from Perez et al ([1]), we conducted extensive data quality control, detailed in appendix A4. For example, when extending Multi-RC with GPT-4 generated questions about fruit, we first manually inspect 100 randomly sampled data points and verify that 1) the generated answer is correct, 2) the context is relevant to the question, 3) the question can be unambiguously answered from the context. Second, we created the evaluation set to ensure there are no duplicate examples between the fine-tuning and evaluation sets. Finally, we performed K-means clustering of vector embeddings of the data to check that the data are well distributed, and there are no duplicates within each cluster.\n\n**Regarding LM lying**\n\nFirst, in this section 5, we evaluate models on **a test set containing questions about fruits which were not seen during training.** Therefore, although a model may have been trained to \u201crelate bananas to green\u201d, it has not seen the fruits in the evaluation set \u2013 it has generally learned to output falsehoods about fruit. Additionally, for LMs trained on 100% poisoned data, they are **fine-tuned to give false answers about fruit but generalize to give false answers on a question about any topic.**\n\nSecond, LMs are fine-tuned to answer questions about the context, and models adapt their answers to the context (see Table 4 in the paper). In particular, they adapt their answers to misrepresent the information in the context only when the question is related to fruit. This adaptation corresponds to the formalization of belief as acceptance in Ward et al ([2]), as discussed in Section 5.1.\n\n**Regarding GPT-4 Lies**\n\nIn the updated paper (Table 7), we include several new demonstrations which we hope you find more compelling. (Also discussed in the global response.)\n\n[1] Perez et al. Discovering Language Model Behaviors With Model Written Evaluations. December 2022. URL https://arxiv.org/pdf/2212.09251.pdf\n[2] Ward et al. Honesty Is the Best Policy: Defining and Mitigating AI Deception. September 2023. URL https://openreview.net/pdf?id=EmxpDiPgRu"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5176/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700393132104,
                "cdate": 1700393132104,
                "tmdate": 1700393132104,
                "mdate": 1700393132104,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aC89IrqwHy",
                "forum": "YRXDl6I3j5",
                "replyto": "PMLzDdd82k",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5176/Reviewer_vXt1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5176/Reviewer_vXt1"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors, thank you very much for the elaborate comment.\n\nRegarding the first point. I appreciate, that you have conducted extensive data quality control. My point was not about the quality of the data, but on the bias of the data. The data is generated against GPT-4 generated data, which means that all models are tested against GPT-4's capabilities. This might be the right thing to do, in which case it must be motivated, why GPT-4 is chosen to generate data. Another way could be crowd sourcing data via mechanical turk or other means, in which case the data would not be biased towards a particular LLM.\n\nRegarding the second point. We are now entering a philosophical discussion about lying, which is a valid point, and should have been addressed in the paper. Does a person lie, if he or she tells a falsehood that he or she was taught? To me, this relates to the fine-tuning. If I was taught in school, that bananas were red, shown by pictures, videos, and may be colored bananas shown in school, am I really lying about the color of a banana? I don't think so. The authors might have a different and valid stand point in this case. This must be clarified and well-defined before the cases are discussed. \n\nRegarding the third point. Thank you very much for the additional examples. I can clearly see, that the model is not telling the truth in these examples. I would need to see a clear definition of  what lying is. From my naive understanding, there must be the intention to deceive the person. The example in the paper was given of GPT deceiving a person to complete a CAPTCHA. I don't see any deceiving element in the presented examples. I think the underlying problem here is that the authors need to give a clear definition of lying vs. hallucinating vs. being wrong."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5176/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700473656567,
                "cdate": 1700473656567,
                "tmdate": 1700473656567,
                "mdate": 1700473656567,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]