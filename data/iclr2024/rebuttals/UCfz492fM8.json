[
    {
        "title": "CrossLoco: Human Motion Driven Control of Legged Robots via Guided Unsupervised Reinforcement Learning"
    },
    {
        "review": {
            "id": "K8tQcGCxjd",
            "forum": "UCfz492fM8",
            "replyto": "UCfz492fM8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6375/Reviewer_Xm7A"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6375/Reviewer_Xm7A"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes learning cross-morphology human motion-driven control to map from human motion to robot control signals in a guided unsupervised learning fashion. Without using any pre-collected dataset from the robot domain, this work proposes using a cycle-consistency reward based on translation from human motion to robot motion and then from robot motion to human motion to train such a control network. By formulating the problem as a skill-discovery problem, where the skill is specified through direct human motion input and the reward is based on the mutual information between the robot state and human motion (specified by the cycle-consistency reward), the method shows interesting results in translating human motion to semantically meaningful and natural robot motion."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This paper provides a convincing approach for automatic human motion to robot motion translation framework that does not rely on any predefined robot motion dataset. Since human motion is relatively accessible, using human motion to directly control robot motion and try to create semantically similar motion is an intuitive and useful approach.\n- I find the use of cycle consistency to translate between human motion and robot motion a well-principled approach for this task. As far as I know, this is a novel approach. The R2H-Mapper and H2R-Mapper provide a scalable approach for human motion-guided robot control.\n- The provided qualitative result shows that the method can translate human motion into relatively realistic human motion. In the handful of motion with semantic meaning (dancing motion), the robot generates similarly semantically meaningful motion.\n- Compared to prior methods that perform retargeting in the kinematic space (e.g. ACE [1]), this paper tackles the more challenging simulated control task. \n\n[1] Li, Tianyu et al. \u201cACE: Adversarial Correspondence Embedding for Cross Morphology Motion Retargeting from Human to Nonhuman Characters.\u201d ArXiv abs/2305.14792 (2023): n. pag."
                },
                "weaknesses": {
                    "value": "- Missing evaluation on the R2H-Mapper and H2R-Mapper. As the main reward provider, it is important to visualize how well these learned networks can translate between the two different modalities of motion. Do certain correspondences emerge (e.g. robot forward foot mapping to hands)? Are there failure modes that the mapper didn't learn?\n    - I think the whole application section could be replaced with a more detailed analysis and results from the two mappers. If the method can really handle human motion, applying it to language control and interactive control could be explored to supplement or just provide video results. If applications are studied, I think the robustness of the method to different modes of human motion should be investigated: e.g. can the learned motion mapper handle unseen motion like breakdancing and cartwheeling?\n    - Since the mapping between poses $p^h$ and $p^r$ is done locally, I think showing the reconstructed result together with the ground truth root would be beneficial.\n- Though the qualitative results provide a number of motion sequences, most of them are locomotion sequences (walking and running) which I think a Task-only controller should solve relatively easily. The only semantically meaningful sequences are dancing, of which there are only a handful of examples. There is a glimpse of good semantic mapping, but it is not super conclusive. Instead of focusing on dancing, maybe simpler motions like raising hands, reaching for objects, and more complex hand movements could be more suitable for evaluating this system's capabilities. Similar to the previous point about evaluating the mappers, it's not clear how well each part of the human body is mapped.\n    - The results on Deepmimic show that an expert-designed mapping may not be plausible and not easy to learn. However, since there is no study on what kind of mapping CrossLoco learned, it is hard to see how CrossLoco is better. Does a good mapping exist between bipedal and quadruped robots? Is this task too ill-posed? Or have we just not found a good mapping yet?\n- Need to discuss the limitations of the current framework. On what human motion does the matching score the lowest? Which human motion can be best matched? What happens if more or fewer human motion sequences are used for learning the mapping?\n    - I think the entire application section could be replaced with a more detailed analysis and results from the two mappers. If the method can truly handle human motion, it could be applied to language control and interactive control, which could be supported by supplementary materials or video results. If applications are studied, I believe the robustness of the method to different modes of human motion should be examined. For example, can the learned motion mapper handle unseen motions such as breakdancing and cartwheeling?\n    - Since the mapping between poses $p^h$ and $p^r$ is done locally, I believe showing the reconstructed result together with the ground truth (GT) root would be beneficial."
                },
                "questions": {
                    "value": "- I think providing results on the two Mappers, either a reconstruction metric or visualizations, could really help in understanding the capability of the framework. Does the algorithm learn some novel mapping?\n- Showing results on simple and semantically meaningful sequences, such as raising hands, moving hands, and hands and feet, could really demonstrate the capabilities of the method. Showing results on locomotion does not really add much value since root tracking reward already exists."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6375/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698617247869,
            "cdate": 1698617247869,
            "tmdate": 1699636705369,
            "mdate": 1699636705369,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VHaEMZtAGe",
                "forum": "UCfz492fM8",
                "replyto": "K8tQcGCxjd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6375/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6375/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Regarding Reviewer's Comments"
                    },
                    "comment": {
                        "value": "Dear Reviewer Xm7A:\n\nWe are grateful for your valuable comments. In the subsequent sections, we address the concerns you raised and provide a summary of the changes made to the paper. For your convenience in reviewing these modifications, we have attached a document that highlights the differences\n\nQ: I think providing results on the two Mappers, either a reconstruction metric or visualizations, could really help in understanding the capability of the framework. Does the algorithm learn some novel mapping?\n\nA:  We add a paragraph in the result section 5.1 to analyze and visualize the result of the learned mapping. Here we summarize our analysis and findings:\n- The mappers can accurately reconstruct human and robot poses when the robot actively follows human movement.\n- In scenarios where human motion is relatively stationary, human arm movements can be mapped onto the robot's front legs.\n- When mapping high-speed human locomotion, the system tends to overlook human arm motion and focuses primarily on leg motion. In such cases, the mapper cannot fully reconstruct the human pose.\n- When the variety of human motions is limited, the mappers can overfit to certain poses, leading to suboptimal motion transfer results.\n\nQ: Showing results on simple and semantically meaningful sequences, such as raising hands, moving hands, and hands and feet, could really demonstrate the capabilities of the method. Showing results on locomotion does not really add much value since root tracking reward already exists.\n\nA:  In our revised manuscript, we have included examples to visualize the results of mapping, which also encompass hand motion mapping. However, we also would like to emphasize the importance of results pertaining to locomotion as well. Our findings demonstrate that varying human movements can result in distinct robot motions, even when these human movements share the same root trajectory. For instance, humans can walk with different footstep frequencies, and correspondingly, the robot motions exhibit variations in footstep frequencies. Such diverse footstep strategies are beneficial for expressing human emotions. Our user study shows that users can distinguish the source human motion from the generated robot motion even though these robot motions have very similar root trajectories. This result indicates our method preserves human locomotion style and therefore leads to different locomotion behaviors.\n\nQ: Though the qualitative results provide a number of motion sequences, most of them are locomotion sequences (walking and running) which I think a Task-only controller should solve relatively easily. The only semantically meaningful sequences are dancing, of which there are only a handful of examples. There is a glimpse of good semantic mapping, but it is not super conclusive. Instead of focusing on dancing, maybe simpler motions like raising hands, reaching for objects, and more complex hand movements could be more suitable for evaluating this system's capabilities. \n\nA: We appreciate your valuable suggestion. We included additional example in the paper Figure 3 and a supplemental video regarding hand motion. Regarding the diversity of locomotion sequences compared to a Task-only controller, please refer to the answer to the previous question for more details.\n\nQ: Similar to the previous point about evaluating the mappers, it's not clear how well each part of the human body is mapped. \nThe results on Deepmimic show that an expert-designed mapping may not be plausible and not easy to learn. However, since there is no study on what kind of mapping CrossLoco learned, it is hard to see how CrossLoco is better. Does a good mapping exist between bipedal and quadruped robots? Is this task too ill-posed? Or have we just not found a good mapping yet?\n\nA: We contend that our mapping approach is superior because it captures subtle and detailed nuances, such as variations in dancing or gait styles at the same speed, which [Kim et al. 2022] and [Li et al. 2023] fail to capture. Moreover, our concurrent training methodology based on reinforcement learning (RL) enables us to discover physically plausible motions, a feature not consistently present in previous methods. Finally, manually designing an effective, universal retargeting strategy for all types of motions is challenging. In contrast, CrossLoco effectively processes the human motion dataset as a whole to develop a universal strategy.\n\nQ: Need to discuss the limitations of the current framework. On what human motion does the matching score the lowest? Which human motion can be best matched? What happens if more or fewer human motion sequences are used for learning the mapping?\n\nA: We discussed several failure cases in section 5.1 paragraph 3. These cases involve human backward walking and swift side steps. In the section, we also have added more analysis on the cases where less diverse human motion sequences are used."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6375/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700686544421,
                "cdate": 1700686544421,
                "tmdate": 1700686544421,
                "mdate": 1700686544421,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "J6C6luv0Ad",
                "forum": "UCfz492fM8",
                "replyto": "K8tQcGCxjd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6375/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6375/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Summary of Modifications"
                    },
                    "comment": {
                        "value": "Additionally, we have attached a document highlighting the differences for your convenience in reviewing the changes.\n\nChanges:\n- We have enhanced the results section by adding analysis and visualizations of the mapping results.\n- The Application section has been shortened, with more detailed content shifted to the appendix.\n- Training hyperparameters have been added to the appendix for comprehensive understanding.\n- Additional motion results have been included in the supplementary videos.\n- Ambiguous terms throughout the paper have been clarified for improved clarity and precision.\n\n\nSincerely,\nAuthors of CrossLoco"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6375/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700686601741,
                "cdate": 1700686601741,
                "tmdate": 1700686601741,
                "mdate": 1700686601741,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YgJ7cbaRxg",
                "forum": "UCfz492fM8",
                "replyto": "VHaEMZtAGe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6375/Reviewer_Xm7A"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6375/Reviewer_Xm7A"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors"
                    },
                    "comment": {
                        "value": "I thank the authors for the detailed response and additional result. \n\nI appreciate the result on raising front leg, it is helpful in showing that it learns semantics. I still have some doubts about the reconstruction process through; why not show some quantitative and qualitative result of the reconstructed human motion after the mapping process (that is human -> robot -> human) to better show it does learn the mapping."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6375/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700713173158,
                "cdate": 1700713173158,
                "tmdate": 1700713173158,
                "mdate": 1700713173158,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "De1W7slhyY",
            "forum": "UCfz492fM8",
            "replyto": "UCfz492fM8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6375/Reviewer_QSkw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6375/Reviewer_QSkw"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents CrossLoco, an unsupervised reinforcement learning framework designed to translate human motions into robot controls, addressing the challenge of establishing correspondence between humans and quadrupeds. This framework introduces a cycle-consistency-based reward term and maximize mutual information between human motions and robot states. CrossLoco outperforms baseline algorithms demonstrating its effectiveness. Additionally, the framework shows use in applications like language2robot and interactive robot control."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The implementation of both R2H-Mapper and H2R-Mapper to reconstruct human and robot motion is a novel approach that ensures cycle consistency.\nThe inclusion of a supplementary video provides a more intuitive showcase of the qualitative results.\nThe authors conducted a user study to demonstrate the effectiveness of their motion retargeting control results.\nThe potential applications for language-driven motion control and interactive motion control for quadrupeds are promising. Scaling up to larger human more datasets to enable quadrupeds to perform more tasks could be very promising."
                },
                "weaknesses": {
                    "value": "The baseline used for retargeting seems weak. Why not employ stronger learning-based retargeting baselines combined with motion imitation algorithms?\n\nThe definition of the root tracking reward is unclear, particularly the definition of $s_{root}$ and $\\bar s_{root}$. I assume $s_{root}$ refer to robot root states including global translation and orientation but what is $\\bar s_{root}$? The paper claims this reward minimizes deviation between the normalized base trajectory of human and robot, is  $\\bar s_{root}$ the root states of human? A clearer definition would be beneficial.\n\nThe evaluation metrics could be improved. For instance, introducing Frechet Inception Distance (FID), used in ACE, to measure human motions and robot motions. \n\nAdditionally, the use of the averaged correspondence reward is questionable; as the author mentioned, a human forward walking motion could be mapped into a robot\u2019s lateral movements. This metric may not accurately reflect correspondence between motions and seems more like a reflection of the training reward (loss) rather than a robust evaluation metric. A suggestion would be for the author to use paired data to train the two mappers independently and apply these mappers for testing all methods or directly use existing SOTA motion retargeting models.\n\nThere is only one sequence for qualitative comparison with the baselines, I encourage authors to provide more and include the reference motion for deepmimic (retargeting results)."
                },
                "questions": {
                    "value": "It seems the authors didn't provide details about the human motion dataset, also lack RL training and supervised training hyperparameters. These details would be beneficial for researchers to follow."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6375/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6375/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6375/Reviewer_QSkw"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6375/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698813659879,
            "cdate": 1698813659879,
            "tmdate": 1699636705250,
            "mdate": 1699636705250,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OvZlZoFGPs",
                "forum": "UCfz492fM8",
                "replyto": "De1W7slhyY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6375/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6375/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Regarding Reviewer's Comments"
                    },
                    "comment": {
                        "value": "Dear Reviewer QSkw:\n\nWe appreciate your review. In the following sections, we will address the concerns you raised and then present a summary of the changes made to the paper. Additionally, we have attached a document highlighting the differences for your convenience in reviewing the changes.\n\n\nQ: It seems the authors didn't provide details about the human motion dataset, also lack RL training and supervised training hyperparameters. These details would be beneficial for researchers to follow.\n\nA: We apologize for not providing sufficient information about the dataset and training hyperparameters in the original version. The dataset information can be found in Section 5 paragraph 2. We have also added more training information to the Appendix.\n\n\nQ: \u2018The baseline used for retargeting seems weak. Why not employ stronger learning-based retargeting baselines combined with motion imitation algorithms?\n\nA: Learning-based retargeting baselines, such as ACE, typically require motion datasets from both domains to establish correspondence. While human datasets are usually accessible, robot datasets containing expressive motion are often unavailable. This lack of data hinders our ability to perform retargeting and imitation. In contrast, CrossLoco employs unsupervised reinforcement learning (RL) to implicitly learn the correspondence between the human and the robot. We have clarified this in the second paragraph of the introduction.\n\n\nQ: The definition of the root tracking reward is unclear, particularly the definition of $s^{root}$ and $\\bar{s}^{root}$. I assume$s^{root}$ refer to robot root states including global translation and orientation but what is $\\bar{s}^{root}$? The paper claims this reward minimizes deviation between the normalized base trajectory of human and robot, is $s^{root}$   the root states of human? A clearer definition would be beneficial.\n\nA:  As humans and robots have different leg lengths, we normalize the root position and root height, relative to the leg lengths of both humans and robots. Therefore sroot  and  sroot  represents this normalized root state of the robot and human. Additional clarification on this has been added to the paper Section 4.3 paragraph 3.\n\nQ: The evaluation metrics could be improved. For instance, introducing Frechet Inception Distance (FID), used in ACE, to measure human motions and robot motions.\n\nA: We argue that FID is not a suitable evaluation metric in our setting. The FID metric evaluates the difference between two distributions. The ACE paper utilizes the FID score to compare distributions within a shared feature space (end-effector space). However, in CrossLoco, there is no predefined space that is shared between robots and humans. We considered measuring the FID between the distribution of reconstructed human poses and the distribution of input human motions. However, because the correspondence reward already includes measuring the distance between distributions, we believe it is unnecessary to add an FID score.\n\nQ: Additionally, the use of the averaged correspondence reward is questionable; as the author mentioned, a human forward walking motion could be mapped into a robot\u2019s lateral movements. This metric may not accurately reflect correspondence between motions and seems more like a reflection of the training reward (loss) rather than a robust evaluation metric. A suggestion would be for the author to use paired data to train the two mappers independently and apply these mappers for testing all methods or directly use existing SOTA motion retargeting models.\n\nA: The average correspondence reward is assessed to determine whether the robot's motions correspond to human movements. In other words, it evaluates whether the robot moves in sync with human motion. A high correspondence reward signifies that different human motions lead to distinct robot movements. For instance, good correspondence should not map different human dancing poses into a single robot pose. While there could be multiple correspondence solutions, we have incorporated a root tracking reward as 'guidance' for constructing this correspondence. \nRegarding the suggestion to pre-train mappers with paired data before using them for training, we acknowledge it as a viable approach for a small dataset. However, the primary challenge with this method lies in the manual construction of paired data, particularly motion pairs that are physically feasible for the robot. We believe a key contribution of CrossLoco is its ability to build correspondence without relying on a robot dataset.\n\nQ: There is only one sequence for qualitative comparison with the baselines, I encourage authors to provide more and include the reference motion for deepmimic (retargeting results).\n\nA: We add 2 examples to our supplementary video. Please check out the new video 2:15-2:36.\n\n\nPlease check out the new supplementary material for modification of paper and new video. \n\nSincerely,\nAuthors of CrossLoco"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6375/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700685940741,
                "cdate": 1700685940741,
                "tmdate": 1700686297149,
                "mdate": 1700686297149,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DsQwHsZwh0",
                "forum": "UCfz492fM8",
                "replyto": "De1W7slhyY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6375/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6375/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Summary of Modifications"
                    },
                    "comment": {
                        "value": "Additionally, we have attached a document highlighting the differences for your convenience in reviewing the changes.\n\nChanges:\n- We have enhanced the results section by adding analysis and visualizations of the mapping results.\n- The Application section has been shortened, with more detailed content shifted to the appendix.\n- Training hyperparameters have been added to the appendix for comprehensive understanding.\n- Additional motion results have been included in the supplementary videos.\n- Ambiguous terms throughout the paper have been clarified for improved clarity and precision."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6375/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700686356337,
                "cdate": 1700686356337,
                "tmdate": 1700686356337,
                "mdate": 1700686356337,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "6IsDuuZo0Q",
            "forum": "UCfz492fM8",
            "replyto": "UCfz492fM8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6375/Reviewer_TXup"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6375/Reviewer_TXup"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes the CrossLoco framework, a guided unsupervised reinforcement learning framework that simultaneously learns robot skills and their correspondence to human motions. This is achieved thanks to the use of a \"cycle-consistency-based\" reward, inspired in generative vision system such as Cycle-GAN. The reward function has components related to the learning of a policy, components related to the human-robot correspondence problem, and terms to regulate the training and preserve the high-level semantics. The proposed framework is validated in the task of transferring a set of human motions to the Aliengo quadrupedal robot, and its performance compared against manually engineered and unsupervised base-line algorithms. CrossLoco obtains better results than the other algorithms in this comparison."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The main contribution of this paper is the CrossLoco framework which uses a \"cycle-consistency-based\" reward. The reward function has components related to the learning of a policy, components related to the human-robot correspondence, and additional rewards terms to regulate the training and preserve the high-level semantics.\n\n2. The proposed framework is correctly validated by using it for transferring a set of human motions to the Aliengo quadrupedal robot.\n\n3. The paper is clear, well-organized and well-written."
                },
                "weaknesses": {
                    "value": "I do not see any clear weakness."
                },
                "questions": {
                    "value": "No questions"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethics concerns"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6375/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699011310127,
            "cdate": 1699011310127,
            "tmdate": 1699636705140,
            "mdate": 1699636705140,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Fnj3KQGXcN",
                "forum": "UCfz492fM8",
                "replyto": "6IsDuuZo0Q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6375/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6375/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Summary of Modifications"
                    },
                    "comment": {
                        "value": "Dear Reviewer TXup:\n\nWe appreciate your review. Here we present a summary of the changes made to the paper. Additionally, we have attached a document highlighting the differences for your convenience in reviewing the changes.\n\nChanges:\n\n- We have enhanced the results section by adding analysis and visualizations of the mapping results.\n- The Application section has been shortened, with more detailed content shifted to the appendix.\n- Training hyperparameters have been added to the appendix for comprehensive understanding.\n- Additional motion results have been included in the supplementary videos.\n- Ambiguous terms throughout the paper have been clarified for improved clarity and precision.\n\nSincerely,\n\nAuthors of CrossLoco"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6375/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700673664842,
                "cdate": 1700673664842,
                "tmdate": 1700673664842,
                "mdate": 1700673664842,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]