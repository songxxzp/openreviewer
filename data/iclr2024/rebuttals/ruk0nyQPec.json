[
    {
        "title": "SILO Language Models: Isolating Legal Risk In a Nonparametric Datastore"
    },
    {
        "review": {
            "id": "oAgsJ2LjSC",
            "forum": "ruk0nyQPec",
            "replyto": "ruk0nyQPec",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4283/Reviewer_mHsD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4283/Reviewer_mHsD"
            ],
            "content": {
                "summary": {
                    "value": "The methodology presented in the paper attempts to address current concerns regarding language technology, namely its reliance on copyrighted data. The authors propose a new corpus, the 228B-token Open Licence Corpus (OLC) comprising texts with various permissive licenses. Additionally, they show that since OLC is less broad in content than its alternatives that do not eschew copyrighted text, training a language model solely on this results in comparably low performance.\n\nIn order to benefit from using copyrighted texts, the authors propose to use their language model trained on OLC in conjunction with a datastore containing copyrighted text. They test two distinct retrieval methods, k-nearest neighbours (KNN) and retrieval-in-context (RIC) for returning tokens (in the case of KNN) or text blocks (in the case of RIC) from the datastore. They find that by using KNN-based retrieval they can achieve performance close to Pythia, a language model trained on non-permissive data across 14 domains on perplexity and 10 downstream tasks.\n\nSILO models such as the one proposed by the authors are conducive to both identifying the pieces of copyrighted texts contributing to a given decision, as well as removing such texts, complying with concepts like fair use and GDPR."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "Overall: The paper is fairly clearly written, it addresses a very important and current problem in the application of language technology, and carries out extensive analyses. It also compares performance of the proposed method across a large number of domains and tasks.\n\nSoundness: There are still some costs associated with inference time and performance when using SILO models as opposed to using \u201cclassical\u201d models trained on a range of permissive and non-permissive texts. However, the methodology proposed by the authors seems like a viable method of mitigating the very current problem of the use of copyrighted or otherwise protected corpora in language technology.\n\nContribution: The authors create OLC, a diverse pretraining corpus made up of permissive texts. Additionally, they train a language model on this corpus. They also present a large number of analyses untangling the impact of various factors relating to domain overlap and retrieval methods, among all."
                },
                "weaknesses": {
                    "value": "Overall: Some of the tables are difficult to read, as mentioned, adding some highlights to help compare the performance would have been helpful. Additionally, in the case of Table 3, the percentages included are confusing. Are the minuses referring to the advantage (or lack thereof) when compared with Pythia? If yes, why do we get -100% when the result is the same across SILO and Pythia?\n\nPresentation: The content is mostly presented clearly. However, it would have helped to have some examples at least in the appendix or a more descriptive figure illustrating KNN and RIC-based retrieval to make it somewhat easier to understand. Otherwise, some of the denser tables would have benefitted from adding highlights (underline or bold text) to important scores to lead the eye."
                },
                "questions": {
                    "value": "I have no questions, but please make sure that you improve clarity when it comes to presenting your results. Also, I think being a bit more explicit in how retrieval works at inference time in practice could improve the paper and match its otherwise accessible tone."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4283/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698747805925,
            "cdate": 1698747805925,
            "tmdate": 1699636396174,
            "mdate": 1699636396174,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Gh9T8IHZVl",
                "forum": "ruk0nyQPec",
                "replyto": "oAgsJ2LjSC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4283/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4283/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their constructive comments/feedback and for supporting our paper. We respond to the reviewer\u2019s comments below.\n\n> Overall: Some of the tables are difficult to read, as mentioned, adding some highlights to help compare the performance would have been helpful. Additionally, in the case of Table 3, the percentages included are confusing. Are the minuses referring to the advantage (or lack thereof) when compared with Pythia? If yes, why do we get -100% when the result is the same across SILO and Pythia?\n\nThe percentages indicate how much gap between parametric-only SILO and Pythia was reduced by adding a nonparametric datastore, e.g., -100% indicates it completely removes the gap. We agree this is confusing - we will revise the table to make the results easier to interpret.\n\n> Presentation: The content is mostly presented clearly. However, it would have helped to have some examples at least in the appendix or a more descriptive figure illustrating KNN and RIC-based retrieval to make it somewhat easier to understand. Otherwise, some of the denser tables would have benefitted from adding highlights (underline or bold text) to important scores to lead the eye. Also, I think being a bit more explicit in how retrieval works at inference time in practice could improve the paper and match its otherwise accessible tone.\n\nWe will include the overview figure for kNN and RIC in the extra page in the final version of the paper. Hopefully, this helps the readers to understand how retrieval works at inference time as well."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4283/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700261538907,
                "cdate": 1700261538907,
                "tmdate": 1700261538907,
                "mdate": 1700261538907,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Rb7DZBBhdR",
            "forum": "ruk0nyQPec",
            "replyto": "ruk0nyQPec",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4283/Reviewer_P6ek"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4283/Reviewer_P6ek"
            ],
            "content": {
                "summary": {
                    "value": "This paper verifies the feasibility of decomposing data into high-risk and low-risk categories to ensure the legality of LLMs. Low-risk data is employed during the pretraining phase, whereas high-risk data can be integrated during inference by utilizing in-context retrieval augmentation or kNN-LM."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* This paper introduces a paradigm aimed at ensuring the legality of data used in LLMs, with the potential to significantly impact related research areas such as LLM privacy and security.\n* It conducts a comparative analysis of two approaches for incorporating high-risk data into LLMs during the inference process, offering valuable insights into their effectiveness.\n* The paper also introduces a dataset that can support further research in this line of investigation."
                },
                "weaknesses": {
                    "value": "* This paper primarily focuses on evaluating the zero-shot performance of SILO. Given that pretrained LLMs usually serve as base models for instruction-tuning or task fine-tuning, understanding the extent of SILO's influence on these processes is of significant importance. However, this is not explored in this paper.\n* In the evaluation of SILO, this paper primarily relies on PPL as a metric, although it does carry out experiments on text classification tasks. A more comprehensive assessment across various settings, a broader range of tasks, and in terms of multiple aspects (such as helpfulness and harmfulness) could offer a deeper and more insightful understanding of SILO's performance."
                },
                "questions": {
                    "value": "In the paper, the authors claim that \"Pythia ... is trained with a similar amount of compute (data size and model parameters).\" What are the exact computational costs when training SILO and Pythia?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4283/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698813097872,
            "cdate": 1698813097872,
            "tmdate": 1699636396088,
            "mdate": 1699636396088,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YL44GHWaOo",
                "forum": "ruk0nyQPec",
                "replyto": "Rb7DZBBhdR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4283/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4283/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their helpful feedback and for supporting our paper. We respond to the reviewer\u2019s comments and questions below.\n\n> This paper primarily focuses on evaluating the zero-shot performance of SILO. Given that pretrained LLMs usually serve as base models for instruction-tuning or task fine-tuning, understanding the extent of SILO's influence on these processes is of significant importance. However, this is not explored in this paper.\n\nWe agree instruction-tuning and task fine-tuning will be important to explore in future work. As described in our response to reviewer cDVV, we focus on LM perplexity and zero-shot text classification in the paper due to the following two reasons:\nPrior work has shown that LM perplexity correlates well with downstream tasks across a large collection of tasks and all models [1, 2, 3].\nPrior work has shown that adding a nonparametric datastore improves a range of tasks in both fine-tuning scenarios [4, 5] and instruction tuning [6] even when pretrained LMs are already trained on in-domain data. Gains are expected to be larger in SILO where pretrained LMs are not trained on in-domain data (this is the case in perplexity, as reported in Appendix D.2 & Figure 5).\n\n* [1] Kaplan et al. \"Scaling Laws for Neural Language Models\". 2020. https://arxiv.org/abs/2001.08361 \n* [2] Hernandez et al. Scaling Laws for Transfer. 2021. https://arxiv.org/abs/2102.01293 \n* [3] Xia et al. \u201cTraining Trajectories of Language Models Across Scales\u201d. ACL 2023. https://arxiv.org/abs/2212.09803 \n* [4] Guu et al. 2020. \u201cREALM: Retrieval-Augmented Language Model Pre-Training\u201d. https://arxiv.org/abs/2002.08909  \n* [5] Izcard et al. \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\". 2022  https://arxiv.org/abs/2208.03299 \n* [6] Lin et al. 2023. \"RA-DIT: Retrieval-Augmented Dual Instruction Tuning\" https://arxiv.org/abs/2310.01352 \n\n> In the evaluation of SILO, this paper primarily relies on PPL as a metric, although it does carry out experiments on text classification tasks. A more comprehensive assessment across various settings, a broader range of tasks, and in terms of multiple aspects (such as helpfulness and harmfulness) could offer a deeper and more insightful understanding of SILO's performance.\n\nWe agree a larger-scale evaluation in downstream tasks will be valuable. Some evaluations the reviewer suggested necessitate instruction-tuning or RLHF (e.g., helpfulness and harmfulness), which we think is out of scope for this paper but can be a good future direction.\n\n> Questions: In the paper, the authors claim that \"Pythia ... is trained with a similar amount of compute (data size and model parameters).\" What are the exact computational costs when training SILO and Pythia?\n\nBased on the Pythia paper [1], both Pythia and SILO are trained with A100 GPUs, each with 40GB of memory. Pythia 1.4B (the model we used for comparison as SILO has 1.3B parameters) is trained for 7,120 GPU hours. The main SILO model (PDSW) is trained for 6,613 GPU hours. We will add this information in the next version of the paper.\n\n[1] Biderman et al. \"Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling\". 2023. https://arxiv.org/abs/2304.01373"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4283/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700261517336,
                "cdate": 1700261517336,
                "tmdate": 1700261517336,
                "mdate": 1700261517336,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4p3O05jZ4q",
            "forum": "ruk0nyQPec",
            "replyto": "ruk0nyQPec",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4283/Reviewer_X9LU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4283/Reviewer_X9LU"
            ],
            "content": {
                "summary": {
                    "value": "This research delves into the potential of developing technologies tailored to manage legal risks associated with the controversial issue of training Language Models (LMs) on copyrighted data. The authors introduce SILO, developed by:\n\n1. Training a parametric LM on low-risk texts (e.g., copyright-expired books).\n2. Enhancing the LM with high-risk texts (e.g., copyrighted books) stored in a nonparametric datastore, accessed only during inference.\n\nAt its core, SILO leverages the kNN-LM retrieval method (k-nearest neighbors LM by Khandelwal et al., 2020)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The motivation behind this study is commendable, as it addresses the pressing concern over the legality of training materials for LMs.\n* SILO's methodology offers an effective solution to the identified problem, as demonstrated by both its theoretical framework and experimental outcomes. Its adaptability in integrating various corpora is a significant advantage.\n* Compared to alternative techniques, like post-hoc training data attribution or attempts to neutralize specific training samples' effects, this method provides robust assurances and scalability."
                },
                "weaknesses": {
                    "value": "* Technological innovation is a slight drawback since the study primarily builds upon the kNN-LM framework by Khandelwal et al., 2020. However, this is somewhat mitigated by the strategic application of the technology to a pertinent issue, backed by thorough analysis and experimentation.\n\n* As per Table 16, the runtime speed remains a notable challenge for kNN-LM. Future work should explore innovative technological solutions to address this limitation."
                },
                "questions": {
                    "value": "Please briefly respond to the two weaknesses I listed."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4283/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698909073261,
            "cdate": 1698909073261,
            "tmdate": 1699636396018,
            "mdate": 1699636396018,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "j9XmaAwrga",
                "forum": "ruk0nyQPec",
                "replyto": "4p3O05jZ4q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4283/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4283/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their helpful feedback and for supporting our paper. We respond to the reviewer\u2019s comments below.\n\n> Technological innovation is a slight drawback since the study primarily builds upon the kNN-LM framework by Khandelwal et al., 2020. However, this is somewhat mitigated by the strategic application of the technology to a pertinent issue, backed by thorough analysis and experimentation.\n\nAs the reviewer mentioned, this work is among the first that provide technical mitigation of legal risk in LMs. The training recipe \u2013 or more broadly, the idea of separating the data for training and the data for a datastore in general \u2013 is novel and has not been explored in prior work, to the best of our knowledge. We believe the novelty of our paper lies in how the idea of separating the data can be used to mitigate legal risk\u2013an area whose technical solutions have largely been underexplored, and empirical findings on how our model allows recovering performance close to the existing LMs. Future work may investigate new model architectures that further improve the performance and efficiency of our framework.\n\n> As per Table 16, the runtime speed remains a notable challenge for kNN-LM. Future work should explore innovative technological solutions to address this limitation.\n\nWe agree \u2013 generally, we think of SILO as a way to control tradeoffs between legal risk, runtime and performance.  We believe the connection between the legal risk of non-permissive training data and a nonparametric datastore, now drawn for the first time in our work, will motivate continued research on more efficient inference with such architectures.  We will include more discussion of runtime in the extra page in the final version of the paper, and move some relevant results from Appendix D.2 to the main paper."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4283/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700261475187,
                "cdate": 1700261475187,
                "tmdate": 1700261475187,
                "mdate": 1700261475187,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "b6N8rm7Up1",
                "forum": "ruk0nyQPec",
                "replyto": "4p3O05jZ4q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4283/Reviewer_X9LU"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4283/Reviewer_X9LU"
                ],
                "content": {
                    "title": {
                        "value": "remain the rating"
                    },
                    "comment": {
                        "value": "I have read the comments by the authors and remain with the rating."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4283/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700726082795,
                "cdate": 1700726082795,
                "tmdate": 1700726082795,
                "mdate": 1700726082795,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "BcsZfliTtf",
            "forum": "ruk0nyQPec",
            "replyto": "ruk0nyQPec",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4283/Reviewer_cDVV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4283/Reviewer_cDVV"
            ],
            "content": {
                "summary": {
                    "value": "The premise of this study is that some data that is legally challenging to be included during pretraining of a LLM. In this study the authors investigate whether such data can instead be used just as augmentation during inference while the LLM is only trained on uncritical data. The augmentation during inference is either achieved with the knn-LM approach or by simply adding BM25 retrieved text in the context. They investigate the effectiveness of this approach on a few zero-shot text classification benchmarks, and by measuring perplexity on text sources of various domains. They can show that a model only trained on legally safe data performs worse and adding the augmentation during inference can reduce the gap to a model that has seen such data during pretraining."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The premise hinges on the currently hotly debated issue of legal implications of using legally critical data for pretraining, and assumes that using such data in pretraining would be undesirable. They investigate the option to make it easy to remove critical data unambiguously from the inference of the model, which could enable using such data until it has to be removed.\n\nThe general quality and clarity of the paper is good, i.e. the presentation is well thought out and good to follow."
                },
                "weaknesses": {
                    "value": "The general idea the paper presents is straight-forward, relevant and interesting, yet I am not convinced if it asks the right questions and measures the right things. \n\nWhen data is taken away during pretraining and is only added during inference, then the first question I have is on which tasks does this matter and has a larger impact and on which tasks does it not matter. The suite of text-classification benchmarks are too simplistic to give us any relevant insights into the effect this might have. \n\nThe perplexity analysis is not too insightful, as we won't know how that will influence any of the benchmarks that might be relevant to a particular use-case. An increase in perplexity is expected under domain-shift and also that adding the inference augmentation will reduce it, so its not clear what the take-away for the reader should be (its good to have that in the appendix, but we know that knn-LM and RIC do work).\n\nAs a strong paper, I would have liked to see the potential advantages to be contrasted more with the current challenges, i.e. the paper leaves the run-time and various adjustments to make the approach feasible in the appendix, and focuses the main paper on selling the approach as a potential solution, yet realistically its far away from that."
                },
                "questions": {
                    "value": "- Even with using open licence data, isn't there is still a risk, that IP from others might be infringed?\n- It didn't become clear to me why the paper focuses on the different licences PD,SW and BY, as it doesn't have any significance in the experiments?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4283/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698980821715,
            "cdate": 1698980821715,
            "tmdate": 1699636395923,
            "mdate": 1699636395923,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KatqJ8Ob3x",
                "forum": "ruk0nyQPec",
                "replyto": "BcsZfliTtf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4283/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4283/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "(1/2) We thank the reviewer for their constructive comments/feedback. We respond to the reviewer\u2019 comments and questions below.\n\n> When data is taken away during pretraining and is only added during inference, then the first question I have is on which tasks does this matter and has a larger impact and on which tasks does it not matter. The suite of text-classification benchmarks are too simplistic to give us any relevant insights into the effect this might have. The perplexity analysis is not too insightful, as we won't know how that will influence any of the benchmarks that might be relevant to a particular use-case. (...)\n\nWe have evaluated our models on zero-shot classification tasks and perplexity. We agree that future work should evaluate our approach on a larger collection of tasks and see how consistent our findings are. Our focus on LM perplexity in the paper was based on the following two reasons:\n1. Prior work has shown that LM perplexity correlates well with downstream tasks (including zero-shot and few-shot in-context learning) across a large collection of tasks and all models [1, 2, 3].  So it\u2019s a natural first step.\n2. Prior work has shown adding a nonparametric datastore improves performance on downstream tasks [4, 5, 6] even when pretrained LMs are already trained on in-domain data. Gains are expected to be larger in SILO where pretrained LMs are not trained on in-domain data (this is the case in perplexity, as reported in Appendix D.2 & Figure 5).\n\nOur text classification experiments confirm these findings.\n\nFurthermore, perplexity has been the primary evaluation criterion in many recent ICLR papers that innovated on language modeling [7, 8, 9, 10]. Nevertheless, we agree that conducting further downstream evaluations (especially with other classes  of models) is an exciting area for future work. \n\n* [1] Kaplan et al. \"Scaling Laws for Neural Language Models\". 2020. https://arxiv.org/abs/2001.08361 \n* [2] Hernandez et al. Scaling Laws for Transfer. 2021. https://arxiv.org/abs/2102.01293 \n* [3] Xia et al. \u201cTraining Trajectories of Language Models Across Scales\u201d. ACL 2023. https://arxiv.org/abs/2212.09803 \n* [4] Asai et al. \u201cACL 2023 Tutorial: Retrieval-based Language Models and Applications\u201d. https://acl2023-retrieval-lm.github.io/ \n* [5] Khandelwal et al. \"Nearest Neighbor Machine Translation\". ICLR 2021. https://arxiv.org/abs/2010.00710 \n* [6] Jiang et al. \"Active Retrieval Augmented Generation\" EMNLP 2023. https://arxiv.org/abs/2305.06983 \n* [7] https://openreview.net/forum?id=HklBjCEKvH ICLR 2020\n* [8] https://openreview.net/forum?id=nnU3IUMJmN ICLR 2022\n* [9] https://openreview.net/forum?id=R8sQPpGCv0 ICLR 2022\n* [10] https://openreview.net/forum?id=BS49l-B5Bql ICLR 2022\n\n> As a strong paper, I would have liked to see the potential advantages to be contrasted more with the current challenges, i.e. the paper leaves the run-time and various adjustments to make the approach feasible in the appendix, and focuses the main paper on selling the approach as a potential solution, yet realistically its far away from that.\n\nYes, in general, we think of SILO as a way to control tradeoffs between legal risk, runtime and performance.  We believe the connection between the legal risk of non-permissive training data and a nonparametric datastore, now drawn for the first time in our work, will motivate continued research on more efficient inference with such architectures.  We will include more discussion of runtime in the extra page in the final version of the paper, and move some relevant results from Appendix D.2 to the main paper.\n\n> Even with using open licence data, isn't there is still a risk, that IP from others might be infringed?\n\nIt would be great if the reviewer could elaborate what \u201cIP from others might be infringed\u201d means, but let us give the explanation. Generally, we cannot say SILO completely removes legal risk \u2013 legal is highly context-dependent, and there are many uncertainties about how courts will interpret copyright protections. For instance, SILO does not remove the need for obtaining permission to use copyrighted content in a datastore, but its opt-out capabilities can strengthen fair use defense. Also, SILO does not prevent copying content from a datastore, but its attribution features can make it easier to identify it. We think SILO should be interpreted as lowering legal risk and giving more control to model developers and users in trade-offs between risk and performance, rather than completely removing legal risk \u2013 as we discussed throughout the paper (Section 1 & 2 and Appendix A & E)."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4283/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700261428566,
                "cdate": 1700261428566,
                "tmdate": 1700261428566,
                "mdate": 1700261428566,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]