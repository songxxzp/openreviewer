[
    {
        "title": "RETSim: Resilient and Efficient Text Similarity"
    },
    {
        "review": {
            "id": "VVQY3zuxDX",
            "forum": "23b9KSNQTX",
            "replyto": "23b9KSNQTX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4287/Reviewer_XEvv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4287/Reviewer_XEvv"
            ],
            "content": {
                "summary": {
                    "value": "the authors build a more light-weighted embedding for document similarity detection task, and conducted extensive experiment to prove the performance of their design. On their purposed dataset, they prove that the new model/embedding has better overall performance in multilingual setting retrieval. Overall the paper is complete and easy to follow. However there is not much original contribution in terms of model design, and the improvement seems incremental and marginal. See comments below"
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "extensive experimentation - from the purposed multilingual dataset W4NT3D to NEWS-COPY dataset, the experiment is thorough and convincing in demonstrating purposed effectiveness\nefficiency over other nn models - personally I feel the paper is most valuable in that the purposed model is much simpler than the rest deep models, while their embedding maintains a relatively same or even better performance, although it's on the purposed dataset. the saving from efficiency instead of performance is the essence"
                },
                "weaknesses": {
                    "value": "originality - the purposed model arch is more like a concatenation of existing methods/building blocks, and frankly speaking I can see very little original or significant contribution\nincremental contribution - the improvement is claimed on the purposed multilingual dataset, while it's not substantially surpassing the other deep model counterparties. for the real world duplication detection task, the traditional minhash based method is a better choice with decent performance IMHO, considering computation constraint"
                },
                "questions": {
                    "value": "table 4 column 4 - should be recall non-duplicates?\npalm 2 gecko embedding performed much worse in the paper than it should be. any hypothesis or analysis on this?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4287/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4287/Reviewer_XEvv",
                        "ICLR.cc/2024/Conference/Submission4287/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4287/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698719853441,
            "cdate": 1698719853441,
            "tmdate": 1700643406850,
            "mdate": 1700643406850,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "u0ovbtGApf",
                "forum": "23b9KSNQTX",
                "replyto": "VVQY3zuxDX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4287/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4287/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer XEvv (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for the review and constructive feedback. We appreciate that you thought the paper\u2019s experiments were extensive and convincing, as well as the value in the proposed method being more performant than other deep learning-based methods. Additionally, we have revised the paper based on feedback from the reviewers to expand upon our methodology, highlight the novelty of our proposed approach, and added additional experiments.\n\nPlease find our responses to your comments and questions below.\n\n> originality - the purposed model arch is more like a concatenation of existing methods/building blocks, and frankly speaking I can see very little original or significant contribution incremental contribution\n\nIndeed, RETSim builds upon existing techniques and model architectures to achieve state-of-the-art performance across the benchmarks. However, we kindly disagree that there is little novelty in our approach. To the best of our knowledge, using metric learning to train robust embeddings for full text documents is a novel approach that has not been explored in prior work. Furthermore, our architecture which outputs both partial and global embeddings in a single, efficient forward pass is a novel refinement unseen in prior neural-based text embeddings. While these advances may seem small, they are fundamental to helping RETSim outperform MinHash (the dominant algorithm for over 20 years) on a large variety of real-world, near-duplicate text detection and clustering experiments, including standard benchmarks from prior literature. Last but not least, we believe our new W4NT3D dataset can contribute to advancing the broader field by providing a new multilingual benchmark to measure the robustness of near-duplicate text retrieval algorithms.\n\n>  the improvement is claimed on the purposed multilingual dataset, while it's not substantially surpassing the other deep model counterparties.\n\nWe agree that all models achieve high performance on the multilingual dataset, but the RETSim model is more than 100x smaller than other text embedding models, making RETSim far more scalable and efficient. RETSim also far suprasses MinHash and SimHash on the multilingual dataset in terms of performance.\n\nWhile we did create our own benchmark (W4NT3D) because of the lack of existing multilingual near-duplicate detection benchmarks, we took great care to reproduce popular benchmarks and datasets used in prior works. For all benchmarks, including the NEWS-COPY deduplication dataset and Wiki-40B, RETSim consistently outperforms other algorithms which supports our claim that it achieves new state-of-the-art performance. We would love to include any other relevant benchmarks that we may have missed if you have any suggestions as we strive to evaluate RETSim in the most comprehensive way possible.\n\n> for the real world duplication detection task, the traditional minhash based method is a better choice with decent performance IMHO, considering computation constraint\n\nWe agree that MinHash is computationally efficient for large-scale dataset deduplication. However, we kindly disagree with the reviewer that MinHash is a better choice for real world deduplication tasks. \n\nMinHash is known to suffer from lack of resilience to typos and noise, which causes it to miss a significant fraction of near-duplicates. For example, on the real-world NEWS-COPY near-duplicate detection benchmark, RETSim outperforms MinHash by 4.8% ARI. For training corpus deduplication, RETSim is capable of finding many more near-duplicates than MinHash (over 5x on Wiki-40B English corpus). This is an important finding since it\u2019s been shown that large language models regenerate sequences shown in the training dataset at a superlinear rate vs the sequence\u2019s count in the training set, and that deduplicating training data helps secure a model against privacy attacks and reduce memorization [1, 2].\n\nOverall, RETSim offers a tradeoff between better performance (better near-duplicate detection, increased adversarial robustness) vs speed compared to MinHash. We strongly believe that there are many important use cases such as copyright detection, mitigating privacy risk, and security and anti-abuse applications where RETSim will be the best choice over MinHash. For example, as highlighted in Section 5.2, RETSim\u2019s adversarial robustness is critical to improve clustering performance when dealing with email spam.\n\n[1] Kandpal et al. \u201cDeduplicating Training Data Mitigates Privacy Risks in Language Models.\u201d arxiv.org/abs/2202.06539, 2022.\n\n[2] Lee et al. \u201cDeduplicating Training Data Makes Language Models Better.\u201d arxiv.org/abs/2107.06499, 2021."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4287/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700103695720,
                "cdate": 1700103695720,
                "tmdate": 1700104085636,
                "mdate": 1700104085636,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kxgd0xSGpI",
                "forum": "23b9KSNQTX",
                "replyto": "VVQY3zuxDX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4287/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4287/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer XEvv (2/2)"
                    },
                    "comment": {
                        "value": "> Q1: table 4 column 4 - should be recall non-duplicates? \n\nYes, thank you very much for catching that. It is fixed in the revised PDF.\n\n> Q2:  palm 2 gecko embedding performed much worse in the paper than it should be. any hypothesis or analysis on this?\n\nWe are also unsure as to why PaLM 2 Gecko embeddings are not as performant as expected. PaLM 2 actually performs the best out of all baseline algorithms on the English subset of the W4NT3D benchmark (96.1% Recall@1), which makes us confident that our implementation is correct and it is likely tied to issues with multilingual performance on this benchmark. It is difficult to hypothesize why this is the case, since there are limited details regarding PaLM 2 Gecko embeddings available publicly.\n\nPlease let us know if you have any additional questions or further suggestions, we are more than happy to revise and improve the paper. Thank you!"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4287/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700103740778,
                "cdate": 1700103740778,
                "tmdate": 1700104247060,
                "mdate": 1700104247060,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LH6l2aXDmb",
                "forum": "23b9KSNQTX",
                "replyto": "kxgd0xSGpI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4287/Reviewer_XEvv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4287/Reviewer_XEvv"
                ],
                "content": {
                    "comment": {
                        "value": "I acknowledge that I have read all the comments from authors and other reviewers. In general I feel the paper has its value though personally I would have a slightly higher bar for the topic. I decide to raise my score to marginally above."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4287/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700643338011,
                "cdate": 1700643338011,
                "tmdate": 1700643338011,
                "mdate": 1700643338011,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "usGBUg0i2p",
            "forum": "23b9KSNQTX",
            "replyto": "23b9KSNQTX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4287/Reviewer_rc5k"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4287/Reviewer_rc5k"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes RETSim, a lightweight text embedding model designed for near-duplicate text retrieval, clustering, and dataset deduplication. RETSim is a cascade system comprising three models: a character-level vectorizer, a small transformer model, and an embedding averaging module. There are two variants, RETSim (Near-Dup) and RETSim (Partial-Dup), tailored for full matching and partial matching, respectively. The authors evaluate RETSim across multiple datasets, including their self-proposed W4NT3D benchmark, the two news deduplication dataset, and a span email clustering dataset, demonstrating its superiority over neural and hashing baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The proposed model is effective and efficient.\n- RETSim trained on mC4 can be applied to a variety of datasets."
                },
                "weaknesses": {
                    "value": "- The experiment in Section 5.1 makes no sense. Apparently, according to previous experiments, RETSim should be able to find more duplicates. How using RETSim may impact downstream language model pre-training remains unclear.\n- Some baselines are missing in Table 4 and Table 7.\n- Certain aspects of the method are not clearly explained, such as the process for obtaining the (512, 24)-dimension tensor from the vectorizer (Section 3.1) and the Multi-Similarity Loss, which appears to be a less common concept that deserves more detailed explanation (Section 3.2).\n- The paper lacks a dedicated Conclusion section, with Section 7 not effectively serving as a substitute conclusion."
                },
                "questions": {
                    "value": "- It seems that the dataset used in the experiment in Section 5.2 is a non-open dataset. The authors should provide more details on the dataset.\n- For a same amount of input, does RETSim (Near-Dup) produce more embeddings than RETSim (Partial-Dup)? If yes, why isn\u2019t the speed separately measured in Table 6? (I think RETSim (Partial-Dup) is slower, since it needs to retrieve against a significantly larger set of text embeddings during de-duplication?)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4287/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4287/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4287/Reviewer_rc5k"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4287/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698753119667,
            "cdate": 1698753119667,
            "tmdate": 1700202480433,
            "mdate": 1700202480433,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XENxJ0aIgM",
                "forum": "23b9KSNQTX",
                "replyto": "usGBUg0i2p",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4287/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4287/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer rc5k (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their detailed review and helpful comments, which helped us improve the paper. Based on your suggestions, we have revised the paper to add three additional baselines to Table 4, added a separate conclusion section, and expanded upon our methodology to improve clarity.\n\nPlease find our detailed responses to your questions below and the updated paper PDF for the revisions.\n\n> Q1: The experiment in Section 5.1 makes no sense. Apparently, according to previous experiments, RETSim should be able to find more duplicates. How using RETSim may impact downstream language model pre-training remains unclear.\n\nDeduplicating training corpuses is very important for large language models, as it is shown to improve performance as well as reduce memorization and privacy risks [1, 2]. In Section 5.1, we demonstrate that RETSim is capable of finding substantially more duplicates than MinHash, especially for partially-matching documents. We agree that empirically evaluating the impact of using RETSim vs MinHash to deduplicate training corpuses for large language models will be a great addition to the paper. We are looking into adding this experiment for the final revision to see if it produces meaningful improvements.\n\n[1] Kandpal et al. \u201cDeduplicating Training Data Mitigates Privacy Risks in Language Models.\u201d arxiv.org/abs/2202.06539, 2022.\n\n[2] Lee et al. \u201cDeduplicating Training Data Makes Language Models Better.\u201d arxiv.org/abs/2107.06499, 2021.\n\n> Q2: Some baselines are missing in Table 4 and Table 7.\n\nThank you for the suggestions, we have revised Table 4 in the paper to add additional baseline results for neural-based text embeddings (Multilingual USE, Multilingual E5-Base, and LaBSE). RETSim outperforms these additional baselines for the main metrics as well (macro F1 and accuracy). Please see the updated paper for additional details.\n\nFor Table 7, the spam email clustering experiment is run on production data, so we are only allowed to run vetted models. Thus, models like Multilingual E5-Base are not available for us to use, so we are unable to include it as a baseline. SimHash + LSH was the original algorithm being used in production so it was selected as a baseline. Due to data retention policies, we are unable to add additional baselines to this experiment at the moment but will look into rerunning the experiment for the final revision of the paper.\n\n> Q3: Certain aspects of the method are not clearly explained, such as the process for obtaining the (512, 24)-dimension tensor from the vectorizer (Section 3.1) and the Multi-Similarity Loss, which appears to be a less common concept that deserves more detailed explanation (Section 3.2).\n\nThank you for the feedback, we agree and we have updated the PDF to include more detail and background context for the text vectorizer used in RETSim and for Multi-Similarity Loss. Please see Section 2 and Section 3 in the revised paper for the updates.\n\nWe use the RETVec character encoder to obtain the (512, 24)-dimensional tensor representing the input text. The character encoder produces a binary representation of each Unicode character, which is then padded to 24 bits. This character-based representation has been shown to be compact, efficient, and resilient in prior work, which is why it was selected as the vectorizer for RETSim.\n\nWe also agree that Multi-Similarity Loss is less known compared to alternatives such as Triplet Loss. Multi-Similarity Loss is based on a general weighting framework for pair-based losses and has been shown to be state-of-the-art for pair-based metric learning. We added a better description of the loss in the revised paper.\n\n\n> Q4: The paper lacks a dedicated Conclusion section, with Section 7 not effectively serving as a substitute conclusion.\n\nThank you for the feedback, we agree and we have updated the paper to add a separate conclusion section (Section 8). Please see the revised paper PDF for the updates."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4287/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700103236534,
                "cdate": 1700103236534,
                "tmdate": 1700104076382,
                "mdate": 1700104076382,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jg7ZHrZqH4",
                "forum": "23b9KSNQTX",
                "replyto": "usGBUg0i2p",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4287/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4287/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer rc5k (2/2)"
                    },
                    "comment": {
                        "value": "> Q5: It seems that the dataset used in the experiment in Section 5.2 is a non-open dataset. The authors should provide more details on the dataset.\n\nThank you for the suggestion, we have revised the paper to include more details on the dataset, including a better description of the content, ground truth labels, and example types of adversarial attacks. Please see Section 5.2 in the revised paper for the updated details.\n\n\n> Q6: For a same amount of input, does RETSim (Near-Dup) produce more embeddings than RETSim (Partial-Dup)? If yes, why isn\u2019t the speed separately measured in Table 6? (I think RETSim (Partial-Dup) is slower, since it needs to retrieve against a significantly larger set of text embeddings during de-duplication?)\n\nYes, RETSim (Partial-Dup) will produce more embeddings than RETSim (Near-Dup) because it will return multiple embeddings for longer texts, rather than a single embedding. However, the inference speed is equivalent since both partial and global embeddings are returned in a single inference call to RETSim. Thus, in Table 6, the reported embedding speed of RETSim is the same for both versions.\n\nWe did not compare the indexing and retrieval speeds of RETSim since it would depend more on the specific vector search engine used. For large scale experiments, RETSim can be used with an approximate nearest neighbor (ANN) search algorithm and vector index such as USearch or FAISS for efficient indexing and retrieval at the billion or even trillion scale. ANN algorithms scale sublinearly with respect to index size. Thus, although RETSim (Partial-Dup) will produce more embeddings per document, we expect that the indexing and query speed will not be substantially slower.\n\nThank you for the question, we have revised the paper based on your feedback to clarify why the embedding speed is the same for the two RETSim versions. We also highlighted the fact that RETSim (Partial-Dup) offers a tradeoff between finer-grained deduplication at the cost of producing more embeddings to index and search.\n\n---\n\nThank you again for the helpful feedback! Please let us know if you have any additional questions or suggestions, we are more than happy to make further revisions to improve the paper."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4287/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700103432989,
                "cdate": 1700103432989,
                "tmdate": 1700103432989,
                "mdate": 1700103432989,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "O9K4Cq4he7",
                "forum": "23b9KSNQTX",
                "replyto": "jg7ZHrZqH4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4287/Reviewer_rc5k"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4287/Reviewer_rc5k"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. The authors' feedback, along with the revisions made to the paper, has effectively addressed most of my questions. I've updated my review accordingly.\n\nAdditionally, I'd like to propose that the authors emphasize in the paper that the \"speed\" presented in Table 6 specifically means the embedding/hashing time in the table caption and body text. It is important to clarify that this measurement excludes the indexing & retrieval time from the overall speed calculation."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4287/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700202456055,
                "cdate": 1700202456055,
                "tmdate": 1700202456055,
                "mdate": 1700202456055,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "553tzWQpA7",
            "forum": "23b9KSNQTX",
            "replyto": "23b9KSNQTX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4287/Reviewer_pVcX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4287/Reviewer_pVcX"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces RETSim (Resilient and Efficient Text Similarity) to identify near duplicate whole text or partial text efficiently, with applications in deduplication, clustering, etc. The proposed model is designed to be lightweight and learned with simple corruption-like augmentations (hence mimicking the typical dedup errors). The proposed method has been carefully tuned and engineered and demonstrated better performance than both non-learned approaches and learned methods."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The problem of dedup is a fundamental one and has many traditional applications. In the era of LLMs, it gains one more important role as deduplicating the training dataset, which is one of the crucial factors to training a good LLM. \n\n* Although the proposed technical solution is simple, it is designed with care (to balance the sophistication and effectiveness) and hyper-parameters are carefully tuned. \n\n* The performance results are excellent over a comprehensive set of tasks and datasets."
                },
                "weaknesses": {
                    "value": "* There is no analysis on why the almost straight-forward design produces better results (F1) than other learned algorithms. From the ablation study (e.g., Sec 6), it seems the benefit is more from tuning parameters?\n\n* Similarly, how important is the training corpus and the data augmentation methods? E.g., what if the data distribution, corruption pattern, etc. on mC4 is drastically different from those in the testing corpus? \n\n* MinHash + LSH achieves a strong performance on real dataset (Table 4). It seems there is no detail on the parameter of the LSH part, and therefore it is hard to know how much performance loss is due to the LSH part (e.g., if the number of \"OR\" part is small, it may hurt the performance). Also why +LSH? I can understand that MinHash + LSH is a practical solution for large corpus due to its good runtime efficiency, but run-time efficiency is not report for the proposed method. In fact, it is not clear whether and which index you use to find the near duplicate (chunks) with your method. Do you use a vector index and which one? \n\n* It will be better to perform a case study or detailed error analysis to understand the false positive and false negatives for the proposed method."
                },
                "questions": {
                    "value": "See the questions asked in the \"Weakness\" part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4287/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4287/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4287/Reviewer_pVcX"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4287/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698937652894,
            "cdate": 1698937652894,
            "tmdate": 1699636396451,
            "mdate": 1699636396451,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "utNNINbLq6",
                "forum": "23b9KSNQTX",
                "replyto": "553tzWQpA7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4287/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4287/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer pVcX (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their positive feedback and insightful comments, which helped us to improve the paper. Based on your feedback, we have revised the paper to add more details on the baseline algorithms and vector index. We also added an additional section to the Appendix (A.6), where we sampled a set of false positives and false negatives from the NEWS-COPY dataset to further analyze the efficacy of RETSim.\n\nPlease find our responses to your questions below and the revised paper for more details.\n\n> Q1: There is no analysis on why the almost straight-forward design produces better results (F1) than other learned algorithms. From the ablation study (e.g., Sec 6), it seems the benefit is more from tuning parameters?\n\nIndeed, the architecture used by RETSim is fairly straightforward. We did try numerous more complex architectures and pre-training methodologies, with minimal to no gain at the expense of significantly slower models. Our best guess as to why we are not able to leverage more advanced models or larger capacity is that the problem of near-duplicate detection and syntactic textual similarity is simple enough to solve with a 2-block transformer architecture. The main difficulty to get high performance was training data generation, where finding the right balance between not enough and too much text augmentation was difficult to achieve. On the other hand, we do not believe that hyperparameter tuning was the reason RETSim outperformed other methods. Some suboptimal hyperparameter settings (e.g. 128-dim embedding size) from the ablation study are still roughly close in performance as the final selected value, whereas RETSim often outperformed baseline algorithms by a significant margin.\n\nThe main reason why RETSim surpasses existing neural text embeddings and hash-based algorithms for near-duplicate detection is that it is significantly more resilient to noise such as adversarial attacks and typos (Figure 2). We attribute this improvement to RETSim\u2019s character-level tokenizer which has been shown to be more typo-robust than subword-level tokenizers, and also pre-training the RETSim embedding using text augmentations for adversarial robustness. We understand that this may not be a very satisfactory scientific answer, but empirically, this is what allowed RETSim to achieve state-of-the-art performance on near-duplicate text detection benchmarks while remaining computationally efficient.\n\n\n> Q2: Similarly, how important is the training corpus and the data augmentation methods? E.g., what if the data distribution, corruption pattern, etc. on mC4 is drastically different from those in the testing corpus?\n\n\nWe train on a large corpus (Multilingual C4) containing ~27TB of crawled web data and many different text augmentations, which we believe is an important contributing factor for RETSim\u2019s performance on near-duplicate benchmarks especially in the case of noise or typos. The amount of augmentation used in the training dataset was an important design decision that we carefully tuned. We found that too little text augmentation led to not enough generalization, while too much led to poor performance.\n\nEmpirically, we find that RETSim is able to generalize well to previously unseen types of noise and data distributions that are different from the training dataset. This is exemplified by its success in the email spam experiment, which consisted of spam emails handcrafted by attackers using targeted attacks and corruption patterns meant to bypass existing spam filters that RETSim was not trained on. The NEWS-COPY dataset is another real-world benchmark based on historical news articles with noise mostly from OCR errors, neither of which RETSim was directly trained on. RETSim is able to outperform both neural-based embeddings and ngram-based algorithms on these benchmarks."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4287/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700102758684,
                "cdate": 1700102758684,
                "tmdate": 1700102758684,
                "mdate": 1700102758684,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Hb953lS1yb",
                "forum": "23b9KSNQTX",
                "replyto": "553tzWQpA7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4287/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4287/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer pVcX (2/2)"
                    },
                    "comment": {
                        "value": "> Q3: MinHash + LSH achieves a strong performance on real dataset (Table 4). It seems there is no detail on the parameter of the LSH part, and therefore it is hard to know how much performance loss is due to the LSH part (e.g., if the number of \"OR\" part is small, it may hurt the performance).  Also why +LSH? I can understand that MinHash + LSH is a practical solution for large corpus due to its good runtime efficiency, but run-time efficiency is not report for the proposed method.\n\nThank you for the suggestion, we agree that this is important to report. The settings for MinHash and other algorithms for this experiment are included in Appendix A.3. For MinHash + LSH, we use 256 hash functions, trigrams, and a Jaccard similarity threshold of 0.5. Using 256 hash functions is recommended by the datasketch library [1] to improve performance over the default 128 hash functions and also commonly used in the literature, so we believe that the performance loss from using LSH should be minimal. We do use MinHash + LSH for computational efficiency, since using MinHash without LSH would have taken significantly longer to run than RETSim and MinHash + LSH (which only takes a few minutes for this dataset).\n\nWe have revised the paper to explicitly state the parameter settings for MinHash + LSH in the main paper as well.\n\n[1] MinHash \u2014 datasketch 1.6.4 documentation (https://ekzhu.com/datasketch/minhash.html)\n\n> Q4: In fact, it is not clear whether and which index you use to find the near duplicate (chunks) with your method. Do you use a vector index and which one?\n\nThank you for the suggestion, we have revised the paper to include this information. We use USearch\u2019s default vector index for the Wiki-40B deduplication experiment, which uses HNSW for approximate search (Section 5.1). We use our implementation of exact nearest neighbor search for all other experiments on smaller datasets for embedding models, including RETSim.\n\n> Q5: It will be better to perform a case study or detailed error analysis to understand the false positive and false negatives for the proposed method.\n\nThis is an excellent suggestion, we have added a set of sampled false positives and false negatives on the NEWS-COPY deduplication dataset to Appendix A.6 (Tables 16 and 17) as a case study. From manual evaluation, we noticed that false negatives on this dataset seem to be due to texts being heavily corrupted by typos or OCR errors. On the other hand, we noticed that for many false positive examples (Table 17), the texts do appear to be near-duplicates of each other (and RETSim detects them as near-duplicates), although the ground truth label in the dataset does not mark them as near-duplicates. This is an interesting discovery and we have revised the paper to make a note of potentially noisy labels for the NEWS-COPY dataset. Beyond analyzing specific examples, we are uncertain how to do detailed error analysis to explain why the model makes certain mistakes, and we did not find any particular patterns that suggest systematic shortcomings that we can highlight.\n\nPlease let us know if you have any additional questions or suggestions. Thank you again for the helpful feedback!"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4287/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700102991882,
                "cdate": 1700102991882,
                "tmdate": 1700102991882,
                "mdate": 1700102991882,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "byh2kqq6cK",
            "forum": "23b9KSNQTX",
            "replyto": "23b9KSNQTX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4287/Reviewer_XRFc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4287/Reviewer_XRFc"
            ],
            "content": {
                "summary": {
                    "value": "This work addresses the problem of learning robust metric embeddings for near-duplicate text retrieval, clustering, and dataset deduplication tasks. Towards this end it proposes a lightweight multilingual deep learning model called RETSim. The work argues that previous solutions for near-duplicate text retrieval are not resilient to typos in the text documents and are vulnerable to adversarial attacks. Further, some of the previous solutions are very parameter-sensitive and consequently need heavy tuning and/or are too big to be used effectively in real-world applications of near-duplicate text retrieval. The proposed solution to the problem of learning robust metric embeddings for near-duplicate text retrieval makes use of a large typo-augmented training corpus, RETVec text vectorizer, a lightweight  transformer block (Hua et al., 2022) and  a metric learning training regime. The tokenizer splits the text into multiple chunks of size at most 512 characters and encodes each chunk using a character encoder. Each encoded chunk is fed to a lighweight transformer that computes a 256 dimensional embedding for the chunk. The embeddings from all the chunks of a text input are then combined into an embedding for the full text by averaging. \n\nThe model is trained on a dataset derived from the multilingual C4 dataset by applying sentence, work and character level augmentations on the text. Multi-Similarity Loss is used in training which attempts to make augmented versions of the same text closer to each other in the embedding space. \n\nIn addition to addressing the technical problem of measuring similarity of near-duplicate texts, this work introduces a new benchmark for the evaluation of such methods. The benchmark dataset called W4NT3D is particularly oriented towards evaluation of near-duplicate text retrieval models on multilingual text corpora in the presence of typos, word manipulations, and sentence/paragraph-level modifications. \n\nThe work presents results from the experiments comparing the proposed method with several baselines including some hash-based methods and some deep-learning-based methods. Results are presented for W4NT3D dataset. The proposed method performs better than all the baselines overall but not by a huge margin (Multilingual E5-Base is very close) and for Chinese and Japanese languages, it performs slightly worse than the best baseline for these languages (Multilingual USE). \n\nThe experimental study also reports results on different types of augmentations on the same dataset. Interestingly, all methods seem to do well for paragraph and sentence level augmentations. For word-level augmentations, hashing based methods do worse than other methods and for character-level augmentations, the proposed method fares best.\n\nFurther experimental results are provided for two real-world near-duplicate datasets - NEWS-COPY Deduplication dataset and CORE Near-Duplicates dataset. The proposed method gives significantly better results on NEWS-COPY dataset compared to all the baselines. On CORE Near-Duplicates dataset, the method fares marginally better than the hashing-based baselines but no comparison is made with neural-network-based methods.\n\nThe work discusses a practical application of the proposed method - spam email clustering. Results show significant improvement in clustering spam emails compared to three baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. A well-designed approach for near-duplicate detection that is light-weight, robust to typos and works on multilingual data.\n2. Encouraging improvements in near-duplicate detection over both hashing-based and neural-network-based methods though Multilingual E5-Base seems to be reasonably competitive.\n3. A new dataset is made available that can be used for near-duplicate detection studies."
                },
                "weaknesses": {
                    "value": "1. RETSim is generally slower than MinHash and particularly slow on CPU (Table 6)."
                },
                "questions": {
                    "value": "1. In the deduplication experiments on Wiki-40B (English), was RETSim trained on this portion of Wiki-40B? If so then could that also be one of the reasons why RETSim is able to detect substantially more duplicates than the baselines?\n\n2. Why isn't Multilingual E5-Base used as a baseline in spam email clustering experiments?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4287/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699547174451,
            "cdate": 1699547174451,
            "tmdate": 1699636396355,
            "mdate": 1699636396355,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "a3XRhHxBJ7",
                "forum": "23b9KSNQTX",
                "replyto": "byh2kqq6cK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4287/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4287/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer XRFc"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their detailed review and helpful comments. We have also revised the paper based on the feedback we received, including making updates to the text to improve clarity and additional experiments.\n\nPlease find our responses to your questions and comments below.\n\n> Q1: RETSim is generally slower than MinHash and particularly slow on CPU (Table 6).\n\nYes, it is true that RETSim is generally slower than MinHash, particularly on CPU. On the other hand, RETSim is much faster and smaller than existing neural-based embeddings such as Multilingual E5-Base, and significantly closes the speed performance gap between hash-based and neural network-based methods. Our current tokenizer implementation is single-threaded and in Python, so we believe there is substantial room for improvement in terms of speed. Overall, RETSim provides a tradeoff between better performance (better near-duplicate detection, increased adversarial robustness) vs speed compared to hash-based algorithms. We strongly believe that RETSim offers a broadly applicable algorithm that has already proven itself on large scale tasks such as spam email filtering, and surpasses state-of-the-art on near-duplicate detection benchmarks from the literature.\n\n> Q2: In the deduplication experiments on Wiki-40B (English), was RETSim trained on this portion of Wiki-40B? If so then could that also be one of the reasons why RETSim is able to detect substantially more duplicates than the baselines?\n\nRETSim is trained only on the Multilingual C4 dataset [1]. We do not fine-tune RETSim on Wiki-40B (or on any other dataset after training). We do expect that Wikipedia data shows up in the Multilingual C4 dataset, although it would represent a very small percentage of the dataset (which is around 27TB of data). Furthermore, RETSim is a very small model (~536k parameters) and we do not repeat examples during training, so we do not believe that the model has the capacity or capability to memorize any Wikipedia training data that would help it on the Wiki-40B benchmark.\n\nIn addition, RETSim is capable of generalizing to previously unseen text datasets, as shown in the spam email experiment (Table 7). This experiment uses production email data never seen by the models during training and RETSim\u2019s superior performance on it supports our hypothesis that RETSim has learned a generalizable text embedding that works well across previously-unseen text distributions, languages, and adversarial conditions. This is most likely the reason why RETSim detects substantially more duplicates than baseline algorithms like MinHash, which has been shown to perform poorly in the presence of noise.\n\n[1] Xue et al. \u201cmT5: A Massively Multilingual Pre-trained Text-to-text Transformer.\u201d arxiv.org/abs/2010.11934, 2020.\n\n\n> Q3: Why isn't Multilingual E5-Base used as a baseline in spam email clustering experiments?\n\nThe spam email clustering experiment is run on production data and we are only allowed to run vetted models on our internal system. Thus, Multilingual E5-Base is not available for us to use internally, so we are unable to include it as a baseline in the spam email clustering experiments.\n\nPlease let us know if you have any additional questions or suggestions for improving the paper. Thank you again for the review!"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4287/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700102501310,
                "cdate": 1700102501310,
                "tmdate": 1700102501310,
                "mdate": 1700102501310,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "F7vYpf1nHE",
                "forum": "23b9KSNQTX",
                "replyto": "a3XRhHxBJ7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4287/Reviewer_XRFc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4287/Reviewer_XRFc"
                ],
                "content": {
                    "title": {
                        "value": "Author response"
                    },
                    "comment": {
                        "value": "Thanks for your response. I acknowledge that I've read it."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4287/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700477040387,
                "cdate": 1700477040387,
                "tmdate": 1700477040387,
                "mdate": 1700477040387,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]