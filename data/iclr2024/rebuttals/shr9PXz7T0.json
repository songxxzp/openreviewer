[
    {
        "title": "Large Language Models Are Not Robust Multiple Choice Selectors"
    },
    {
        "review": {
            "id": "rVw5kdLNJO",
            "forum": "shr9PXz7T0",
            "replyto": "shr9PXz7T0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission215/Reviewer_VDjF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission215/Reviewer_VDjF"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the issue of sensitivity to answer option order in large language models (LLMs), which can lead to biased predictions. It introduces a new method called PriDe to mitigate this sensitivity by estimating and correcting for the model's bias during inference. The results show that PriDe can reduce the prediction sensitivity."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This paper studies the LLMs' sensitivity to order of answer options, which is an important problem in current LLM evaluation, and provides empirical analysis of the underlying reasons.\n- The proposed method PriDe operates on test time without introducing extra computational cost, which is suitable for current LLMs. \n- The authors conduct extensive experiments including different models, tasks, ablation studies, cross-task evaluation, etc."
                },
                "weaknesses": {
                    "value": "- The proposed method requires sampling test samples first to estimate the prior, which may introduce another dimension of sensitivity of the selection of the test samples. The accuracy of this estimation might vary based on the quality and representativeness of these samples.\n- I understand the procedure of cyclic permutation and full permutation, but how are they used as the debiasing methods? Do the authors take the best result of the permutations as the prediction?\n- The authors use the balance of recalls and Rstd as the major metrics throughout the paper. Can the authors formally define this? I didn't immediately get it.\n- The writing and presentation need more improvement, e.g., I think the proposed PriDe is quite intuitive but the authors introduce too many unnecessary notations ($d_i, o_i, g_i, x_i, .... $) before getting into the real introduction of the method, which makes the reading difficult."
                },
                "questions": {
                    "value": "- The proposed method basically follows estimate-then-mitigate, which is somewhat similar to the calibrate-before-use (Zhao et al. ICML 2021) paper, though this one targets a different setting and is not directly applicable to MCQs. But it would be interesting to compare the differences and know if calibrate-before-use can also help with MCQ sensitivity."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission215/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698012547321,
            "cdate": 1698012547321,
            "tmdate": 1699635946993,
            "mdate": 1699635946993,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7AJIPsWhPo",
                "forum": "shr9PXz7T0",
                "replyto": "rVw5kdLNJO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission215/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission215/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer VDjF (1/2)"
                    },
                    "comment": {
                        "value": "Thanks for your constructive comments! We address your concerns or questions as follows.\n\n> The proposed method requires sampling test samples first to estimate the prior, which may introduce another dimension of sensitivity of the selection of the test samples. The accuracy of this estimation might vary based on the quality and representativeness of these samples.\n\nGood question! While we do test sample sampling mainly out of the need of experiments, in practice we usually expect the estimated prior to be less sensitive to estimation sample selection. We here supplement statistics in the 5 runs of PriDe (0-shot, $\\alpha=5\\%$). We present the mean (as reported in our paper) \u2206 RStd and \u2206 Acc as well as their best/worst values (averaged over all the 20 LLMs):\n\n| Benchmarks | \u2206 RStd (mean) | \u2206 RStd (best \u2193) | \u2206 RStd (worst \u2191) | \u2206 Acc (mean) | \u2206 Acc (best \u2191) | \u2206 Acc (worst \u2193) |\n| ---------- | ------------- | --------------- | ---------------- | ------------ | -------------- | --------------- |\n| MMLU       | -7.6          | -8.0            | -7.3             | 1.2          | 1.3            | 1.1             |\n| ARC        | -5.6          | -6.8            | -4.3             | 1.3          | 1.7            | 0.8             |\n| CSQA       | -6.9          | -7.7            | -6.0             | 1.7          | 2.2            | 1.3             |\n\nWe observe that the selection of estimation samples may introduce slight fluctuations in debiasing results, and even in the worst-case scenario, PriDe still leads to a notable debiasing performance (decrease in RStd and increase in Acc). Therefore, we believe PriDe's sensitivity to the selection of estimation samples lies within an acceptable range and does not obscure its merit (effectiveness and efficiency).\n\n> I understand the procedure of cyclic permutation and full permutation, but how are they used as the debiasing methods? Do the authors take the best result of the permutations as the prediction?\n\nCyclic and Full Permutation can be viewed as having a debiasing effect, as they involve swapping options and averaging prediction distributions over different permutations. They can intuitively mitigate the model's bias for option IDs or options' ordering positions. This is similarly done in recent work [1] and [2], where they swap candidate responses to mitigate GPT-4's evaluation bias.\n\nFor Full Permutation, there is only one possible permutation set (i.e., all possible permutations). For cyclic permutations, there might be multiple possible permutation sets (as long as we ensure one pairing between each option ID and option content). The selection of cyclic permutation sets is not our focus, as our method PriDe can be directly combined with any reasonable cyclic permutation set. In the main text, we use the simplest and most intuitive set for Cyclic Permutation, e.g., $\\{ (1,2,3,4), (2,3,4,1), (3,4,1,2), (4,1,2,3) \\}$ for 4-option MCQ tasks. We show in **Section 3.1 and Figure 16 in Appendix F** that selecting other cyclic permutation sets leads to similar debiasing results.\n\n[1] Wang, Peiyi, et al. \"Large language models are not fair evaluators.\" *arXiv preprint arXiv:2305.17926* (2023).\n\n[2] Zheng, Lianmin, et al. \"Judging LLM-as-a-judge with MT-Bench and Chatbot Arena.\" *arXiv preprint arXiv:2306.05685* (2023).\n\n> The authors use the balance of recalls and Rstd as the major metrics throughout the paper. Can the authors formally define this? I didn't immediately get it.\n\nSure! The recall of an option ID $d_i$ is defined as:\n$$\n\\mathrm{Recall}(d_i) = \\frac{ \\\\# (\\text{correct answer is } d_i \\ \\\\&\\text{ prediction is } d_i) }{ \\\\# (\\text{correct answer is } d_i)} \\times 100 \\\\%,\n$$\nwhile RStd (Std of recalls) is:\n$$\n\\mathrm{RStd}=\\mathrm{Std}( \\\\{ \\mathrm{Recall}(d_i) \\\\}\\_{i=1}^n) = \\sqrt{\\frac{\\sum_{i=1}^n (\\mathrm{Recall}(d_i) - \\mu)^2}{n}}, \\text{where } \\mu = \\frac{1}{n} \\sum_{i=1}^n \\mathrm{Recall}(d_i).\n$$\nOur motivation of using this measurement is illustrated in Section 2.2."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission215/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699917302849,
                "cdate": 1699917302849,
                "tmdate": 1700637160645,
                "mdate": 1700637160645,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KiQHkWNqBW",
                "forum": "shr9PXz7T0",
                "replyto": "rVw5kdLNJO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission215/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission215/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer VDjF (2/2)"
                    },
                    "comment": {
                        "value": "> The writing and presentation need more improvement, e.g., I think the proposed PriDe is quite intuitive but the authors introduce too many unnecessary notations ($d_i, o_i, g_i, x_i, .... $) before getting into the real introduction of the method, which makes the reading difficult.\n\nWe would like to clarify the reasons for introducing these formal notations before the proposed method. We attempted to introduce the proposed method first and then intersperse or supplement the introduction of the permutation-based baseline on which our method relies. However, we found that this compromised the integrity of the writing content. We also found that without introducing these formal notations, the writing would become very repetitive and lengthy (we would have to repeatedly use the same terms to avoid ambiguity). Additionally, we believed that formal notations could aid in deriving general solution forms. All these above led us to adopt the current formal notations and writing logic.\n\nIf you believe there is a better way to present or structure the content, we would greatly appreciate it and be open to taking your suggestions!\n\n> The proposed method basically follows estimate-then-mitigate, which is somewhat similar to the calibrate-before-use (Zhao et al. ICML 2021) paper, though this one targets a different setting and is not directly applicable to MCQs. But it would be interesting to compare the differences and know if calibrate-before-use can also help with MCQ sensitivity.\n\nWe are willing to discuss the difference from Contextual Calibration (Zhao et al.). We make a preliminary attempt to adapt Contextual Calibration to MCQ debiasing as follows:\n\n1. For each test sample, we use the default input and obtain the prediction distribution $\\mathbf{p}$.\n2. We then replace all the options with the same content-free text: the null string `''`, `N/A`, or `[MASK]`, as in Zhao et al., and estimate the model's prediction distribution over the option IDs, denoted as $\\mathbf{p}_0$ (we use all the content-free texts and take the average of their $\\mathbf{p}_0$).\n3. We use $\\mathbf{p}/\\mathbf{p}_0$ after normalization as the \"calibrated\" prediction distribution, as done in Zhao et al.\n\nSo, from the perspective of implementation, Contextual Calibration is similar to PriDe. The key difference lies in how we estimate $\\mathbf{p}_0$, which we refer to as \"prior\" in our work. The results are shown below (`gpt-3.5-turbo-0613`, 0-shot ARC, for a quick verification):\n\n| Methods                | RStd | Acc |\n| ---------------------- | ------ | ----- |\n| Default                | 3.3    | 84.3  |\n| PriDe ($\\alpha=5\\\\%$)   | 2.3    | 84.2  |\n| Contextual Calibration | 4.8    | 83.1  |\n\nWe find that Contextual Calibration fails to mitigate selection bias (RStd) and may impair model performance (Acc). It implies that the \"prior\" ($\\mathbf{p}_0$) estimated by Contextual Calibration cannot reflect the model's selection bias in MCQs and may also be hard to interpret."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission215/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699917891892,
                "cdate": 1699917891892,
                "tmdate": 1700602748832,
                "mdate": 1700602748832,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "x5rvHtmlMn",
                "forum": "shr9PXz7T0",
                "replyto": "rVw5kdLNJO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission215/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission215/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A Gentle Reminder of the Final Feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer VDjF,\n\nWe would like to thank you for your time and comments. We hope our previous response has adequately resolved your questions or concerns. As the deadline for the ICLR rebuttal period is approaching, we look forward to hearing your feedback on our response, and would be pleased to clarify any additional questions.\n\nBest,\n\nAuthors"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission215/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700616258084,
                "cdate": 1700616258084,
                "tmdate": 1700616258084,
                "mdate": 1700616258084,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ZwkJ7MIRFr",
            "forum": "shr9PXz7T0",
            "replyto": "shr9PXz7T0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission215/Reviewer_G9aZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission215/Reviewer_G9aZ"
            ],
            "content": {
                "summary": {
                    "value": "This work presents a comprehensive analysis of the selection bias issue in large language models (LLMs) when dealing with multiple choice questions (MCQs).\nThe experimental results identify the root cause of this bias as the LLMs' token bias, which leads to a preference for specific option IDs when predicting answers.\nBased on these observations, this work proposes a label-free, inference-time debiasing method called PriDe, which effectively mitigates selection bias."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The empirical analysis is thorough, involving 20 LLMs and three benchmark datasets. This extensive evaluation provides strong evidence for the existence of selection bias in LLMs and its impact on their performance in MCQ tasks. The identification of token bias as the primary source of this issue is a valuable insight that can inform future research on LLMs and their limitations.\n\n2. The proposed PriDe method is effective when the computing cost is limited. Further analysis on generalizability reveals that the prior estimated by PriDe can be generalized across tasks."
                },
                "weaknesses": {
                    "value": "1. It seems that PriDe achieves comparable performance with simple baselines when the computation cost is not limitated. In application scenarios, we always first estimate the prior without concerning the computation cost, then apply this prior to serve applications.\nIt would be better if PriDe could have a higher upper boudn performance."
                },
                "questions": {
                    "value": "1. The generalization analysis indicates that the bias for a certain model is consistent across different tasks.\nCould you further demonstrate this with more statics or results?\nIt would also help to enhace the claimed interpretability."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission215/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission215/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission215/Reviewer_G9aZ"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission215/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698564589615,
            "cdate": 1698564589615,
            "tmdate": 1700621613950,
            "mdate": 1700621613950,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Tq4cplfdMR",
                "forum": "shr9PXz7T0",
                "replyto": "ZwkJ7MIRFr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission215/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission215/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer G9aZ"
                    },
                    "comment": {
                        "value": "Thanks for your positive comments! We address your concerns or questions as follows.\n\n> It seems that PriDe achieves comparable performance with simple baselines when the computation cost is not limitated. In application scenarios, we always first estimate the prior without concerning the computation cost, then apply this prior to serve applications. It would be better if PriDe could have a higher upper boudn performance.\n\n(This question is similar to the one raised by Reviewer CGVT, so we use the same answer)\n\nWhen the budget is sufficient, using more permutations does yield better debiasing effects and performance improvements. As discussed in Section 4.3, this is akin to \"mixture of experts\" or \"model ensemble\". Our method, on the other hand, provides a computation-efficient alternative. We believe this could be beneficial for debiasing in scenarios with constrained/limited computational resources, such as platforms like the HuggingFace LLM Leaderboard, where a large number of models need to be evaluated on numerous benchmarks.\n\n> The generalization analysis indicates that the bias for a certain model is consistent across different tasks. Could you further demonstrate this with more statics or results? It would also help to enhace the claimed interpretability.\n\nOf course! Here we compute the L1 distance (due to its intuitiveness, as in our response to Reviewer CGVT) between the estimated priors from different domains to illustrate PriDe's cross-domain generalization (0-shot, averaged over all the LLMs, $\\alpha=5\\%$; priors' L1 distance is averaged over 5 runs).\n\n| Domain 1 \\ Domain 2 | STEM  | Social Science | Humanities | Others | ARC   |\n| ------------------- | ----- | -------------- | ---------- | ------ | ----- |\n| **STEM**            | 0     | 0.104          | 0.094      | 0.106  | 0.121 |\n| **Social Science**  | 0.104 | 0              | 0.099      | 0.067  | 0.076 |\n| **Humanities**      | 0.094 | 0.099          | 0          | 0.110  | 0.125 |\n| **Others**          | 0.106 | 0.067          | 0.110      | 0      | 0.087 |\n| **ARC**             | 0.121 | 0.076          | 0.125      | 0.087  | 0     |\n\nWe think these priors' gaps are usually marginal, which could verify that for a certain model, its prior for option IDs is similar across domains."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission215/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699917223356,
                "cdate": 1699917223356,
                "tmdate": 1700637134827,
                "mdate": 1700637134827,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ainFpnKfD4",
                "forum": "shr9PXz7T0",
                "replyto": "ZwkJ7MIRFr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission215/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission215/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A Gentle Reminder of the Final Feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer G9aZ,\n\nWe would like to thank you for your time and comments. We hope our previous response has adequately resolved your questions or concerns. As the deadline for the ICLR rebuttal period is approaching, we look forward to hearing your feedback on our response, and would be pleased to clarify any additional questions.\n\nBest,\n\nAuthors"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission215/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700616219229,
                "cdate": 1700616219229,
                "tmdate": 1700616219229,
                "mdate": 1700616219229,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xGA8QsxkEn",
                "forum": "shr9PXz7T0",
                "replyto": "ainFpnKfD4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission215/Reviewer_G9aZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission215/Reviewer_G9aZ"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your response"
                    },
                    "comment": {
                        "value": "I have no more questions. I have updated my score."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission215/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700621701254,
                "cdate": 1700621701254,
                "tmdate": 1700621701254,
                "mdate": 1700621701254,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "fCk5YNTStz",
            "forum": "shr9PXz7T0",
            "replyto": "shr9PXz7T0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission215/Reviewer_rhkS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission215/Reviewer_rhkS"
            ],
            "content": {
                "summary": {
                    "value": "This paper experimentally discovers an issue that LLMs are vulnerable to option position changes, or the Option-Order Sensitivity problem, in MCQs due to their inherent \u201cselection bias.\u201d It proposes a label-free, inference-time debiasing method(PriDe) to mitigate the selection bias. The experimental results demonstrate the claim and the usefulness of the PriDe."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "I really appreciate the paper conducted extensive experiments to demonstrate and analyze the Option-Order Sensitivity problem. Some observations are really interesting; for example, even the same models with different parameter sizes but trained using the same data exhibit different position preferences. \n\nThe PriDe is intuitive but also effective."
                },
                "weaknesses": {
                    "value": "It would be better to cite \"Leveraging large language models for multiple choice question answering\" or other related papers when mentioning the Option-Order Sensitivity problem since they have found the problem earlier than the work of this paper.\n\nIt would be better to analyze more technicals, including self-consistency."
                },
                "questions": {
                    "value": "Please refer to the weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission215/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission215/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission215/Reviewer_rhkS"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission215/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698759067369,
            "cdate": 1698759067369,
            "tmdate": 1699635946779,
            "mdate": 1699635946779,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZioikYksHQ",
                "forum": "shr9PXz7T0",
                "replyto": "fCk5YNTStz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission215/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission215/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer rhkS"
                    },
                    "comment": {
                        "value": "Thanks for your positive comments! We address your concerns or questions as follows.\n\n> It would be better to cite \"Leveraging large language models for multiple choice question answering\" or other related papers when mentioning the Option-Order Sensitivity problem since they have found the problem earlier than the work of this paper.\n\nWe appreciate your suggestion! We will add the citation in the revision.\n\n> It would be better to analyze more technicals, including self-consistency.\n\nIn Section 2.6, we experimented with simple prompting strategies, considering their popularity in recent research, to observe whether they have a positive impact on debiasing (finding that they do not). We did not explore too much into prompting engineering, for the following reasons:\n\n1. Our empirical analysis in Sections 2.3-2.5 cannot motivate us to work on prompting engineering, i.e., we intuitively believe that prompting engineering is not the fundamental means of debiasing (and may also be tricky).\n2. Complex prompting strategies (such as self-consistency) are designed primarily to enhance model performance rather than to debias. Moreover, they typically rely on powerful but often commercial, closed-source LLMs like ChatGPT, Claude, and PaLM, making them less applicable to open-source LLMs like LLaMA.\n3. Complex prompting strategies are often expensive, especially when involving much sampling or heuristic filtering.\n\nWe also supplement the results of Self-Consistency on ARC (this benchmark has a small scale, suitable for quick verification). We employ `gpt-3.5-turbo-0613`, sample 10 Chain-of-Thought paths, and then vote on the predicted results.\n\n| Methods          | RStd \u2193 | Acc \u2191 |\n| ---------------- | ------ | ----- |\n| Default          | 3.3    | 84.3  |\n| Removing IDs     | 0.6    | 84.9  |\n| Chain-of-Thought | 3.4    | 84.5  |\n| Self-Consistency | 4.5    | 88.9  |\n\nAs expected, Self-Consistency improved Acc. However, like other prompting strategies, it cannot mitigate selection bias and even somewhat amplifies it (RStd increases), which is inconsistent with our goal of debiasing. We believe that investigating the impact of prompting strategies on LLMs' behavioral bias would be an intriguing research problem."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission215/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699917165570,
                "cdate": 1699917165570,
                "tmdate": 1699917165570,
                "mdate": 1699917165570,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hXWoxjKKTX",
                "forum": "shr9PXz7T0",
                "replyto": "fCk5YNTStz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission215/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission215/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A Gentle Reminder of the Final Feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer rhkS,\n\nWe would like to thank you for your time and comments. We hope our previous response has adequately resolved your questions or concerns. As the deadline for the ICLR rebuttal period is approaching, we look forward to hearing your feedback on our response, and would be pleased to clarify any additional questions.\n\nBest,\n\nAuthors"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission215/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700616198199,
                "cdate": 1700616198199,
                "tmdate": 1700616198199,
                "mdate": 1700616198199,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "hIXDKsGzbg",
            "forum": "shr9PXz7T0",
            "replyto": "shr9PXz7T0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission215/Reviewer_CGVT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission215/Reviewer_CGVT"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigated the LLMs' sensitivity to position changes in multiple-choice questions, discovered that token bias is the main cause/ Furthermore, the authors proposed a way to efficiently suppress this bias and improve accuracy."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. It flows! The writing is perfect. All sections follow each other naturally, from problem to observation, to diagnosis, to ruling out simplistic solutions, to proposed solutions. In each step, there are corresponding experiments to substantiate it.\n2. There are some clever experiment designs in diagnosing the cause, and the experiments are carried out with caution (e.g. replacing symbols to confirm).\n3. Comprehensive experiments on many models and datasets."
                },
                "weaknesses": {
                    "value": "1. When the compute budget is unbounded, the proposed method sometimes has a slight accuracy disadvantage compared to full perm."
                },
                "questions": {
                    "value": "1. In deriving the method, there are a few key assumptions, e.g. Prior for option IDs depends mostly on q. Is it possible to empirically verify this assumption?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission215/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698838347860,
            "cdate": 1698838347860,
            "tmdate": 1699635946707,
            "mdate": 1699635946707,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XDDXG5VnsE",
                "forum": "shr9PXz7T0",
                "replyto": "hIXDKsGzbg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission215/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission215/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer CGVT"
                    },
                    "comment": {
                        "value": "Thanks for your positive comments! We address your concerns or questions as follows.\n\n> When the compute budget is unbounded, the proposed method sometimes has a slight accuracy disadvantage compared to full perm.\n\n(This question is similar to the one raised by Reviewer G9aZ, so we use the same answer)\n\nWhen the budget is sufficient, using more permutations does yield better debiasing effects and performance improvements. As discussed in Section 4.3, this is akin to \"mixture of experts\" or \"model ensemble\". Our method, on the other hand, provides a computation-efficient alternative. We believe this could be beneficial for debiasing in scenarios with constrained/limited computational resources, such as platforms like the HuggingFace LLM Leaderboard, where a large number of models need to be evaluated on numerous benchmarks.\n\n> In deriving the method, there are a few key assumptions, e.g. Prior for option IDs depends mostly on q. Is it possible to empirically verify this assumption?\n\nWe removed the dependency on $x^I$ in $P_\\textrm{prior}$ because it could be a minimally strong assumption necessary for our derivation. We are also pleased to empirically verify this assumption, that is, whether swapping options (i.e., different $x^I$ w.r.t. $I$) would change the derived $P_\\textrm{prior}$? If the answer is no, then our assumption makes sense.\n\nIn our main text, we use the cyclic permutation set $\\{ (1,2,3,4), (2,3,4,1), (3,4,1,2), (4,1,2,3) \\}$ for 4-option MCQ tasks. Our verification contains the following steps:\n\n1. Modify the default-ordered options as $(1,2,4,3)$ or $(4,3,2,1)$.\n2. Use the corresponding cyclic set $\\{ (1,2,4,3), (2,4,3,1), (4,3,1,2), (3,1,2,4) \\}$ or $\\{ (4,3,2,1), (3,2,1,4), (2,1,4,3), (1,4,3,2) \\}$ to derive $P_\\textrm{prior}'$.\n3. Check if $P_\\textrm{prior}'$ is close to $P_\\textrm{prior}$. We use the L1 distance as measurement (averaged over all the test samples), due to its intuitiveness: $ d(\\mathbf{p}, \\mathbf{q})= \\sum_i |p_i - q_i|. $\n\n| Models      | MMLU $(1,2,4,3)$ | ARC $(1,2,4,3)$ | MMLU $(4,3,2,1)$ | ARC $(4,3,2,1)$ |\n| ----------- | ---------------- | --------------- | ---------------- | --------------- |\n| llama-7B    | 0.014            | 0.014           | 0.013            | 0.013           |\n| llama-13B   | 0.034            | 0.047           | 0.035            | 0.045           |\n| llama-30B   | 0.068            | 0.079           | 0.066            | 0.077           |\n| llama-65B   | 0.069            | 0.081           | 0.070            | 0.082           |\n| llama-2-7B  | 0.028            | 0.022           | 0.028            | 0.023           |\n| llama-2-13B | 0.061            | 0.059           | 0.058            | 0.056           |\n| llama-2-70B | 0.095            | 0.106           | 0.096            | 0.109           |\n| **Average** | **0.053**        | **0.058**       | **0.052**        | **0.058**       |\n\nWe can see that the difference between $P_\\textrm{prior}'$ (i.e., estimated with a different permutation set) and $P_\\textrm{prior}$ is marginal (their L1 distance is quite small), which we believe could validate the soundness of our assumption."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission215/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699917091963,
                "cdate": 1699917091963,
                "tmdate": 1699998580556,
                "mdate": 1699998580556,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8tREwfTsji",
                "forum": "shr9PXz7T0",
                "replyto": "hIXDKsGzbg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission215/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission215/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A Gentle Reminder of the Final Feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer CGVT,\n\nWe would like to thank you for your time and comments. We hope our previous response has adequately resolved your questions or concerns. As the deadline for the ICLR rebuttal period is approaching, we look forward to hearing your feedback on our response, and would be pleased to clarify any additional questions.\n\nBest,\n\nAuthors"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission215/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700616161689,
                "cdate": 1700616161689,
                "tmdate": 1700616161689,
                "mdate": 1700616161689,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]