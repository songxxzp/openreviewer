[
    {
        "title": "End-to-End Spatio-Temporal Action Localisation with Video Transformers"
    },
    {
        "review": {
            "id": "K4A3zwz9f4",
            "forum": "Va4t6R8cGG",
            "replyto": "Va4t6R8cGG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2440/Reviewer_PJAV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2440/Reviewer_PJAV"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a fully end-to-end, DETR-based action localization model.  Their method is a one-stage, proposal-free method. The authors factorize the queries into spatial queries and temporal queries, which allows a consistent parameterization across different datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is generally well-written.\n\nThe experiments look solid and many details have been provided in the paper and in the supplementary. The authors have also promised to release the code."
                },
                "weaknesses": {
                    "value": "Firstly, I cannot help but feel like the proposed designs and improvements are rather small over previous works. For instance, most of the novelty comes from the spatial and temporal factorized queries, which, while practical and beneficial for experiments, is not very interesting.\n\n\n\nFor the experiments, it feels like the largest improvements came from ViViT/L backbone, on AVA and AVA-K. Similarly, the pretraining using CLIP also seems to contribute most of the improvements in UCF-101-24, and much of the improvement in AVA and AVA-K. Comparatively, the improvements (especially on AVA, AVA-K) are not significant when the pretraining settings are the same as previous works (TubeR). \n\nBut, I understand that the authors have issues reproducing their TubeR\u2019s code, i.e., the actual improvement is currently hard to quantify. Due to the similarities between the two pipelines, I suggest the authors to run some experiments according to TubeR\u2019s method (i.e., their action-based parameterization and no query factorization) using the exact pre-training settings in this paper, and report them, which I believe will add to the experimental contributions of this paper.\n\n\n\n\n\nOverall, the idea is not very interesting to me, but the experiments look solid, and seems to provide a good baseline for future works."
                },
                "questions": {
                    "value": "Some other questions are below.\n\n1)\tHow exactly are the queries binded to each person in the video? I don\u2019t see any such explicit constraints.\n2)\tHow fast is the proposed method compared to other methods? I understand that there are some GFLOPs comparisons in the supplementary, but it is difficult to compare the methods due to the presence of other parts (such as LTC or person detector). Could we see a speed comparison instead?\n3)\tI am rather curious regarding the possible integration with other foundational models. Since the proposed method requires special designs (such as spatial and temporal query factorization), is it more difficult or easier to integrate with other video-based foundational models (as reported in Table 7), which tend to be able to perform many other tasks as well."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2440/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698250785520,
            "cdate": 1698250785520,
            "tmdate": 1699636179943,
            "mdate": 1699636179943,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "gnKjk6lISz",
            "forum": "Va4t6R8cGG",
            "replyto": "Va4t6R8cGG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2440/Reviewer_J8Sj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2440/Reviewer_J8Sj"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a fully end-to\u0002end, transformer based model that directly ingests an input video, and outputs tubelets, which requires no additional pre-processing and post-processing. The extensive ablation experiments on four spatio-temporal action localisation benchmarks verify the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ This paper is well-written and well-organized.\n+ Good performance on the popular benchmarks."
                },
                "weaknesses": {
                    "value": "- This paper does not seem to be the first work of fully end-to-end spatio-temporal localization, while TubeR has proposed to directly detect an action tubelet in a video by simultaneously performing action localization and recognition before. This weakens the novelty of this paper. The authors claim the differences with TubeR but the most significant difference is that the proposed method is much less complex.\n- The symbols in this paper are inconsistent, e.g., b.\n- The authors need to perform ablation experiments to compare the proposed method with other methods (e.g., TubeR) in terms of the number of learnable parameters and GFLOPs."
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2440/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2440/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2440/Reviewer_J8Sj"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2440/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698555426260,
            "cdate": 1698555426260,
            "tmdate": 1699636179870,
            "mdate": 1699636179870,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "LLNVlcVKNz",
            "forum": "Va4t6R8cGG",
            "replyto": "Va4t6R8cGG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2440/Reviewer_zmts"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2440/Reviewer_zmts"
            ],
            "content": {
                "summary": {
                    "value": "The presented works address the problem of action tubelet prediction without the requirement of memory banks from similar work by Zhao et.al. The main contribution of the work is that it is able to perform well while removing the need for a memory bank when the same backbone is used. One can train this tubelet prediction when sparse annotations are available."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The idea of the factorised query is a good one, it makes spatial temporal query search space quite tractable. Not sure if that is from the author is it borrowed from Zhao et al or others. \n- Being able to predict tubelet even when sparse annotations are available is a plus. \n- Removing the need of a memory bank is a good step forward toward generalisation"
                },
                "weaknesses": {
                    "value": "I think the paper is written well but the numbers are a bit overhyped. The proposed work is a good extension of Zhao et al 2023b (SOTA), however, the numbers in the table show that most of the dramatic improvement over SOTA is because of the use of a better transformer backbone and better pertaining. At the same time, it improves over SOTA slightly 31.1 to 31.4 without using memory banks but the decoder used in STAR is bigger.  \n\nMinor negative\nThe authors mentioned that they do not require any post-processing in the abstract but they do for the causal linking algorithm, which should be cited to Singh et al. (2017) because Kalogeiton et al., 2017 borrow from the aforementioned. \n\nMore or less I am happy with the paper, please try to answer the question below so I can participate in the discussion."
                },
                "questions": {
                    "value": "Table one shows significant improvement over TubeR with the use of person-bound tubelets compared to action-bound. Then why the gap is so small in Table 4 between TubeR and STAR with CSN backbone?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2440/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698871150927,
            "cdate": 1698871150927,
            "tmdate": 1699636179804,
            "mdate": 1699636179804,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "KpK0wCtMHh",
            "forum": "Va4t6R8cGG",
            "replyto": "Va4t6R8cGG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2440/Reviewer_dyfN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2440/Reviewer_dyfN"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes an architecture for Spatio-Temporal Action Detection in videos. The proposed architecture, namely STAR, can be trained end-to-end without the need of additional human detectors or external memory banks. The technical design is simple yet effective. Experiments are done on 4 different datasets: AVA, AVA-Kinetics, UCF-24, JHMDB. Ablation studies are thorough and enough to understand the design choice. STAR outperforms or on par with state-of-the-art methods on the four evaluating benchmarks. Written presentation is clear and easy to read and follow."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The proposed architecture is very simple and still being effective, be on par or outperform state-of-the-art approaches. On small datasets such as UCF and JHMBD, START strongly outperforms previous methods. On larger datasets such as AVA, AVA-Kinetics, STAR also gives competitive performance.\n- Solid ablation experiments: The paper provides a thorough set of ablation experiments to validate most of components / design choices.\n- Written presentation is with high clarity which helps the readers easy to read and follow."
                },
                "weaknesses": {
                    "value": "- On AVA and AVA-Kinetics, it seems the key recipe for STAR is using CLIP, without CLIP STAR achieves 30-31 on AVA and 35-36 on AVA-Kinetics which are much lower compared state-of-the-art (e.g., VideoMAE v2: 42.6 on AVA and 43.9 on AVA-Kinetics). Even with model with less-pre-training, i.e., Co-fine-tuning gets 36.1 and 36.2, respectively on AVA and AVA-K. Can we have a direct comparison with other method such as TubeR where TubeR is pre-trained with CLIP & K700? Also can the author(s) provide further discussion / insights about the role of CLIP, what make it useful for STAR that much?"
                },
                "questions": {
                    "value": "- In table 7, the paper flagged InternVideo and VideoMAE v2 as \"web-scale pre-trained\", what is the size / definition of web-scale? Why CLIP, JFT, or IG65M is not considered \"web-scale\"?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2440/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698949441343,
            "cdate": 1698949441343,
            "tmdate": 1699636179743,
            "mdate": 1699636179743,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bNN3ZKH2si",
                "forum": "Va4t6R8cGG",
                "replyto": "KpK0wCtMHh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2440/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2440/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the review, and we are glad to see that they recognised our method's effective simplicity, extensive ablations supporting the method\u2019s design, and clarity of presentation. We hope to resolve the remaining questions through this discussion.\n\n\n**Specific responses**\n\n\n*On AVA and AVA-Kinetics, it seems the key recipe for STAR is using CLIP, without CLIP STAR achieves 30-31 on AVA and 35-36 on AVA-Kinetics which are much lower compared state-of-the-art (e.g., VideoMAE v2: 42.6 on AVA and 43.9 on AVA-Kinetics). Even with model with less-pre-training, i.e., Co-fine-tuning gets 36.1 and 36.2, respectively on AVA and AVA-K.*\n\nA direct comparison to VideoMAE v2 or Co-finetuning would not be fair, as these methods differ in:\n * The amount of video data they have been exposed to. For example, Video MAE v2 was pre-trained on \u201caround 1.35M clips in our mixed dataset and this is the largest dataset ever used for video masked autoencoding\u201d; and Co-finetuning was trained on WTS (a proprietary dataset; > 50M), Kinetics 700 (> 500K), Moments in Time (> 700K) and Something-Something V2 (> 160K) video datasets; whereas our method only made use of Kinetics 400 (> 200K) or Kinetics 700 for video pre-training.\n* Both, Co-finetuning and VideoMAE are two-stage methods, meaning that they require external bounding box proposals produced by object detectors applied independently to each frame at high resolution. In contrast, our method is end-to-end and does not require external box proposals during inference.\n* And, in the case of VideoMAE v2, the backbone used is a much larger model. VideoMAE v2 uses ViViT/g - a billion parameter model instead of ViViT/L used by our method.\n\nOn top of that, neither VideoMAE v2, nor Co-finetuning are capable of producing tubelet predictions, even if external box proposals were provided for every frame.\n\nHowever, to facilitate the comparison with Co-finetuning, we have trained STAR models that use IN21K + Kinetics 700 pre-training on AVA Kinetics. Our results below show that we outperform Co-finetuning in this more controlled comparison despite being a faster end-to-end method that directly outputs tubelets, and despite the fact that Co-finetuning additionally trains on SSv2 and MiT datasets (> 860K additional videos not seen by our models).\n\n| Method  | mAP50  |\n|---|---|\n| Co-finetuning (IN21K + K700 + MiT + SSv2)  | 33.1 |\n| STAR/L (IN21K + K700)  | **33.7** |\n\n\n*Can we have a direct comparison with other method such as TubeR where TubeR is pre-trained with CLIP & K700?*\n\nUnfortunately a comparison to TubeR has proven to be challenging (see General response, as well as Supp. Section B). However, to address the reviewer's question, we have provided a more direct comparison to the Co-finetuning method (see previous part of this response and the table within).\n\n*Also can the author(s) provide further discussion / insights about the role of CLIP, what make it useful for STAR that much?*\n\nCLIP initialisation has been shown to lead to strong empirical results across a variety of tasks, such as for example semantic segmentation (Koppula *et al.*),  video classification (Lin *et al.*; Pan *et al.*) or object detection (Minderer *et al.*, 2022). We speculate that this has to do with the data that public CLIP models have been pre-trained on. However, as the details of  the exact data mixture used for pre-training these models is not available, we cannot be certain.\n\n*In table 7, the paper flagged InternVideo and VideoMAE v2 as \"web-scale pre-trained\", what is the size / definition of web-scale? Why CLIP, JFT, or IG65M is not considered \"web-scale\"?*\n\nWe thank the reviewer for their sharp observation. Admittedly, we did not apply a stringent criterion to what would constitute a \u201cweb-scale foundational model\u201d, and instead used it to highlight models that have been exposed to large amounts of video data during pre-training. InternVideo and VideoMAE v2 clearly fall in this category, as they have respectively been pre-trained on 12M and over 1M videos. However, IG65M (65M videos) and WTS (> 50M videos) pre-training should also be included in this category, and that is an oversight on our side. We will mark IG65M and WTS models as grey in Table 7, and also change the wording to emphasise large-scale video pre-training. Finally, we would like to highlight that both - VideoMAE v2 and InternVideo are models with over 1B parameters, whereas our largest models contain ~400m parameters (i.e. less than 40%).\n\n\n**References**\n\nKoppula *et al.*, 2022 \u201cWhere Should I Spend My FLOPS? Efficiency Evaluations of Visual Pre-training Methods\u201d\n\nLin *et al.*, 2022 \u201cFrozen CLIP Models are Efficient Video Learners\u201d\n\nPan *et al.*, 2022 \u201cST-Adapter: Parameter-Efficient Image-to-Video Transfer Learning\u201d\n\nMinderer *et al.*, 2022 \u201cSimple Open-Vocabulary Object Detection with Vision Transformers\u201d"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2440/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699998081381,
                "cdate": 1699998081381,
                "tmdate": 1699998081381,
                "mdate": 1699998081381,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]