[
    {
        "title": "Decoupled Marked Temporal Point Process using Neural Ordinary Differential Equations"
    },
    {
        "review": {
            "id": "kYCyBTg3ee",
            "forum": "BuFNoKBiMs",
            "replyto": "BuFNoKBiMs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4790/Reviewer_yduQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4790/Reviewer_yduQ"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new Dec-ODE model to learn marked temporal point processes (MTPPs). The Dec-ODE model takes into account the individual events' influence on the underlying dynamics of the whole process and models it as a neural-ODE. Since the model is decoupled into ground intensity and conditional mark distribution, the new approach can compute the integrals for different parts in parallel, making the training faster. The experiment shows the effectiveness of the new model."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed approach is novel. The paper takes into account the events' influence on the underlying dynamics of the later process, which is a significant factor but under-explored before.\n2. The decoupled model facilitates parallel computing of the costly integrals that occur in the neural MTPP formulations. \n3. The influence model gives Dec-ODE some extent of explainability as a model of MTPP."
                },
                "weaknesses": {
                    "value": "1. There is a lack of comparison in computation time across different models. The parallel computing scheme is shown to reduce the computation time of neural ODE, but it is unclear whether Dec-ODE runs faster than baseline models like THP."
                },
                "questions": {
                    "value": "1. I could not find the reference Yang el al., 2020 for the THP baseline. Is the correct one be [1]?\n\n[1] Zuo, S., Jiang, H., Li, Z., Zhao, T., & Zha, H. (2020, November). Transformer hawkes process. In International conference on machine learning (pp. 11692-11702). PMLR."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4790/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4790/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4790/Reviewer_yduQ"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4790/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698641952275,
            "cdate": 1698641952275,
            "tmdate": 1699636461370,
            "mdate": 1699636461370,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RTZmHL4hH0",
                "forum": "BuFNoKBiMs",
                "replyto": "kYCyBTg3ee",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4790/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4790/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for highly appreciating the novelty and ideas of our work. We provide answers to your concerns below:\n\n- **Comparison in computation time across different models?** Please see the general response at the top where we provide evaluations for computation time per epoch. \n- **Reference for THP missing?** We appreciate the reviewer for pointing it out, and we have rectified this error with the author name and conducted a thorough review of all other references to ensure correctness."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4790/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700145803676,
                "cdate": 1700145803676,
                "tmdate": 1700145829918,
                "mdate": 1700145829918,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "SXF2OkUWN9",
            "forum": "BuFNoKBiMs",
            "replyto": "BuFNoKBiMs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4790/Reviewer_Ux3F"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4790/Reviewer_Ux3F"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new neural ODE-based framework for modeling marked Hawkes/mutually-exciting point processes. The framework models the intensity function (of time) as a summation of event-triggered trajectories, each of which solves a neural ODE with an initial state that depends on the time of each event and its mark. The additivity of event-triggered trajectories on the intensity function allows us for the efficient parallel computation for model training and the explainable analysis of the dependence between events, which is the advantage over previous neural network-based Hawkes processes."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The idea of using neural ODEs to model the effect of an event on the subsequent events (essentially, triggering kernel) in temporal point processes is novel and promising.\n- The parallel computing scheme of fitting in the proposed model is beneficial for practical purposes.\n- The validity of the proposed model was evaluated on various real-world data."
                },
                "weaknesses": {
                    "value": "- The authors insist that an advantage of the proposed method (Dec-ODE) over conventional methods (e.g., Yang et al., 2020) is that Dec-ODE can compute multiple integrations simultaneously [Section 3.2]. But Dec-ODE demands to solve ODEs to compute integrations, which is generally more time-consuming than Monte Carlo integrations needed in conventional methods. In Experiment, there are no comparative analysis about computation time (sec/iter), and the authors\u2019 insistence about computation efficiency remains to be verified. \n- The authors insist that an advantage of Dec-ODE over conventional methods (e.g., Yang et al., 2020) is the explainability due to the decoupled structure. But the decoupled structure has been adopted intensively in the literature, which were not in the benchmark models in Experiment. To verify the insistence, the authors need to compare Dec-ODE with references with the decoupled structure, which would make the pros/cons of the proposed model clearer.\n- RMTPP is a standard benchmark model in the literature, and should be included in comparative experiments. Otherwise, the authors need to mention the reason for not including it. \n- The explanation of the experiment setup seems to be inadequate for reproducibility. The details of neural networks (e.g., activation functions) and the machine spec used in the experiments should be shown.\n- The detailed equation about how to compute the derivative of the likelihood regarding model parameter is not shown.\n- Discussions about the limitation of the proposed model are not in the paper.\n- To the best of my knowledge, there are sentences that seem technically inaccurate as follows, which raises a question about the paper\u2019s quality:\n\t- [3rd paragraph in Section 1] the authors introduce neural Hawkes process (Omi et al., 2019) as a marked temporal point process (MTPP), but the Omi\u2019s paper did not consider the marks of each event. Also, there are two Omi\u2019s papers (2019a and 2019b), but they look identical.\n\t- [2nd paragraph  in Section 2] the authors explains that the modeling of intensity function is preferred over the pdf $f^*(t)$ due to the complex dynamics of $f^*(t)$, but the complex dynamics depending on the past events is not limited to $f^*(t)$.\n\t- [2nd paragraph in Section 2] $\\\\int_0^{\\\\infty} f^*(s) ds = 0$ is correctly $\\\\int_0^{\\\\infty} f^*(s) ds = 1$.\n\t- [3rd paragraph in Section 2] the sentence \u201c$\\\\mathcal{N}_g = \\\\{ t_i \\\\}$ is a temporal point process\u201d seems to fail to explain the definition of $\\\\mathcal{N}_g (t_N)$ in Eq. (2). Do the authors mean that $\\mathcal{N}_g (t)$ is a counting process?\n\t- [Eq. (9)] the definition of $f_{\\theta}(h,t)$ is not found.\n\t- [References] The authors of Transformer Hawkes process are not Yang et al., but Zuo et al."
                },
                "questions": {
                    "value": "See Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4790/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4790/Reviewer_Ux3F",
                        "ICLR.cc/2024/Conference/Submission4790/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4790/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698703374404,
            "cdate": 1698703374404,
            "tmdate": 1700153919844,
            "mdate": 1700153919844,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "aGF1Or3oKr",
                "forum": "BuFNoKBiMs",
                "replyto": "SXF2OkUWN9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4790/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4790/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate the reviewer for the constructive comments, which will greatly help improve the paper's quality and soundness. Please see our answers to the questions below, and we hope to overturn the reviewer's decision. \n\n- **Is Dec-ODE computationally more efficient than others?** The reviewer raised a good point and we are sorry for the misleading text in Section 3.2. What we intended to argue was that computation with Neural ODEs is heavy by its nature but we mitigated the issue with the characteristics of the differential equation within our proposed framework, simultaneously solving the multi-dimensional differential equation with respect to the hidden state $h_t$. The actual computation time per epoch is given in the comment to all reviewers, where Dec-ODE takes comparable or less time than the SOTA approach. Please see the revised text in the updated manuscript highlighted in yellow starting from the bottom of page 4. We would like to make it clear that faster computation is not the advantage of our model (nor we argue it as a contribution in the introduction), but we proposed an efficient learning of a flexible model which would have taken much longer computation without our proposed optimization scheme. However, we still want to mention that numerical computation of integrals often leads to more accurate results than MCMC, which may lead to faster convergence even if sec/iter may be slower. Moreover, other methods that involve transformer do not consider the temporal nature (e.g., Markovian) of the events and need to perform integration iteratively whenever a new event is introduced. \n\n- **Decoupled structure is already well-known?** We are aware that the decoupling structure of time and event in MTPP is a general concept as we also mentioned in the Background section for MTPP. However, to the best of knowledge, decoupling individual events through separate intensity functions has been rarely studied. Through some investigation, we found a recent work VI-DPP(Panos et al., AISTATS 2023) which uses the decoupled structure of time and event. We are currently performing experiments with VI-DPP to compare decoupling behavior of Dec-ODE, whose results will be updated soon.\n\n- **Why not compare with RMTPP?** We appreciate the reviewer for pointing us to the RMTPP as a fundamental baseline. We reported the results only from the latest literature so we missed it, but we are more than happy to include RMTPP results in our paper. The results from RMTPP under our experimental setting are given in the table below which is now updated in the revised paper. The results from RMTPP compared with Dec-ODE can be found in the table below. The implementation provided by the author of RMTPP was employed, with minor adjustments made to accommodate length-varying sequences and present the results in a consistent format. The hyper parameters were found using a grid search from the list of options that can be found in RMTPP Section 6.4 paragraph 5. In most cases RMTPP showed a good performance in the time prediction task with best RMSE on StackOverflow data, and its NLL in MIMIC dataset showed a better result compared to Dec-ODE. \n\n- **Implementation details?** We feel sorry that we missed these important implementation descriptions. We utilized simple 3-layer Multi Layer Perceptrons (MLPs) for implementing the Neural ODE $\\gamma()$ with linear activations. Specifically, softplus normalization on the weights is used for parameterizing $\\gamma (\\theta)$ as in (Chen et al., ICLR 2021). For decoders, 3-layer MLPs with ReLU activations have been used. All experiments were performed with a single NVIDIA RTX A6000.  \n\n- **Details on deriving the derivative of the likelihood?** The derivative of the likelihood regarding the model parameter is calculated using the adjoint sensitivity method in (Chen et al. NeurIPS 2018), which is extensively discussed in the referenced paper. For detailed description and implementation please refer to the Neural ODEs (Chen et al. NeurIPS 2018) and the public repository (https://github.com/rtqichen/torchdiffeq)."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4790/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700143263779,
                "cdate": 1700143263779,
                "tmdate": 1700145555456,
                "mdate": 1700145555456,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "65T9Y2Atqp",
                "forum": "BuFNoKBiMs",
                "replyto": "SXF2OkUWN9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4790/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4790/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "- **Limitations?**  A major limitation is that we assume that each $\\mu(t)$ is independent from other events. Even though such an approach is beneficial for interpretability, it may be limited in learning inter-event relationships. Also, even though Dec-ODE shows comparable training time, it requires longer training time for an iteration than most conventional methods due to the flexible but complex nature of ODEs. However, notice that the actual computation time per epoch is quite reasonable and even better than ANHP (shown in the common question) given our proposed training scheme. Finally, the experimented version of the Dec-ODE can only express a self-excitatory effect, where each event only increases the intensity but not decreases, which makes the model less flexible. Actually, if we design the model to both increase and decrease the intensity simultaneously, the performance becomes better as we have shown in the Appendix A.1. However, in this case, the parallel training scheme in Section 4.3  cannot be adopted as it violates eq. (16) in the paper, and therefore we had to leave it in the Appendix.\n\n- **Technical errors?** We have corrected all the technical issues addressed by the reviewer and went through the paper again to check for other errors if there were any. These errors can be easily corrected in the text and do not affect the soundness of our model. We really appreciate the reviewer for going through the paper thoroughly to improve its quality. \n   - Reference errors for Omi et al, NeurIPS 2019 and Zuo et al, ICML 2020?: Corrected.\n   - Complex dynamics depending on the past events is not limited to $f^*(t)$?: Our objective was to underscore the challenge of modeling the probability density function $f^*(t)$ with a parametric model. As the intricate nature of the dynamics involved is quite diverse, describing these temporal variations through intensity functions allows for a more flexible modeling, often achieved with nonparametric methods.\n   - Do the authors mean that N_g(t) is a counting process?: The $N_g$ is a temporal point process with a realization $\\\\{ t_i \\\\}_{i=1} ^N$. For clarity, the text regarding the counting process in the 1st paragraph in Section 2 is removed for more precise delivery. \n   - In 2nd paragraph in Section 2, $\\int f^*(t) = 0$?: It is now $\\int_0 ^{\\infty} f^*(t) = 1$.\n   - In Eq. (9), the definition of $f_\\theta(\u210e,t)$ is not found?: It should be $\\gamma (\\bf{h_t}, t, \\mathbf{e_i} ;\\theta) $ and not $f_\\theta ()$, where $\\mathbf{e_i}$ is a vector of  events corresponding to $\\mathbf{h_t}$."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4790/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700145278785,
                "cdate": 1700145278785,
                "tmdate": 1700145435946,
                "mdate": 1700145435946,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "36zMVHWbVa",
                "forum": "BuFNoKBiMs",
                "replyto": "SXF2OkUWN9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4790/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4790/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "|  |  | MOOC |  |  | Reddit |  |  | Retweet |  |  | SO |  |  | MIMIC |  |\n|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n|  | RMSE | ACC | NLL | RMSE | ACC | NLL | RMSE | ACC | NLL | RMSE | ACC | NLL | RMSE | ACC | NLL |\n| RMTPP | 0.4732 (0.0124) | 20.98 (0.29) | -0.3151 (0.0312) | 0.9527 (0.0164) | 29.67 (1.19) | 3.5593 (0.07) | 0.9901 (0.0164) | 51.72 (0.33) | -2.1803 (0.0246) | **1.0172 (0.0112)** | 53.95 (0.32) | 2.1561 (0.0217) | 0.8591 (0.093) | 78.20 (5.0) | **1.1668 (0.15)** |\n| Dec-ODE | **0.4669 (0.0117)** | **42.08 (0.44)** | **-2.2885 (0.1913)** | **0.9335 (0.0169)** | **62.32 (1.1)** | **1.367 (0.1258)** | **0.9849 (0.0164)** | **60.22 (0.23)** | **-2.8965 (0.0304)** | 1.0372 (0.0110) | **55.58 (0.29)** | **2.0633 (0.0161)** | **0.6529 (0.1734)** | **85.06 (3.65)** | 1.3547 (0.3972) |"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4790/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700150613624,
                "cdate": 1700150613624,
                "tmdate": 1700150672151,
                "mdate": 1700150672151,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xYlAXeYwoU",
                "forum": "BuFNoKBiMs",
                "replyto": "SXF2OkUWN9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4790/Reviewer_Ux3F"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4790/Reviewer_Ux3F"
                ],
                "content": {
                    "title": {
                        "value": "Response to author comments"
                    },
                    "comment": {
                        "value": "Thank you for the authors\u2019 clarification and efforts. I stand by the point that the proposed model is promising but not clearly impactful. Still I appreciate the effort and I will increase my score to 5.\n\n**Is Dec-ODE computationally more efficient than others?** Thank you for the comparative analysis about computation time, which makes the computational pros/cons of the proposed method clearer. \n\nNote: I cannot understand why the authors mention about MCMC. The authors might to confuse Monte Carlo integration with MCMC?\n\n**Decoupled structure is already well-known?** MTPP models with decoupled structures have been actively investigated in the context of discovering latent network structures: For example, (Iwata, 2013) and (Yuan, 2019) adopted exponential decay kernels, and (Linderman, 2014) adopted logistic-normal density kernels. The idea of MTPP with decoupled structure is a straightforward extension of the classical Hawkes process, and is itself rather conventional. The comparison with VI-DPP is of course valuable, but the authors should discuss the necessity of the Neural ODE-based model against the references. Indeed, the idea of using neural ODEs to model triggering kernels in marked temporal point processes is new, but there is no value in using Neural ODE itself.\n\n**Why not compare with RMTPP?** Thank you for the analysis with RMTPP.\n\n**Technical errors?** If the authors define $\\mathcal{N}_g$ as a realization of {t_i}, they should define $\\mathcal{N}_g (t)$ separately, which is used in several equations. If the authors define $\\mathcal{N}_g (t)$ as the total number of events occurring before $t$, then $\\mathcal{N}_g (t)$ should be introduced as a counting process."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4790/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700154114094,
                "cdate": 1700154114094,
                "tmdate": 1700154767320,
                "mdate": 1700154767320,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "odbbSEXBP0",
            "forum": "BuFNoKBiMs",
            "replyto": "BuFNoKBiMs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4790/Reviewer_A48s"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4790/Reviewer_A48s"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new modeling framework for marked point processes, where the mark distribution and the conditional intensity function are treated as two separate modeling objectives. These two objectives are assumed to depend on a common latent process that is evolved through a neural ODE model. The proposed algorithm is more computationally efficient than existing methods and provides more interpretable model estimates. The effectiveness of the algorithm is demonstrated by comparisons of five benchmark data sets to state-of-art methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is well-written and the presentation is clear. The proposed idea is sound and may have a large potential impact."
                },
                "weaknesses": {
                    "value": "No simulation studies are conducted to demonstrate the estimation accuracy of the proposed algorithm when the data-generating process is assumed in the paper."
                },
                "questions": {
                    "value": "The paper is quite well written, so I don't have many questions. My major concern is that no simulation studies are conducted to demonstrate the estimation accuracy of the proposed algorithm when the data-generating process is assumed in the paper. Some comparisons with existing methods are desirable as well. That way, one can have better ideas of the advantages and limitations of the proposed method, in terms of estimation accuracy, predictive accuracy, and computing times."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4790/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699377076416,
            "cdate": 1699377076416,
            "tmdate": 1699636461182,
            "mdate": 1699636461182,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6o26jYKJs5",
                "forum": "BuFNoKBiMs",
                "replyto": "odbbSEXBP0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4790/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4790/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We sincerely thank the reviewer for highly valuing the ideas and presentations given in our paper. \n\n- **No simulation studies?**: We are currently performing a simulation study with synthetically generated data from Neural Hwakes Process (NeurIPS 2017). We will update the results soon, which will also be discussed in the paper (or in the Appendix). Regarding the computing time, the comparisons on the real-world data are given in the general response at the top of this rebuttal."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4790/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700146080932,
                "cdate": 1700146080932,
                "tmdate": 1700146152482,
                "mdate": 1700146152482,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]