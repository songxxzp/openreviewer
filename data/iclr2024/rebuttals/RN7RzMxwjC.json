[
    {
        "title": "Harmony World Models: Boosting Sample Efficiency for Model-based Reinforcement Learning"
    },
    {
        "review": {
            "id": "E3stUXd0Mz",
            "forum": "RN7RzMxwjC",
            "replyto": "RN7RzMxwjC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission615/Reviewer_pH7N"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission615/Reviewer_pH7N"
            ],
            "content": {
                "summary": {
                    "value": "Harmony World Models: Boosting Sample Efficiency for Model-based Reinforcement Learning\n\nIn this paper, the authors observe that the coefficients of reward loss and dynamic loss, if not chosen carefully, could affect the performance of the final model-based RL algorithm.\nAnd the authors propose to use a set of learnable parameters to approximate loss scale to balance the loss.\nThere are performance improvements introduced as shown in the experiments."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is well written and easy to understand.\n2. The proposed method is quite straight-forward, making it reproducible and easy to apply to existing algorithms."
                },
                "weaknesses": {
                    "value": "1. There\u2019s no good ablation experiments on different reward blending schemes.\n\n    The chosen method for blending/weighting the loss terms looks reasonable, but not necessarily the best method or even the most reliable one. \n\n    There needs to be some other weighting methods tested in the experiment sections.\n\n    Some simple weighting methods should be compared, which include for example the one in [1] and some other straight-forward heuristic methods.\n\n2. It would also be helpful to show how sensitive the proposed method is. \n\n    Does it require separate tuning for each task? \n    \n    If it needs to be tuned, how much effort is needed? Is it stable across random seeds?\n\n3. More baselines and environments are needed.\n\n    The proposed method is only compared against dreamerV2 in a small subset of tasks. \n\n    What will happen if it is compared to MuZero, TD-MPC and SimPle etc as mentioned in the paper?\n\n    And what will happen if the algorithm is tested in similar environments such as GO, those atari environments or locomotion control tasks from image?"
                },
                "questions": {
                    "value": "The performance of DreamerV2 seems too bad in some environments like Cheetah Run, Walker Run, which is hard to figure out why considering DreamerV2 was applied to far harder problems and succeeded and DreamerV1 has better performance with the same number of training samples.\n\nWhy does it happen?\n\n[1] Kendall, Alex, Yarin Gal, and Roberto Cipolla. \"Multi-task learning using uncertainty to weigh losses for scene geometry and semantics.\" In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 7482-7491. 2018."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission615/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698355719511,
            "cdate": 1698355719511,
            "tmdate": 1699635989185,
            "mdate": 1699635989185,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RUnjFhu6xY",
                "forum": "RN7RzMxwjC",
                "replyto": "E3stUXd0Mz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission615/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission615/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer pH7N"
                    },
                    "comment": {
                        "value": "We sincerely thank Reviewer pH7N for providing a detailed review and insightful questions. \n\n**Q1**: Comparisons to different reward blending schemes\n\nWe want to clarify that **the major contribution of our paper is to reveal the overlooked potential of sample efficiency in MBRL through a multi-task perspective, not to propose a specific multi-task method**. Our experiments find that our straightforward method is dramatically effective and sufficient to address the problem revealed in our paper.\n\nWe have compared with different multi-task methods and discussed why these methods can hardly make more improvements in the problem of our paper. Please refer to $\\underline{\\text{Q2 of the global response}}$ for the detailed response.\n\n**Q2**: Sensitivity of HarmonyWM\n\n**Our HarmonyWM does not add hyperparameters** to our base method and, therefore, does not need additional tuning for different tasks. The improvement is stable across random seeds, as shown in the confidence intervals reported in our figures.\n\n**Q3**: Additional baselines and environments\n\nInevitably, the scale of our experiments is restricted due to our limited computational resources. We have conducted experiments on 11 (Metaworld, RLBench, DMCR) and 4 environments (DMC) in the main paper and appendix of the original submission. During rebuttal, we added 4 additional environments (Assembly in Metaworld and Qbert/Seaquest/Boxing in Atari). The number of experimental environments in our work has been on par with or larger than most recent publications on MBRL (please refer to $\\underline{\\text{response to Q3 of Reviewer 72ZS}}$).\n\nRegarding compared baselines, since our HarmonyWM is based on DreamerV2, it is natural to compare it mainly with DreamerV2 to reveal the significance of harmonizing loss scales among world model learning tasks. We mention MuZero, TD-MPC and SimPLe in Figure 1 to show the high-level spectrum of world model learning. Some model-based methods are tailored to a specific area and therefore unfair to conduct head-to-head comparisons. \n\nRegarding the environments Reviewer pH7N proposed, it is beyond our computational resources to conduct experiments on GO, and also unfair since MuZero with MCTS is mainly tailored to board games and unable to handle continuous control, while both Dreamer and HarmonyWM are more general architectures. For the Atari domain, SimPLe is a weak baseline and is outperformed by DreamerV3, therefore we perform additional comparisons to DreamerV3. The corresponding results can be found in $\\underline{\\text{Fig. 23 of Appendix D.10}}$ of our latest revision. For locomotion control tasks from images, we have already presented comparisons to TD-MPC and DreamerV2 on the DMCR domain. Results can be found in $\\underline{\\text{Fig. 10(b) of main paper}}$ and $\\underline{\\text{Fig. 18 of Appendix D.5}}$.\n\n\n**Q4**: DreamerV2 performance on DMC Remastered benchmark\n\nRegarding the comment that DreamerV2 performs poorly in environments like Cheetah Run and Walker Run, we need to clarify that we use the **DMC Remastered** benchmark in our main paper, **which is different from and more challenging than the original DMC**, as it contains various random visual distractors, showcased in the $\\underline{\\text{left of Fig 8 of main paper}}$, and has proven difficult for model-based agents to learn on. DreamerV2 performs well on the original DMC benchmark, as shown in $\\underline{\\text{Fig. 17 of Appendix D.5}}$, but performs poorly on DMC Remastered as it suffers from the domination of reconstructing irrelevant visual distractors in world model learning.\n\nPlease let us know whether our response addresses all your concerns. We are more than happy to discuss more if you have further questions or more suggestions to improve our paper."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission615/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700148607670,
                "cdate": 1700148607670,
                "tmdate": 1700549094318,
                "mdate": 1700549094318,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mU5sGnUxAX",
                "forum": "RN7RzMxwjC",
                "replyto": "E3stUXd0Mz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission615/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission615/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Request of Reviewer's attention and feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer pH7N,\n\nThanks again for your dedication to reviewing our paper.\n\nWe write to kindly remind you that this is the last few days of the reviewer-author discussion period. We have made every effort to address the concerns you suggested and improve our paper:\n\n* We have made comprehensive comparisons to other weighting methods and discussed why these methods can hardly make more improvements in the problem of our paper.\n* We clarify that our method does not add hyperparameters, and the improvement is stable across random seeds.\n* We perform additional experiments on the Atari domain proposed by you and show that even though the issue of observation modeling domination is not as pronounced in the Atari domain, our method can still enhance the base method's performance on more intricate tasks.\n* We clarify that we use the DMC Remastered benchmark in our main paper, which is different from and more challenging than the original DMC. This factor accounts for the poor performance of DreamerV2 on the DMCR Cheetah Run and Walker Run tasks.\n\nPlease kindly let us know if you have any remaining questions. If our responses have addressed your concerns, would you please consider re-evaluating our work based on the updated information? Looking forward to your reply.\n\nSincerely,\n\nAuthors"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission615/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700548828025,
                "cdate": 1700548828025,
                "tmdate": 1700548828025,
                "mdate": 1700548828025,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ClMkHfFvH4",
                "forum": "RN7RzMxwjC",
                "replyto": "E3stUXd0Mz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission615/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission615/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Discussion period ends soon"
                    },
                    "comment": {
                        "value": "Dear Reviewer pH7N,\n\nThank you once more for your invaluable review.\n\nIt is a kind reminder that **this is the last day of the 12-day Reviewer-author discussion period**. Taking your insightful suggestions seriously, we have made every effort to provide all the experiments and clarifications that we can.\n\nIf you have read our response, **we would greatly appreciate your acknowledgment and re-evaluation of our work**. We remain eagerly available for any further questions or discussions you may have. Anticipating your reply.\n\nBest regards,\n\nAuthors."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission615/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700670073046,
                "cdate": 1700670073046,
                "tmdate": 1700670073046,
                "mdate": 1700670073046,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "jktg6GIMD7",
            "forum": "RN7RzMxwjC",
            "replyto": "RN7RzMxwjC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission615/Reviewer_WtCd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission615/Reviewer_WtCd"
            ],
            "content": {
                "summary": {
                    "value": "This paper identifies the multi-task essence (the dynamics and the reward) of world models and analyzes the disharmonious interference between different tasks. The authors empirically find that adjusting the coefficient of reward loss (which is overlooked by most previous works) can emphasize task-relevant information and improve sample efficiency. To adaptively balance the reward and the dynamic losses, the authors propose a harmonious loss function that learns a global reciprocal of the loss scale using uncertainty. The experiments show that the proposed method outperforms Dreamerv2 with a considerable gap and has better generality than DreamerV3 and DreamerPro."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper systematically identifies the multi-tasking essence of world models, which is essential to model-based methods and is overlooked by most previous work. The whole paper is well-written and easy to follow.\n\n> The proposed method has generality to other model-based RL algorithms and significantly improves the sample efficiency."
                },
                "weaknesses": {
                    "value": "> The novelty of the proposed method is limited. The authors build the optimization process of world models as a multi-task optimization and then use uncertainty weighting to harmonize loss scales among tasks. More state-of-the-art methods, e.g., multi-objective optimization or multi-agent learning methods, can be considered and may further improve the performance."
                },
                "questions": {
                    "value": "> In Fig.15 & 16, HarmonyWM $ w_d=1 $ learns faster than Harmony WM at the beginning of some tasks. Could the authors give some explanations about this?\n\n> The authors claim \"the root cause as the disharmonious interference between two tasks in explicit world model learning: due to *overload of redundant observation signals* ...\". Why does, for example, Denoised MDP, which identifies the information that can be safely discarded as noises, serve as a baseline in the experiments?\n\n>  Is there a sufficient reason why the reward modeling and the observation modeling tasks are scaled to the same constant? Should we maintain the equilibrium between these tasks? Or can we emphasize some tasks at some specific states?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission615/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698685920389,
            "cdate": 1698685920389,
            "tmdate": 1699635989117,
            "mdate": 1699635989117,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kAxhi9HFVw",
                "forum": "RN7RzMxwjC",
                "replyto": "jktg6GIMD7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission615/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission615/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer WtCd"
                    },
                    "comment": {
                        "value": "We sincerely thank Reviewer WtCd for providing a detailed review, insightful questions, and a positive evaluation of our paper.\n\n**Q1**: Limited novelty of the proposed method\n\nWe want to clarify that **the major contribution of our paper is to reveal the overlooked potential of sample efficiency in MBRL through a multi-task perspective**, and to propose a decent way to address this. We acknowledge that our method for balancing different tasks in world models is straightforward, but as pointed out by Reviewer pH7N, **this simplicity serves as an advantage, making it easy to reproduce and apply to existing algorithms**.\n\nRegarding the comment that we use uncertainty weighting to harmonize loss scales, we would like to note that our method differs from uncertainty weighting in several aspects. Uncertainty weighting is derived from maximum likelihood estimation, while our motivation is to harmonize loss scales among tasks. Uncertainty weighting makes assumptions on the distributions behind losses, while our method removes these assumptions and makes it possible to also balance the KL loss. Moreover, we introduce a rectified version to avoid extremely large task weights. Please refer to the $\\underline{\\text{last paragraph of Sec 3 of main paper in original submission}}$ for a more detailed explanation on this.\n\n\n**Q2**: Comparison to multi-task or multi-objective baselines\n\nOur work is irrelevant to multi-agent learning methods, as the multi-task perspective is presented for understanding the world model learning part of MBRL, instead of the policy learning part. \n\nRegarding multi-task learning methods, we have conducted comparison experiments and detailed discussions. Please refer to $\\underline{\\text{Q2 of the global response}}$.\n\n**Q3**: Comparison to Denoised MDP\n\nAlthough we share a similar point with Denoised MDP to enhance task-centric representations, our approach is entirely irrelevant to it and has the advantage of more simplicity. Our contributions regarding world model optimization are **orthogonal** to Denoised MDP regarding architecture and can be combined with it to further improve performance, which is left for future work.\n\nNevertheless, we present additional comparisons to Denoised MDP in $\\underline{\\text{Fig. 22 of Appendix D.9 of latest revision}}$, where Denoised MDP trails behind our HarmonyWM.  Denoised MDP performs information decomposition by changing the MDP transition structure and utilizing the reward as a guide to separate controllable information. However, since Denoised MDP does not modify the weight for the reward modeling task, the observation modeling task can still dominate the learning process. Consequently, the training signals from the reward modeling task may be inadequate to guide decomposition.\n\n**Q4**: Ablation on adjusting $w_d$\n\nWe observe that HarmonyWM ($w_d=1$) learns faster than HarmonyWM at the beginning of the Hammer task in terms of success rate. We argue that this is because in this task, high success rates do not strictly correspond to high return. The hammer task can be hacked by pushing the nail using the arm instead of the hammer, which results in the task's success with a low reward. When measured in terms of episode returns, HarmonyWM performs comparably with HarmonyWM ($w_d=1$) at the beginning and slightly outperforms HarmonyWM ($w_d=1$) at the end of training.\n\n**Q5**: Necessity of scaling different tasks to the same constant\n\nAs we demonstrated in our analysis ($\\underline{\\text{Sec 2.3 of main paper}}$), domination of either task in world model learning cannot fully exploit the potential of sample efficiency; thus, we do need to make a proper balance these two tasks in world model learning. We find it is dramatically effective to simply maintain the equilibrium between them, i.e., rescaling losses to the same constant. It is interesting to explore a more fine-grained balancing strategy, for example, as suggested by the reviewer, to emphasize some tasks a bit more at some specific states. This is left for future work.\n\nPlease let us know whether our response addresses all your concerns. We are happy to discuss more if you have further questions or suggestions to improve our paper."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission615/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700148570186,
                "cdate": 1700148570186,
                "tmdate": 1700148570186,
                "mdate": 1700148570186,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KQ8iy3GPUx",
                "forum": "RN7RzMxwjC",
                "replyto": "jktg6GIMD7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission615/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission615/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Request of Reviewer's attention and feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer WtCd,\n\nThanks again for your dedication to reviewing our paper.\n\nWe write to kindly remind you that this is the last few days of the reviewer-author discussion period. We have made every effort to address the concerns you suggested and improve our paper:\n\n* We clarify that a major contribution of our paper is to reveal the overlooked potential of sample efficiency in MBRL through a unified multi-task perspective into world model learning, besides proposing a decent way to address this. We also note that our method differs from uncertainty weighting in several aspects.\n* We have made comprehensive comparisons to multi-task baselines and discussed why these methods can hardly make more improvements in the problem of our paper.\n* We clarify that Denoised MDP is orthogonal to our technical contributions. Nevertheless, we provide additional comparison results against Denoised MDP and explain the possible reasons for its inferior performance.\n* We replied to the questions you mentioned in the Questions part.\n\nPlease kindly let us know if you have any remaining questions. If our responses have addressed your concerns, would you please consider re-evaluating our work based on the updated information? Looking forward to your reply.\n\nSincerely,\n\nAuthors"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission615/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700548789206,
                "cdate": 1700548789206,
                "tmdate": 1700548789206,
                "mdate": 1700548789206,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UH4QBzEYUs",
                "forum": "RN7RzMxwjC",
                "replyto": "jktg6GIMD7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission615/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission615/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Discussion period ends soon"
                    },
                    "comment": {
                        "value": "Dear Reviewer WtCd, \n\nThank you once more for your invaluable review.\n\nIt is a kind reminder that **this is the last day of the 12-day Reviewer-author discussion period**. Taking your insightful suggestions seriously, we have made every effort to provide all the experiments and clarifications that we can.\n\nIf you have read our response, **we would greatly appreciate your acknowledgment and re-evaluation of our work**. We remain eagerly available for any further questions or discussions you may have. Anticipating your reply.\n\nBest regards,\n\nAuthors."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission615/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700670053198,
                "cdate": 1700670053198,
                "tmdate": 1700670053198,
                "mdate": 1700670053198,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PJi16YbDBU",
            "forum": "RN7RzMxwjC",
            "replyto": "RN7RzMxwjC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission615/Reviewer_72ZS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission615/Reviewer_72ZS"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes viewing model learning from a multi-task perspective and introduces a method to adjust the trade-off between observation loss and reward loss to train the world model. Combined with DreamerV2, the proposed Harmony World Model achieves a noticeable performance improvement over DreamerV2 in several domains."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed method is novel and interesting.\n2. Compared to DreamerV2, there is a significant improvement in performance.\n3. The paper is well-written and easy to follow."
                },
                "weaknesses": {
                    "value": "1. Although the method proposed in this paper is novel, DreamerV3 has already addressed the issue of differing scales in reconstructing inputs and predicting rewards using Symlog Predictions. This paper primarily conducts extensive experiments in comparison with DreamerV2 and lacks more comparisons and discussions with DreamerV3. This significantly diminishes the contributions of this paper.\n\n2. Moreover, as TDMPC is mentioned in the paper as one of the Implicit MBRL methods, I believe it should also be considered as one of the important baselines for comparisons across different benchmarks.\n\n3. The experimental environments chosen in the paper are all easy tasks from different domains. I think experiments should be conducted in more challenging environments, such as Hopper-hop and Quadruped-run in DMC, Assembly and Stick-pull in MetaWorld, etc. Moreover, given the current trend in the community to conduct large-scale experiments, having results from only eight environments seems somewhat limited."
                },
                "questions": {
                    "value": "See weaknesses above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission615/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698699784699,
            "cdate": 1698699784699,
            "tmdate": 1699635989052,
            "mdate": 1699635989052,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NFBQWcY59Y",
                "forum": "RN7RzMxwjC",
                "replyto": "PJi16YbDBU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission615/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission615/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 72ZS (Part 1)"
                    },
                    "comment": {
                        "value": "Many thanks to Reviewer 72ZS for providing a thorough review and valuable comments.\n\n**Q1**: Clarifications and comparison with DreamerV3\n\n\nReviewer 72ZS commented that DreamerV3 has already *addressed the issue of differing scales in reconstructing inputs and predicting rewards using Symlog Predictions*. We need to clarify that **using Symlog Predictions does not solve our problem of seeking a balance between reward modeling and observation modeling**. First of all, the Symlog transformation only shrinks extremely large values but is unable to rescale various values into exactly the same magnitude, while our harmonious loss properly addresses this by dynamically approximating the reciprocals of the values. More importantly, the primary reason why $L_r$ has a significantly smaller loss scale is the difference in dimension: as we have stated in our main paper, the observation loss $L_o$ usually aggregates $H\\times W\\times  C$ dimensions, while the reward loss $L_r$ is derived from only a scalar (a detailed explanation of why $L_o$ is summed over dimensions can be found in $\\underline{\\text{response to Q3.4 of Reviewer QvDA}}$ ). In summary, **using Symlog Predictions as DreamerV3 only mitigates the problem of differing per-dimension scales** (typically across environment domains) by a static transformation, **while our method aims to dynamically balance the overall loss scales across tasks in world model learning, considering together per-dimension scales, dimensions, and training dynamics**.\n\nIn practice, DreamerV3 does not use Symlog predictions in reconstructing visual inputs due to unified scales of visual inputs across domains. Besides, DreamerV3 uses twohot symlog predictions for the reward predictor to replace the MSE loss in DreamerV2. This approach increases the scale of the reward loss, but is insufficient to mitigate the domination of the observation loss. We observe that the reward loss in DreamerV3 is still significantly smaller than the observation loss, especially for visually demanding domains such as RLBench, where the reward loss is still two orders of magnitude smaller. \n\nOverall, **our contributions are orthogonal to the modifications in DreamerV3** and can be combined with DreamerV3 (which we brand as Harmony DreamerV3) to improve performance further. We have already presented the results of our Harmony DreamerV3 in $\\underline{\\text{Fig. 9 of main paper in the original submission}}$, and we provide additional results in $\\underline{\\text{Fig. 20 of Appendix D.7}}$ of our latest revision. We apologize for the limited discussion on DreamerV3, and we have added a discussion section in $\\underline{\\text{Appendix D.7 of our latest revision}}$.\n\n**Q2**: Comparison with TD-MPC\n\nWe want to highlight that we have already **presented aggregated results of TD-MPC on three Meta-world tasks** in $\\underline{\\text{Fig. 10(b) of main paper}}$. Moreover, we present **the individual learning curves of TD-MPC compared to our HarmonyWM and DreamerV2 on six Meta-world and DMCR tasks** in $\\underline{\\text{Fig. 18 of Appendix D.5 in the original submission}}$. These results show that TD-MPC is unable to learn a meaningful policy in some tasks, which further highlights the value of our explicit-implicit method."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission615/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700148519671,
                "cdate": 1700148519671,
                "tmdate": 1700148519671,
                "mdate": 1700148519671,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3YR3epV8zD",
                "forum": "RN7RzMxwjC",
                "replyto": "PJi16YbDBU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission615/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission615/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 72ZS (Part 2)"
                    },
                    "comment": {
                        "value": "**Q3**: Difficulties and scales of experimental environments\n\nWe **respectfully disagree with the comment that our experimental environments are all easy tasks**. According to the classification standard of Seo et al. [1], ***Sweep-Into*, *Hammer* and *Push* can all be regarded as challenging environments** in the Meta-world domain. The RLBench domain itself is considered more demanding than Meta-world due to its realistic visual observations. It's also worth noting that we use **DMC Remastered, which is different from and more challenging than the original DMC**, as it contains various random visual distractors and has proven difficult for model-based agents to learn on [6]. More details about the environments we use can be found in $\\underline{\\text{Appendix C.1}}$, and we apologize for not referring to these in our main paper.\n\nRegarding the challenging environments proposed by reviewer 72ZS, we have already reported our results on DMC Quadruped-run in $\\underline{\\text{Fig. 17 of Appendix D.5}}$, along with several other DMC environments. We apologize for not referring to them in our main paper. **The *Push* task reported in our main paper is of similar difficulty to *Assembly* and *Stick-pull*.** Nevertheless, we provide additional results on *Assembly* in $\\underline{\\text{Fig. 19 of Appendix D.6}}$ of our latest revision. \n\nRegarding the comment that our experiments lack variety and that we should perform large-scale experiments, we need to clarify that we perform **our experiments on fifteen instead of eight environments**: six from Meta-world, two from RLBench, three from DMC Remastered, and four from standard DMC (in Appendix). We add another 4 environments (Assembly in Metaworld and Qbert/Seaquest/Boxing in Atari) in our latest revision, which sums to a total of 19. Inevitably, the scale of our experiments is restricted due to our limited computational resources. However, this scale is very common in recent model-based RL publications, such as APV (9 envs) [2], DreamerPro (12 envs) [3], IsoDream (5 envs) [4], Curious Replay (5 envs) [5], IPV (12 envs) [6] and so forth.\n\n[1] Seo et al. Masked world models for visual control. CoRL, 2022.\n\n[2] Seo et al. Reinforcement learning with action-free pre-training from videos. ICML, 2022.\n\n[3] Deng et al. Dreamerpro: Reconstruction-free model-based reinforcement learning with prototypical representations. ICML, 2022.\n\n[4] Pan et al. Iso-Dream: Isolating and Leveraging Noncontrollable Visual Dynamics in World Models. NeurIPS, 2022.\n\n[5] Kauvar et al. Curious Replay for Model-based Adaptation. ICML, 2023.\n\n[6] Wu et al. Pre-training contextualized world models with in-the-wild videos for reinforcement learning. NeurIPS, 2023.\n\nPlease let us know whether our response addresses all your concerns. We are happy to discuss more if you have further questions or suggestions to improve our paper."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission615/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700148534756,
                "cdate": 1700148534756,
                "tmdate": 1700548470582,
                "mdate": 1700548470582,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Vu0hHQjG82",
                "forum": "RN7RzMxwjC",
                "replyto": "PJi16YbDBU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission615/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission615/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Request of Reviewer's attention and feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer 72ZS,\n\nThanks again for your dedication to reviewing our paper.\n\nWe write to kindly remind you that this is the last few days of the reviewer-author discussion period. We have made every effort to address the concerns you suggested and improve our paper:\n\n* We clarify that using Symlog Predictions in DreamerV3 does not address our problem and that the modifications of DreamerV3 are orthogonal to our technical contributions. Nevertheless, we provide additional comparison results against DreamerV3, where our method still proves effective.\n* We clarify that we have already presented the results of TD-MPC in our original submission, and results show that TD-MPC cannot learn a meaningful policy in some tasks, further highlighting the value of our explicit-implicit method.\n* We explain in detail the environments we use in our paper and that they are all challenging tasks. We also provide additional results on challenging environments proposed in your review. The results further demonstrate the superiority of our method on difficult tasks.\n* We clarify the number of environments we use and compare it to that of recent model-based RL publications to show that the scale of our experiments is sufficient.\n\nPlease kindly let us know if you have any remaining questions. If our responses have addressed your concerns, would you please consider re-evaluating our work based on the updated information? Looking forward to your reply.\n\nSincerely,\n\nAuthors"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission615/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700548722488,
                "cdate": 1700548722488,
                "tmdate": 1700548722488,
                "mdate": 1700548722488,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1g6nhVIUv8",
                "forum": "RN7RzMxwjC",
                "replyto": "PJi16YbDBU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission615/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission615/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Discussion period ends soon"
                    },
                    "comment": {
                        "value": "Dear Reviewer 72ZS,\n\nThank you once more for your invaluable review.\n\nIt is a kind reminder that **this is the last day of the 12-day Reviewer-author discussion period**. Taking your insightful suggestions seriously, we have made every effort to provide all the experiments and clarifications that we can.\n\nIf you have read our response, **we would greatly appreciate your acknowledgment and re-evaluation of our work**. We remain eagerly available for any further questions or discussions you may have. Anticipating your reply.\n\nBest regards,\n\nAuthors."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission615/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700670032998,
                "cdate": 1700670032998,
                "tmdate": 1700670032998,
                "mdate": 1700670032998,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9jBhocmGZA",
            "forum": "RN7RzMxwjC",
            "replyto": "RN7RzMxwjC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission615/Reviewer_QvDA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission615/Reviewer_QvDA"
            ],
            "content": {
                "summary": {
                    "value": "Typical MBRL methods optimize for two tasks - observation modeling (aka explicit MRL) and reward modeling (aka implicit MBRL). The paper hypothesizes that there is interference between the two tasks and proposed an alternate scheme for weighing the losses corresponding to the two tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is well written and flows nicely.\n\n2. The authors experiment with 4 set of environments and the results look reasonable"
                },
                "weaknesses": {
                    "value": "1. The paper calls out the following as it contribution: \"To the best of our knowledge, our work, for the first time, systematically identifies the multitask essence of world models and analyzes the disharmonious interference between different tasks, which is unexpectedly overlooked by most previous work\". While I agree that MBRL can be formulated could be multi-task RL problem (it is clearly multi-objective problem), the paper does not do a through job at analyzing \"the disharmonious interference between different tasks\". e.g. they do not study if and why there is an interference between different tasks. Note that in the multi-task literature, interference often refers to progress on one task, hindering the progress of another task. What the authors show is that adhoc setting of scalars for the different losses can hurt the performance on the RL task but they do not show that it hurts the performance on the tasks being directly optimized for. The distinction is important in the multi-task setup (which is what the paper uses).\n\n2. While the paper formulates the problem as a multi-task problem, they do not compare with any multi-task baseline that could balance between the different losses in equation 3. So while they show that adjusting the loss coefficients help, they do not show if their approach is better than other multi-task approaches."
                },
                "questions": {
                    "value": "Listing some questions (to make sure I better understand the paper) and potential areas of improvement. Looking forward to engaging with the authors on these questions and the points in the weakness section.\n\n1. In equation 1, the input to the representation model $q_{\\theta}$ should be $o_t$ right ?\n2. The paper seems to suggest that \"observation modeling\" task is a new task. Is that correct ?\n3. The word \"harmony\" in HWM - does it come from some mathematical properties or it refer to \"harmony\" (tuning) between the losses?\n4. Regarding \"the scale of L r is two orders of magnitude smaller than that of L o, which usually aggregates H \u00d7 W \u00d7 C dimensions\", it usually doesnt matter what the dimensions of output are as the loss is averaged over the dimensions.\n5. Are the findings 1, 2, 3 are new findings ?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission615/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698717119371,
            "cdate": 1698717119371,
            "tmdate": 1699635988981,
            "mdate": 1699635988981,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mK02PqBSKx",
                "forum": "RN7RzMxwjC",
                "replyto": "9jBhocmGZA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission615/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission615/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer QvDA"
                    },
                    "comment": {
                        "value": "Many thanks to Reviewer QvDA for providing a thorough review and valuable questions. \n\n**Q1**: Clarification on the multi-task essence of world models\n\nWe apologize for using the misleading word 'interference' to describe the problem in our original submission. In our work, we conduct a dedicated investigation ($\\underline{\\text{Sec 2.3 of main paper}}$) and demonstrate that observation modeling and reward modeling can benefit from each other, as shown in $\\underline{\\text{Fig 2 of main paper}}$ and our additional quantitative results in $\\underline{\\text{Q1 of the global response}}$. More importantly, we also reveal that domination of either task, as in many existing MBRL literature, does not properly exploit these multi-task benefits. Thus, the **main problem our work addresses can be more appropriately named task 'domination'** instead of 'interference,' and we propose a simple yet effective strategy to dynamically balance tasks in world models. We have modified our wording in the text of our $\\underline{\\text{latest revision}}$, and thank you for pointing out this.\n\nPlease also refer to $\\underline{\\text{Q1 of the global response}}$ for the detailed response.\n\n**Q2**: Comparison to multi-task baselines\n\nWe have compared different multi-task methods and discussed why these methods can hardly make more improvements in the problem of our paper. Please refer to $\\underline{\\text{Q2 of the global response}}$ for the detailed response.\n\n**Q3**: Clarifications and minor questions\n\n1. The input to the representation model $q_\\theta$ is the previous latent $z_{t-1}$, the action $a_{t-1}$, and current observation $o_t$, as formulated in our main paper. Detailed architecture of Dreamer has been illustrated in $\\underline{\\text{Fig 6 of main paper}}$, while Fig 1 only presents a simplied one. You may refer to 4 below or Dreamer's original papers [1,2] for additional information on this.\n2. Observation modeling is not a new task. As shown in $\\underline{\\text{Fig 1 of main paper}}$, while it is not necessary for MBRL, the observation modeling task has always been a dominating task in explicit MBRL methods facilitating representation learning. Our work emphasizes that previous literature has overlooked the multi-tasking essence of world model learning and that proper mitigation of task dominations can significantly improve MBRL.\n3. The word 'harmony' here refers to harmony between the losses (tasks), where none of the two tasks dominates in world model learning, and either task can benefit from the other's progress.\n4. The observation loss $L_o$ is computed as a sum over all dimensions, instead of an average. The world model in the Dreamer framework is formulated as sequential variational inference [1] and optimized by the standard variational bound on log likelihood (ELBO): $\n\\mathcal{L}(\\theta) =\\mathbb{E}\\_{q\\_{\\theta}\\left(z_{1:T}|a_{1:T}, o_{1:T}\\right)} \\Big[ \\sum\\_{t=1}^{T} \\Big(\n    {-\\log p\\_{\\theta}(o\\_{t}|z\\_{t}) -\\log p\\_{\\theta}(r\\_{t}|z\\_{t}) } {+\\text{KL}\\left[q\\_{\\theta}(z\\_{t}|z\\_{t-1},a\\_{t-1}, o\\_{t}) \\Vert p\\_{\\theta}(\\hat{z}\\_{t}|z\\_{t-1}, a\\_{t-1}) \\right]}\n    \\Big)\\Big],$ where the observation loss $L_o=-\\log p_{\\theta}(o_{t}|z_{t})=-\\sum_{i=1}^{h}\\sum_{j=1}^w \\log p_{\\theta}(o_{t}^{(i,j)}|z_{t})$ is natually in the form of a sum over pixels $o_{t}^{(i,j)}$. Averaging over dimensions is equivalent to assigning a very small weight to the observation loss and leads to KL term domination and inferior results.\n5. To the best of our knowledge, findings 1, 2, 3 are new for world model learning. We have already discussed the differences between our findings and previous relevant literature in the $\\underline{\\text{last paragraph of Sec 2 in the original submission}}$. \n\n[1] Hafner et al. Learning latent dynamics for planning from pixels. ICML 2019.\n\n[2] Hafner et al. Dream to control: Learning behaviors by latent imagination. ICLR 2020.\n\nPlease let us know whether our response addresses all your concerns. We are happy to discuss more if you have further questions or suggestions to improve our paper."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission615/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700148415862,
                "cdate": 1700148415862,
                "tmdate": 1700148947336,
                "mdate": 1700148947336,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BR7lOp5nJq",
                "forum": "RN7RzMxwjC",
                "replyto": "9jBhocmGZA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission615/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission615/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Request of Reviewer's attention and feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer QvDA,\n\nThanks again for your dedication to reviewing our paper.\n\nWe write to kindly remind you that this is the last few days of the reviewer-author discussion period. We have made every effort to address the concerns you suggested and improve our paper:\n\n* We clarify our multi-task perspective into world models and stress that a major contribution of our paper is to reveal the overlooked potential of sample efficiency in MBRL through a unified multi-task perspective into world model learning.\n* We clarify our misleading word 'interference' and have modified our wording to 'domination' in our latest revision.\n* We have made comprehensive comparisons to multi-task baselines and discussed why these methods can hardly make more improvements in the problem of our paper.\n* We replied to the questions you mentioned in the Questions part.\n\nPlease kindly let us know if you have any remaining questions. If our responses have addressed your concerns, would you please consider re-evaluating our work based on the updated information? Looking forward to your reply.\n\nSincerely,\n\nAuthors"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission615/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700548558320,
                "cdate": 1700548558320,
                "tmdate": 1700548558320,
                "mdate": 1700548558320,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aXt21EewUa",
                "forum": "RN7RzMxwjC",
                "replyto": "9jBhocmGZA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission615/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission615/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Discussion period ends soon"
                    },
                    "comment": {
                        "value": "Dear Reviewer QvDA,\n\nThank you once more for your invaluable review.\n\nIt is a kind reminder that **this is the last day of the 12-day Reviewer-author discussion period**. Taking your insightful suggestions seriously, we have made every effort to provide all the experiments and clarifications that we can.\n\nIf you have read our response, **we would greatly appreciate your acknowledgment and re-evaluation of our work**. We remain eagerly available for any further questions or discussions you may have. Anticipating your reply.\n\nBest regards,\n\nAuthors."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission615/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700670002363,
                "cdate": 1700670002363,
                "tmdate": 1700670002363,
                "mdate": 1700670002363,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]