[
    {
        "title": "UniAudio: An  Audio Foundation Model Toward Universal Audio Generation"
    },
    {
        "review": {
            "id": "Ff1TtcnAsI",
            "forum": "nhgTmx1TZJ",
            "replyto": "nhgTmx1TZJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8597/Reviewer_gngw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8597/Reviewer_gngw"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors train a single multi-scale transformer to perform multiple different audio tasks. They demonstrate that, by learning these tasks together, they get a performance boost for this model, and achieve state of the art results on multiple tasks (against specialist models). They show that the existing model can be fine tuned to perform well on novel tasks. In addition, they describe how to translate multiple input modalities into a universal data space with a neural codec to allow this to happen, and discuss the merits and detriments of this system against other approaches."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "* All data used is public, so results can be reproduced\n* Compute required is not out of the realms of most academic institutions (16 x MI200-64G GPUs ~ $64k) - particularly if, as is suggested, this model is taken and simply fine tuned upon\n* I am not aware of another model which has covered all of these tasks nor comprehensively demonstrated that combining different input modalities benefits each task; the authors show the benefit of training on multiple modalities by training their model on just the one task, and then again on all task, and by comparison with the current sota (generally single-task) model\n* Care and attention has been taken to compare with recent sota models in each domain, and to perform both objective and subjective evaluations. In addition, an anonymised website of audio generations was provided"
                },
                "weaknesses": {
                    "value": "I would like to emphasise that all the following are suggestions for discussion / future work - IMHO this contribution is more than sufficient for publication in its current form.\n* Reliant on the input representation - any losses incurred by input representation cannot be modelled, and if a new representation were to be used, a new model must be trained\n* As noted in the limitation section, there's no demonstration of fine tuning to an unknown modality (which would require a new special modality token to be included in the vocabulary)\n* The evaluation is so wide ranging it's difficult to parse in tables. Perhaps a bar chart or plot could improve the interpretation of relative performance with sota benchmarks?\n* Only amazon turk was used for evaluation - a broader human evaluation with subject experts would be very interesting"
                },
                "questions": {
                    "value": "* It is not explained exactly why the 4 tasks were selected for the fine tuning study, was there any reason?\n* Is there any reason that the process could not be end to end (i.e. must the neural codec be learned prior and fixed)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "10: strong accept, should be highlighted at the conference"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8597/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698685958416,
            "cdate": 1698685958416,
            "tmdate": 1699637075241,
            "mdate": 1699637075241,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OWfsK7hPlN",
                "forum": "nhgTmx1TZJ",
                "replyto": "Ff1TtcnAsI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8597/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8597/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer gngw"
                    },
                    "comment": {
                        "value": "We thank the reviewer for recognizing our contributions. We do appreciate the constructive comments the reviewer provided to us to\nfurther improve our paper. We are delighted to have the following discussion with the reviewer.  \n\n**Q1:**  Reliant on the input representation - any losses incurred by input representation cannot be modelled, and if a new representation\nwere to be used, a new model must be trained. \n\n**A:**  We are happy to have this discussion.  \n\n(1) Theoretically, any information that is not retained by the input models (T5, mHuBERT, Codec) will not be modeled by UniAudio.\nHowever, we attempt to convince the reviewer that, the input models adopted in our experiments are exhaustively validated in downstream tasks (thanks to the community efforts) and should be able to reserve most of the necessary information. \n\n(2) UniAudio is exactly built upon these input models and needs to be trained again if the input models are changed. However, all input\nmodels adopted in this work are publicly available and our training code is also released to help our users. The reproducibility of our work is secured.\n\n**Q2:** As noted in the limitation section, there\u2019s no demonstration of fine-tuning to an unknown modality (which would require a new\nspecial modality token to be included in the vocabulary). \n\n**A:** We thank the reviewer for reading our paper carefully. We agree with the reviewer that introducing new modalities is interesting and\nplan to address this topic in future work.   \n\n(1) Currently, UniAudio supports multiple modalities (phone, text, codec, MIDI, semantic), which is probably sufficient for most users to\ndefine their new audio generation tasks.  \n\n(2) By extending the vocabulary and adding the data from new modalities, UniAudio can possibly support more modalities.  \n\n**Q3:** The evaluation is so wide ranging it\u2019s difficult to parse in tables. Perhaps a bar chart or plot could improve the interpretation of\nrelative performance with sota benchmarks?  \n\n**Revision:** We appreciate this comment. Following this instruction, we revised our tables in the main content and added multiple bar\ncharts in the appendix to present our experimental results.  \n\n**Q4:** Only amazon turk was used for evaluation - a broader human evaluation with subject experts would be very interesting\n\n**A:** Thanks for this suggestion. We agree with the reviewer that human evaluation of AI-generated content is a popular open problem.\nWe are willing to include more professional raters in our evaluation procedure and share more insights into how the generated content can better human benefits.\n\n**Q5:** It is not explained exactly why the 4 tasks were selected for the fine-tuning study, was there any reason?  \n\n**A:** We thank the reviewer for this question on our experimental setup.  \n\n(1) We clarify that we split 7 tasks for first-stage training and 4 tasks for fine-tuning to verify that the proposed UniAudio can act as a\nfoundation model and continuously support new audio generation tasks. Instead, all 11 tasks can be possibly trained together from scratch.  \n\n(2) When splitting the tasks, we intend to ensure all modalities have been learned during the first-stage training over the 7 tasks.  \n\n(3) The 4 tasks for fine-tuning are comparatively small in data volume, which is well aligned with the situation that our users intend to\nbuild a new audio generation task but do not have large-scale data in the beginning.  \n\n**Q6:** Is there any reason that the process could not be end to end (i.e. must the neural codec be learned prior and fixed)?\n\n**A:** We agree with the reviewer that building a universal audio generation model in a more end-to-end paradigm is promising and beneficial.  We are willing to work in this direction. However, the following problems could be challenging.  \n\n(1) **Input:** Some necessary prior knowledge encoded in the input models can hardly be learned by training an audio generation model.\nE.g., the textual semantics provided by the T5 model.  \n\n(2) **Output:** We consider our method as LM-based, which is featured by discrete unit prediction. Getting rid of audio tokenization (e.g.,\naudio codec) can possibly change the predicting target to continuous representations and is not aligned with the LM-like paradigm. Also,\npredicting the continuous representations can possibly encounter a larger dynamic range than predicting the discrete units, which may\npose additional technical challenges.  \n(3) **Speed:** If we directly generate waveform like the previous work by Wavenet (van den et al, 2016), the sequence would be very long, the prediction space would be very large, and the inference speed would be extremely slow.\n\nReference: van den et al, WaveNet: A Generative Model for Raw Audio, in InterSpeech 2016"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8597/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700730429948,
                "cdate": 1700730429948,
                "tmdate": 1700730429948,
                "mdate": 1700730429948,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "R8nZGZXl50",
            "forum": "nhgTmx1TZJ",
            "replyto": "nhgTmx1TZJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8597/Reviewer_EAVx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8597/Reviewer_EAVx"
            ],
            "content": {
                "summary": {
                    "value": "- The authors present a foundation model for audio generation and editing which encompasses 11 tasks spanning multiple modalities of audio such as speech, music, and general sounds. \n- The paper proposes a different approach to modeling audio tokenized into discrete codes using audio codecs. The proposed architecture is a single multi-scale transformer which involves a global transformer which operates on a summarized form of each audio frame, and a local transformer which performs autoregressive generation of codes within an audio frame. The design is proposed as a way to alleviate training transformers on very long sequences of flattened audio codes (SPEAR-TTS), or having train an autoregressive transformer on one level of codes and another non-autoregressive transformer on the remaining level of audio codes (VALL-E).\n- The training process involves 7 different tasks with various task indicator tokens being used. The remaining tasks are incorporated in a fine-tuning phase post training of the base model.\n- The authors perform evaluation using subjective and objective metrics and compare against various other models which may be specialized for each task. The results indicate that UniAudio is competitive against all the baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- As the authors show in the first table, this system is the first to tackle so many tasks in audio generation using a single backbone with only additional fine-tuning.\n- The proposed model architecture is different from those used in other audio generation models and shows some gains. The comparison is not necessarily fair because of differences in training data, but it does give the community a new option to consider while building audio generation models.\n- Results for speech data and denoising are decent."
                },
                "weaknesses": {
                    "value": "- The paper is a very dense read with some details relegated to the appendix while some details difficult to glean from the text. For example, the details about the audio tokenizer is moved to the appendix and no specifics are mentioned in the main text. Also, the text and the figure in the model architecture section are a little difficult to understand. I think I got it after reading through a few times, but adding some annotations to the figure and its caption will greatly improve readability.\n- While the results for speech are decent, I found some issues w.r.t words being skipped. Also, the results on text to music/audio are pretty poor. The authors have not discussed any of the limitations in terms of quality of the generated audio despite the model being competitive with current state-of-the-art. \n- The paper does not offer too much in terms of insights. Not to take away from the effort it takes to setup such a large scale experiment, but my main takeaways from the paper are as follows:\n  1. Existing audio codecs are not suitable for wide range of tasks, so one needs to train them on more diverse data.\n  2. As long as we collect 100k+ hours of audio data we can achieve such performance using the multi-scale transformer. \n\n  The multi-scale transformer architecture is definitely interesting to the community but the architecture contribution is not well ablated, and in my opinion, that is the most important part of the paper.\n- Minor correction: In Table 3, subjective and objective metrics column headers should be interchanged."
                },
                "questions": {
                    "value": "- Wonder if the authors can add any ablations that answer questions related to the effect of data quantity?\n- Did the authors consider using DAC as the audio codec? Those models are trained on a more diverse dataset and also have other improvements over EnCodec."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8597/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698863140905,
            "cdate": 1698863140905,
            "tmdate": 1699637075135,
            "mdate": 1699637075135,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "caGfVjQG07",
                "forum": "nhgTmx1TZJ",
                "replyto": "R8nZGZXl50",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8597/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8597/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer EAVx"
                    },
                    "comment": {
                        "value": "We thank the reviewer for recognizing our contributions. We do appreciate the constructive comments the reviewer provided to us to\nfurther improve our paper. We are delighted to have the following discussion with the reviewer.  \n\n**Q1:** The paper is a very dense read with some details relegated to the appendix while some details difficult to glean from the text. For\nexample, the details about the audio tokenizer is moved to the appendix and no specifics are mentioned in the main text. Also, the text\nand the figure in the model architecture section are a little difficult to understand. I think I got it after reading through a few times, but\nadding some annotations to the figure and its caption will greatly improve readability.  \n\n**Revision:**  We do appreciate the reviewer\u2019s patience and apologize for the confusion caused by our interpretation. We do the following\nrevisions to improve the readability:  \n\n(1) We checked each appendix of the paper and made sure all appendices were properly mentioned in the main context, including our\nbuilding process of the codec model.  \n\n(2) We revised the diagram in the model architecture Section.  \n\n(3) We changed some tables in the main content into diagrams for ease of understanding.  \n\n(4) We double-checked all the text and annotations in the figures.  \n\n**Q2 :** While the results for speech are decent, I found some issues w.r.t words being skipped. Also, the results on text to music/audio\nare pretty poor. The authors have not discussed any of the limitations in terms of quality of the generated audio despite the model being\ncompetitive with current state-of-the-art.  \n\n**A:** We do appreciate these constructive comments. Please allow us to clarify as follows:  \n\n(1) The samples in our demo pages are randomly selected and are reasonable to contain errors, including errors like skipping some words. We share that, we do not intend to exclude these errors to faithfully reflect our system\u2019s performance. We acknowledge that UniAudio may generate some sub-optimal samples. We attempt to convince the reviewer that, the quantitative evaluation results of UniAudio could effectively reflect UniAudio\u2019s performance.  \n\n(2) We agree with the reviewer that our results on the text-to-music task do not outperform the strong prior work MusicGen. However, we encourage the reviewer to check that:  \n\n(i) In Appendix B.4 which present our detailed results on text-to-music task, UniAudio outperforms many prior works except MusicGen\nand Noise2Music.  \n\n(ii) Both MusicGen (20khrs) and Noise2Music (280khrs) adopted much more labeled data than our UniAudio (8khrs). Their data is private.\nThe Million Song Dataset adopted in this work is the largest text-music dataset we can publicly access.  \n\n(ii) Even being inferior to some prior works, we attempt to convince the reviewer that our results on the text-to-music task are self-consistent  to prove the benefit of building a unified audio generation model.  \n\n(3) **Our Revision:**  We do appreciate the reviewer\u2019s comments that the quality risk of our model should be clearly stated. In the\nlimitation section, we state *The samples generated by UniAudio are not guaranteed in quality and may contain errors.*"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8597/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700729792400,
                "cdate": 1700729792400,
                "tmdate": 1700729792400,
                "mdate": 1700729792400,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ih66iSoj8e",
            "forum": "nhgTmx1TZJ",
            "replyto": "nhgTmx1TZJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8597/Reviewer_kfiC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8597/Reviewer_kfiC"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a LM-like audio model, that is able to conduct multiple (11) audio generation tasks. Audio is tokenized with a EnCodec-like quantizer to fit for LM. Multi-scale Transformer is used for handling multiple tokens per audio frames. Experiment was conducted by training on 165K hours audio data."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The overall approach is sound. It's great to see a single model can handle so many different audio generation tasks."
                },
                "weaknesses": {
                    "value": "1. The novelty of this paper is very limited. All the components used in the paper are previously existing. The main contribution of this paper is to train a multi-task model with a mixture of multiple datasets. \n2. The overall contribution to the research community seems very limited. There is no much insights can be drawn from this paper to benefit the community. There is no ablation study conducted. It looks more of an engineering work than a research work.\n3. The presentation of this paper needs improvement. It seems presented in an eye-catching, but misleading way. For example, it's improper to put Table 1 as the first thing in the content of this paper, because it lacks context and is confusing. Even worse, it's not a fair comparison -- the cited papers didn't include experiments on specific tasks doesn't mean that the methods presented in those papers are incapable of those tasks."
                },
                "questions": {
                    "value": "None."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Discrimination / bias / fairness concerns"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "See the 3rd item in \"weaknesses\"."
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8597/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699226223299,
            "cdate": 1699226223299,
            "tmdate": 1699637075030,
            "mdate": 1699637075030,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gFSyJUTXNA",
                "forum": "nhgTmx1TZJ",
                "replyto": "ih66iSoj8e",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8597/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8597/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer kfiC"
                    },
                    "comment": {
                        "value": "We thank the reviewer for recognizing our contributions. We do appreciate the constructive comments the reviewer provided to us to\nfurther improve our paper. We are delighted to have the following discussion with the reviewer.  \n\n**Q1:** The novelty of this paper is very limited. All the components used in the paper are previously existing. The main contribution of\nthis paper is to train a multi-task model with a mixture of multiple datasets.  \n\n**A:**  We thank this comment and are delighted to have the following discussion with the reviewer in terms of our contributions:  \n\n(1) **Contribution on independent tasks:** Although rapid advances in LM-based audio generation, this technique is not widely adopted\nin some tasks. We attempt to convince the reviewer that applying the LM-based audio generation technique to tasks like instructed text\u0002to-speech and speed editing is novel.  \n\n(2) **Contribution on unified tasks definition:** We attempt to reach a consensus with the reviewer that the proposed unified task\nformulation is of sufficient contribution. Considering the distinctions among all these audio generation tasks, such as the model backbones, training objective, generative paradigm, and task-specific domain knowledge, proposing a unified formulation for all 11 tasks is of sufficient contribution.  \n\n(3) **Contribution on architecture:** We hope the reviewer can acknowledge our contribution to model architecture design. The proposed\nmulti-scale transformer architecture is specifically designed for long audio token sequences with neural codec. The proposed architecture fully preserves the inter- and intra-frame auto-regressive property. The performance and efficiency of the proposed architecture are vali\u0002dated by the experiments and ablation study (see section 3.4.2).  \n\n(4) **Contribution on experiments:** We also encourage the reviewer to check our refined experimental section: we experimentally prove\nthat building a unified audio generation model is feasible and even beneficial to all of the tasks included. We further validate that the proposed UniAudio model can easily be generalized to unseen audio generation tasks with simple fine-tuning. These experimental results support our claim that UniAudio can be a foundation model that supports emergent audio generation needs in future research.  \n\n**Q2:** The overall contribution to the research community seems very limited. There is no much insights can be drawn from this paper to benefit the community. There is no ablation study conducted. It looks more of an engineering work than a research work.  \n\n**A:** We thank this comment and deeply apologize for the confusion caused by our interpretation. Please allow us to clarify as follows:  \n\n(1) **Contributions and Insights:** First, we encourage the reviewer to check the contributions we summarized in Q1. Secondly, we\nwould like to share and discuss the insights as follows: (i) Compared with building independent models for each task and each domain,\nbuilding a universal audio generation model can effectively save human efforts and resource input. (ii) The universal audio generation\nmodels can potentially serve as foundation models that can generalized to unseen audio generation tasks. (iii) The goal of this research,\nachieving universal generation, is aligned with the observations of the pioneer works (Wang et al, 2022, Sanh et al, 2022) in domains other than audio. These pioneer works demonstrate that multi-task learning leads to a more advanced understanding of the target domain and improves the generalization towards unseen tasks.\n\n(2) **Abalation Study:** We apologize that we did not make our ablation studies clearly presented in the original manuscript. We would\nlike to refer the reviewer to section 3.4 for our ablation studies and corresponding analysis. In these studies, we validate that (i) building a\nunified audio generation model is beneficial to all audio generation tasks included and (2) the proposed multi-scale transformer architecture fully preserves the autoregressive property while maintaining efficiency.\n\n(3) **Research or Engineer Work:** We attempt to reach a consensus with the reviewer that our work has contributions to both research\nand engineering. Our research contributions have been summarized above. We hope the reviewer will agree that these items are of sufficient research value. We also appreciate the reviewer for recognizing our efforts in engineering. Also, We have released our code to share our engineering efforts with the community."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8597/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700729185442,
                "cdate": 1700729185442,
                "tmdate": 1700729185442,
                "mdate": 1700729185442,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FemtiwPq0R",
            "forum": "nhgTmx1TZJ",
            "replyto": "nhgTmx1TZJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8597/Reviewer_dsre"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8597/Reviewer_dsre"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a foundation model for audio generation capable of handling a number of different tasks, all of which output audio (including speech, environmental sounds, and music) conditioned on input from multiple modalities including text, audio, and MIDI.  \n\nThe paper also introduces a two-level Transformer architecture where the top-level attention operates across audio *frames* (where each frame consists of multiple discrete tokens) and the bottom-level attention operates on the tokens within a frame."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "I am very much in favor of the goal of the paper: to introduce a multitask foundation model for audio generation.  I am also totally on board with the proposed architecture; decomposing the quadratic attention into frame-level and token-level is an excellent idea for handling the longer sequence lengths that are necessary for freeform audio generation."
                },
                "weaknesses": {
                    "value": "Two main areas of weakness:\n\n1) Most of the evaluation tasks do not seem ideal for demonstrating the effectiveness of an architecture designed to enable long sequence lengths; other than text-to-music, the tasks are all quite local, with no real dependencies longer than a few seconds.  Notably, text-to-music is the task on which the model in the paper performs worst compared to baseline.  And the experimental evaluation of the architecture vs others consumes only a small section of the paper and is fairly inconclusive.\n\nEssentially, the architecture seems overkill for most of these tasks, and for the task for which one might expect it to help most (text-to-music), possibly the smaller training data size prevents the model from taking advantage of its capability.\n\n2) The paper does not convincingly demonstrate that training a single model on speech-, music-, and sound-generating tasks exhibits synergy across domains.  While for most of the evaluation tasks the multitask model outperforms the single task model, a) the difference in performance is small and b) it's not clear that the model really benefits from training on all of speech, music, and sound compared to a separate speech model, music model, and sound model as this experiment was not performed.  It's also not clear how much of the benefit depends on specifics of the datasets and tasks used here vs a more general principle; e.g. the overall amount of speech data here is much larger than the other domains and so a music task might benefit from training jointly with speech more than would be the case if the amount of music data were larger.\n\nOverall, I'm just not satisfied that the main hypothesis of the paper -- training a model on all audio generation domains at once is better than training separate models for each domain -- is sufficiently backed up with experimental evidence.  The fact that the joint model outperforms state of the art on about half the tasks (and *not* the music tasks) corroborates my dissatisfaction here.\n\nOne other possible reason for training on multiple domains could be: only a small quantity of training data exists for certain domains.  However, for the case of foundation models this is certainly not true; it is usually *labels* that are in short supply, and raw audio -- speech, music, and sound -- exists in enormous quantity."
                },
                "questions": {
                    "value": "1) Basically, convince me that training on speech helps with music, even if one has access to enormous unlabeled music datasets.  I'm totally willing to believe that training on all the speech tasks at once is helpful.\n\n2) I do believe that the architecture proposed is going to outperform the comparison architectures for audio generation.  But the experiment demonstrating this isn't especially compelling; did you perform other experiments on the different architectures?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8597/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699231815743,
            "cdate": 1699231815743,
            "tmdate": 1699637074927,
            "mdate": 1699637074927,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "MKEQyYGhYl",
                "forum": "nhgTmx1TZJ",
                "replyto": "FemtiwPq0R",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8597/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8597/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Review dsre"
                    },
                    "comment": {
                        "value": "We thank the reviewer for recognizing our contributions. We do appreciate the constructive comments the reviewer provided to us to\nfurther improve our paper. We are delighted to have the following discussion with the reviewer.  \n**Q1:** Discussion on the potential adoption of unlabeled data and domain-specific models.  \n\n**A:** We believe the reviewer is most interested in this direction so we reply to it first. We do agree with the reviewer the paper should\nconsider the unlabeled data and domain-specific models. We summarize and respond to the reviewer\u2019s comments in the following aspects.  \n\n(1) **Unlabeled Data:** We appreciate and agree with the reviewer that the adoption of unlabeled data (e.g., unlabeled music) can lead to\nperformance improvement. We then did experiments in this direction and our primary experiments support the reviewer\u2019s comment (see\nTable 1 below). We do appreciate this inspiration. \n \n(2) **Unified vs. Domain-Specific:** We also agree with the reviewer that we haven\u2019t presented comparative results between the proposed unified model and the domain-specific models (e.g., music-specific models). We attempt to convince the reviewer that foundation models of these two kinds have different features, serve different application scenarios, and thus should not necessarily be compared. We agree that, compared with the unified model, the domain-specific models may have more expertise in that domain and lead to better performance on domain-specific tasks. However, building domain-specific models for each domain also requires more effort and resource input. By contrast, the unified model is expected to obtain prior knowledge in multiple domains and can be more flexible in handling newly defined audio generation tasks. Thus, we believe both the universal and domain-specific foundation models are worth exploring.  \n\n(3) **Our Revisions:** We thank the reviewer for this inspiration. To avoid potential over-claim or ambiguity, in the limitation section,\nwe clearly state that *Current UniAudio considers neither unlabeled data nor domain-specific foundation models, which can possibly further improve the overall performance*; and in the experimental setup section, we state *UniAudio is built on labeled datasets*.\n\nTable 1: The ablation study to explore the influence of adding unlabeled music data on text-to-music tasks. With\njoint training paradigm: (1) text-to-music is trained with the 8khrs labeled data (Million Song dataset); (2) Unlabeled\nmusic prediction is trained as a mask-and-predict task and is based on the 8k hours of unlabeled music data (Luca\net.al). The adoption of unlabeled music data does achieve performance improvement on text-to-music task.\n\n| Setting                                    | FAD ($\\downarrow$) | KL ($\\downarrow$) | OVL. ($\\uparrow$) | REL. ($\\uparrow$) |\n|--------------------------------------------|--------------------|-------------------|-------------------|-------------------|\n| Text-to-Music only                         | 5.24               | 1.80              | 64.4$\\pm$2.1      | 66.2$\\pm$2.4      |\n| Text-to-Music + Unlabeled music prediction | 3.95               | 1.85              | 65.4$\\pm$1.6      | 68.8$\\pm$1.5      |\n\n\n**Q2:** Most of the evaluation tasks do not seem ideal for demonstrating the effectiveness of an architecture designed to enable long sequence lengths.  \n\n**A:** We thank the reviewer for reminding us to clarify UniAudio\u2019s capability to generate long audio. We are delighted to discuss this with\nthe reviewer as below.  \n\n(1) We are happy to share that the proposed UniAudio can actually support to generate long audio. In our experimental setup, the\nglobal transformer supports 3,000 audio frames at maximum; the training target is truncated to 20 seconds, with the possibility of further\nincrease.  \n\n(2) We also agree with the reviewer that long-form audio generation is a promising direction to explore. However, supporting long-form\ngeneration is less claimed as a key strength of UniAudio. We would like to clarify that, although we propose the multi-scale transformer\narchitecture to handle the overly-long sequences in audio generation, this issue is more raised from the adoption of the neural codec model and residual vector quantization (Zeghidour et al., 2021) than the original length of the audio.  \n\n(3) **Our Revision:** We encourage the reviewer to check our demo page with more long-form examples. The newly updated examples\nhave lengths of around 20 seconds, which is aligned with the common needs of audio generation."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8597/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700728120296,
                "cdate": 1700728120296,
                "tmdate": 1700728120296,
                "mdate": 1700728120296,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]