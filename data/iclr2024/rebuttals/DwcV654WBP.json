[
    {
        "title": "TVTSv2: Learning Out-of-the-box Spatiotemporal Visual Representations at Scale"
    },
    {
        "review": {
            "id": "577ajxueBu",
            "forum": "DwcV654WBP",
            "replyto": "DwcV654WBP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9381/Reviewer_A6Wr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9381/Reviewer_A6Wr"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a video foundation model called TVTSv2 for learning out-of-the-box spatiotemporal visual representations.\nIt aims to solve the issue of performance degradation compared to image foundation models when adapting them to video.\nThe degradation is attributed to distortion in language supervision from end-to-end tuning of the text encoder on noisy ASR transcripts.\nA partially frozen text encoder is proposed, freezing shallow layers while tuning deep layers, to retain generalization and learn new semantics. The authors state that SOTA results were achieved on zero-shot action recognition and text-to-video retrieval: TVTSv2 surpasses recent methods like CLIP-ViP, ImageBind, and InternVideo on several metrics. Ablations showed partially frozen text training avoids degradation and enables knowledge transfer. Masking was shown to improve efficiency for large models without sacrificing too much performance. Fine-tuning performance was also competitive, suggesting the approach does not hurt downstream training."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The strengths of the paper include:\n- Produces strong out-of-the-box spatiotemporal representations for zero-shot usage and surpasses recent state-of-the-art methods substantially in zero-shot action recognition, including models trained on more data.\n- Achieves new state-of-the-art results on multiple video understanding benchmarks.\n- Provides an effective strategy for pre-training on large, noisy video transcript datasets as it retains performance on downstream fine-tuning, unlike some other self-supervised methods.\n- The approach facilitates scaling up to large models by incorporating masking techniques. It avoids catastrophic forgetting of language knowledge via partially frozen training\nIn conclusion, the paper strongly suggests potential for pre-trained models to support out-of-the-box video applications.\nThe claims seem reasonably well supported by the results, as the proposed TVTSv2 models clearly surpass prior state-of-the-art in zero-shot action recognition and retrieval across multiple datasets. The ablation studies also provide evidence for the benefits of the partial freezing strategy and incorporation of masking techniques."
                },
                "weaknesses": {
                    "value": "Limitations:\nThe paper is a bit disorganized. THe architecture is followed by the empirical study followed by further description of the model (trainig objectives etc.). It would read better if the approach description was in one place. If the empirical degradation study calls for rmodification of the training objective, that should be spelled out more explicitly. \nThe attention masks in hte figures are used as an argument for good performance. However, it is not clear what woud be the ground truth atteniton mask. COuld it be that in the video clips the objeects/actions of interest were the only moving parts in the scene causing the attention grab?\nThe joint attention module is not described clearly. Is it the same as in CLIP?\nFine-tuning performance was not extensively benchmarked on more diverse downstream tasks, so the claims about out-of-the-box task-agnostic approach could be substantiated better.\nThe largest model studied is still limited compared to huge image models, so scalability past 1 billion parameters is unvalidated.\nPotential societal impacts of large foundation models were not addressed."
                },
                "questions": {
                    "value": "Was any experimentation done with different freeze ratios or objectives for the text encoder?\nHow much performance gain was directly attributable to the partially frozen text training versus other modifications?\nThe narrative around the attentional maps could be stronger.\nWhat can the authors say about model biases? Are actions of all sexes/races recognized at comparable accuracy?\nMore extensive evaluation of fine-tuning performance on diverse downstream tasks would be desirable for supporting the \"out-of-the-box\" claim."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9381/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698699648569,
            "cdate": 1698699648569,
            "tmdate": 1699637183599,
            "mdate": 1699637183599,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pS35suq11S",
                "forum": "DwcV654WBP",
                "replyto": "577ajxueBu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9381/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9381/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer A6Wr"
                    },
                    "comment": {
                        "value": "**W1: The paper is a bit disorganized.**\n\n**W1.1: It would read better if the approach description was in one place.**\n\n**A:** Thanks for the advice. We have reorganized the paragraphs and put the approach description together in the revised version.\n\n**W1.2: If the empirical degradation study calls for modification of the training objective, that should be spelled out more explicitly.**\n\n**A:** The empirical degradation study is disentangled from the training objective, and it corresponds to the partially frozen pre-training strategy. Sorry for the confusion, we have reorganized the paragraphs to make readers easier to follow.\n\n**W2: It is not clear what the ground truth attention mask is. Could it be that in the video clips the objects/actions of interest were the only moving parts in the scene causing the attention grab?**\n\n**A:** That's an insightful query. Indeed, selectively masking objects or actions that are semantically linked could potentially enhance model performance. However, creating accurate ground truth masks for this purpose is a labor-intensive task, especially considering the scale of pre-training that involves millions of videos. Therefore, we've opted for a more feasible approach: employing a random masking strategy that does not depend on ground truth data. Despite its simplicity, this method proves quite effective due to the vast amount of training data we use. Our models demonstrate excellent out-of-the-box performance, highlighting the capability of this straightforward technique to learn robust representations (Appendix D Table 18).\n\n**W3: The joint attention module is not described clearly. Is it the same as in CLIP?**\n\n**A:** No, we adopt the divided space-time attention instead of the joint attention. An illustration of the detailed architecture is provided in Appendix C Figure 3. For the intra-frame tokens, i.e., spatial-related tokens, we add the same spatial positional embeddings, and for tokens at the same position across different frames, i.e., temporal-related tokens, we add the same temporal positional tokens. For each token, it first attends to the temporal-related tokens, then attends to the spatial-related tokens. Note that the [CLS] token is attended in both temporal and spatial self-attention.\n\nIn addition, we also compared the performance between CLIP-style joint attention and the divided space-time attention. Please refer to Appendix D Table 19 for details.\n\n**W4&Q4: More extensive evaluation of fine-tuning performance on diverse downstream tasks would be desirable for supporting the \"out-of-the-box\" claim.**\n\n**A:** Thank you for your insightful feedback. In our paper, the phrase \"out-of-the-box\" refers to creating versatile video representations that can be immediately applied to various downstream tasks without the need for further fine-tuning. Our goal is to offer such foundational models to the community, potentially accelerating its progress by eliminating the need for costly pre-training and fine-tuning.\n\nWe value your advice and recognize that a strong out-of-the-box representation should yield good fine-tuning performance. To this end, we conducted basic fine-tuning of our pre-trained models on DiDeMo and LSMDC datasets, deliberately avoiding complex techniques like optimal parameter searching. The findings, presented in Appendix D Table 15, indicate that our straightforward approach does enhance fine-tuning performance to a certain extent. Furthermore, we have included linear classification results where a classifier was trained on top of the frozen model backbone. Our models outperform both self-supervised and language-guided models in these tests, as detailed in Appendix D Table 13. This further demonstrates the effectiveness and versatility of our proposed training methodology."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9381/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700413907448,
                "cdate": 1700413907448,
                "tmdate": 1700413907448,
                "mdate": 1700413907448,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "E60zlLaTes",
            "forum": "DwcV654WBP",
            "replyto": "DwcV654WBP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9381/Reviewer_tEsw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9381/Reviewer_tEsw"
            ],
            "content": {
                "summary": {
                    "value": "This paper, TVTSv2, is the second version of TVTS paper. It focuses on the foundation model of video representation learning. Specifically, this paper first points out the so called degradation issue existing in video representation field. Based on this degradation observation, this paper proposes a hypothesis that such degradation is from the noisy text data. Accordingly, it freezes the shallow layers of text encoder while training the deeper layers to alleviate this issue. In this way, the zero-shot performance is significantly improved to show the great generalization ability of the proposed training strategy. Very comprehensive experiments empirically validate the model effectiveness."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Video representation learning is a very challenging task, especially for large-scale scenario. I recognize such an exploration in this field.\n2. The stated degradation issue is interesting observation. Proposing solution based on it is well-motivated.\n3. The large-scale experiments are definitely an advantage of this work. They cover several evaluation scenarios."
                },
                "weaknesses": {
                    "value": "I mainly concern about the technical contribution. As mentioned in the draft, the proposed degradation-free training strategy freezes the shallow layers while tuning the deeper layer of text encoder. This training strategy is more like an engineering trick by tuning parts of the large-scale model, which can be commonly used for practical large-scale training."
                },
                "questions": {
                    "value": "Please refer to above sections for details. As mentioned in the weakness part, I mainly concern the technical contribution. On the other hand, I recognize the other parts of contribution of this paper. I would like to encourage the author to further emphasize the technical contribution of this paper for discussion. In addition, I am also willing to check other reviewers' comments for my final decision."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9381/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698713087221,
            "cdate": 1698713087221,
            "tmdate": 1699637183467,
            "mdate": 1699637183467,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Az4nBZHZ09",
                "forum": "DwcV654WBP",
                "replyto": "E60zlLaTes",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9381/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9381/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer tEsw"
                    },
                    "comment": {
                        "value": "**W1: Concern about technical contribution. The degradation-free training strategy is more like an engineering trick.**\n\n**A:** Thanks, we are willing to highlight the technical merits of our work, both from **experimental results** and the **underlying motivation**.\n\n**(i)** Experimentally, to demonstrate that our degradation-free training strategy goes beyond a mere engineering workaround, we introduced an alternative baseline. This baseline involves a ViT-B/32 model equipped with adapters [1] (M$ _{\\text{adapter}}$) inserted between layers of the text encoder, where only the adapter-introduced parameters are adjustable during training. The retrieval results on MSR-VTT, detailed in Appendix D Table 16, show that this approach is inferior to our method. This reinforces our assertion that end-to-end tuning can lead to style overfitting, even when the text encoder remains unchanged, proving that our partially frozen strategy is not a simplistic fix but a considered solution.\n\n**(ii)** From a motivational standpoint, scalability remains a significant and challenging research area, as recognized by Reviewer hjUh. This field is marked by several pioneering works featured in leading conferences, like VideoMAE V2 [2] and FLIP [3]. In our view, it is the simplicity and efficacy of these methods that render them ideal as foundational models, requiring minimal code adjustments for integration into existing frameworks. Upon acceptance, we plan to release all relevant codes, pre-trained models, and well-crafted APIs for immediate application. We are confident that these foundational models will greatly benefit the broader community, particularly academic institutions with limited resources for large-scale experiments. Our contributions, we believe, merit recognition at prestigious conferences such as ICLR.\n\n| Method | R@1 | R@5 | R@10 | MdR |\n| :--- | :---: | :---: | :---: | :---: |\n| CLIP | 30.6 | 54.4 | 64.3 | 4.0 |\n| M$ _{\\text{ours}}$ | 34.5 | 58.5 | 67.7 | 3.5 |\n| M$ _{\\text{adapter}}$ | 28.8 | 54.8 | 65.7 | 4.0 |\n\n> [1] Parameter-Efficient Transfer Learning for NLP, ICML 2019\n\n> [2] VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking, CVPR 2023\n\n> [3] Scaling Language-Image Pre-Training via Masking, CVPR 2023"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9381/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700413608960,
                "cdate": 1700413608960,
                "tmdate": 1700413608960,
                "mdate": 1700413608960,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "RJ9cHExJdo",
            "forum": "DwcV654WBP",
            "replyto": "DwcV654WBP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9381/Reviewer_hjUh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9381/Reviewer_hjUh"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents TVTSV2, an ambitious attempt to create a task-agnostic foundation model for spatiotemporal visual representations. The authors extend the dual-stream framework of CLIP and introduce a degradation-free pre-training strategy to maintain performance. While the work shows promising results in text-to-video retrieval tasks, it could benefit from a deeper evaluation of generalizability across various downstream applications. Furthermore, the paper's focus on scalability is commendable but lacks an in-depth computational and memory usage analysis, which is vital for high-dimensional video data. Robustness against noisy or incomplete data remains unexplored, posing questions on the model's applicability in real-world scenarios. Overall, the paper makes a notable contribution but requires further scrutiny in these areas to substantiate its claims fully."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Task-Agnostic Focus: The paper aims to create a foundation model that is task-agnostic, addressing a significant need for models that can generalize across various applications without requiring fine-tuning.\n- Novel Pre-training Strategy: The introduction of a degradation-free pre-training method is a notable innovation. It suggests a way to train complex models without losing performance, which is particularly challenging in the realm of video data.\n- Extension of Existing Architectures: The paper builds upon well-established models like CLIP but adapts them for spatiotemporal data. This approach leverages existing successes in the field while pushing into new domains.\n- Initial Empirical Success: The paper demonstrates promising results in text-to-video retrieval tasks, indicating that the model is not just theoretically sound but also empirically effective.\n- Scalability: The paper addresses the important issue of scalability, which is crucial for the practical application of machine learning models, especially for high-dimensional data like video.\n- Comprehensive Evaluation: The paper seems to include a variety of evaluation metrics and comparisons with state-of-the-art models, adding credibility to its claims.\n\nThe idea seems to follow the surgical fine-tuning [1]  idea by freezing shallow layers during the fine-tuning.\nReference :\n1- Lee, Yoonho, et al. \"Surgical fine-tuning improves adaptation to distribution shifts.\" arXiv preprint arXiv:2210.11466 (2022)."
                },
                "weaknesses": {
                    "value": "The paper primarily focuses on text-to-video retrieval tasks for its empirical evaluation, which may not sufficiently support its claim of being a task-agnostic foundation model. To fully establish its task-agnostic capabilities, the model should be rigorously evaluated on multiple downstream tasks such as action recognition, video summarization, and anomaly detection. Assessing performance on these additional tasks would provide a more comprehensive view of the model's adaptability and generalizability\n\nThe paper discusses scalability but falls short of providing a detailed computational and memory complexity analysis. This is crucial for practical applications involving high-dimensional video data. The authors should include empirical evaluations that quantify the model's computational time and memory usage during both training and inference."
                },
                "questions": {
                    "value": "Could you elaborate on the choice of tasks for empirical evaluation? How do you envision the model's performance on other types of spatiotemporal tasks like action recognition or anomaly detection?\n\nHow does the model perform under conditions of noisy or incomplete data? Have you considered evaluations that specifically test the model's robustness?\n\nWhile the paper discusses scalability, it lacks specific metrics on computational and memory requirements. Could you provide more detailed analyses on these aspects?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9381/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9381/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9381/Reviewer_hjUh"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9381/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698768966740,
            "cdate": 1698768966740,
            "tmdate": 1699637183362,
            "mdate": 1699637183362,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xHGEp1Skh5",
                "forum": "DwcV654WBP",
                "replyto": "RJ9cHExJdo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9381/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9381/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer hjUh"
                    },
                    "comment": {
                        "value": "**W1&Q1: Elaborate on the choice of tasks for empirical evaluation and envision the model's performance on other types of spatiotemporal tasks like action recognition or anomaly detection.**\n\n**A:** Thanks for your advice! We have conducted relative experiments to envision the model's performance in the revision.\n\nFor action recognition, the results are reported in Table 3 in our initial manuscript. We use the prompt template \"a person [CLASS]\" (e.g., a person running) for HMDB-51 [1], UCF-101 [2], Kinetics-400 [3], and Kinetics-600 [4], and evaluate SSV2 on a multi-choice setting, namely SSV2-MC. The detailed settings are stated in Appendix A. Our model brings significant improvements, i.e., 8.9%, 9.1%, 11.6% and 12.9% absolute gain on HMDB-51, UCF-101, Kinetics-400, Kinetics-600 and SSV2-MC respectively. The prompt-based method, i.e. ActionCLIP [5], also degrades on this test, possibly due to overfitting to manually constructed templates. When it turns to larger models, we even surpass X-Florecnce [6], a single-stream model with comparable video encoder parameters, by a large margin, revealing the superior scalability of our training paradigm. Even for the challenging SSV2-MC that contains various motion dynamics, our model still achieves promising results, solidifying the contribution of masked transcript sorting that promotes fine-grained spatiotemporal representation learning. In addition, we provide the liner classification results in Appendix D Table 13, where we surpass either  self-supervised or language-guided models.\n\nFor anomaly detection, we evaluate the performance on UCF-Crime [7] test split, which aims to recognize 13 anomaly events. According to Table 3 in our revised manuscript, our models also made remarkable improvements compared to the CLIP baseline and CLIP-ViP, unveiling their adaptability and generalizability.\n\n> [1] HMDB: A large video database for human motion recognition, ICCV 2011\n\n> [2] UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild, arXiv 2012\n\n> [3] The Kinetics Human Action Video Dataset, arXiv 2017\n\n> [4] A Short Note about Kinetics-600, arXiv 2018\n\n> [5] ActionCLIP: A New Paradigm for Video Action Recognition, arXiv 2021\n\n> [6] Expanding Language-Image Pretrained Models for General Video Recognition, ECCV 2022\n\n> [7] Real-world Anomaly Detection in Surveillance Videos, CVPR 2018\n\n**W2&Q3: Providing a detailed computational and memory complexity analysis during both training and inference.**\n\n**A:** Nice suggestion. We've updated our manuscript to include a comprehensive analysis of computational and memory complexity, which you can find in Appendix D Table 12. During the training phase, we optimized the batch sizes to fully utilize available GPU memory. Our analysis, especially the GFLOPs comparison between the video and text encoders, reveals that the most computationally intensive part is video encoding, particularly the spatiotemporal attention module. However, the use of an efficiently designed divided space-time attention mechanism ensures that even our largest model, i.e., ViT-H/14, can be run on consumer-level GPUs like the RTX 3090, thanks to its relatively modest memory requirements during inference.\n\n| Model | Training BS $\\times$ GPU | Training Mem | Inference Mem | Video GFLOPs | Text GFLOPs |\n| :--- | :---: | :---: | :---: | :---: | :---: |\n| B/32 | 24 $\\times$ 32 | 30.2 GB | 3.3 GB | 69 | 3 |\n| B/16 | 12 $\\times$ 64 | 27.5 GB | 4.5 GB | 278 | 3 |\n| H/14 | 2 $\\times$ 80 | 31.3 GB | 8.8 GB | 2650 | 24 |\n\n**Q2: How does the model perform under conditions of noisy or incomplete data?  Consider evaluations that specifically test the model's robustness.**\n\n**A:** Thanks for the constructive advice! To test the robustness of our model, we randomly mask some patches in each frame during inference to simulate corrupted data. The retrieval performance under different masking ratios on MSR-VTT is reported in Appendix Table  18. Thanks to the masked contrastive pre-training, our models are robust to data corruption. Impressively, even minor data distortions, such as 10% or 20% masking, do not significantly impact performance. More notably, our models demonstrate commendable robustness, maintaining considerable effectiveness even under substantial data incompleteness, with a masking ratio as high as 50%. This clearly underscores the reliability and adaptability of our models in handling corrupted data scenarios.\n\n| Model | Masking Ratio | R@1 | R@5 | R@10 | MdR |\n| :--- | :---: | :---: | :---: | :---: | :---: |\n| B/16 | 0 | 35.9 | 61.2 | 71.3 | 3.0 |\n| | 10 | 35.4 | 61.1 |71.0 | 3.0 |\n| | 20 | 35.0 | 60.6 | 70.4 | 3.0 |\n| | 50 | 32.6 | 60.0 | 69.9 | 3.0 |\n| H/14 | 0 | 38.2 | 62.4 | 73.2 | 3.0 |\n| | 10 | 37.3 | 62.1 | 72.7 | 3.0 |\n| | 20 | 36.0 | 61.6 | 72.1 | 3.0 |\n| | 50 | 35.2 | 60.4 | 70.1 | 3.0 |"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9381/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700413099072,
                "cdate": 1700413099072,
                "tmdate": 1700413099072,
                "mdate": 1700413099072,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "r3JEOduxeC",
            "forum": "DwcV654WBP",
            "replyto": "DwcV654WBP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9381/Reviewer_NPSQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9381/Reviewer_NPSQ"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors focus on building large-scale robust video models for out-of-the-box usage, which means the learned features can be used directly for novel tasks.\n\nThe authors have conducted detailed experiments and found that tuning text encoder end-to-end causes overfitting, thus losing generalization ability. To fix the issue, they propose to tune the text encoder partially.\n\nFinally, they adopt the transcript sorting task and masking techniques to scale up pretraining. The 1B model achieves new SOTA results on out-of-the-box tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is well-written and organized, with clear figures and tables.\n- The logic is clear and easy to follow.\n- Extensive ablation studies and analysis demonstrate the authors' statements."
                },
                "weaknesses": {
                    "value": "Overall, I appreciate the simple yet effective techniques in this paper. However, considering the differences between TVTSv2 and TVTSv1, the current paper may not be suitable for a conference but a journal as an extension:\n\n1. Different tuning: The key difference between the two versions is how to tune the text encoder. It is an interesting finding but may not be a qualified novelty for a new conference paper. And the poor performances caused by the weak initialization (Appendix D)?\n2. Same objectives/architecture/masking: The transcript sorting task and masking techniques have also been used in TVTSv1, though the masking strategies are different. And the architectures are the same (as Frozen[1]), where the residuals are skip-connected.\n\nConsidering the minor difference, I suggest the authors submit the paper as an extensive journal paper, but not a novel conference paper.\n\n----\nReference:\n\n[1] Bain, Max et al. \u201cFrozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval.\u201d ICCV 2021."
                },
                "questions": {
                    "value": "1. For DiDeMo, the code shows that it was tested on test split. Should it be tested on validation split?\n2. In Table 2, why the single-stream models are de-emphasized? \n3. In Table 2, should the authors list the size of the pretraining data for a clear comparison? It seems that the results for UMT are on a small 5M data, but others are on larger data.\n4. In Table 2, why the authors do not consider ViT-L and directly scale it to ViT-H?\n5. In Table 3, do those models good at retrieval also perform well, like OmniVL, CLIP-ViP, and UMT?\n\n----\nReference:\n\n[1] Wang, Junke et al. \u201cOmniVL: One Foundation Model for Image-Language and Video-Language Tasks.\u201d NeuIPS 2022.\n\n[2] Xue, Hongwei et al. \u201cCLIP-ViP: Adapting Pre-trained Image-Text Model to Video-Language Representation Alignment.\u201d ICLR2023.\n\n[3] Li, Kunchang et al. \u201cUnmasked Teacher: Towards Training-Efficient Video Foundation Models.\u201d ICCV2023."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9381/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698806976678,
            "cdate": 1698806976678,
            "tmdate": 1699637183246,
            "mdate": 1699637183246,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oRRpvXtXRS",
                "forum": "DwcV654WBP",
                "replyto": "r3JEOduxeC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9381/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9381/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer NPSQ"
                    },
                    "comment": {
                        "value": "**W1: Minor difference between TVTSv1 and TVTSv2**\n\n**W1.1: The partially frozen tuning strategy lacks novelty.** \n\n**A:** Thanks, we are willing to emphasize the novelty of the proposed partially frozen tuning. Note that it plays a vital role (rather than a trick) in training on the easy-to-scale-up ASR-video pairs, which is not trivial given that few previous papers consider tackling the text distortion issue. We carefully break down the experiment step and pinpoint the problem. We identify the problem via comprehensive experiments and figure out a simple yet effective solution that is in favor of practical use and can be well transferred to different datasets (Table 6) and vision models (Table 7). The logic, motivation and ablation studies are mostly acknowledged by all reviewers.\n\nTo further underscore our contribution, we introduce a ViT-B/32-based alternate baseline, integrating adapters [1] between text encoder layers, denoted as M$_{\\text{adapter}}$. During training, only the adapter-introduced parameters are trainable. The MSR-VTT retrieval results, detailed in the subsequent table, show that this alternative method yields comparatively lower performance. This outcome reinforces our assertion that end-to-end tuning can cause style overfitting, even when the text encoder remains unaltered. It also confirms that our approach of partially frozen tuning is not just a superficial fix, but a substantive and effective solution to the identified problem. We have added this discussion in our revised manuscript (Appendix D Table 16).\n\n| Method | R@1 | R@5 | R@10 | MdR |\n| :--- | :---: | :---: | :---: | :---: |\n| CLIP | 30.6 | 54.4 | 64.3 | 4.0 |\n| M$_{\\text{ours}}$ | 34.5 | 58.5 | 67.7 | 3.5 |\n| M$_{\\text{adapter}}$ | 28.8 | 54.8 | 65.7 | 4.0 |\n\nWe believe that a paper's contribution should not be limited to proposing novel methods. Extensive empirical studies (i.e., analyzing why performance degrades), useful findings/insights (i.e., the overfitted text model), scalable training schemes (i.e., the simple yet practical partial freezing), and foundation models (i.e., TVTSv2 models that can generalize to various tasks) are also essential to the community, especially in the era of big models. Such contributions are also acknowledged by Reviewer hjUh (Task-Agnostic Focus, Novel Pre-training Strategy, etc.) and Reviewer A6Wr (Effective Pre-training Strategy).\n\n> [1] Parameter-Efficient Transfer Learning for NLP, ICML 2019\n\n**W1.2 TVTSv1's poor performances may be caused by weak initialization (Appendix D).**\n\n**A:** We admit that a strong initialization yields better performance, as reported in Appendix D Table 20. **However, naively relying on stronger initialization leads to degraded performance when pre-training on large-scale ASR-paired videos.** As shown in Table 1 in our initial manuscript (the subsequent Table) , after equipping CLIP weights, all fully tuned variants still lag behind the partially tuned competitors (L3 vs L7, L5 vs L8), and they even degrade compared to the CLIP baseline (L1 vs L3, L5), indicating the importance of the proposed partial frozen tuning strategy for producing general-purpose video representations.\n\n| No | Method | ASR | Alt-text | Kinetics-400 (ZS AR) | DiDeMo (ZS T2V) |\n| :--- | :--- | :---: | :---: | :---: | :---: |\n| L1 | CLIP | $\\times$ | $\\times$ | 42.7 (+0.0) | 24.7 (+0.0) |\n| L3 | M$_{\\text{ASR-Full}}$ | YT-Temporal | $\\times$ | 45.7 (+3.0) | 19.3 (-5.4) |\n| L5 | M$_{\\text{All-Full}}$ | YT-Temporal | WebVid-2.5M | 47.9 (+5.2) | 24.2 (-0.5) |\n| L7 | M$_{\\text{ASR-Partial}}$ | YT-Temporal | $\\times$ | 48.1 (+5.4) | 26.8 (+2.1) |\n| L8 | M$_{\\text{All-Partial}}$ | YT-Temporal | WebVid-2.5M | 50.8 (+8.1) | 29.8 (+5.1) |"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9381/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700410986721,
                "cdate": 1700410986721,
                "tmdate": 1700410986721,
                "mdate": 1700410986721,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "581RiM0g3z",
                "forum": "DwcV654WBP",
                "replyto": "IszAKmxz7r",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9381/Reviewer_NPSQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9381/Reviewer_NPSQ"
                ],
                "content": {
                    "title": {
                        "value": "Response for the authors"
                    },
                    "comment": {
                        "value": "Sorry for the late response. As I claimed before, **I sincerely appreciate the simple yet effective techniques** and the authors's effort in my questions. \n\nI would like to first articulate my viewpoint. **For me, the paper's key finding of `partial tuning text encoder` is surprising, just like `mask pretraining` in FLIP. However, such a finding may not be enough for a conference paper. I acknowledge scalability is vital for current foundation models, but it's not well verified in this paper. For example, how about the `ViT-g` and how about `large noisy dataset (e.g., 25M/100M)`.** I understand the authors' constraints due to limited GPU resources, which hinder thorough verification. **So it's really hard for me to decide whether the paper should be accepted and I intend to consult with other reviewers for a final decision.** Currently, considering the relation between `V1` and `V2`, a  journal extension might be a more fitting avenue for this paper.\n\n----\n\nAs for the authors' responses:\n\n**(1)**  The concept of `single-stream` in the paper remains unclear to me. Could you please elucidate the meaning of `N & M` in the context of computational complexity during indexing? Additionally, why not opt for a simpler approach by utilizing the visual encoder and incorporating an additional classification head for linear probing?\n\n**(2)**  Concerning models with a `cross-modal encoder`, i.e., UMT in the table, would performance be enhanced by an additional encoder? Based on my experience with ALBEF-style architectures, the use of a matching loss via a `cross-modal encoder` typically bolsters performance.\n\nAnother minor suggestion: The table orders can be adjusted for better reading in the Appendix."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9381/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700666392858,
                "cdate": 1700666392858,
                "tmdate": 1700666392858,
                "mdate": 1700666392858,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]