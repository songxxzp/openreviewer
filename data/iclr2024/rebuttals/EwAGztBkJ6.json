[
    {
        "title": "On the Generalization of Gradient-based Neural Network Interpretations"
    },
    {
        "review": {
            "id": "RMskAX4GSZ",
            "forum": "EwAGztBkJ6",
            "replyto": "EwAGztBkJ6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7141/Reviewer_Cg7V"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7141/Reviewer_Cg7V"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates the generalization of gradient-based interpretation methods in deep learning. The authors demonstrate the significant influence of the sample size on the interpretation, such as saliency maps, in deep neural networks. Then they derive two generalization bounds for common gradient-based interpretation techniques by using the analysis presented in Bartlett et al. (2017). Notably, for SmoothGrad, they show that the generalization error of interpretation has a linear decrease with the standard deviation of the SmoothGrad noise. The paper complements these findings with numerical results demonstrating the impact of training sample size on interpretation outcomes."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The paper is well-written and investigates a topic that has not been explored before, namely the generalization of gradient-based interpretation."
                },
                "weaknesses": {
                    "value": "My primary concern lies in understanding the importance of investigating the generalization aspect of interpretation methods, and the validation of the generalization definition in this paper. Additionally, the theoretical contributions in this paper are somewhat restricted, as a substantial portion of the analysis is derived from Bartlett et al. (2017), with the key distinction being the definition of \"loss\" and \"error\".\n\nFor more detailed questions, please refer below."
                },
                "questions": {
                    "value": "1. Regarding the definition of $f^*$ in Eq.(5):\nFirstly, there is no guarantee that you only have one such optimal classifier. In the case where Eq.(5) returns a set of $f^*$, doesn't the definition of loss in Eq.(6) become problematic? While all the $f^*$ will yield the same testing performance, they do not necessarily produce the same output for the interpretation method. In other words, $\\mathrm{Loss}_I(f,x)$ is also a function of $f^*$, denoted as $\\mathrm{Loss}_I(f^*,f,x)$. This differs significantly from studying standard generalization error. Therefore, the generalization error defined in Eq.(6) can vary for the same $f$ and $x$. \n\n2. Another question arises in this context. Considering that there could be multiple $f^*$ with identical testing performance, and they may not produce the same saliency maps, why does it matter if the saliency maps are influenced by the training data? Is there any guarantee that different $f^*\\in\\mathcal{F}$ with different weight parameters will return identical outputs from the interpolation methods (for the same $x$)?\n\n3. Let's assume there is only one such $f^*$. According to Theorem 1, the generalization bound implies that interpretation demands a larger training set compared to the standard classification problem. An essential mystery in the success of deep learning is that overparameterized neural networks can generalize well without needing more data than the number of parameters. If Simple Gradient Method and Integrated Gradient prove to be unreliable with the same amount of data, does this imply that they are ineffective for interpreting deep neural networks?\n\n4. What is the fundamental implication of establishing generalization bounds for gradient-based interpretation methods when we cannot ensure good performance on the training data? In the context of standard generalization error, we can say an upper bound is provided to guide the minimization of empirical risk while controlling key quantities in the bound.  However, for interpretation methods, let $\\hat{f}^*$ be the empirical minimizer for a given training dataset, even if standard training leads to $f_w\\to\\hat{f}^*$, $\\mathrm{Loss}_I(\\hat{f}^*,x)=||I(\\hat{f}^*,x)-I(f^*,x)||_2$ can still be large. In other words, we lack clarity on whether interpretation methods might overfit to the training data (in the sense defined in Eq.(5)). In light of this uncertainty, discussing regularization is not feasible at this point."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7141/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698536732328,
            "cdate": 1698536732328,
            "tmdate": 1699636845519,
            "mdate": 1699636845519,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "li7LPfxDvA",
                "forum": "EwAGztBkJ6",
                "replyto": "RMskAX4GSZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7141/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7141/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' Response to Reviewer Cg7V"
                    },
                    "comment": {
                        "value": "We thank Reviewer Cg7V for his/her time and feedback. Here is our response to the comments and questions in the review:\n\n**1- \u201cThe importance of investigating the generalization aspect of interpretation methods\u201d**\n\n**Re:** Please refer to our general response to comment 1.\n\n**2- The role of** $f^*$ **in our definition of interpretation loss**\n\n**Re:**  Please refer to our general response to comment 2.\n\n**3- \"There is no guarantee that you only have one such optimal classifier.\"**\n\n**Re:** We note that if the minimizer to the population risk function is not unique, we could define  $f^*(x)=E_{f\\sim \\mathrm{Unif}(F^*)} [f(x)]$ as the expectation under the uniform distribution on the set $F^*$ of optimal population solutions, and our generalization guarantees for Theorems 1,2 will hold for this choice of $f^*$.\n\nFurthermore, as we discussed in the general response to comment 2, $I(f^*,x)$ in our definition of interpretation loss can be changed to $E_{S,A}[I(f_{A(S)},x)]$ where we take the expected value of  the interpretation map over the randomness of training set $S$ of size $n$ and a stochastic learning algorithm $A$. In that case, we have a unique expected value which can also be used for the analysis. Please note that the same Theorems 1,2 will also hold for the choice of reference map $I(f^*,x) = E_{S,A}[I(f_{A(S)},x)]$.\n\n\n**4-\u201cIf Simple Gradient Method and Integrated Gradient prove to be unreliable with the same amount of data, does this imply that they are ineffective for interpreting deep neural networks?\u201d**\n\n **Re:** This question is precisely the motivation of our generalization study, which highlights the importance of the generalization error (or variance) of an interpretation map with respect to the randomness of training data in the selection of an explanation method and its hyperparameters. For example, if SmoothGrad has a significantly lower generalization error in a learning setting, should a practitioner prefer SmoothGrad over SimpleGrad? As we argued in our general response to comment 1, we believe the answer to this question is yes and our work aims to better understand the generalizability of gradient-based maps."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7141/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700669886451,
                "cdate": 1700669886451,
                "tmdate": 1700669932090,
                "mdate": 1700669932090,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "n7Qo6MhpEU",
            "forum": "EwAGztBkJ6",
            "replyto": "EwAGztBkJ6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7141/Reviewer_DbhP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7141/Reviewer_DbhP"
            ],
            "content": {
                "summary": {
                    "value": "The work derives generalization bounds incorporating gradient based interpretations, which yield non-trivial results. It shows that the generalization of interpretations requires more training, and show that it can be improved with spectral normalization."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is interesting, and the bound is both non-trivial and important. It shows that generalization bounds may incorporate intuitive signals for the human observer. Further, it yields more into how neural network interpretations work. The empirical experiments match the theoretical results."
                },
                "weaknesses": {
                    "value": "The overall paper is good other than minor comments on the presentation (see Questions)"
                },
                "questions": {
                    "value": "Figure 1: the text and legend can be enlarged/improved. How were the lines produced? what exactly is shown in Figure 1b? What does each point represent? It seems like the lines don't represent the data. Have the authors considered different seeds for each network to add more points to the graph?\n\n** Possible missing references:**\n\n[1] Galanti, T., Galanti, L., & Ben-Shaul, I. (2023). Comparative Generalization Bounds for Deep Neural Networks. Transactions on Machine Learning Research, (ISSN 2835-8856)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7141/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698830205844,
            "cdate": 1698830205844,
            "tmdate": 1699636845410,
            "mdate": 1699636845410,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "P5RsdkoV7U",
                "forum": "EwAGztBkJ6",
                "replyto": "n7Qo6MhpEU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7141/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7141/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' Response to Reviewer Dbhp"
                    },
                    "comment": {
                        "value": "We thank Reviewer Dbhp for his/her time and feedback on our work. In the following, we respond to the comments and questions of the reviewer.\n\n**1- Figure 1b**    \n\n**Re: In Figure 1.b, we plot the Spearman rank correlation between the two interpretation maps of the networks $f_{S_1},f_{S_2}$ trained under disjoint training sets $S_1,S_2$ in the y-axis vs. the (Pearson) correlation coefficient between the argmax (i.e. prediction) of the softmax layer (i.e. the fraction of samples with the same argmax prediction by $f_{S_1},f_{S_2}$) in the x-axis. For each neural network architecture, represented by a different color in the figure, we show the points corresponding to split factors $\\frac{1}{2},\\frac{1}{4},\\frac{1}{8},\\frac{1}{16}$. We have made this point more clear in the revision. The training of all the networks in the experiments have been initialized at different random initializations to simulate the full randomness of the stochastic optimization algorithm for training the neural nets.\n\n**2- Missing Reference** \n\n**Re:** We thank the reviewer for mentioning the missing reference, which we have discussed in the revision."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7141/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700669326064,
                "cdate": 1700669326064,
                "tmdate": 1700669326064,
                "mdate": 1700669326064,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "I0z60kG1gW",
            "forum": "EwAGztBkJ6",
            "replyto": "EwAGztBkJ6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7141/Reviewer_Yqqc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7141/Reviewer_Yqqc"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the problem of generalization of gradient-based saliency maps of deep networks. Theoretical bounds are shown and experiments are carried out to validate the results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper is well written upto the experiments section. After that I find it difficult to comprehend the presentation. Details in Questions."
                },
                "weaknesses": {
                    "value": "I find the main problem the paper is trying to address somewhat contrived. Firstly, the usual motivation for post-hoc explanation of deep networks is to explain the prediction of a **given** network (trained or otherwise) on a **given** sample, as we get insights as to why a particular decision was made. From this perspective, I do not see the motivation to study how well the input gradients will generalize from train set to test set (in expectation). Why are we interested in knowing the MSE loss between our current network an the optimal network as defined in eq(6). \n\nContinuing on my first point, the authors further claim \"the generalization condition is necessary for a proper interpretation result on test samples, it is still not sufficient for a satisfactory interpretation performance\". I fail to appreciate this statement since it is not clear satisfactory interpretation (as defined by the authors) is the gradient-based saliency map generated for our optimal f* that minimizes the population loss. In my opinion, this is of little interest from the perspective of understand why a given network made a particular decision. I invite the authors to convince me otherwise. \n\nLastly, post-hoc explanations have been criticized for some time now in the community due to their reliability in explaining deep network presentations [1,2,3] (some of the references analyze the saliency map techniques evaluated in this work). This means that even if I had the **exact** same saliency map as the ground truth f*, methods like integrated gradients and simple gradients are simply unreliable in explaining what the deep network is doing. \n\nGiven, the above three arguments I fail to appreciate the utility of studying the generalization of saliency maps for deep networks. \n\nLastly, I find the experiment section not clearly written. I expand on this in specific questions below. \n\n1. Adebayo, J., Gilmer, J., Muelly, M., Goodfellow, I., Hardt, M., & Kim, B. (2018). Sanity checks for saliency maps. Advances in neural information processing systems, 31.\n2. Shah, H., Jain, P., & Netrapalli, P. (2021). Do input gradients highlight discriminative features?. Advances in Neural Information Processing Systems, 34, 2046-2059.\n3. Adebayo, Julius, et al. \"Post hoc explanations may be ineffective for detecting unknown spurious correlation.\" International conference on learning representations. 2021."
                },
                "questions": {
                    "value": "1. Figure 2 is not clear. The caption says \"we observe that model pairs generate increasingly consistent interpretations.\" It is not clear o me what is being compared here for consistent interpretation? Since f* (the optimal classifier) is never accessible to us. What is the baseline here then?\n\n2. The following statement is unclear \"We train a neural net for every data subset\nfor 200 epochs. To further improve the interpretation generalization, we allow models to train on\n\u201cmore data\u201d by using pre-trained ImageNet weights, then fine-tuning for 50 epochs.\" Why are we training for 200 epochs and then taking Pretrained weights and fine-tuning for 50 more epochs? Do we start with the fine tuned weights and train for 250 epochs. This should be made clear. \n\n3. The following is unclear \"On test set data, we plot\nthe normalized Spearman correlation of network interpretations against softmax predictions.\" What exactly is the equations for this computation? Is the softmax predictions the argmax of the softmax or the entire k dimensional softmax scores? What are the two quantities among which the correlation is computed?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7141/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699049754405,
            "cdate": 1699049754405,
            "tmdate": 1699636845303,
            "mdate": 1699636845303,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CmjuzPoJOS",
                "forum": "EwAGztBkJ6",
                "replyto": "I0z60kG1gW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7141/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7141/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' Response to Reviewer Yqqc"
                    },
                    "comment": {
                        "value": "We thank Reviewer Yqqc for his/her time and feedback. The following is our response to the reviewer\u2019s comments:\n\n**1- The motivation to study how well the input gradients will generalize from train set to test set**\n\n**Re:** Please refer to our general response to comment 1.\n\n**2-\u201cit is not clear satisfactory interpretation (as defined by the authors) is the gradient-based saliency map generated for our optimal** $f^*$ **that minimizes the population loss\u201d**\n\n**Re:** Please refer to our general response to comment 2.\n\n**3- \u201cThis means that even if I had the exact same saliency map as the ground truth** $f^*$, **methods like integrated gradients and simple gradients are simply unreliable in explaining what the deep network is doing.\u201d**\n\n**Re**: We would like to clarify that our generalization analysis does not focus on finding the interpretation map under ground-truth solution $f^*$. As argued in the general response to the previous comment, we only attempt to measure and control the variance of the interpretation map with respect to the randomness of training data used for training the neural network. Also, our analysis reveals the application of Gaussian smoothing in SmoothGrad to control the variance of the interpretation map. \n\n\n**4- (Figure 2) \"Since** $f^*$ **(the optimal classifier) is never accessible to us. What is the baseline here then?\"**\n\n**Re:** As we have described in the paragraph titled \"Experimental design\" on page 7, we have split the original training set into disjoint subsets with split factors $\\frac{1}{2},\\frac{1}{4},\\frac{1}{8},\\frac{1}{16}$ and Figure 2 shows how the Grad-Cam maps for a group of test data will look for two neural nets trained using those disjoint (thus independent) training sets. In this experiment, we do not consider any population solution $f^*$ and only display the consistency of the interpretation maps between neural nets trained with independent training data, to show how the consistency could vary with the size of the training set.\n\n**5- \u201cWhy are we training for 200 epochs and then taking pretrained weights and fine-tuning for 50 more epochs?\u201d**\n\n**Re:** The size of the datasets we used in our numerical analysis are all bounded by 100,000. Therefore, even for the maximum split factor $\\frac{1}{2}$ we could compare the consistency of interpretation maps for neural nets trained using maximum 50,000 training data. To simulate a setting where the two neural nets are trained on more samples, we follow the idea of initializing the neural net at a pretrained network on ImageNet (trained on more than 1 million samples) and applying 50 epochs of training on the samples in the $\\frac{1}{2}$-splits of the original training sets. The numerical results in Figure 2 shows that this idea significantly improves the consistency between the two networks\u2019 interpretation maps, supporting our hypothesis that using more training data could lead to less variance in the interpretation maps. \n\n\n**6- \u201cIs the softmax predictions the argmax of the softmax or the entire k dimensional softmax scores?\u201d**\n\n**Re:** In Figure 1.b, we plot the Spearman rank correlation between the two interpretation maps of the networks $f_{S_1},f_{S_2}$ in the y-axis versus the (Pearson) correlation coefficient between the argmax (i.e. prediction) of the softmax layer (i.e. the fraction of samples with the same argmax prediction by $f_{S_1},f_{S_2}$) in the x-axis. We have clarified this point in the revision."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7141/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700669216298,
                "cdate": 1700669216298,
                "tmdate": 1700669216298,
                "mdate": 1700669216298,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]