[
    {
        "title": "Functional Bayesian Tucker Decomposition for Continuous-indexed Tensor Data"
    },
    {
        "review": {
            "id": "Tqa5MTXf2e",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3091/Reviewer_TaJq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3091/Reviewer_TaJq"
            ],
            "forum": "ZWyZeqE928",
            "replyto": "ZWyZeqE928",
            "content": {
                "summary": {
                    "value": "This paper introduces the Functional Bayesian Tucker Decomposition (FunBaT)\nalgorithm. Instead of treating the factors of the Tucker decomposition as\nmatrices with discrete row indexing, the authors instead think of the factors\nas latent Gaussian process function priors. Earlier work in this area of\n[Schmidt, ICML 2009] considers a simpler \"continuous CP decomposition\" for\ntensors.\nThe authors then make connections\nto stochastic differential equations (SDEs) and conditional expectation\npropagation (CEP) for the learning task, which is somewhat analogous to a\ncontinuous version of alternating least squares. The authors provide experiments\non synthetic and real-world datasets, of order $K=2$ and $K=3$ respectively."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Extends work from Tucker to CP, since CP is a generalization of Tucker if you\n  restrict the core tensor to be diagonal\n- A \"continuous\" version of Tucker decomposition on the factors is an excellent\n  idea to explore\n- Uses interesting real-world datasets with continuous features (indices), and\n  the results are comprehensive and compelling. That said, the US-TEMP\n  experiment could be improved since a core shape of $(1,1,1)$ is used, which\n  means a product of the factor functions is being learned."
                },
                "weaknesses": {
                    "value": "- There is not enough emphasis on related work, especially with [Fang et al.,\n  ICML 2022], which appears to be a *very similar paper*. This discussion should happen\n  much earlier in the paper, and there should be more than one mention of this\n  Fang et al. (2022), as it it almost gets buried in its current form. Missing\n  seminal work:\n  * \"Bayesian Tensor Regression\" by [Guhaniyogi et al., JMLR 2017]\n- While the synthetic experiments have the potential to be very strong and\n  compelling, it would be useful to see how the tensor reconstruction looks as a\n  function of the number of samples. It may not be surprising in its current\n  form that we can recover ground truth with $650$ samples.\n- In the Beijing Air experiments, please explain if non-uniform core shapes were\n  explored (as they might better help fit the data). A recent ICML paper\n  investigates core shape selection and could be of interest:\n  * \"Approximately Optimal Core Shapes for Tensor Decompositions\" by [Ghadiri\n    et al., ICML 2023]"
                },
                "questions": {
                    "value": "### Questions\n- [page 01] Is Tucker decomposition really a more compact low-rank\n  representation? Maybe the factor matrices can be more compact, but doesn't\n  this come at the cost of a larger \"core structure\" then, e.g., CP\n  decomposition?\n- [page 03] By \"successive derivatives of $m$-th order\", do you instead mean to\n  write $\\frac{d^m f(x)}{dx^{m}}$?\n- [page 03] \"the trained model cannot handle new objects with never-seen\n  indices\" -- this is not necessarily true, and is the core idea behind tensor\n  completion.\n- [page 04] Why do you use $\\tau^{-1}$ as Gaussian noise (i.e., variance)\n  instead of $\\sigma^2$?\n- [page 06] Is the preset (scalar) mode rank $R$ for a given Tucker core of\n  shape $(r_1, \\dots, r_K)$ the product? I.e., $R = \\prod_{i=1}^K r_i$? If so,\n  this should be stated explicitly and you need to remove the claim that \"the\n  linear costs of both time and space ...\" since $R$ is exponential in the order\n  $K$.\n\n### Typos and suggestions\n- [page 01] \"there were finite\" --> \"there are finite\"\n- [page 01] \"tensor train(TT)\" --> \"tensor train (TT)\"\n- [page 01] \"Tucker decomposition is famous for ...\" --> \"Tucker decomposition\n  is widely-used for ...\"\n- [page 01] \"and time..\" --> \"and time.\"\n- [page 01] \"or simpler CP format\" --> \"or the simpler CP format\"\n- [page 01] \"which is with more compact ...\" --> \"which is a more compact ...\"\n- [page 02] Inconsistent subheading capitalization (see ICLR style guide and\n  example paper)\n- [page 02] \"under the following settings\" --> \"under the following setting\"\n- [page 02] typo at end of sentence with \"groups of latent factors\" (should be\n  a comma instead of a period)\n- [page 02] suggestion: consider superscripting factor matrices/vectors as\n  ${U}^{(k)}$ instead of ${U}^k$ to remove any ambiguity in meaning\n- [page 02] \"The classic CANDECOMP/PARAFAC (CP)\" --> \"The classic CP\" (you\n  already introduced the acronym)\n- [page 02] missing space: factors: $y_{i}$ (same for many other sentences on\n  this page)\n- [page 03] \"denote as $f \\sim$\" --> \"denoted as $f \\sim$\"\n- [page 03] consider using $\\ell$ instead of $l$ as a hyperparameter in the\n  Matern kernel\n- [page 03] missing space: \"efficient $O(n)$ inference\"\n- [page 03] No need to give the meaning of \"FunBaT\" again\n- [page 04] \"preset latent rank:$\\{r_1,\\dots,r_K\\}$ --> \"preset latent rank\n  $(r_1, \\dots, r_K)$. Missing space between subsequent sentences.\n- [page 04] suggestion: when you shift to \"continuous-indexed tensors\", it may\n  be more clear to say that $(i_1^{n},\\dots, i_K^{n}) \\in \\mathbb{R}^{K}$\n- [page 04] missing space: \"Namely, \"\n- [page 05] typo: \"massage merging\" --> \"message merging\"\n- [page 06] punctuation typos in Algorithm 1 inputs\n- [page 06] suggestion: Substantially more discussion / math can be given to\n  FunBaT-CP. The focus of the paper is on Tucker decomposition, but this is a\n  very nice special case for which it would be nice to know more of its\n  properties.\n- [page 06] Missing space: \"function factorization is(Schmidt, 2009).\" along\n  with other citations in this paragraph.\n- [page 07] typo: \"alternating least square\" --> \"alternating least squares\".\n  Many more typos / punctuation errors in this paragraph.\n- [page 09] typo: \"lantide mode\""
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3091/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3091/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3091/Reviewer_TaJq"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3091/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697241825905,
            "cdate": 1697241825905,
            "tmdate": 1700684125391,
            "mdate": 1700684125391,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PlWyU6F2UN",
                "forum": "ZWyZeqE928",
                "replyto": "Tqa5MTXf2e",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3091/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3091/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We greatly thank and appreciate the reviewer's time and effort in offering insightful and detailed comments and reviews. We address the comments below:( C: comments; R: responses)\n\n>C1: \"not enough emphasis on related similar work **BCTT** (Fang, et al. Bayesian Continuous-Time Tucker Decomposition, ICML 2022)\", \"Missing seminal work: \"Bayesian Tensor Regression\" by [Guhaniyogi et al., JMLR 2017]\"\n\nR1: Good point. We acknowledge the use of similar techniques **BCTT**, specifically the utilization of state-space Gaussian Processes (GP) to model the latent dynamics in Tucker decomposition. As this similarity has been noted in short in the related works section of our paper, we do plan to follow your suggestion and highlight this more prominently in subsequent versions on their **significant differences in formulation and inference. Please refer to the response (R1) to reviewer MPRT for more detailed elaborations**. For the \"Bayesian Tensor Regression\"[Guhaniyogi et al., JMLR 2017], we will add it in the citations and related works section.\n\n>C2: \"For the synthetic experiments,  it would be useful to see the tensor reconstruction looks as a function of the number of samples?\"\n\nR2: Good suggestion! We add reconstruction RMSE with different numbers of samples, here are the results(we generate 1300 samples on the surface of the 3D function, and randomly select parts of samples with Gaussian noise (STD=0.02) for training, evaluate on the remaining samples:):\n\n|  Number of training samples   | Observation Ratio |   RMSE |    \n| :--- |---: |---:  |\n| 130   | 0.1   | 0.128   | \n| 260  | 0.2      | 0.102  |\n| 390  |   0.3    | 0.068   | \n| 420 |   0.4    | 0.041   | \n| 650 |   0.5    | 0.027   | \n| 780 |   0.6    | 0.026   | \n| 810 |   0.7    | 0.025   | \n\nAs we can see, the RMSE decreases with the increase of the number of training samples. With 650 samples, the RMSE is less than 0.03, which is similar to the noise level and can be seen as a good reconstruction. We will add the results in the latest version.\n\n>C3: explain if non-uniform core shapes were explored in the Beijing Air experiments with \"Approximately Optimal Core Shapes for Tensor Decompositions\" by [Ghadiri et al., ICML 2023]\n\nR3: Thanks for raising the great reference! We will add the discussion on non-uniform core in our experiments with the latest work [Ghadiri et al., ICML 2023].\n\n>C4: \"Is Tucker decomposition really a more compact low-rank representation?\"\n\nR4: Good point! We do agree that CP is more compact and that statement may be not accurate enough. We will polish it in the latest version. For a more detailed discussion on the compactness between Tucker  CP and TT, please refer to the response (R4) to reviewer MPRT.\n\n> C5: By \"successive derivatives of \nm-th order\", do you instead mean to write $d^m f(x)/ dx^m$?\n\nR5: Yes, we mean $d^m f(x)/ dx^m$. We will polish it to make it more clear.\n\n> C6: \"The trained model cannot handle new objects with never-seen indices\" -- this is not necessarily true, and is the core idea behind tensor completion.\"\"\n\nR6: Our statement was intended to highlight that traditional tensor models typically rely on grid-structured data characterized by discrete indices, and thus may struggle with grid-free interpolation for new objects identified by real-valued indices. We apology for any confusion caused and will revise our wording to clarify this point more effectively.\n\n> C7: \"why use $\\tau^{-1}$ instead of $\\sigma^2$ for the Gaussian noise?\"\n\nR7: Good question. In the Bayesian framework we employ, the most common modeling approach for Gaussian noise involves assuming that its inverse: $\\tau$ \u2014also known as 'precision'\u2014 follows a Gamma distribution(see statement under equation 11). This convention is widely used in Bayesian analysis for Gaussian likelihood with observation noise.\"\n\n> C8: Is the preset (scalar) mode rank $R$\n for a given Tucker core of shape $(r_1, \\ldots, r_K)$\n the product? I.e., $R = \\prod_{k=1}^K r_k$?\n\nR8: No, actually the preset (scalar) mode rank $R$ is rank for each mode and the shape of core is $(R, R, \\ldots, R)$. We will add the clarification in the latest version. For the detailed statement for linear cost with rank $R$, please refer to the **response (R4) for reviewer BN7i**\n\n>C9: Typos and suggestions \n\nR9: Thanks for the correction! We do greatly appreciate the reviewer's time and effort in offering such detailed help. We will fix them in the latest version."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3091/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700448158318,
                "cdate": 1700448158318,
                "tmdate": 1700448158318,
                "mdate": 1700448158318,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VU715MuFtw",
                "forum": "ZWyZeqE928",
                "replyto": "Tqa5MTXf2e",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3091/Reviewer_TaJq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3091/Reviewer_TaJq"
                ],
                "content": {
                    "comment": {
                        "value": "I have read the author response and am willing to increase my rating from 3 to 5."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3091/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700684065133,
                "cdate": 1700684065133,
                "tmdate": 1700684108148,
                "mdate": 1700684108148,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "llpFA3fhCf",
                "forum": "ZWyZeqE928",
                "replyto": "Tqa5MTXf2e",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3091/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3091/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank you so much for reading our response and increasing the score!"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3091/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700684743979,
                "cdate": 1700684743979,
                "tmdate": 1700684767376,
                "mdate": 1700684767376,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0b2eYyTgFj",
            "forum": "ZWyZeqE928",
            "replyto": "ZWyZeqE928",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3091/Reviewer_BN7i"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3091/Reviewer_BN7i"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a function Tucker decomposition for tensors with continuous-valued indices. The authors firstly use a Gaussian process to map indices to Tucker factors and then contract these factors to obtain the entry value. To efficiently learn the GP prior, an algorithm based on state-space GP and expectation propagation is derived. For experiments, the authors test the model on synthetic data and several spatiotemporal data imputation tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The authors study the functional tensor decomposition for continuously-indexed tensors. This seems to be an interesting and novel topic in the field tensor decomposition and may have some new applications."
                },
                "weaknesses": {
                    "value": "1. The setting of continuously-indexed tensors is new in the community of tensor decomposition. However, my main concern is how this task is related and different from traditional regression tasks. Why do we need such construction of tensors and what is the significance of the proposed function Tucker decomposition. Considering the experiments, the authors show applications in spatiotemporal data imputation. However, there are many existing methods, including interpolation like GP, VAE [1], GAN [2], LSTM [3], diffusion models [4] and many of their variants. Besides, the problem setting is very similar to CPNN [5]. For a better understanding of the paper, I think it might be better to have a discussion of these lines of works and empirical comparisons\n- [1]. Mattei, & Frellsen. (2019). MIWAE: Deep generative modelling and imputation of incomplete data sets. ICML.\n- [2]. Yoon, et al. (2018). Gain: Missing data imputation using generative adversarial nets. ICML.\n- [3]. Cao, et al. (2018). Brits: Bidirectional recurrent imputation for time series. NeurIPS\n- [4]. Tashiro, et al. (2021). Csdi: Conditional score-based diffusion models for probabilistic time series imputation. NeurIPS.\n- [5]. Tancik, et al. (2020). Fourier features let networks learn high frequency functions in low dimensional domains. NeurIPS.\n\n2. Since the proposed model employs nonlinear GPs, it might be better to compare with some nonlinear or GP-based tensor decompositions. Also, baselines for continuous-time tensor decompositions are also good choices, as the authors also mentioned in the related work."
                },
                "questions": {
                    "value": "1. Compared with GP regression, the main difference of the proposed model is preserving the Tucker structure in the final layer. I am wondering why this construction is so helpful as shown in empirical results. \n2. There are matrix inversion and multiplications in the update rules. Why is the time complexity linear with the rank $R$? \n3. The authors adopted three resolution settings for BeijingAir datasets. What is the setting for US-TEMP?\n\ntypo: In paragraph above Figure 1, whore tensor -> whole tensor?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3091/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3091/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3091/Reviewer_BN7i"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3091/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698758820476,
            "cdate": 1698758820476,
            "tmdate": 1700704191566,
            "mdate": 1700704191566,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dzvpzPjYoY",
                "forum": "ZWyZeqE928",
                "replyto": "0b2eYyTgFj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3091/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3091/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the careful review! We address the comments below:( C: comments; R: responses)\n\n>C1: \"Relation and Differences Between Functional Tensor Models and Traditional Regression Tasks and Models [1,2,3,4,5].\n\nR1: We are grateful to the reviewer for posing such an insightful question and for the excellent references provided, particularly CPNN [5]. We will certainly cite these and expand our discussion on this topic in the latest version of our manuscript.\n\nA brief and high-level response is that, in addition to **learning a good mapping from indexes to values**\u2014the focus of traditional regression tasks and models\u2014**functional tensor models** emphasize **learning robust representations of latent dynamics**. This approach not only facilitates more flexible and robust prediction but also enhances interpretability and utility for downstream tasks.\n\nConsider the US-TEMP dataset as an illustrative example. Each observation entry represents a temperature value corresponding to specific spatial-temporal indexes, such as (time, latitude, longitude). This is a quintessential spatial-temporal dataset. Various model types, ranging from simple linear regression to advanced deep models like CNNs, GNNs, VAEs, and diffusion models, can handle this task. However, these models are typically **black-box** in nature, offering limited insight into latent dynamics and lacking flexibility for potential downstream tasks. For instance, predicting temperature at a new location with additional features, like *altitude*, would typically require retraining the model from scratch.\n\nConversely, our proposed method is capable of learning latent dynamics and providing a **low-rank, dynamic representation** of key factors within the data. As demonstrated in Figure 2, the learned latent dynamics not only yield accurate predictions but also help elucidate the evolving processes underlying the data. We further emphasize the potential of these representations for downstream tasks. Taking the example of incorporating *altitude* as a new feature, our method allows for the addition of a new mode to the tensor. We can reuse the learned representations of existing features and only train the new mode dynamics. These **reusable representations** are crucial for achieving more flexible and general modeling capabilities.\"\n\n> C2: Compare with some nonlinear or GP-based tensor decompositions and continuous-time tensor decompositions.\n\nR2: Good suggestion! We add the comparison with GP-based tensor decomposition: **GPTF**(Zhe, et al. Distributed flexible nonlinear\ntensor factorization, 2016)  and continuous-time tensor decomposition: **BCTT**(Fang, et al.Bayesian Continuous-Time Tucker Decomposition, 2022) on the three datasets of BeijingAir. The mean of RMSE over 5 runs are shown:\n<!-- add tables -->\n\n|  PM2.5   | R=2 |   R=3 |     R=5 |    R=7 |    \n| :---        |            ---: |---:  | ---:  | ---:  | \n| **GPTF**   | 0.424   | 0.425   |  0.477  | 0.464   |\n|**BCTT**  | 0.375      | 0.372  | 0.386   | 0.371    |\n| **FunBaT-CP**  |   0.296    | 0.294   | 0.292   | **0.296**   |\n| **FunBaT** |   **0.288**    | **0.291**   | **0.288**   | 0.318   |\n\n|  PM10    | R=2 |   R=3 |     R=5 |    R=7 |    \n| :---        |            ---: |---:  | ---:  | ---:  | \n| **GPTF**   | 0.454  | 0.479   |  0.535  | 0.531   |\n|**BCTT**  | 0.408      | 0.421  | 0.443   | 0.420    |\n| **FunBaT-CP**  |   0.343    | **0.347**   | 0.352   | **0.335**   |\n| **FunBaT** |   **0.328**    | 0.348   | **0.338**  | 0.347   |\n\n|  SO2   | R=2 |   R=3 |     R=5 |    R=7 |    \n| :---        |            ---: |---:  | ---:  | ---:  | \n| **GPTF**   | 0.459   | 0.475   |  0.540  | 0.504   |\n|**BCTT**  | 0.411      | 0.418  | 0.398   | 0.410    |\n| **FunBaT-CP**  |   **0.386**    | **0.384**   | **0.385**   | **0.385**   |\n| **FunBaT** |   **0.386**    | 0.386   | 0.388   | 0.390   |\n\n As we can see, for every rank, **FunBaT** is with the best performance. The **BCTT** gets the second-best performance, as it can model the continuous dynamics for time mode, but not for other modes. The **GPTF** is with the worst performance. The reason is that **FunBaT** is a with-nature model for functional tensor data, as we discussed in C1-R1."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3091/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700447807141,
                "cdate": 1700447807141,
                "tmdate": 1700447807141,
                "mdate": 1700447807141,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kgawK1fS5N",
                "forum": "ZWyZeqE928",
                "replyto": "0b2eYyTgFj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3091/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3091/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> C3: Why the construction of Tucker is helpful compared with GP regression?\n\nGood question! We claim that our model actually **wraps multiple GP regressions with a tensor structure(Tucker or CP)**. In the case of a $K$-mode tensor with rank $R$, our approach involves managing $K \\times R$ separate GP regressions. This is in stark contrast to a conventional GP regression, which typically relies on a single kernel to capture the entirety of multi-view data patterns. The tensor-based design **splits the high-order data and multi-view interactions into low-rank, distinct subspaces**. This decomposition **significantly simplifies the learning process for each individual GP**. The concept of decomposing and representing complex, high-dimensional data in a low-rank format is a broadly adopted strategy in the field of machine learning.\"\n\n>C4: Why is the time complexity linear with the rank $R$?\n \nR4: Great point! The state-space GP is assigned to each dimension of the mode independently (equation 7). Thus, for mode with rank $R$, we will have $R$ separate state-space GP, and each GP's state dimension\u2014determining the matrix mat/inverse cost in KF/RTS\u2014 is only related to kernel-decided order $m$ ($m=1$ for $Matern-3/2$  kernel, $m=2$ for $Matern-5/2$  kernel).  During the inference over specific mode,  **we run the KF and RTS over the $R$ separate state-space dynamics, saying, a loop over $R$ to run KF and RTS.** Thus, the time complexity is linear with the rank $R$. \n\nWe do understand the confusion is that we use the compact notation $Z^k$ to refer the concatenated states of all $R$ dimensions. We will add the clarification in the latest version and thanks again for pointing out this point.\n\n> C5:  Why not put a multi-resolution setting for US-TEMP?\n\nR5: Good point! We do not add the multi-resolution setting for US-TEMP is because the data's original size is relatively small: (15 latitudes, 95 longitudes, 267 timestamps), compared with BeijingAir, whose size is (428 atmospheric pressure, 501 temperature, 1461 timestamps). Discretization will make the data even smaller, and not suitable for real-world applications. We will add the discussion on this in the latest version.\n\n> C6:  Typo in the paragraph above Figure 1\n\nR6: Thanks for the correction! We will fix it in the latest version."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3091/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700447900594,
                "cdate": 1700447900594,
                "tmdate": 1700447900594,
                "mdate": 1700447900594,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fauzuUPkCz",
                "forum": "ZWyZeqE928",
                "replyto": "kgawK1fS5N",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3091/Reviewer_BN7i"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3091/Reviewer_BN7i"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the authors to address my concerns. I would like to raise my score to 6."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3091/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700704164965,
                "cdate": 1700704164965,
                "tmdate": 1700704164965,
                "mdate": 1700704164965,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "K3bHdclvpM",
            "forum": "ZWyZeqE928",
            "replyto": "ZWyZeqE928",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3091/Reviewer_wp4E"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3091/Reviewer_wp4E"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposed an Bayesian method for tensor Tucker decomposition, where the data are assumed to have continues index. Each column of the factor matrices is modeled using Gaussian process (GP), and efficient method was proposed to infer the parameters of the GP. The idea of this paper is quite interesting,\nand the experimental results are convincing."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1, the idea that model factor matrices using GP is new.\n2, efficient inference methods are proposed to determine the unknown parameters."
                },
                "weaknesses": {
                    "value": "1, the methods need to set the rank of the tensor manually, which is usually unknown in practice.\n3, In table 1, the MSE of the proposed method much lower than the baselines. But no explanation is provided to show why."
                },
                "questions": {
                    "value": "1, Does the equation 4 implies the smoothness of z(x)?\n2, Compared to Tucker decomposition, CP decomposition offers several advantageous properties, particularly its uniqueness, making it more favored in many applications. However, it is worth noting that CP decomposition typically requires larger factor matrices compared to Tucker decomposition. Therefore, it is interesting to discuss the computational complexity of the proposed method in relation to the rank of the tensor and evaluate its applicability in scenarios that require setting a large rank."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3091/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3091/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3091/Reviewer_wp4E"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3091/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698822669137,
            "cdate": 1698822669137,
            "tmdate": 1699636254945,
            "mdate": 1699636254945,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "synfrsWcgM",
                "forum": "ZWyZeqE928",
                "replyto": "K3bHdclvpM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3091/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3091/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank reviewer wp4E for the valuable comments and support! We address the comments below:( C: comments; R: responses)\n\n> C1: Explanation of MSE of the proposed method much lower than the baselines in Table 1.\n\n R1: Good question! For the significant performence gain of the proposed method in Table 1, we think the main reasons are:\n1.  The three pollution tensor datasets have inherent and strong **mode-wise continuity**, as their modes are : (pressure, temperature, time). The proposed method can capture the mode-wise continuity by the **mode-wise dynamics**. However, the baselines, like all static tensor methods, can not model that fact.\n2.  The observations of the three datasets are **sparse**. The observation ratio is less than 0.001%. The FTT series baseline, with **too many basis functions**, is easy to get **overfitting** issue, as it is with  The proposed method is with **robust** modeling, as it is with probabilistic inference. The Bayesian framework can be seen as a **regularization** to avoid overfitting.   \n   \n> C2: Equation 4 implies the smoothness of z(x)?\n\n R2: \"Good point! As equation 4 indicates,  z(x)  is a Linear Time-Invariant (LTI) Stochastic Differential Equation (SDE), so we cannot say it's **smooth** under the strict mathematical definition. In fact, we cannot label any SDE or stochastic process as **smooth**, as stochastic processes can be unbounded. However, we can state that z(x)  is **stationary**. Given that z(x)  is an LTI SDE, its mean and autocovariance function are time-invariant. Generally speaking, with its stationary characteristics, the curve of z(x)  is unlikely to exhibit significant abrupt changes, meaning that its curve tends to be relatively smooth. \n\n\n> C3: Discussion on the computational complexity over rank for CP and Tucker. \n\n R3: Good point! We do agree the trade-off between the efficiency of CP and the capacity of Tucker is tricky and crucial for real-world applications. We will add a discussion on this in the future version."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3091/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700447694064,
                "cdate": 1700447694064,
                "tmdate": 1700447694064,
                "mdate": 1700447694064,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "P9iReUCbFd",
            "forum": "ZWyZeqE928",
            "replyto": "ZWyZeqE928",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3091/Reviewer_MPRT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3091/Reviewer_MPRT"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a method called Functional Bayesian Tucker Decomposition (FunBaT) to handle continuous-indexed tensor data. Traditional tensor decomposition methods are designed for discrete and finite-dimensional indexes, but real-world data often contains continuous indexes such as geographic coordinates. FunBaT solves this problem by treating the continuous-indexed data as the interaction between the Tucker core and a group of latent functions. Gaussian processes are used as functional priors to model the latent functions, and an equivalent stochastic differential equation is used to reduce computational cost. The paper introduces an efficient inference algorithm based on advanced message-passing techniques. FunBaT outperforms existing methods in synthetic and real-world applications while being able to identify interpretable patterns. The paper also provides explanations on tensor decomposition, function factorization, the use of Gaussian processes as state-space models, and the FunBaT model and algorithm. Overall, the paper offers a solution for tensor decomposition on continuous-indexed tensor data."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Extension to continuous-indexed tensor data: The paper proposes a method called Functional Bayesian Tucker Decomposition (FunBaT) that extends traditional Tucker decomposition to handle continuous-indexed tensor data. \n2. Utilization of Gaussian processes as functional priors: FunBaT models the latent functions in continuous-indexed data using Gaussian processes as functional priors. This allows for flexible and efficient modeling of the latent functions. Additionally, the paper reduces computational cost by constructing an equivalent stochastic differential equation, further enhancing the efficiency of the method.\n3. Improved performance and interpretable patterns: FunBaT is demonstrated to outperform existing methods in both synthetic and real-world applications. It not only achieves lower prediction errors but also has the ability to identify interpretable patterns in the data."
                },
                "weaknesses": {
                    "value": "1. The paper's innovation and significance may not be convincingly conveyed. The idea of using Gaussian processes to model N factor matrices for continuous data appears to be straightforward and intuitive. Furthermore, the paper's modeling approach is very similar to that of reference [3], which employs Gaussian processes to model core tensors. \n2. There are now many function decomposition models based on Tucker decomposition, e.g., [1,2], so the Introduction section regarding related works appears to be inaccurate.\n3. Furthermore, the significance of this paper isn't fully articulated. Merely stating that there is currently no function Tucker decomposition doesn't seem to provide a sufficiently compelling reason for its importance.\n4. Regarding the optimization algorithm, this paper is also closely related to some previous work, e.g., [3]\n[1] M.Imaizumi, K.Hayashi (2017). \"Tensor Decomposition with Smoothness\". PMLR: International Conference on Machine Learning 2017\n[2] Luo Y, Zhao X, Li Z, et al. Low-Rank Tensor Function Representation for Multi-Dimensional Data Recovery[J]. arXiv preprint arXiv:2212.00262, 2022.\n[3] Fang S, Narayan A, Kirby R, et al. Bayesian Continuous-Time Tucker Decomposition[C]//International Conference on Machine Learning. PMLR, 2022: 6235-6245."
                },
                "questions": {
                    "value": "1. What are the advantages of this function tensor decomposition method compared to existing approaches? Utilizing the Tucker decomposition allows for the representation of higher-order tensors and effectively mitigates the problem of the curse of dimensionality. Why does the Tucker decomposition lead to a more compact and flexible low-rank representation compared with TT and CP?\n2. What is the primary motivation behind this paper in comparison to other function CP, Tucker, and TT decompositions? If this method was established solely because GP-based functional Tucker decomposition does not currently exist, I believe the motivation appears to be rather weak.\n3. What is the primary innovation of this paper? Apart from considering continuous tensor indices in the modeling, is there a deeper level of differences compared to Bayesian Continuous Tucker Decomposition?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3091/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698831478902,
            "cdate": 1698831478902,
            "tmdate": 1699636254850,
            "mdate": 1699636254850,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "x8Nghq1PtB",
                "forum": "ZWyZeqE928",
                "replyto": "P9iReUCbFd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3091/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3091/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank reviewer MPRT for their careful review! We address the comments below:( C: comments; R: responses)\n\n>C1: \u201c**FunBAT** modeling approach is very similar to that of **BCTT** (Bayesian Continuous-Time Tucker Decomposition, ICML 2022)\u201d , What is the primary innovation of **FunBAT**? What's the deeper level of differences between **FunBAT** and **BCTT**.\n \nR1: Good point! We acknowledge the use of similar techniques **BCTT**, specifically the utilization of state-space Gaussian Processes (GP) to model the latent dynamics in Tucker decomposition. This similarity has been noted in short  in the related works section of our paper, and we plan to highlight this more prominently in subsequent versions on their differences: \n\n- **Differences in target scenarios and formulations**:   **BCTT** focuses on time-series tensor data, whereas ,**FunBAT** is centered on functional tensor data. This difference in application leads to distinct challenges and modeling:  **BCTT** involves **one Tucker-core dynamic** with static factors, **FunBAT** employs a static core with **groups of mode-wise dynamics**. This fundamental difference in formulation leads to varied inferences.\n\n- **Differences in Inference Algorithms**:\n Due to the divergent formulations, the inference algorithms between the two methods show significant differences. **For each observation**, **BCTT** needs to infer only **one state of the single temporal dynamic**. In contrast, **FunBaT** requires inferring **multiple states of multiple dynamics**, depending on each mode's index of the observation. which is more challenging. This results in distinctly different procedures in terms of message approximation, passing, and merging. Specifically, in **BCTT**, all Tucker core time-varying functions share the same timestamp, and then can be treated as **one temporal dynamic** by stacking. Thus, we only need to approximate the message once, and then the dynamic inference is in a **synchronous** manner. However, in **FunBaT**,  we will run a loop over tensor mode to do mode-wise conditional moment matching, and then get the messages factors fed to different functions. The inferece of multiple dynamics is in a **asynchronous** manner. \n\n>C2: Existing functional Tucker decomposition works e.g., [1,2]\n\nR2: Thanks for the excellent reference. We will modify our claim. We will add them in citations and correct the statements.\n\n>C3: What are the advantages of this function tensor decomposition method compared to existing approaches? What's the primary motivation of this paper? Is it only because \"there is no functional Trucker?\"\n\nR3: Great question! The main advantage of **FunBAT** is that **it's a scalable and flexible Bayesian model**, with inherent abilities for robust modeling and probabilistic inference. For example, it can **offer uncertainty-aware prediction at any index (equation 17)**. However, most existing works on functional tensor taking **deterministic approximation**, saying, the weighted sum of basis functions(FTT series, [1]), or MLP ([2]), can not do this. \n\nThe Bayesian methods, featured for robustness and uncertainty quantification, are underused in this field. The only work in the early year [Schmidt,2009] that bridged the Bayesian method and function tensor, is not scalable and limited to CP. Thus, as there is  **no scable Bayesian method for functional tensor**, we design **FunBAT** by utilizing advanced Bayesian learning tools, like CEP[Wang,2019] and state-space GP[Solin,2016]. That is our primary innovation and motivation.\n\n We also want to claim **the proposed Bayesian framework is not only limited for Tucker, it also supports CP(section 4.2)**. We highlight the \"Functional Tucker\" as Tucker formulation is more challenging.  Furthermore, we believe the proposed framework can be extended to TT too, which will form a general scalable Bayesian framework fitting various decomposition methods. We will explore more in-depth in this direction in our future work.\n\n>C4:  Why does the Tucker decomposition lead to a more compact and flexible low-rank representation compared with TT and CP?\n\nR4: Good point! In our view, compared with CP, Tucker is with stronger capacity as it models all possible interactions. Compared to TT, Tucker's rank setting is more flexible, as there is no constraint for the mode rank. However, TT sets a chain of matrix multiplication to get tensor entry, constraining the size(rank) of adjacent mode to fit the valid matrix multiplication. Moreover, the compact core tensor of Tucker can provide insights of the interpretability of which interactions are crucial, it will help people better understand the data.  On the other side, we acknowledge that the statement may be not accurate enough and ambiguous, and will polish it in the latest version."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3091/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700447610298,
                "cdate": 1700447610298,
                "tmdate": 1700447610298,
                "mdate": 1700447610298,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]