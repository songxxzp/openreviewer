[
    {
        "title": "Connect Later: Improving Fine-Tuning for Robustness with Targeted Augmentations"
    },
    {
        "review": {
            "id": "qGTNdQxtUg",
            "forum": "YGTSLDAPqb",
            "replyto": "YGTSLDAPqb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6720/Reviewer_erbX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6720/Reviewer_erbX"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes Connect Later to improve model robustness in domain adaptation scenarios. The approach first leverages generic augmentations to pretrain on combined unlabeled source and target data. It then employs carefully designed targeted augmentations during fine-tuning on labeled source data to better connect the source and target domains based on knowledge of their distribution shift. The experiment shows the effectiveness of Connect Later over several baselines on three datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is clearly written in general, so the overall quality of the paper is satisfactory."
                },
                "weaknesses": {
                    "value": "1. The proposed framework hinges on the manual design of the transformation distribution and requires knowledge about the distribution shift between source and target domains to create effective targeted augmentations. This property limits the applicability of the proposed method in scenarios where the shift is unknown or diverse. \n\n2. On iWildCam, Connect Later only marginally outperforms ERM+targeted augmentations, which indicates that if there is diverse distribution shifts, the proposed method is not quite effective."
                },
                "questions": {
                    "value": "Could the authors test the sensitivity of the performance of Connect Later with respect to the design choice of transformation distribution?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6720/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698689584495,
            "cdate": 1698689584495,
            "tmdate": 1699636771990,
            "mdate": 1699636771990,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5mrQ0s5ns7",
                "forum": "YGTSLDAPqb",
                "replyto": "qGTNdQxtUg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6720/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6720/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the feedback. erbX notes that \u201cthe paper is clearly written\u201d and \u201cthe experiment shows the effectiveness of Connect Later over several baselines on three datasets\u201d. We address specific points below:\n\n> On iWildCam, Connect Later only marginally outperforms ERM+targeted augmentations, which indicates that if there is diverse distribution shifts, the proposed method is not quite effective.\n\nWe ran additional seeds for iWildCam to clarify these results, for a total of 15 seeds. We report 95% confidence intervals on the mean estimate in parentheses and demonstrate that **the mean F1 results of ERM+targeted augs are outside of the 95% confidence intervals of Connect Later.**\n\n|                      | ID Test Macro F1 | OOD Test Macro F1 |\n|---|---:|---:|\n| ERM                  | 46.4 (0.5)       | 30.4 (0.6)        |\n| Standard fine-tuning | 46.4 (0.8)       | 31.2 (0.6)        |\n| ERM + targeted augs  | 51.4 (0.6)       | 36.1 (0.7)        |\n| DANN                 | 48.5 (3.2)       | 31.9 (1.6)        |\n| CORAL                | 40.5 (1.6)       | 27.9 (0.5)        |\n| Noisy Student        | 47.5 (1.0)       | 32.1 (0.8)        |\n| Connect Later        | 51.7 (0.8)       | 36.9 (0.7)        |\n\n**To further strengthen our results, we tested on another benchmark dataset, Camelyon17-WILDS, and show that Connect Later sets a new state-of-the-art on this benchmark for DenseNet121.** Connect Later applied with the stain color jitter targeted augmentation achieved 94.9% +/- 0.4% accuracy OOD compared to the current state-of-the-art (93.8 +/- 0.2%, cite ICON). **Overall, Connect Later achieves the state-of-the-art OOD performance on iWildCam-WILDS (for ResNet-50), Camelyon17-WILDS (for DenseNet-121), and AstroClassification.**\n\n\n|                      | ID Val Avg Acc | OOD Test Avg Acc |\n|---|---:|---:|\n| ERM                  | 89.3 (0.9)     | 65.2 (1.1)       |\n| Standard fine-tuning | 92.3 (0.2)     | 91.4 (0.9)       |\n| ERM + targeted augs  | 96.7 (0.0)     | 90.5 (0.4)       |\n| DANN                 | 86.1 (1.3)     | 64.5 (1.2)       |\n| CORAL                | 92.3 (0.7)     | 62.3 (1.9)       |\n| ICON (SOTA)          | 90.1 (0.3)     | 93.8 (0.2)       |\n| Connect Later        | **98.5 (0.0)**    | **94.9 (0.4)**       |\n\n\n> Could the authors test the sensitivity of the performance of Connect Later with respect to the design choice of transformation distribution?\n\n- **During the rebuttal period, we verified that Connect Later (with targeted augmentations) outperforms Connect Later with generic augmentations at fine-tuning time in OOD performance, tested on AstroClassification (79.9% vs. 72.7%)**. The difference in ID accuracy between Connect Later and standard fine-tuning+generic augmentations is much smaller (80.54% vs 79.9%) in comparison. This suggests that the choice of augmentation is important, especially for OOD performance.\n- For variations on targeted augmentations, Gao et al. 2023 tested a variant (Copy-Paste All Backgrounds) of the targeted augmentation we use for iWildCam (Copy-Paste Same Y). Copy-Paste segments out the animal from the image and pastes it onto a different background. Copy-Paste Same Y requires that the background is from a camera trap that has observed this animal species before, preventing, e.g., camels being pasted in water. Copy-Paste All Backgrounds does not leverage this requirement. **Gao et al. found a 2% degradation in OOD performance with Copy-Paste All Backgrounds compared to Copy-Paste Same Y, indicating that the realism of the targeted augmentation is important.** \n- For pretraining augmentations, we performed an ablation on the masking percentage for the AstroClassification task (Fig 3) and demonstrated that both ID and OOD performance are relatively robust to the chosen masking percentage between 20-90%. \n\n> The proposed framework hinges on the manual design of the transformation distribution and requires knowledge about the distribution shift between source and target domains to create effective targeted augmentations. This property limits the applicability of the proposed method in scenarios where the shift is unknown or diverse.\n\n**We describe a general procedure for designing targeted augmentations from the source and target data distributions in Section 4 of the paper**, as well as an example of designing the redshifting augmentation used for AstroClassification within this framework. While these augmentations do require some manual design, **a relatively small amount of manual design leads to strong empirical results (SOTA on AstroClassification, iWildCam-WILDS, Camelyon17-WILDS + best performance on new dataset Redshifts).**"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6720/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700381750116,
                "cdate": 1700381750116,
                "tmdate": 1700381750116,
                "mdate": 1700381750116,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3oDiOYnaZs",
            "forum": "YGTSLDAPqb",
            "replyto": "YGTSLDAPqb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6720/Reviewer_atYU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6720/Reviewer_atYU"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes Connect Later to improve robustness. It involves performing self-supervised pretraining (e.g., masked autoencoding or contrastive learning) followed by finetuning with augmentations designed with knowledge of the distribution shift. Supportive results were shown on several real-world dataset."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Evaluations were done on real world datasets with supportive results."
                },
                "weaknesses": {
                    "value": "1. Generalizability of the method. \n    - If target augmentations involve knowledge of the distribution shift, why pre-training is needed? If we are able to generate the target data, where other options include using powerful generative models [1], why do we need pre-training? It seems to be more a question of when is pre-training necessary. \n    - It is also not clear if pre-training in general is useful as the experimental results were only shown with one type of pre-training method and a different pre-training for different datasets. How sensitive are the results to the choice of pre-training method?\n    - From Table 2, it seems like the results are sensitive to the target augmentations. For iwilds most of the gains seem to come from the targeted augmentations. But for the astro datasets the ID test acc for astroclassification is better without targeted augmentations. One a new task or dataset, how should one choose the targeted augmentations?\n2. Test-time methods, like test-time augmentation/training also makes use of unlabelled data. These methods aim to adapt to the distribution shift as they occur. Furthermore, some of these pre-training objectives like masked auto-encoding have been used for test-time adaptation [2]. It may be beneficial to pose these methods wrt Connect Later.\n4. Presentation\n    - Sec 4 gives the technical details for creating augmentations before explaining the tasks. It may not be clear what the tasks for Astroclassification and Redshift are about and so the technical details are not easy to understand. I would suggest moving Sec 5 before 4.\n    - In Fig 2, top row, what do the colors mean? Why does augmentation introduce errors?\n    - It may be useful to have additional columns in the main table of results (e.g. Tab 2), that describes what the different methods are. E.g., whether finetuning augmentations are used, and with checks in the rows for the relevant methods.\n\n\n\n\n----\n[1] Diversify Your Vision Datasets with Automatic Diffusion-based Augmentation. NeurIPS\u201923 \n\n[2] Y. Gandelsman, et al. Test-Time Training with Masked Autoencoders. NeurIPS\u201923"
                },
                "questions": {
                    "value": "See the points raised in weaknesses for questions and suggestions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6720/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698749236498,
            "cdate": 1698749236498,
            "tmdate": 1699636771877,
            "mdate": 1699636771877,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Z9Jo2nrCT5",
                "forum": "YGTSLDAPqb",
                "replyto": "3oDiOYnaZs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6720/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6720/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the feedback. atYU notes that **supportive results were shown on several real-world datasets**. We answer specific questions below:\n\n> If target augmentations involve knowledge of the distribution shift, why pre-training is needed? If we are able to generate the target data, where other options include using powerful generative models [1], why do we need pre-training? It seems to be more a question of when is pre-training necessary.\n\n**The ablations in the paper show that pretraining is necessary**: specifically, ERM+targeted augs removes the pretraining step from Connect Later and results in worse OOD accuracy on all datasets compared to Connect Later (67.5% vs. 79.9% OOD accuracy on AstroClassification, 36.3% vs. 37.2% OOD macro F1 on iWildCam).\nNote that even though targeted augmentations can transform source data to more closely resemble target data, they are not perfect and do not describe the target distribution fully. Pretraining incorporates unlabeled data from the target to learn a better representation of the target data. Generative models such as Stable Diffusion could also be used to more easily design a targeted augmentation, and we view them as complementary to our framework. \n\n> the experimental results were only shown with one type of pre-training method and a different pre-training for different datasets. How sensitive are the results to the choice of pre-training method?\n\n- **Connect Later improves OOD accuracy with both masked autoencoding (AstroClassification, Redshifts) and contrastive pretraining (iWildCam-WILDS, Camelyon17-WILDS).**\n- During the rebuttal period, we ran the standard fine-tuning baseline with SWaV contrastive pretraining on another benchmark, Camelyon17-WILDS, and found that **the choice of pretraining method is not the source of inconsistent OOD results from pretraining.** On Camelyon17-WILDS, contrastive pretraining with standard fine-tuning improves accuracy over both ERM+targeted augs and ERM. This is very different from iWildCam-WILDS, where contrastive pretraining with standard fine-tuning did not even improve accuracy over ERM. \n\n- On Camelyon17-WILDS, Connect Later also achieves the new state-of-the-art ID and OOD accuracy. **Overall, Connect Later achieves the state-of-the-art OOD accuracy on iWildCam-WILDS (for ResNet-50), Camelyon17-WILDS (for DenseNet-121), and AstroClassification.**\n\n|                      | ID Val Avg Acc | OOD Test Avg Acc |\n|---|---:|---:|\n| ERM                  | 89.3 (0.9)     | 65.2 (1.1)       |\n| Standard fine-tuning | 92.3 (0.2)     | 91.4 (0.9)       |\n| ERM + targeted augs  | 96.7 (0.0)     | 90.5 (0.4)       |\n| DANN                 | 86.1 (1.3)     | 64.5 (1.2)       |\n| CORAL                | 92.3 (0.7)     | 62.3 (1.9)       |\n| ICON (previous SoTA)          | 90.1 (0.3)     | 93.8 (0.2)       |\n| Connect Later        | **98.5 (0.0)**     | **94.9 (0.4)**       |\n\n- **Instead, we explain the inconsistency in OOD results of pretraining by differences in the connectivity of domains and classes in each dataset.** During the rebuttal period, we investigated why pretraining was much more effective for OOD performance for AstroClassification than for iWildCam by evaluating connectivity following Shen et al., 2022 [1]. We found that one of the assumptions made in [1] that governs when contrastive pretraining works for UDA was violated for iWildCam, but not for AstroClassification.\n\n- Specifically, we evaluate the connectivity parameters alpha, beta, and gamma (alpha: across-domain (same class, different domain) connectivity, beta: across-class (same domain, different class) connectivity, gamma: across-both (different domain, different class) connectivity) for iWildCam and AstroClassification. We use the multi-crop augmentation strategy used in SwAV contrastive pretraining for iWildCam and the masking augmentation used in AstroClassification pretraining, respectively. Shen et al., 2022 requires that min(alpha, beta) > gamma for contrastive pretraining to work as a UDA method. We find that beta (across-class) < gamma (across-both) for iWildCam, which violates Shen et al.,\u2019s assumption, whereas the assumption holds for AstroClassification. We also observe that both across-domain connectivity and across-class connectivity are much higher for AstroClassification compared to iWildCam, verifying our reasoning for the differing OOD downstream performance with pretraining.\n| Dataset             | across-domain | across-class | across-both  |\n|---|---:|---:|---:|\n| iWildCam            | 0.116         | 0.071        | 0.076        |\n| AstroClassification | 0.287         | 0.159        | 0.097        |\n\n[1] Connect, Not Collapse: Explaining Contrastive Learning for Unsupervised Domain Adaptation, 2022."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6720/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700381458806,
                "cdate": 1700381458806,
                "tmdate": 1700381458806,
                "mdate": 1700381458806,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3FllfiHd3V",
                "forum": "YGTSLDAPqb",
                "replyto": "3oDiOYnaZs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6720/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6720/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Hello!\n\nWe were wondering if you've had a chance to look over our comments? We show that the choice of pretraining method is not the cause for inconsistent results by testing with SwAV contrastive pretraining on another benchmark dataset, Camelyon17-WILDS, where pretraining does improve over ERM. We also show that Connect Later sets a new state-of-the-art on Camelyon17-WILDS with DenseNet-121. There are also other experiments and details in the comments above!\n\nPlease let us know if you have other questions or concerns. Thank you!"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6720/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700596422822,
                "cdate": 1700596422822,
                "tmdate": 1700596445633,
                "mdate": 1700596445633,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BtBhMFtG3v",
                "forum": "YGTSLDAPqb",
                "replyto": "3oDiOYnaZs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6720/Reviewer_atYU"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6720/Reviewer_atYU"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the clarifications and discussions. My biggest concern is still on the generality of the method, e.g. how would one use the proposed method on a new dataset? It seems like different datasets uses different pre-training methods/only considers one pre-training method and the target augmentations need to be carefully designed. Thus, I am inclined to keep my score."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6720/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700650398017,
                "cdate": 1700650398017,
                "tmdate": 1700651671958,
                "mdate": 1700651671958,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9T4OHHKswH",
            "forum": "YGTSLDAPqb",
            "replyto": "YGTSLDAPqb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6720/Reviewer_vvbp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6720/Reviewer_vvbp"
            ],
            "content": {
                "summary": {
                    "value": "This manuscript deals with the domain adaptation problem, i.e., the setting in which there is a domain shift between a source and target domain and where only unlabelled data is available for the target domain. One approach to this problem is to use self-supervised learning using both the source and target domain data, followed by fine-tuning with the labelled source domain data.\n\nThe authors begin with the observation that this approach leads to inconsistent results compared to regular supervised training (ERM). They hypothesize this is because the self-supervised methods learn features that are strongly domain-specific, which means that when the model is fine-tuned the classifier from the last layer might not generalize beyond the source distribution. The authors propose a solution which involves using targeted augmentations (i.e., augmentations that are specifically designed to remove spurious domain-dependent features) during the fine-tuning phase. They show that this improves performance on three different datasets."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper looks at an interesting problem: Unsupervised domain adaptation is a very relevant problem (e.g., in many cases practitioners have labelled datasets available from a curated/lab setting, but need to generalize to actual deployment conditions). At the same time, self-supervised learning has become very popular, but its performance in the context of unsupervised domain adaptation (UDA) is still unclear.\n\nThis paper introduces a new dataset (RedShifts), and contains some encouraging results on two different domains (camera trips and astronomical observations) while performing several relevant ablations (model scale, whether to train the last layer before fine-tuning, strength of pre-training augmentations).\n\nThe paper is relatively easy to read, with good writing and a clear structure."
                },
                "weaknesses": {
                    "value": "Overall, I find this paper lacking in a variety of areas:\n\nThe authors' refer several times to Shen et al. (2022). This paper argues that contrastive pre-training is beneficial as long as data augmentations ensure that augmented examples must be more likely to change class or domain than changing both. The authors of this manuscript seem to argue in section 3 that the bad results they observe are explained by a violation of this assumption from Shen et al. (2022). However, this isn't tested rigorously (see section 6 of Shen et al., 2022, for examples on how to evaluate connectivity on real world datasets).\n\nA second shortcoming is that different self-supervised methods were used for the iWildCam-WILDS dataset (SWaV contrastive learning) and the astronomical time-series (masked autoencoding). This makes it hard to draw conclusions from table 1, since it is unclear which differences can be attributed to the use of different pre-training objectives.\n\nThe results on the iWildCam dataset are not very strong (given the means and standard deviations listed, there's a 20% chance that ERM + targeted augs outperforms Connect Later for any run). This leaves the improvements in the astronomical time-series datasets as the stronger proof of Connect Later's performance. But these results are a bit confusing to me: How is it that standard fine-tuning improves in-domain performance (tables 1 and 2)? This suggests to me that these datasets are possibly very small and benefit strongly from the regularization that pre-training provides. In that case, is Connect Later really addressing a domain shift issue? Or is it just addressing a regular overfitting issue? The latter seems plausible given the very small size of the dataset (6,74 objects). I would argue that in order to draw conclusions, the method should be tested on a wider variety of larger datasets.\n\nA baseline that seems to be missing as well is the use of general augmentations during fine-tuning (my understanding is that standard fine-tuning uses no augmentations at all). This would clarify whether it is important to use targeted augmentations during fine-tuning, or whether any form of augmentations would be fine.\n\nOverall, I find the insights that this paper provides limited: It is not surprising to me that targeted augmentations would help with fine-tuning, given that they were shown to help with regular ERM. The bigger question to me is the one raised in section 3: Why does pre-training not always lead to increased OOD performance? This question remains largely unanswered. I think this paper would be better if it provided, for example, (1) clear evidence that the assumptions from Shen et al. (2022) are routinely violated in real-world datasets and commonly used augmentations, (2) a thorough evaluation of how different pre-training techniques (MAEs, contrastive learning) perform in the UDA setting, or (3) strong evidence that targeted augmentations are particularly important compared to general augmentations."
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6720/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698895189549,
            "cdate": 1698895189549,
            "tmdate": 1699636771753,
            "mdate": 1699636771753,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pg2lQpV8nQ",
                "forum": "YGTSLDAPqb",
                "replyto": "9T4OHHKswH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6720/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6720/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the feedback. vvbp notes that the paper **looks at an interesting problem** and contains **encouraging results on two different domains**. We answer specific questions below:\n\n> Why does pre-training not always lead to increased OOD performance? This question remains largely unanswered. I think this paper would be better if it provided, for example, (1) clear evidence that the assumptions from Shen et al. (2022) are routinely violated in real-world datasets and commonly used augmentations\t\n\n**We empirically evaluated connectivity during the rebuttal period and show that the connectivity measurements of iWildCam-WILDS violates the assumptions from Shen et al, 2022 that govern the success of contrastive pretraining for domain adaptation, while AstroClassification has much higher connectivity** (details below). This validates our intuition that generic augmentations, such as the ones used in SwAV contrastive pretraining, are not enough to connect the domains for certain datasets and shifts. \n\nSpecifically, we evaluate the connectivity parameters alpha, beta, and gamma (alpha: across-domain (same class, different domain) connectivity, beta: across-class (same domain, different class) connectivity, gamma: across-both (different domain, different class) connectivity) for iWildCam and AstroClassification.\nWe use the multi-crop augmentation strategy used in SwAV contrastive pretraining for iWildCam and the masking augmentation used in AstroClassification pretraining, respectively. Shen et al., 2022 requires that min(alpha, beta) > gamma for contrastive pretraining to work as a UDA method. **We find that beta (across-class) < gamma (across-both) for iWildCam, which violates Shen et al.,\u2019s assumption, whereas the assumption holds for AstroClassification. We also observe that both across-domain connectivity and across-class connectivity are much higher for AstroClassification compared to iWildCam, verifying our reasoning for the differing OOD downstream performance with pretraining.** We thank the reviewer for this suggestion and will add this to the paper in the final revision.\n\n| Dataset             | across-domain | across-class | across-both  |\n|---|---:|---:|---:|\n| iWildCam            | 0.116         | 0.071        | 0.076        |\n| AstroClassification | 0.287         | 0.159        | 0.097        |\n\nWe follow Shen et al., 2022 to measure connectivity. First, we train a binary classifier to distinguish between class-domain pairs (e.g. for across-domain connectivity, the classifier distinguishes between augmented examples with the same class but different domain). We compute the connectivity as the test 0-1 error of these classifiers (1 - test_accuracy), averaged over class-domain pairs. \n\n> different self-supervised methods were used for the iWildCam-WILDS dataset (SWaV contrastive learning) and the astronomical time-series (masked autoencoding). This makes it hard to draw conclusions from table 1, since it is unclear which differences can be attributed to the use of different pre-training objectives.\n\n**During the rebuttal period, we ran the standard fine-tuning baseline with SWaV contrastive pretraining on another benchmark, Camelyon17-WILDS, and found that the choice of pretraining method is not the source of inconsistent OOD results from pretraining.** On Camelyon17-WILDS, contrastive pretraining with standard fine-tuning improves accuracy over both ERM+targeted augs and ERM. This is very different from iWildCam-WILDS, where contrastive pretraining with standard fine-tuning did not even improve accuracy over ERM. Our results on Camelyon17-WILDS and iWildCam-WILDS both use SwaV contrastive learning, showing that the choice of pretraining method is not the source of inconsistent OOD results. **On Camelyon17-WILDS, Connect Later also achieves the new state-of-the-art ID and OOD accuracy.**\n\n|                      | ID Val Avg Acc | OOD Test Avg Acc |\n|---|---:|---:|\n| ERM                  | 89.3 (0.9)     | 65.2 (1.1)       |\n| Standard fine-tuning | 92.3 (0.2)     | 91.4 (0.9)       |\n| ERM + targeted augs  | 96.7 (0.0)     | 90.5 (0.4)       |\n| DANN                 | 86.1 (1.3)     | 64.5 (1.2)       |\n| CORAL                | 92.3 (0.7)     | 62.3 (1.9)       |\n| ICON          | 90.1 (0.3)     | 93.8 (0.2)       |\n| Connect Later        | **98.5 (0.0)**     | **94.9 (0.4)**       |\n\nDANN and CORAL results are from Sagawa et al., 2022 (10 seeds) and the previous state-of-the-art results are from ICON (https://github.com/a-tea-guy/ICON, 3 seeds). All other results are averaged over 20 seeds. We report 95% confidence intervals in parentheses."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6720/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700380696610,
                "cdate": 1700380696610,
                "tmdate": 1700380696610,
                "mdate": 1700380696610,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]