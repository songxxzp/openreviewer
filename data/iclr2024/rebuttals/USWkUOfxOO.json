[
    {
        "title": "Pseudo-Calibration: Improving Predictive Uncertainty Estimation in Domain Adaptation"
    },
    {
        "review": {
            "id": "myFGIogAzV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission516/Reviewer_M3LQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission516/Reviewer_M3LQ"
            ],
            "forum": "USWkUOfxOO",
            "replyto": "USWkUOfxOO",
            "content": {
                "summary": {
                    "value": "The paper proposes a confidence calibration method for unsupervised domain adaptation. First, the adapted network is applied to the samples from the target domain to obtain pseudo labels. Next, a mixup technique is used on pairs of target samples with different pseudo-labels to create a synthetic sample and a soft-label from each pair. Finally, the synthesized pseudo-labeled set is used to find the optimal Temperature Scaling by minimizing the negative likelihood function."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is clearly written and easy to follow. The paper reports extensive experiments and shows improved results compared to previous methods. It is interesting to see that previous UDA calibration methods don't work well and in some cases, they are even worse than the uncalibrated model."
                },
                "weaknesses": {
                    "value": "My main concern is that the method is heuristic and not clearly motivated. It is well known that a mixup training procedure yields more calibrated models. However, it is unclear from the paper why learning Temperature Scaling using a convex combination of pseudo-labeled samples yields meaningful temperature values.  How is this method derived from the UDA situation?  What happens if we apply the\nsame technique in a standard network training setup (without domain shift problems ) to calibrate the network?\nThe mixture coefficient lambda is set to 0.65.  Again this looks like a heuristic.  is there any theoretical justification for this value? \nI would expect a symmetrical mixture combination (lambda=0.5) to be a suitable choice.  The success of a UDA method in different domain shift situations depends on the size of the gap between the domains.  I would expect that the UDA calibration would be dependent on how well the UDA method manages to close the domain gap. I don't see how the proposed method handles this aspect of the UDA calibration problem."
                },
                "questions": {
                    "value": "What happens when we apply the proposed mixup procedure to other UDA calibration methods such as vector scaling and matrix scaling?\n\nWhat happens if we apply the same technique in a standard network training setup (without domain shift problems ) to calibrate the network?  Does it work well?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission516/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697354428433,
            "cdate": 1697354428433,
            "tmdate": 1699635978534,
            "mdate": 1699635978534,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lMXzUzkxL2",
                "forum": "USWkUOfxOO",
                "replyto": "myFGIogAzV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission516/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission516/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer M3LQ - Part 1"
                    },
                    "comment": {
                        "value": "> **Q1**: My main concern is that the method is heuristic and not clearly motivated.\n\n**A1**: While we acknowledge that our method is heuristic, it's important to highlight that the designs of our PseudoCal method are rooted in clear motivations and can be intuitively explained. **The inspiration for the PseudoCal framework stems from the factorized negative log-likelihood (NLL) objective**. The factorization implies that if two datasets share the same correct-wrong statistics (i.e., an identical count of correct and wrong predictions), they should exhibit similar temperatures after NLL optimization. In the context of PseudoCal, our objective is to synthesize a labeled set that mirrors the correct-wrong statistics of real target samples. We choose to **employ mixup** for sample synthesis due to its simplicity and its **ability to elegantly leverage the target-domain data structure learned by the UDA model**. Additionally, **leveraging the well-established cluster assumption**, we anticipate a sample-level correspondence between mixed samples and real target samples, facilitated by suitable values of the mix ratio $\\lambda$. Accurate sample-level correspondences ensure similar correct-wrong statistics, ultimately guaranteeing that the temperature obtained with the labeled pseudo-target set closely approximates the Oracle target-domain temperature.\n\n> **Q2**: It is well known that a mixup training procedure yields more calibrated models. However, it is unclear from the paper why learning Temperature Scaling using a convex combination of pseudo-labeled samples yields meaningful temperature values.\n\n**A2**: We recognize the effectiveness of mixup training in enhancing model calibration performance. However, in the context of UDA, we observe that **this effectiveness falls short of expectations**, as illustrated by the calibration results of **DINE in Table 6**. DINE, a source-free UDA method **utilizing training-stage mixup** with unlabeled target data, exhibits **substantial calibration errors** in the trained model. In contrast, our **PseudoCal significantly reduces ECE errors for DINE models**, as evidenced by the results presented in Table 6.\n\nIn **explaining the efficacy of PseudoCal**, it is crucial to highlight the importance of **ensuring similar correct-wrong statistics between the mixed samples and real target samples**. This is a key factor because the NLL objective employed by TempScal is directly influenced by both prediction and label information. The factorization of the NLL implies that correct and wrong predictions (evaluated by labels) have contrasting effects on the NLL optimization process. When there are similar correct-wrong statistics, the NLL optimization employed in TempScal becomes alike, leading to similar temperatures. Note that **for mixed samples**, correctness is assessed based on the predictions of the mixed samples and their corresponding mixed labels. Conversely, **for real target samples**, correctness is evaluated using the target predictions and unavailable target ground truths. **Detailed evidence supporting the similarity** in correct-wrong statistics is provided through **visualizations in Figure 4 in the Appendix** and **quantitative analysis results in Table 10** in the Appendix.\n\n> **Q3**: How is this method derived from the UDA situation? The success of a UDA method in different domain shift situations depends on the size of the gap between the domains. I would expect that the UDA calibration would be dependent on how well the UDA method manages to close the domain gap. I don't see how the proposed method handles this aspect of the UDA calibration problem.\n\n**A3**: **One novelty** of our PseudoCal approach lies in adopting a **target-domain perspective** to address the calibration problem in UDA, distinguishing it from existing methods that primarily rely on source data to address the covariable shift challenge. It's essential to note that **in the UDA field, various studies, including many in the recently popular source-free UDA settings, also operate without source data.** \nSimilar to previous target-specific works, PseudoCal **relies on a well-trained UDA model that effectively captures the target-domain data structure.** The absence of source data is compensated by leveraging the knowledge embedded in the UDA model. In situations where the UDA model struggles to learn the target-domain structure, the cluster assumption may not hold, potentially damaging the sample-level correspondence between mixed samples and real target samples. This aspect is evident in **Table 6, where, in the I$\\to$S task, both SHOT and DINE models exhibit low accuracy, resulting in high calibration errors** even after being calibrated with PseudoCal. This underscores the **importance of a well-trained UDA model in ensuring the effectiveness of PseudoCal**."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission516/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700546491836,
                "cdate": 1700546491836,
                "tmdate": 1700546491836,
                "mdate": 1700546491836,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "B7FEX7cRrW",
            "forum": "USWkUOfxOO",
            "replyto": "USWkUOfxOO",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission516/Reviewer_SpZE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission516/Reviewer_SpZE"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a post-hoc calibration method for unsupervised domain adaptation. It utilizes the typical temperature scaling method and adapts it to UDA by generating pseudo-labels on target set, turning the unsupervised setting into a supervised one. Experiments on multiple datasets demonstrate the effectiveness of the method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper gives the interesting observation about NLL minimization: Temperature scaling provides similar temperatures if two datasets have similar correct-wrong patterns.\n2. Experiments are well-done and shows SOTA performance."
                },
                "weaknesses": {
                    "value": "My concerns on this paper mainly concentrates on the proposed method, which I did not fully understand given the limited justifications.\n1. Given the first point mentioned in \"Strengths\", the paper only claimed it as an observation but without further explaination.\n2. The method of using Mixup for pseudo-label generation is not convincing to me, even given the \"Analysis\" section. (a) While the ground-truth labels are not available, can I just assign random labels for each target sample and then adopt PseudoCal (i.e. How is the UDA inference process helpful)? (b) Since $\\lambda$ is a fixed scalar and we do not know whether the target sample prediction result is true or not, what is the point of the last 8 lines in \"Analysis\" part. (c) The pseudo code in Appendix A does not do what equation (3) says for $y_{pt}$. Also, since $\\lambda$ is a fixed scalar in (0.5, 1.0), what does the \"if else\" statement for pseudo_target_y do as \"else\" is never visited?\n3. What is the validation set adopted for Temperature scaling in the experiments?\n4. Table 2~5 shows a phenomenon: Temperature scaling sometimes achieves the best performance. What might be the reason?\n5. The style of figure 2 is new to me. However, not good-looking nor clear."
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission516/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission516/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission516/Reviewer_SpZE"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission516/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697837286099,
            "cdate": 1697837286099,
            "tmdate": 1700587548684,
            "mdate": 1700587548684,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "MGQDUG8TL3",
                "forum": "USWkUOfxOO",
                "replyto": "B7FEX7cRrW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission516/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission516/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer SpZE - Part 1"
                    },
                    "comment": {
                        "value": "> **Q1**: Given the first point mentioned in \"Strengths\", the paper only claimed it as an observation but without further explanation.\n\n**A1**: The observation stems directly from the inherent characteristics of the factorized NLL objective. As clarified in **Section 3.1, the optimization process for correct and wrong predictions in the factorized NLL objective inherently seeks contrasting temperature values.** Specifically, **correct predictions aim for a small temperature to sharpen** the softmax probability distribution, while **wrong predictions aim for a large temperature to flatten** the softmax probability distribution. Consequently, when dividing the confidence ranges of (0.0, 1.0) into multiple small bins and ensuring two datasets share the **same correct-wrong statistics (i.e., an identical count of correct or wrong predictions for each confidence bin)**, a highly consistent NLL optimization process is expected. This consistency results in obtaining similar temperatures after the optimization process. We hope this clarification provides a better understanding of the rationale behind this observation.\n\n> **Q2**: (a) While the ground-truth labels are not available, can I just assign random labels for each target sample and then adopt PseudoCal (i.e. How is the UDA inference process helpful)?\n\n**A2**: PseudoCal is a novel and general framework designed to leverage a synthesized labeled pseudo-target set for optimizing TempScal and obtaining an accurate estimation of the Oracle target-domain temperature. The crucial factor ensuring the success of PseudoCal lies in having this pseudo-target set exhibit similar correct-wrong statistics to the real target samples. **However, we cannot know the actual correct-wrong statistics of the real target samples due to the unavailability of target-domain ground-truth labels.** \n\nReturning to the initial question, while it\u2019s feasible to use randomly assigned labels for calibration with target samples, **there is no assurance of favorable calibration performance because the resulting correct-wrong statistics would be entirely random**. This is demonstrated in **Table 9 under \"Pseudo-Label,\"** where we employ **hard one-hot target model predictions as labels** for real target samples and apply TempScal, resulting in **substantial calibration errors**.\n\nThe effectiveness of PseudoCal with inference-stage mixup relies on establishing a sample-level correspondence between the pseudo-target set and the real target samples, partially guaranteed by the cluster assumption. **The UDA inference process is to obtain predictions** for mixed samples, and **correctness is evaluated** based on these predictions and the corresponding mixed labels. This inference process is critical for **leveraging the cluster assumption** and building the required sample-level correspondence. We have provided comprehensive evidence to illustrate this, including detailed illustrations in **Figure 4 in the Appendix** and extensive quantitative statistics in **Table 10 in the Appendix**. \n\n> **Q3**: (b) Since $\\lambda$ is a fixed scalar and we do not know whether the target sample prediction result is true or not, what is the point of the last 8 lines in \"Analysis\" part.\n\n**A3**: The \"Analysis\" part in Section 3.2 aims to provide **an intuitive analysis and explanation of how mixed samples can exhibit similar correct-wrong statistics as real target data**. This is crucial for ensuring that applying TempScal with these mixed samples can yield a temperature similar to the Oracle target temperature. In the **revised paper**, we have enhanced the presentation for better clarity, incorporating **more illustrations** of correct-wrong statistics, as demonstrated in **Figure 1(b) and Figure 4 in the Appendix**. \n\nIt's essential to clarify that we **analyze the dataset-level similar correct-wrong statistics through the sample-level correspondence of correct or wrong predictions. The definition of correctness differs between mixed samples and real samples.** For mixed samples, correctness is assessed based on the predictions of the mixed samples and their corresponding mixed labels. Conversely, for real target samples, correctness is evaluated using the target predictions and unavailable target ground truths. PseudoCal never accesses target ground truths and relies solely on target images and their model predictions to synthesize a labeled pseudo-target set, comprising mixed samples and mixed labels. **For a quantitative analysis of the sample-level correspondence, we kindly refer the reviewer to Appendix D.**\n\nThe use of **target ground truths serves exclusively for analysis and explanation purposes** to study the success of PseudoCal. This involves explaining and showcasing similar correct-wrong statistics between the pseudo-target set and real target samples, as discussed in both the Analysis part in Section 3.2 and Appendix D."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission516/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700532374726,
                "cdate": 1700532374726,
                "tmdate": 1700532374726,
                "mdate": 1700532374726,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wKACkL40l7",
                "forum": "USWkUOfxOO",
                "replyto": "Rrqb0hIelE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission516/Reviewer_SpZE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission516/Reviewer_SpZE"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response"
                    },
                    "comment": {
                        "value": "The authors have fully addressed most of my concerns and revised the paper to a large degree. I am willing to raise my score given the effort the authors have made. \n\nFor Figure 2, it is just not academia enough to me... However, it is only my personal preference and I won't treat it as a weakness if the other reviewers are OK with it. This is also my attitude towards the paper, as I won't stand in the way if all other reviewers agree to accept it."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission516/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700587508990,
                "cdate": 1700587508990,
                "tmdate": 1700587508990,
                "mdate": 1700587508990,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "SEbsY4N4Qh",
            "forum": "USWkUOfxOO",
            "replyto": "USWkUOfxOO",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission516/Reviewer_dbpq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission516/Reviewer_dbpq"
            ],
            "content": {
                "summary": {
                    "value": "In this work, the authors propose PseudoCal, which applies a mix-up strategy to generate pseudo-labels for unlabelled data to predict calibration uncertainty in an unsupervised domain adaptation problem. The proposed approach is examined on multiple image classification and semantic segmentation benchmarks, showing its superiority over SOTA."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ The paper is written well, and the idea is easy to follow.\n+ The experiments cover extensive benchmarks of multiple settings, e.g closed-set, partial-set and Source-free,  and show marginal performance boosing w.r.t ECE. \n+ A simple and clear pseudo code is provided in the appendix."
                },
                "weaknesses": {
                    "value": "- The novelty is weak because mix-up is a well-known strategy in image classification tasks. From my perspective, the methodology is the same as the previous ICLR 2018 work, \"mixup: BEYOND EMPIRICAL RISK MINIMIZATION\". \n- The mix-up is conducted at input level, which would introduce artifacts. It would be good to include some visualization in the appendix together with quantitative results. \n- I am skeptical about some numbers in the table. When lamba = 0 or 1, PseudoCal becomes typical TempScal with pure negative or positive samples. However, the ECEs shown in the tables are different. For example, in Home dataset, TempScal gets 11.06 but PseudoCal with lamba = 1 achieves ~20.\n- How did the author get the results of other methods, cite from the original papers, or re-implement those approaches?"
                },
                "questions": {
                    "value": "Please see the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission516/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698724366060,
            "cdate": 1698724366060,
            "tmdate": 1699635978372,
            "mdate": 1699635978372,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PG0qWVi26s",
                "forum": "USWkUOfxOO",
                "replyto": "SEbsY4N4Qh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission516/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission516/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer dbpq - Part 1"
                    },
                    "comment": {
                        "value": "> **Q1**: The novelty is weak because mix-up is a well-known strategy in image classification tasks. From my perspective, the methodology is the same as the previous ICLR 2018 work, \"mixup: BEYOND EMPIRICAL RISK MINIMIZATION\".\n\n**A1**: We appreciate the reviewer's comments and **respectfully disagree** with the assertion that **\u201cthe methodology is the same as the previous ICLR 2018 work [1]\u201d.** \n\n1) While acknowledging the foundational work of [1] and its impact on the field, we **highlight distinct differences** in how mixup is employed in our work, as **discussed in Section 4.3 \"Comparison with training-stage mixup\"** and summarized below:\n\n- **Different application stage**: Unlike existing mixup works [1, 2], which leverage mixup during the training stage for model training, **our PseudoCal** approach employs mixup during the **inference stage**, specifically for UDA model calibration, **without altering model parameters**.\n\n- **Different mix ratio ($\\lambda$)**: In prior works **[1, 2], effective** mixup commonly utilizes **$\\lambda$ values near 1**, typically **randomly sampled** from a beta-distribution, i.e., $\\lambda \\sim Beta(\\alpha, \\alpha)$, where $\\alpha \\in [0.1, 0.4]$. Notably, $\\lambda$ values significantly **deviating from 1.0** are considered **ineffective due to the manifold intrusion problem [2, 3]**. On the contrary, **PseudoCal** exhibits **optimal** performance with **fixed $\\lambda$** values significantly **deviating from 1.0**. However, employing **$\\lambda=1.0$** in PseudoCal (i.e., PseudoCal($\\lambda=1.0$)) leads to **significant calibration errors**.\n\n- **Different calibration performance**: We present compelling evidence from two perspectives. **First**, we examine **DINE [4]**, a source-free domain adaptation method **employing the training-stage mixup** strategy [1, 2] during model training. As outlined in **Table 6**, our observations indicate that UDA models trained with DINE still manifest substantial calibration errors. In contrast, our **PseudoCal effectively calibrates the DINE model**, leading to notable reductions in calibration errors. Specifically, the ECE (%) error drops from 21.81 to 12.82 on DomainNet and from 58.85 to 47.76 on ImageNet. Furthermore, when we **substitute our mixup strategy with the training-stage mixup [1, 2] in our PseudoCal framework**, denoting this variant as **\"Mixup-Beta\" in Table 9**, we observe the superior performance of our mixup strategy. These significant performance differences underscore the distinct nature and advantage of our mixup strategy.\n\n2) Moreover, it is crucial to highlight that our usage of mixup serves as only one technical facet of our framework; **the novelty of our paper extends far beyond this component**, as summarized below.\n\n- **Novel target-domain perspective**: We approach the challenging calibration problem in UDA as **an unsupervised calibration issue in the target domain**, **diverging from the previous** treatment as a **cross-domain covariate shift** problem seen in competing approaches listed in Table 1 of our submission.\n\n- **Novel TempScal objective factorization**: We introduce a **unique factorization of the negative log-likelihood (NLL)** objective used in TempScal, a distinctive feature not identified in relevant UDA model calibration studies compared in Table 1.\n\n- **Novel PseudoCal framework**: Our innovative post-hoc framework, PseudoCal, addresses model calibration in domain adaptation through a novel strategy for synthesizing pseudo-target samples. This sets it apart from competing methods, as thoroughly demonstrated in **Table 1 of our submission**.\n\n- **Comprehensive empirical study**: Our extensive experiments with PseudoCal encompass 10 domain adaptation methods across 5 UDA scenarios. **Notably, we include the calibration under source-free UDA settings and domain-adaptive semantic segmentation for the first time**. PseudoCal consistently exhibits significant outperformance over all competing methods.\n\n>**Q2**: The mix-up is conducted at input level, which would introduce artifacts. It would be good to include some visualization in the appendix together with quantitative results.\n\n**A2**: Thank you for your valuable suggestion. We have **added visualizations** of input-level mixup for all UDA benchmarks in **Figure 5 of the Appendix** for a comprehensive understanding."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission516/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700520262735,
                "cdate": 1700520262735,
                "tmdate": 1700520262735,
                "mdate": 1700520262735,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "yO90B5485K",
            "forum": "USWkUOfxOO",
            "replyto": "USWkUOfxOO",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission516/Reviewer_fgV1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission516/Reviewer_fgV1"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the unsupervised domain adaptation from the aspect of target-domain specific unsupervised. Specifically, the PseudoCal framework is proposed. In the inference stage, this method uses a mixup strategy to generate a pseudo-target set and perform supervised calibration on it. To clarify the effectiveness of the PseudoCal, the authors conduct analysis through the cluster assumption. The experiments show that the proposed method outperforms other state-of-the-art baselines on several datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tThe paper presents a new perspective to make up the UDA uncertainty calibration research and unifies the covariate shift and label shift scenarios in UDA.\n2.\tThe proposed method uses the generated pseudo-target set to convert the unsupervised problem into a supervised one.\n3.\tGood set of experiments for yielding state-of-the-art results on several datasets."
                },
                "weaknesses": {
                    "value": "1.\tThe analysis of the mixup in the inference strategy is not sufficient. The comparison between the training stage mixup strategy in other methods and the inference stage is not obvious, the authors should strengthen your highlight.\n2.\tWriting can be improved. In addition, the experiment results could present more in the \u201cAccuracy\u201d metric, which is also intuitive."
                },
                "questions": {
                    "value": "How many pseudo-target samples will be generated? Does the number of n_{pt} be the same with target sample number?\n\tTo find the value of mix ratio \\lambda, the authors analyze the ECE on different UDA scenarios. However, it seems that the black-box source-free scenario does not have a corresponding analysis. \n\tThe mixup strategy and the temperature scaling both are calibration methods. What are the results if the pseudo target set is just generated by the model predicting, not using the mixup strategy?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission516/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698936588420,
            "cdate": 1698936588420,
            "tmdate": 1699635978300,
            "mdate": 1699635978300,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CGnEuyJJxJ",
                "forum": "USWkUOfxOO",
                "replyto": "yO90B5485K",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission516/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission516/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer fgV1 - Part 1"
                    },
                    "comment": {
                        "value": "> **Q1**: The analysis of the mixup in the inference strategy is not sufficient. The comparison between the training stage mixup strategy in other methods and the inference stage is not obvious, the authors should strengthen your highlight.\n\n**A1**: In Section 4.3, we delve into a detailed analysis of the mixup operation's impact on the calibration performance of our PseudoCal. **Two key implementation-level factors are considered**: the use of hard one-hot labels or soft predictions for the mixup process in Equation 3, and the value of the mix ratio $\\lambda$. Empirical results across various UDA methods and settings are presented in **Figure 3(c)-(d)**. Specifically, we highlight the crucial role of $\\lambda$ values, **emphasizing in the main text that \"a $\\lambda$ value closer to 0.5 generates more ambiguous samples, resulting in increased wrong predictions, whereas a $\\lambda$ value closer to 1.0 has the opposite effect.\" For readers interested in detailed quantitative analysis, we provide a comprehensive discussion in Appendix D.**\n\nIn comparing our approach to mainstream training-stage mixup [1,2], our goal is to **highlight the novel aspects of our mixup strategy**. We emphasize three key novelties. **Firstly**, we employ **mixup during the inference stage** rather than the traditional training stage. **Secondly**, our mixup strategy exhibits effectiveness with **a fixed $\\lambda$ value significantly deviating from 1.0**, a departure from the prevalent use of values close to 1.0 in existing mixup approaches. **Thirdly**, applying training-stage mixup directly to train a UDA model, as seen in UDA methods like **DINE, still results in substantial calibration errors.** Conversely, our PseudoCal can significantly reduce ECE errors for such models, as evidenced by the DINE results presented in **Table 6**. Additionally, in the ablation study of PseudoCal, where we **replace our $\\lambda$ usage** with values generated through the beta-distribution employed in existing mixup, the results under **\"Mixup-Beta\" in Table 9** show that our mixup strategy outperforms traditional mixup.\n\n\n> **Q2**: Writing can be improved. In addition, the experiment results could present more in the \u201cAccuracy\u201d metric, which is also intuitive.\n\n**A2**: Thank you for your valuable feedback on writing. We have revised the manuscript to enhance clarity and provide more detailed results. Regarding the \"Accuracy\" metric, we have clarified in the **first paragraph of Section 4.2** that, in all tables, \"\u2018Accuracy\u2019 (%) denotes the target accuracy of the **fixed UDA model**.\" This addition only serves to offer readers a clear understanding of the UDA model's discriminative ability in the target domain. **As indicated in Table 1 and Figure 2, our PseudoCal** method is a **post-hoc** calibration approach that solely **utilizes the UDA model for inference without changing \"Accuracy\" of the model**. It's worth noting that many existing calibration methods, such as **TempScal, CPCS, and TransCal**, are **also post-hoc** in nature.\n\n> **Q3**: How many pseudo-target samples will be generated? Does the number of $n_{pt}$ be the same with target sample number?\n\n**A3**: Yes, for all experiments, the number of $n_{pt}$ is almost the same as the target sample number $n_t$. Specifically, in the **Implementation details**, we mentioned that \u201cWe use the UDA model for **one-epoch inference with mixup to generate the labeled pseudo-target set**.\u201d Additinoally, as indicted in our pseudocode in **Appendix A**, during the inference process, we **only mix real target samples that have different pseudo labels**, i.e., cross-cluster perturbation. Therefore, in general, **$n_{pt}$ would be slightly smaller than $n_t$** due to **ignoring the pairs of the shuffled real target samples that have the same pseudo label**.\n\n> **Q4**: To find the value of mix ratio \\lambda, the authors analyze the ECE on different UDA scenarios. However, it seems that the black-box source-free scenario does not have a corresponding analysis.\n\n**A4**: We appreciate the thorough examination of our ablation experiments in Table 9 and analysis experiments in Table 10 (Appendix). Both tables maintain a consistent setting for UDA methods. In **our initial submission**, we presented the black-box source-free method **DINE** and the white-box source-free method **SHOT under the same parent UDA setting, i.e., source-free UDA**. Consequently, we only reported SHOT ablation results. **Following the valuable suggestion, we have included the results of DINE in both Table 9 and Table 10** to facilitate comprehensive comparisons **without altering our initial conclusions**."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission516/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700519379015,
                "cdate": 1700519379015,
                "tmdate": 1700519379015,
                "mdate": 1700519379015,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]