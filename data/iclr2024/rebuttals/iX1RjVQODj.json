[
    {
        "title": "Contrastive Preference Learning: Learning from Human Feedback without Reinforcement Learning"
    },
    {
        "review": {
            "id": "Io7tbPciJT",
            "forum": "iX1RjVQODj",
            "replyto": "iX1RjVQODj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6593/Reviewer_WpDS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6593/Reviewer_WpDS"
            ],
            "content": {
                "summary": {
                    "value": "This work introduces Contrastive Preference Learning (CPL), a novel algorithm designed for learning optimal policies from preferences, eliminating the need to learn reward functions. CPL integrates the regret-based preference framework with the principle of Maximum Entropy, establishing a one-to-one correspondence between advantage functions and policies.\nThe experimental results highlight CPL's superior performance compared to SFT and offline Reinforcement Learning (P-IQL)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The motivation is evidently well-defined.\n2. It adeptly combines theoretical analysis with empirical findings.\n3. The proposed method is written in a clear and easily understandable manner."
                },
                "weaknesses": {
                    "value": "This article exclusively compares CQL with offline RL, but to my knowledge, the majority of RLHF (Reinforcement Learning from Human Feedback) algorithms employ **online** RL algorithms [1]. There appears to be a fundamental distinction between these two training paradigms. Offline algorithms exclusively train the model on static datasets, whereas online algorithms train the model on the trajectories gathered by the training policies. \n\nI strongly encourage the authors to include a baseline that trains the reward model using the dataset and subsequently employs an **online** training methodology, such as PPO. This addition is crucial to substantiate the authors' claims.\n\n\n[1] Training language models to follow instructions with human feedback."
                },
                "questions": {
                    "value": "What if you were to employ an online RL algorithm for the reinforcement learning experiment instead of an offline one?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6593/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6593/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6593/Reviewer_WpDS"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6593/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698583268069,
            "cdate": 1698583268069,
            "tmdate": 1700618001194,
            "mdate": 1700618001194,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bKGUedXTy2",
                "forum": "iX1RjVQODj",
                "replyto": "Io7tbPciJT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6593/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6593/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer WpDS -- We have added a PPO baseline."
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for taking the time to read our work, and were pleased to hear that it \u201cadeptly combines theoretical analysis with empirical findings\u201d and is \u201cclear and easily understandable\u201d.\n\nThe reviewer states a single weakness of our work \u2013 that we do not compare to online algorithms like PPO with a learned reward model. We have run this baseline, but would like to additionally discuss our choice for not including it originally below.\n\n1. **In sequential settings, online methods like PPO require online access to the MDP, which can be impossible**. Consider a multi-step version of the dialogue domain brought up and cited by the reviewer. At a minimum, sequential dialog data would be a sequence of at least two steps $s_1, a_1, s_2, a_2$ where $s$\u2019s are the prompts and $a$\u2019s are the LLMs completions. While the first prompt $s_1$ could be sampled from a fixed dataset, and the first action $a_1$ could be sampled from the LLM, the second prompt $s_2$ would need to be provided by a user as it depends on $a_1$! As PPO relies on the on-policy policy gradient, we would need humans to interact with the language model `batch_size` number of times for *every* gradient step in the sequential setting. Thus, using online methods like PPO in *sequential* domains can be infeasible. We would like to point out that the paper and RLHF domain the reviewer mentions is a contextual bandits setting (one step) and not a true sequential problem. In Appendix A we show that in this bandits setting CPL reduces to DPO, which can also outperform PPO on LLM training [1]. \n\n2. **Online methods like PPO assume the cost of data collection is essentially zero.** PPO requires rolling out the policy to estimate the gradient. In many applications, like robotics, this is very expensive and time consuming, making learning from offline data a way more attractive alternative. In fact, in that community, the offline learning approach has largely been adopted. \n\n3. Many contemporary works on RLHF for robotics or control use only offline data [2,3,4].\n\nFinally, we ran a PPO baseline with a KL-penalty on the reward as commonly used in RLHF [5]. Before discussing results we would like to note that this comparison is unfair to CPL as:\n\n1. The PPO baseline has access to online samples from the MDP.\n\n\n2. The PPO baseline uses 25x the number of state-action pairs as CPL for 2.5K Dense and 4x the amount for 20K sparse. This is because PPO uses 3.84 million additional online transitions plus the original 160,000 for CPL Dense and 1.28 million for CPL Sparse).\n\n\n3. PPO required far more hyper-parameter tuning. We had to bound the learned rewards, bound the policy standard deviation, and aggressively tune the KL-divergence.\n\nDespite all of this, we found PPO to be extremely unstable and often underperformed CPL in 4 of the environments. Note that we verified the performance of our PPO implementation on standard Benchmarks (Hopper-v2, exceeds performance [here](https://spinningup.openai.com/en/latest/spinningup/bench.html)). Our full results are in Table 1 for two different values of the reward KL-penalty, 2 and 5, and learning curves for PPO are in Appendix C. We have copied them here for convenience:\n\n*2.5K Dense Results*\n| | Bin Picking | Button Press | Door Open | Drawer Open | Plate Slide | Sweep Into |\n|---------|-----------------|-----------------|-----------------|-----------------|-----------------|------------------|\n| PPO KL2 | 83.7 $\\pm$ 3.7 | 22.7 $\\pm$ 1.9 | 79.3 $\\pm$ 1.2 | 66.7 $\\pm$ 8.2 | 51.5 $\\pm$ 3.9 | 55.3 $\\pm$ 6.0 |\n| PPO KL5 | 83.2 $\\pm$ 2.4 | 18.7 $\\pm$ 3.1 | 68.8 $\\pm$ 3.0 | 59.0 $\\pm$ 2.5 | 43.0 $\\pm$ 2.2 | 50.7 $\\pm$ 15.4 |\n| CPL | 80.0 $\\pm$ 2.5 | 24.5 $\\pm$ 2.1 | 80.0 $\\pm$ 6.8 | 83.6 $\\pm$ 1.6 | 61.1 $\\pm$ 3.0 | 70.4 $\\pm$ 3.0 |\n\n*20K Sparse Results*\n\n|         | Bin Picking     | Button Press    | Door Open       | Drawer Open     | Plate Slide     | Sweep Into      |\n|---------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|\n| PPO KL2 | 68.0 $\\pm$ 4.3 | 24.5 $\\pm$ 0.8 | 82.8 $\\pm$ 1.6 | 63.2 $\\pm$ 6.6 | 60.7 $\\pm$ 4.2 | 58.2 $\\pm$ 0.6 |\n| PPO KL5 | 71.1 $\\pm$ 6.7 | 23.0 $\\pm$ 1.8 | 71.3 $\\pm$ 2.1 | 64.7 $\\pm$ 7.7 | 48.0 $\\pm$ 3.9 | 53.7 $\\pm$ 1.7 |\n| CPL     | 83.2 $\\pm$ 3.5 | 29.8 $\\pm$ 1.8 | 77.9 $\\pm$ 9.3 | 79.1 $\\pm$ 5.0 | 56.4 $\\pm$ 3.9 | 81.2 $\\pm$ 1.6 |\n\nPPO faces a number of challenges in the sequential domain: the variance of policy gradients and value estimation increases with horizon, and instability of the KL-divergence penalty for continuous policies. Again, in 4/6 environments CPL attains better performance than PPO using 1/4 of the data.\n\nAs we have addressed the reviewers' only stated concern with the draft through this experiment, we would kindly request that the reviewer reconsider their score.\n\n[1] Rafailov, Sharma, Mitchell, Direct Preference Optimization. NeurIPS 2023\n\n[2] Kim et al. Preference Transformer. ICLR 2023."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6593/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700243527334,
                "cdate": 1700243527334,
                "tmdate": 1700423542698,
                "mdate": 1700423542698,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "i8m02XWoZO",
                "forum": "iX1RjVQODj",
                "replyto": "Io7tbPciJT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6593/Reviewer_WpDS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6593/Reviewer_WpDS"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your response!\n\nI am very grateful to the authors for conducting experiments on online reinforcement learning.\n\nI requested such experiments because the authors extensively emphasize the advantages of CPL for RLHF. Without clarifying that the entire setting of the article is offline, I believe an online baseline is necessary.\n\nFor the experiments of PPO, it seems PPO-KL2 outperforms PPO-KL5 in almost all experiments. I understand that the authors may have time constraints to tune the hyper-parameters, and I believe that online algorithms also have great potential.\n\nOverall, the author did address my concern, and I would like to increase my score."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6593/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700617949021,
                "cdate": 1700617949021,
                "tmdate": 1700617978663,
                "mdate": 1700617978663,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "47eN47dQCf",
            "forum": "iX1RjVQODj",
            "replyto": "iX1RjVQODj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6593/Reviewer_moLC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6593/Reviewer_moLC"
            ],
            "content": {
                "summary": {
                    "value": "The authors address critical aspects of the PBRL framework, with a specific emphasis on the optimization challenges in the RL phase. To solve this problem, the authors introduce a novel approach called Contrative Preference Learning (CPL). This method leverages a regret-based model of human preferences, from which a contrastive objective is derived with the principle of maximum entropy. This approach bypasses the need for reward learning and RL, instead directly learn the policy through a supervised learning paradigm. To evaluate the effectiveness of CPL, the authors conducted experiments within the offline PbRL setting, comparing it against strong baselines in terms of the success rate across distinct tasks in the Metaworld domain. The experimental results show that CPL outperforms baselines with less runtime and smaller model size. The primary contribution of this work lies in the conversion of the traditional two-phase PbRL framework into a novel paradigm capable of directly learning the policy with a new contrastive objective."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "This work focuses on addressing critical challenges in PbRL. It is well-motivated and accompanied by a clear and thorough discussion of existing issues within both the reward learning and RL phases.\n\nThe proposed CPL bypasses the need for reward learning and RL by optimizing a supervised objective, enabling it to learn policy from offline high-dimensional suboptimal data. Moreover, it can be applied to arbitrary MDPs. I feel this approach can be seen as a counterpart to DPO, as discussed by the authors in the paper\u2014one for NLP tasks with LLMs and the other for continuous control tasks. This work has the potential to make a significant impact in the community, and I am eager to see how CPL performs in broader applications.\n\nGenerally, the organization and presentation of the content are well-structured, facilitating ease of reading and comprehension. The authors provide comprehensive theoretical proofs that make the work sound. The experimental results are impressive in terms of runtime, model size, and performance. In the limitation section, I appreciate the authors acknowledge the imperfections of the human model and raise considerations regarding the application of this approach to online human feedback."
                },
                "weaknesses": {
                    "value": "Please see Questions."
                },
                "questions": {
                    "value": "1. I still have questions regarding regret-based preference model. I agree with the authors that the regret-based preference model makes more sense when we consider the hand-engineered example in section 2. However, when we talk about data collection with a real human, the human labeler would have a preference over two trajectory segments. This implies the existence of underlying dense rewards that explain the human's preferences. In such cases, I feel that the key issue lies in the hand-engineered reward is incorrect (i.e., reward design issue) in your example, rather than in the issues of the reward-based preference model.\n\nTherefore, when we consider experiments with real humans and apply the reward-based preference model, could it also perform effectively? Is it possible that the learned reward captures the regret information to a large extent? Please correct me if I have misunderstood.\n\n2. Despite considering the model complexity of CPL, the results are promising. In terms of feedback efficiency, does CPL require more human preference data compared to the conventional two-phase PbRL framework in order to perform well? This is especially relevant considering the Metaworld tasks in the experiments, where obtaining dense data could be challenging if collected from real humans.\n\n3. In the experiments, the authors pretrain the model with behavior cloning. To what extent does this pretraining phase impact the model's final performance? Does P-IQL also have this pretraining phase?\n\n4. Similar to DPO, CPL employs a supervised learning framework without reward learning and RL. Does it potentially lose the generalization power of RL?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6593/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6593/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6593/Reviewer_moLC"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6593/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698806633003,
            "cdate": 1698806633003,
            "tmdate": 1699636749859,
            "mdate": 1699636749859,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kfoPjJyvSt",
                "forum": "iX1RjVQODj",
                "replyto": "47eN47dQCf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6593/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6593/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer moLC (1/2)"
                    },
                    "comment": {
                        "value": "We were excited to hear that the reviewer found our work to be \u201cwell-motivated\u201d, \u201cclear\u201d, and that it \u201chas the potential to make a significant impact in the community\u201d. The reviewer's comments were largely based on a set of additional questions we seek to answer here.\n\n**The Regret Preference Model**\n\nThe regret preference model states that human *judgements* are better captured by an optimal advantage function, and does not make any claims about what underlying reward functions humans use. Going back to the example in Section 2, we did not mean to claim that the human would intend to use a sparse reward function \u2013 a user providing preferences preferring movements towards a goal could have a number of different underlying reward functions, many of which could yield the same optimal policy and optimal advantage function. \n\nThough the focus of our paper is not the validity of the regret preference model, for which we would refer the reader to [1], we do provide a number of additional insights on the regret preference model.\n\n1. *Theoretical*: In Appendix A, we prove that the optimal advantage is just a shaped version of the reward function that results in the same optimal policy. The reviewer suggests that humans use dense rewards to make decisions, and Corollary 1 shows exactly that the advantage is a dense reward function.\n\n2. *Empirical*: In the new Section 4.4, we show that CPL works on real human data, indicating that the regret-model holds on real human data. \n\nAgain we would encourage the reader to examine [1], as it contains a number of additional examples (like stochastic environments) and experimental results which support the validity of the regret-preference model. \n\n**Does CPL require more human preference data in comparison to two-phase PBRL**\n\nTo demonstrate that CPL is effective with limited real-human preference data we perform additional evaluations on the PBRL benchmarks from [2]. These benchmarks use only 100 or 500 real human preferences for expert or replay datasets from D4RL respectively. \n\nWe showed in Section 4.3 that CPL performs better with dense preference data. To overcome this, we use a logistic regression model to predict which of two segments a user prefers from the preference data, and use it to relabel a larger offline RL dataset. Critically, this logistic regression model is not a reward or advantage model, as it predicts a single value for the *entire* segment.\n\nWe find that CPL performs similarly to baselines on 2/4 datasets, outperforms in 1, and performs worse in 1. Yet, CPL remains far simpler.\n\n\n**Pretraining**\n\n\nWe have included results on Metaworld in state without any pre-training, and find that CPL performs similarly \u2013 no pretraining performs better in some cases and pretraining performs better in others. This ablation has been included in Appendix C and is copied below as well.\n\n|                   | Bin Picking     | Button Press    | Door Open       | Drawer Open     | Plate Slide     | Sweep Into      |\n|-------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|\n| CPL               | 80.0 $\\pm$ 2.5 | 24.5 $\\pm$ 2.1 | 80.0 $\\pm$ 6.8 | 83.6 $\\pm$ 1.6 | 61.1 $\\pm$ 3.0 | 70.4 $\\pm$ 3.0 |\n| CPL (No Pretrain) | 81.0 $\\pm$ 1.5 | 30.0 $\\pm$ 2.7 | 76.8 $\\pm$ 2.1 | 88.7 $\\pm$ 1.9 | 50.8 $\\pm$ 3.5 | 80.0 $\\pm$ 2.5 |\n\n\nThis ablation shows that the use of the conservative regularizer in CPL can help it learn effectively even without pre-training. Note that our results on D4RL (Table 3) also do not use any pre-training.  \n\n\nWe did not use pre training for P-IQL for two reasons:\n\n\n1. P-IQL uses a weighted BC objective ($\\min \\mathbb{E} [ e^{Q-V} \\log \\pi]$ and thus already has a loss function very similar to BC. \n2. In IQL value learning is completely detached from policy learning, and thus pretraining $\\pi$ does not improve estimates of $Q$ and $V$. \n\n**Does CPL lose the generalization power of RL**\n\nWe are not 100% certain what the reviewer is referring to by the \u201cgeneralization power of RL\u201d. \n\nTo our best understanding, there are two types of generalization potentially at play in DeepRL: 1) the generalization of neural networks and 2) RLs ability to use the ground-truth reward function to extrapolate in new parts of the state space. For 1), CPL still has the generalization power of neural networks. For 2), we would like to point out that this does not necessarily apply in RLHF, as the accuracy of the reward model is dependent on the preference data. Thus, RLHF methods that use an explicit reward model at the end of the day, still rely on the generalization of supervised reward learning. CPL is just cutting out the middleman.\n\nThis approach has already shown to be effective for LLMs [3]. By removing the need for extra components, we believe CPL will help scale to larger datasets/models where better generalization is typically found. \n\ncontinued...\n\n[1] Knox et al.  https://arxiv.org/abs/2206.02231"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6593/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700243462311,
                "cdate": 1700243462311,
                "tmdate": 1700243462311,
                "mdate": 1700243462311,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mR45pTbNY4",
                "forum": "iX1RjVQODj",
                "replyto": "qR1NSQEX2B",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6593/Reviewer_moLC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6593/Reviewer_moLC"
                ],
                "content": {
                    "title": {
                        "value": "Post Rebuttal Comments"
                    },
                    "comment": {
                        "value": "Thanks for the authors' responses, and I appreciate the inclusion of the additional experiments, which have addressed my concerns and clarified a few questions. After carefully reviewing both the authors' responses and other reviewers' comments, I am inclined to maintain my score and recommend accepting the paper."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6593/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700291750781,
                "cdate": 1700291750781,
                "tmdate": 1700291750781,
                "mdate": 1700291750781,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "IAbOywqKNV",
            "forum": "iX1RjVQODj",
            "replyto": "iX1RjVQODj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6593/Reviewer_Vhgt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6593/Reviewer_Vhgt"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a Contrastive Preference Learning (CPL) framework for learning optimal policies from human preference data without learning the reward function. Specifically, the paper models human preferences using the advantage function and proposes a general loss function for learning policies. The loss function ensembles the contrastive learning objective and can be optimized directly without learning a reward function. As a result, the method can scale to high-dimensional environments and sequential RLHF problems (i.e., beyond contextual bandits). Theoretically, by optimizing the loss function, CPL provably converges to the optimal policy of the underlying max-entropy RL problem. The paper tests one instantiation of the CPL framework and shows its promising performance in practice."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The proposed algorithmic framework is novel and elegant. The motivation for the problem is clear.\n\n- The method is scalable without the use of RL.\n\n- The experimental results are adequate.\n\n- The paper is very well-written and easy to follow."
                },
                "weaknesses": {
                    "value": "I did not identify any noticeable weaknesses."
                },
                "questions": {
                    "value": "Since the CPL loss function has a super elegant form, is it possible to derive finite sample analysis for learning a near-optimal policy like [1]?\n\n[1] Zhu et al., Principled Reinforcement Learning with Human Feedback from Pairwise or K-wise Comparisons, ICML 2023"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6593/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6593/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6593/Reviewer_Vhgt"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6593/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698989158936,
            "cdate": 1698989158936,
            "tmdate": 1699636749755,
            "mdate": 1699636749755,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dOXDsLk8bP",
                "forum": "iX1RjVQODj",
                "replyto": "IAbOywqKNV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6593/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6593/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the review -- let us know if you have questions!"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for their time in reviewing our work. We are happy to hear that our algorithm was \u201cnovel and elegant\u201d, and that it\u2019s \u201cmotivation\u2026 is clear\u201d.\n\nAs the reviewer has pointed out no weaknesses, we will just mention the following revisions to the draft:\n\n1. Additional PPO baseline. We have modified Table 1 with an additional PPO baseline. Note that as PPO is an online RL method, it requires 4x the amount of state-action data as CPL plus online interactions with the MDP. Despite that, it still underperforms in many settings and exhibits low variance. \n\n2. Additional Benchmark with Real Human Feedback. We have added additional experiments on the D4RL benchmark using only 100 to 500 real-human queries. \n\n3. Additional Theoretical revisions. We have heavily modified the appendix and Section 3.3 including a new proposition on CPLs regularization with finite data.\n\n4. Additional ablations on pretraining in Appendix C."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6593/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700243352197,
                "cdate": 1700243352197,
                "tmdate": 1700243352197,
                "mdate": 1700243352197,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "x4BhIdZRej",
            "forum": "iX1RjVQODj",
            "replyto": "iX1RjVQODj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6593/Reviewer_NpZJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6593/Reviewer_NpZJ"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents Contrastive Preference Learning (CPL), an algorithm to learn optimal policies from preferences without learning exlicitly a reward function which is commonly done in RLHF scenario. This circumvents the issue of having an underoptimized/overoptimized reward model. The authors then show the performance of CPL for MetaWorld benchmark."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. learning a reward model from human preferences has flaws. And on top of that, using RL to optimize for this reward model can sometimes lead to poor performance. This paper solves this issue by not having a reward model.\n2. CPL has supervised objectives so it is scalable \n3. The proposed algorithm is generic"
                },
                "weaknesses": {
                    "value": "1. In my experience, learning a \"good\" reward model and then doing RL always outperforms offline RL algorithms. The authors only compare it with IQL and not with methods with reward models to highlight more\n2. The authors claim that the method is generic but then it is only applied to MetaWorld benchmark. The RLHF scenario is much more interesting in aligning language models with human feedback."
                },
                "questions": {
                    "value": "How does CPL compare with RLHF for language models scenario?\nHow does CPL compare with other baselines, which may or may not have reward models"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6593/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699298567483,
            "cdate": 1699298567483,
            "tmdate": 1699636749642,
            "mdate": 1699636749642,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "MLAnXgn8jE",
                "forum": "iX1RjVQODj",
                "replyto": "x4BhIdZRej",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6593/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6593/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer NpZJ"
                    },
                    "comment": {
                        "value": "Thank you for taking the time to review our work. We were happy to hear that the reviewer found our work to be \u201cgeneric\u201d, \u201cscalable\u201d, and overcoming challenges in reward optimization. The reviewer had two primary questions about our work, first concerning our baselines and second concerning our benchmarks.\n\n**Baselines**\n\nThe reviewer asked us to compare CPL to methods that learn a reward model. \n\n1. We would graciously like to point out that P-IQL does indeed learn a reward model first, which could then be applied with any RL Algorithm. We chose to use IQL since it is generally a very  high-performing offline RL method [1] and has been used as a standard in many prior RLHF for control works [2, 3].\n\n2. We have additionally compared CPL to PPO with a learned reward function based on comments from Reviewer WpDS.  We would like to point out that this comparison is *unfair* to CPL, as PPO requires additional online data to estimate the policy gradient. While in bandits settings (like LLMs) this can be done without access to the MDP, in sequential settings (like control, or 2+ steps of dialogue) we need to interact with the MDP to get full trajectories, which can be expensive (for ex running a robot in the real world). Despite the fact that PPO uses online data and 4x the number of state-action pairs, it is highly unstable in comparison to CPL and often underperforms it by a large margin. We have included full results in Table 1. \n\n3. Our results are consistent with other recent works [3, 4] which have also shown that learning an explicit reward model is unnecessary to achieve good performance. \n\n**Benchmarks**\n\n1. The reviewer mentioned that RLHF domains with LLMs would be interesting to explore. We whole-heartedly agree that RLHF with LLMs is a great possible application of CPL. Our work, however is about settings with *sequential* data  in particular, which in dialogue would constitute multiple turns of interaction with a human user. To our knowledge there were no such publicly available RLHF datasets with 2+ turns of dialogue at the time of submission. We believe such an investigation is a large undertaking, enough to warrant its own publication.\n\n2. We would kindly like to push back against the notion that robotics domains are less interesting. We chose to evaluate in robotics domains precisely because they capture the sequential nature of CPL, and show that our method applies to general MDPs, instead of the  simpler contextual bandit setting used in standard RLHF with LLMs.\n\n3. We have included additional benchmark results on more robotics domains using only 100 or 500 queries of real-human feedback [2]. We hope this serves as another datapoint for the applicability of both CPL and the regret-preference model. Please see Table 3 in the paper for results, or the general response for results. We find that CPL is able to perform similarly to other methods while being vastly simpler as it requires no Q-learning. CPL also exhibits lower variance.\n\nWe hope the reviewer can consider these additional empirical results, as well as our revised theoretical contributions when assessing our work. \n\n[1] Kosterikov et al. Implicit Q-Learning ICLR 2022.\n\n[2] Kim et al. Preference Transformer. ICLR 2023.\n\n[3] Hejna et al. Inverse Preference Learning. NeurIPS 2023\n\n[4] Rafailov, Sharma, Mitchell, Direct Preference Optimization. NeurIPS 2023"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6593/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700243323450,
                "cdate": 1700243323450,
                "tmdate": 1700243323450,
                "mdate": 1700243323450,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]