[
    {
        "title": "Comprehensive Comparison between Vision Transformers and Convolutional Neural Networks for Face Recognition Tasks"
    },
    {
        "review": {
            "id": "XIBGDe4oe6",
            "forum": "CCo8ElCT7v",
            "replyto": "CCo8ElCT7v",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5630/Reviewer_RorU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5630/Reviewer_RorU"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a comprehensive comparison between ViTs and CNNs for face recognition tasks, focusing on face identification and verification. The study evaluates six models (EfficientNet, Inception, MobileNet, ResNet, VGG, and ViTs) on five diverse datasets, highlighting the performance, robustness, and inference speed of ViTs compared to CNNs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper conducts thorough experiments on various network architectures for different evaluation tasks and datasets in face recognition.\n2. It offers valuable insights for the design of network structures in face recognition applications."
                },
                "weaknesses": {
                    "value": "1. This paper seems more like an experimental evaluation report, primarily focusing on the organization of test numbers.\n2. It would be beneficial for the authors to extrapolate some new insights from these figures, potentially providing fresh perspectives on training or evaluation in face recognition."
                },
                "questions": {
                    "value": "None."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5630/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698504554281,
            "cdate": 1698504554281,
            "tmdate": 1699636583438,
            "mdate": 1699636583438,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TKUX0yRizP",
                "forum": "CCo8ElCT7v",
                "replyto": "XIBGDe4oe6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5630/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5630/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer RorU"
                    },
                    "comment": {
                        "value": "We extend our gratitude for your thoughtful evaluation of our paper on the comparative analysis of Vision Transformers (ViTs) and Convolutional Neural Networks (CNNs) for face recognition tasks. Your insights are invaluable in refining our study.\n\nWe appreciate your recognition of the thorough experimentation conducted across diverse network architectures and datasets, providing insights into the performance of these models in face recognition applications. However, we understand your concerns regarding the presentation and contribution aspects of our work.\n\nWe acknowledge the need to move beyond a mere presentation of experimental results and strive to offer deeper insights and novel perspectives derived from our findings. In our revised manuscript, we will extrapolate and discuss new insights garnered from the results, exploring avenues for fresh perspectives in training methodologies, evaluation techniques, and potential optimizations specific to face recognition tasks. Our goal is to not solely present numerical outcomes but to derive insights that could impact the design and implementation of network structures in face recognition applications.\n\nWe understand the importance of expanding the narrative beyond a pure experimental evaluation and will focus on extrapolating new insights to elucidate deeper implications and potential avenues for further exploration within the field of face recognition.\n\nYour feedback is immensely valuable, and we are committed to addressing the outlined weaknesses and enhancing the manuscript to provide richer insights and broader perspectives derived from our experimental evaluations.\n\nWe're actively implementing your suggestions to refine our manuscript comprehensively. Our revised version will be uploaded before the discussion period ends."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5630/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700496613956,
                "cdate": 1700496613956,
                "tmdate": 1700496613956,
                "mdate": 1700496613956,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UfmzvusRNy",
                "forum": "CCo8ElCT7v",
                "replyto": "XIBGDe4oe6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5630/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5630/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Implementation of Reviewer RorU suggestions"
                    },
                    "comment": {
                        "value": "We sincerely appreciate the time and effort you dedicated to evaluating our paper and providing constructive feedback. Your insights have played a crucial role in refining our work.\n\nIn response to your valuable feedback, we've made concerted efforts to extrapolate new insights from our experimental results. Specifically, in Section 3 and more prominently in Section 3.4, we've delved deeper into the implications of our findings, providing a more nuanced discussion of the performance ViTs in comparison to CNNs for face recognition tasks. While we might not have uncovered some new insights, we believe our comparative analysis contributes to the ongoing discourse and helps illuminate the landscape of ViTs and CNNs in face recognition, potentially guiding future research directions.\n\nThroughout the manuscript, we've implemented minor improvements to enhance the overall clarity and coherence of our presentation. We aim to strike a balance between the detailed organization of experimental results and the derivation of meaningful insights that extend beyond a mere numerical account.\n\nYour feedback has been instrumental in shaping the direction of our revisions, and we remain dedicated to providing a manuscript that not only presents thorough experimental evaluations but also offers valuable perspectives for the design and optimization of network structures in face recognition applications."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5630/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700648583628,
                "cdate": 1700648583628,
                "tmdate": 1700648583628,
                "mdate": 1700648583628,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0xwmQsMUKH",
            "forum": "CCo8ElCT7v",
            "replyto": "CCo8ElCT7v",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5630/Reviewer_6T9x"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5630/Reviewer_6T9x"
            ],
            "content": {
                "summary": {
                    "value": "The paper presented a fair and comprehensive benchmark of 5 CNN-type networks and Vision Transformer on 5 face recognition benchmark datasets in terms of face verification and validation tasks. This benchmark enforced the exact training and testing set splits for fair comparison and compared the training and test accuracy, the number of parameters and inference time. The results show that the ViT network compares favorably to those CNN-type networks w.r.t. the face recognition performance and computation complexity on these benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper performed a rigorous evaluation of EfficientNet, Incpetion, MobileNet, ResNet, VGG and ViT networks by training them for face recognition tasks and compared their performance thoroughly."
                },
                "weaknesses": {
                    "value": "Certainly this is a quite useful technical report on the evaluation of different popular network architectures for face recognition tasks. I am not convinced this work\u2019s \u201cparamount significance as it pioneers a comprehensive evaluation of ViTs against CNNs\u201d.\n\nThis benchmark compared the performance of 6 vanilla networks trained for face recognition tasks. In fact, there are many dedicated face recognition methods and pipelines including face detection and alignment, etc. The FR field probably cares more about the end-to-end performance of the whole face recognition pipeline.\n\nThe FRTE is probably the most thorough evaluation for the industry, which tests the performance of binary programs provided by different FR vendors on a blind set with no limitation of the training dataset or anything.\n\nFace recognition Technology Evaluation (FRTE) organized by NIST\nhttps://pages.nist.gov/frvt/html/frvt1N.html\n\nImportant references missing:\nDeepFace: closing the gap to human-level performance in face verification, CVPR 2014. \nDeep learning face representation by joint identification-verification, NIPS 2014\nComparing vision transformer and convolutional neural networks for image classification: a literature review, 2023."
                },
                "questions": {
                    "value": "The performance appears saturated on some face recognition datasets. The results of several cases may affect the benchmark. Is there any more challenging test set for face recognition?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed.",
                        "Yes, Responsible research practice (e.g., human subjects, data release)"
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5630/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698744938261,
            "cdate": 1698744938261,
            "tmdate": 1699636583337,
            "mdate": 1699636583337,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dQFX2ZhR1o",
                "forum": "CCo8ElCT7v",
                "replyto": "0xwmQsMUKH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5630/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5630/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 6T9x"
                    },
                    "comment": {
                        "value": "Thank you sincerely for the meticulous review of our paper on the comparative evaluation of Vision Transformers (ViTs) and CNN-type networks for face recognition tasks. Your insights and observations are invaluable in refining our study.\n\nWe deeply appreciate your recognition of the rigor applied in benchmarking various network architectures for face recognition across diverse datasets. Our aim was to provide a focused and comparative analysis of these models' performances, focusing on training accuracy, test accuracy, parameter count, and inference time, ensuring a comprehensive evaluation.\n\nWe acknowledge your point regarding the scope of our work within the larger context of face recognition methodologies. While our study primarily focuses on the individual performance of network architectures, we agree that a comprehensive evaluation might include the entire face recognition pipeline, encompassing aspects like face detection and alignment. We aimed to delve specifically into the comparison of ViTs and CNNs in isolation, highlighting their standalone effectiveness in face recognition tasks.\n\nRegarding the missing references, we appreciate your suggestions and will duly consider incorporating these important references to enrich the discussion and further contextualize our findings within the broader landscape of face recognition literature.\n\nConcerning the saturation of performance on certain datasets, we understand your concern. While we acknowledge the observation of performance saturation on the VGG-Face 2 dataset, it's crucial to note that across the other selected datasets, we did not encounter similar saturation phenomena. We selected these datasets intentionally to encompass a range of challenges, including people diversity, varying distances from the camera, and occlusions caused by masks and glasses. However, we acknowledge the possibility of additional datasets that might present further challenges, and we will explore and consider including more demanding test sets for a more comprehensive evaluation.\n\nWe aim to address your comments and enhance the manuscript by providing a more contextualized discussion and ensuring a clearer articulation of the scope and limitations of our study.\n\nYour feedback is immensely valuable, and we are dedicated to incorporating these suggestions to improve the quality and relevance of our work.\n\nWe are actively implementing your suggestions to refine our manuscript comprehensively. Our revised version will be uploaded before the discussion period ends."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5630/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700496493162,
                "cdate": 1700496493162,
                "tmdate": 1700496493162,
                "mdate": 1700496493162,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "k3jbj2y2Ol",
                "forum": "CCo8ElCT7v",
                "replyto": "0xwmQsMUKH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5630/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5630/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Implementation of Reviewer 6T9x suggestions"
                    },
                    "comment": {
                        "value": "We greatly appreciate your thorough evaluation of our manuscript and the valuable suggestions you provided. Your insights have been instrumental in refining our study.\n\nRegarding your suggestion about incorporating the FRTE evaluation, we sincerely appreciate the consideration. We carefully reviewed the suggested FRTE methodology, recognizing its significance in assessing the end-to-end performance of face recognition systems. However, due to the time constraints within the discussion period and the considerable preparation required to conform to the FRTE format, it was challenging to integrate this evaluation into our current study. We aimed to provide a focused comparison between ViTs and CNNs in this iteration, focusing on their individual performances in face recognition tasks across various datasets.\n\nIn response to your comment about missing important references, we've taken your suggestion seriously. The references you highlighted have been thoughtfully incorporated into the revised manuscript (section 1). They now contribute significantly to contextualizing our work within the broader landscape of face recognition literature, enriching the discussion.\n\nYour feedback has been immensely helpful, and we are committed to ensuring our manuscript reflects the improvements suggested."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5630/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700647909515,
                "cdate": 1700647909515,
                "tmdate": 1700647909515,
                "mdate": 1700647909515,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "H3xefwZsNE",
            "forum": "CCo8ElCT7v",
            "replyto": "CCo8ElCT7v",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5630/Reviewer_dZCP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5630/Reviewer_dZCP"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents an empirical, mainly quantitative, comparison between CNNs and Vision Transformers (ViTs) as evaluated on both face identification and face verification. Five different CNNs and one VT are evaluated across six datasets. It is found that the ViT (ViT-B32) almost consistently outperforms all CNN architectures. The experiments are systematic."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* Empirical investigations of the behavior and performance of neural networks is of large importance. It is brave of a paper to take a step back and systematically compare architectures rather than constantly presenting new -- potentially not as thoroughly tested -- models or add-on modules.\n\n* We are in the middle of a paradigm shift between vision Transformers and CNNs so it is certainly important right now to try and map out empirical differences between the two.\n\n* Five datasets and six models are used (extensive evaluation)\n\n* The code is made public.\n\nMinor but positive things:\n\n* The three paragraph of the introduction had a good flow and were easy to read (see only that the first face paragraph could be a bit more specific)\n* 2.1 is informative and well-written"
                },
                "weaknesses": {
                    "value": "* No uncertainty (e.g., standard deviations across random seeds) are presented for the different results. This may be ok since the models are evaluated across many different datasets, but in that case the seed should be fixed and that should be stated.\n\n* Missing a clear motivation for why facial recognition is the investigated task. Has it previously not been done for this field, are CNNs still considered the main models there? Also, it would be constructive to discuss the ethical risks vs. benefits of surveillance computer vision applications. (An ethical statement could be in order.)\n\n* Missing a comment on how the hyperparameters common for all architectures were selected (e.g., following another paper's set up, just as standard hyperparameters, etc). It is important that they were not selected to optimize a specific architecture, and it would be good to convince the reader about this.\n\n* I would avoid referring to my own paper as having 'paramount significance' (strong wording, verging on over-selling). The number of times the word 'remarkable/y' (6) is a bit exaggerated as well.\n\n* No explanation is offered for why the ResNet surpassed the ViT in Fig. 7b.\n\nDetailed minor suggestions:\n* References should be in parentheses (Dosovitskiy et al. (2020))\n* This paragraph could be made more specific (since you claim that it in fact does present **very specific** challenges), it is currently not so informative: \"...that presents very specific features and challenges. The main challenges are related to the low inter-class variance and the high intra-class variance that can be observed in most face image datasets Cao et al. (2018); Huang et al. (2008). This makes face recognition a more difficult task than...\"\n* I would (sadly) avoid using the wording 'in spirit' in this context (2.1, page 3)\n* 'Convoluting' should be changed to 'convolving', and quotations should be removed\n* page 4, \"an for\" >> \"and for\"\n* It could be nice with a table summarizing the 5 datasets and tasks.\n* Fig. 6: would be nice to have accessible in the caption whether this dataset is made for face verification or face identification."
                },
                "questions": {
                    "value": "* Page 5, \"is bounded between 0 (the worst measure of separability) and 1 (a perfect measure of separability), with 0.5 indicating that a network has no class separation capability whatsoever.\" >> if 0.5 already has 0 separability, what happens between 0.5 and 0? Maybe rephrase\n* 3.3: it is not clear to me if you use the checkpoint from the best epoch or from the 25th (last) epoch for the test results in Table 2? If you just use the last epoch (which I suspect since you say that each model has been trained for 25 epochs), it would be more informative to report the validation accuracy at this epoch (for the model you actually use.)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5630/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698775494902,
            "cdate": 1698775494902,
            "tmdate": 1699636583016,
            "mdate": 1699636583016,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GX1NV4jLw8",
                "forum": "CCo8ElCT7v",
                "replyto": "H3xefwZsNE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5630/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5630/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer dZCP"
                    },
                    "comment": {
                        "value": "We extend our gratitude for the detailed and insightful evaluation of our manuscript on the comparison between Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) in face recognition tasks. Your feedback is invaluable in refining our study and enhancing its overall quality.\n\nFirstly, we appreciate your acknowledgment of the empirical nature of our work and the significance of systematically comparing architectures. We aimed to provide a comprehensive analysis of ViTs against CNNs across diverse datasets, aiming for clarity and depth in our approach.\n\nRegarding the absence of standard deviations across random seeds, we recognize the importance of robustness in presenting results. In response, we will ensure to clarify our methodology, including fixed seed settings, particularly across diverse datasets, to provide a more robust evaluation framework.\n\nThe motivation behind our investigation into facial recognition as the primary task stems from the evolving landscape of computer vision and the ongoing paradigm shift between traditional Convolutional Neural Networks (CNNs) and emerging architectures like Vision Transformers (ViTs). While previous studies have explored face recognition methodologies, our focus lies in presenting a comparative analysis between ViTs and CNNs specifically within this domain. CNNs have historically been the cornerstone of face recognition models; however, with the emergence of ViTs and their potential advantages, a comprehensive evaluation becomes imperative.\n\nConcerning the hyperparameters, we apologize for the oversight in not explicitly detailing their selection process. We will provide a clear rationale for their choice, ensuring they were not optimized for specific architectures, thus assuring readers of the neutrality of our approach.\n\nWe acknowledge the critique regarding the tone and phrasing within the manuscript and will revise the language to maintain a balanced and accurate representation of our work without overstating its significance.\n\nRegarding the specific suggestions for improvements in formatting, references, and terminology, we will meticulously address these points to enhance the overall clarity and readability of the manuscript. Additionally, we will consider the inclusion of a summarized table for datasets and tasks and provide more accessible information in figure captions for improved clarity.\n\nThe questions and clarifications raised regarding separability measures and epoch selection for test results in Table 2 will be explicitly addressed in the revised manuscript to ensure comprehensive and transparent reporting.\n\nWe genuinely appreciate the constructive feedback provided, and we are committed to incorporating these suggestions to enhance the quality, clarity, and relevance of our study.\n\nWe are actively implementing your suggestions to refine our manuscript comprehensively. Our revised version will be uploaded before the discussion period ends."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5630/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700496318658,
                "cdate": 1700496318658,
                "tmdate": 1700496318658,
                "mdate": 1700496318658,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jWwqnkF4Gx",
                "forum": "CCo8ElCT7v",
                "replyto": "H3xefwZsNE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5630/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5630/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Implementation of Reviewer dZCP suggestions"
                    },
                    "comment": {
                        "value": "We express our sincere appreciation for your meticulous evaluation of our manuscript on CNNs and Vision Transformers (ViTs) in face recognition. Your constructive feedback has been integral to refining our work.\n\nWe'd like to apprise you of the substantial improvements we've made in response to your suggestions. To address concerns about uncertainty in our results, we included fixed seed settings across diverse datasets, ensuring the reliability and stability of our comparative findings. This update is explicitly detailed in the first paragraph of section 3.\n\nMoreover, we have enhanced the clarity of our motivations for selecting facial recognition as the investigative task. This revision is reflected in a more explicit and detailed second paragraph of section 1, outlining why we chose this domain to evaluate CNNs and ViTs.\n\nRegarding the selection of common hyperparameters, we've included a comment in the first paragraph of section 3, clarifying our approach and assuring readers that these settings were not optimized for any specific architecture.\n\nWe have duly modified the language in the final paragraph of section 4 to avoid overemphasizing the significance of our work, ensuring a more balanced presentation.\n\nAdditionally, in response to the question about the figure where VGG (we understand the reviewer meant VGG and not ResNet, as this is the network that outperformed ViT in this scenario) outperforms ViT (Fig. 7b), we've incorporated an explanation in the last paragraph of section 3.4, addressing the reasons behind this observed occurrence.\n\nWhile we've strived to implement as many suggested changes as possible, some limitations within the paper space restricted the inclusion of a table summarizing datasets, a more detailed paragraph about specific dataset challenges, and alterations to figure captions. We regret any inconvenience this might cause and appreciate your understanding of these constraints.\n\nWe are immensely grateful for your comprehensive assessment and remain committed to refining our manuscript. Your insights have significantly contributed to enhancing the quality, clarity, and relevance of our study."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5630/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700645562721,
                "cdate": 1700645562721,
                "tmdate": 1700645562721,
                "mdate": 1700645562721,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hCG27IcDrj",
                "forum": "CCo8ElCT7v",
                "replyto": "H3xefwZsNE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5630/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5630/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answers to the questions raised by Reviewer dZCP"
                    },
                    "comment": {
                        "value": "## Page 5, \"is bounded between 0 (the worst measure of separability) and 1 (a perfect measure of separability), with 0.5 indicating that a network has no class separation capability whatsoever.\" >> if 0.5 already has 0 separability, what happens between 0.5 and 0? Maybe rephrase.\n\nWe acknowledge the reviewer's observation regarding the potential confusion in the phrasing of the sentence about the Area Under the Curve (AUC). In our revised manuscript, we have rephrased this statement for better clarity. Now, the sentence explicitly articulates that an AUC of 0 denotes the worst measure of ~separability~ performance, an AUC of 1 signifies the best measure of ~separability~ performance, and an AUC of 0.5 indicates that a network lacks any class separation capability.\n\n## 3.3: it is not clear to me if you use the checkpoint from the best epoch or from the 25th (last) epoch for the test results in Table 2? If you just use the last epoch (which I suspect since you say that each model has been trained for 25 epochs), it would be more informative to report the validation accuracy at this epoch (for the model you actually use.).\n\nWe want to clarify that the reported results in both Table 1 and Table 2 stem from the checkpoint of the best epoch. Although we ensured that all models completed 25 epochs during training, the values presented in these tables reflect the accuracies obtained using the weights from the best epoch. As explicitly mentioned in the manuscript, the accuracy values in Table 1 correspond to the highest achieved on both the training and validation sets during training. This distinction ensures that the reported accuracies represent the models' best performance on the given datasets."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5630/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700647228252,
                "cdate": 1700647228252,
                "tmdate": 1700647228252,
                "mdate": 1700647228252,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "s1fTxHxAJr",
            "forum": "CCo8ElCT7v",
            "replyto": "CCo8ElCT7v",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5630/Reviewer_D6eT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5630/Reviewer_D6eT"
            ],
            "content": {
                "summary": {
                    "value": "In this manuscript, the authors attempt to study the performance of general-purpose Vision Transformers in Face Recognition scenarios and contrast their findings against general-purpose Convolutional Neural Network architectures. They claim that ViT performance perform better than the compared CNNs in this scenario."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The document is mostly well-presented in its structure, use of the English language, and figures."
                },
                "weaknesses": {
                    "value": "This work completely disregards other popular works in the face recognition literature. The authors compare general-purpose CNNs and ViT_B32 when efficient face recognition-specific approaches are already available such as MobileFaceNet (Chen et al. 2018), ShuffleFaceNet (Mart\u00ednez-Diaz et al., 2019), VarGFaceNet (Yan et al., 2019), GhostFaceNets (Alansari et al., 2022), EdgeFace (Geroge et al., 2023), among others. Furthermore, does not mention previous studies on transformers for face recognition (Zhong et al., 2021) and part-based face recognition with vision transformers (Sun et al., 2022), for example. They also do not comment on recently popular Hybrid (ViT+CNN) approaches, as in EdgeFace and MobileFaceFormer (Li., 2023). \nThe datasets described are not divided into scenarios and do not include relevant challenging datasets (e.g. using TinyFace (Zheng et al., 2018) and SurvFace (Zheng et al., 2018) to complement low resolution comparisons with SCface). In general, this study misses many comparisons in the state of the art for face recognition scenarios such as: cross age with AgeDB (Moschoglou et al., 2017), cross pose with CFP (Sengupta et al., 2016), racial-bias analysis with RFW (Wang., 2018), among many others."
                },
                "questions": {
                    "value": "Suggestions:\n- Familiarize with recent literature specific on face recognition and gather benchmark on key datasets.\n- Analyze the components that make the face recognition-specific approaches more accurate on face recognition scenarios."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No particular comments in this section"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5630/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698948536518,
            "cdate": 1698948536518,
            "tmdate": 1699636582872,
            "mdate": 1699636582872,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "AwyHMjB8Dz",
                "forum": "CCo8ElCT7v",
                "replyto": "s1fTxHxAJr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5630/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5630/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer D6eT"
                    },
                    "comment": {
                        "value": "We sincerely appreciate the insightful feedback provided on our manuscript evaluating Vision Transformers (ViTs) against Convolutional Neural Networks (CNNs) for face recognition tasks. Your detailed assessment has been immensely helpful in refining our study.\n\nWe acknowledge the rich landscape of face recognition literature, encompassing specialized approaches like MobileFaceNet, ShuffleFaceNet, VarGFaceNet, among others, and recent studies on transformers for face recognition. While our paper might not extensively delve into these specific methodologies, our primary intent was not to challenge or replicate the state-of-the-art in face recognition but rather to present a comparative analysis between ViTs and CNNs within this domain.\n\nIn response to your feedback, we will include explicit mentions and discussions of previous studies utilizing both transformers and CNNs for face recognition. We aim to highlight their significance within the context of our analysis, thereby elucidating the specific contributions of our work.\n\nRegarding the choice of datasets, our intention was to provide a diverse set covering a range of challenges pertinent to real-world scenarios, such as people diversity, varying distances from the camera, and occlusions caused by masks and glasses. While we acknowledge the importance of additional challenging datasets like TinyFace and SurvFace, our selected datasets were chosen deliberately to address these challenges, even if not explicitly listed in your suggestions.\n\nWe are committed to enhancing the clarity of our paper by explicitly delineating the objectives and scope of our comparative study. Additionally, we will include a broader acknowledgment of pertinent literature, thus enriching the context and relevance of our findings within the broader landscape of face recognition research.\n\nWe are grateful for the opportunity to improve our work and are dedicated to incorporating your valuable suggestions to elevate the quality and relevance of our manuscript.\n\nWe are actively implementing your suggestions to refine our manuscript comprehensively. Our revised version will be uploaded before the discussion period ends."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5630/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700496023290,
                "cdate": 1700496023290,
                "tmdate": 1700496023290,
                "mdate": 1700496023290,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "AE0yMYD9aJ",
            "forum": "CCo8ElCT7v",
            "replyto": "CCo8ElCT7v",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5630/Reviewer_dZCP"
            ],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5630/Reviewer_dZCP"
            ],
            "content": {
                "title": {
                    "value": "Final comment"
                },
                "comment": {
                    "value": "I have taken part in the other reviews, authors' replies, and updated manuscript. Thank you to the authors for your detailed replies to my review. I maintain my score at 6, since the comment by D6eT which knows the recent literature very well worries me slightly, otherwise I might have considered raising it to 7 after the improvements of the paper."
                }
            },
            "number": 10,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5630/-/Official_Comment"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1700690522245,
            "cdate": 1700690522245,
            "tmdate": 1700690522245,
            "mdate": 1700690522245,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]