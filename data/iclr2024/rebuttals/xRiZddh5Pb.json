[
    {
        "title": "Learning from A Single Graph is All You Need for Near-Shortest Path Routing"
    },
    {
        "review": {
            "id": "P9dWgFwGus",
            "forum": "xRiZddh5Pb",
            "replyto": "xRiZddh5Pb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7891/Reviewer_HSXi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7891/Reviewer_HSXi"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the All-Pairs Near-Shortest Path (APNSP) problem using the Markov Decision Process (MDP) framework and proposes a DNN-based approach to learning the local forwarding policy by predicting the Q-values. The neural network model takes the state features and action features as input, which include the distance information of the current state (node) and the next state (one neighbor of the current node). The models are trained based on supervised learning, where the samples are collected from a seed graph. In addition, the authors provide theory results to demonstrate the generalization properties of the model. Experiments have been shown to evaluate the scalability and generalizability of the proposed approach."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- S1: This paper provides a practical MDP framework for the APNSP problem. \n- S2: The empirical results of the proposed approach show comparable performance to the baseline greedy forwarding approach."
                },
                "weaknesses": {
                    "value": "- W1: The proposed approach is reasonable but lacks novelty. Using Q-learning as a heuristic to solve optimization problems is straightforward and somehow trivial. Regarding the APNSP problem, similar approaches have been previously proposed, e.g.,\n  -  Wu, Yaoxin, Wen Song, Zhiguang Cao, Jie Zhang, and Andrew Lim. \"Learning improvement heuristics for solving routing problems.\" IEEE transactions on neural networks and learning systems 33, no. 9 (2021): 5057-5069.\n  - Bi, Jieyi, Yining Ma, Jiahai Wang, Zhiguang Cao, Jinbiao Chen, Yuan Sun, and Yeow Meng Chee. \"Learning generalizable models for vehicle routing problems via knowledge distillation.\" Advances in Neural Information Processing Systems 35 (2022): 31226-31238.\n\n\n- W2: The theoretical results of the generalizability of the proposed approach are not warranted. The concept of \u201clearnable\u201d mentioned in Theorems 0, 1, and 2 needs formal treatments \u2013 for example, sample complexity, PAC-learnability, and regret bound.  \n  - W2-1: The RankPre property and optimal ranking seem to be the core concepts of the theorem, but the relationship between these two concepts and the model\u2019s generalizability power is unclear. First, the RankPre seems to be the property of the raking metric m. It looks weird because the property's content is like a conclusion. Second, regarding the optimal ranking, the authors require the monotonically increasing order of the ranking metric for the neighbors of a node in the graph. However, there are no arguments showing the reason for this requirement and its relationship with the generalizability results. \n  - W2-2: it is confusing to me about  \u201ca learnable DNN\u201d. The authors claim that there exists a learnable DNN that can achieve optimal ranking without any further description. For example, what is the hypothesis space? How to train the model, and what is the sample complexity?\n  - W2-3: In the proof of theorem 0, the authors claim that the DNN can achieve \u201coptimal ranking\u201d by learning a group of weights for each input feature that matches those used in the ranking metrics. From my understanding, this means a zero empirical risk which does not have any further discussions. \n  - W2-4:  It is unclear how closely the implementable algorithm relates to the theoretical results.  The proposed approach does not seem to be scalable. It is not clear about \u201ca subset of training samples\u201d.  Even from an intuitive perspective, according to the description, if this collected subset sample is limited to only a small portion of the graph (including the origin and destination), how can the model learned from here be applied to all graphs? \n\n- W3: There\u2019s no experimental comparison with other state-of-the-art approaches for the All-Pairs Near-Shortest Path (APNSP) problem. \n\n- Minor comments:\n  - In section 2.2, when defining the Q-value, it is unclear about the notation L and t.\n  - The authors explain the idea behind designing the input features in Proposition 1, wherein a ranking metric using the designed distance input features following a specific format that meets the RankPres property. It is not straightforward to me why this format would satisfy the property. Detailed proof would be helpful. \n  - The shapes in Figure 3 are hard to recognize."
                },
                "questions": {
                    "value": "What does it mean, mathematically, by \"learnable\" in the theorems?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7891/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698176024744,
            "cdate": 1698176024744,
            "tmdate": 1699636968412,
            "mdate": 1699636968412,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "u43iFUuSfB",
            "forum": "xRiZddh5Pb",
            "replyto": "xRiZddh5Pb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7891/Reviewer_D6sq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7891/Reviewer_D6sq"
            ],
            "content": {
                "summary": {
                    "value": "This paper looks at the (near-)shortest path routing problem on geometric random graphs in Euclidean and hyperbolic metric spaces. Each instance of the problem consists of a graph and a pair of source and destination nodes. There are two settings, one where the input feature for each node is simply the (Euclidean/hyperbolic) distance to the destination and one where each node additionally receives its stretch factor (essentially how far the node lies off of the straight line distance between the source and destination nodes) as input.\nThe authors train a neural network to calculate a score independently for each neighbour of a node, so that the highest score (Q-value) neighbour can be chosen as the next location towards the destination. The input for the scoring network is the node itself and one of its neighbours. By repeating the scoring for each neighbour, a step can be taken, and by repeating these steps multiple times, a path can be formed from the source to the destination node.\nThe authors show that their neural network with just the distance to the destination as input is able to learn the greedy policy of always choosing the neighbour that is nearest to the destination. They also show that their neural network can outperform greedy when the stretch factor is also provided. Their network is able to learn effective policies with very little training data."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The paper is easy to follow. Routing is generally an interesting topic, though a bit narrow."
                },
                "weaknesses": {
                    "value": "The problem is framed in a way that the neural network has very little learning to do. In the simpler case the network essentially just has to learn to put a minus sign in front of a neighbour\u2019s distance to the destination. In this way the node with the highest score will correspond to the neighbour with the lowest distance to destination and will be selected as per the greedy heuristic. Even in the slightly more complex setting, the network learns a simple policy - as revealed also by the authors in Figure 3 and Equation (4).  \n\nThere is no interesting contribution in terms of the wider setting of neural algorithm learning. There are many other papers that replace a heuristic step in an algorithm with a neural network, so this cannot be considered a novel approach. \n\nThe problem setting is very narrow and easily (approximately) solved with very simple (and very efficient) heuristics. The authors do not show any significant advantage over using a neural network to replace these simple heuristics, neither in terms of performance, nor in terms of efficiency. \n\nBarring a contribution in terms of performance or efficiency, I would expect new results in terms of what a neural network is able to do, but learning a simple linear relation is nothing surprising, even if very little data is required. Planning multiple steps ahead would already be much more interesting and more complex for this problem. Of course this would require giving the whole graph as an input to the neural network, rather than just a pair of nodes. \n\nThe main selling point of this paper seems to be how little data is required to learn an algorithm that generalises effectively. But in the context of what the neural network has to learn here, this is not particularly impressive.\n\nLittle effort has been made to push the model to its limits. One could include a set of \u201cdifficult\u201d graphs, where the greedy strategy fails.\n\nAlgorithmically this problem both settings are well understood since 20 years. It's not clear why we need a learning approach in the first place."
                },
                "questions": {
                    "value": "The major questions are in the weaknesses. Here are some minor questions:\n1. In Section 2.3, the distance to destination is presumably the Euclidean/hyperbolic distance and not the graph distance, but this is not clearly specified.\n2. In Theorem 1 we assume that a (linear) ranking metric that satisfies RankPres exists (i.e. gives an optimal ranking of neighbouring nodes) and claim that it is therefore learnable. But unless I am missing something, the theorem then essentially just says we can learn a linear function with a neural network. Calling this a Theorem seems to be an overstatement. And surely the interesting cases would be when the assumption does not hold.\n3. Where is the proof for Proposition 1?\n4. Why are the page numbers in roman numerals?\n5. Footnote 1 on page v essentially describes \u201cmonotonically non-decreasing\u201d, perhaps you want to use this term instead.\n6. In Figure 2, what epsilon value is used?\n7. Throughout the paper, accuracy is reported, but what about other metrics? For example, the average relative cost increase versus the optimal solution would be interesting.\n8. Where are the results that confirm the claim on page vii \u201cthat the performance of all the learned policies exactly match the prediction accuracy of Greedy Forwarding\u201d?\n9. Do you have a citation to back up the claim on page viii that \u201cGF was believed to work close to the optimal routing\u201d? There are several theoretical works that go beyond greedy forwarding. Moreover, these works could even be used as baselines."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7891/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698682275875,
            "cdate": 1698682275875,
            "tmdate": 1699636968295,
            "mdate": 1699636968295,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1G6yR6QiGK",
                "forum": "xRiZddh5Pb",
                "replyto": "u43iFUuSfB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7891/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7891/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Q: In Figure 2, what epsilon value is used?\n\nA: We list the simulation parameters in Table 3 in the appendix. The $\\epsilon$ value (i.e., the margin for shortest paths prediction) is set to 0.05.\n\nQ: Do you have a citation to back up the claim on page viii that \u201cGF was believed to work close to the optimal routing\u201d?\n\nA: The following citation [3] states that Greedy Forwarding achieves nearly optimal routing in hyperbolic space.\n\n[3] Fragkiskos Papadopoulos, Dmitri Krioukov, Mari\u00b4an Bogun\u00b4a, and Amin Vahdat. \"Greedy forwarding in dynamic scale-free networks embedded in hyperbolic metric spaces.\" In IEEE INFOCOM, pp. 1\u20139, 2010."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7891/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700606064824,
                "cdate": 1700606064824,
                "tmdate": 1700606064824,
                "mdate": 1700606064824,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dMj6oZeh4F",
            "forum": "xRiZddh5Pb",
            "replyto": "xRiZddh5Pb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7891/Reviewer_JN8R"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7891/Reviewer_JN8R"
            ],
            "content": {
                "summary": {
                    "value": "The authors use neural networks to solve the problem of finding (approximately) shortest paths in geometric graphs using only local information. \n\nThey consider graphs obtained by sampling a number of points in Euclidean or hyperbolic space, and connecting with an edge any two points whose distance is smaller than a fixed threshold. In such graphs, they consider routing algorithms that given current node u, and knowing that we are going from s to t, pick the next node v from neighbors of u based only on information about s, t, and u (and not about the whole graph). A classic heuristic of this kind, called Greedy Forwarding, simply picks the neighbour that minimizes the geometric distance to target t.\n\nThe authors propose a more elaborate approach for routing. They associate with each node v two features that depend on the source and target: 1) geometric distance from target d(v,t) and 2) stretch (d(s,v)+d(v,t))/d(s,t). Then, they train a neural network that takes as input features of two neighboring nodes u, v and outputs an approximation of the so-called Q-value. The Q-value is supposed to be high if it is a good routing decision to go to node v when being at node u, and in practice it is simply the negative length of the shortest path from v to the target. Then, to perform the actual routing, one has to evaluate the model on all neighbors of the current node and pick the one that gave the highest output value.\n\nThe network is trained using either supervised learning or RL, using only a small sample of nodes from a single graph. Both approaches generalize well and beat the Greedy Forwarding method. The metric used for evaluating routing approaches is the percentage of node pairs for which the evaluated approach finds a path with length within a multiplicative constant from the true shortest path length. I have not found information on what the constant is.\n\nThe authors also analyze what the network has learned. If only the distance feature is used, the network learns to mimic the Greedy Forwarding algorithm (which is nice, but not very surprising, see, e.g., \"A new dog learns old tricks (...)\" ICLR'19 paper). When both features are used, the network's behavior can be approximated with a piecewise linear function with only 2 pieces."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The presented approach improves over the standard Greedy Forwarding algorithm while still being reasonably simple to implement.\n\nThe paper is well written and easy to read."
                },
                "weaknesses": {
                    "value": "The comparison with prior work is insufficient. The only benchmark used is Greedy Forwarding, which uses only one feature (distance from target). It is not clear if the presented improvement comes from using more features or from a more elaborate method to use these features. More importantly, it is not clear how other (previously known and possibly simpler) heuristics compare to the proposed approach.\n\nI do not like the fact that the authors start with a neural network without checking first a simpler model (e.g. linear regression).\n\nEven though the authors keep using the phrase \"deep neural networks\", they only use networks with two hidden layers.\n\nEven though this is primarily an experimental paper, and the experiments described do not seem to require any specialised hardware nor proprietary datasets, the authors do not provide their source code nor anything else that would make reproducing their results easier (or at least I haven't found any such thing)."
                },
                "questions": {
                    "value": "Have you tried simpler models, say linear regression, before using neural networks?\n\nCould you provide experimental comparison with some prior works that also use stretch factor?\n\nWhat do you mean by \"class of all graphs whose nodes are uniformly distributed\" (page 2)? Do you mean class of distributions over graphs?\n\nWhat value of zeta(O,D) do you use in experiments?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7891/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7891/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7891/Reviewer_JN8R"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7891/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698770344816,
            "cdate": 1698770344816,
            "tmdate": 1699636968178,
            "mdate": 1699636968178,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3zCTb2hZNm",
                "forum": "xRiZddh5Pb",
                "replyto": "dMj6oZeh4F",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7891/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7891/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Q: Have you tried simpler models, say linear regression, before using neural networks?\n\nA: For the input features based on only distance, using linear regression also learns a ranking metric that matches the greedy forwarding routing. However, for the input features based on both distance and stretch factor, using linear regression learns a ranking metric that underperforms the one learned using neural networks in terms of the APNSP prediction accuracy.\n\nQ What value of $\\zeta(O,D)$ do you use in experiments?\n\nA: Since $\\zeta(O,D)$ denotes the path stretch of the endpoints (i.e., $\\frac{d_{sp}(O,D)}{d_{e}(O,D)}$), it varies for different (O, D) pairs across different random graphs."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7891/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700605866772,
                "cdate": 1700605866772,
                "tmdate": 1700605866772,
                "mdate": 1700605866772,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DlWdDAiY48",
            "forum": "xRiZddh5Pb",
            "replyto": "xRiZddh5Pb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7891/Reviewer_zGzT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7891/Reviewer_zGzT"
            ],
            "content": {
                "summary": {
                    "value": "This work presents a novel idea of deriving local routing policies using MDP formulation and small set of random graphs in Euclidean and hyperbolic metric spaces. The idea is supported by theories and their proof. The local routing policy can be trained by two methods: 1) supervised learning with shortest path distance known or 2) deep reinforcement learning. Experimental results show that the trained policy outperforms the greedy routing algorithm."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper tackles a critical routing problem in computer network, either in wireless or wired setting. The work presents the fundamental graph routing problem using generalized graphs and argues that one can train a DNN that has a local optimal policy with sampled seed graphs."
                },
                "weaknesses": {
                    "value": "The work presents theories and their proof. However, the overall writing is not easy to follow. In addition, the experimental results are limited. The work only considers 20 graphs in evaluation of the policies."
                },
                "questions": {
                    "value": "Can the proposed method used in graphs where the number of neighboring nodes can be different for every node? From the example and the formulation, it seems that the number of neighboring nodes is constant. If the number can vary, how do you setup your DNN and RL framework?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7891/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699081541604,
            "cdate": 1699081541604,
            "tmdate": 1699636968049,
            "mdate": 1699636968049,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Yrecu2ZIn2",
                "forum": "xRiZddh5Pb",
                "replyto": "DlWdDAiY48",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7891/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7891/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Q: Can the proposed method used in graphs where the number of neighboring nodes can be different for every node? From the example and the formulation, it seems that the number of neighboring nodes is constant. If the number can vary, how do you setup your DNN and RL framework?\n\nA: The proposed method takes $f_s(O,D,v)$, $f_a(O,D,u)$ as the input to predict the $Q$ value for an edge $(v, u)$. It is applied to each of the neighbors of node v, regardless of the degree of $v$, to predict the respective $Q$ values for all of its neighbors and to thus select a neighbor with the highest predicted $Q$ value as the next forwarder. Thus, it suffices for graphs whose nodes have different degrees."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7891/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700605558431,
                "cdate": 1700605558431,
                "tmdate": 1700605558431,
                "mdate": 1700605558431,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "M5i6KHcoLx",
                "forum": "xRiZddh5Pb",
                "replyto": "Yrecu2ZIn2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7891/Reviewer_zGzT"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7891/Reviewer_zGzT"
                ],
                "content": {
                    "comment": {
                        "value": "Can you please elaborate the implementation details if the work can be applied to graphs whose nodes have different degrees?"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7891/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631395519,
                "cdate": 1700631395519,
                "tmdate": 1700631395519,
                "mdate": 1700631395519,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gC2s3l2jMn",
                "forum": "xRiZddh5Pb",
                "replyto": "DlWdDAiY48",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7891/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7891/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Let us assume we can learn a model with a given training sample set from a seed graph $G^*$ (size= 50, density=2) whose nodes have a number of neighbors varying from 1 to 3. Let the data sample set be derived from node $v$ and its three neighbors $u_1$, $u_2$, and $u_3$. Thus, the data sample set includes respectively three X and Y vectors: $X = \\{\\langle [f_{s}(O, D, v),f_{a}(O, D, u_1)] , [f_{s}(O, D, v),f_{a}(O, D, u_2)], [f_{s}(O, D, v),f_{a}(O, D, u_3)] \\rangle\\}$; $Y= {\\langle [Q^*(v,u_1)],  [Q^*(v,u_2)], [Q^*(v,u_3)] \\rangle\\}$. \n\nNote that $Q^*(v, u)$ denotes the optimal $Q$ value (i.e., the negative length of shortest path from $v$ to $D$ passing through $u$).\n\nAgain, let us assume model $m'$ is learned with the data sample set to predict a Q value for a given a routing node $v$ and its neighbor $u$ associated with a origin-destination pair $(O, D)$. Our work enables to test the learned model in a graph with different size and density. Let the test graph $G'$ (size= 100, density=4) have node degrees varying from 1 to 5. For a routing node $v'$ in $G'$ and its five neighbors $u_i'$, $i = 1 ... 5$, we feed $[f_{s}(O', D', v'),f_{a}(O', D', u_i')]$ into the model $m'$ to output the single predicted $Q$ value for each $(v', u_i')$ pair, respectively. After the five times prediction for all $v'$'s neighbors $u_1'$, $u_2'$, $u_3'$, $u_4'$, and $u_5'$, we will receive five predicted $Q$ values. The neighbor with the highest predicted $Q$ value will be selected as the next forwarder."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7891/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700670338164,
                "cdate": 1700670338164,
                "tmdate": 1700671291548,
                "mdate": 1700671291548,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "pP2dG9cwn6",
            "forum": "xRiZddh5Pb",
            "replyto": "xRiZddh5Pb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7891/Reviewer_KPLY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7891/Reviewer_KPLY"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the problem of finding approximate all-pair shortest paths for 2D Euclidean and Hyperbolic random graphs. Specifically, the shortest paths should be returned in a localized way, such that given source and destination s and t and an intermediate vertex v, it returns the next \u201chop\u201d from v towards the nearly-shortest path to the destination.\n\nThe motivation to consider the two types of random graphs is that they are good models for capturing sensor networks and social networks.\n\nAt a high level, the proposed method is for every vertex v find a ranking of the neighbors of v using DNN. The crucial claim is that, if there exists a good enough *linear* ranking metric, then simply learning on a single seed graph would generalize to all graphs. Then the next step is to construct such a ranking metric for the two special graph families (i.e., Euclidean and Hyperbolic). The step of finding a ranking metric is via heuristic methods that are tested in the experiments."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The problem is well-motivated.\n\nThe method of learning from a single graph is an interesting proposal, and figuring out how/why this works is an interesting research problem.\n\nThe algorithms are designed in a systematic way through a framework (e.g., defining and analyzing RankPres ranking metrics) instead of testing a collection of ad hoc heuristics."
                },
                "weaknesses": {
                    "value": "The technical presentation is mathematically informal, and the statement of Theorems (and even definitions) is very unclear. This makes it extremely difficult to justify the correctness of the proofs/claims, which in turn questions the soundness of the proposed \u201clearning from a single graph\u201d method. I listed several concrete gaps in the proofs in the next \u201cQuestions\u201d section."
                },
                "questions": {
                    "value": "1. Page 3, you mention that V is inside a square of side length \\sqrt{n R^2 / \\rho} \u2014 This bound makes sense to me only when you restrict the random process in a finite R^2 area, instead of the entire Euclidean plane. However, you did not explicitly mention that the points are only drawn from R^2 area.\n\n2. In Section 2.1, the paragraph of \u201cThe APNSP Problem\u201d:\n\n- How is v quantified In the mapping \\pi(O, D, v)? Can v be arbitrary? What if v is far from any near-shortest path between O and D?\n\n- You mentioned \u201c\\xi(O, D) (1 + \\epsilon)\u201d is the user-specified factor \u2014 I think that only \\epsilon is what the user can specify, since \\xi(O, D) even depends on the algorithm which is designed after the user specifies the parameters.\n\n- What\u2019s the variables of the \u201cmax\u201d in equation (1)? Over all \\pi? And what about G \u2014 I think that G is randomized, so it does not make sense to take the max over G; instead, you may want to define (1) as something like max_\\pi E_G[Accoracy_{G, \\pi}].\n\n3. Page 4, second paragraph of section 3:\n\n- You said f_s and f_a maps from V, but then you used f_s(O, D, v) and f_a(O, D, u), whose parameters are not from V.\n\n- What is Q-value, did you define it exactly? At least you should provide some reference. This is important since your Theorems depend on Q-value, so the proof depends on the exact definition of it.\n\n4. Theorem 1: \n\n- How small can the V\u2019 be? I think V\u2019 = V seems to always work, and not restricting the size of V\u2019 makes this claim useless (?) \n\n- Can you give a formal definition of the meaning of \u201clearnable\u201d in this context?\n\n- From the proof of Theorem 1, it seems you want to say as long as the RankPres property holds for all v \\in V, and you just train H using any one v \\in V as in Theorem 0, then the H value on any other v\u2019 \\in V also preserves the ranking. This is a strong claim, and I don\u2019t see why this is true. The fact that Theorem 0 can yield a good H for a given v does not mean the *same* H can work for every other point v\u2019 \u2014 why couldn\u2019t it be that you apply Theorem 0 again on v\u2019 and you get a different H\u2019?\n\n5. Theorem 2:\n\n- Notice that Theorem 1 is applicable on one (fixed) graph G. But in the proof of Theorem 2, it seems you want to say you can apply Theorem 1 on an arbitrary graph, then wishes that it preserves the learnability property for *all* graphs simultaneously. This is a similar issue as in the proof of Theorem 1.\n\n6. In the paragraph immediately below Theorem 1 (and also a paragraph immediately below Theorem 2), you discussed the case when the RankPres property is not satisfied for all nodes, but for most of them, then \u201cwith high probability\u201d things can still work. I\u2019m not sure \u2014 what do you mean by \u201cwith high probability\u201d? What\u2019s the randomness in this context? Can you give some formal justification?\n\n\n7. In section 4.1, you mention you need to choose the seed graph carefully. However, in Theorem 2, it does not seem to have any restriction on the seed graph, which means any graph can make Theorem 2 hold. Then what\u2019s the point of discussing how to select a seed graph here? In what sense could it help you?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7891/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699173942651,
            "cdate": 1699173942651,
            "tmdate": 1699636967935,
            "mdate": 1699636967935,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "j0plYWxXDV",
                "forum": "xRiZddh5Pb",
                "replyto": "pP2dG9cwn6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7891/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7891/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Q: In section 4.1, you mention you need to choose the seed graph carefully. However, in Theorem 2, it does not seem to have any restriction on the seed graph, which means any graph can make Theorem 2 hold. Then what\u2019s the point of discussing how to select a seed graph here? In what sense could it help you?\n\nA: In Appendix B, we adopt Discounted Cumulative Gain (DCG) to formally define the ranking similarity and choose the seed graph that achieves the highest average ranking similarity across nodes. As evidenced by Figures 4-9, a good choice of seed graph is likely to exist in a set of graphs with modest size (e.g., 50) and high density (e.g., 5) in the Euclidean space and with high average node degree (e.g., 4) in the hyperbolic space."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7891/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700605445982,
                "cdate": 1700605445982,
                "tmdate": 1700605445982,
                "mdate": 1700605445982,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "lgufvhu7CL",
            "forum": "xRiZddh5Pb",
            "replyto": "xRiZddh5Pb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7891/Reviewer_w67p"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7891/Reviewer_w67p"
            ],
            "content": {
                "summary": {
                    "value": "The authors consider the problem of learning a local routing policy in a graph. This policy uses only information available at a node and its surrounding nodes. The authors present some results regarding conditions under which it is possible to learn the optimal ranking policy given a set of features. The authors formulate the routing problem as a deterministic Markov decision process, and propose a supervised learning procedure that estimates the optimal Q-values of a seed graph. The authors provide some computational experiments comparing the effectiveness of this procedure to a reinforcement learning approach and a greedy forwarding approach."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is, for the most part, clearly written and well-organized. The problem of learning local routing policies in a graph is fairly interesting."
                },
                "weaknesses": {
                    "value": "Theorems 0 to 2 have extremely strong assumptions. In particular, the RankPres property is a very strong property. These results could be strengthened greatly by first identifying some weaker property, and showing some weaker result under this property. For example, you could perhaps show that there is some probability that the RankPres approximately holds, and then have some result that shows that there is some policy with some bounds on the error that can be obtained in such a case. For this reason, Theorem 0 is nearly trivial.\n\nThe authors have not clearly defined the phrase \"learnable\", which plays a key role in Theorems 0, 1, and 2, but I believe that Theorem 1 is false for any reasonable interpretation of this phrase. For example, one could define a graph wherein there is a node $v$ with a unique neighbor $u$. Then, the training sample for the singleton set ${v}$ would consist of a singleton set $\\{(X_v, Y_v) \\}$. This isn't enough to characterize a relationship between the local features and the Q-value function.\n\nThe authors call \"Proposition 1\" a proposition, but provide only numerical experiments that they claim validate this proposition. Further, these numerical experiments do not validate this proposition at all. The proposition claims that a ranking metric that satisfies the Rank Pres property exists for almost all nodes in almost all graphs G. The numerical experiments show that a particular ranking metric approximately satisfies this property for a large proportion of graphs. This leaves a large gap. I am almost certain that this proposition is false as written, unless an atypical meaning is given to the phrase \"almost all nodes in almost all graphs G\". It seems that Euclidean graphs that do not admit a RankPres metric based on the given features should occur with non-zero probability. I suspect that this proposition holds asymptotically, both as $n \\to \\infty$ and as $\\rho \\to \\infty$, but this remains to be shown.\n\nThe choice of seed graph seems like it should be quite important, but I could not find any details on how this graph was selected. Is it just randomly generated with the parameters listed in Table 3?\n\nThe accuracy metric used by the authors is somewhat strange. It is okay to show some results using this metric, but it would be better to additionally show results that are instead based on a more normal measure of performance, such as the average/median ratio between the path achieved and the shortest path."
                },
                "questions": {
                    "value": "In theory, as density of the graph increase, the greedy policy should become close to optimal. However, this is not reflected in Fig (a) of the computational results. Can you explain the discrepancy?\n\nHow is the seed graph selected?\n\nA much more natural accuracy metric would be to define $\\eta(O,D) = \\begin{cases}1 \\textup{ if } \\frac{d_p(O,D)}{d_{sp}(O,D)} \\leq 1+\\epsilon, \\\\\\\\ 0 \\textup{ otherwise}. \\end{cases}$\n\nWhy did the authors additionally include the factor $\\zeta(O,D)$ in their accuracy metric?\n\nThe authors seem to assume that a linear model in the features would be sufficient to produce an optimal policy. If this is the case, why bother with an entire deep neural network, rather than a simpler model (such as a linear model)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7891/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7891/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7891/Reviewer_w67p"
                    ]
                }
            },
            "number": 6,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7891/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699393264599,
            "cdate": 1699393264599,
            "tmdate": 1699636967812,
            "mdate": 1699636967812,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lmzdty8we7",
                "forum": "xRiZddh5Pb",
                "replyto": "lgufvhu7CL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7891/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7891/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Q: Why did the authors additionally include the factor \u03b6(O,D) in their accuracy metric?\n\nA: The use of path stretch \u03b6(O,D) in Equation 2 is to normalize the margin for shortest paths prediction (given \u03f5 is constant).  The results in [2] show that the variance of path stretch becomes more significant as the network density \u03c1 decreases. In sparse graphs (e.g., \u03c1=1.4, 2), the path stretch can vary in [1.0, 10.0] since some of the shortest paths is prone to be found around the holes. However, the path stretch in dense graphs (e.g., \u03c1=4, 5) is likely to vary only within the range of [1.0, 3.5]. Thus, we exploit \u03b6(O,D) to mitigate the gap between sparse and dense networks for APNSP prediction .\n\n[2] Chen, Yung-Fu, Kenneth W. Parker, and Anish Arora. \"QF-Geo: Capacity Aware Geographic Routing using Bounded Regions of Wireless Meshes.\" arXiv preprint arXiv:2305.05718 (2023)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7891/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700605398442,
                "cdate": 1700605398442,
                "tmdate": 1700605398442,
                "mdate": 1700605398442,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]