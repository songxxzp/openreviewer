[
    {
        "title": "Loci-Segmented: Improving Scene Segmentation Learning"
    },
    {
        "review": {
            "id": "niKBfPaxpm",
            "forum": "r0BcyqWAcj",
            "replyto": "r0BcyqWAcj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5948/Reviewer_wWut"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5948/Reviewer_wWut"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes Loci-s (Loci-Segmented) to tackle the problem of slot-oriented scene representation. Loci-s is an extension of the Loci architecture with structure change, additional inputs and etc. The proposed methods shows state-of-the-art performance on the challenging MOVi-E dataset, demonstrating its ability to deal with complex environments."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1)This paper extends the Loci model to Loci-s with several innovations.\n2)The advancements in Loci-s collectively contribute to a 32.06% relative improvement in IoU on the challenging MOVi-E dataset compared to state-of-the-art models like SAVi++."
                },
                "weaknesses": {
                    "value": "1)Since the proposed method is built upon Loci, I think there should be more comparisons between Loci and Loci-s in the experimental section.\n\n2)The authors mentioned that instead of the residual structure used in Loci, the encoder and decoder subnetworks in Loci-s have been revamped to adopt a ConvNeXt-like architecture. I wonder how much performance improvement is brought by this structure change. \n\n3)\u201cThe third methodology involves the deployment of a specialized segmentation network akin to YOLACT (Bolya et al., 2019). \u201d How is this segmentation network trained, and what is its performance?\n\n4)The proposed method incorporates some additional input information for performance boost, e.g. the segmentation input (seg), depth map (sd). How much is the time cost? \n\n5)(NOT IMPORTANT) Seems there is a missing reference (shown as ?) in sentence \u201cLoci is rather closely related to other slot-based object processing architectures (Elsayedet al.; ?; Kipf et al., 2022; Wu et al., 2023)...\u201d"
                },
                "questions": {
                    "value": "Same as weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5948/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698721951894,
            "cdate": 1698721951894,
            "tmdate": 1699636634121,
            "mdate": 1699636634121,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PyYHI7xbmr",
                "forum": "r0BcyqWAcj",
                "replyto": "niKBfPaxpm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5948/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5948/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer to Reviewer wWut"
                    },
                    "comment": {
                        "value": "Please also refer to our reply to all reviews for a clarification of the key merits of our work. \n\nThank you for your insightful comments. Here are our replies: \n\n1) The original Loci cannot be trained with dynamic backgrounds (it uses a Gaussian-Mixture-Model to extract one single background for training- and test set) and therefore a direct comparison is not applicable. \n\n2) ConvNeXt has proven to be the superior residual CNN by the original paper (A ConvNet for the 2020s) we therefore adapted this superior design since it also nicely fits with our Hyper-Convolution approach. Although it would certainly be an interesting ablation we kindly ask the reviewer to consider that it might be out of scope (and energy wasteful) for us to re-ablate already established designs.\n\n3) The segmentation network was trained supervised using a Cross-Entropy loss with an optimal instance mask assignment (computed with the Hungarian algorithm on the IoU). We now also detail this in the methods and report the segmentation accuracy in the appendix.\n\n4) There is no real additional cost besides training the additional segmentation network. All methods (rnd), (reg) and (seg) are used in a warm-up phase where the first frame is shown during 10 cycles that involve only the encoder and decoder. The computational cost of the segmentation network is comparable to one additional slot in Loci-s."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5948/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700529189871,
                "cdate": 1700529189871,
                "tmdate": 1700529189871,
                "mdate": 1700529189871,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mcygNkr8OQ",
            "forum": "r0BcyqWAcj",
            "replyto": "r0BcyqWAcj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5948/Reviewer_xtqd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5948/Reviewer_xtqd"
            ],
            "content": {
                "summary": {
                    "value": "This work focuses on the compositional scene representation and proposes a scene segmentation neural network based on the previous model named Loci. To build their model, they extend Loci with three modifications, including a pre-trained dynamic background\nmodule, a hyper-convolution encoder module, and a cascaded decoder module. Extensive experiments conducted on the MOVi dataset show the effectiveness of the proposed method. Besides, the proposed method can generate well-interpretable latent representations and may serve as a foundation-model-like interpretable basis for solving downstream tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Good performance. The proposed method achieves good performance on the MOVi dataset.\n2. The proposed can generate well-interpretable latent representations, which is helpful in building interpretable foundation models."
                },
                "weaknesses": {
                    "value": "To ACs and authors: I am not an expert in this field and cannot find any strong reasons to reject this work. Please refer to other reviewers' comments for rebuttal and decision."
                },
                "questions": {
                    "value": "None."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5948/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5948/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5948/Reviewer_xtqd"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5948/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698772811864,
            "cdate": 1698772811864,
            "tmdate": 1699636634000,
            "mdate": 1699636634000,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "w0cjJehhFL",
                "forum": "r0BcyqWAcj",
                "replyto": "mcygNkr8OQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5948/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5948/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer to Reviewer xtqd"
                    },
                    "comment": {
                        "value": "Thank you for your encouraging comments. Please refer to our replies to all reviewers for a detailed summary of further improvements and the key highlights of our work for your convenience."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5948/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700529019220,
                "cdate": 1700529019220,
                "tmdate": 1700529019220,
                "mdate": 1700529019220,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "VYSDH7p4Pn",
            "forum": "r0BcyqWAcj",
            "replyto": "r0BcyqWAcj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5948/Reviewer_czo6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5948/Reviewer_czo6"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an architecture for unsupervised scene segmentation given RGB or RGBD video input. The method builds on the \"Loci\" paper, but re-designs many components and adds in a pre-trained foreground/background segmentation model. The main result is that this combination of changes greatly improves results, both qualitatively and quantitatively."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Quantitatively the method here clearly outperforms prior work (on the mIOU metric) in the the MOVI-* datasets."
                },
                "weaknesses": {
                    "value": "Overall this paper is very difficult to follow. The \"Loci\" method, on which this is based, is never quite made clear on its own, and then every subsequent section makes big changes to the architecture without much motivation, and without a connecting story or high-level idea. \n\nThe section on the \"Background Module\" never mentions this, but the abstract and the section on \"Segmentation Preprocessing\" describe the background module as \"pre-trained\", apparently for a segmentation task that \"distinguishes foreground entities from the background context\". My guess is that much of the performance gain is coming from this. \n\nI have a variety of smaller questions which the authors may like to answer, but overall it seems to me that this paper needs a very heavy rewrite."
                },
                "questions": {
                    "value": "What are Gestalt codes? \n\nWhat are the two predictions about object positions? The paper says \"we introduce a dedicated background processing module that generates both predictions about object positions as pixel densities\".\n\nThe paper mentions using something called \"GateL0RD units\" but these are never really described. \n\nThe paper says that the \"Gestalt codes are binarized to create an information bottleneck that fosters the development of factorized compositional entity encodings.\" I am unclear on why binarization will make the representation compositional. \n\nSection 2.1 focuses on improving Loci's \"object tracking abilities\", but the earlier section (describing Loci) never mentioned any object tracking happening, and tracking is never mentioned again. What is the idea here? \n\nThe paper mentions that the decoder \"reconstructs the predicted scene via slot-wise density maps as object masks.\" What are slot-wise density maps? \n\nThe paper briefly mentions an \"L0 loss on gate openings\" but it is not clear what ground truth is used for this loss. Is it maybe just a regularization term, penalizing the L0 norm?\n\nSection 2.2 introduces a depth input and an equation to normalize it, but it is not clear where this fits with the inner loop described in the previous section.\n\nFor Table 2 it would be great to clarify what dataset these experiments happen in, and what the metrics are."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5948/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698814579513,
            "cdate": 1698814579513,
            "tmdate": 1699636633877,
            "mdate": 1699636633877,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ExRpjqXIBL",
                "forum": "r0BcyqWAcj",
                "replyto": "VYSDH7p4Pn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5948/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5948/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer to Reviewer czo6"
                    },
                    "comment": {
                        "value": "Please also refer to our reply to all reviews for a clarification of the key merits of our work. \n\nThank you for your insightful comments. Here are our replies: \n\n - We polished the overall presentation (see answer to all reviewers) and apologize for the mediocre accessibility of some of the novel components in the previous version of the paper. \n\n - The Background module indeed is trained supervised using  ground truth foreground masks. We now clarify this in the methods. Clearly, slot attention methods like Loci greatly benefit from an accurate background module. \n\n - Gestalt-Codes are a compressed latent vector that encodes position invariant object properties needed to reconstruct the object at any location determined by the Position-Code. We also make this more clear now in our section on the (base) Loci Architecture.\n\n - We clarified the sentence about \"pixel densities\". It should now be much clearer that the Uncertainty Network as part of the Background Module computes a foreground mask, which is learned supervised.\n\n - GateL0RD units are LSTM-like recurrent cells with an even stronger shielding. We now further clarify this in the paper.\n\n - The binarization of Gestalt codes was introduced in the original Loci paper as a strong bottleneck that outperformed a variational one, but since it is not really relevant for the understanding of Loci-s we removed this sentence. \n\n - The main objective of the original Loci paper was object tracking trough occlusions, but since Loci-s focuses on object segmentation we removed the statements regarding object tracking.\n\n - Slot-wise density maps are the instance segmentation masks computed by each slot. We clarify this now.\n\n - The L0 loss on gate openings is a regularization loss that punishes any gate values except zero with the same gradient. But since this inner loop is not relevant for the segmentation task we moved it to the appendix. This also puts the depth input into a more consistent story line.\n\n - We added descriptions for the datasets and metrics from the compositional scene understanding paper."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5948/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700528944170,
                "cdate": 1700528944170,
                "tmdate": 1700528944170,
                "mdate": 1700528944170,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "plutJtupp2",
            "forum": "r0BcyqWAcj",
            "replyto": "r0BcyqWAcj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5948/Reviewer_vymZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5948/Reviewer_vymZ"
            ],
            "content": {
                "summary": {
                    "value": "The paper extends the location and identity tracking architecture Loci to scene segmentation by adding a pre-trained dynamic background\nmodule, a hyper-convolution encoder module, and a cascaded decoder module. The proposed method and each components are validated to be effectiveness by extensive experiments."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The experiments are extensive."
                },
                "weaknesses": {
                    "value": "1. The work is a little incremental, compared to Loci, so that its novelty is slim.\n2. The principle and motivation of the proposed modules are not clearly explained."
                },
                "questions": {
                    "value": "see weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5948/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699195009811,
            "cdate": 1699195009811,
            "tmdate": 1699636633732,
            "mdate": 1699636633732,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3p6Lzb5swo",
                "forum": "r0BcyqWAcj",
                "replyto": "plutJtupp2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5948/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5948/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer to Reviewer vymZ"
                    },
                    "comment": {
                        "value": "Please refer to our objection with respect to the incremental nature of our work in the general reply. \n\nWe agree that we did not motivate the novel models sufficiently well and have done so now (see paper blue passages). To summarize: \n- Background Module: Necessary to effectively distinguish complex dynamic backgrounds from foreground objects, addressing a key limitation in the original Loci system, which only worked if the background was static for the whole training and test set.\n- Hyper-Convolution Encoder: To integrate top-down feedback, enhancing object-specific encoding and improving bottom-up processing. We have an ablation in the appendix that shows a significant improvement in IoU and depth reconstruction for Hyper-Convolutions.\n- Cascaded Decoder: To sequentially generate object masks, depth maps, and RGB reconstructions, leading to more accurate and detailed segmentations.\n\nThese modules are integral to the enhanced performance of Loci-s in complex scene segmentation, marking a critical step beyond the capabilities of the original Loci architecture."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5948/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700528640624,
                "cdate": 1700528640624,
                "tmdate": 1700528640624,
                "mdate": 1700528640624,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]