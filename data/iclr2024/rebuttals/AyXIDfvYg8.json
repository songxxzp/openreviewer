[
    {
        "title": "Symmetric Neural-Collapse Representations with Supervised Contrastive Loss: The Impact of ReLU and Batching"
    },
    {
        "review": {
            "id": "mTiEmUecpk",
            "forum": "AyXIDfvYg8",
            "replyto": "AyXIDfvYg8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9066/Reviewer_ScWC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9066/Reviewer_ScWC"
            ],
            "content": {
                "summary": {
                    "value": "This paper applies RELU activation in the last layer of models in supervised contrastive learning (SCL). With that, SCL can learn representations that converge to the OF geometry irrespective of the level of class imbalance. The author theoretically and empirically verify the effectiveness of this method. Besides, this paper finds that the batch selection is important in representation geometry and they design a batch selection strategy (batch-binding)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The theoretical analysis and empirical results cooperate well. Figure 1, 2, 3 empirically demonstrate the advantages of the additional RELU function.\n2. The motivation is clear and the paper is written well.\n3.  The proposed method is simple and effective."
                },
                "weaknesses": {
                    "value": "1. Based on the empirical results in the paper (Table 1), the improvements in the test accuracy on CIFAR-100 are not significant, even when the imbalance ratio is large. It would be better to show the advantages and disadvantages of learned representations in different downstream tasks (e.g., Does SCL+ReLU performs better in fine-tuning tasks?).\n2. It would be better to conduct some additional experiments on large scale datasets (e.g., ImageNet100).\n3. It seems that batch size is an important argument when applying sample selections according to the theoretical analysis. It would be better to add the ablation study on that.\n4. Based on the analysis, it seems that various activation functions can achieve the similar effect as ReLU. Is it possible to add a discussion about that?"
                },
                "questions": {
                    "value": "See my comments above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9066/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698661519201,
            "cdate": 1698661519201,
            "tmdate": 1699637141879,
            "mdate": 1699637141879,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uXoWMOstgR",
                "forum": "AyXIDfvYg8",
                "replyto": "mTiEmUecpk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9066/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9066/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response to Reviewer ScWC"
                    },
                    "comment": {
                        "value": "Thank you for constructive feedback and insightful questions. Please find our response below in addition to the global response:\n\n**Batch size:**\nThanks for highlighting the role of batch size. Indeed, for contrastive loss and in the analysis of our model, batch size plays an important role in defining the set of global solutions. This is the case as the batching scheme specifies the interactions among samples during training. Our theoretical framework outlines the necessary and sufficient conditions for the batching to achieve a unique minimizer for contrastive loss. Following your suggestion, we conducted additional experiments for varying batch sizes to empirically explore its impact on the final learned embeddings. \nThe detailed results of these new experiments are presented in Section E.4.3 in the appendix of the revised draft. In these experiments, we examine the effect of batch size in two scenarios, depending on whether ReLU is applied on the final embedding layer or not. We compare the learned features with OF when ReLU is present at the final layer, and with ETF (as predicted by Graf et. al (2021)) when ReLU is removed. Our observations indicate that the presence of ReLU reduces the reliance on batch size, leading to consistent convergence to OF. In contrast, in the absence of ReLU, large batches are necessary to achieve the global optimizer ETF. We also note that Khosla et.al (2020) argue that increased batch sizes lead to improved performance. In particular, they typically use batch sizes extending up to 6144. Our findings suggest that with the addition of ReLU, one can potentially achieve comparable performance with smaller batch sizes, which offers venues for reducing the training computational cost.\n\n**Activation functions:**\n\nThank you for bringing light to this subtle point. We considered ReLU as the primary focus of our study in terms of the activation function due to its simplicity and frequent use in practice. While it is interesting to study different activation functions, our motivation was to identify that a simple and commonly used activation function can significantly alter the implicit geometry of features.\n\nThat said, we agree that this is indeed an interesting question. The theoretical analysis only assumes that the post-activation features are non-negative and span the region $[0, \\infty]$. It is reasonable to anticipate that activation functions satisfying this requirement lead to the same result. However, the non-linearity of the activation function is likely to play a role in the optimization dynamics and needs further investigation. We have planned to conduct specific experiments and report our findings in the paper."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9066/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700481933248,
                "cdate": 1700481933248,
                "tmdate": 1700481933248,
                "mdate": 1700481933248,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fUiIqobnuX",
                "forum": "AyXIDfvYg8",
                "replyto": "mTiEmUecpk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9066/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9066/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer,\n\nThank you for your time and effort in reviewing our work. We appreciate your useful inputs, and have made sincere efforts to address your comments. Thank you for the constructive suggestions of batch-size ablation studies and discussion on other activation functions. These discussions will be useful additions to our paper. On the test accuracy discussion, we would like to refer you to our global response (including additional results on gains in worst-class accuracy reported in Sec. E.8 in the revised appendix). If you have more questions, we\u2019re happy to provide further information.\n\nWe would appreciate it if you would consider re-evaluating your rating."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9066/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700712988905,
                "cdate": 1700712988905,
                "tmdate": 1700713189939,
                "mdate": 1700713189939,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "VmYxQe05vC",
            "forum": "AyXIDfvYg8",
            "replyto": "AyXIDfvYg8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9066/Reviewer_q7qd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9066/Reviewer_q7qd"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a variants of SCL loss to restore the symmetrical geometry of the class-mean learned embeddings in the presence of unbalance class samples, just adding a ReLU activation after the projection layer. It theoretically demonstrates that due to the existence of ReLU, the last-layer feature embeddings of samples with the same labels will be finally aligned, while that of samples with different labels will be orthogonal, when we employ the full-batch training.  While implement mini-batch training, it also showcases that mini-batch selection strategy will significantly influence the learned embedding geometry."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The proposed approach stands out for its simplicity and effectiveness. It offers valuable insights into the subject matter, shedding light on the restoration of symmetrical geometry in the presence of unbalanced class samples.\n\n - The theoretical analysis is well-founded, enhancing the credibility of the approach. Experimental results, while limited to relatively simple datasets like MNIST, CIFAR10/CIFAR100, and TinyImageNet, do support the method's efficacy."
                },
                "weaknesses": {
                    "value": "The main concerns still lie in the experimental parts.\n\n- The experiments primarily utilize CNN-based architectures. It would be beneficial to explore the applicability of this approach to train other architectures, such as Transformers, to gauge its versatility across neural network types.\n\n-  While the proposed approach demonstrates superiority on simpler datasets, questions remain regarding its performance on more complex and larger datasets. The potential impact of the non-negative constraints imposed by the ReLU activation on representation ability in more intricate tasks needs further investigation."
                },
                "questions": {
                    "value": "The minimum of the loss is not  zero and varies with the number of samples in different classes."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9066/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698845796061,
            "cdate": 1698845796061,
            "tmdate": 1699637141778,
            "mdate": 1699637141778,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5CQsGZ4ggY",
                "forum": "AyXIDfvYg8",
                "replyto": "VmYxQe05vC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9066/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9066/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your appreciation of our messages and analysis, and for your constructive feedback. Please find our response below in addition to the global response:\n\n**Additional experiments on transformer architecture**:\nThank you for the suggestion of extending the experiments to transformer-based architectures. Motivated by the question, for the revised version of our paper, we have included additional experiments on vision transformers (see Fig.22). Based on the observations, our hypothesis of OF geometry holds here as well, thus solidifying the claim of the generality of our results across various architectures.\n\nQuestions:\n\n**The minimum of the loss is not zero and varies with the number of samples in different classes:**\n\nThat is correct. Due to fixing the norm of the features to a constant value (say 1), the loss does not go to 0. The minimum value is exactly given by the lower bounds in Eq (3) and (4), respectively, for full-batch and mini-batch optimizations. For full batch SCL, we show the loss evolution in a DNN experiment, where the loss is seen to converge exactly to our lower bounds in Theorem 1. This prediction is made possible due to our theoretical analysis.\n\nTo demonstrate the effectiveness of our Theorem 1, in Fig 3, we show how closely the empirical loss in a full-batch DNN experiment converges to the predicted lower bound."
                    },
                    "title": {
                        "value": "Author response to Reviewer q7qd"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9066/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700481506432,
                "cdate": 1700481506432,
                "tmdate": 1700481623921,
                "mdate": 1700481623921,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NQi7iZ7uqA",
                "forum": "AyXIDfvYg8",
                "replyto": "5CQsGZ4ggY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9066/Reviewer_q7qd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9066/Reviewer_q7qd"
                ],
                "content": {
                    "title": {
                        "value": "The  concern regarding the representation ability"
                    },
                    "comment": {
                        "value": "My concerns still revolve around the representation ability of UAM+. Assuming that the feature of the last layer is $n$-dimensional, the representation space of the feature is constrained to only the positive space due to the existence of UAM+, reducing it to $\\frac{1}{2^n}$ of the original representation space, shrinking at an exponential rate. Thus, it raises questions about whether the proposed method is sufficient for learning from large and complex datasets. I raised this concern with the authors during the original review, requesting them to conduct experiments on large-scale datasets to address this question, but it appears that the authors have not implemented these experiments."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9066/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700629341687,
                "cdate": 1700629341687,
                "tmdate": 1700629341687,
                "mdate": 1700629341687,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gqCk4bqD0L",
                "forum": "AyXIDfvYg8",
                "replyto": "VmYxQe05vC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9066/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9066/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response!\nPlease allow a couple of remarks:\n1. First, because of neural collapse (i.e. features of each class collapse to their class means), the features lie on a k-dimensional subspace. In other words, and following the reviewer\u2019s notation for convenience (i.e. n for the feature dimension, and, say, N for the train-set size and k for the number of classes) the rank of the (N x n) feature matrix H is k. Thus, there is no \u201cshrinking of the representation space.\u201d\n\n2. Second, the above holds irrespective of the inclusion or not of ReLU. In fact,  even if ReLU is not present and one considers a simplex ETF geometry for which features are maximally separated, then the cosine of the angle between the features is -1/(k-1). Thus, the cosine actually approaches 0 (same as the angles of OF), as the number of classes k increases.\n\n3. Third, let\u2019s even consider the case of CE optimization on balanced data, i.e. the setting of the original work by [Papyan et al. (2020)] and a long list of follow-up papers. In that setting, again, the features of the last layer usually undergo a ReLU nonlinearity in most common architectures (eg. ResNet). Hence, your concern would still apply to that setting, which is however arguably well-established by now in the community.\nFor the above reasons, there are **no** concerns with regards to the \u201csufficiency of the method for learning from large and complex datasets.\u201d\n\nWith regards to additional experiments: we kindly note that our paper identifies a new phenomenon, which we not only theoretically justify, but also experimentally confirm on four different datasets (MNIST, CIFAR10, CIFAR100, TinyImagenet) of both STEP and LT imbalance trained on four different architectures (MNIST, VGG, ResNet, ViT). Given the range of scales and complexities of these datasets/architectures, we believe this forms sufficient evidence in support of the identified phenomenon. We also kindly note that the breadth of the experiments is consistent with the rest of the literature on neural collapse phenomena (e.g. Graf et al. (2020), Ji et al. (2022), Sukenik et al. (2023), Thrampoulidis et al. (2022), Yaras et al. (2023), Zhu et al. (2021), Zhou et al. (2022a), Zhou et al. (2022b) etc). Please note that our experimental evidence is more extensive than several prior works. As a matter of fact, and to the best of our knowledge, we are the first to demonstrate neural-collapse geometry convergence for a transformer architecture (ViT). (Thanks again for your suggestion!)\n\nPS: By \u201cUAM\u201d, we believe the reviewer is referring to UFM (Unconstrained Features Model)"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9066/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700638983285,
                "cdate": 1700638983285,
                "tmdate": 1700639042800,
                "mdate": 1700639042800,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gvNCk2WacJ",
                "forum": "AyXIDfvYg8",
                "replyto": "gqCk4bqD0L",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9066/Reviewer_q7qd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9066/Reviewer_q7qd"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your further clarifications. In the era of \"large model\" and \"big data\", I still think the experimental results for training ResNet-18/34 and a two-layer ViT on MNIST, CIFAR10, CIFAR100 and Tiny ImageNet is not so convincing. Hence, I will keep the score."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9066/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700670215052,
                "cdate": 1700670215052,
                "tmdate": 1700670215052,
                "mdate": 1700670215052,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ZtWg9l9MXB",
            "forum": "AyXIDfvYg8",
            "replyto": "AyXIDfvYg8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9066/Reviewer_BEvk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9066/Reviewer_BEvk"
            ],
            "content": {
                "summary": {
                    "value": "## Background summary \nThis paper focus on the issue of neural collapse (NC), when hidden features of sample class collapse into one another, and orthogonal framing (OF), when the mean vectors of hidden representation of each class, are orthogonal to one another. For the theoretical framework, the paper chooses the unconstrained features model (UFM), where the loss is minmized over the classification layer and last hidden layers, unconstrained from the other model parameters. \n- *UFM with cross entropy loss:* we have \n$\\min_{w_c,h_i\\in R^d}\\mathcal{L}_{CE}(\\\\{w_c\\\\}, \\\\{h_i\\\\})$ \n- *UFM with supervised contrastive loss (SCL)* for full batch (for mini-batch, it is over $i\\in [n]$):  \n$\\min_{h_i\\in R^d} \\sum_{i\\in B} \\frac{1}{n_{B,y_i} - 1} \\sum_{j\\in B, y_i=y_j, i\\neq j} \\log\\left(1 + \\sum_{\\ell\\neq i, j} \\exp(h_i^\\top h_\\ell - h_i^\\top h_j)\\right)$\n- *Symmetrical solutions under balanced classes.* Intuitively, when classes are balanced (same number of samples per class) both cross entropy & contrastive loss, repel the samples from seperate classes, while absorb the class within one class. This has the effect of the finding the geometry that is symemtric w.r.t. to the class identity, resulting in a equiangular tight frame (ETF). This attraction/repulsion can be seen in the logits $\\sum_{\\ell\\neq i} \\exp(h_i^\\top h_\\ell)$\n- *Non-symmetrical solutions when classes are not balanced*  When the classes are not balanced, the symmetry between class will be broken. For example, if one class completely dominates the training set, both cross-entropy and contrastive loss will lead to solutions, where the majority class is almost anti-parallel to other class means. \n\n## Main contribution: symmetric solutions by passing last hidden layer through ReLU\nThe main contribution of the paper is showing that if the spectral loss is optimised over the space of all-positive hidden layers, namely by passing it through a ReLU activation, denoted as UFM_+, the UFM leads to symmetrical solutions, i.e., NC & OF properties are held again. \n \n- Thm1 proves that for the full batch contrastive loss, if we additionally pass the hidden layers through a ReLU, ie, optimize  last hidden layers over elementwise positive vectors $h_i\\ge 0$, global minima is achieved when for all $i,j \\in [n]$ we have $h_i^\\top h_j = 1(i=j)$. \n- Thm2. proves a similar property for mini-batch contrastive loss, where the collapse and orthogonality property hold within each batch (for samples within each batch $B\\in\\mathcal{B}$ and pair within batch $i,j\\in B$, we have $ h_i^\\top h_j = 1(i = j)$. \n- Cor 2.1: shows a batching propery that if held, the neural collapse and orthgonal frame property hold across the full batch, which can be explained as 3 conditions: \n   1) every batch has a sample from each class \n   2) The batch connectivity graph is connected (nodes $u$ are samples $u\\in [n]$, edges represent $u$ and $v$ appeared in the same batch $u,v\\in B$)\n 3) The sub-graphs corresponding to each class are also connected\n- The paper validates the theories on various ResNet architectures. The experiments show two variations of unbalanced datasts (step and long-tailed) which seem to show that the proposed approach (passing last layer throgh ReLU) does indeed lead to symmetric solutions.\nThe experiments also include a result (Table 1) that shows the proposed approach leads to a higher test accuracy."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The presentation. The presentation of results, both in terms of writing, mathematical notation and formula, and the figures, is exquisite. The authors take many positive steps in ensuring that the concepts are clear and simple to grasp for the reader. \n2. The selection of the research problem and the ideas presented by the author are (to the best of my knowledge) original. The simplicity of the ideas as well as the presentation of the results make the results intriguing to read."
                },
                "weaknesses": {
                    "value": "Main issue\n- While the stated objective of the paper, i,e, embeddings that satisfy NC & OF properties, is clear, it not at all clear  that it is a justified goal. For example, consider the simple case of binary classification. The optimal solution without ReLU will map two classes to some anti-parallel vectors $u$ and $v = -u$. This means that the hidden representations for one class will have the maximum degree of separation $\\| u - v\\| = 2$. Now, if we pass them through ReLU, forcing them to be element-wise positive, the solutions will be instead some $u$ and $v$ such that $u^\\top v = 0$. Thus, the their mean-class distance will be $\\| u - v\\| = \\sqrt{2}.$ Without any formalities, what doesn't seem to be a benefit in terms of distance, doesn't seem to be a good approach for generalization either. Even if we make the two classes here very unbalanced, still the anti-parallel geometry seems to be better than the orthogonal approach.  Since the authors do not make any formal/theoretical comments on this topic, the only relevant evidence they provide in this direction is Table 1 test accuracy results. Because of the aforementioned reasons, I consider the results of table 1 to be particularly surprising. Thus, if the the main evidence to support utility of the approach is empirical, I would like much more substantive experiments. Some elaborate explanations or at least speculative comments by the authors will also go a long way to explain the benefits of the orthogonal mean-classes. \n- My second main issue is about the novelty and significance of the contributions. The theory, while stated clean and clearly, is rather thin. The main theorems are not very surprising, and are rather straightforward. As elaborated in the questions section, If I'm not mistaken, similar results can be replicated for the cross-entropy loss in a similar straightforward fashion. On the empirical front, while the results are definitely interesting (notably Table 1), they are not comprehensive to solidify that point. For example, if this approach has any benefits for improving test accuracy, there should be much broader experiments, covering different model architectures, datasets, and most importantly, competing methods, that deal with unbalanced classes. I am not advocating here for state-of-the-art performance. However, it is important to see the empirical results in the context of prior/similar work, so that the reader may assess the empirical benefits for themselves. That said, I am open to reconsidering my views (both on theory and empirical contributions) upon hearing authors responses. \n\nMinor issues: \n- I think there is a slightly overuse of abbreviations in this paper which make it slightly hard to read. For example, I had to go back several times to read what ETF is, or other notations. If possible, please keep the shorthands to a minimum. \n- There is a small discrepancy between eq(1) and eq(2) in the presence of (1+ ... ) within the logarithm and the summation is over $\\ell\\neq i, j$ while another is over $\\ell \\neq i$. Perhaps authors can comment on this? \n- In Figures 2 & 5 axes are not clearly defined, eg. caption can say x-axis ... y-axis ..."
                },
                "questions": {
                    "value": "While the authors present the ideas for symmetrising non-balanced the contrastive loss, to me it seems like the cross-entropy loss will benefit from the same approach. For example, consider the CE loss for $c$-class classification for the full batch is given by:\n$$\\mathcal{L} =\\sum_{i=1}^n\\log\\left( \\frac{\\exp(h_i^\\top w_{y_i} ) }{\\sum_k^c \\exp(h_i^\\top w_k)}\\right)$$\nIt is rather trivial to see that if we pass the last hidden layer through ReLU, since that elementwise condition $h_i\\ge 0$ ensures that $w_k$'s are also positive, and the resulting inner products $h_i^\\top w_i$ will be always non-negative, and the global optimum is achieved when neural collapse and orthognal frames are achieved. For example, if the hidden dimension is larger than number of classes $h,w_k\\in R^d, d\\ge c,$ then we can set $w_1,\\dots, w_c$ to the standard basis $w_k := e_k$ and collapse hidden layers onto the corresponding vectors $h_i := e_{y_i}.$ Therefore, my questions are:\n- Do authors agree with my reasoning? Please correct me if my reasoning above is flawed. \n- If yes, why have authors left out cross entropy in their analysis, given that it's leading to the same result as the contrastive loss results?\n- Is there any pros/cons if we compare symmetrized CE loss, and contrastive loss, both by passing hidden layer through ReLU?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9066/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9066/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9066/Reviewer_BEvk"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9066/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698854718700,
            "cdate": 1698854718700,
            "tmdate": 1699637141673,
            "mdate": 1699637141673,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "X2mMW42CRW",
                "forum": "AyXIDfvYg8",
                "replyto": "ZtWg9l9MXB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9066/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9066/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response to Reviewer BEvk (Part 1)"
                    },
                    "comment": {
                        "value": "We thank you for their detailed review and questions. We are pleased to see your recognition of our efforts in presentation. Please find our detailed response below, in addition to the comments in the global response.\n\n**For example, consider the simple case of binary classification...**:\nThank you for prompting this discussion. The statement regarding antipodal features for binary SCL without ReLU is exactly correct. In fact, this holds in general for balanced multi-class setup as well: When the dataset is balanced Graf et al. (2020) shows that the embedding geometry will form an ETF in the absence of ReLU. In an ETF geometry, we have $\\frac{\\mu_c^\\top\\mu_{c'}}{\\|\\mu_c\\|\\|\\mu_{c'}\\|}=\\frac{-1}{k-1}$ for the mean-embeddings of any two classes. As opposed to OF where $\\frac{\\mu_c^\\top\\mu_{c'}}{\\|\\mu_c\\|\\|\\mu_{c'}\\|}=0$.\n\nTherefore, for ETF, $\\|\\mu_c-\\mu_{c'}\\|^2 = \\frac{2k}{\\tau(k-1)}$ and in the case of OF $\\|\\mu_c-\\mu_{c'}\\|^2 = \\frac{2}{\\tau}$. $\\tau$ is the temperature of the SCL loss. While the distance in the ETF geometry is larger for a fixed temperature parameter, we note that by simply scaling the temperature parameter $\\tau$ by $\\frac{k-1}{k}$, we can achieve an equivalent absolute distance in the learned geometry in the presence of ReLU. Thus, in terms of this measure, the addition of ReLU does not compromise the trained model.\n\nOn the contrary, as shown in Fig. 1, in the imbalanced setup, without ReLU, the minority classes are drawn closer to each other, yielding a reduced absolute distance between classes. The introduction of ReLU, however, maintains an equal pairwise distance between classes. Hence, in terms of this measure, the addition of ReLU is in fact helpful.\n\nFinally, as noted in Section D.1.2, centering the OF by $\\frac{1}{k}\\sum_c\\mu_c$ results in an ETF. So, in summary, the geometries learned in balanced or binary scenarios, with or without ReLU, can be considered equivalent, accounting for a shift and scaling.\n\n**Novelty and significance**\n\nThank you for appreciating the originality of our work and ideas. We believe that solving for the global minimizer feature-classifier configuration with the SCL loss under is non-trivial and novel. We elaborate below:\n\n1. Symmetric feature geometry: the first surprising observation that we support with theory is the fact that ReLU symmetrizes the feature geometry regardless of imbalances. To the best of our knowledge, such an outcome is unique to the SCL and stands out from several prior works considering class imbalance in CE and other losses. For e.g., Thrampoulidis et al. (2022), Fang et al. (2021) discuss that the feature and classifier geometry depends critically on the imbalance. We further identify the appropriate theoretical model for the observation, and solve for the exact geometry of the global optimizers and the optimal value of the loss function.\n2. Batching matters: the first study of global optimizers of the SCL, Graf et al. (2020) only considers the setting of balanced data. In their words, the theoretical analysis is based on \u201ca combinatorial argument which hinges on the balanced class assumption\u201d. The analysis therein requires the specific batching case of all combinations of examples in the dataset of a given batch-size. On the other hand, our mini-batch analysis is applicable to arbitrary batching schemes. We discover the batching conditions under which a unique minimizer geometry exists- this condition allows for a large class of batching schemes, and includes the specific case of Graf et al. (2020) as a special case. We leverage our theoretical results to concretely explain geometry observations made in DNN experiments as a function of batching schemes.\n\nWe discuss the question of possible extension to CE below, within the response to the specific question.\n\nMinor issues\n\n**overuse of abbreviations:** Thanks for the feedback. We have added a table defining all the important abbreviations used in the paper in one place at the beginning of the appendix.\n\n**eq(1) and eq(2):** Eq (1) and (2) are equivalent, in terms of the loss function. The apparent difference is only in presentation, in that in Eq (2), the sum over $l$ includes $j$, so that when $l = j$, the innermost exponential term becomes $\\exp(\\mathbf{h}_i^\\top\\mathbf{h}_l - \\mathbf{h}_i^\\top\\mathbf{h}_j) = \\exp(0) = 1$.\n\n******** Continued below *********"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9066/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700481047977,
                "cdate": 1700481047977,
                "tmdate": 1700481083765,
                "mdate": 1700481083765,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1jPfi7aKOQ",
                "forum": "AyXIDfvYg8",
                "replyto": "ZtWg9l9MXB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9066/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9066/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response to Reviewer BEvk (Part 2)"
                    },
                    "comment": {
                        "value": "Questions:\n\n**Question on CE:**\nIt turns out that adding ReLU at the final embedding layer does not guarantee that the classifiers $w_k$ learned by CE are positive. To see this, take a simple binary setting. In the binary setting, we can assume that we only search for a single classifier, as at the optimal solution $w_2=-w_1$. Now, suppose the embeddings $h_i, i=1:n$ are fixed to an OF: $h_i=e_{y_i}$, and further suppose the norm of the classifier is fixed (since only the direction matters for the final decision rule). Then, the max-margin classifier that optimizes the objective is $w\\propto[1,-1]$. In short, the optimal classifier can have negative entries despite the embeddings being constrained to the positive orthant.\n\nOur empirical observations also support this claim. First, we note that ResNet networks incorporate ReLU in the final embedding layer (so they fit in this setup). In our ResNet experiments, we have compared the embeddings learned by CE and SCL. First, in Fig. 7, we report the convergence of the embeddings to the OF, and we observe that CE learns embeddings that do not converge to the OF (despite the positivity constraints). Second, in Fig. 1 and 10, we plot the heatmap of the learned embeddings with CE for different imbalance ratios. These results indicate that merely introducing ReLU to the embedding layer does not ensure the orthogonality of features learned by CE. Moreover, the experiments in Thrampoulidis et al. (2022) on networks that have ReLU at the hidden layer show that the embedding geometry alters with imbalance ratio, further suggesting that simply constraining the embeddings to be positive for the CE loss does not guarantee orthogonality of the features. In general, solving for the global minimizer feature-classifier configuration with CE loss is non-trivial. Several works have tackled this problem with different assumptions [Zhu et al. (2021), Graf et al. (2021), Fang et al.(2021), Thrampoulidis et al. (2022), etc]. In light of these observations, we believe exploring the impact of the final layer adjustments on CE deserves a separate study.\n\nWe appreciate the interesting questions, and are happy to provide any further clarifications."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9066/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700481110894,
                "cdate": 1700481110894,
                "tmdate": 1700481168527,
                "mdate": 1700481168527,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oJE5hPC2Ns",
                "forum": "AyXIDfvYg8",
                "replyto": "ZtWg9l9MXB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9066/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9066/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer,\n\nThank you for your time and effort in reviewing our work. We have made diligent efforts to address your comments and we hope that our responses have addressed your questions. If you have more questions, we\u2019re happy to provide further information.\n\nWe would appreciate it if you would consider re-evaluating your rating."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9066/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700696618684,
                "cdate": 1700696618684,
                "tmdate": 1700708550918,
                "mdate": 1700708550918,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7JPnr6vchT",
            "forum": "AyXIDfvYg8",
            "replyto": "AyXIDfvYg8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9066/Reviewer_PL8u"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9066/Reviewer_PL8u"
            ],
            "content": {
                "summary": {
                    "value": "This work studies the training of deep neural networks with a supervised contrastive objective. Empirically, it is shown that if there are ReLU activations after the last layer, then the learned features per class collapse, and that the features of different classes are orthogonal to each other. This is supported by an analytic argument that characterizes the minimum of the loss function as a function of the features (instead of the network weights). This feature geometry is independent of the class ratios. Moreover, a batching strategy is derived that guarantees the emergence of this feature geometry."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The finding of this work; that training a deep neural network with a supervised contrastive loss and ReLU activations leads to the same feature geometry irrespective of the class imbalances is very surprising. Prior work showed that this is not the case for the cross entropy loss. Moreover, it is shown that this feature geometry is a *common* minimizer for all batches. This results in a straightforward proof, as there are no 'averaging effects' to account for.\n- The paper is well written\n- Related work (with one exception) is extensively discussed and compared to."
                },
                "weaknesses": {
                    "value": "- Significance: The work appears to be written on the premise that achieving a symmetric (orthogonal or simplex) feature geometry is beneficial.  For example, a batching strategy is devised that guarantees such a geometry. While symmetry seems reasonable for class-balanced data, for imbalanced data this is not so clear. I did not see an argument supporting this premise, and thus also not for why using the proposed batching strategy  is a good idea. Moreover, there is few empirical support, as the use of ReLUs does not have any effect on classification accuracy, cf. Table 1.\n\n- The analytical argument requires a very strong assumption: unconstrained features. Moreover, only the loss minimizer is derived, but optimization effects for this non-convex problem are not accounted for. On the other hand, this strong assumption allows for results that are architecture independent.\n\n- The empirical part of this work would be stronger, if the theoretical findings are verified on a large scale imbalanced dataset (compared to the mid-scale datasets CIFAR10, CIFAR100 and TinyImagenet). \n\n-  The very related work [Wang & Isola, Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere, ICML 2020] is not cited. This work studies feature geometries when minimizing the (unsupervised) contrastive loss and should be compared to."
                },
                "questions": {
                    "value": "Are there empirical differences when training models with and without ReLU activations after the last layer. If so, can such differences be explained by the targeted feature geometry? Are such differences more pronounced when using the batching strategy?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9066/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698928137783,
            "cdate": 1698928137783,
            "tmdate": 1699637141566,
            "mdate": 1699637141566,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cmXRVLpivI",
                "forum": "AyXIDfvYg8",
                "replyto": "7JPnr6vchT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9066/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9066/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response to Reviewer PL8u"
                    },
                    "comment": {
                        "value": "Thank you for your supportive comments and useful feedback. We reply to the questions below in addition to the global response:\n\n**UFM assumption:** We would like first to emphasize that our work addresses the problem for highly overparameterized models and training beyond the zero-training error. This is a setting considered in the majority of previous works on neural collapse [Mixon et al. (2020); Fang et al. (2021); Graf et al. (2021);Lu & Steinerberger (2020)]. The UFM-inspired theoretical analysis in the literature is also motivated by this overparameterization assumption. In fact, UFM assumes that overparameterized models are capable of representing any function mapping the input to the embedding space. As pointed out by the reviewer, this indeed is a strong assumption, although one that has proved to be very effective. Our findings alongside the results in a series of previous related works confirm this model's efficacy in predicting the properties of the embeddings learned at the final layer for large enough models. In other words, the high expressivity of overparameterized models makes the UFM a useful tool for identifying the global minimizer feature configuration, so that the impact of the loss function itself can be isolated from the role of the network architecture. Our work shows that the UFM remains a useful mathematical model even with the non-linear activation function of ReLU. \n\nOn the optimization aspect, we believe that studying the optimization effects on the UFM has scope for future work. Our experiments provide evidence for consistent convergence towards the global minimizer (OF geometry) despite the non-convexity of the network optimization. \n\n**Empirical:**\nWe appreciate the reviewer's recommendation regarding experiments on large-scale datasets and consider adding these experiments to the final version of the paper. Along these lines, we are excited to report new experimental findings in support of our hypothesis of OF geometry on transformer architecture. We have included additional experiments on vision transformers (see Fig.22). Based on our current observations, our hypothesis of OF geometry holds here as well, thus solidifying the claim of the generality of our results across various architectures.\n\nIn summary, we would like to note that our current findings are backed up by experiments spanning various architectures and datasets, including MLP, ResNet, and DenseNet applied to MNIST, CIFAR10, CIFAR100, and TinyImageNet. Across this range of tasks, from simpler and smaller ones like MNIST (10 classes) to more complex and larger tasks like TinyImageNet (200 classes), we have not observed significant changes in the convergence behavior or the communication of our key message (see Fig 2) as long as the network and dimension of output features are large enough.\nThe observations from all of the above experiments provides us with the confidence to believe that the core message of our paper remains largely unchanged. This is the case for both our main findings: convergence to OF and the efficacy of the new batch-binding scheme.\n\n**Missing Citation:** Thank you for pointing this out. We agree that Wang, Isola (2020) is a relevant work. It appears that not mentioning the same was an editing oversight on our part. We have included the reference and commented on the relationship.\n\n**Empirical differences:** At the moment the main empirical differences observed are the following : the test accuracies (Tab. 1) and new analysis on worst-class accuracies between with and without ReLU. We discuss this in the global response as well. We are not aware of differences in the batching aspects. In fact, the observation in Fig. 21 suggests that the batch-binding scheme is beneficial even when training without ReLU, for convergence to the final feature geometry."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9066/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700482922436,
                "cdate": 1700482922436,
                "tmdate": 1700649328570,
                "mdate": 1700649328570,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0w4RJ5zIfJ",
                "forum": "AyXIDfvYg8",
                "replyto": "cmXRVLpivI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9066/Reviewer_PL8u"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9066/Reviewer_PL8u"
                ],
                "content": {
                    "comment": {
                        "value": "I have read the authors response. It appears that my first and major concern (missing motivation for promoting a symmetric feature geometry) has not been addressed. \n\nMoreover, I disagree with the authors, that a 'major empirical difference can be noted in the test accuracies (Tab. 1)'. The observed differences are not significant and could be due to randomness in the (presumably 5) training runs. The results in the new table in Section E.8 are convincing and show that adding ReLUs improves classification accuracy on worst classes. However, at the cost of classification accuracy in the best classes, as overall accuracy remains unchanged (as concluded from Tab 1).\n\nI will keep my rating."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9066/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700646865308,
                "cdate": 1700646865308,
                "tmdate": 1700646865308,
                "mdate": 1700646865308,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aSjOcZBHvR",
                "forum": "AyXIDfvYg8",
                "replyto": "aMs8n2ar1C",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9066/Reviewer_PL8u"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9066/Reviewer_PL8u"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the clarification"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9066/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700650934884,
                "cdate": 1700650934884,
                "tmdate": 1700650934884,
                "mdate": 1700650934884,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "C6iQts6eUB",
                "forum": "AyXIDfvYg8",
                "replyto": "7JPnr6vchT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9066/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9066/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your time and effort in reviewing our paper. On the motivation, we would kindly refer you to our comments in the global response. At the outset, the explicit goal was not to symmetrize the feature geometry, but to **uncover and understand** the same. This is in line with the research goal of the rapidly growing interest in neural collapse and its implications for DNN training with various objective functions. Please note that there is **no prior work** that characterizes the feature geometry and thus the behavior of the loss function in inducing the same, for the SCL under imbalances. Prior to our work, the understanding of the **intricate role of mini-batch optimization** was limited as well. We have identified a setting where the geometry is predictable, incidentally observing that it is symmetric. Based on the works in literature on related loss functions under class imbalance, this result remains surprising. Additionally, we show that the convergence quality is excellent, in practical experiments, as compared to experimental results on geometry under imbalances in literature. Thus, we believe that our discovery, supported by concrete theoretical results, paves the way to important future questions in understanding the loss functions that are used across datasets and model architectures, and identifying characteristic properties of various loss objectives."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9066/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700731645605,
                "cdate": 1700731645605,
                "tmdate": 1700731941836,
                "mdate": 1700731941836,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]