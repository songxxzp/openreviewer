[
    {
        "title": "Plug-And-Play Controllable Graph Generation With Diffusion Models"
    },
    {
        "review": {
            "id": "C6t6RQHX4j",
            "forum": "xh0XzueyCJ",
            "replyto": "xh0XzueyCJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6338/Reviewer_CFiy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6338/Reviewer_CFiy"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to have a constraint instead of a condition for generative modeling. The main evaluation metric is VAL_c, which measures the proportion of generated graphs that satisfy the constraint."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "\u2013 constraint-based generation is a useful and novel avenue on graph generation"
                },
                "weaknesses": {
                    "value": "\u2013 It seems that the constrained generation settings is a completely different task from conditional generation (Hoogeboom 2022, Xu 2020) so, I think that the \u201cconstrained\u201d keyword is more suitable?\n\n\u2013 Since the tasks of Hoogeboom 2022, Xu 2020 are different, the comparisons with baselines are not really fair from what I read in the paper. You should compare the proposed \u201cconstrained\u201d approach, with other constrained methods, maybe an adaptation of Bar-tal 2023 that you refer to? I am not convinced that measuring VAL_c on GDSS is fair against your model? This is why GDSS and EDP-GNN have the VAL_c metric approximately at zero, see Table 3."
                },
                "questions": {
                    "value": "\u2013 Can your approach be applied also into conditional generation, e.g. Hoogeboom 2022 etc?\n\n\u2013 \u201cinfluences the sampling process in an obscure and uninterpretable manner\u201d: can you elaborate?\n\n\u2013 \u201cSuch controls are not differentiable and no method exists that can control the generation for these properties without relying on curating additional labeled datasets or retaining the entire generative mode\u201d: from what I understand conditional generation is a different setup, so does not seem fair to compare what data is required in constrained generation ?\n\n\u2013 Condition-based Control definition: Can you write what y and c are? Or link to the part where you formalize it.\n\n\u2013 Constraint-based Control definition: can you clarify better the difference with soft control. Or refer to where you are defining it.\n\n\u2013 Does the plug-and-play approach require training of 2 models? Could you point to where you discuss the advantage to training the soft constrained methods?\n\n\u2013 Figure 2: Sampling process of PRODIGY (red) versus existing methods (Jo et al., 2022). Why are you comparing them, if in the experiments you are not?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6338/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6338/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6338/Reviewer_CFiy"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6338/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697619042790,
            "cdate": 1697619042790,
            "tmdate": 1699636697913,
            "mdate": 1699636697913,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "u2b0lCZTlq",
                "forum": "xh0XzueyCJ",
                "replyto": "C6t6RQHX4j",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6338/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6338/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the Reviewer CFiy (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for providing constructive feedback on our work. Below, we provide detailed responses to their concerns and hope that the following discussion can encourage them to increase their score towards acceptance. \n\n> It seems that the constrained generation settings is a completely different task from conditional generation (Hoogeboom 2022, Xu 2020) so, I think that the \u201cconstrained\u201d keyword is more suitable?\n\nWe are not sure why the reviewer thinks we are focusing on conditional generation as we have never used that term anywhere to describe our approach. We would appreciate it if the reviewer could point out what has confused them. We do call our approach controllable generation as a constraint is a form of control. \n\n> Can your approach be applied also into conditional generation, e.g. Hoogeboom 2022 etc?\n\nYes, as shown in Section 5.3 (dipole moment was also considered in Hoogeboom et al., 2022) and Appendix D.1, the problem of conditional generation can be solved using our approach. One can argue that the problem of conditional generation can be reduced to the more general constrained generation (the focus of this work). The objective of conditional generation can be seen as minimizing the difference (MAE or MSE) in the property of the generated samples and that of the given conditioning samples. More formally, this implies constraining the conditioned property of the generated samples within a certain bound ($\\epsilon$) of the median/mean of the given conditioning samples. Theoretically, this will minimize the MAE/MSE of the property between the generated samples and the conditioning samples. \n\n> Since the tasks of Hoogeboom 2022, Xu 2020 are different, the comparisons with baselines are not really fair from what I read in the paper. You should compare the proposed \u201cconstrained\u201d approach, with other constrained methods, maybe an adaptation of Bar-tal 2023 that you refer to? I am not convinced that measuring VAL_c on GDSS is fair against your model? This is why GDSS and EDP-GNN have the VAL_c metric approximately at zero, see Table 3 \n\n>\u201cSuch controls are not differentiable and no method exists that can control the generation for these properties without relying on curating additional labeled datasets or retaining the entire generative mode\u201d: from what I understand conditional generation is a different setup, so does not seem fair to compare what data is required in constrained generation ?\n\nWe argue that comparison against base and conditional-generation models is fair in the absence of any constrained graph generation baseline in the literature. \n\n**Comparison with the base models:** To the best of our knowledge, there do not exist any baselines for constrained graph generation, that we can compare against. Thus, we decided to highlight the plug-and-play improvement over the base model. We test if our sampling strategy can improve the satisfaction of constraints that are not already satisfied by the original sampling methods. Our comparison with the base models in terms of $\\text{Val}_C$ is to show that our method can improve the constraint validity significantly without significantly affecting the distributional metrics. Furthermore, we cannot have Bar-Tal et al. 2023 as a baseline since the proposed method in their work focuses on image generation and is limited to specific image-based constraints (panorama, aspect ratio, and spatial guiding) and any extensions of their method to graph-level constraints are not directly meaningful and hence not the focus of this work. \n\n**Comparison with conditional-generation models:** Our comparisons against the conditional-generation approaches are geared towards demonstrating that our approach is general enough that it can handle condition-based control as well. These experiments are not meant to assess the basic efficacy of our approach. We formulate the objective of their paper as a constraint and apply our method to achieve similar performance. In particular, we consider a constraint such that the value of the conditioned property is close (within $\\epsilon$) to the mean/median of the given data. This would imply that the generated samples have property values closer to the conditioned value. In Section 5.3 and Appendix D.1, we thus compare the conditional generation methods with the constrained generation approach of PRODIGY. We use simple linear models to estimate these properties and find that even with these models, we can achieve lower MAE of the predicted properties between the generated and conditioning data samples. This implies that even with highly biased predictors, we can lower the error rate by effectively satisfying the constraint satisfaction and thus, keeping the property within $\\epsilon$ bounds of the predicted median."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6338/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700373536377,
                "cdate": 1700373536377,
                "tmdate": 1700378743817,
                "mdate": 1700378743817,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GvJY4Siywy",
                "forum": "xh0XzueyCJ",
                "replyto": "C6t6RQHX4j",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6338/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6338/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the Reviewer CFiy (2/2)"
                    },
                    "comment": {
                        "value": "> \u201cinfluences the sampling process in an obscure and uninterpretable manner\u201d: can you elaborate?\n\nAn interpretable control on the generation should imply that the generated samples satisfy the given control. But conditional generation techniques instead consider the specified given control as a random variable and approximate the probability distribution $p(x | c(x) = c_0)$ where one can have $p(x=x\u2019 | c(x) = c_0) > 0$ even though $c(x\u2019) \\neq c_0$. This is unexpected and each step in the sampling process thus becomes obscure and uninterpretable. Let us know if this doesn\u2019t clarify the concern and we would be happy to elaborate.\n\n> Condition-based Control definition: Can you write what y and c are? Or link to the part where you formalize it.\n\nWe note that $c(G, y)$ is a boolean function which is $1$ when $y_c(G) = y$ and $0$ otherwise. Here, $y_c(G)$ calculates some property of the graph $G$ and $y$ is a value that this property must take. As such, this is a standard definition in the existing literature and we discuss it in point 1 of controlled generation on page 3. We have also revised the definition in the updated draft. \n\n> Constraint-based Control definition: can you clarify better the difference with soft control. Or refer to where you are defining it.\n\nAs we discuss in the Controlled Generation part of Section 2, the main difference is that in a soft control, we need to sample a graph $G$ given the control $c(G)$ as a dependent random variable, while for a hard control or constraint, we require the sampled graph $G$ to satisfy the control $c(G)$. For the soft control, one estimates a conditional probability distribution $p(G | c(G))$ and samples the graph $G$ from this. But this does not imply that the sampled graph would hold $c(G)$. This is because of the cyclic nature of the corresponding Bayesian network since a random variable $c(G)$ affects the distribution of $G$ (i.e., $p(G | c(G))$) and a given $G$ also affects the value of $c(G)$ (i.e., $p(c(G) | G)$). It thus forms an infinite Markov chain to necessitate that the sampled $G$ would imply $c(G)$. Constraint-based control specification, on the other hand, necessitates the control to be satisfied (hence, hard). We have clarified this better in the revised draft.\n\n\n> Does the plug-and-play approach require training of 2 models? Could you point to where you discuss the advantage to training the soft-constrained methods?\n\nWe think the reviewer has misunderstood our approach regarding the necessity of training. We wish to clarify that we do not require training of two models. In fact, we do not train any new model but rather solve an optimization problem at every sampling step. The key contribution is to show that such an optimization problem can be solved efficiently, thus, removing the requirement of training a new predictor model or a new generative model, while allowing for the precise controls to be satisfied exactly. As we discuss in the Controlled Generation part of Section 2, the advantage over existing soft-constrained methods is two-fold \u2013 (1) Our method can be plugged onto any pre-trained diffusion model\u2019s sampling without training any new model, and (2) We can satisfy precisely defined hard constraints on the generated samples that cannot be done using guidance-based methods. \n\n> Figure 2: Sampling process of PRODIGY (red) versus existing methods (Jo et al., 2022). Why are you comparing them, if in the experiments you are not?\n\nWe are indeed comparing these two in the experiments. In tables 2,3,5,6, we compare PRODIGY (red) against GDSS (Jo et al., 2022)."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6338/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700373579956,
                "cdate": 1700373579956,
                "tmdate": 1700378689929,
                "mdate": 1700378689929,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3VVqmFrFqQ",
                "forum": "xh0XzueyCJ",
                "replyto": "C6t6RQHX4j",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6338/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6338/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Requesting feedback on the rebuttal"
                    },
                    "comment": {
                        "value": "Thank you once again for your review! We have tried to carefully address your concerns in our responses and it would be very valuable to us if you could provide your feedback. If any issues still remain that need to be resolved, we would love to address them during the discussion period."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6338/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700594135087,
                "cdate": 1700594135087,
                "tmdate": 1700594135087,
                "mdate": 1700594135087,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "JjOCEPyazM",
            "forum": "xh0XzueyCJ",
            "replyto": "xh0XzueyCJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6338/Reviewer_DbHE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6338/Reviewer_DbHE"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a plug-and-play method to control the diffusion-based graph generation under certain constraints. In each step of diffusion, the generated graph is edited with the nearest feasible solution under the constraints, which can be solved with the Lagrangian method. The authors derive closed-form solutions for some well specified constraints commonly seen. Experiments show the proposed approach can effectively control the generation process to obtain results that satisfy the constraints."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Precise control with arbitrary constraints is a desired property for graph generation. The authors propose a general solution for certain classes of constraints. The authors derive closed-form solution for many commonly used constraints under the proposed projection approach. The detailed derivation may also inspire future research where new constraints may appear.\n2. The empirical results are comprehensive. The authors show the proposed method can effectively control the generation direction towards graphs that satisfy the constraints, on both 2D graphs and 3D graphs. They further demonstrate the sensitivity of the approach to the interpolation value $\\gamma_t$$. Analysis on the efficiency of the approach is also provided."
                },
                "weaknesses": {
                    "value": "1. Though when the constraints cover all the graphs in the test set, the generated distribution is largely unaffected, in scenarios where the constraints do take effect, the generated distribution obviously deviates from the original distribution. And the deviation is model-sensitive. For example, in Table 2, with edge count as the constraint, the performance of GDSS downgrades by two folds on Clus. while EDP-GNN is largely unaffected. Also, on the dataset of Enzymes and Grid, both GDSS and EDP-GNN suffer a lot from the constraints. Furthermore, Figure 12 shows a lot of unrealistic molecular graphs (e.g. very large rings, disconnected graphs, very long bridging bonds) with the constraints on atom count.\n2. The proposed projection paradigm main only applied to constraints with simple calculation process so that the optimization problem has a closed-form solution. When the constraints become complicated (e.g. involves non-linearity), it might be arduous or impossible to find such closed-form solution."
                },
                "questions": {
                    "value": "1. Maybe a typo in section 4 right under equation 3: $\\Pi_c(z)\u00a0=\u00a0\\arg\\min_{z\\in\u00a0c}||z-x||^2$\uff0cshould be $\\Pi_c(x) = \\arg\\min_{z\\in c}||z-x||^2$"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6338/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6338/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6338/Reviewer_DbHE"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6338/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697632598524,
            "cdate": 1697632598524,
            "tmdate": 1699636697793,
            "mdate": 1699636697793,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bfRsT3vNJ1",
                "forum": "xh0XzueyCJ",
                "replyto": "JjOCEPyazM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6338/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6338/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We are delighted by the reviewer\u2019s positive recommendation of our work and we hope that any remaining concerns get addressed in the following discussion to further increase their scores. We also thank them for pointing out the typo that we have fixed in the revised version.\n\n> Though when the constraints cover all the graphs in the test set, the generated distribution is largely unaffected, in scenarios where the constraints do take effect, the generated distribution obviously deviates from the original distribution. And the deviation is model-sensitive. \n\nWe would like to note that our performance difference in different base models is not a weakness but rather a strength of our method as we show plug-and-play improvement over these pre-trained models without being specific to any of them. Thus, as more advanced diffusion models are designed in the future, our method will automatically provide further improvement over them for constrained generation. \n\n> For example, in Table 2, with edge count as the constraint, the performance of GDSS downgrades by two folds on Clus. while EDP-GNN is largely unaffected. Also, on the dataset of Enzymes and Grid, both GDSS and EDP-GNN suffer a lot from the constraints. \n\nPlease refer to our general comment on higher MMDs ([link](https://openreview.net/forum?id=xh0XzueyCJ&noteId=BJRokXMqZa)). \n\n> Furthermore, Figure 12 shows a lot of unrealistic molecular graphs (e.g. very large rings, disconnected graphs, very long bridging bonds) with the constraints on atom count.\n\nUnrealistic substructures such as large-cycle rings can arise as the constraint does not control for the size of the cycles and only cares about the counts of atoms. Furthermore, the probability distribution approximated by GDSS is not perfect and the original GDSS also samples such unrealistic graphs. However, the flexibility of our method allows us to control such unrealistic structures if needed by formalizing them as constraints. For example, large cycles can be controlled (just like we control the number of triangles). At this point, since finding the maximum cycle length in a graph is an NP-complete problem, controlling the maximum cycle length becomes a non-scalable problem for larger graphs (since we can\u2019t even calculate it). Future works can explore other efficient methods to incorporate this constraint for larger graphs with certain approximations. Similarly, one can control for connectivity and bridging in the graph since both are formal graph-theoretic functions but we leave them for future works to consider.  \n\n> The proposed projection paradigm main only applied to constraints with simple calculation process so that the optimization problem has a closed-form solution. When the constraints become complicated (e.g. involves non-linearity), it might be arduous or impossible to find such closed-form solution.\n\nWe would like to note that this is the first work on constrained graph generation and to the best of our knowledge, PRODIGY is the only method that can support plug-and-play constrained generation of any graph-level constraint, once it\u2019s formally defined. \nWe consider a wide range of constraints and find that they have efficient projection operators by following the fixed template of Section 4.1. While we make certain assumptions about the form of the constraint, this does not restrict the applicability of this method to any new constraint. Contrary to the reviewer\u2019s comment, we do not require a closed-form solution to exist for the optimization problem. In fact, as noted in Table 1 and Appendix A, the exact value of $\\mu$ is never known in closed form and found using inexact linear and bisection search algorithms. Furthermore, we also consider approximations instead of exact projections for efficiency reasons and find superior results (see Triangle count constraint, Appendix A.2). All in all, while the proposed method requires solving an optimization problem, it does not require exact solutions and leave enough room for approximate and intricate solutions to be devised for specific constraints following the proof template (using Lagrangian operators) provided in Section 4.1 and Appendix A. We also discuss extensions to general non-linear constraints (assuming ReLU non-linearity) in Appendix B.3 but find that the optimization problem can get harder to solve and future works can focus on devising more efficient solutions."
                    },
                    "title": {
                        "value": "Response to the Reviewer DbHE"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6338/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700373417962,
                "cdate": 1700373417962,
                "tmdate": 1700373737572,
                "mdate": 1700373737572,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mgWOceh4sH",
                "forum": "xh0XzueyCJ",
                "replyto": "JjOCEPyazM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6338/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6338/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Requesting feedback on the rebuttal"
                    },
                    "comment": {
                        "value": "Thank you once again for your review! We have tried to carefully address your concerns in our responses and it would be very valuable to us if you could provide your feedback. If any issues still remain that need to be resolved, we would love to address them during the discussion period."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6338/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700594103233,
                "cdate": 1700594103233,
                "tmdate": 1700594103233,
                "mdate": 1700594103233,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9Nk2mu4aCJ",
                "forum": "xh0XzueyCJ",
                "replyto": "mgWOceh4sH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6338/Reviewer_DbHE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6338/Reviewer_DbHE"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the response"
                    },
                    "comment": {
                        "value": "Thank you for the detailed response. By mentioning \"closed-form solution\", I mean closed-form solution for the projection operator in equation (4), that is, $\\varphi_{\\mu}^X$ and $\\varphi_{\\mu}^A$ has a analytical form (either with or without parameter $\\mu$). For complicated functions, such analytical form might be arduous to derive (e.g. commonly used molecular property functions like QED and logP. Anyway, I admit this is a starting work in constrained graph generation, and I would like to keep my original positive score."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6338/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700660908041,
                "cdate": 1700660908041,
                "tmdate": 1700660908041,
                "mdate": 1700660908041,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7KKU7mMazx",
            "forum": "xh0XzueyCJ",
            "replyto": "xh0XzueyCJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6338/Reviewer_pKPg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6338/Reviewer_pKPg"
            ],
            "content": {
                "summary": {
                    "value": "This work presents a conditional graph generation framework that can be applied in a plug-and-play manner to pretrained diffusion models. In particular, this work proposes to project the denoised samples to the constraint set at each sampling step which results in graphs that satisfy constraints, which could be applied to hard non-differentiable constraints that previous diffusion models are not applicable to."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is well-written and easy to follow, with sufficient background, related works, and problem setup.\n\n- The motivation of this work, i.e., generating graphs from the data distribution that satisfies hard constraints, is clear which previous diffusion models are not directly applicable due to the non-differentiable constraints.\n\n- This work presents various practical constraints on graph structures or properties (e.g., edge count, degree, valency, dipole moment) and the corresponding projection operators.\n\n- The experimental results show that the proposed method is able to generate graphs that satisfy given conditions in a plug-and-play manner without re-training or fine-tuning the pre-trained diffusion models."
                },
                "weaknesses": {
                    "value": "- The main concern is that using the proposed approach seems to have inferior generation quality compared to the original diffusion model. For example, in Table 2, GDSS+PRODIGY results in a significantly higher clustering coefficient MMD for the Community-small dataset and Enzymes dataset. This is problematic as the generated graphs should primarily follow the data distribution, not only satisfying the constraint.\n\n- For some constraints, e.g., Degree and Molecular weight, the proposed method does not seem to achieve high validity (lower than 70%) even though the denoised samples are projected to the constrained set. In particular, GDSS+PRODIGY shows similar validity to GDSS for molecular weight constraint. Through analysis clarifying the reason for failing to satisfy the constraint is required.\n\nI would like to raise my score if these concerns are sufficiently addressed."
                },
                "questions": {
                    "value": "- Are the MMD results of Table 2 measured between the generated graphs and the constraint-filtered test set? I presume the MMD is not measured between generated graphs and the original test set as the MMD results are different among the constraints for the same dataset.\n\n- What is the reason for GDSS+PRODIGY showing similar validity to GDSS for molecular weight constraint?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6338/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6338/Reviewer_pKPg",
                        "ICLR.cc/2024/Conference/Submission6338/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6338/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698669148216,
            "cdate": 1698669148216,
            "tmdate": 1700718103413,
            "mdate": 1700718103413,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "locfQK6ZOJ",
                "forum": "xh0XzueyCJ",
                "replyto": "7KKU7mMazx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6338/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6338/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the Reviewer pKPg"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer\u2019s recognition of the strengths of our work and we hope the following discussion will be able to address their remaining concerns. \n\n> The main concern is that using the proposed approach seems to have inferior generation quality compared to the original diffusion model. For example, in Table 2, GDSS+PRODIGY results in a significantly higher clustering coefficient MMD for the Community-small dataset and Enzymes dataset. This is problematic as the generated graphs should primarily follow the data distribution, not only satisfying the constraint.\n\nPlease refer to our general comment on higher MMDs ([link](https://openreview.net/forum?id=xh0XzueyCJ&noteId=BJRokXMqZa)). \n\n> For some constraints, e.g., Degree and Molecular weight, the proposed method does not seem to achieve high validity (lower than 70%) even though the denoised samples are projected to the constrained set. In particular, GDSS+PRODIGY shows similar validity to GDSS for molecular weight constraint. Through analysis clarifying the reason for failing to satisfy the constraint is required.\n\n> What is the reason for GDSS+PRODIGY showing similar validity to GDSS for molecular weight constraint?\n\nLower satisfaction of given constraints in the experiments cited by the reviewer is primarily because we have to round our constraint-satisfying continuous solution to a corresponding discrete solution. As mentioned in Section 4, we consider a continuous relaxation of the constrained set, and the projection step projects the adjacency and atom-type matrices to the closest continuous solution that satisfies the constraint. At this point, we can guarantee the constraint is fully satisfied. However, since the graph structure is discrete, we round this continuous solution at the end to the closest discrete value and find the constraint validity on the rounded solution. This means that the constraint may not always be satisfied by the discrete solution. While specific rounding strategies to satisfy the constraints can be devised, this is a challenging task and must be carefully designed based on the application. For simplicity, we stick to the existing threshold-based rounding strategy as in the literature (Niu et al., 2020; Jo et al., 2022). In the experiments, we find that this is often very effective while being considerably efficient. \n\nReflecting on the reviewer\u2019s suggestion, we discuss and analyze possible reasons why low satisfaction may be achieved in specific cases including the ones cited by the reviewer: \n\n1. Molecular weight constraint: This tends to be the case because the closest point found by the projection operator tends to make the $X$ matrix uniform across different atom types. When a low satisfaction is achieved on this constraint, we see in the failure cases that it tends to converge to $X_{ij} \\rightarrow W / (n \\sum_j m_j)$. This is not ideal during rounding as even though $\\sum_{i, j} m_j X_{ij} \\le W$, we can have $\\sum_{i} m_{\\arg\\max_j X_{ij}} > W$ since $X_{ij} \\approx X_{ik}$ ($j \\ne k$). This can be alleviated with additional constraints on $X$ (such as $|X_{ij} - X_{ik}| > \\epsilon$ for some $\\epsilon$) but we leave such engineering tweaks for specific constraints for future works to examine.\n2. Ego-small (Edge Count and Degree): We believe this comes due to the limited number of discrete graphs that are possible around the constraint-feasible set. Ego-small consists of typically sparse graphs with few (~5) nodes. Our constraints of at most $3$ edges and at most $5$ maximum degree allow for very few graphs that tend to be harder to reach through simple rounding techniques.  \n3. EDP-GNN, Community-small (Edge Count and Degree): We believe the low satisfaction, when it happens in this case, comes because we consider an aggressive $\\gamma_t = 1$ projection for EDP-GNN. This may not be ideal as it can drift the sampling procedure too much leading into a specific continuous region that may not be close to the learned distributions of (discrete) graphs. EDP-GNN only consists of Langevin sampling and thus, one cannot schedule a $\\gamma(t)$ as a function of $t$ as in GDSS. But we believe a scalar value of $\\gamma_t < 1$ might give better results and we will update the results in the paper to include this (will share here as soon as they are available). \n\n> Are the MMD results of Table 2 measured between the generated graphs and the constraint-filtered test set? I presume the MMD is not measured between generated graphs and the original test set as the MMD results are different among the constraints for the same dataset.\n\nYes, as mentioned in Section 5.2, MMD results in Table 2 are found using the generated and constraint-filtered test graphs. Since in this experiment, we are only interested in generating graphs that satisfy specific constraints, unsatisfiable graphs do not form meaningful ground-truth samples for comparison. Hence, we filtered these out to compute the MMD."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6338/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700373318735,
                "cdate": 1700373318735,
                "tmdate": 1700378526896,
                "mdate": 1700378526896,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RE9pbyeByI",
                "forum": "xh0XzueyCJ",
                "replyto": "locfQK6ZOJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6338/Reviewer_pKPg"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6338/Reviewer_pKPg"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the response"
                    },
                    "comment": {
                        "value": "Thank you for the detailed response, especially the analysis of possible reasons why low satisfaction may be achieved.\n\nAlthough I think the problem this work is targeting (i.e., constrained graph generation) to be important and the proposed method to be novel in the context of graph generation, I think that the experiments could be further improved. \n\n- Since the authors claim that MMD w.r.t. some properties cannot completely identify a graph distribution, there should be alternative metrics (or other experiments) to show that the samples from the PRODIGY do belong to the data distribution. Current results fail to demonstrate this.\n\n- I do not understand the t-test results in the general comments. Which MMD (Deg., Clus., Orb.) was the target for the t-test? I find the Clus. results on Enzymes and Deg. results on Community-small to be significantly different on GDSS."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6338/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700573636378,
                "cdate": 1700573636378,
                "tmdate": 1700573636378,
                "mdate": 1700573636378,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LLl6MjSKR4",
                "forum": "xh0XzueyCJ",
                "replyto": "7KKU7mMazx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6338/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6338/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the follow-up question 2/2"
                    },
                    "comment": {
                        "value": "> I do not understand the t-test results in the general comments. Which MMD (Deg., Clus., Orb.) was the target for the t-test? I find the Clus. results on Enzymes and Deg. results on Community-small to be significantly different on GDSS.\n\nIn the general comment, we consider the hypothesis that given a constraint, the MMD of GDSS/EDP-GNN for any property is different from the MMD of GDSS+PRODIGY/EDP-GNN+PRODIGY for that property and dataset. Thus, we combine different properties in the same statistical analysis because each property is as important as the other and find no statistically significant result for the same. \n\nTo specifically address what the reviewer has suggested, one must instead test the hypothesis that x-MMD(GDSS/EDP-GNN | dataset) is different than x-MMD(GDSS/EDP-GNN+PRODIGY | dataset), where x is Deg., Clus., Orb.. We give the results for these tests below. We highlight the rows which are found to be statistically different ($p <0.05$) and note that a positive t-statistic mean x-MMD(GDSS | dataset) > x-MMD(GDSS+PRODIGY | dataset) while a negative t-statistic mean x-MMD(GDSS/EDP-GNN | dataset) < x-MMD(GDSS/EDP-GNN+PRODIGY | dataset). This shows that PRODIGY statistically worsens the MMD in only the deg. and orb. MMDs for Grid (that we already discussed above). This is given the fact that PRODIGY significantly lowered the MMDs in 7 cases and improved the constraint validity in all cases. This establishes that the strengths of PRODIGY in satisfying constraints overshadow increasing the MMD in one dataset (Grid). \n\n| Model | Dataset | MMD | t-statistic | p-value |\n| -- | -- | -- | -- | -- |\n| EDP-GNN | Community-small | **Deg.** | **4.43** | **0.01** |\n| | | Clus. | -2.67 | 0.06 | \n| | | Orb. | -0.97 | 0.38 |\n| | | Combined | -0.52 | 0.63 |\n| | Ego-small | Deg. | -0.70 | 0.52 |\n| | | **Clus.** | **3.65** | **0.02** | \n| | | **Orb.** | **3.60** | **0.02** | \n| | | Combined | 1.70 | 0.16 |\n| | Enzymes | Deg. | -1.45 | 0.22 | \n| | | **Clus.** | **44.0** | **1.59e-06** | \n| | | Orb. | -0.18 | 0.86 | \n| | | Combined | -0.02 | 0.98 |\n| | Grid | **Deg.** | **-9.82** | **6.02e-4** |\n| | | **Clus.** | **inf** | **0.0** |\n| | | **Orb.** | **-15.56** | **9.94e-05**|\n| | | Combined | -0.69 | 0.52 |\n| GDSS | Community-small | Deg. | 0.50 | 0.64 |\n| | | Clus. | -0.97 | 0.38 |\n| | | Orb. | 0.22 | 0.83 |\n| | | Combined | -0.25 | 0.82 |\n| | Ego-small | Deg. | -0.88 | 0.43 |\n| | | **Clus.** | **3.10** | **0.04** |\n| | | Orb. | 0.95 | 0.39 |\n| | | Combined | 1.32 | 0.26 | \n| | Enzymes | Deg. | -0.79 | 0.47 |\n| | | Clus. | -1.92 | 0.13 |\n| | | Orb. | 1.06 | 0.35 |\n| | | Combined | -0.96 | 0.39 |\n| | Grid | **Deg.** | **-6.91** | **2.29e-3** |\n| | | **Clus.** | **15.50** | **1.01e-4** | \n| | | **Orb.** | **-6.48** | **2.91e-3** |\n| | | Combined | -1.79 | 0.15 |"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6338/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700593497522,
                "cdate": 1700593497522,
                "tmdate": 1700593721285,
                "mdate": 1700593721285,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kONchQcFR0",
                "forum": "xh0XzueyCJ",
                "replyto": "LLl6MjSKR4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6338/Reviewer_pKPg"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6338/Reviewer_pKPg"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for detailed response"
                    },
                    "comment": {
                        "value": "Thank you for the response. The t-test results have addressed my concerns about MMD results of PRODIGY. I understand that the experiments of Table 2 deal with highly constrained settings and MMD is not a perfect metric for measuring if the generated samples are really from the data distribution. Therefore, quantitatively showing if the generated graphs preserve the data characteristics, for example, satisfying the two community structures for Community-small or the chain-like structure of Enzymes would further strengthen the authors' claim. In this sense, evaluation on benchmark datasets used in SPECTRE, DiGress, or DruM (e.g., Planar dataset or SBM dataset) is highly recommended.\n\nI raise my score from 5 to 6 as the authors have addressed my concerns, but still evaluate this work as borderline."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6338/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700718090881,
                "cdate": 1700718090881,
                "tmdate": 1700718090881,
                "mdate": 1700718090881,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "sL8hTp04U9",
            "forum": "xh0XzueyCJ",
            "replyto": "xh0XzueyCJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6338/Reviewer_86tq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6338/Reviewer_86tq"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces PRODIGY (PROjected DIffusion for Generating constrained Graphs), a plug-and-play methodology for generating graphs that adhere to designated constraints by leveraging pre-trained diffusion models. Addressing the intricacies of controllable graph generation, the technique integrates a projection operator into the reverse diffusion process to ensure alignment with the specified constrained space. Notably, PRODIGY augments the capabilities of existing diffusion models to satisfy stringent constraints without compromising the proximity to the original data distribution."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The proposed method implements controllable graph generation by applying a plug-and-play approach based on the pre-trained diffusion model without fine-tuning, this is a new perspective to reduce experimental costs."
                },
                "weaknesses": {
                    "value": "1. My biggest concern is whether such an operation, which pulls the embedding towards the designated constrained space through projection during the reverse process, is effective in more practical scenarios. The paper proposes some compromises to determine whether to pull towards the original data distribution or the constrained space during the reverse process. However, from the experimental results, I find that the performance of this approach is not entirely satisfactory. For example, in the Community-small and Ego-small datasets, the MMD metric worsens in many cases after applying the PRODIGY method. Additionally, in the QM9 molecular generation experiment, adding PRODIGY poses a significant risk of decreasing validity and novelty. \n\n2. Another concern I have pertains to the potential impact of such operations on the convergence of the Langevin Dynamics process. It would be prudent to provide a proof for the convergence of the Projected Inexact Langevin Dynamic/Algorithm follows some ideas from PSLA [1]. \n\n3. I also have some concerns regarding the practicality of the proposed method. From the projection operations summarized in the paper, the proposed method seems more suitable for controllable generation for atomic features X or structural information A related properties, such as valency, atom count, and molecular weight. However, in real-world applications, such as constrained molecular generation, we mostly expect the generated molecules to exhibit certain graph-level properties. \n\nMinor:\n\n4. Some notations should be introduced when they are first proposed, such as $\\textbf{Z}_\\theta$ in Eq. (3).\n\n[1] Lamperski, A. (2021, July). Projected stochastic gradient langevin algorithms for constrained sampling and non-convex learning. In Conference on Learning Theory (pp. 2891-2937). PMLR."
                },
                "questions": {
                    "value": "Why does the method in the paper only demonstrate its effectiveness on continuous diffusion methods and not on recently proposed discrete diffusion methods, such as DiGress+PRODIGY?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6338/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698834450145,
            "cdate": 1698834450145,
            "tmdate": 1699636697523,
            "mdate": 1699636697523,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kO3Mz4SJ1S",
                "forum": "xh0XzueyCJ",
                "replyto": "sL8hTp04U9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6338/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6338/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review"
                    },
                    "comment": {
                        "value": "We thank the reviewer for recognizing the strengths of our work and providing constructive feedback to improve it further. Below, we respond to specific concerns raised by the reviewer: \n\n> My biggest concern is whether such an operation, which pulls the embedding towards the designated constrained space through projection during the reverse process, is effective in more practical scenarios. The paper proposes some compromises to determine whether to pull towards the original data distribution or the constrained space during the reverse process. However, from the experimental results, I find that the performance of this approach is not entirely satisfactory. For example, in the Community-small and Ego-small datasets, the MMD metric worsens in many cases after applying the PRODIGY method. Additionally, in the QM9 molecular generation experiment, adding PRODIGY poses a significant risk of decreasing validity and novelty.\n\nPlease refer to our general comment on higher MMDs ([link](https://openreview.net/forum?id=xh0XzueyCJ&noteId=BJRokXMqZa)) in some dataset/model settings on general graphs. In addition, the reviewer raised concerns regarding a decrease in validity in QM9, which happens for EDP-GNN under molecular weight and atom count constraints. We would like to note that (1) This is attributed to the underlying model of EDP-GNN as it only denoises the adjacency matrix while these constraints are dependent on only the node attribute matrix (thus, the reverse sampling of $A$ is not affected by the projection of $X$ and can decrease the validity), and (2) Our framework gives the ability to further increase the validity directly by using the valency constraint on top of any other constraint. \nFurther, decreased novelty under atom count constraint in QM9 primarily arises from the fact that we study a highly constrained setting, where we need to generate molecules with only C and O atoms as compared to the unconstrained setting where we can generate over C, N, O, and F. This naturally limits the total number of possible molecules and thus, limits the novel molecules we can generate over the C and O molecules that already exist in the dataset. Thanks to the reviewer\u2019s comments, we have realized that comparing raw novelty metrics is unfair to our method since we are exploring a limited set of molecules compared to GDSS. We believe a better metric to study would be the proportion of molecules that are both novel and constraint-valid. For this example, we can see it would be roughly $0.33 \\times 81.04 = 26.74\\%$ for GDSS while it would be $1.00 \\times 67.67 = 67.67\\%$ for GDSS+PRODIGY. We will add the constraint-valid and novelty (VN%) metric in the revised version."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6338/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700372645622,
                "cdate": 1700372645622,
                "tmdate": 1700378151434,
                "mdate": 1700378151434,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Dk3aDEJEmm",
                "forum": "xh0XzueyCJ",
                "replyto": "sL8hTp04U9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6338/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6338/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Theoretical convergence"
                    },
                    "comment": {
                        "value": "> Another concern I have pertains to the potential impact of such operations on the convergence of the Langevin Dynamics process. It would be prudent to provide a proof for the convergence of the Projected Inexact Langevin Dynamic/Algorithm follows some ideas from PSLA [1].\n\nWe appreciate this suggestion but note that such a proof of convergence is extremely non-trivial and out of the scope of this work. \n\nHaving said that, as we note in Section 2 (Projected Sampling), modern diffusion models (Song et al., 2021; Jo et al., 2022) differ from theoretical Langevin dynamics considered in Lamperski, 2021, and others. This implies that the proofs in PSLA cannot be directly transferred to our current work due to various reasons \u2013 \n1. We do not know of a corresponding convergence proof even for the unconditional generation of diffusion-based models. Since our method is based on these models, any proof of convergence of the overall sampling becomes highly non-trivial. \nIn the theoretical works, the (unnormalized) probability distribution is known exactly and assumed to have a certain form (uniformly sub-Gaussian), while such assumptions do not hold for diffusion models where the probability density function (or equivalently, the score function) is approximated through arbitrary (graph) data. For real-world graph data, such sub-Gaussian assumptions are not well-motivated. \n2. We consider a non-zero drift coefficient (which is assumed to be $0$ in PSLA) for more expressivity, which complicates the theoretical analysis further. \n3. Our proposed method is a generalization of PSLA in the sense that we do not project the noisy sample completely but instead move the sample only slightly in the direction of the projection (see the last paragraph of Section 4). In particular, we consider $G_{t-1} = (1 - \\gamma_t)\\tilde{G_{t-1}} + \\gamma_t \\prod_{C} \\tilde{G_{t-1}}$, where $\\tilde{G_{t-1}}$ is obtained from $G_t$ following the reverse process. In the experiments, we find that $\\gamma_t = (t/T)^p $ for some $p$ gives the best results, which is in contrast with PSLA which is an instantiation of our method with $\\gamma_t = 1$. \n\nFurther, reflecting upon the reviewer\u2019s comments, we analyzed the drift caused by the PRODIGY method on the original sampling by bounding $\\lVert G_T - \\tilde{G_T} \\rVert$ assuming the same initial random graph $G_0$ and Reverse$(G_t) \\sim$ Reverse$(\\tilde{G_t})$. In particular, we show that $\\lVert G_T - \\tilde{G_T} \\rVert\\le 2 \\sum_{t} \\gamma_t \\lVert \\prod_{C}(G_t) - \\tilde{G_t} \\rVert$. Since the term inside the summation is what we optimize in every iteration under a given constraint, our method optimizes the upper bound to be minimal. Furthermore, $\\gamma_t$ gives the flexibility to be close to the original graphs if needed. To prove the statement, we first note that $\\lVert G_T - \\tilde{G_T} \\rVert = \\lVert (G_T - G_{T-1}) + (G_{T-1} - G_{T-2}) + \\cdots + (G_1 - G_0) - (\\tilde{G_T} - \\tilde{G_{T-1}}) - (\\tilde{G_{T-1}} - \\tilde{G_{T-2}}) - \\cdots - (\\tilde{G_1} - G_0) \\rVert$ $\\le \\lVert (G_T - G_{T-1}) - (\\tilde{G_T} - \\tilde{G_{T-1}}) \\rVert + \\cdots + \\lVert (G_1 - G_0) - (\\tilde{G_1} - G_0) \\rVert$. By definition, $G_t - \\tilde{G_T} = \\gamma_t (\\prod_{C}(G_t) - \\tilde{G_t})$. Thus, we get $\\lVert G_T - \\tilde{G_T} \\rVert\\le 2 \\sum_{t} \\gamma_t \\lVert \\prod_{C}(G_t) - \\tilde{G_t} \\rVert$. \nFinally, we note that a lack of proof does not undermine the effectiveness of our work since we show empirically that PRODIGY can (1) converge to the underlying model\u2019s learned distribution when the test set is a subset of the constraint feasible set (see Section 5.4), and (2) generate graphs that satisfy hard constraints (see Section 5.2)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6338/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700372975426,
                "cdate": 1700372975426,
                "tmdate": 1700499929368,
                "mdate": 1700499929368,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rBZLleJli4",
                "forum": "xh0XzueyCJ",
                "replyto": "sL8hTp04U9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6338/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6338/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Practicality of the method"
                    },
                    "comment": {
                        "value": "> I also have some concerns regarding the practicality of the proposed method. From the projection operations summarized in the paper, the proposed method seems more suitable for controllable generation for atomic features X or structural information A related properties, such as valency, atom count, and molecular weight. However, in real-world applications, such as constrained molecular generation, we mostly expect the generated molecules to exhibit certain graph-level properties.\n\nWe would like to clarify that our method is suitable for and indeed applies to graph-level properties. Any (global) property $h$ of an attributed 2-D graph $G = (X, A)$ can be written as a function of its parts $h(G) = h(X, A)$. We consider general constraints of the form $(h_1(X, A) \\le 0) \\land (h_2(X, A) \\le 0) \\land \\cdots (h_k(X, A) \\le 0)$. For instance, the number of triangles and edges are all global properties of the graph, while total molecular weight and number of certain atoms are global properties of a 2D molecule. We also consider the dipole moment, which is a global property of the 3D molecular structure. Finally, we also show extensions to general linear (Appendix B.2, D.1) and non-linear properties (with ReLU non-linear activations in Appendix B.3) of a graph using parameterized models. In this work, we show, for the first time, that one can control generation from pre-trained models under precisely defined constraints efficiently on a wide array of graph-level properties. Future works can extend our approach to satisfy specific constraints more efficiently. \n\nOne can further argue that the exact form of complex molecular properties as a function of its constituents may not be known when these are only experimentally observed (for example, boiling points, chemical reactivity, etc.). However, this is not a limitation of our method but rather a dependence on chemistry research that aims to find such closed-form functions of all molecular properties in terms of their structure. One way that is increasingly adopted by computational chemists is to learn neural networks from data for these properties (Hoogeboom et al., 2022). This gives us an exact predictive function of these properties and one can then constrain the predicted value from this parameterized model. This would imply constraining the value of the underlying property that we also validate through experiments (Appendix D.1 and Section 5.3 with simple parameterized linear models). \n\nWe hope this helps clarify the concern but if the reviewer still believes some graph-level properties would not be allowed in our proposed framework, then it would be great if they could give specific examples of the same."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6338/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700373058448,
                "cdate": 1700373058448,
                "tmdate": 1700373058448,
                "mdate": 1700373058448,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "M1Ygf33W3i",
                "forum": "xh0XzueyCJ",
                "replyto": "sL8hTp04U9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6338/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6338/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Discrete diffusion models"
                    },
                    "comment": {
                        "value": "> Why does the method in the paper only demonstrate its effectiveness on continuous diffusion methods and not on recently proposed discrete diffusion methods, such as DiGress+PRODIGY?\n\nThe focus of our work on supporting plug-and-play sampling-based constrained graph generation for continuous-time diffusion models is a design choice borne out of both the recently reported generation results of state-of-the-art methods in this field and a carefully conducted feasibility analysis on our end.\n\nFirst, it is not clear that discrete models are necessarily the best choice and provide state-of-the-art performance on unconditional graph generation. As a case in point, in Section 2 (start of page 3), we note that more recent advancements in continuous-time models (DruM, Jo et al., 2023) have been shown to outperform discrete models such as DiGress (however, note that the codes and models for DruM are not publicly available yet but we are in communication with their authors and our method can directly apply on their trained models without any modifications).  As for the DiGress model cited by the reviewer, we recognize that their ability to do conditional generation (separate focus from ours but their definition of conditional generation is generalizable to our constraint-based formulation) is useful and provides a meaningful comparison point for us. Considering this, in Appendix D.1, we compared the guidance-based approach of DiGress against our constrained generation result with GDSS to generate molecules with given HOMO and dipole moment values. We found PRODIGY to provide comparable performance even with weaker underlying continuous time models. This is because we can generate molecules close to the median property value which reduces the error even with a highly biased model. \n\nIn terms of feasibility, the discrete models pose significant efficiency challenges for constrained generation. These challenges primarily stem from the combinatorial explosion of the space of constraint-feasible graphs. The constraint set for discrete graphs is formed by a set of discrete points with no definite structure, unlike the continuity and convexity in the continuous case. Due to these reasons, the solution to the optimization problem becomes an instance of integer programming, and thus, results in an NP-hard problem. The projection step would then involve iterating over all the graphs in the constrained set to find the closest one. This explodes as the size of the constrained set increases, which can be exponential even for a simple edge-count constraint since $|E| \\le B$ has $\\binom{n^2}{B}$ different graphs that satisfy the constraint (where n is the number of nodes). Thus, constraint satisfaction for discrete diffusion models can be NP-hard and we leave it for future works to explore innovative workaround solutions. \n\nFinally, we would like to note that this is the first work on constrained graph generation, and to the best of our knowledge, PRODIGY is the only method that can support plug-and-play constrained generation of any graph-level constraint, once it\u2019s formally defined. We believe that it is an unfair expectation for a single work to solve a highly novel and challenging problem in all available settings. \nIn light of this, we would highly appreciate it if you would be kind to reassess our work and score it based on the above clarifications. We are happy to engage in further discussions and would love to provide any clarifications that can help alleviate any of your outstanding concerns.\n\n*Jo, Jaehyeong, Dongki Kim, and Sung Ju Hwang. \"Graph Generation with Destination-Driven Diffusion Mixture.\" arXiv preprint arXiv:2302.03596 (2023).*"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6338/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700373147465,
                "cdate": 1700373147465,
                "tmdate": 1700378381955,
                "mdate": 1700378381955,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2BUAu84ZbW",
                "forum": "xh0XzueyCJ",
                "replyto": "sL8hTp04U9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6338/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6338/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Requesting feedback on the rebuttal"
                    },
                    "comment": {
                        "value": "Thank you once again for your review! We have tried to carefully address your concerns in our responses and it would be very valuable to us if you could provide your feedback. If any issues still remain that need to be resolved, we would love to address them during the discussion period."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6338/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700594077470,
                "cdate": 1700594077470,
                "tmdate": 1700594077470,
                "mdate": 1700594077470,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jDXChYvpTX",
                "forum": "xh0XzueyCJ",
                "replyto": "sL8hTp04U9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6338/Reviewer_86tq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6338/Reviewer_86tq"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. Could you please revise your current version and mark the revisions as blue?"
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6338/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700731429662,
                "cdate": 1700731429662,
                "tmdate": 1700731429662,
                "mdate": 1700731429662,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]