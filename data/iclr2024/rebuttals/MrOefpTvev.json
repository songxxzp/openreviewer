[
    {
        "title": "Rethinking Texture Patterns in Transformer Neural NetWork for Medical Image Analysis"
    },
    {
        "review": {
            "id": "Rf0LWMt3R3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1673/Reviewer_M5DR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1673/Reviewer_M5DR"
            ],
            "forum": "MrOefpTvev",
            "replyto": "MrOefpTvev",
            "content": {
                "summary": {
                    "value": "The authors propose a transformer network that incorporates a module extracting texture features (Laplacian, Grey Level Occurrence Matrices, and VQ) in particular and evaluate it on binary classification tasks of two medical datasets of different body sites and modalities (ultrasound and X-ray)."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "It has been previously shown that fusing classic texture features and learned features, or putting a particular focus on texture extraction within networks by having texture-extracting modules can be beneficial for image classification in comparison to general-purpose CNNs, in particular in biomedical applications when little (labelled) training data is available. Hence, the idea of introducing texture features in the context of ViT is interesting and promising. \n\nThe authors consider two different data sets, in two different modalities, and different body sites, both are relatively big from a biomedical perspective, which is a strength of the paper.\n\nThe authors provide a sensitivity study on some hyperparameters of the texture features, and an ablation study of additionally including shape features in the training.\nThey compare their method with three baselines (MobileNet, SWIN and ViT)."
                },
                "weaknesses": {
                    "value": "Methods & Experiments\nThe description of the technical details how the texture features are extracted and used within the network is not completely clear to me. Which parameters within the texture layers are learned? How is their position embedded as described in Fig 1? Are the texture features concatenated again with the features from the transformer stream that also takes them as input (as Fig 1 suggests)? The description of the method provided in the manuscript would not be enough for me to reimplement the method.\nAdditionally, I am confused regarding the data preprocessing (see below).\nHence, I find it hard to assess the experiments and think the manuscript would benefit from a revision of the text.\n\n\n\n\nOriginality\nThe authors provide a lengthy background sections on the use CNNs and ViT in general and in medical applications, but only very little on work that combines texture descriptors and CNNs. Since this is a crucial part of introduced method, I suggest developing this part of the background section a bit more in trade-off with the more general background. Some of that work may include (follow-up work on this should be further checked and also considered to compare to as a baseline):\nDetection of cervical cancer cells based on strong feature CNN-SVM network (Jia et al., 2020)\nF. Juefei-Xu, V. N. Boddeti and M. Savvides, \"Local Binary Convolutional Neural Networks\", Proc. CVPR, 2017.\nL. Li et al., \"Face spoofing detection with local binary pattern network\", J. Visual Commun. Image Represent.\nG. Levi and T. Hassner, \"Emotion recognition in the wild via convolutional neural networks and mapped binary patterns\", Proc. of ACM Int. Conf. on Multimodal Interaction\n\u201cLearning Texture Transformer Network for Image Super-Resolution\u201d, Fuzhi Yang, Huan Yang, Jianlong Fu, Hongtao Lu, Baining Guo; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020\n\nI also miss some references to the original work introducing the texture features used in this study, i.e. the Laplacian patterns provide a reference to Hao et al. from 2023, the GLCM patterns to Cao et al. 2021, etc. These authors may have used those patterns in their studies, but this should go into the relevant background section. These texture features have been introduced very long ago, e.g. GLCM in the early 70s in Haralick R. M., Shanmugam K. & Dinstein I. Textural Features for Image Classification. IEEE Transactions on Systems, Man, and Cybernetics 3, 610\u2013621 (1973), which should be provided as a reference accordingly.\n\n\nClarity\nEqn (1): $\\mathcal{R}$ is defined as a 2-dim Euclidean space just before the definition of the Laplace operator, but x,y are also in $\\mathcal{R}$, which doesn\u2019t work out and has to be adjusted. When revisiting this definition, mind that this is a discrete Laplace operator, i.e. x and y should be $\\mathcal{N}_x, \\mathcal{N}_y \\subset \\mathbb{N}$, and $I$ a function mapping from $\\mathcal{N}_x \\times \\mathcal{N}_y$ to $[0,1]$, or whichever intensity range is chosen.\n\nThe manuscript would benefit greatly from some being reiterated by a native speaker or software to correct for some of the used grammar, punctuation, and choice of words. Overall it is clear enough to understand the authors' intentions and it is for the most part not ambiguous, but there are grammar mistakes and confusing phrases on many occasions.  \n\nThere are many examples in the text, but just to pick one, the paragraph following Eqn (1):\n\u201cThe calculation of this [Laplacian] magnitude reveals that the Laplacian not only extracts local texture details but also maintains certain aspects of local topological structures within the image. To address any concerns about its global behavior, we employ statistical methods such as histogram analysis, which helps in capturing and representing the overall distribution of these local patterns across the entire image\u201d.\nSentences like these leave me wondering in which way the magnitude (or its calculation to be more precise) maintains aspects of local topological structures? How it addresses concerns about its global behavior? What are these concerns at all? What is the histogram analysis used? In what way is that a statistical method?\n\nThe paragraph on the VQ pattern is quite unclear. What is the 1-ring neighbor? I assume because the authors then talk about 9 principal components, that they mean a 3x3 neighborhood which they decompose, but it is not clear from the text why there are 9 eigenvalues/vectors.\n\nFrom the paragraph on GLCM it\u2019s not really possible to later conclude what the implementation details mean, when the authors say \u201c\tThe gray level is another important parameters. If it is very large, the GLCM and the histogram will be too sparse. If it is too small, both histogram and GLCM do not make any sense in statistics. Therefore, we empirically keep at least 6 hits for each bar in average.\u201d Please try to define the required parameters when introducing GLCM and use the same terminology throughout the paper.\n\nAdditionally, there are sometimes spaces missing, words repeating, text used in math environment.\n\n\n\n\n-----------------------------------------------------------------------\nAfter Author-Reviewer-Discussion:\n\nI acknowledge having read through the other reviewers' reviews and remain with my initial score."
                },
                "questions": {
                    "value": "Algorithm Histogram_GPU - what is the significance of this function? In which way is it GPU-specific?\n\nI think all the algorithms provided in the manuscript could be moved to the appendix.\n\nIt comes as a bit of a surprise to read in 3.4 that in some cases also shape features are extracted and merged with the texture features. How are they extracted? How are they merged with other features during training?\n\nWhat are the dimensions of the images in the pneumonia dataset?\n\nHow many images were there in each class in the breast dataset?\n\nAll images were resized to 28x28 pixels? In the implementation details it then says they were resized to 72x72? Please clarify.\n\nWhat does \u201cBy considering the small size of image patches, we chose some small gray levels to test the performance of TxTN to reduce the side effect of sparse matrix(as the characterization of texture patterns).\u201d mean?\n\nIn Fig. 1 it says \u201cTexture Representation and position embedding\u201d, how does the position embedding work?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1673/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1673/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1673/Reviewer_M5DR"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1673/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697462605419,
            "cdate": 1697462605419,
            "tmdate": 1700640623510,
            "mdate": 1700640623510,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "OvunfPScsx",
            "forum": "MrOefpTvev",
            "replyto": "MrOefpTvev",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1673/Reviewer_ej1o"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1673/Reviewer_ej1o"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed an improved ViT model by introducing texture layers into attention parts for medical image classification. The texture layers are designed by standard GLGM, Laplacian modules. Experiments on two medical datasets demonstrate that paying more attention to texture features can improve classification performance than ViT models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Good performance gain\n2. Clear motivation and writing"
                },
                "weaknesses": {
                    "value": "1. Technical novelty is lacking. Transformer-based methods can better capture high-level information rather than textural features. These insights have been shown in previous works, see [1]. Here, CNN may be able to learn the textural information well, see [2]. Therefore, why not combine both for medical tasks? Furthermore, the comparison is not sufficient. there are many customized methods for medical tasks, which should be included for the comparison. Finally, do the fixed GLGM, Laplacian modules denote all textural features? More analyses are needed here.\n[1] Ghiasi, Amin, et al. \"What do vision transformers learn? a visual exploration.\" arXiv preprint arXiv:2212.06727 (2022)\n[2] Geirhos R, Rubisch P, Michaelis C, et al. ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness[J]. arXiv preprint arXiv:1811.12231, 2018.\n\n2. Section 3.2.3 does not show the relation between VQ and textural features.\n3. The clinical motivation is not clear. For X-ray and Ultrasound images, both adopting transformer methods and using textural features are not explained from the clinical views."
                },
                "questions": {
                    "value": "See the above weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1673/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1673/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1673/Reviewer_ej1o"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1673/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697502587476,
            "cdate": 1697502587476,
            "tmdate": 1699636095478,
            "mdate": 1699636095478,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "EBB9woimM1",
            "forum": "MrOefpTvev",
            "replyto": "MrOefpTvev",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1673/Reviewer_yqHn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1673/Reviewer_yqHn"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a texture transformer network (TxTN) by integrating three texture layers to enhance the discriminative capability of Vision Transformer (ViT) for medical image analysis. Three texture patterns (GLCM, VQ and Laplacian) are embedded into the design of ViT architecture to address its major shortcomings, including topological destruction, the loss of geometrical information and the lack of global characteristics. Then, the comparison experiments verify its effectiveness evaluated on two public medical datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. This approach tries to improve the performance of the vision transformer, which is essential for further medical image analysis. \n2. Experimental results show the value of the proposed method on two public medical datasets compared to some baselines."
                },
                "weaknesses": {
                    "value": "1. The first weakness is the lack of sufficient comparison experiments. This paper's chosen baselines (MobileNet, SWIN and VIT) are designed for general image classification in CV. The authors should select other specific baseline algorithms on breast and pneumonia datasets. \n2. This paper could add more comparison visualization figures among different approaches. \n3. The proposed texture transformer network lacks the novelty. The proposed pattern& texture module could be seen as one of the pre-processing operations."
                },
                "questions": {
                    "value": "1. Please give the evidence to support the claim: \"This inspirational idea stems from one important insight into the architecture of ViT and its major shortcomings, including topological destruction, the loss of geometrical information and the lack of global characteristics.\"\n2. Why are all images resized to 72*72 in the experiments? It is too small."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1673/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699510428931,
            "cdate": 1699510428931,
            "tmdate": 1699636095418,
            "mdate": 1699636095418,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]