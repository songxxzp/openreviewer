[
    {
        "title": "Inverse Approximation Theory for Nonlinear Recurrent Neural Networks"
    },
    {
        "review": {
            "id": "22szj8Qzry",
            "forum": "yC2waD70Vj",
            "replyto": "yC2waD70Vj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4420/Reviewer_6cRG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4420/Reviewer_6cRG"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates the inverse approximation theory for nonlinear recurrent neural networks (RNN) and extends the curse of memory from linear RNNs to nonlinear RNNs. The contribution of this paper can be summarized as follows.\n1. Authors define the memory of nonlinear RNNs, which is consistent with the memory of linear RNNs.\n2. Authors introduce a notion of stable approximation.\n3. Authors prove a Bernstein-type approximation theorem for nonlinear functional sequences through nonlinear RNNs, which says that if a target function can be stably approximated by nonlinear RNNs, then the target function must have exponential decaying memory.\n4. Based on the theoretical result, the authors propose a suitable parameterization that enables RNN to stably approximate target functions with non-exponential decaying memory."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. To the best of my knowledge, this is the first paper studying the inverse approximation theorem for nonlinear RNNs.\n2. The approximation of neural networks determines the existence of a low-error solution and has been widely investigated. The inverse approximation theorem, which is harder and less studied, concerns the efficiency of approximation, which is a crucial aspect of utilizing neural networks in practice.\n3. The nonlinearity is of central interest in empirical implementations and requires more challenging proofs."
                },
                "weaknesses": {
                    "value": "1. The definition of memory for nonlinear RNNs is consistent with that for linear RNNs, but the rationality of the proposed definition needs more explanations. The authors review the definition of memory for linear RNNs and observe that if the memory decays fast, then the target has short memory, and vice versa. The observation can be obtained directly from the definition and satisfies the intuition of memory. But for the memory for nonlinear RNNs, authors motivate it from a derivative form. This motivation is too mathematical and lacks an intuitive explanation. What is the relationship between the decaying speed of memory and the dependence of the target function on early inputs?\n2. The parameterization motivated by the theories should be explained in more detail. The parameterization is based on an important claim that the eigenvalue real part being negative leads to stability, which is not explained in section 3. This makes the logic in section 3.4 incomplete and the parameterization hard to understand. Can authors provide an intuitive explanation for this?\n----\nThe authors' answers are convincing and I have updated my ratings."
                },
                "questions": {
                    "value": "See weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4420/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4420/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4420/Reviewer_6cRG"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4420/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697985695147,
            "cdate": 1697985695147,
            "tmdate": 1700470857501,
            "mdate": 1700470857501,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oZq7pYwrlP",
                "forum": "yC2waD70Vj",
                "replyto": "22szj8Qzry",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4420/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4420/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their thoughtful comments and constructive feedback.\n\nWeaknesses:\n\n1. `The definition of memory for nonlinear RNNs is consistent with that for linear RNNs, but the rationality of the proposed definition needs more explanations.`\n\n(**Rationality of the proposed definition**)\nPeople use the outcomes of long-term memory tasks to demonstrate the long-term memory of models. \nAlthough the performance in common experiments, such as addition problems, copying tasks, and associative recalls is indicative of long-term memory attributes, it is important to note that these experiments are heuristic in nature.\nWe propose the memory function definitions for a precise, task-independent characterization of the model memories. (Definition 3.1 Page 5)\n\n`The authors review the definition of memory for linear RNNs and observe that if the memory decays fast, then the target has short memory, and vice versa. The observation can be obtained directly from the definition and satisfies the intuition of memory. But for the memory for nonlinear RNNs, authors motivate it from a derivative form. This motivation is too mathematical and lacks an intuitive explanation. What is the relationship between the decaying speed of memory and the dependence of the target function on early inputs?`\n\n(**Relation between memory decaying speed and target dependence on early inputs**)\nAn intuitive explanation is that current memory functions characterize how fast the memory about the sudden input change at $t=0$ decays as $t$ grows. \nInputs such as Heaviside inputs, reversed inputs, and impulse inputs all have this feature. \nTherefore the memory function evaluated over these inputs can be interpreted as **a measure for \"dependence of the target function on early inputs\".** \n$$\\rho_T := \\sup_{x \\neq 0} \\frac{|y_T(\\mathbf{i}^x)|}{|x|}  = \\sup_{x \\neq 0} \\frac{H_T((x, 0, ...))}{|x|}. \\quad \\textrm{Impulse-inputs-based memory}$$\nWe show the \"equivalence\" of different memory functions in linear functional sense in Appendix F and G. \nSince memory functions over Heaviside inputs are easier to evaluate numerically (as shown in Appendix H), we adopt the corresponding derivative form (as shown in Equation 5) to define the memory function. \n\n(**Why special inputs**)\nA natural question is why we need a particular set of inputs rather than use any continuous input. \nThe identity functional $H_t(\\mathbf{x}) = x_t$ takes the input sequence $x_t = \\frac{1}{t}, t > 0$ to a polynomial decaying output $y_t = \\frac{1}{t}$. However, this does not mean the functional has a polynomial-decaying memory. In fact, this identity functional has no memory of past inputs in the sense that changing $x_0$ does not influence the output of $y_t$ with $t > 0$. \nThe above example indicates the selection of inputs to extract the memory pattern needs to be chosen properly. \nThis is also one of the reasons we select the Heaviside inputs as the test inputs to construct the derivative form."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4420/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700301806351,
                "cdate": 1700301806351,
                "tmdate": 1700301806351,
                "mdate": 1700301806351,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JRcghEUUZa",
                "forum": "yC2waD70Vj",
                "replyto": "22szj8Qzry",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4420/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4420/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "2. `The parameterization motivated by the theories should be explained in more detail. The parameterization is based on an important claim that the eigenvalue real part being negative leads to stability, which is not explained in section 3. This makes the logic in section 3.4 incomplete and the parameterization hard to understand. Can authors provide an intuitive explanation for this?`\n\nThank you for raising this constructive question. \n\n(**Connection between negative eigenvalue real parts and stability**)\nFor continuous-time linear RNN, the bounded-input bounded-output stability is **equivalent** to eigenvalues of $W$ having negative real parts. \nOur proof of Bernstein theorem shows that the eigenvalues real part being negative is a necessary condition for stability of the continuous-time nonlinear RNN (see Lemma A.7 in Appendix A.10 Proof of Lemmas). \nTherefore the **stability boundary** for eigenvalues of $W$ is the line in complex plane with zero real parts $\\{z|\\textrm{Re}(z)=0\\}$. \n\n(**Intuitive explanation of the effects of reparameterization**) \nWithout reparameterization, the approximation of long-term memory requires the eigenvalue real parts come close to 0 from the negative direction, while the stability requires the eigenvalues bounded away from 0. \nTherefore the learning of long-term memory cannot be achieved stably. \nWith stable reparameterization, the approximation of long-term memory still requries the eigenvalues to be close to 0, but the stability does not enforce the eigenvalues to be bounded away from 0. \nTherefore it's possible to learn long-term memories stably. \n\n(**Motivation for reparameterization**)\nTheorem 3.9 proves that if the eigenvalues are bounded away with distance $\\beta$ from the stability boundary, then the memory decays exponentially with speed $e^{- \\beta t}$. \nTherefore, when models are learning targets with long-term memories, the eigenvalues are required to come close to the stability boundary. \nIn Section 3.4 (Page 7 and 8), we show that stable reparameterization allows the eigenvalues to come close to the stability boundary $\\{z|\\textrm{Re}(z)=0\\}$ without having the stability issue. \n\n(**Empirical evidence**)\nThe reparameterization method has been adopted in various previous works ([1,2,3,4,5]) to improve the optimization of recurrent models such as spectral RNN and state-space models. \nHowever, the previous works usually attribute the performance to the optimization stability of reparameterization. \nOur work highlights that stable reparameterization plays a dual role: it not only contributes to the stable training of dynamical system, but is also **necessary for the learning of long-term memories**. \n\n--- \n\nReferences:\n\n[1]. (SVD parameterization) Zhang, Jiong, Qi Lei, and Inderjit Dhillon. \"Stabilizing gradients for deep neural networks via efficient svd parameterization.\" In International Conference on Machine Learning, pp. 5806-5814. PMLR, 2018.\n\n[2]. (S4) Albert Gu, Karan Goel, and Christopher Re. \"Efficiently Modeling Long Sequences with Structured State Spaces.\" In International Conference on Learning Representations. 2021\n\n[3]. (DSS) Gupta, Ankit, Albert Gu, and Jonathan Berant. \"Diagonal state spaces are as effective as structured state spaces.\" Advances in Neural Information Processing Systems 35 (2022): 22982-22994.\n\n[4]. (S5) Smith, Jimmy TH, Andrew Warrington, and Scott Linderman. \"Simplified State Space Layers for Sequence Modeling.\" In The Eleventh International Conference on Learning Representations. 2022.\n\n[5]. (LRU) Orvieto, Antonio, Samuel L. Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. \"Resurrecting recurrent neural networks for long sequences.\" arXiv preprint arXiv:2303.06349 (2023)."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4420/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700301827449,
                "cdate": 1700301827449,
                "tmdate": 1700301827449,
                "mdate": 1700301827449,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "COirvl2LxK",
                "forum": "yC2waD70Vj",
                "replyto": "JRcghEUUZa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4420/Reviewer_6cRG"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4420/Reviewer_6cRG"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your answers. I am convinced and have updated my rating."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4420/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700470917197,
                "cdate": 1700470917197,
                "tmdate": 1700470917197,
                "mdate": 1700470917197,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DlFPNFP97G",
            "forum": "yC2waD70Vj",
            "replyto": "yC2waD70Vj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4420/Reviewer_dqeg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4420/Reviewer_dqeg"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces an inverse approximation theorem for non-linear recurrent neural networks, extending known, results for the linear case. After introducing new tools that enable analyzing nonlinear dynamics, it shows a somewhat negative result that nonlinear RNNs suffer from the same curse of memory as linear RNNs. Finally, the authors suggest a type of reparametrization that could possibly enable stability and good approximation properties."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "As an outsider to the field, the paper introduces/uses formal concepts that I find insightful:\n\n- The memory function that is used in the paper mathematically characterizes the intuitive behavior of RNNs.\n- The notion of stable approximation is an interesting proxy for how a function can be learned by gradient descent which, from my very limited knowledge, seems to be rare in approximation theory.\n\nOverall, I found overall the paper well written, in a way that is accessible to a decently large audience."
                },
                "weaknesses": {
                    "value": "I am not qualified enough to identify the weaknesses of the paper."
                },
                "questions": {
                    "value": "I am a bit confused with the Heaviside input at the end of Page 4. Intuitively, I would have put it the other way around: the network is submitted to an input $x$ until 0, and then evolves autonomously later. This would allow us to measure how fast the network forgets about $x$. Can you elaborate on the choice in the paper?\n\nThe reparametrization that you propose reminds me of the one used in the LRU architecture (Orvieto et al. 2023, ICML) and present in some deep-state space models (see references in the LRU paper), it may be nice to mention this. I\u2019m not sure I fully understand what your theoretical results show for the reparametrization. To the best of my understanding, even with such a reparametrization, RNN would still only be able to learn in the same manner targets with decaying memory. Can you elaborate more on that?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4420/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4420/Reviewer_dqeg",
                        "ICLR.cc/2024/Conference/Submission4420/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4420/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698391963278,
            "cdate": 1698391963278,
            "tmdate": 1700472166111,
            "mdate": 1700472166111,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Y5yMAtsL32",
                "forum": "yC2waD70Vj",
                "replyto": "DlFPNFP97G",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4420/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4420/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the positive feedback. \n\nQuestions:\n\n1. `I am a bit confused with the Heaviside input at the end of Page 4. Intuitively, I would have put it the other way around: the network is submitted to an input $x$ until 0, and then evolves autonomously later. This would allow us to measure how fast the network forgets about $x$. Can you elaborate on the choice in the paper?`\n\nThank you for proposing the new scheme for memory function evaluation.\nWe hereafter call the new input scheme \"reversed inputs\". \nLet $\\mathbf{x}$ be constant inputs and $\\mathbf{u}^x$ be Heaviside inputs, the reversed inputs can be written as\n$$\\mathbf{r}^x = \\mathbf{x} - \\mathbf{u}^x.$$ \nThe new scheme can also be used to define a memory function and establish a similar result as Theorem 3.9 (see Appendix G, Page 28). \nHere we show the equivalence to our definition in the following sense:\n\n(**Equivalence over linear functionals**)\nBased on the linear functional representation, $H_t(\\mathbf{r}^x) = C - H_t(\\mathbf{u}^x)$ for some constant $C$. \nTherefore the memory functions defined via $|\\frac{dH_t}{dt}|$ are the same for both Heaviside inputs and reversed inputs. \nThey can extract the same memory function $|\\rho|$ of the linear functional. \n\n(**Numerical equivalence over nonlinear functionals**)\nIn the nonlinear case, they are not exactly the same but the empirical experiments indicate the qualitative behaviours are usually the same. \nIn Appendix H, Figure 10 (Page 29), we construct randomly-initialized recurrent models and evaluate the memory functions over different inputs to demonstrate the numerical equivalence. \nThis similarity indicates that memory function is a good surrogate tool to compare the memory patterns. \n\n(**Intuitive explanation of equivalence**)\nFor both Heaviside and reversed inputs, the input change occurs at time $t=0$. \nAs time $T$ increases, the memory function $\\rho(T)$ reflects the impact of input change after time $T$. \nTherefore the Heaviside inputs and reversed inputs are equivalent in characterizing the long-term memory effects. \n\nOne of the reasons to choose Heaviside inputs over reversed inputs is that the Heaviside inputs are easier to evaluate numerically (see Appendix H, Page 28-30)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4420/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700301706049,
                "cdate": 1700301706049,
                "tmdate": 1700301706049,
                "mdate": 1700301706049,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "s7EaaAMtMy",
                "forum": "yC2waD70Vj",
                "replyto": "DlFPNFP97G",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4420/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4420/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "2. `The reparametrization that you propose reminds me of the one used in the LRU architecture (Orvieto et al. 2023, ICML) and present in some deep-state space models (see references in the LRU paper), it may be nice to mention this. I\u2019m not sure I fully understand what your theoretical results show for the reparametrization. To the best of my understanding, even with such a reparametrization, RNN would still only be able to learn in the same manner targets with decaying memory. Can you elaborate more on that?`\n\nWe have added the LRU [1] and related works [2,3,4,5] to the stable reparameterization discussion in Section 3.4. \n\nThe reparameterization does not fundamentally change the models' exponential decay pattern. \nBoth Theorem 3.9 and Section 3.4 have no contradiction with this result. \nTheorem 3.9 shows that without reparameterization, when learning long-term memory, one cannot achieve stability and approximation at the same time. \nBut the inverse approximation theorem does not limit the memory pattern of reparameterized models. \n**Section 3.4 illustrates that with stable reparameterization, we can achieve the approximation of long-term memory without sacrificing the stability.** \nIn particular, in Figure 4, we show that the approximation of polynomial-memory target can be achieved stably with the RNN using exponential or softplus parameterization.\n\n\n\n--- \n\nReferences:\n\n[1]. (LRU) Orvieto, Antonio, Samuel L. Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. \"Resurrecting recurrent neural networks for long sequences.\" arXiv preprint arXiv:2303.06349 (2023).\n\n[2]. (HIPPO) Gu, Albert, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. \"Hippo: Recurrent memory with optimal polynomial projections.\" Advances in neural information processing systems 33 (2020): 1474-1487.\n\n[3]. (S4) Albert Gu, Karan Goel, and Christopher Re. \"Efficiently Modeling Long Sequences with Structured State Spaces.\" In International Conference on Learning Representations. 2021\n\n[4]. (S5) Smith, Jimmy TH, Andrew Warrington, and Scott Linderman. \"Simplified State Space Layers for Sequence Modeling.\" In The Eleventh International Conference on Learning Representations. 2022.\n\n[5]. (Universality and memory decay of SSM) Wang, Shida, and Beichen Xue. \"State-space Models with Layer-wise Nonlinearity are Universal Approximators with Exponential Decaying Memory.\" Advances in neural information processing systems 37 (2023)."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4420/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700301744582,
                "cdate": 1700301744582,
                "tmdate": 1700301744582,
                "mdate": 1700301744582,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ursQDFj0cv",
                "forum": "yC2waD70Vj",
                "replyto": "s7EaaAMtMy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4420/Reviewer_dqeg"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4420/Reviewer_dqeg"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your answers. The paper seems to be a solid contribution, I have increased my rating to 8."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4420/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700472151468,
                "cdate": 1700472151468,
                "tmdate": 1700472151468,
                "mdate": 1700472151468,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zJwK9K6L0T",
            "forum": "yC2waD70Vj",
            "replyto": "yC2waD70Vj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4420/Reviewer_aNtd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4420/Reviewer_aNtd"
            ],
            "content": {
                "summary": {
                    "value": "This paper demonstrates a Bernstein-type result for nonlinear Recurrent Neural Networks, meaning that it characterizes the type of functions that can be approximated by RNNs. The result shows that if a target sequence can be approximated by a RNN, then it has an exponentially decaying memory (for a notion of memory made precise in the paper). A similar result was proven in the literature for linear RNNs and this paper extends the result to the non-linear case."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is well presented and pleasant to read. Extending previously known results to the non-linear case is interesting and brings added value. It gives a theoretical framework to understand the common saying that RNNs are unable to learn long-time dependencies in time series. A modification of the parametrization of RNNs is proposed to remedy this problem.\n\nTechnically, the paper introduces a new notion of memory that holds for nonlinear functionals and which extends the linear case, as well as a notion of stable approximation (which corresponds to the existence of a ball of parameters with uniformly good approximation properties). This proof technique is interesting and could perhaps be applied to other architectures.\n\nExperiments support the theoretical findings."
                },
                "weaknesses": {
                    "value": "I have no strong reservations about the paper. I do have a question about Figure 3, which I did not understand. I am willing to raise the rating should this interrogation be answered. [Update on Nov. 18: the authors clarified this point in their comments below, and I updated my rating accordingly].\n\nI do not understand the filtering part of the experiment. Since we know from the theoretical results that RNNs can only represent exponentially decaying functions, the teacher models should all be exponentially decaying? So why is any filtering required? Furthermore, if indeed filtering is required, is it also performed before plotting the left-side plot? Otherwise we should be seeing some non-converging data points?\nAs a consequence, I am wondering if the takeaway of the experiment should be \u201cstable approximation implies exponential decay\u201d or rather \u201cexponential decay implies stable approximation\u201d.\n\n**Minor remarks that do not influence the rating**\n+ Page 5: I found the paragraph above Definition 3.2 hard to follow. I am not sure to understand the reason for introducing the terminology  \u201cqueried memory\u201d. The experiments of Appendix B are important in my opinion because they suggest that RNNs may share the exponential decay property with LSTMs, but not Transformer. But I do not understand why they are introduced at this point in the paper, and not later on, e.g. in the conclusion.\n+ Page 7: the proof sketch is difficult to follow, and I am not sure it brings significant added value. If you need additional space in the main paper, I\u2019d suggest putting the proof sketch in the Appendix just before the proof.\n+ Page 7, line -2: \u201cIn order\u201d -> \u201cin order\u201d\n+ Page 8: \u201capproaching 0\u201d -> \u201capproaching 0 on the negative side\u201d?\n+ Page 8: \u201cTake exponential\u2026\u201d -> \u201cTaking exponential\u201d\n+ Page 8, figures 2 and 3: please avoid using plus sign and arrow inside the text. Consider replacing it by \u201cand\u201d and \u201cimplies\u201d.\n+ Page 16: \u201cacivations\u201d -> \u201cactivations\u201d\n+ Page 16: could you briefly explain or give a reference for Lyapunov equations?"
                },
                "questions": {
                    "value": "Do the authors think their proof technique could also be applied to LSTMs/GRUs? I imagine there would be many technical difficulties, but conceptually do the authors think it would be possible? Giving this insight in the paper might be interesting for readers.\n\nSee also weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4420/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4420/Reviewer_aNtd",
                        "ICLR.cc/2024/Conference/Submission4420/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4420/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698396379332,
            "cdate": 1698396379332,
            "tmdate": 1700314788156,
            "mdate": 1700314788156,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Fb7SRqOTHv",
                "forum": "yC2waD70Vj",
                "replyto": "zJwK9K6L0T",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4420/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4420/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the positive feedback. \n\nWeaknesses:\n\n`I have no strong reservations about the paper. I do have a question about Figure 3, which I did not understand. I am willing to raise the rating should this interrogation be answered.`\n\n`I do not understand the filtering part of the experiment. Since we know from the theoretical results that RNNs can only represent exponentially decaying functions, the teacher models should all be exponentially decaying? So why is any filtering required? Furthermore, if indeed filtering is required, is it also performed before plotting the left-side plot? Otherwise we should be seeing some non-converging data points? As a consequence, I am wondering if the takeaway of the experiment should be \u201cstable approximation implies exponential decay\u201d or rather \u201cexponential decay implies stable approximation\u201d.`\n\nYes, the takeaway for the filtering experiment is \"stable approximation and decaying memory implies exponential decay\". \nThis paper primarily focuses on the inverse-type approximation theorem.\nThe assertion that \"Exponential decay of the target implies stable approximation\" is a forward-type (Jackson-type) result that has not been established as of now.\n\nIn our theory, we show that nonlinear RNNs with decaying memory are decaying exponentially. \nRandomly initialized nonlinear RNNs might not have decaying memory. \nIn Appendix I.1 (Page 30) and Figure 11 (Page 31), we present examples of nonlinear RNNs having non-decaying memory.\nIn particular, in Figure 11 (b), the trajectory of hidden state derivatives indicate the existence of periodic memory function from the 2D tanh RNN. \nTherefore, we construct several targets in the experiment for Figure 3 and use decaying memory and existence of stable approximation by smaller RNNs as the criterions to filter the targets teacher models. \n\nThe reason we pick larger RNNs as teacher models is that nonlinear RNNs are universal approximators. \nTherefore from the density perspective, larger models with randomly initialized weights should have more diverse memory pattern than the smaller models.\nWe also investigate the memory patterns of transformers. \nIt can be seen in Appendix I.2 (Page 31 and 32) that transformers do not have a decaying memory so the stable approximation cannot be achieved by nonlinear RNNs: We failed to achieve a smaller validation loss with larger models ($m=64$) over 1000 epochs of training.\n\n\n\n`Minor remarks`: Thank you for the detailed writing suggestions and pointing out the typos, we have corrected them in the paper revision (marked in blue). \n1. Page 5: The queried memory is proposed as an empirical evaluation of the target memory. \n    As our memory function is defined by a derivative form, the queried memory is the memory evaluated in practice to approximate the memory function. \n    In Appendix F and G (Page 28), we discuss the memory function queried by other test inputs and the equivalence over linear functional are discussed. \n    Numerical equivalence is demonstrated with the memory function of the randomly-initialized models in Appendix H (Page 29). \n2. Page 7: We move the proof sketch to the Appendix and replace it with the discussion on the extension to GRU/LSTM. \n3. Page 16: Lyapunov equations: $W_m^T P_m + P_m W_m = -Q_m$ is a commonly used equation in control theory and stability analysis. \n    We add the reference for this equation from Nonlinear System [1]."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4420/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700301552249,
                "cdate": 1700301552249,
                "tmdate": 1700301552249,
                "mdate": 1700301552249,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ckqijp5LvF",
                "forum": "yC2waD70Vj",
                "replyto": "zJwK9K6L0T",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4420/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4420/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Questions:\n\n1. `Do the authors think their proof technique could also be applied to LSTMs/GRUs? I imagine there would be many technical difficulties, but conceptually do the authors think it would be possible? Giving this insight in the paper might be interesting for readers.`\n\nThe proof of Theorem 3.9 relies on the limiting behaviour analysis around the equilibrium points.\n**We give a brief discussion for the analysis for GRU, the detailed calculation is given in Appendix J** (Page 33). \nFor nonlinear RNNs, the hidden state dynamics over Heaviside inputs $\\mathbf{u}^x$ is\n    $$\\frac{dh_t}{dt} = \\sigma(Wh_t + b).$$\nFor GRU, based on the update rule $h_t = (1-z_t) \\odot h_{t-1} + z_t \\odot \\hat{h_t} $ and corresponding definition for gates $z_t$, the continuous hidden state dynamics over Heaviside inputs is\n    $$\\frac{dh_t}{dt} =\\sigma(U_z h_t + b_z) \\odot (\\phi(U_h (\\sigma(U_r h_t + b_r) \\odot h_t) + b_h) - h_{t}).$$\nFor nonlinear RNNs, the asymptotic behaviour can be characterized via linearization around equilibrium points. \nHowever, the hidden dynamics of GRU are more complex. \nIf $b_h = 0$, we show $h^*=0$ is an equilibrium point. \n$$\\sigma(b_z) \\odot (\\phi(U_h (\\sigma(b_r) \\odot 0) + 0) - 0) = 0.$$\nThe same linearization method from Theorem 3.9 can be used and the assumption of stable approximation requires $\\textrm{Diag}(\\sigma(b_z)) (U_h \\textrm{Diag}(\\sigma(b_r)) - I)$ to be Hurwitz. \nSimilar exponential decay memory can be obtained around the analysis of $h^*=0$. \nHowever, $h^*=0$ might not the only equilibrium points. \nThe general case with $b_h \\neq 0 $ is harder to analyze as the equilibrium points are much harder to solve. \nIt requires more techniques to decide the convergence around equilibrium points for GRU. \nThe case for LSTM is more complex as the model structure is more complicated than that of RNN and GRU. \n\nTo summarize, our techniques can be applied to GRU or LSTM but further analyses are needed to extend the result.\n\n--- \nReferences:\n\n[1]. Hassan K. Khalil. Nonlinear systems third edition (2002), 2002"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4420/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700301668537,
                "cdate": 1700301668537,
                "tmdate": 1700301668537,
                "mdate": 1700301668537,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nulKVVbpAb",
                "forum": "yC2waD70Vj",
                "replyto": "ckqijp5LvF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4420/Reviewer_aNtd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4420/Reviewer_aNtd"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your detailed answer that clarifies all my questions. I have updated my rating."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4420/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700314708537,
                "cdate": 1700314708537,
                "tmdate": 1700314708537,
                "mdate": 1700314708537,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "88bDPNR93J",
            "forum": "yC2waD70Vj",
            "replyto": "yC2waD70Vj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4420/Reviewer_NURo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4420/Reviewer_NURo"
            ],
            "content": {
                "summary": {
                    "value": "The authors provides an inverse approximation theory for a certain class of nonlinear recurrent neural network (RNN), showing that, for the class of nonlinear RNN to approximate the sequence, the sequence should have an exponential decaying memory structure. It demonstrates the difficulties of RNN in learning and representing a longer-time dependence in the data."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is relatively well written and it extends the previous theories in linear RNN. While heuristically it is not difficult to show that a popular RNN, such as LSTM or GRU, has an exponentially decaying memory, by approximating the internal state of the RNN with a relaxation equation, this manuscript provides a robust proof about such behavior."
                },
                "weaknesses": {
                    "value": "While it the manuscript is clearly written with well defined problem set up, my concern is the fit to the scope of ICLR. On one hand, I believe that the analysis and definitions provided in the manuscript can be of interest in developing theories about RNN. On the other hand, the scope of the manuscript is too narrow and it is unclear what insight this study provides to benefit a broader class of RNNs or Deep Learning in general. I believe that the authors need to consider giving more insights from their results for a broader context. The stable parameterization section is not very convincing. Can you give a more concrete example about the effects of the reparameterization? Possibly, using a dynamical system or real data?"
                },
                "questions": {
                    "value": "NA"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4420/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698697168279,
            "cdate": 1698697168279,
            "tmdate": 1699636415889,
            "mdate": 1699636415889,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "S3Je7oKLcz",
                "forum": "yC2waD70Vj",
                "replyto": "88bDPNR93J",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4420/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4420/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their thoughtful comments and constructive feedback.\n\nWeaknesses:\n\n1. `While it the manuscript is clearly written with well defined problem set up, my concern is the fit to the scope of ICLR. On one hand, I believe that the analysis and definitions provided in the manuscript can be of interest in developing theories about RNN. On the other hand, the scope of the manuscript is too narrow and it is unclear what insight this study provides to benefit a broader class of RNNs or Deep Learning in general.`\n\nThe scope of this paper is the theoretical analysis of nonlinear RNNs' memory pattern. \nThis paper presents the first inverse approximation result for nonlinear RNNs. \nIt is important for the understanding and comparison of model architectures. \nIn particular, our paper points out the memory limitation of nonlinear RNNs does not only come from the optimization difficulty (such as vanishing/exploding gradient issues [1, 2]), but also more importantly comes from the model architecture. \n\nThe important practical insights are as follows.\n- We define a memory function for nonlinear functionals that can be used to compare the memory patterns of different targets and models.\n- We give a definition of stable approximation, which is a more suitable approximation assumption in the setting of gradient-based optimization. \n- We prove the first inverse approximation theorem that shows the memory limitation of nonlinear RNNs comes from the model architecture. \n- Our analysis points out the stable parameterization as a solution to memory limitation of the architecture. \n\nThe memory function and stable approximation concepts can be adopted in the study of **any** sequence-to-sequence relationships. \nBased on these two concepts, we prove in Theorem 3.9 (Page 7) that nonlinear functionals learned by nonlinear RNNs must exhibit an exponentially decaying memory. \nOur result shows that the exponentially decaying memory is an inherent limitation of the architecture. \nThe vanishing/exploding gradient phenomenon is not the only issue for learning long-term memory, and improving optimization itself is not sufficient to resolve the memory issue. \nFurthermore, in Section 3.4 first paragraph (Page 7), we identify reparameterization of recurrent weights as a feasible solution to resolve the memory limitation of recurrent architecture."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4420/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700301464133,
                "cdate": 1700301464133,
                "tmdate": 1700301464133,
                "mdate": 1700301464133,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Z13qGqzzwO",
                "forum": "yC2waD70Vj",
                "replyto": "88bDPNR93J",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4420/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4420/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "2. `I believe that the authors need to consider giving more insights from their results for a broader context. The stable parameterization section is not very convincing. Can you give a more concrete example about the effects of the reparameterization? Possibly, using a dynamical system or real data?`\n\n**The learning of long-term memory requires the largest eigenvalue real parts of recurrent weights converging to 0 (from negative direction).**\n\nThe effects of reparameterization are two-fold. \n- For approximation, based on the proof of Bernstein theorem, we argue that stable parameterization can be used to resolve the memory limitation of nonlinear RNNs. \n    In the first paragraph of Section 3.4 (Page 7), we show that the stable parameterization enables the radius-$\\beta$ stability without forcing the eigenvalues of recurrent weights to be bounded away from 0. \n    **Without reparameterization, the stability requires the eigenvalues to be bounded away from 0. With stable parameterization, the stability can be achieved without enforcing the eigenvalues to be bounded away from 0.**\n    A broad class of parameterizations, capable of overcoming the intrinsic memory limitation of the nonlinear RNN architecture, has been identified in Section 3.4.\n- For optimization, reparameterization is used to improve the optimization stability in linear time-invariant system and state-space models. \n    The exponential and softplus parameterizations have been adopted for the stability of linear RNN layer in state-space models [3,4,5]. \n\nAny linear time-invariant dynamical system corresponds to a linear functional. \nThe synthetic dataset of linear functional with polynomial memory $y_t = H_t(\\mathbf{x}) = \\int_{-\\infty}^t \\rho(t-s) x_s ds,$ $\\rho(t-s) = \\frac{1}{(t+1)^{1.5}}$ serves as a dynamical system example with long-term memory. \nIn Figure 2 and Figure 4, we show that RNNs without reparameterization cannot learn target with polynomial memory and RNN using exponential parameterization and softplus parameterization can stably approximate the polynomial memory targets. \nThe details of this dynamical system example are given in Appendix D. \n**In Appendix E, we further demonstrate that three stable parameterizations improve the learning of long-term memory**, as evidenced by accelerated optimization in image classification tasks on MNIST. \nCompared to scenarios without reparameterization, these stable parameterizations demonstrate better final validation and test accuracies over three repeats:\n\n|         | Test loss(std err)      | Test accuracy (std err)|\n|---------|-------------------------|--------------|\n| softplus (stable)| 0.8903 (0.0004)         | 71.36(0.03)  |\n| exp (stable)     | 0.9028 (7.2E-05)        | 70.55(0.01)  |\n| inverse (stable) | 0.9082 (0.0005)         | 70.77(0.05)  |\n| direct (unstable)  | 0.9177 (0.0105)         | 68.47(0.06)  |\n\n\n---\n\nReferences\n\n[1]. Bengio, Yoshua, Patrice Simard, and Paolo Frasconi. \"Learning long-term dependencies with gradient descent is difficult.\" IEEE transactions on neural networks 5, no. 2 (1994): 157-166.\n\n[2]. (Vanishing gradient) Hochreiter, Sepp. \"The vanishing gradient problem during learning recurrent neural nets and problem solutions.\" International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems 6, no. 02 (1998): 107-116.\n\n[3]. (LRU) Orvieto, Antonio, Samuel L. Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. \"Resurrecting recurrent neural networks for long sequences.\" arXiv preprint arXiv:2303.06349 (2023).\n\n[4]. (S4) Albert Gu, Karan Goel, and Christopher Re. \"Efficiently Modeling Long Sequences with Structured State Spaces.\" In International Conference on Learning Representations. 2021\n\n[5]. (S5) Smith, Jimmy TH, Andrew Warrington, and Scott Linderman. \"Simplified State Space Layers for Sequence Modeling.\" In The Eleventh International Conference on Learning Representations. 2022."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4420/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700301496052,
                "cdate": 1700301496052,
                "tmdate": 1700301496052,
                "mdate": 1700301496052,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]