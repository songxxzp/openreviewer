[
    {
        "title": "ProbTS: A Unified Toolkit to Probe Deep Time-series Forecasting"
    },
    {
        "review": {
            "id": "thruv3HOSe",
            "forum": "wMXH8tTQE3",
            "replyto": "wMXH8tTQE3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7486/Reviewer_kFUJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7486/Reviewer_kFUJ"
            ],
            "content": {
                "summary": {
                    "value": "The paper highlights two main directions of deep learning for time series forecasting - architecture design, and probabilistic forecasting heads. They present a new library which attempts to address both directions, and present some benchmark results and empirical studies."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper presents a nice position and overview on the research directions for time series forecasting within the deep learning community."
                },
                "weaknesses": {
                    "value": "Unfortunately, this paper tries to do too much and too little at the same time.\n1. As a paper introducing a new library, there is insufficient details of the design and implementation of the library. It also has insufficient comparison with existing libraries - what sets it apart from existing work? \n\n    a. Table 1 does not really make sense -- the header for column 1 is \"Model\", all the comparisons are different models, but ProbTS is not a model. It would make more sense to compare ProbTS with other libraries/packages (e.g. GluonTS, TSLib, etc.) rather than specific models/papers.\n\n    b. More attributes for libraries should be compared -- metrics, datasets, data transformations, data loading efficiency, ...\n\n    c. More library comparisons should be added [1, 2, 3], and many others.\n\n    d. The characterization of GluonTS as \"each specializing in a single forecasting paradigm, fall short of our research objective to unify the two distinct research branches\" is not accurate -- new architectures can and have been implemented in it. Also see how it has been used in [4]. \n\n2. As a benchmark paper, it fails to perform a comprehensive evaluation in both dimensions of architecture design and probabilistic forecasting head. \n\n    a. In Table 4, only a small number of methods from each dimension has been evaluated on. \n\n    b. A more comprehensive evaluation, combining different architectures with different probabilistic forecasting heads can be presented.\n\n3. As a an empirical study, it does not yield any definitive insights into the interplay between architecture design and probabilistic forecasting head.\n\n    a. More insights regarding various architecture designs should be given -- e.g. for architectures like Autoformer -- how can be attach probabilistic heads, since the architecture design outputs the prediction based on seasonality + trend? What about PatchTST, how does patching affect probabilistic heads?\n\nNote that I am not saying the paper should achieve everything mentioned above, but one particular direction should be chosen to go all in.\n\n[1] https://github.com/unit8co/darts\n\n[2] https://github.com/salesforce/Merlion\n\n[3] https://github.com/facebookresearch/Kats\n\n[4] https://github.com/kashif/pytorch-transformer-ts"
                },
                "questions": {
                    "value": "None"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7486/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7486/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7486/Reviewer_kFUJ"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7486/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698639419230,
            "cdate": 1698639419230,
            "tmdate": 1699636903324,
            "mdate": 1699636903324,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "N1ybuZ7OS8",
                "forum": "wMXH8tTQE3",
                "replyto": "thruv3HOSe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7486/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7486/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer kFUJ (Part 1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your thorough review of our paper. We apologize for any misunderstanding and lack of clarity regarding the strengths of our work. We hope the following clarifications and explanations can help address your concerns.\n\n**Response to Weakness 1**\n\nWe would like to clarify that the primary contribution of this paper is to unify two key aspects in deep time-series forecasting: customized neural architecture designs and advanced probabilistic estimations tailored for time series. **This work is set apart from existing studies in that we strive to unify these two aspects and expose underexplored yet crucial topics, accompanied by our findings and insights, to the research community**. Specifically, this paper discloses and analyzes several unanswered questions:\n\n- How do different data characteristics and forecasting horizons influence methodological designs in previous studies? What are the implications when evaluating them in underexplored scenarios?\n- What are the strengths and weaknesses of either focusing on neural architecture designs or emphasizing advanced probabilistic estimations? How will existing approaches perform on different evaluation metrics, when forecasting distributional or point-level forecasts?\n- Why does the development of these two research branches lead to different preferences on decoding schemes, either autoregressive or non-autoregressive?\n\nWe've briefly summarized our insights towards these questions in the introduction of our paper and included more detailed results and analyses in Section 4. What we want to clarify and emphasize most is that **ProbTS is not a traditional tool** aiming to consolidate all library attributes, such as metrics, datasets, data transformations, and data loading efficiency, into one place. Instead, we envision **ProbTS as a unique research-oriented tool** that unifies divergent yet equally important research in deep time-series forecasting, reveals overlooked challenges, and opens up new avenues for future research.\n\nIn response to your constructive suggestions regarding the wording of the \"Model\" column in Table 1 and the additional discussions between this work and existing tools, we have made appropriate adjustments. For instance, we have changed the \"Model\" column to \"Studies\", indicating that each entry in this column represents an individual paper studying specific problems. We have also compared ProbTS with existing tools from various perspectives, as shown in the following table.\n\n| Tools | Provide datasets | Short-term Benchmarking | Long-term Benchmarking | Prob. Forecasting | Advanced NN Arch. |\n| :--- | :---: | :---: | :---: | :---: | :---: |\n| Darts [4] | \u221a | x | x | \u221a | \u221a |\n| Merlion [5] | \u221a | \u221a | x | x | \u221a |\n| Kats [6] | x | x | x | x | x |\n| pytorch-transformer-ts [3] | \u221a | x | x | x | \u221a |\n| TSlib [1] | \u221a | \u221a | \u221a | x | \u221a |\n| GluonTS [2] | \u221a | \u221a | x | \u221a | x |\n| ProbTS | \u221a | \u221a | \u221a | \u221a | \u221a |\n\nIndeed, GluonTS is an excellent tool that comes closest to ours. However, when we attempt to unify two different research branches, we find that GluonTS is highly wrapped and leans toward probabilistic forecasting, making it difficult to incorporate different data preprocessing, normalization, and evaluation implementations used by existing studies in neural architecture designs. As a result, we decided to develop ProbTS from scratch as a research-oriented tool specializing in unifying neural architecture designs and probabilistic estimations in deep time-series forecasting.\n\n[1] Alexandrov, Alexander, et al. \"Gluonts: Probabilistic time series models in python.\" *arXiv preprint arXiv:1906.05264* (2019).\n\n[2] [https://github.com/thuml/Time-Series-Library](https://github.com/thuml/Time-Series-Library)\n\n[3] [https://github.com/kashif/pytorch-transformer-ts](https://github.com/kashif/pytorch-transformer-ts)\n\n[4] [https://github.com/unit8co/darts](https://github.com/unit8co/darts)\n\n[5] [https://github.com/salesforce/Merlion](https://github.com/salesforce/Merlion)\n\n[6] [https://github.com/facebookresearch/Kats](https://github.com/facebookresearch/Kats)"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7486/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700502116327,
                "cdate": 1700502116327,
                "tmdate": 1700502116327,
                "mdate": 1700502116327,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NVJLcHzD8i",
                "forum": "wMXH8tTQE3",
                "replyto": "thruv3HOSe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7486/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7486/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer kFUJ (Part 2/2)"
                    },
                    "comment": {
                        "value": "**Response to weakness 2 & 3**\n\nThank you for your comments. We appreciate your observations and would like to emphasize that our evaluation does indeed provide a robust comparison of existing methods across two distinct research branches.\n\nFirstly, for our benchmarking, we have included the most representative and top-performing baselines from each branch. This includes highly competitive neural architectures such as PatchTST, TimesNet, D-linear, N-HiTS, as well as recent advanced probabilistic forecasting approaches like CSDI, TimeGrad, and Trans MAF. We are certainly open to incorporating additional methods into our comparisons. If you could suggest any that surpass the performance or possess unique advantages over these existing baselines, we would be more than willing to consider them.\n\nSecondly, we've created some ablation variants by removing the probabilistic head from GRU NVP and Trans MAF, which are represented by the GRU and Transformer models in Table 4. Concerning your proposal of stacking probabilistic heads on customized architectures, such as Autoformer and PatchTST, it's crucial to note that this isn't a simple undertaking. It either necessitates transforming these non-autoregressive architectures to the autoregressive scheme (as seen in GRU MAF, GRU NVP, Trans MAF, TimeGrad) or integrating probabilistic modules in a non-autoregressive manner. For example, CSDI combines architecture designs with non-autoregressive probabilistic estimations. However, CSDI struggles in long-term forecasting due to its increasing inefficiency as the forecasting horizon expands, and its inability to handle long trending or strong seasonality effectively.\n\nIn summary, the task of combining customized architectures with advanced probabilistic estimations presents many unresolved challenges and is not as simple as adding a probabilistic head over an existing architecture. After disclosing and analyzing these remaining problems in this paper, we believe there are many opportunities ahead to explore how we can better combine customized architectures with advanced probabilistic estimations while addressing the limitations of existing probabilistic forecasting methods. In this regard, ProbTS aims to serve as a foundational tool to facilitate future research."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7486/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700502167664,
                "cdate": 1700502167664,
                "tmdate": 1700502167664,
                "mdate": 1700502167664,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gBqxgaAWZf",
                "forum": "wMXH8tTQE3",
                "replyto": "thruv3HOSe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7486/Reviewer_kFUJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7486/Reviewer_kFUJ"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the response. My thoughts and score currently remains the same.\n\n1. Could you explain concretely what you mean by \"ProbTS as a unique research-oriented tool that unifies divergent yet equally important research in deep time-series forecasting, reveals overlooked challenges, and opens up new avenues for future research.\"? What are the features or toolkit design that allows ProbTS to be so unique?\n2. I disagree with the characterization that \"we find that GluonTS is highly wrapped and leans toward probabilistic forecasting, making it difficult to incorporate different data preprocessing, normalization, and evaluation implementations used by existing studies in neural architecture design\". GluonTS has an extensive data pre-processing pipeline (https://github.com/awslabs/gluonts/tree/dev/src/gluonts/transform), supports many normalization techniques (https://github.com/awslabs/gluonts/blob/dev/src/gluonts/torch/scaler.py), and has a full suite of evaluation metrics (https://github.com/awslabs/gluonts/tree/dev/src/gluonts/ev). It is also easily extendible, allowing to add new transforms, models, and evaluation metrics.\n3. It is difficult to evaluate the benefits of ProbTS without looking at the code or any design documents about the library.\n4. GluonTS already implements 2 of 4 of the \"advanced neural architectures\" mentioned, PatchTST and DLinear. \n5. pytorch-transformer-ts uses GluonTS to implement many more \"advanced neural architectures\", and it does probabilistic forecasting.\n6. The paper is very limited by not analyzing the interplay between architecture and probabilistic head. How can we disambiguate the effects of architecture and probabilistic head? Does performance stem from different architecture or head? Yes, it is not straightforward, but that would constitute a significant contribution rather than simply benchmarking different methods which arguably are not fair comparisons with each other."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7486/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700532521623,
                "cdate": 1700532521623,
                "tmdate": 1700532521623,
                "mdate": 1700532521623,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Tz6E74oKty",
                "forum": "wMXH8tTQE3",
                "replyto": "thruv3HOSe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7486/Reviewer_kFUJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7486/Reviewer_kFUJ"
                ],
                "content": {
                    "comment": {
                        "value": "It seems from the latest response that the positioning of ProbTS is now as an empirical study, rather than a toolkit, so I shall now give my thoughts from the perspective of an empirical study (and therein lies my original critique -- this paper has split its focus between a library/toolkit, benchmark, and empirical study, failing to hit the mark on any).\n\n1. Findings regarding datasets are not well-supported.\n\n    a. Exchange-S/L, Electricity-S/L, and possibly Traffic-S/L are exactly the same datasets, Solar has strong seasonality -- it is not clear how the data supports the conclusion that long term forecasting datasets have more trend/seasonality than short term forecasting datasets. \n\n    b. How does the JS div score lead to the conclusion that the datasets have more complex data distributions? Does this mean that all non-gaussian data distributions are more complex? What about a Student-T distribution head or Negative-binomial distribution head? These are simple modelling choices that can handle non-Gaussian data.\n\n2. Findings regarding models do not constitute significant contributions.\n\n    a. \"Probabilistic methods excel in modeling complex data distributions\" -- probabilistic methods obviously are better than point forecasting methods at modelling distributions.\n\n    b. \"Customized network architecture performs remarkably on long-term forecasting.\" -- similarly, these \"customized network architectures\" are methods proposed specifically for the LSTF setting, and naturally performs better on the LSTF setting.\n\n    c. \"Probabilistic methods may produce poor point forecasts even with a superior CRPS score\"  -- again, these findings are based on very shaky experimental design. How do we know probabilistic methods are worse than point forecasting methods on the NMAE metric due to it being a probabilistic method, rather than the architecture design? It has superior CRPS score due to the nature of the metric favouring probabilistic forecasts, but if you gave the more advanced architecture design a probabilistic head, would it have gotten a superior CRPS score? If you want to make such claims, the experiments have to be carefully designed, performing ablations on the different components. We cannot draw such conclusions from the benchmark presented.\n\n    d. \"Customized network architecture on short-term forecasting remains underexplored\" -- If you consider that the \"customized network architectures\" have mostly been proposed for the LSTF task, then this statement is straightforward and does not require experiments to arrive to"
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7486/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700585261435,
                "cdate": 1700585261435,
                "tmdate": 1700585284581,
                "mdate": 1700585284581,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "pUik0XFcZC",
            "forum": "wMXH8tTQE3",
            "replyto": "wMXH8tTQE3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7486/Reviewer_aD4Z"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7486/Reviewer_aD4Z"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose ProbTS, a toolkit for timeseries forecasting that implements a wide range of methods, and report a series of benchmarks that are thoroughly analyzed."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This work presents an interesting analysis of various time-series forecasting methods, the authors do a great job bridging the gap between two different branches. The benchmarking of these methods across various datasets is valuable and the analysis is very insightful. The work reflects on the current strategies, and provides a unified view of current approaches and existing challenges, and would be invaluable for researchers working on these problems.\n- I find the analyses incredibly insightful. The differences between the CRPS and NMAE metrics is interesting.\n- The proposed toolbox is thorough and provides a unified framework for comparing various methods at an equal footing (same data pre-processing..). The most recent methods are implemented. This tool should be useful for researchers and could help bridge the gap between the two branches."
                },
                "weaknesses": {
                    "value": "1. The datasets being studied are on the smaller scale. While these are the main benchmark datasets used in the field, comparing methods on datasets of varying sizes would be important. One might suggest that probabilistic methods excel with large amounts of data. \n2. A noticeably absent aspect of time series is its multi-variate nature. Some methods like PatchTST for example independently process channels. Do different methods present limitations from not modeling the cross-channel interactions?"
                },
                "questions": {
                    "value": "1. Does ProbTS use standard hyperparameter tuning packages like raytune? \n2. For transformer-based models, how were patch sizes determined? \n3. How does model performance compare to performances reported in each method's respective paper? Were all the methods implemented in ProbTS reproduced successfully?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7486/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698812913795,
            "cdate": 1698812913795,
            "tmdate": 1699636903202,
            "mdate": 1699636903202,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mwb5B2WzPi",
                "forum": "wMXH8tTQE3",
                "replyto": "pUik0XFcZC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7486/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7486/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer aD4Z (Part 1/3)"
                    },
                    "comment": {
                        "value": "We are deeply grateful for your acknowledgment of the unique contributions made by this work. Your recognition of our analyses as both interesting and insightful is highly appreciated. In what follows, we will address your remaining concerns and questions specifically.\n\n**Response to Weaknesses 1**\n\nWe appreciate your emphasis on the significance of comparing methods across datasets of varying sizes. Your viewpoint on this matter aligns with similar insights raised in Weakness 2 by Reviewer eUu7 and Question 2 by Reviewer AnhU. For further details, please refer to our responses to their queries. In the following, we present a comprehensive response to your question.\n\nFirst, we would like to clarify that the datasets currently included in ProbTS already cover a wide range of sizes. Below is a table summarizing the statistics of these datasets. As demonstrated, these datasets span a wide range of scales, from smaller sets containing 792 timesteps to larger ones with 69,680 timesteps, and in their dimensionality, ranging from low (7) to high (2000). We believe this wide array of datasets adequately substantiates our findings.\n\nTable 1. Statistics of datasets.\n\n| Dataset | # Var. | Timesteps |\n| --- | --- | --- |\n| ETTh1-L / ETTh1-L | 7 | 17, 420 |\n| ETTm1-L / ETTm2-L | 7 | 69, 680 |\n| Electricity-L | 321 | 26, 304 |\n| Traffic-L | 862 | 17, 544 |\n| Weather-L | 21 | 52, 696 |\n| Exchange-L | 8 | 7, 588 |\n| ILI-L | 7 | 966 |\n| Exchange-S | 8 | 6, 071 |\n| Solar-S | 137 | 7, 009 |\n| Electricity-S | 370 | 5, 833 |\n| Traffic-S | 963 | 4, 001 |\n| Wikipedia-S | 2000 | 792 |\n\n\nFurthermore, we have undertaken a quantitative analysis to discern the relationship between the data scale and the superiority of probabilistic methods. Our analysis revealed that dataset size does not play a pivotal role in influencing the design paradigm of the two branches under consideration. In our preliminary analysis, we examined the impact of data volume on performance and did not identify any clear correlation patterns, as demonstrated in the tables below.\n\nTable. The correlation coefficient between the data volume and the relative performance improvement of CRPS/NMAE compared to the baseline model (GRU).\n\n| Model | DLinear | PatchTST | GRU NVP | TimeGrad | CSDI |\n| --- | --- | --- | --- | --- | --- |\n| # Var. | 0.2422 / 0.2422 | -0.2676 / -0.2676 | -0.1856 / -0.2136 | -0.1665 / -0.1793 | -0.2315 / -0.2592 |\n| # Total timestep | -0.1422 / -0.1422 | 0.3821 / 0.3821 | 0.3072 / 0.3329 | 0.2860 / 0.2971 | 0.3542 / 0.3826 |\n| # Var. x Timestep | 0.0162 / 0.0162 | 0.0166 / 0.0166 | -0.0068 / -0.0011 | 0.0082 / 0.0117 | -0.0053 / -0.0133 |"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7486/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700501903001,
                "cdate": 1700501903001,
                "tmdate": 1700503362907,
                "mdate": 1700503362907,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lOnAzejEvy",
                "forum": "wMXH8tTQE3",
                "replyto": "pUik0XFcZC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7486/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7486/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer aD4Z (Part 2/3)"
                    },
                    "comment": {
                        "value": "**Response to Weaknesses 2**\n\nThank you for bringing up the issue of modeling cross-channel interactions in time series forecasting. We have given this aspect careful thought, but as of now, we haven't discerned a clear pattern relating the modeling of cross-channel interactions to overall model performance. We have prepared a summary table illustrating how models from each branch handle the multivariate aspect, as detailed below.\n\nTable. Summary of how existing models handle multivariate time series. \n\n| Model | Research branch | Process channels independently |\n| --- | --- | --- |\n| N-BEATS | Customized neural architectures | \u221a |\n| N-HiTS | Customized neural architectures | \u221a |\n| Autoformer | Customized neural architectures | x |\n| Informer | Customized neural architectures | x |\n| LSTF-Linear | Customized neural architectures | x / \u221a |\n| PatchTST | Customized neural architectures | x / \u221a |\n| TimesNet | Customized neural architectures | x |\n| DeepAR | Probabilistic estimation | \u221a |\n| GP-copula | Probabilistic estimation | x |\n| LSTM NVP | Probabilistic estimation | x |\n| LSTM MAF | Probabilistic estimation | x |\n| Trans MAF | Probabilistic estimation | x |\n| TimeGrad | Probabilistic estimation | x |\n| CSDI | Probabilistic estimation | x |\n| SPD | Probabilistic estimation | x |\n\nA notable trend is the prevalent use of a channel-mixing approach in most studies. However, findings are diverse, with models like DLinear [1] and PatchTST [2] suggesting that processing channels independently can yield superior results, while others like CSDI [3] indicate that explicit modeling of cross-channel interactions offers significant advantages. This diversity points to the ongoing exploration of the impact of cross-channel interactions on forecasting performance.\n\nAdditionally, we have consistently noted that as dimensionality escalates, the modeling of cross-channel interactions necessitates greater memory capacity, especially for methods based on probabilistic estimation.\n\nWe sincerely value your insightful feedback, and we plan to incorporate a discussion on this topic in our revised manuscript. We hope this will stimulate more in-depth investigations into this significant aspect of time series forecasting.\n\n[1] Zeng, Ailing, et al. (2023). Are transformers effective for time series forecasting?.\" Proceedings of the AAAI. Vol. 37. No. 9.\n\n[2] Nie, Yuqi, et al. (2023). A time series is worth 64 words: Long-term forecasting with transformers. Proceedings of the ICLR. \n\n[3] Tashiro, Yusuke, et al. (2021). Csdi: Conditional score-based diffusion models for probabilistic time series imputation.\" Proceedings of the NeurIPS."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7486/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700501970272,
                "cdate": 1700501970272,
                "tmdate": 1700501970272,
                "mdate": 1700501970272,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GdpNwOBcMd",
                "forum": "wMXH8tTQE3",
                "replyto": "pUik0XFcZC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7486/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7486/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer aD4Z (Part 3/3)"
                    },
                    "comment": {
                        "value": "**Response to Question 1**\n\nAs outlined in Appendix C.1, we adopted an exhaustive grid search approach for model tuning. We acknowledge your valuable suggestion to employ standard hyperparameter tuning packages. In future iterations, we will investigate incorporating such packages to improve the efficiency of parameter tuning, particularly when introducing additional baselines.\n\n**Response to Question 2**\n\nTo ensure a fair comparison, we set the hyperparameters via thorough standard tuning procedures. For those models and datasets where hyperparameters were previously provided, we reused the reported patch size that had been demonstrated to deliver optimal results, a finding that our reproduction also validates. When it came to short-term forecasting and patch size settings were not available, we conducted a search for the ideal patch sizes within a range of 4 to 8. This was done considering the shorter lookback window and forecasting horizon.\n\n**Response to Question 3**\n\nIn most instances, where detailed reproducible specifications were provided (for both datasets and models), we successfully reproduced results by directly applying or slightly adjusting the reported hyperparameters. For those scenarios without clear guidance, such as evaluating probabilistic methods in long-term forecasting scenarios, we conducted a unified grid search over some critical hyperparameters on the validation set to determine the final experimental setups.\n\nHowever, it's important to approach comparisons of our results with those in the original papers cautiously due to subtle differences in metric calculations. Some methods denormalized their experimental results before evaluation [1,2,3], while others calculated the metric scores directly without denormalization [4,5], creating a challenge for direct comparison. To address this, our toolkit preserves both denormalized and non-denormalized results, facilitating a unified evaluation and comparison. In our main paper, we report the evaluation metrics calculated at the original value scale (after denormalization).\n\nWe made every effort to ensure a fair evaluation of all baselines, and we plan to open-source all the code and configuration files. This commitment to fairness underpins the insights we derived, which form the primary contribution of our paper. We appreciate your understanding of these complexities and welcome further suggestions to enhance the transparency and robustness of our comparisons.\n\n[1] Rasul, Kashif, et al. (2021). Multivariate probabilistic time series forecasting via conditioned normalizing flows.\u00a0Proceedings of the ICLR.\n\n[2] Rasul, Kashif, et al. (2021). Autoregressive denoising diffusion models for multivariate probabilistic time series forecasting. Proceedings of the ICML.\n\n[3] Tashiro, Yusuke, et al. (2021). CSDI: Conditional score-based diffusion models for probabilistic time series imputation. Proceedings of the NeurIPS.\n\n[4] Wu, Haixu, et al. (2022). Timesnet: Temporal 2d-variation modeling for general time series analysis. Proceedings of the ICLR.\n\n[5] Nie, Yuqi, et al. (2022). A time series is worth 64 words: Long-term forecasting with transformers. Proceedings of the ICLR."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7486/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700502012731,
                "cdate": 1700502012731,
                "tmdate": 1700502012731,
                "mdate": 1700502012731,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "XFpXG3OlAs",
            "forum": "wMXH8tTQE3",
            "replyto": "wMXH8tTQE3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7486/Reviewer_JfDV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7486/Reviewer_JfDV"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces ProbTS, a novel toolkit aimed at bridging the gap between two prominent research branches in time-series forecasting: one focused on customized neural network architectures and the other on advanced probabilistic estimations. The paper highlights key insights from the toolkit's analysis, revealing that long-term forecasting scenarios often exhibit strong trending and seasonality patterns, while short-term scenarios have more complex data distributions. It also identifies the strengths and weaknesses of different methodological focuses, showing that probabilistic forecasting excels in modeling data distributions, but may produce poor point forecasts. Additionally, the autoregressive decoding scheme is effective in cases with strong seasonality but struggles with pronounced trending, while the non-autoregressive scheme is preferred for long-term forecasting. The paper concludes by emphasizing the potential of combining these research branches to revolutionize time-series forecasting and anticipates that ProbTS will catalyze groundbreaking research in the field."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper possesses several strengths. Firstly, it is well-written, displaying a high level of clarity and organization. The insights provided are undeniably valuable, shedding light on the challenging questions arising from the divergence in data scenarios, methodological approaches, and decoding schemes within the realm of time-series forecasting. The paper effectively highlights the significant gap in the existing literature, where no prior solution has successfully bridged the divide between these two distinct research branches. This emphasis on addressing an unexplored area of research stimulates further groundbreaking work in the field. Moreover, the sharing of the ProbTS toolkit included in the paper will benefit the research community, offering a practical resource to help researchers understand and effectively handle these complex issues, ultimately fostering collaboration and collective progress in the field."
                },
                "weaknesses": {
                    "value": "While this paper offers valuable insights and contributions, there are a few areas where it could be improved. Firstly, while the ProbTS toolkit is undoubtedly a valuable resource, for me, the insights presented in the paper are very informative, and I believe that placing a stronger emphasis on these insights would have been greatly beneficial.\n\nAdditionally, the paper could benefit from more extensive discussions on other critical characteristics of time-series forecasting, such as dimensionality, data length, or the volume of training data. These factors can significantly impact forecasting performance, and a deeper exploration of their effects would have been highly informative.\n\nMoreover, the insights presented in the paper could have been more rigorously developed and supported. The use of synthetic datasets and controlled experiments could have strengthened the empirical evidence, particularly since the datasets used in the analysis exhibit diverse characteristics that might confound the results.\n\nLastly, a minor point of improvement lies in the Contributions section of the Introduction, where the term CRPS is mentioned before its definition. Providing a definition before using the abbreviation would enhance the clarity of the paper."
                },
                "questions": {
                    "value": "See Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7486/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7486/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7486/Reviewer_JfDV"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7486/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699325591187,
            "cdate": 1699325591187,
            "tmdate": 1699636903094,
            "mdate": 1699636903094,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EQpFr6DCB5",
                "forum": "wMXH8tTQE3",
                "replyto": "XFpXG3OlAs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7486/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7486/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer JfDV"
                    },
                    "comment": {
                        "value": "Thank you for your comprehensive review and constructive feedback. We also particularly appreciate your recognition of our work. We hope that the following responses will address your concerns and contribute to enhancing the completeness and rigor of this paper.\n\n**Response to Weaknesses 1**\n\nWe greatly appreciate your suggestion and welcome any specific recommendations you may have to further highlight these insights. In the submitted manuscript, we briefly outlined these insights in the Introduction section and provided detailed discussions in Section 4. The key insights of each paragraph have been accentuated by bolding them and placing them at the beginning of each section. In the upcoming revision, we plan to improve our writing to ensure that the core insights of each paragraph are conveyed more effectively to the reader.\n\n**Response to Weaknesses 2**\n\nWe appreciate your suggestion about the need for a more extensive discussion on the critical characteristics of time-series forecasting, such as dimensionality, data length, and the volume of training data. We did examine these factors in our preliminary studies, but they did not profoundly influence the design paradigm of the two branches under consideration.\n\nWe did not observe a significant correlation between these factors and model performance, as indicated in the tables below. As these findings did not provide substantial insights, we chose not to include them in the paper. Nevertheless, we believe the ProbTS Toolkit is well-equipped to facilitate more in-depth analyses of these characteristics in future research if needed.\n\nTable 1. The correlation coefficient between the data volume and the relative performance improvement of CRPS/NMAE compared to the baseline model (GRU).\n\n| Model | DLinear | PatchTST | GRU NVP | TimeGrad | CSDI |\n| --- | --- | --- | --- | --- | --- |\n| # Var. | 0.2422 / 0.2422 | -0.2676 / -0.2676 | -0.1856 / -0.2136 | -0.1665 / -0.1793 | -0.2315 / -0.2592 |\n| # Total timestep | -0.1422 / -0.1422 | 0.3821 / 0.3821 | 0.3072 / 0.3329 | 0.2860 / 0.2971 | 0.3542 / 0.3826 |\n| # Var. x Timestep | 0.0162 / 0.0162 | 0.0166 / 0.0166 | -0.0068 / -0.0011 | 0.0082 / 0.0117 | -0.0053 / -0.0133 |\n\n\n**Response to Weaknesses 3**\n\nWe appreciate your constructive feedback. We concur that controlled experiments and the use of synthetic datasets could provide more robust empirical evidence. Consequently, in response to your suggestion, we have generated synthetic datasets including a baseline dataset, one with pronounced trends, another with strong seasonality, and a dataset displaying complex data distribution. Each series within these datasets was created by combining trend, seasonality, noise, and anomaly components with controlled characteristics. The details are provided in the table below.\n\nTable 2. Quantitative assessment of intrinsic characteristics for synthetic datasets.\n\n| Dataset/Horizon | Trend | Seasonality | JS Div. |\n| --- | --- | --- | --- |\n| Normal | 0.105 | 0.302 | 0.261 |\n| Strong Trend | 0.554 | 0.302 | 0.248 |\n| Strong Seasonality | 0.105 | 0.791 | 0.272 |\n| Complex data distribution | 0.064 | 0.190 | 0.469 |\n\nSubsequent experiments conducted on these synthetic datasets, utilizing representative models, validated our empirical findings from other datasets examined with ProbTS. Noteworthy observations include the declining performance of autoregressive decoding models such as TimeGrad in the presence of increasing trends, an enhancement in performance for models employing autoregressive decoding with intensifying seasonality, and the competitive performance exhibited by probabilistic methods like CSDI when dealing with more complex data distributions.\n\nTable 3. Results on synthetic datasets. The look-back window and forecasting horizon are 30.\n\n| Model | DLinear | PatchTST | TimeGrad | CSDI |\n| --- | --- | --- | --- | --- |\n| Normal (CRPS) | 0.013  | 0.012 | 0.024 | 0.013 |\n| Normal (NMAE) | 0.013  | 0.012 | 0.032 | 0.014 |\n| Strong Trend (CRPS) | 0.001 | 0.001 | 0.042 | 0.010 |\n| Strong Trend (NMAE) | 0.001 | 0.001 | 0.048 | 0.007 |\n| Strong Seasonality (CRPS) | 0.014 | 0.012 | 0.022 | 0.020 |\n| Strong Seasonality (NMAE) | 0.014 | 0.012 | 0.028 | 0.027 |\n| Complex data distribution (CRPS) | 0.301 | 0.275 | 0.283 | 0.269 |\n| Complex data distribution (NMAE) | 0.301 | 0.275 | 0.338 | 0.301 |\n\nThese observations further reinforce our findings, and we plan to incorporate these synthetic experiments and analyses in future revisions of our work. In addition, we aim to include the code for generating synthetic datasets within the ProbTS toolkit to encourage and facilitate further exploration of this area by the wider research community. We greatly appreciate your valuable feedback!\n\n**Response to Weaknesses 4**\n\nThank you for your suggestion. We will take care to ensure that all abbreviations, including CRPS, are clearly defined prior to their first use in the revised version of the paper."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7486/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700501626430,
                "cdate": 1700501626430,
                "tmdate": 1700503339865,
                "mdate": 1700503339865,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9Ey8TkNN9k",
                "forum": "wMXH8tTQE3",
                "replyto": "EQpFr6DCB5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7486/Reviewer_JfDV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7486/Reviewer_JfDV"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response to my review and for providing the additional results. I appreciate the effort you've invested in addressing the comments raised during the review process.\n\nI would like to commend you on the quality of your work. Having carefully considered your responses and the supplementary materials, I am confident that the paper has maintained its high standard. I believe the rating I assigned in my initial review accurately reflects the level of your research, and I see no need for adjustments."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7486/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700538766574,
                "cdate": 1700538766574,
                "tmdate": 1700538766574,
                "mdate": 1700538766574,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "pSVDZpXVzc",
            "forum": "wMXH8tTQE3",
            "replyto": "wMXH8tTQE3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7486/Reviewer_AnhU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7486/Reviewer_AnhU"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a toolkit to evaluate time-series forecasting methods on various datasets. They observe that there are two main branches: long-term forecasting where data revlease strong trends and seasonality patterns, and a second branch oriented towards short-term forecasting\n\nHighlighting that different data characteristics and forecasting horizons prefer different design\n\nLong - term forecasting : specializing in neural network architecture design with various inductive biases, restricting themselves to point-forecasts\nShort - lean towards conventional neural network designs"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Authors implement quite a few models which are evaluated on on the datasets\n- The framework provides a standardized way of evaluating methods"
                },
                "weaknesses": {
                    "value": "There are multiple time-series survey/benchmark papers in the literature for forecasting which emphasize standardization across datasets [1], others that emphasize architectural studies [2] and [3] which classifies time-series forecasting methods along the same direction as this work.\n\nIt\u2019s not clear where the authors proposed framework fits amongst these previous studies on time-series forecasting, it looks like another way of characterizing time-series forecasting models which is partially covered in [3]\n\n[1] Godahewa, Rakshitha, et al. \"Monash time series forecasting archive.\" arXiv preprint arXiv:2105.06643 (2021).\n[2] Elsayed, S., et al. \"Do we really need deep learning models for time series forecasting? arXiv 2021.\" arXiv preprint arXiv:2101.02118.\n[3] Januschowski, Tim, et al. \"Criteria for classifying forecasting methods.\" International Journal of Forecasting 36.1 (2020): 167-177."
                },
                "questions": {
                    "value": "- Why don\u2019t authors compare with simpler methods such as XGBoost (with hand crafted features?) \n   - it's quite hard to beat this baseline on the datasets that were used in this paper.\n- The datasets used are quite small, I'm curious if these findings hold if we increase dataset size\n- Hyperparameters and preprocessing steps used for these datasets could dramatically effect model performance. Were these tuned individually for each of the methods? And why is this not included as part of the text\n- What is the guiding mechanism for determining whether a dataset suits the short-forecast or long-forecast category? Is it simply the forecasting window? Or rather intrinsic property to the dataset \n- I believe although initially the paper tries to consider both model/and data aspects of the time-series forecasting domain it fails to provide concrete guidance on how one effects the other, i.e. a quantifiable way of delineating which approach should be taken"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7486/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699345606774,
            "cdate": 1699345606774,
            "tmdate": 1699636902952,
            "mdate": 1699636902952,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Yp45CLxMfJ",
                "forum": "wMXH8tTQE3",
                "replyto": "pSVDZpXVzc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7486/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7486/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer AnhU (Part 1/3)"
                    },
                    "comment": {
                        "value": "Thank you for your insightful comments and for recognizing some unique contributions of our work. We would like to further clarify our motivation and the core position of this work to enhance your understanding. We regret any misunderstanding and would like to assure you that this work is not just about combining long-term and short-term forecasting.\n\nFirstly, the primary motivation of this work is to **unify two equally important yet independently developed research branches in deep time-series forecasting**:\n\n- One branch focuses on customized neural architecture designs but only provides point-level forecasts.\n- The other branch specializes in probabilistic estimations for distributional forecasts but rarely includes architecture designs.\n\nSecondly, during this unification research, we identified several critical aspects that need to be aligned, including:\n\n- The role of different data characteristics (trend, seasonality, complexity of data distribution) and forecasting setups (long-term vs short-term) in influencing the methodological designs and choices.\n- The existence of several unexplored scenarios and overlooked challenges in the literature due to different methodological designs in rarely visited data scenarios. The significant methodological aspects we've summarized include:\n    - Probabilistic vs non-probabilistic forecasting paradigms and evaluation metrics.\n    - Customized vs general neural architecture designs.\n    - Autoregressive vs non-autoregressive decoding schemes.\n\nThirdly, after conducting experimental comparisons and analyses of these methodological designs across diverse data scenarios, we present our unique findings and insights on the strengths and weaknesses of different methods. We believe the challenges we uncovered and the new opportunities we identified will pave the way for future research aimed at better combining the benefits of neural architecture designs and probabilistic estimation abilities.\n\nWe hope the above clarification of our paper's logic can help to reinforce your understanding of the unique contributions of our work. With this context, we will proceed to address your specific concerns in more detail.\n\n**Response to Weaknesses** \n\nWe appreciate your reference to these important studies on time-series benchmarking. We plan to incorporate a discussion on these works in our revised paper. However, it's crucial to highlight the fundamental differences between our work, ProbTS, and these studies.\n\nIn brief, [1] provides a broad range of datasets and benchmarks a multitude of forecasting methods, [2] primarily compares gradient boosted decision trees with deep learning methods, and [3] aims to categorize existing methods in different aspects, appearing more like a literature review. Specifically, neither [1] nor [2] consider probabilistic forecasting, while [3] focuses on fine-grained classifications of forecasting methods beyond the simple \"machine learning\" or \"statistical\" dichotomy.\n\nIt's noteworthy that these studies were published at least two years ago. Given the rapid development in this field, our work is unique in its recognition of the need to **unify the research in customized neural architectures and advanced probabilistic estimation abilities**, both of which are vital components in deep time-series forecasting but have evolved independently in recent literature. In this context, our work reveals several unique insights and findings, such as:\n\n- The distinction between long-term and short-term forecasting, along with different data characteristics.\n- Probabilistic vs non-probabilistic forecasting paradigms and evaluation metrics.\n- Customized vs general neural architecture designs.\n- Autoregressive vs non-autoregressive decoding schemes.\n\nIn summary, our work is not merely another benchmarking study nor a comprehensive review covering all aspects of time-series forecasting. We view our work as a critical step in bridging the prominent yet divergent branches in contemporary research on time-series forecasting.\n\n[1] Godahewa, Rakshitha, et al. (2021). Monash time series forecasting archive. *arXiv preprint arXiv:2105.06643*.\n\n[2] Elsayed, S., et al. (2021). Do we really need deep learning models for time series forecasting?. *arXiv preprint arXiv:2101.02118*.\n\n[3] Januschowski, Tim, et al. (2020). Criteria for classifying forecasting methods. *International Journal of Forecasting*, *36*(1), 167-177."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7486/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700501347726,
                "cdate": 1700501347726,
                "tmdate": 1700501347726,
                "mdate": 1700501347726,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gjmiet6iCi",
                "forum": "wMXH8tTQE3",
                "replyto": "pSVDZpXVzc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7486/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7486/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer AnhU (Part 2/3)"
                    },
                    "comment": {
                        "value": "**Response to Question 1**\n\nWe are grateful for your thoughtful suggestion to include traditional tree-based methods, such as XGBoost, in our comparison. Following the implementation provided by [1], we found that XGBoost indeed exhibits competitive performance, even surpassing neural network (NN) models in certain scenarios.\n\nHowever, for datasets with more complex data distributions such as Solar and Electricity, advanced probabilistic estimation methods demonstrated a significant advantage over both traditional learning methods and point estimation methods. This underscores the versatility and strength of these advanced probabilistic methods in handling complex forecasting scenarios.\n\nDespite the competitive performance of XGBoost, one of its limitations is its reliance on handcrafted features, which contradicts the recent trend toward automatic representation learning inherent in NN models. This is one of the key reasons why, in line with many recent studies, we've chosen to focus on deep time-series forecasting.\n\nFurthermore, it's important to note that both tree-based methods, such as XGBoost, and NN models necessitate extensive hyper-parameter tuning. Despite our rigorous hyper-parameter tuning efforts, we did not achieve satisfactory performance with XGBoost on the Wikipedia dataset. This outcome may suggest that additional feature engineering is required for this specific dataset. However, considering the time constraints and the substantial efforts involved in preparing responses, we have decided to exclude this dataset from the subsequent table.\n\nYour insightful suggestion has undoubtedly enriched our study. By demonstrating the strengths of both XGBoost and NN models, we've provided a more comprehensive view of the current landscape of time-series forecasting. We believe this expanded perspective will enhance the overall depth and value of our paper. We greatly appreciate your contribution to this improvement.\n\n| Model (Metric) | Category | Solar | Electricity | Traffic | Exchange |\n| :--- | :--- | :---: | :---: | :---: | :---: |\n| XGBoost (CRPS) | GBDT | 0.599 | 0.074 | 0.196 | 0.011 |\n| XGBoost (NMAE) | GBDT | 0.599 | 0.074 | 0.196 | 0.011 |\n| NLinear (CRPS) | Customized NN | 0.560 | 0.083 | 0.233 | 0.010 |\n| NLinear (NMAE) | Customized NN | 0.560 | 0.083 | 0.233 | 0.010 |\n| PatchTST (CRPS) | Customized NN | 0.496 | 0.076 | 0.202 | 0.010 |\n| PatchTST (NMAE) | Customized NN | 0.496 | 0.076 | 0.202 | 0.010 |\n| TimeGrad (NMAE) | Probabilistic estimation | 0.359 | 0.052 | 0.164 | 0.011 |\n| TimeGrad (NMAE) | Probabilistic estimation | 0.445 | 0.067 | 0.201 | 0.014 |\n| CSDI (CRPS) | Probabilistic estimation | 0.366 | 0.050 | 0.146 | 0.008 |\n| CSDI (NMAE) | Probabilistic estimation | 0.484 | 0.065 | 0.176 | 0.011 |\n\n[1] Elsayed, S., et al. (2021). Do we really need deep learning models for time series forecasting?. *arXiv preprint arXiv:2101.02118*.\n\n\n**Response to Question 2**\n\nWe appreciate your insightful question. ProbTS has been designed and tested on a diverse range of datasets, as illustrated in the table below. These datasets vary significantly in scale, ranging from small (792) to large (69,680), and in dimensionality, from low (7) to high (2000). We believe this broad spectrum of datasets adequately validates our findings.\n\nTable 1. Statistics of datasets.\n\n| Dataset | # Var. | Timesteps |\n| --- | --- | --- |\n| ETTh1-L / ETTh1-L | 7 | 17, 420 |\n| ETTm1-L / ETTm2-L | 7 | 69, 680 |\n| Electricity-L | 321 | 26, 304 |\n| Traffic-L | 862 | 17, 544 |\n| Weather-L | 21 | 52, 696 |\n| Exchange-L | 8 | 7, 588 |\n| ILI-L | 7 | 966 |\n| Exchange-S | 8 | 6, 071 |\n| Solar-S | 137 | 7, 009 |\n| Electricity-S | 370 | 5, 833 |\n| Traffic-S | 963 | 4, 001 |\n| Wikipedia-S | 2000 | 792 |\n\nIn addition, we specifically investigated the impact of dataset size on model performance. Our analysis, depicted in Table 3, did not reveal a clear correlation between dataset size and model performance. Consequently, this aspect was not emphasized in our paper.\n\nTable 2. The correlation coefficient between the data volume and the relative performance improvement of CRPS/NMAE compared to the baseline model (GRU).\n\n| Model | DLinear | PatchTST | GRU NVP | TimeGrad | CSDI |\n| --- | --- | --- | --- | --- | --- |\n| # Var. | 0.2422 / 0.2422 | -0.2676 / -0.2676 | -0.1856 / -0.2136 | -0.1665 / -0.1793 | -0.2315 / -0.2592 |\n| # Total timestep | -0.1422 / -0.1422 | 0.3821 / 0.3821 | 0.3072 / 0.3329 | 0.2860 / 0.2971 | 0.3542 / 0.3826 |\n| # Var. x Timestep | 0.0162 / 0.0162 | 0.0166 / 0.0166 | -0.0068 / -0.0011 | 0.0082 / 0.0117 | -0.0053 / -0.0133 |"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7486/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700501426482,
                "cdate": 1700501426482,
                "tmdate": 1700503386835,
                "mdate": 1700503386835,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lSzjdlxeUp",
                "forum": "wMXH8tTQE3",
                "replyto": "pSVDZpXVzc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7486/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7486/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer AnhU (Part 3/3)"
                    },
                    "comment": {
                        "value": "**Response to Question 3**\n\nYour question is indeed pertinent. As specified in Appendix C.1, we carried out an extensive grid search for models, tuning hyperparameters individually for each method. Given the large number of models and the variance in hyperparameters across different datasets and forecasting scenarios, we chose not to include a detailed account of parameter configurations in the main text of the paper. However, we have made the specific hyperparameter configurations available in an open-source format along with the code.\n\nIn response to your valuable suggestion, we will include the key hyperparameter settings in the upcoming revision to offer readers a more accessible reference point. Thank you for raising this important issue.\n\n\n**Response to Question 4**\n\nWe appreciate your thoughtful question. The categorization of datasets into short-term or long-term forecasting is primarily influenced by practical necessities in various domains, rather than our personal determination of which dataset is best suited for a particular forecast duration. For instance, certain datasets like Wikipedia are commonly utilized for short-term forecasting. Others, such as Weather-L and ETTm1-L / ETTm2-L, are typically employed for long-term forecasting. Some datasets, like Electricity and Traffic, are applicable to both short-term and long-term forecasting.\n\nIn this paper, our aim is not just to distinguish between short-term and long-term forecasting or varying domains. Instead, we strive to identify inherent dataset characteristics that may guide different methodological designs. The key characteristics we have identified include the strength of trending, the strength of seasonality, and the complexity of data distribution. These traits are a collective result of the length of forecasting horizons and the specific data domains.\n\nOur exploration into the interplay between data characteristics and model designs prompts us to envision future research that would conduct more comprehensive evaluations. These evaluations would span across different domains and forecasting horizons (short versus long), and would utilize a variety of evaluation metrics, both at the distributional and point level. We believe that ProbTS is well-equipped to facilitate this kind of comprehensive analysis. We value your feedback and will strive to clarify our presentation further in the next revision.\n\n**Response to Question 5**\n\nThank you for your thoughtful question. We appreciate the opportunity to elucidate the synergistic relationship between model and data aspects in our research.\n\nFirstly, our work does indeed shed light on the interplay between these two aspects. We have found that specific data characteristics favor certain methodological designs, leading to a convergence in different studies, a phenomenon we have outlined in our paper. \n\nSecondly, we have evaluated models in data scenarios that have been scarcely studied in the past. This approach has allowed us to uncover overlooked challenges and unresolved problems in the field, thus contributing to a more holistic understanding of the topic.\n\nFor example, our work includes the following findings.\n\n- Probabilistic methods demonstrate proficiency in modeling complex data distributions, even though they may produce poor point forecasts even with a superior CRPS score (Section 4.2).\n- Customized network architecture shows remarkable performance in long-term forecasting, yet it remains an underexplored area in short-term forecasting scenarios (Sections 4.3).\n- Autoregressive models excel in handling strong seasonality but face challenges with pronounced trends, while both autoregressive and non-autoregressive decoding schemes perform comparably well in short-term forecasting (Section 4.4).\n\nLastly, the intricate interplay between data and model is a persistent theme in machine learning research. We have developed ProbTS in the hope that it will facilitate a deeper exploration of this dynamic in the time-series domain. ProbTS achieves this by providing comprehensive evaluation scenarios and metrics, and by unifying essential modeling components.\n\nWe hope this clarifies the intent and findings of our work. Thank you for prompting this valuable discussion."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7486/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700501502570,
                "cdate": 1700501502570,
                "tmdate": 1700501502570,
                "mdate": 1700501502570,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "57mz80yauF",
            "forum": "wMXH8tTQE3",
            "replyto": "wMXH8tTQE3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7486/Reviewer_eUu7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7486/Reviewer_eUu7"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a novel framework for joint training and evaluation of deep time series models on a multitude of datasets available in the literature. The key feature of the proposed approach is the ability to combine and evaluate probabilistic and non-probabilistic methods in one place."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Work on developing a unified deep learning framework for time-series forecasting is very much appreciated\n- The case study reveals interesting insights comparing short term and long term and probabilistic and non-probabilistic models"
                },
                "weaknesses": {
                    "value": "- ProbTS does not include any naive and statisitcal models (e.g. ETS). The lack of good functioning naive/statistical models for probabilistic forecasting is actually a significant gap in the modern deep learning literature. Could you please include a few methods from this area as baselines in the proposed framework?\n- The benchmark contains many datasets, however key datasets that have been instrumental in designing some of the current architectures are missing. Can you include M4, M5, TOURISM?\n- Most datasets included in the benchmark are small-scale. For the purpose of studying model scaling and ability to model complex distributions, it feels urgent that large scale time series datasets are included in modern benchmarks. In this context, I can think of FRED from https://arxiv.org/abs/2002.02887"
                },
                "questions": {
                    "value": "- Does your benchmark support zero-shot/few-shot/transfer learning training/testing, pretrained models, model zoo? If not, is it easy to extend it to this scenario? Can you touch on this topic in the paper?\n- Does the framework support datasets that don't fit in RAM, what is the mechanism for dataset storage and loading? How do you deal with the licenses of original datasets?\n- I included a number of questions and concerns and will be very happy to revise my score accordingly if all of them are addressed meticulously."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7486/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699380603150,
            "cdate": 1699380603150,
            "tmdate": 1699636902825,
            "mdate": 1699636902825,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BYKNAGNRMw",
                "forum": "wMXH8tTQE3",
                "replyto": "57mz80yauF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7486/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7486/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer eUu7 (Part 1/3)"
                    },
                    "comment": {
                        "value": "Thank you for your thorough review. We are particularly grateful for your recognition of the key strengths of our paper, specifically our unification of probabilistic and non-probabilistic methods to reveal novel insights, identify current challenges, and suggest potential directions for future research. In response to your additional queries, we trust that the following explanations will satisfactorily address your concerns.\n\n\n**Response to Weaknesses 1**\n\nThank you for this insightful suggestion. As per your recommendation, we have incorporated a few classical statistical methods such as ARIMA and ETS into the ProbTS framework. This addition serves to further solidify and enrich our empirical research. We invite you to review the table below which compares these baseline methods with state-of-the-art deep learning techniques.\n\n| Model (Metric) | Solar | Electricity | Traffic | Wikipedia | Exchange |\n| :--- | :---: | :---: | :---: | :---: | :---: |\n| ARIMA (CRPS) | 1.000 | 0.164 | 0.461 | 0.348 | 0.009 |\n| ARIMA (NMAE) | 1.000 | 0.164 | 0.461 | 0.348 | 0.009 |\n| ETS (CRPS) | 0.580 | 0.121 | 0.413 | 0.685 | 0.011 |\n| ETS (NMAE) | 0.580 | 0.121 | 0.413 | 0.685 | 0.011 |\n| ETS-prob (CRPS) | 0.795 | 0.123 | 0.38 | 0.625 | 0.008 |\n| ETS-prob (NMAE) | 0.695 | 0.129 | 0.433 | 0.697 | 0.011 |\n| PatchTST (CRPS) | 0.496 | 0.076 | 0.202 | 0.257 | 0.010 |\n| PatchTST (NMAE) | 0.496 | 0.076 | 0.202 | 0.257 | 0.010 |\n| CSDI (CRPS) | 0.366 | 0.050 | 0.146 | 0.219 | 0.008 |\n| CSDI (NMAE) | 0.484 | 0.065 | 0.176 | 0.259 | 0.011 |\n\nAs can be observed, deep learning methods significantly outperform these simple statistical baselines. This highlights the crucial role of capturing non-linear dependencies in generating accurate forecasts. Such comparisons underscore the necessity of further research to enhance deep time-series forecasting, which is one of the primary goals of our paper. Additionally, we have noted that ARIMA and ETS have distinct performance tendencies depending on specific data characteristics. For instance, the ARIMA method struggles with datasets like 'Solar' which exhibit weak trending and strong seasonality, whereas the ETS model shows better adaptability. Conversely, in cases of strong trending and weak seasonality, as seen in the 'Wikipedia' dataset, the ARIMA model significantly outperforms the ETS model.\n\n[1] Asteriou, Dimitros; Hall, Stephen G. (2011). \"ARIMA Models and the Box-Jenkins Methodology\". *Applied Econometrics* (Seconded.). Palgrave MacMillan. pp.265-286.[ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier))[978-0-230-27182-1](https://en.wikipedia.org/wiki/Special:BookSources/978-0-230-27182-1).\n\n[2] Hyndman, Rob J., and Athanasopoulos, George. *Forecasting: principles and practice*, 3rd edition, OTexts, 2021.[https://otexts.com/fpp3/expsmooth.html](https://otexts.com/fpp3/expsmooth.html)\n\n[3] [https://www.statsmodels.org/dev/examples/notebooks/generated/ets.html](https://www.statsmodels.org/dev/examples/notebooks/generated/ets.html)"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7486/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700500682862,
                "cdate": 1700500682862,
                "tmdate": 1700502654543,
                "mdate": 1700502654543,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uXV3I6x2IH",
                "forum": "wMXH8tTQE3",
                "replyto": "57mz80yauF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7486/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7486/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer eUu7 (Part 2/3)"
                    },
                    "comment": {
                        "value": "**Response to Weaknesses 2**\n\nThank you for your suggestion to incorporate the M4, M5, and TOURISM datasets, which are indeed key datasets for univariate time-series forecasting. In light of your advice, we have analyzed these datasets and conducted experiments comparing different forecasting methods.\n\nOur findings indicate that the characteristics of these new datasets do not extend beyond the scope of the existing datasets used in this paper. Furthermore, the additional tests run on these new datasets further reinforce the major findings and conclusions of our paper. We provide more detailed information about these two points below for your convenience.\n\nTable 1 presents the comparison of the new datasets (M4, M5, TOURISM) with the existing datasets. As per the data analysis strategy detailed in our paper, we focus on three key aspects: the strength of trending, the strength of seasonality, and the complexity of data distribution. From the table, it is evident that these new datasets do not introduce particularly unique characteristics, with the exception of M4-Daily, which may exhibit fewer seasonal patterns. The main difference between the new datasets and the existing ones is that the new datasets are all designed for univariate time-series forecasting, while the datasets in our study primarily target multivariate time-series forecasting. Thanks to your constructive suggestion, we now have a broader spectrum covering both univariate and multivariate scenarios, thereby reinforcing our findings.\n\nTable 1. Quantitative assessment of the intrinsic characteristics of the datasets reported in our paper. The JS Div. denotes Jensen-Shannon divergence, where a lower score indicates closer approximations to a Gaussian distribution.\n\n| Dataset | Trend | Seasonality | JS Div. |\n| :--- | :---: | :---: | :---: |\n| Exchange | 0.9982 | 0.1256 | 0.2967 |\n| Solar | 0.1688 | 0.8592 | 0.5004 |\n| Electricity | 0.6443 | 0.8323 | 0.3579 |\n| Traffic | 0.2880 | 0.6656 | 0.2991 |\n| Wikipedia | 0.5253 | 0.2234 | 0.2751 |\n\nTable 2. Quantitative assessment of the intrinsic characteristics of the univariate datasets.\n\n| Dataset | Trend | Seasonality | JS Div. |\n| :--- | :---: | :---: | :---: |\n| M4-Weekly | 0.7677 | 0.3401 | 0.5106 |\n| M4-Daily | 0.9808 | 0.0467 | 0.4916 |\n| M5 | 0.3443 | 0.2480 | 0.6011 |\n| TOURISM-Monthly | 0.7979 | 0.6826 | 0.3291 |\n\nTable 3 includes the experimental results of some representative methods. These findings align with our initial observations discussed in the paper. For example, methods that specialize in probabilistic estimation, such as GRU NVP and TimeGrad, demonstrate superior performance on datasets with complex distributions like M4-Weekly and M5. On the other hand, on the TOURISM-Monthly dataset, which has a relatively simple data distribution, baseline methods equipped with a simple point forecaster, like DLinear and PatchTST, also perform well. Additionally, both autoregressive and non-autoregressive decoding schemes show comparable performance in short-term forecasting.\n\nTable 3. Results on M4, M5, and TOURISM datasets. We utilize a lookback window of 3H, with 'H' denoting the forecasting horizon.\n\n| Dataset (Metric) | DLinear | PatchTST | GRU NVP | \n| :--- | :---: | :---: | :---: |\n| M4-Weekly (CRPS) | 0.081  | 0.089 | 0.066 | \n| M4-Weekly (NMAE) | 0.081  | 0.089  | 0.077 | \n| M4-Daily (CRPS) | 0.034  | 0.035  | 0.030 | \n| M4-Daily (NMAE) | 0.034 | 0.035 | 0.038  | \n| M5 (CRPS) | 0.891 | 0.898 | 0.679 |\n| M5 (NMAE) | 0.891 | 0.898 | 0.864 |\n| TOURISM-Monthly (CRPS) | 0.168  | 0.136 | 0.171  | \n| TOURISM-Monthly (NMAE) | 0.168 | 0.136  | 0.223 |"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7486/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700500740688,
                "cdate": 1700500740688,
                "tmdate": 1700502706402,
                "mdate": 1700502706402,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mwVl5NxGHw",
                "forum": "wMXH8tTQE3",
                "replyto": "57mz80yauF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7486/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7486/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer eUu7 (Part 3/3)"
                    },
                    "comment": {
                        "value": "**Response to Weaknesses 3**\n\nThank you for your insightful observation regarding the scale of the datasets used. We concur with your assertion that the inclusion of large-scale datasets is crucial for future research development. As per your recommendation, we plan to progressively incorporate significant, distinctive, and large-scale datasets into ProbTS, starting with the FRED dataset collection.\n\nIn the meantime, we would like to underscore that the diversity of data is another crucial aspect that warrants thorough investigation, which is the primary focus of this paper. Evaluating learning ability across different data scales, variate numbers, and domains with a wide scope of data characteristics is of great importance. Therefore, this paper is dedicated to evaluating diverse data characteristics and conducting comprehensive analyses to shed light on overlooked challenges and unresolved issues.\n\nThe datasets included in this paper are listed below. As can be seen, ProbTS already accommodates a wide range of data scales, both in terms of the number of timesteps (ranging from 792 to 69,680) and the number of variates (ranging from 7 to 2,000). Please note that most existing studies only employ a subset of these datasets to evaluate the effectiveness of their neural architecture designs or probabilistic estimation strategies.\n\n| Dataset | # Var. | Timesteps |\n| :--- | :---: | ---: |\n| ETTh1-L / ETTh1-L | 7 | 17, 420 |\n| ETTm1-L / ETTm2-L | 7 | 69, 680 |\n| Electricity-L | 321 | 26, 304 |\n| Traffic-L | 862 | 17, 544 |\n| Weather-L | 21 | 52, 696 |\n| Exchange-L | 8 | 7, 588 |\n| ILI-L | 7 | 966 |\n| Exchange-S | 8 | 6, 071 |\n| Solar-S | 137 | 7, 009 |\n| Electricity-S | 370 | 5, 833 |\n| Traffic-S | 963 | 4, 001 |\n| Wikipedia-S | 2000 | 792 |\n\n\n**Response to Question 1**\n\nThank you for bringing up these forward-thinking questions. We concur that zero-shot/few-shot/transfer/pretraining are all valuable and under-explored paradigms in the realm of time-series forecasting.\n\nAs stated in our paper, our focus is on unifying probabilistic and non-probabilistic methods and facilitating comprehensive evaluations across diverse data characteristics. In doing so, we highlight many overlooked challenges, unresolved issues, and the strengths and weaknesses of existing approaches. Hence, we chose to concentrate on these aspects within this paper.\n\nLooking ahead, we believe that revealing these unique insights to the community will also be instrumental in advancing research in time-series forecasting within the zero-shot/few-shot/transfer/pre-training paradigms. This is because all these learning paradigms involve the selection of probabilistic or non-probabilistic estimations, custom or general architectures, and autoregressive or non-autoregressive decoding schemes. As for ProbTS itself, we believe it is certainly feasible to extend its scope beyond the supervised paradigm of time-series forecasting. This is due to the fact that we have compiled a wide range of datasets and aim to unify neural architecture designs and probabilistic estimation abilities. Based on your suggestion, we will include a discussion on these aspects in the revised paper, thus presenting a broader vision and greater potential impacts.\n\n\n**Response to Question 2**\n\nCurrently, we have not encountered issues with RAM insufficiency. To facilitate machine learning on large quantities of data that do not fit into RAM, it's essential to establish a comprehensive data pipeline that systematically manages data sharding, indexing, shuffling, queuing, and so forth. For this purpose, we could learn from successful toolkits in language and vision domains. However, it's important to note that the focus of this paper is on unifying different research branches in time-series forecasting, rather than designing efficient pipelines to handle large-scale time-series data.\n\nRegarding your query about licenses, rest assured that all datasets utilized in our experiments are open-sourced and publicly accessible. We've compiled the sources of these datasets in the table below for your reference.\n\n| Dataset | Source |\n| :--- | :--- |\n| ETT | Zhou, Haoyi, et al. \"Informer: Beyond efficient transformer for long sequence time-series forecasting.\" Proceedings of the AAAI. Vol. 35. No. 12. 2021. |\n| Electricity | https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014 |\n| Traffic | http://pems.dot.ca.gov/ |\n| Weather | https://www.bgc-jena.mpg.de/wetter/ |\n| Exchange | Lai, Guokun, et al. \"Modeling long-and short-term temporal patterns with deep neural networks.\" Proceedings of SIGIR. 2018. |\n| ILI | https://gis.cdc.gov/grasp/fluview/fluportaldashboard.html |\n| Solar | https://forecastingdata.org/ |\n| Wikipedia | Gasthaus, Jan, et al. \"Probabilistic forecasting with spline quantile function RNNs.\" The 22nd international conference on artificial intelligence and statistics. PMLR, 2019. |"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7486/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700501038193,
                "cdate": 1700501038193,
                "tmdate": 1700502789292,
                "mdate": 1700502789292,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]