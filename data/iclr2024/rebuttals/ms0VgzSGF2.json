[
    {
        "title": "Bridging State and History Representations: Understanding Self-Predictive RL"
    },
    {
        "review": {
            "id": "M37yeWXV7l",
            "forum": "ms0VgzSGF2",
            "replyto": "ms0VgzSGF2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1362/Reviewer_sXU8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1362/Reviewer_sXU8"
            ],
            "content": {
                "summary": {
                    "value": "The work overviews existing abstractions for state/history representations and their \"conditions\", and, with an implication graph, shows how are they connected (supported by proofs). It overviews various RL algorithms and classifies them by which abstraction and conditions they use. Finally, based on their theoretical findings, the authors suggests a minimalist algorithm (and its variants), which they empirically evaluate and compare, from various angles."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The paper is concise and clearly written. It is noteworthy that the studied topic is very large and requires lot of details, and that the Appendix contains some interesting details and even novel contributions (e.g., details regarding the implication graph), which implies that a journal format would suit this work better. Nevertheless, the authors manged to fit the most interesting information in the page limit, and hence I will focus only on the main text. \n\nGiven by the number of works focusing on representation learning (Table 1), it is clear that it is an important topic. Hence the theoretical connections given in the paper between the individual parts of the abstractions is very significant to avoid double and superfluous work. Also, the contribution about stop-gradients, connection to representation collapse and the experiments showing the rank of the weight matrices is very interesting. The theoretical part of the paper is very well done and would alone justify my rating of the paper. \n\nIn the experimental section, the authors verify their theoretical contributions. The experiments about stop-gradients are good. The experiments showing different variants of the minimalistic algorithm (Figure 3) are somewhat inconclusive, or require further discussion (e.g., claims about better/similar performance compared to ALM(3), or no justification for superior performance of ALM on Humanoid). I appreciate the negative result in 5.2 for ZP objective hypothesis."
                },
                "weaknesses": {
                    "value": "As mentioned, the paper sometimes outsources interesting details into the Appendix and the experiment in 5.1 is inconclusive and requires unbiased and fair discussion (also see questions)."
                },
                "questions": {
                    "value": "- In Sec. 5.2, you write: \"Surprisingly, model-free RL (\u03d5_{Q\u2217}) performs worse than \u03d5_O.\" Isn't that expected?\n- Given the scope of the article, why did you preferer conference vs. journal? (not answering this question can be understandable)\n- Please elaborate on \"This suggests that the primary advantage ALM(3) brings to model-free RL, lies in state representation rather than policy optimization\" from Sec. 5.1. Isn't your model based on ALM? How does the stripped-down version of ALM (that you use as a basis for your augmentations) compare to \u03d5_Q\u2217 (TD3)? Is it the same?\n\nSuggestions:\n- define FKL / RKL abbr. - good place good be on p. 5 after \"includes forward and reverse KL\".\n- p. 5: detached from the computation graph [OR] using a copy ...\n- in Fig. 6 caption, mention that RP+OP / RP+ZP are phased; it is not obvious what the difference is and why they should perform worse\n\nNitpicking:\n- legend in Figure 5 is slightly different from the graphs"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1362/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698688896521,
            "cdate": 1698688896521,
            "tmdate": 1699636063798,
            "mdate": 1699636063798,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xAaATj5VXz",
                "forum": "ms0VgzSGF2",
                "replyto": "M37yeWXV7l",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1362/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1362/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' response"
                    },
                    "comment": {
                        "value": "Thank you for your positive feedback! \n\n> The experiments showing different variants of the minimalistic algorithm (Figure 3) are somewhat inconclusive or require further discussion (e.g., claims about better/similar performance compared to ALM(3)).\n> Please elaborate on \"This suggests that the primary advantage ALM(3) brings to model-free RL, lies in state representation rather than policy optimization\" from Sec. 5.1. \n\nPlease see our general response on these two points. \n\n> No justification for superior performance of ALM on Humanoid. \n\nWe have recognized this gap and have included a justification in our revision for ALM(3)'s superior performance in the Humanoid task. We suggest that this is likely attributed to the effectiveness of SVG policy optimization in leveraging first-order gradient information from latent dynamics, a feature that seems particularly advantageous in high-dimensional tasks like Humanoid.\n\n\n> Isn't your model based on ALM? How does the stripped-down version of ALM (that you use as a basis for your augmentations) compare to \u03d5_Q\u2217 (TD3)? Is it the same?\n\nOur minimalist algorithm shares the goal of learning self-predictive representations with ALM, but diverges in the approach to policy optimization. The key differences in policy optimization between our algorithm and various ALM versions are detailed in Appendix D.2 and can be summarized as follows:\n\n- ALM(3): uses SVG with 3-step horizon planning and additional intrinsic rewards\n- ALM-no-model: uses SVG with 1-step horizon planning \n- ALM(0): uses model-free TD3 in the latent space\n- Our minimalist algorithm: uses model-free TD3 in the latent space, akin to ALM(0), but with distinct encoder objectives regarding detaching latent states (see Appendix D.2 for details)\n\nRegarding your question on the comparison to $\\phi_{Q^*}$ (TD3), the stripped-down version of ALM (assumed to be ALM-no-model or ALM(0)) differs from [TD3 in the original state space ($\\phi_{Q^*}$)] as it operates in the latent state space and aims to learn $\\phi_L$. \n\n\n> In Sec. 5.2, you write: \"Surprisingly, model-free RL ($\\phi_{Q\u2217}$) performs worse than $\\phi_O$.\" Isn't that expected?\n\nThis finding was indeed unexpected. The rationale is that model-free RL, not requiring observation prediction, would presumably be less susceptible to distractions. However, our results indicate otherwise. We have also included the explanation to the new version.  \n\n> Given the scope of the article, why did you preferer conference vs. journal? (not answering this question can be understandable)\n\nWe chose to submit to ICLR due to its high relevance to our research topic (representation learning) and its high impact factor. Additionally, we plan to submit to a journal for more in-depth reviews.\n\n\n> define FKL / RKL abbr. - good place good be on p. 5 after \"includes forward and reverse KL\".\n\nThanks and we've added it. \n\n> p. 5: detached from the computation graph [OR] using a copy ...\n\nHere we emphasized that the copy of $\\phi$ is also detached from the computation graph (i.e., it is a `deepcopy` in python), so we think \"and\" is more appropriate. \n\n> in Fig. 6 caption, mention that RP+OP / RP+ZP are phased; it is not obvious what the difference is and why they should perform worse\n\nThe caption of Figure 6 has been updated to indicate that RP+OP / RP+ZP use a phased training approach, as detailed in Section 3.1. \n\nOur initial hypothesis did not anticipate a significant performance difference between the phased and end-to-end approaches, given that both utilize the same R2D2 backbone and target similar representations. However, the end-to-end approach was found to be at least comparable, if not superior, to the phased one in our experiments. We speculate this could be due to the phased approach dealing with accumulated errors from three separate losses (reward prediction, OP or ZP, Q-learning loss) as opposed to the end-to-end approach's two losses (OP or ZP, Q-learning loss), potentially complicating the optimization process.\n\n\n> legend in Figure 5 is slightly different from the graphs\n\nWe have carefully reviewed the figure and its legend but did not identify any discrepancies. Could you please specify which part of the legend you find to be different from the graph? \n\nHope our responses have resolved your concerns!"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700178325284,
                "cdate": 1700178325284,
                "tmdate": 1700178325284,
                "mdate": 1700178325284,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CgFYVfE031",
                "forum": "ms0VgzSGF2",
                "replyto": "xAaATj5VXz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1362/Reviewer_sXU8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1362/Reviewer_sXU8"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their response and appreciate the answers.\n\nRegarding the legend in Fig. 5 - the markers have a white border in the graphs, while in the legend they do not. It is of least priority, though."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700298419590,
                "cdate": 1700298419590,
                "tmdate": 1700298419590,
                "mdate": 1700298419590,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BC8MUgdHGY",
                "forum": "ms0VgzSGF2",
                "replyto": "M37yeWXV7l",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1362/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1362/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your positive feedback and pointing the issue out! Upon reviewing the code, we found that the absence of borders around markers in legends seems a default behavior of the seaborn library. It is a bit hard to change this behavior. Therefore, to maintain consistency, we opted to remove the edges from the markers in the plots, which also makes the plots clearer. This change has been applied to the updated figure in the latest PDF. Please have a look!"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700344030507,
                "cdate": 1700344030507,
                "tmdate": 1700344245273,
                "mdate": 1700344245273,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7eE24YTuEu",
            "forum": "ms0VgzSGF2",
            "replyto": "ms0VgzSGF2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1362/Reviewer_kS5Z"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1362/Reviewer_kS5Z"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a unification of various state representation learning algorithms under the umbrella of self-predictive representations. They draw relations between different algorithms under their terminology and even suggest why stop-gradients are useful when learning self-predictive representations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. I agree very much with the paradox of choice that is stated in the paper. I think a unification is much needed, and this paper is valuable from that point of view.\n2. The paper is generally well-written.\n3. Table 1 and Figure 1 are useful to understand the framework."
                },
                "weaknesses": {
                    "value": "See questions"
                },
                "questions": {
                    "value": "1. While the main message of the paper seems clear, I am struggling to understand the core message of the empirical section. Few questions:\n  - Should we be using minimalist over everything else? If so, ALM outperforming minimalist in Figure 3 on multiple occasions would not support that.\n  - Is minimalist better than $\\phi_{Q^{*}}$? I think this is somewhat obvious, which is the reason we have all these works on representation learning (this is not a criticism, just a remark).\n  - Is it broadly to categorize the entire zoo of representation learning algorithms into their essential components i.e. $\\phi_{Q^*}$, $\\phi_L$, $\\phi_O$ and just study how they perform empirically?\n  - Misc point: \n     - I understand the point of the different loss function and representation collapse experiments, and those are useful.\n     - It would be very useful to include $\\textit{why}$ it is important to test the questions that are posed in Section 5, and what the final algorithm recommendation should be.\n     - The paper discusses this paradox of choice, but I still find it unclear how to make a choice at the end of the paper.\n2. The sample efficiency claims in Section 5.1 are unclear to me. It appears that ALM(3) outperforms $\\phi_L$ in all cases. It is true that $\\phi_L$ does better than $\\phi_{Q^*}$, but the former does not seem to be. I would suspect that ALM(3) will do better because it is explicitly learning a reward model, whereas $\\phi_L$ is suggesting that the reward model/representations are implicitly learned based on their implication graph.\n3. Is there intuition for why $\\phi_O$ may struggle with distractors? The question is posed in the empirical section, but it\u2019s a bit unclear what motivates this question.\n\nI should note that I do like the paper, but the above things are confusing. I would be willing to re-evaluate the score based on the response to the above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "n/a"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1362/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1362/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1362/Reviewer_kS5Z"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1362/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698764230070,
            "cdate": 1698764230070,
            "tmdate": 1700238146889,
            "mdate": 1700238146889,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ToynzxfQCL",
                "forum": "ms0VgzSGF2",
                "replyto": "7eE24YTuEu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1362/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1362/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' response"
                    },
                    "comment": {
                        "value": "Thank you for your positive feedback! In response to the main concerns about clarity, we've extensively revised our paper. This includes adding a new section on recommendations (Section 6) and a detailed discussion on motivations (Appendix G). We've also carefully addressed other writing concerns through targeted revisions and detailed responses below. \n\n> It would be very useful to include *why* it is important to test the questions that are posed in Section 5. \n> Is there intuition for why $\\phi_O$ may struggle with distractors? The question is posed in the empirical section, but it\u2019s a bit unclear what motivates this question.\n\nThank you for highlighting these points! To address your queries, we have detailed the motivation behind each hypothesis in **Appendix G (\"Motivating Our Hypotheses\")**. This section aims to clarify the rationale behind our research questions, including the specific inquiry about the performance of $\\phi_O$ in the presence of distractors. We invite you to review this dedicated section and would appreciate your feedback on its clarity. \n\n> It would be very useful to include what the final algorithm recommendation should be.\n\nThis a very good point! Please see our general response on our **recommendations**. \n\n> Should we be using minimalist over everything else? If so, ALM outperforming minimalist in Figure 3 on multiple occasions would not support that.\n\nOur intention is not to position the minimalist algorithm as superior to others, such as ALM, in all aspects. The primary goal of the minimalist algorithm is to provide a straightforward means to validate the hypotheses derived from our theoretical framework. Its simplicity aids in easy implementation and understanding. Additionally, it serves as a baseline for future research, enabling separate examination of the impacts of representation learning and policy optimization. \n\nAs an example, in Section 5.1, we discuss how ALM's performance advantage over TD3 at 500k steps in MuJoCo tasks is largely attributable to its approach to representation learning, illustrating the utility of the minimalist algorithm in isolating these factors for analysis.\n\n> Is minimalist better than $\\phi_{Q^*}$? I think this is somewhat obvious, which is the reason we have all these works on representation learning (this is not a criticism, just a remark).\n\nOur experiments demonstrate that the minimalist algorithm indeed outperforms $\\phi_{Q^*}$ in the tasks we evaluated, which include standard MuJoCo, distracting MuJoCo, and MiniGrid. However, it's important to note that these findings may not generalize to every RL task. The performance of deep RL algorithms can vary significantly depending on the specific task structure. \n\n> Is it broadly to categorize the entire zoo of representation learning algorithms into their essential components i.e. $\\phi_{Q^*},\\phi_L,\\phi_O$, and just study how they perform empirically?\n \nYes, we think it is indeed a good future direction for empirical studies.  Alongside this, we also see significant potential in developing new theoretical frameworks. These could include understanding the interplay between representation learning and policy optimization, as well as establishing formal guarantees about the relative sample efficiency of learning different representations, such as $\\phi_L$ over $\\phi_O$ in distracting tasks, and $\\phi_O$ over $\\phi_{Q^*}$ in tasks with sparse rewards.\n\n> The sample efficiency claims in Section 5.1 are unclear to me. It appears that ALM(3) outperforms $\\phi_L$ in all cases. \n\nOur data shows varied results across different tasks. In Walker2d and HalfCheetah, $\\phi_L$ outperforms ALM(3), while in the Ant task, they demonstrate comparable sample efficiency *at 500k steps*. However, in the Humanoid task, ALM(3) significantly outperforms $\\phi_L$. For further clarification, please refer to our general response on this topic.\n\n\n> I would suspect that ALM(3) will do better because it is explicitly learning a reward model, whereas $\\phi_L$ is suggesting that the reward model/representations are implicitly learned based on their implication graph.\n\nWe want to clarify that although ALM(3) explicitly learns a reward model, its encoder objective does *not* include reward prediction, as evidenced by [their code](https://github.com/RajGhugare19/alm/blob/main/agents/alm.py#L209) uses `z_dist.sample()` that detaches the latent states when learning the reward model. Therefore, it's unlikely that explicit reward prediction is a significant factor in learning an effective encoder. Rather, ALM(3)'s enhanced performance can more plausibly be attributed to its use of SVG planning for policy optimization. This clarification has been added to the new version of our paper.\n\n\nHope our responses have resolved your concerns!"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700177743557,
                "cdate": 1700177743557,
                "tmdate": 1700177743557,
                "mdate": 1700177743557,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XmcKDTt6zZ",
                "forum": "ms0VgzSGF2",
                "replyto": "ToynzxfQCL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1362/Reviewer_kS5Z"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1362/Reviewer_kS5Z"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you to the authors for their response. I am satisfied with the response. I do like the paper and will raise the score. I think the documentation/unification of all these algorithms and distilling them down to their core components is valuable."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700238123765,
                "cdate": 1700238123765,
                "tmdate": 1700238123765,
                "mdate": 1700238123765,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xREkuhGMJS",
            "forum": "ms0VgzSGF2",
            "replyto": "ms0VgzSGF2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1362/Reviewer_5DUm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1362/Reviewer_5DUm"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a conceptual framework for unifying many existing techniques for decision-making-focused representation learning based on the concept of \"self-prediction\". Within this framework, the paper then proposes a simple and seemingly novel \"minimalist\" approach to construct representations simply by training a model-free agent and simultaneously learning an abstract forward model while preventing gradients from flowing backward through the encoder from the targets. The framework allows a large number of representation learning methods to be evaluated side-by-side while controlling for other factors, which enables the paper to present and evaluate several hypotheses about the various learning objectives. The experiments suggest that the proposed \"minimalist\" approach works well."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Overall, this is a nice contribution!\n\nThe shared framework is helpful for comparing a seemingly endless number of decision-making-focused representation learning methods. It distills these methods down to their core representation learning ideas, which allows for comparing those ideas without needing to worry about the remaining complexity (such as the particular RL algorithm employed). I suspect the community will find this quite valuable.\n\nThe paper also offers interesting insights, such as the fact that online targets do not guarantee the same fixed points as the objectives with stop-gradients.\n\nThe proposed \"minimalist\" approach is extremely simple and clean. It's shocking that this hasn't been tried before, yet I cannot think of an example where it has. Time will tell if this approach generalizes to other problems, but if it does, it will greatly simplify many projects that rely on representation learning.\n\nThe experiments seem well designed, the baselines seem well chosen, and the results look promising. I particularly liked the experiments measuring the change in matrix rank over time---nice job. That is clear evidence that the method avoids representation collapse."
                },
                "weaknesses": {
                    "value": "My concerns are minor.\n\n- I'm not familiar with using the phrase \"distracted MDPs\" to mean MDPs with distracting elements.\n- Maybe I'm wrong, but I feel like we already know that ZP + $\\phi_{Q*}$ implies RP.\n- Top of p5: The use of $\\mathbb{P}$ and $\\mathbb{Q}$ is confusing, given that $P$ and $Q$ are often overloaded in RL.\n- Towards end of sec 4.2, last para: \"...cosine similarity between columns of the learned $\\phi$. As expected by Thm. 3, [...] stay several orders of magnitude smaller when using stop-gradient.\" How does Thm 3 predict low absolute cosine similarity? Because we start with full rank?\n- Fig 3. Kind of hard to distinguish lines.\n- Sec 5, first para: would be helpful to summarize the findings when introducing the hypotheses.\n- p7, penultimate para: I don't know about similar sample efficiency in ant. It seems a lot worse. I also don't know if I agree with the conclusion that \"the primary advantage ALM(3) brings to model-free RL, lies in the state representation rather than policy optimization, except for Humanoid.\" I feel like it's too soon to conclude something as sweeping as that.\n- Fig 5. A little difficult to read. See if maybe log return would help?\n- Top of p9, minigrid experiment. This feels like a minor bait-and-switch. Normally minigrid uses pixels, no? And why does detached rank drop on minigrid when theory suggests it should remain high?\n- \"Validation of end-to-end hypothesis\" (and throughout). It's a bit hard to keep track of which is which. Text uses $\\phi_o$, $\\phi_L$, but fig uses OP, ZP, RP+OP, RP+ZP."
                },
                "questions": {
                    "value": "Do the authors have a clear recommendation on when to use OP vs ZP vs RP+[*]? It would be nice to have some clear takeaways after all this analysis."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1362/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698803181679,
            "cdate": 1698803181679,
            "tmdate": 1699636063656,
            "mdate": 1699636063656,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1G3spCaecP",
                "forum": "ms0VgzSGF2",
                "replyto": "xREkuhGMJS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1362/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1362/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' response"
                    },
                    "comment": {
                        "value": "Thank you for your positive feedback! \n\n> I'm not familiar with using the phrase \"distracted MDPs\" to mean MDPs with distracting elements.\n\nWe appreciate this observation and have made adjustments to clarify our terminology. We've changed \"distracted MDPs\" to \"distracting MDPs\" in our paper. Additionally, we've included a footnote stating, \"Distracting MDPs refer to MDPs with distracting observations irrelevant to optimal control in this work,\" to prevent any potential confusion.\n\n> Maybe I'm wrong, but I feel like we already know that ZP + $\\phi_{Q^*}$ implies RP.\n\nYou raise a valid point. While the connection between ZP + $\\phi_{Q^*}$ and RP might seem intuitive, our work contributes to formalizing this relationship. The inverse Bellman equation in our proof, also used in imitation learning literature [Garg et al., IQ-Learn: Inverse soft-Q Learning for Imitation], provides a novel perspective specific to state or history representations for RL. To the best of our knowledge, this precise articulation is new in both MDPs and POMDPs. \n\n> Top of p5: The use of $\\mathbb P$ and $\\mathbb Q$ is confusing, given that $P$ and $Q$ are often overloaded in RL.\n \nWe acknowledge this potential confusion. To address it, we have now replaced $\\mathbb Q$ with $\\mathbb P$ in our paper, eliminating the notational conflict and avoiding confusion with Q-values in RL.\n\n> How does Thm 3 predict low absolute cosine similarity? Because we start with full rank?\n\nIn this experiment, the latent state dimension is set to $2$ and we orthogonally initialize the $\\phi$ matrix. This results in $\\phi^\\top \\phi = [1, 0; 0, 1]$ initially. According to Thm 3,  $\\phi^\\top \\phi$ maintains its initial value in continuous-time. Thus, the cosine similarity (and the inner product) between the two columns of $\\phi$ will remain close to $0$ in discrete-time.\n\nTherefore, Thm 3 predicts low absolute cosine similarity because we initialize $\\phi$ as an orthogonal matrix, which is a stronger condition of having full rank. \n\n> Fig 3. Kind of hard to distinguish lines.\n\nAcknowledging the difficulty in distinguishing lines in Figure 3, we also want to point out that the overlap of curves mainly illustrates similar performance trends. Here we offer a qualitative comparison at 500k steps using symbols: $<$ for worse, and $\\ll$ for much worse. Hope this will help you read the figure. \n\n- Walker2d: ALM(3) and TD3 (they are quite similar) $\\ll$ OP and ZP methods (they are quite similar).\n- HalfCheetah: TD3 $<$ ALM(3) $<$ OP methods $<$ ZP methods.\n- Ant: TD3 $\\ll$ OP-FKL $<$ the other methods. Note that ALM(3) initially leads but later declines.\n- Humanoid: TD3 and ZP methods $<$ OP methods $\\ll$ ALM(3). \n\n> Sec 5, first para: would be helpful to summarize the findings when introducing the hypotheses.\n> Do the authors have a clear recommendation on when to use OP vs ZP vs RP+[*]? It would be nice to have some clear takeaways after all this analysis.\n\nThanks for your suggestion! We've added our findings to the paper and please the general response on **recommendations**. \n\n> p7, penultimate para: I don't know about similar sample efficiency in ant. It seems a lot worse. I also don't know if I agree with the conclusion that \"the primary advantage ALM(3) brings to model-free RL, lies in the state representation rather than policy optimization, except for Humanoid.\" I feel like it's too soon to conclude something as sweeping as that.\n\nPlease see our clarification on this in our general response.\n\n> Fig 5. A little difficult to read. See if maybe log return would help?\n\nWe tried to plot the log returns but found it did not help clarity much. This is due to the scale of episode returns, which are mainly around $10^3$, with the exception of the Ant. In the Ant with $\\ge 2^7$ distractors, all methods are close to $0$, indicating they are equally bad. For the other environments with large number of distractors, the results clearly show that TD3 is the worst performer. Following this, OP-$\\ell_2$, OP-FKL, and ZP-FKL exhibit similar performance levels. ZP-$\\ell_2$ and ZP-RKL stand out as the best performers.\n\n> And why does detached rank drop on minigrid when theory suggests it should remain high?\n\nWe acknowledge that the observed drop in detached rank in the MiniGrid experiment is intriguing, especially since our theory does not predict a significant degradation in such cases. We plan to explore this further in our future research.\n\n\n>\"Validation of end-to-end hypothesis\" (and throughout). It's a bit hard to keep track of which is which. Text uses $\\phi_O, \\phi_L$  but fig uses OP, ZP, RP+OP, RP+ZP.\n\nTo address this, we have revised the text in the new version of our paper to enhance readability and ensure consistency between the text and figures."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700178104632,
                "cdate": 1700178104632,
                "tmdate": 1700193557883,
                "mdate": 1700193557883,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Gmuk7WkwZv",
                "forum": "ms0VgzSGF2",
                "replyto": "53iw0SDefi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1362/Reviewer_5DUm"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1362/Reviewer_5DUm"
                ],
                "content": {
                    "title": {
                        "value": "Re: deterministic encoders"
                    },
                    "comment": {
                        "value": "I have to side with the authors regarding deterministic encoders. If the function mapping from input to latent state is a deterministic function, then it's a deterministic encoder, regardless of what loss you want to use. There is a nice interpretation that MSE corresponds to a Gaussian assumption, but the encoder itself can be deterministic nevertheless. It may produce an output that is a random variable, and that random variable may be approximately normally distributed, but if the function is deterministic, I'm inclined to call it a deterministic encoder."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700672276701,
                "cdate": 1700672276701,
                "tmdate": 1700672276701,
                "mdate": 1700672276701,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "toS9JKAYho",
                "forum": "ms0VgzSGF2",
                "replyto": "wkD5mL4A5B",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1362/Reviewer_5DUm"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1362/Reviewer_5DUm"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your response"
                    },
                    "comment": {
                        "value": "Thank you for the response. I think this work will be valuable for helping make sense of the connections between all these different methods."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700677327581,
                "cdate": 1700677327581,
                "tmdate": 1700677327581,
                "mdate": 1700677327581,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "GN9Sjhlp1W",
            "forum": "ms0VgzSGF2",
            "replyto": "ms0VgzSGF2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1362/Reviewer_fv1F"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1362/Reviewer_fv1F"
            ],
            "content": {
                "summary": {
                    "value": "The paper attempts at providing a unified view at self-predicting reinforcement learning. Different self-predicting representation targets are described, and the prior work is classified according to the target learned. Based on the unified view, a minimalist algorithm learning a self-predicting representation is learned. The algorithm is evaluated on a set of benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper encompasses a broad range of recent and ongoing work on self-predicting RL, providing a unifying view. Theoretical results are presented, with proofs in the appendix. An algorithm proposed in the paper is evaluated on benchmarks."
                },
                "weaknesses": {
                    "value": "The results in the paper are either trivial or indecisive. The paper is built around an insight that different works do similar things trying to optimize self-prediction of certain features, but this is, in my opinion, trivial. A classification of things that can be optimized for self-prediction is worth a survey, but this paper is not a survey. The paper uses a lot of abbreviations, the proofs a sketchy and uncommented, and veryfiying or even following the proofs takes tremendous effort.\n\nThe empirical evaluation is unconvincing. According to the plots presented in the paper, the proposed unified algorithm does not outperform (and does not always perform comparably) to algorithms from the literature. Looking at the algorithm pseudocode and implementation, this is not surprising, given that the 'minimalist algorithm' is more of a boilerplate, which, when filled with details, reduces to one of the earlier published algorithms. However, any practical implementation requires attending to details, which the unifying minimalist algorithms fails to achieve. \n\nThe paper would be extremely hard to follow, in my opinion, for an outsider, or for someone less familiar with the slang of a particular research group. For example, the paper discusses (and presents theoretical results) wrt to \"stop-gradient\" technique without formally defining the technique (which is described in passing and requires referring to the cited sources to understand the paper).\n\nI believe that his research may have a potential, but for a publication, I would suggest deciding on a small subset of ideas among those sketched in this submission, presenting them thoroughly and rigorously, with proofs that are possible to follow, and accompanied by an implementation that brings competitive results, in some form."
                },
                "questions": {
                    "value": "In the introduction, you are writing \"However, this abundance of\nmethods may have inadvertently presented practitioners with a \u201cparadox of choice\u201d, hindering their\nability to identify the best approach for their specific RL problem.\" \n\nHow does your paper help practitioners identify and use the best approach for their specific RL problem? Can you give an example of application? For example, suggest (and describe) a simple RL problem, and show how your result help choose the best solution."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1362/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698829688574,
            "cdate": 1698829688574,
            "tmdate": 1699636063582,
            "mdate": 1699636063582,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oy0HUNdXcp",
                "forum": "ms0VgzSGF2",
                "replyto": "GN9Sjhlp1W",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1362/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1362/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' response"
                    },
                    "comment": {
                        "value": "Thank you for your feedback! It seems the main concern is the limited contribution. We agree that many prior methods are based on a similar intuition of self-prediction. However, these methods come from different perspectives and have various theoretical properties with nuanced-yet-crucial implementation differences (see Table 1 and Appendix C). Our objective is to contextualize these varied approaches within a unified framework, aiming to distill fundamental principles of representation learning in RL. This endeavor seeks not just to deepen theoretical understanding and prevent overlapping research efforts, but also to offer practical guidance to practitioners (Section 6), as elaborated in our general response.\n\n\n> How does your paper help practitioners identify and use the best approach for their specific RL problem? \n\nWe have included recommendations for practitioners in Section 6 in the revision. For more detailed information, please refer to our general response on **recommendations**.\n\n> The results in the paper are either trivial or indecisive. The paper is built around an insight that different works do similar things trying to optimize self-prediction of certain features, but this is, in my opinion, trivial. \n\nWe respectfully disagree with the view that our work is trivial or unconvincing.  Contrary to this opinion, the other three reviewers (5DUm, kS5Z, sXU8) have recognized the significance and value of our unified framework, which is much needed to the RL community. 5DUm noted its utility in enabling comparisons of various methods based on their core representation learning principles without being confounded by other components. sXU8 commented on its potential to prevent redundant research efforts. \n\n\n> The paper uses a lot of abbreviations.\n\nTo enhance readability, we have implemented clickable abbreviations that link directly to their definitions. Additionally, we occasionally spell out the full names of these abbreviations to aid recall.\n\n> The proofs a sketchy and uncommented, and verifying or even following the proofs takes tremendous effort.\n\nAre there any specific proofs that you found challenging to follow? We would appreciate your pointing them out and offer further clarifications as needed.\n\n> The empirical evaluation is unconvincing. According to the plots presented in the paper, the proposed unified algorithm does not outperform (and does not always perform comparably) to algorithms from the literature. \n\nThe aim of the experiments and the paper as a whole is not to propose a SOTA or competitive method, but rather understand various self-predictive methods within the broader context of representation learning. The empirical evlauation is intended solely to validate our hypotheses suggested by our theory. In fact, our empirical results validate most of our hypotheses. \n\nNotably, the findings related to stop-gradients were particularly convincing, as acknowledged by the other reviewers (5DUm, kS5Z, sXU8). For detailed clarification on the comparisons with ALM(3), please refer to our general response.\n\n> Looking at the algorithm pseudocode and implementation, this is not surprising, given that the 'minimalist algorithm' is more of a boilerplate, which, when filled with details, reduces to one of the earlier published algorithms. However, any practical implementation requires attending to details, which the unifying minimalist algorithms fails to achieve.\n\nTo the best of our knowledge, our algorithm, employing end-to-end training with a self-predictive auxiliary task, is novel within the POMDP literature. Is the reviewer aware of prior work that can be reduced to our algorithm in POMDPs? \n\nIn addition, we do care about practical implementation in our algorithm. For example, we tuned the coefficient of the auxiliary task for each benchmark. We also base our algorithm on existing, well-tuned implementations, rather than writting a new one from scratch."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700177472191,
                "cdate": 1700177472191,
                "tmdate": 1700206070389,
                "mdate": 1700206070389,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GdAaFn5vKJ",
                "forum": "ms0VgzSGF2",
                "replyto": "oy0HUNdXcp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1362/Reviewer_fv1F"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1362/Reviewer_fv1F"
                ],
                "content": {
                    "comment": {
                        "value": "I am going to re-read the paper carefully and update my review based on this. It possible that I was too quick judging the paper, based on other reviewers' opinions. It is also possible that the paper's style is beyond my mental ability of comprehension."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700303773996,
                "cdate": 1700303773996,
                "tmdate": 1700303773996,
                "mdate": 1700303773996,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XpXhIWie7q",
                "forum": "ms0VgzSGF2",
                "replyto": "GdAaFn5vKJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1362/Reviewer_fv1F"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1362/Reviewer_fv1F"
                ],
                "content": {
                    "title": {
                        "value": "More detailed comments`"
                    },
                    "comment": {
                        "value": "I've taken time to re-read the paper. Here are my notes (I am not picky, if you want me to be picky there will be ten times more such notes) from 4.5 first pages (and one proof from the appendix).\n\nPage 2: \"encoder of the value function\" -> \"encoding learned with/for value function\"\n\nPage 2: next latent state prediction --- the distribution of the encodings rather than the state is predicted.\n\nPage 2: \"encoder is capable of predicting\" the encoder is defined only encodes the state.\n\nPage 3: \"expected next latent state\" --- the latent state is not defined on a codomain for which the mean can be defined (e.g. R^n or Z^n). \n\nPage 4: \"The bootstrapping effect ...\" phi does not in \"both sides of the next latent state prediction\" explicitly, one must guess that z' = phi(h') .\n\nPage 4-5\" \"we parameterize an encoder .. deterministic case ... stochastic case. This is not a stochastic encoder, but a probabilistic one. A probabilistic encoder returns a distribution \\Delta(Z), a stochastic encoder returns a sample from a distribution. This is significantly different. There is almost no way to train a 'deterministic encoder' using differentiable models (deep networks).\n\nPage 5: after expr (1), whjere D(., .) \\in R compares two distribution. When (1) reaches minimum --- it does not have to, functions on R do not have to have a minimum.If it is on R+, then it still would be good to know what features D should possess.\n\nPage 5: Equations (2) and (3) expectations are on variable o' which does not appear in the expression under expectation. \n\nPage 5: D (apparently divergence) uses different notation in (1) --- D(., .) and in (3) D(.||.) . This is important because D(., .) commonly implies a metric, or at least a symmetric quantity, while D(.||.) commonly implies assymetric function, such as divergence.\n\nProof of proposition 1 (appendix): second terms in eqs (90) and (91) are mutually cancelled, despite being different."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700581801167,
                "cdate": 1700581801167,
                "tmdate": 1700581801167,
                "mdate": 1700581801167,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TarLHw7a2P",
                "forum": "ms0VgzSGF2",
                "replyto": "XpXhIWie7q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1362/Reviewer_fv1F"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1362/Reviewer_fv1F"
                ],
                "content": {
                    "title": {
                        "value": "Updated impression"
                    },
                    "comment": {
                        "value": "My updated impression is that the paper is very sloppily written, hard to follow, and that the proofs can't be trusted. The first proof I checked had an obvious error.\n\nThe paper may have some great ideas, but it must significantly rewritten and improved before it can be considered for publication."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700581896890,
                "cdate": 1700581896890,
                "tmdate": 1700581896890,
                "mdate": 1700581896890,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TUUenxYCXf",
                "forum": "ms0VgzSGF2",
                "replyto": "GN9Sjhlp1W",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1362/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1362/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your valuable feedback! We have addressed the main concerns around some confusing terminologies and the seemingly incorrect proof in the appendix. Below are our responses to each point, with the changes in the updated PDF highlighted in magenta.\n\n> Page 2: \"encoder of the value function\" -> \"encoding learned with/for value function\"\n\nWe've fixed this term to be \"the encoder *learned for* the value function\" in the new revision to avoid confusion. \n\n> Page 2: next latent state prediction --- the distribution of the encodings rather than the state is predicted.\n\nWe've renamed this term to be \"next latent state *distribution* prediction (ZP)\" to avoid confusion.\n\n\n> Page 2: \"encoder is capable of predicting\" the encoder is defined only encodes the state.\n\nWe've changed this to \"the encoder *can be used to* predict\" for clearer understanding. \n\n> Page 3: \"expected next latent state\" --- the latent state is not defined on a codomain for which the mean can be defined (e.g. R^n or Z^n).\n\nWe've added this assumption into Appendix A.2 (see \"Remark on the latent state distribution\") to address your concern. \n\n> Page 4: \"The bootstrapping effect ...\" phi does not in \"both sides of the next latent state prediction\" explicitly, one must guess that z' = phi(h') .\n\nWe've added this explanation \"since $z'$ also relies on $\\phi(h')$\" to make it clearer. \n\n> Page 4-5\" This is not a stochastic encoder, but a probabilistic one. A probabilistic encoder returns a distribution \\Delta(Z), a stochastic encoder returns a sample from a distribution. This is significantly different. \n\nWe've renamed all instances of \"stochastic encoder\" to \"*probabilistic encoder*\" to avoid confusion between these two concepts. \n\n\n> There is almost no way to train a 'deterministic encoder' using differentiable models (deep networks).\n\nWe believe we can, and actually have, trained a deterministic encoder with neural networks in our implementation. The deterministic encoder $f_\\phi$ takes input a history $h$ and outputs the latent state $z$. We assume $f_\\phi$ is differentiable w.r.t. $\\phi$ for any history $h$, which is the standard assumption in deep RL.\n\n> Page 5: after expr (1), where D(., .) \\in R compares two distribution. When (1) reaches minimum --- it does not have to, functions on R do not have to have a minimum.If it is on R+, then it still would be good to know what features D should possess.\n\nWe've changed $\\mathbb R$ to $\\mathbb R_{\\ge 0}$ (the set of positive reals and zero) to ensure the existence of a minimum. \n\n> Page 5: Equations (2) and (3) expectations are on variable o' which does not appear in the expression under expectation.\n\nWe want to clarify that $o'$ indeed appears in the expression through $h' = (h, a, o')$. We define $h'$ in the \"MDPs and POMDPs\" in the background section. \n\n> Page 5: D (apparently divergence) uses different notation in (1) -- $D(., .)$ and in (3) $D(.||.)$.\n\nWe've unified the notation to $\\mathbb D(\\cdot\\mid \\mid \\cdot)$ to avoid confusion. Since we allow both symmetric and asymmetric functions, we use this notation. \n\n> Proof of proposition 1 (appendix): second terms in eqs (90) and (91) are mutually cancelled, despite being different.\n\n\nWe believe our proof is correct. The two cancelled terms in (90) and (91) are equal due to the following elementary fact:  Inner product is a bilinear form, i.e., $\\langle a + b, c \\rangle = \\langle a, c \\rangle + \\langle b, c \\rangle$. Therefore, it follows that for a constant vector $a$ and random vector $X$ (with PDF $P_X$), \n$$\n\\mathbb E [ \\langle a, X \\rangle ] = \\int_{x} P_X(x) \\langle a, x \\rangle dx = \\int_{x}  \\langle a, P_X(x) x \\rangle dx = \\langle a, \\int_x P_X(x)x dx \\rangle = \\langle a, \\mathbb E[X] \\rangle.\n$$\nNote that in our case, $g_\\theta(f_\\phi(h),a)$ does not depend on $o'$, thus it can be treated as a constant for the derivation. \n\nWe hope our responses and the revisions in the manuscript have resolved the concerns raised. Looking forward to your feedback!"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700605189093,
                "cdate": 1700605189093,
                "tmdate": 1700605743902,
                "mdate": 1700605743902,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "53iw0SDefi",
                "forum": "ms0VgzSGF2",
                "replyto": "TUUenxYCXf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1362/Reviewer_fv1F"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1362/Reviewer_fv1F"
                ],
                "content": {
                    "title": {
                        "value": "not resolved"
                    },
                    "comment": {
                        "value": "There are many more such shortcomings in the paper. I am not here to check and fix it, just to point at the current situation. \n\nRegarding the proof of proposition 1: the proposition 1 is a known fact: minimizing squared error minimizes error of the mean of the distribution. However, the proof is written is wrong in the sense that it does not prove anything, unless you add clarifications. \n\nRegarding deterministic encoder. Any encoder is probabilistic. If it \"outputs latent state\" and you use some loss with regard to that state, you assume a distribution of the latent state for which the loss is the log likelihood. For example, mean squared loss is the assumption of normally distributed latent state. My comment was about incoherence of the paper, there is no deterministic vs. stochastic encoder dilemma, the encoder is always probabilistic, with the difference in the assumed output distribution. You are discussing the difference between assuming parametric, e.g. normal (or Laplace for MAE, for example) distribution and a non-parametric (e.g. categorical on a sample set or density estimated by a network) distribution on the output on the encoder.\n\nI can further elaborate on problems in the paper, but I will refrain from that.  The paper is very sloppy. It should be fully rewritten and properly prepared before it can be published, in my opinion. Not in this form."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700651478780,
                "cdate": 1700651478780,
                "tmdate": 1700651478780,
                "mdate": 1700651478780,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]