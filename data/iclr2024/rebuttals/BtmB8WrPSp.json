[
    {
        "title": "Sparse-PGD: An Effective and Efficient Attack for $l_0$ Bounded Adversarial Perturbation"
    },
    {
        "review": {
            "id": "4Q2PgMd6WQ",
            "forum": "BtmB8WrPSp",
            "replyto": "BtmB8WrPSp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3339/Reviewer_RqW9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3339/Reviewer_RqW9"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method for creating sparse adversarial perturbations. The authors evaluate the approach comparing with existing sparse image-specific attacks, against model robust to $\\ell_\\infty$, $\\ell_2$, and $\\ell_1$ perturbations."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "+ the attacks seem to be run correctly in the evaluation\n+ tested against robust models\n+ interesting approach to achieve sparse perturbations"
                },
                "weaknesses": {
                    "value": "- experimental evaluation should be improved\n- contributions are not fully supported by the experimental evidence and should be clarified"
                },
                "questions": {
                    "value": "Overall, the paper is easy to read and well written. The proposed contribution is significant, however the claims should be supported better by the experimental evidence.\n\n**Experimental evaluation should be improved.** The authors claim the approach is explained with image classification as an example, but the approach should be applicable to any kind of data. This is inconsistent with how the method is evaluated. In fact, the authors write in the introduction:\n\n> For image inputs, we consider the pixel sparsity, which is more meaningful than feature sparsity and consistent with existing works (Croce & Hein, 2019c; Croce et al., 2022). That is, a pixel is considered perturbed if any of its channel is perturbed, and sparse perturbation means few pixels are perturbed.\n\nSo this means that a value of perturbation equal to x corresponds to x pixels changed, but each pixel might contain up to three features. This is written only in the introduction, which makes the evaluation metrics used later for the experiments unclear. \nMoreover, it would be interesting to see the results of this method without this additional constraint. The approach can be still developed, simply by creating a mask for every channel. However, removing this limit would make the attack comparable with many other white-box sparse attacks, including:\n\n* EAD https://arxiv.org/abs/1709.04114\n* VFGA https://arxiv.org/abs/2011.12423\n* PDPGD https://arxiv.org/abs/2106.01538\n* BB https://arxiv.org/abs/1907.01003\n* FMN https://arxiv.org/abs/2102.12827\n\n**Unclear difference with SAIF.** The authors state that the attack method is similar to the SAIF attack (beginning of sect. 4.1). However, they don't explain clearly what the difference is and what they add to this similar attack to make it perform better. This should be discussed in sect. 4.1"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3339/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3339/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3339/Reviewer_RqW9"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3339/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697961764269,
            "cdate": 1697961764269,
            "tmdate": 1699636283342,
            "mdate": 1699636283342,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1trV2HYo7M",
                "forum": "BtmB8WrPSp",
                "replyto": "4Q2PgMd6WQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3339/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3339/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer RqW9,\n\nWe thank reviewer RqW9 for the helpful review! We are glad that you agree that our proposed contribution is significant. In response to your questions, we offer the following point-to-point answers:\n\n1. Experimental evaluation should be improved. The authors claim the approach is explained with image classification as an example, but the approach should be applicable to any kind of data. This is inconsistent with how the method is evaluated.\n\n    **Reply**: To help illustrate the formulation, we used image classification as an example and evaluated the proposed attack on CIFAR-10, CIFAR-100 and ImagNet-100. Similar to previous attacks [1 - 3], we don't have any constraint on the input. As a result, the method can be applicable to any data.  Besides, all these datasets are intensively used and benchmarked in the existing literature [1 - 4] to evaluate the model's robustness.  Lastly, since it is a novel attack method, rather than empirically proving it is applicable to some specific application in some specific data domain (although we agree that it will be a good potential future work), **our main contribution** is general: that is, our attack method empirically outperforms the state-of-the-art sparse attack (including both white-box and black-box attacks) and those models adversarially trained by the attack show the strongest robustness against various sparse attacks. \n\n2. It would be interesting to see the results of this method without this additional constraint. The approach can be still developed, simply by creating a mask for every channel.\n\n    **Reply**: Thanks for pointing this out, we will consider your comments and try to improve our work in future work.\n\n3. Unclear difference with SAIF\n\n    **Reply**: For the attack process, though we adopt a similar decomposition method to the perturbation to SAIF=, **our attack design is significantly different from the previous work in two perspectives:** \n    1. Instead of using an $\\ell_1$ convex surrogate for the $l_0$ norm constraint of the sparsity mask as done in SAIF, we introduce a continuous mask $\\widetilde{m}$ and project it back to the feasible set. To prevent the magnitude of $\\widetilde{m}$ from being too big, we add a sigmoid function before projecting $\\widetilde{m}$ to the binary mask $m$. This **new but simple** strategy has proven to be effective as shown in the ablation studies (Table 2, Sec 5.3) where we observed a huge robust accuracy drop from 49.5\\% to 22.2\\%. Moreover, we further improve the exploration capability for the mask $m$ by a random reinitialization mechanism, which is shown to improve the performance in the ablation study. This design alleviates the problem of local convergence, which is reported in SAIF.\n    2. The ablation studies (Table 2, Sec 5.3) also indicated naively decomposing the perturbation by $\\delta = p \\odot m$ can **deteriorate the performance**, which further corroborates the effectiveness of our attack designs."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3339/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700660562784,
                "cdate": 1700660562784,
                "tmdate": 1700660562784,
                "mdate": 1700660562784,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "o8E6wBzxWT",
            "forum": "BtmB8WrPSp",
            "replyto": "BtmB8WrPSp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3339/Reviewer_DCtb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3339/Reviewer_DCtb"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors investigate the performance of Sparse-PGD, an l0 attack for crafting adversarial examples. Specifically, the authors note how little attention there has been on evaluating the robustness of machine learning models based on l0 threat models. To this end, the authors propose an attack that is specifically optimized for this threat model, borrowing ideas from SAIF. Their method, Sparse-PGD, is built from a magnitude tensor and a sparsity mask, whose design attempt to tackle known problems in l0-based optimization with convergence and gradient explosion. In their evaluation, they compare their attacks against a variety of other attacks and (adversarially trained) models and demonstrate compelling results. The paper concludes with an ablation study on varies components of their attack and a brief experiment on adversarial training."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "**Significance/Originality**- $\\ell_0$-based attacks have received much less attention than other $\\ell_p$ threat models, but represents more realistic threat models in many domains such as network data\n\n**Quality**- Explorations on adversarial training and a broad set of baselines gives a good measure of attack performance.\n\n**Clarity**- Background is well-written, gives a good summary of the field of AML and the various threat models, making it appealing to a broader audience"
                },
                "weaknesses": {
                    "value": "* Optimization is unclear - Section 4.1 requires additional details. Arguments are made concerning when a relaxation is necessary (i.e., through a projection), yet later it is claimed that the relaxation exhibits deficiencies, so the original optimization is used instead. After reading 4.1, it is unclear what optimization sPGD actually entails and what is used in the evaluation.\n* Evaluation methodology - There are many important details are not present in the evaluation and necessary plots are missing (see questions for details)\n* Contribution of attacks introduced in this work is unclear - It does not seem appropriate to add Sparse-RS as part of sAA, given that Sparse-RS is used verbatim from prior work. The evaluation should only include the contributions made in this work.\n* Incomplete characterization of l0-based attacks - JSMA (Papernot, 2016) is not mentioned or evaluated against, even though it is the first l0-based attack"
                },
                "questions": {
                    "value": "Thank you for your contribution to ICLR. It was an interesting read. Below, I summarize some of my main questions concerning this work.\n\n1. Section 4.1 can be confusing at times - Section 4.1 should be revisited, given that there seem to be inconsistencies in the motivation of certain decisions and the optimization itself is unclear. Specifically: (a) for updating the magnitude tensor, are p and delta the same variable? (b) Why is the l2-norm of the loss taken in (5)? (c) For updating the sparsity mask, what is gradient ascent performed on? (d) it is unclear what, \"Since elements in m are 0 or 1, we use sigmoid to normalize elements in m-tilde to be 0 or 1\" is trying to say; aren't elements in m in [0, 1] because of (6)? (e) the argument that projection on the binary set Sm is discarded because coordinate descent is suboptimal is unclear; why is such a projection introduced to be later argued as suboptimal and thus discarded? (In fact, this observation is stated twice) (f) it is unclear where the projection onto the binary set Sm is used in gp and why it is used in tandem with gp-tilde if gp exhibits both non-convergence and gradient explosions, and (g) there are many terms that are co-dependent with other terms throughout 4.1--it is challenging to understand precisely what are the main ingredients of Sparse-PGD, why they matter, and what decisions influenced their design.\n\n2. Evaluation could be clearer - While I appreciate the extensive evaluation, it does not appear to disclose sufficient information to measure the performance of sAA. Specifically, (a) a distortion vs accuracy curve should be plotted, so that we can understand the performance curves of sAA against baselines. Reporting the final results at a fixed norm boundary is not readily indicative of attack performance, given that are are many values of k a defender would consider to be \"adversarial\", (b) when attacking against adversarially trained models, perturbations must stay within the threat model. That is, it should be made clear that, when attacking an l-infinity-based model, the l0 perturbations also do not exceed, e.g., 8/255. Otherwise, it is not clear to me what insights are to be drawn from attacking a model whose threat model is violated, (c) mixing threat models does not seem sound. It is not clear why black-box attacks are compared to white-box attacks, etc. White-box threat models should only be compared to white-box attacks, and likewise for black-box attacks.\n\n3. Attack configuration does not seem fair - It is not clear to me why Sparse-RS is included within sAA when it is used verbatim from prior work. So that readers can understand the core contributions of this work, comparisons against baselines should only be evaluated against the introduced attacks. Moreover, the JSMA (Papernot, 2016) is one of the first l0-based attacks to be introduced in the literature. It is unclear to me why this not compared against in this work."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3339/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698782686252,
            "cdate": 1698782686252,
            "tmdate": 1699636283259,
            "mdate": 1699636283259,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FCN6wgCzRG",
                "forum": "BtmB8WrPSp",
                "replyto": "o8E6wBzxWT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3339/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3339/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal [1/3]"
                    },
                    "comment": {
                        "value": "Dear reviewer DCtb,\n\nThanks for your insightful and constructive comments. In response to your questions, we offer the following point-to-point answers:\n\n1. For updating the magnitude tensor, are p and delta the same variable? \n\n    **Reply**: In our work, we decompose the sparse perturbation $\\delta$ into a magnitude tensor $p\\in\\mathbb{R}^{h\\times w\\times c}$ and a sparsity mask $m\\in\\{0,1\\}^{h\\times w\\times 1}$, i.e., $\\delta =p \\odot m$. We update $p$ and $m$ separately, and then obtain $\\delta$ with the updated $p$ and $m$.\n\n2. Why is the l2-norm of the loss taken in (5)? \n\n    **Reply**: According to the first-order Taylor expansion of loss function, $L(x+v)\\approx L(x)+\\nabla L(x)^T v$. Finding the steepest ascent direction of the loss function is equivalent to finding the $v$ which maximizes the directional derivative. In Euclidean space, if we want to find the steepest ascent direction within the step size of the unit norm, i.e., ||v||2=1 , the direction is $v=\\nabla L(x)/||\\nabla L(x)||_2$ [1].  \nIn our scenarios, we want to update $\\widetilde{m}$ with the step size $\\beta$, so the updating can be written as $\\widetilde{m} = \\widetilde{m} + \\beta \\cdot \\nabla_{\\widetilde{m}} L / (||\\nabla_{\\widetilde{m}} L||_2 + \\gamma)$, where $\\gamma$ is a small constant to prevent the denominator from being zero.\n\n3. For updating the sparsity mask, what is gradient ascent performed on? \n\n    **Reply**: The $l_0$ bounded perturbation $\\delta$ can be decomposed into magnitude tensor $p$ and a sparsity mask $m$, i.e., $\\delta =p \\odot m$. We aim to maximize the loss $L(\\theta, x+p\\odot m)$ through updating $p$ and $m$ with gradient ascent. Since $m$ is sparse and discrete, directly optimizing it is challenging. Instead, we introduce a continuous variable $\\widetilde{m} \\in \\mathbb{R}^{h\\times w\\times 1}$ and project $\\widetilde{m}$ to the feasible set $S_m$ before multiplying it with the magnitude tensor $p$ to calculate the sparse perturbation $\\delta$. Since $\\widetilde{m}$ is a continuous variable, we can optimize it using gradient ascent.\n\n4. It is unclear what, \"Since elements in m are 0 or 1, we use sigmoid to normalize elements in m-tilde to be 0 or 1\" is trying to say; aren't elements in m in [0, 1] because of (6)? \n\n    **Reply**:  We are sorry about the confusion, we have modified the claim in the paper. In addition, we clarify that the sigmoid function used here is to prevent $\\widetilde{m}$ from being too large in magnitude. When the magnitude of $\\widetilde{m}$ is large, its gradient will vanish to zero by applying the sigmoid function, this mechanism will prevent $\\widetilde{m}$ from being too large and ensure the numeric stability. In the same settings of Table 2 in section 5.3, if we discard the sigmoid function, the robust accuracy increases from 20.0 to 68.5, which further demonstrates the effectiveness of the sigmoid function.\n\n5. The argument that projection on the binary set Sm is discarded because coordinate descent is suboptimal is unclear; why is such a projection introduced to be later argued as suboptimal and thus discarded? (In fact, this observation is stated twice) \n\n    **Reply**: Due to $S_{m}$, the gradient of $p$ is sparse, so at most $k$ elements of $p$ are updated at one iteration, which is equivalent to $k$-coordinate descent. Previous work [2] already demonstrates that coordinate descent results in slow convergence and suboptimal performance. In addition, we want to emphasize that the projection operator is only discarded during the back-propagation, it still works in the forward propagation. Moreover, as we indicate in Table 3, the projected and unprojected gradients are complementary in terms of performance, none of them is always better than the other, so we run sPGD twice with different backward functions in sAA for comprehensive evaluation.\n\n6. It is unclear where the projection onto the binary set Sm is used in gp and why it is used in tandem with gp-tilde if gp exhibits both non-convergence and gradient explosions\n\n    **Reply**: This question is already addressed in the explanations above."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3339/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700659480214,
                "cdate": 1700659480214,
                "tmdate": 1700659480214,
                "mdate": 1700659480214,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PMak1UVKvl",
                "forum": "BtmB8WrPSp",
                "replyto": "o8E6wBzxWT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3339/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3339/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal [2/3]"
                    },
                    "comment": {
                        "value": "7. There are many terms that are co-dependent with other terms throughout 4.1--it is challenging to understand precisely what are the main ingredients of Sparse-PGD, why they matter, and what decisions influenced their design.\n\n    **Reply**: The main idea of our method is to decompose a $l_0$ bounded perturbation $\\delta$ into a magnitude tensor $p$ and a sparsity mask $m$. \n\n    - Since $p$ is only constrained in the image box $[0, 1]^{h\\times w\\times c}$ which is similar to the $l_\\infty$ constraint, we update it using $l_\\infty$-PGD.\n    - Updating a sparse and discrete $m$ is challenging in optimization, we update its continuous alternative $\\widetilde{m}$ and then projecting $\\widetilde{m}$ to obtain $m$.\n    - Due to the binary projection onto $S_{m}$, the update of $p$ is equivalent to $k$-coordinate descent, which leads to slow convergence and suboptimal performance. Inspired by training pruned neural networks and lottery ticket hypothesis [3-6], we discard the projection $S_{m}$ when calculating the gradient of $p$ to ensure every element of $p$ can be updated. However, in the experiments, we found the unprojected and projected gradients are complementary in terms of performance.\n    - Random reinitialization is proposed to prevent $m$ from getting trapped into local maxima, because $m$ changes only when the relative magnitude ordering of the continuous alternative $\\widetilde{m}$ changes. In other words, slight changes in $\\widetilde{m}$ usually mean no change in $m$.\n\n8. A distortion vs accuracy curve should be plotted, so that we can understand the performance curves of sAA against baselines. Reporting the final results at a fixed norm boundary is not readily indicative of attack performance, given that are are many values of k a defender would consider to be \"adversarial\"\n\n    **Reply**: Thank you for pointing this out, we added the curves of sAA in Figure 1 (b).\n\n9. When attacking against adversarially trained models, perturbations must stay within the threat model. That is, it should be made clear that, when attacking an l-infinity-based model, the l0 perturbations also do not exceed, e.g., 8/255. Otherwise, it is not clear to me what insights are to be drawn from attacking a model whose threat model is violated\n\n    **Reply**: In the previous work [7], when evaluating the robustness of an $l_\\infty$-robust model against $l_2$ attacks, they do not constrain the $l_\\infty$ norm of the $l_2$ perturbations to be $8/255$ or something else. The results in [7] also indicate that a model that is robust to a specific attack is usually not robust to another attack at all. In addition, when we validate whether a model is robust to a white-box attack, we should follow the principle that the attacker knows model architecture and model weights; training algorithm and training data; test time randomness (either the values chosen or the distribution) of the defender [8]. In the evaluation, we should not compromise the defender, so what we do is just to generate $l_0$ bounded perturbations without any other restrictions. \n\n10. Mixing threat models does not seem sound. It is not clear why black-box attacks are compared to white-box attacks, etc. White-box threat models should only be compared to white-box attacks, and likewise for black-box attacks.\n\n    **Reply**: As shown in AutoAttack [9], they also combine white-box and black-box attacks to obtain a reliable and comprehensive evaluation of adversarial robustness, since white-box attacks sometimes have suboptimal performance due to gradient masking, but black-box attacks are immune from it because they do not rely on the gradient information. Empirically, among current adversarial attacks generating $l_0$ bounded perturbations, black-box attacks usually outperform white-box ones. For example, we see black-box sparse-RS outperforms white-box PGD$_0$ in many cases, depending on the evaluated models. In summary, we need to combine both white-box and black-box attacks to reliably and comprehensively evaluate the robustness, because they are complimentary.\n\n11. Attack configuration does not seem fair - It is not clear to me why Sparse-RS is included within sAA when it is used verbatim from prior work. So that readers can understand the core contributions of this work, comparisons against baselines should only be evaluated against the introduced attacks. Moreover, the JSMA (Papernot, 2016) is one of the first l0-based attacks to be introduced in the literature. It is unclear to me why this not compared against in this work.\n\n    **Reply**: Thanks for pointing this out, we have cited it in the paper. Although JSMA is one of the first $l_0$ attacks, its code is not publicly available, making it difficult for reproduction. In addition, JSMA is not included in the existing works we use as baselines either. Considering these two points, we do not include it in the comparison."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3339/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700659507994,
                "cdate": 1700659507994,
                "tmdate": 1700660470230,
                "mdate": 1700660470230,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cnxjJunaGT",
            "forum": "BtmB8WrPSp",
            "replyto": "BtmB8WrPSp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3339/Reviewer_DbyG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3339/Reviewer_DbyG"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a variant of PGD for $\\ell_0$-bounded adversarial perturbations, named Sparse-PGD (sPGD), which jointly optimizes a dense perturbation and a sparsity mask. Then, sPGD, on different loss functions and with two alternative formulations, is used to form, together with an existing black-box attack, Sparse-AutoAttack (sAA), which aims at extending the AutoAttack to the $\\ell_0$-threat model. In the experiments on CIFAR-10 and CIFAR-100, leveraging its multiple components, sAA improves upon the robustness evaluation of existing attacks. Finally, sPGD is used in adversarial training to achieve SOTA $\\ell_0$-robustness."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Adapting PGD to optimize $\\ell_0$-bounded attacks is a challenging task, and sPGD is shown to often outperform existing attacks, especially white-box ones. Moreover it can be integrated into the adversarial training framework.\n\n- Extending AA to the $\\ell_0$-threat model would be important, and sPGD might be a promising step in such direction."
                },
                "weaknesses": {
                    "value": "- While sAA seems effective (Table 1), there are some concerns in my opinion: first, according to Fig. 1a, the attacks notably benefit from more iterations. In particular, Sparse-RS shows significant improvements between 3k and 10k iterations for all models, which means that the results reported in Table 1 might be suboptimal. Second, in Tables 6, 7 and 8, CS alone appears to be better than sAA on the models robust to $\\ell_0$-attacks: while CS is evaluated on a subset of points only, an improvement of more than 3% (Table 6) seems significant to hint to the fact that, even on the full test set, the results of sAA might be improved. Finally, in most cases the robust accuracy of the best individual attack (either RS or sPGD) is quite higher (2-3%) than their worst-case, i.e. sAA, which suggests that each attack is suboptimal.\n\n- The budget of iterations of the attacks is not justified: looking at Fig. 1a it seems that more iterations would significantly improve the results, especially for RS. If I understand it correctly, sPGD is used for 20 runs ({1 CE, 9 targeted CE} x {projected, unprojected}) each of 300 iterations, for total 6k iterations (each consisting in one forward and one backward pass of the network). However, only 3k queries (forward pass only) are used for RS, which seems unbalanced given that RS provides better results for $\\ell_\\infty$- and (especially) $\\ell_0$-adversarially trained models.\n\n- The claim that no prior works proposed adversarial training for the $\\ell_0$-threat model is imprecise, see e.g. [Croce & Hein (2019)](https://openaccess.thecvf.com/content_ICCV_2019/papers/Croce_Sparse_and_Imperceivable_Adversarial_Attacks_ICCV_2019_paper.pdf). Moreover, the cost of using 100 iterations of sPGD in adversarial training seem very large. Finally, the sAT and sTRADES would need to be added to Fig. 1a, to see how the effect of more queries in RS on the achieved accuracy (see previous points)."
                },
                "questions": {
                    "value": "The main concerns are detailed above. As minor point, it would be interesting to have some evaluation on ImageNet models.\n\nOverall, I like the idea of extending AA to the $\\ell_0$-threat model, but the current results do not convincingly support how the paper proposes to build sAA (e.g. how significantly would the results improve with 2x iterations to every attack, of 4x to RS?). Similarly, the effectiveness of adversarial training with sPGD should be tested more thoroughly."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3339/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698855269998,
            "cdate": 1698855269998,
            "tmdate": 1699636283186,
            "mdate": 1699636283186,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OAttJ73xUH",
                "forum": "BtmB8WrPSp",
                "replyto": "cnxjJunaGT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3339/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3339/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal [1/2]"
                    },
                    "comment": {
                        "value": "Dear reviewer DbyG,\n\nThanks for your insightful and constructive comments. In response to your questions, we offer the following point-to-point answers.\n\nFor problems mentioned in Weaknesses:\n\n1. \\>\\>\\> While sAA seems effective (Table 1), there are some concerns in my opinion: first, according to Fig. 1a, the attacks notably benefit from more iterations. In particular, Sparse-RS shows significant improvements between 3k and 10k iterations for all models, which means that the results reported in Table 1 might be suboptimal. Second, in Tables 6, 7 and 8, CS alone appears to be better than sAA on the models robust to $l_0$-attacks: while CS is evaluated on a subset of points only, an improvement of more than 3\\% (Table 6) seems significant to hint to the fact that, even on the full test set, the results of sAA might be improved. Finally, in most cases the robust accuracy of the best individual attack (either RS or sPGD) is quite higher (2-3\\%) than their worst-case, i.e. sAA, which suggests that each attack is suboptimal.\n\n    **Response**: First, Sparse-RS indeed has performance improvement between 3k and 10k iterations for all models. However, as illustrated in Figure 1 (a), in the cases where our method outperforms Sparse-RS at 3k iterations, it is still substantially better than Sparse-RS when we use more iterations. That is to say, setting the number of iterations to 3k does not affect the performance superiority of our method. However, we are happy to present the results of Sparse-RS with 10k iterations and compare them with our methods. Due to the time constraint, we only run the experiments on a subset of models as shown in Table 1. It is clear that sAA still performs the best in all models except in $\\ell_{\\infty}$-adv model and we acknowledge this as one weakness of our method and will try to improve it in future work. As for the second question, we would like to emphasize that our $l_0$-robust models are just baselines for future work. The major motivation of $l_0$-robust models is to demonstrate the efficiency of our method so that we can utilize it for adversarial training. Due to the challenges of optimization in an $l_0$ bounded space, we notice that the $l_0$-robust models are more vulnerable to black-box attacks, especially sparse-RS, although these models still obtain the best robustness when we consider all attack methods. In Tables 6, 7 and 8, the reason why CS is better than sAA on the models robust to $l_0$-attacks can be attributed to that Sparse-RS underperforms CS on $l_0$-robust models. Despite that, due to the very high complexity of CS (10x slower), we do not adopt it as the black-attack here. We believe this issue can be solved if we have a better adversarial training methods to mitigate the gradient masking problem of the models obtained, which we leave it as future work.\n\n    ***Table 1: Robust accuracy of selected models on Sparse RS where the sparsity level $k=20$. We record the performance of RS in both 3k and 10k iterations and their changes.***\n\n    |Model | Network | RS  | **sPGD$_{\\mathrm{CE}}$** |**sPGD$_{\\mathrm{CE+T}}$** | **sAA**|\n    |----|----|----|----|----|----|\n    |Vanilla | RN-18 | 0.0 $\\to$ 0.0 | 0.0 |0.0 | **0.0**|  \n    |$l_\\infty$-adv. trained, $\\epsilon=8/255$|\n    | PORT | RN-18 |14.6 $\\to$ **9.7**|20.9|16.1 | 10.8|\n    |DKL| WRN-28 | 11.0 $\\to$ **7.0** | 22.5|16.6| 9.6|\n    |$l_2$-adv. trained, $\\epsilon=0.5$|\n    |HAT | PRN-18 |20.5 $\\to$ 13.9 | 13.2 | 10.0 | **9.3**|\n    |DM| WRN-28 | 23.4 $\\to$ 16.1 |20.9| 15.5| **14.1**|\n    |$l_1$-adv. trained, $\\epsilon=12$|\n    |$l_1$-APGD | PRN-18 |33.1 $\\to$ 25.8 |  24.0| 20.0|**19.5**|\n    |$l_0$-adv. trained, $k=20$|\n    |**sTRADES** | PRN-18 |52.1 $\\to$ **41.8** |79.5|77.9 |52.0 |\n\n2.  \\>\\>\\> The budget of iterations of the attacks is not justified: looking at Fig. 1a it seems that more iterations would significantly improve the results, especially for RS. If I understand it correctly, sPGD is used for 20 runs ({1 CE, 9 targeted CE} x {projected, unprojected}) each of 300 iterations, for total 6k iterations (each consisting in one forward and one backward pass of the network). However, only 3k queries (forward pass only) are used for RS, which seems unbalanced given that RS provides better results for $l_0$- and (especially) $l_0$-adversarially trained models.\n\n    ***Response***: We need to clarify that the sPGD with 20 runs ({1 CE, 9 targeted CE} x {projected, unprojected}) is only adopted in sAA. We do not compare the performance of Sparse-RS with sPGD and its variants. By contrast, in Figure 1 (a), we evaluate sPGD with unprojected gradient with different numbers of iterations. The robust accuracies of Sparse-RS and sPGD at the same x-axis value are obtained with the same number of iterations. Therefore, we think the comparison is fair here."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3339/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700657817035,
                "cdate": 1700657817035,
                "tmdate": 1700657817035,
                "mdate": 1700657817035,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RmTl0Uo13v",
                "forum": "BtmB8WrPSp",
                "replyto": "VU1gxMFOee",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3339/Reviewer_DbyG"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3339/Reviewer_DbyG"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for the response and additional experiments.\n\nThe new results show that Sparse-RS with 10k iterations is better than sAA (which even contains Sparse-RS with 3k iterations) in 3/6 cases (all attacks are equally good on the vanilla model). Also, the improvement from 3k to 10k iterations is significant on all classifiers, and it's not clear how the results would evolve with even more queries. The point is that it's not clear that sAA consistently provides better performance than the existing methods, and the budget given to each of the attacks is not sufficiently to achieve good performance. This is also supported by CS sometimes being more effective than sAA (I don't think that the fact that CS is computationally expensive and can't scale to large models is relevant, since it just shows that the evaluation of sAA might be suboptimal). Moreover, the largest drop in robust accuracy from using more iterations happens on the sTRADES model, and then it's not clear how effective the proposed method is for adversarial training.\n\nOverall, the rebuttal doesn't solve the main concerns mentioned in the initial review."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3339/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700662923258,
                "cdate": 1700662923258,
                "tmdate": 1700662923258,
                "mdate": 1700662923258,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "SlZOTWkEe2",
            "forum": "BtmB8WrPSp",
            "replyto": "BtmB8WrPSp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3339/Reviewer_Ngmx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3339/Reviewer_Ngmx"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a \ufeff\ufeffeffective and efficient attack called \ufeffsparse-PGD (sPGD) to generate sparse adversarial perturbations bounded by l_{0} norm, which achieves better performance with \ufeffa small number of iterations. Sparse-AutoAttack (sAA) is presented\ufeff, which is the ensemble of the white-box sPGD and another black-box sparse attack, for reliable robustness evaluation against l_{0} bounded perturbations. \ufeffFurthermore, adversarial training is conducted against l_{0} bounded sparse perturbations. The model trained with the proposed attack is superior to other \ufeffsparse attacks regarding robustness."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ The attacks are evaluated under different norms and limited iterations for fair comparison.\n+ The white-box and black-box are combined \ufefffor comprehensive robustness evaluation.\n+ The impacts of \ufeff\ufeffIteration Number and Sparsity Level are considered and analyzed."
                },
                "weaknesses": {
                    "value": "- Following \ufeffSparse Adversarial and Interpretable Attack Framework (SAIF) [1], which adopts \ufeffa magnitude tensor and sparsity mask same as this paper, the authors further \ufeffdiscard the projection to the binary set when calculating the gradient and use the unprojected gradient to update \ufeffthe magnitude tensor p. \ufeffSparse-AutoAttack (sAA) part has extended the work of \ufeffAutoAttack (AA) [2,3], and the reason for discarding \ufeffthe adaptive step size, momentum and difference of logits ratio (DLR) loss function should be further explained clearly. The paper appears to offer limited new perspectives on the attack process and lacks a notable degree of technical innovation.\n- The authors claim that \u201c\ufeffWe are the first to conduct adversarial training against l_{0} bounded perturbations.\u201d However, related work had also conducted similar experiments [4].\n- This paper has emphasized the contribution of \ufeffcomputational complexity and efficiency but lacks corresponding analysis for \ufeffcomputational complexity and query budgets for comparison.\n- In Table 1 in experimental part, \ufeffRS attack outperforms sPGD_{CE+T} for l_{\u221e} models while more analysis is required.\n- The performance analysis in Subsection 5.1 is not well-organized for clarity.\n- Many parameters in this paper need to be pre-defined. For example, \u2018the current sparsity mask remains unchanged for three consecutive iterations, the continuous alternative fm will be randomly reinitialized for better exploration. \u2018 Why three consecutive iterations? Will choosing a different number affect the results?  What is \\alpha and \\beta? Will \\alpha and \\beta affect the value of \u2018three iterations\u2019? Also for a small \\lambda, it is unclear about how small the \\lambda should be.\n- How do you set up the budget for each attack method to compute the robust accuracy so the comparison is fair?\n\nReferences\n\u00a0\n[1] \ufeffTooba Imtiaz, Morgan Kohler, Jared Miller, Zifeng Wang, Mario Sznaier, Octavia Camps, and Jennifer Dy. Saif: Sparse adversarial and interpretable attack framework. arXiv preprint arXiv:2212.07495, 2022.\n[2] \ufeffFrancesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an en- semble of diverse parameter-free attacks. In International conference on machine learning, pp. 2206\u20132216. PMLR, 2020.\n[3] \ufeffFrancesco Croce and Matthias Hein. Mind the box: l_1-apgd for sparse adversarial attacks on image classifiers. In International Conference on Machine Learning, pp. 2201\u20132211. PMLR, 2021.\n[4] \ufeffFrancesco Croce and Matthias Hein. Sparse and imperceivable adversarial attacks. In\u00a0Proceedings of the IEEE/CVF international conference on computer vision. 2019"
                },
                "questions": {
                    "value": "Pls see the Section Weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3339/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699081600506,
            "cdate": 1699081600506,
            "tmdate": 1699636283113,
            "mdate": 1699636283113,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ykWc7hm1w2",
                "forum": "BtmB8WrPSp",
                "replyto": "SlZOTWkEe2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3339/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3339/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal [1/3]"
                    },
                    "comment": {
                        "value": "Dear reviewer Ngmx,\n\nThanks for your insightful and constructive comments. In response to your questions, we offer the following point-to-point answers:\n\n1. \\>\\>\\> Following Sparse Adversarial and Interpretable Attack Framework (SAIF) [1], which adopts a magnitude tensor and sparsity mask same as this paper, the authors further discard the projection to the binary set when calculating the gradient and use the unprojected gradient to update the magnitude tensor p. Sparse-AutoAttack (sAA) part has extended the work of AutoAttack (AA) [2,3], and the reason for discarding the adaptive step size, momentum and difference of logits ratio (DLR) loss function should be further explained clearly. The paper appears to offer limited new perspectives on the attack process and lacks a notable degree of technical innovation.\n\n    **Response**:  For the attack process, though we adopt a similar decomposition method to the perturbation to SAIF [1], **our attack design is significantly different from the previous work in two perspectives:** \n    1. Instead of using an $\\ell_1$ convex surrogate for the $l_0$ norm constraint of the sparsity mask as done in SAIF, we introduce a continuous mask $\\widetilde{m}$ and project it back to the feasible set. To prevent the magnitude of $\\widetilde{m}$ from being too big, we add a sigmoid function before projecting $\\widetilde{m}$ to the binary mask $m$. This **new but simple** strategy has proven to be effective as shown in the ablation studies (Table 2, Sec 5.3) where we observed a huge robust accuracy drop from 49.5\\% to 22.2\\%. Moreover, we further improve the exploration capability for the mask $m$ by a random reinitialization mechanism, which is shown to improve the performance in the ablation study. This design alleviates the problem of local convergence, which is reported in SAIF.\n    2. The ablation studies (Table 2, Sec 5.3) also indicated naively decomposing the perturbation by $\\delta = p \\odot m$ can **deteriorate the performance**, which further corroborates the effectiveness of our attack designs. \n\n    In terms of backward function for updating $p$, we argue that the original gradient $g_{p} = \\nabla_{\\delta} L(\\theta, x + \\delta) \\odot m$ is sparse due to the sparsity of the mask $m$. **The sparse update will result in sub-optimal performance** similar to coordinate descent when there are limited iterations. To solve this problem, we adopt a novel unprojected gradient $g_{p} = \\nabla_{\\delta} L(\\theta, x + \\delta) \\odot \\sigma(\\widetilde{m})$ inspired by network pruning and lottery ticket hypothesis [6 - 9] and combine it with the original sparse gradient to achieve a better trade-off on exploration and exploitation. \n   \n2. \\>\\>\\> The authors claim that \u201cWe are the first to conduct adversarial training against $l_{0}$ bounded perturbations.\u201d However, related work had also conducted similar experiments [4]. \n\n   **Response**: Thank you for pointing this out. We will modify this claim in the paper and add the results of using PGD$_0$ as adversarial training. The results are reported in Table 1, Sec 1.2.1 of General Response, and we have also included them in the paper. We observe that the model trained with adversarial samples generated by PGD$_0$ can be easily beaten by our proposed attack sPGD, while the model trained with sPGD-generated adversarial samples is still robust against PGD$_0$, so we do not include it in other experiments.\n    \n3. \\>\\>\\> This paper has emphasized the contribution of computational complexity and efficiency but lacks corresponding analysis for computational complexity and query budgets for comparison.\n  \n    **Response**:  In Section 5.2, we study the efficiency by comparing the performance of Sparse-RS with sPGD_CE (Figure 1(a)) and sPGD_CE+T (Figure 2) under different number of iterations. Both figures show that sPGD achieves significantly better performance than RS attack under limited iterations. That is to say, Sparse-RS requires a much larger query budget to achieve comparable performance. \n\n\n4. \\>\\>\\> In Table 1 in experimental part, RS attack outperforms sPGD_CE+T for $l_{\\infty}$ models while more analysis is required.\n\n    **Response**: Thanks for pointing this out. For attacking $l_\\infty$ models, we agree that sPGD is outperformed by RS when there are enough iterations, which means sPGD suffers from gradient masking to some degree in this case. This is also why we included the black-box RS attack in sAA to comprehensively evaluate the robustness. Further mitigating the gradient masking effect when sPGD is applied to some specific models will be our future work."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3339/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700656562935,
                "cdate": 1700656562935,
                "tmdate": 1700656707810,
                "mdate": 1700656707810,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oYu4MgX0iR",
                "forum": "BtmB8WrPSp",
                "replyto": "SlZOTWkEe2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3339/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3339/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal [3/3]"
                    },
                    "comment": {
                        "value": "References: \n> [1] Tooba Imtiaz, Morgan Kohler, Jared Miller, Zifeng Wang, Mario Sznaier, Octavia Camps, and Jennifer Dy. Saif: Sparse adversarial and interpretable attack framework. arXiv preprint arXiv:2212.07495, 2022.\n\n> [2] Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks. In International conference on machine learning, pp. 2206\u20132216. PMLR, 2020.\n\n>[3] Francesco Croce and Matthias Hein. Mind the box: $l_1$-apgd for sparse adversarial attacks on image classifiers. In International Conference on Machine Learning, pp. 2201\u20132211. PMLR, 2021.\n\n>[4] Francesco Croce and Matthias Hein. Sparse and imperceivable adversarial attacks. In Proceedings of the IEEE/CVF international conference on computer vision. 2019.\n\n>[5] Croce, Francesco, and Matthias Hein. \"Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks.\" International conference on machine learning. PMLR, 2020.\n\n>[6] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. In International Conference on Learning Representations, 2019. \n\n>[7] Vivek Ramanujan, Mitchell Wortsman, Aniruddha Kembhavi, Ali Farhadi, and Mohammad Rastegari. What\u2019s hidden in a randomly weighted neural network? In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11893\u201311902, 2020.\n\n>[8] Yonggan Fu, Qixuan Yu, Yang Zhang, Shang Wu, Xu Ouyang, David Cox, and Yingyan Lin. Drawing robust scratch tickets: Subnetworks with inborn robustness are found within randomly initialized networks. Advances in Neural Information Processing Systems, 34:13059\u201313072, 2021.\n\n>[9] Chen Liu, Ziqi Zhao, Sabine S\u00fcsstrunk, and Mathieu Salzmann. Robust binary models by pruning randomly-initialized networks. Advances in Neural Information Processing Systems, 35:492\u2013506, 2022"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3339/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700656645276,
                "cdate": 1700656645276,
                "tmdate": 1700656678533,
                "mdate": 1700656678533,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]