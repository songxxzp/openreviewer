[
    {
        "title": "Sparse-Guard: Sparse Coding-Based Defense against Model Inversion Attacks"
    },
    {
        "review": {
            "id": "a9FLrw4g5w",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6812/Reviewer_ETXH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6812/Reviewer_ETXH"
            ],
            "forum": "kDoKXaucJV",
            "replyto": "kDoKXaucJV",
            "content": {
                "summary": {
                    "value": "The paper investigates the effectiveness of sparse coding-based network architectures as a defense against model inversion attacks (MIAs). More specifically, the approach uses sparse-coded layers in the beginning of a network to control and limit the amount of private information contained in those layers' output features. As a consequence, black-box model inversion attacks in a split-network setting should no longer be able to reconstruct the original (private) input features based on the intermediate outputs of the model. When compared to existing defense strategies (adding Gaussian noise to intermediate activations and augmenting training data with GAN-generated images), the proposed defense outperforms those strategies on the MNIST and Fashion MNIST datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Applying sparse coding-based networks to limit the information contained in the features of later layers in a network is an interesting and novel research direction, not only in the model inversion setting. I think the same approach might even be used in other privacy settings, e.g., membership inference attacks. I do not expect the paper to investigate these settings, just want to highlight possible extensions.\n- The results on MNIST and Fashion MNIST state a clear improvement above existing methods and promise better training-time efficiency. This is also underlined by the qualitative samples depicted in the paper.\n- The approach is well motivated, and the paper is overall well written."
                },
                "weaknesses": {
                    "value": "- The evaluation is rather limited since it only conducts experiments on MNIST and Fashion MNIST, both datasets which are easy to fit by a network due to the overall low sample variance. Finding meaningful shared features in the sparse code layers is rather easy for the model. Also, the samples contain no private information at all. The evaluation should also contain more complex dataset evaluations, e.g., the common CelebA dataset, to prove that the approach is also usable within more complex tasks. Also, repeating the experiments with different seeds to provide a standard deviation of the results would make the evaluation more reliable.\n- The overall evaluation setting seems a bit strange. I understand the split-network setting and that reconstructing inputs given only the intermediate activations indeed can be a privacy breach. But why should the adversary have access to the activations of the training samples? I think a more realistic evaluation should consider unseen (test) samples and then try to reconstruct those given the intermediate activations.\n- Moreover, I think the approach should also be evaluated on common MIAs that utilize GANs to reconstruct training samples based only on the network's weights, e.g., [1, 2, 3]. Otherwise, the defense mechanisms should be positioned only for split-network (and federated learning) settings. Also, the approach should be compared to related information bottleneck defense approaches [4,5].\n- I think the overall technical contribution is rather low since the approach seems to be simply re-using the sparse coding layer framework of Rozell et al. (2008) and demonstrating that such networks can also act as a defense against MIAs. I still think the direction of the paper is interesting but the technical novelty seems limited.\n- The related work is comprehensive but mixes up different model inversion settings and approaches. For example, the [1] proposes MIAs that try to reconstruct features from specific classes by optimizing the latent vectors of a GAN. It uses the target model for guidance (and there exist much more works in this line of research, e.g., [2,3]). This is a completely different setting from the one investigated by the paper, which uses the intermediate activations of training samples to train a decoder-like model. I think a clearer separation between different types of MIAs would make the related work part stronger. Also, mixing works investigating the memorization of training samples in LLMs with vision-based inversion attacks might be confusing to the reader.\n\nSmall remarks:\n- Table captions should be above the table (Table 1)\n- The space after Table 2 should be increased (and manipulating the spaces might even run counter to the official guideline!)\n\n[1] Zhang et al., The Secret Revealer: Generative Model-Inversion Attacks Against Deep Neural Networks, CVPR 2020\n[2] Chen et al., Knowledge-Enriched Distributional Model Inversion Attacks, ICCV 2021\n[3] Struppek et al., Plug & Play Attacks: Towards Robust and Flexible Model Inversion Attacks, ICML 2022\n[4] Wang et al., Improving robustness to model inversion attacks via mutual information regularization, AAAI 2021\n[5] Peng et al., Bilateral Dependency Optimization: Defending Against Model-inversion Attacks, KDD 2022"
                },
                "questions": {
                    "value": "- How much longer does it take to train a network using the Sparse-Guard architecture compared to a model without it?\n- Why is the FID metric valid to evaluate the privacy leakage? Generally, we are interested in how well a single sample can be reconstructed and less about recovering the overall feature distribution."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6812/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6812/Reviewer_ETXH",
                        "ICLR.cc/2024/Conference/Submission6812/Senior_Area_Chairs"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "Note: Not an ethical concern, just want to point out that I'm not a first time reviewer for ICLR (the checkbox cannot be unchecked by me)"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6812/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697448034240,
            "cdate": 1697448034240,
            "tmdate": 1700464165587,
            "mdate": 1700464165587,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zHrxl61D87",
                "forum": "kDoKXaucJV",
                "replyto": "a9FLrw4g5w",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6812/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6812/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ETXH *part 1 of 2*"
                    },
                    "comment": {
                        "value": "**Thank you for your detailed review and your comments that the paper offers an \"interesting and novel research direction\" with many possible extensions. Thank you also for your comments that it is \"well motivated\" and \"well written.\"**\n\n***We believe we address your remaining reservations below, and we would appreciate it if you would improve your score in light of our response. Specifically:***\n\n***Re: limited experiments due to only 2 datasets (MNIST and Fashion MNIST) in datasets:***\n\n- We have now **doubled the datasets to 4 including the high-res CIFAR-10 dataset** and a dataset of medical images that represents a realistic attack setting. We also add a new attack and multiple new defenses as described below. In sum, we have quadrupled the number of experiments. ***Across all (quadrupled) experiments, Sparse-Guard now outperforms benchmarks by a factor of up to 704. It also has the best PSNR (the most important metric) across ~every single experiment~.*** In one single experiment, it loses to the Wang et al. defense in terms of SSIM by a factor of 0.001, while significantly outperforming it in terms of PSNR and FID.\n\n***Re: evaluation setting is strange because we allow the adversary access to training samples to reconstruct instead of unseen test samples:***\n\n-  This aspect of our setup is identical to the standard setups in the benchmarks we consider (Titcomb et al. 2021, Gong et al., 2023). Per Section 2, our goal is to consider settings that capture \u2018worst-case\u2019 black-box attacks with a powerful attacker. Specifically, We suppose that the attacker builds the inverted model using a held-out set of training data. Then, we suppose that the attacker has access to the *training data*'s intermediate network outputs, which he can submit to his inverted model to reconstruct training examples. Of course, you're right that this setting supposes that the attacker has obtained some leaked data. The fact that Sparse-Guard is robust in this setting suggests that it is a good defense even in the `worst-case' with a powerful attacker. In real-world settings, an attacker may have to approximate the training data's intermediate outputs, but that would make an attack more difficult (and also allow for more subjective decisions in terms of how to set this up fairly for experiments).\n\n***Re: evaluating on new MIA's that use GAN's, and evaluating on new defenses:***\n\n- Thank you. We **added the new Plug and Play attack** that you and the other reviewers suggested (the newest of the attacks you suggest), and we also added **two new defenses**: the Wang et al. defense you suggest and the ***new differential-privacy guaranteeing DP-SGD defense of Hayes et al.** that is currently under development at Google DeepMind and Meta AI, and that is currently the only defense with provable guarantees for model inversion attacks. Appendix tables 8-13 show all of these new results. \n- **We run all experiments and all defenses on all 4 datasets (instead of 2) and then re-run them under the Plug-and-play attack.** This new set of experiments on *[4 datasets, 6 benchmarks, 2 attacks, 3 settings]* is now **significantly more comprehensive than other recent papers** including the Plug and Play paper (which evaluates 3 datasets and 3 benchmarks).\n- ***Across all (quadrupled) experiments, Sparse-Guard now outperforms benchmarks by a factor of up to 704. It also has the best PSNR (the most important metric) across ~every single experiment~.*** In one single experiment, it loses to the Wang et al. defense in terms of SSIM by a factor of 0.001, while significantly outperforming it in terms of PSNR and FID.\n\n***Re: whether the technical contribution is rather low since the approach seems to be simply re-using the sparse coding layer framework of Rozell et al. (2008):***\n\n- We are using the new (2022) extension of Rozell sparse coding to convolutional networks, which modifies the the Rozell  2008 update rule (see [1]) to extend it to the convolutional setting. However, Rozell and subsequent sparse-coding architectures including [1] consider a single sparse coded layer. **To our knowledge, our paper is the first time an architecture with intermittent sparse layers has been proposed.** We compare the 'standard' new convolutional architecture of [1] with our intermittent convolutional architecture (the standard one is called `Sparse-Standard' in all of our tables), and we find that our intermittent architecture significantly outperforms it. We thank you for raising this issue and we have revised our paper to clarify (1) the fact that we are using the new convolution sparse-coding extension rather than the 2008 approach as well as (2) the novelty of our architecture compared to a standard sparse-coding architecture.\n     \n        [1] Teti et al., Lateral Competition Improves Robustness Against Corruption and Attack (ICML, 2022)"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6812/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700450872397,
                "cdate": 1700450872397,
                "tmdate": 1700450901871,
                "mdate": 1700450901871,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XT0tvq9BG2",
                "forum": "kDoKXaucJV",
                "replyto": "a9FLrw4g5w",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6812/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6812/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ETXH *part 2 of 2*"
                    },
                    "comment": {
                        "value": "***Re: the related work is comprehensive but....\" [we should clarify settings/approaches and aspects like images vs. LLM's:***\n\n- Thank you for this suggestion -- we have revised this section.\n\n***Re: How much longer does it take to train a network using the Sparse-Guard architecture compared to a model without it?:***\n\n- Sparse-Guard is equal or faster than benchmarks on all experiments except the case of MNIST under the Plug-and-Play attack, so we include a table of the timings for this 'worst-case' experiment in Appendix table 14. In that 'worst-case' experiment, Sparse-Guard offers a significantly better defense than all benchmarks, and it is still faster than Gong et al.'s GAN-based defense, but its 11% slower than Wang et al. \n\n***Re: Why is the FID metric valid?***\n\n- FID is one of the standard metrics for the evaluation of inversion attacks, and it is used in (for example) the Plug and Play paper you suggest. It computes whether the reconstructed images are 'overall-distributionally-similar' to the originals by computing their distributional distance, rather than e.g. pixel-by-pixel distance. It also captures an element of how a reconstructed image 'generally looks like' a training image: in other settings, FID is also often used to evaluate whether fake images generated by GAN's 'look like' they could be real.\n\n***Re: space after Table 2 should be increased:***\n\n- Thank you -- we have fixed this. In general, we noticed many more spacing issues with this year's ICLR template compared to previous years (and also many more than ICML & NeurIPS templates). We are not sure what is causing this, but have looked through the paper to make sure there are no more compressed whitespaces."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6812/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700450928218,
                "cdate": 1700450928218,
                "tmdate": 1700450979782,
                "mdate": 1700450979782,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4iwlqCktT3",
                "forum": "kDoKXaucJV",
                "replyto": "XT0tvq9BG2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6812/Reviewer_ETXH"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6812/Reviewer_ETXH"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for answering my questions and for the additional clarifications. I think those make the paper, particularly the empirical evaluation, stronger. Likewise, I understand and agree with the authors' argument that they investigate the worst-case setting in which the adversary has access to the intermediate activations of training samples, and a powerful defense against this kind of attack will very likely also be effective in weaker settings.\n\nHowever, the method still lacks empirical proof to also work on more complex and fine-grained datasets like facial images, as well as images with higher resolutions. Also, the additional results on PPA, e.g., Tables 12 and 13, seem to be rather weak. While there is a substantial improvement in terms of FID score, the difference in PSNR and SSIM is rather small. But this might be due to the setting, which is different from the split learning setting. \n\nRegarding the FID score, I understand that it is commonly used for evaluating model inversion attacks. In settings like face recognition, it also is somewhat reasonable when the recovered distribution of facial features is close to the true distribution. However, in settings like MNIST, where all samples of one class are quite similar, I do not think that the FID score reveals much about privacy leakage. For example, an attack can be capable of extracting samples that look like training samples but those samples might not correspond to the current input sample, which the attack aims to invert. Consequently, the FID score might be low because the attack could have learned to simply recover the distribution instead of a single sample, which then results in a low SSIM/PSNR and the privacy leakage is rather low.\n\nOverall, I appreciate the author's effort in the rebuttal and think the paper was improved. However, to accept the paper, I think there should be more empirical proof on more fine-grained datasets (e.g., CelebA) and higher resolutions. Nevertheless, I will increase my score."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6812/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700464143129,
                "cdate": 1700464143129,
                "tmdate": 1700464143129,
                "mdate": 1700464143129,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ld5VkAdMds",
                "forum": "kDoKXaucJV",
                "replyto": "8MxnOIIsVF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6812/Reviewer_ETXH"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6812/Reviewer_ETXH"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors,\n\nthanks for the rebuttal, additional experiments, and clarifications. Just a small remark: Plug and Play Attacks are not black-box but white-box since the attack exploits a model's gradients to optimize the GAN's latent vectors."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6812/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700464468651,
                "cdate": 1700464468651,
                "tmdate": 1700464468651,
                "mdate": 1700464468651,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zJx0BWy8rk",
                "forum": "kDoKXaucJV",
                "replyto": "a9FLrw4g5w",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6812/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6812/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response including new CelebA results"
                    },
                    "comment": {
                        "value": "**Thank you for this update. Re: your comment** ***\"However, to accept the paper, I think there should be more empirical proof on more fine-grained datasets (e.g., CelebA)\"*:**\n\n- We have **re-uploaded our paper** with even more experiments **on the CelebA dataset** (our 5th dataset, and our 3rd added since the original submission). Please see Tables 14 and 15. We note that this CelebA dataset is significantly larger in terms of resolution and image count, so compute times for all benchmarks are significantly greater (and the re-upload deadline is just hours away). Therefore, in the interest of time, we compare Sparse-Guard to the best of the benchmarks (Wang et al.) suggested by the reviewers, rather than all benchmarks. Sparse-Guard outperforms this best defense, and we also show that our performance advantage grows greater under the Plug and Play attack. We are adding remaining benchmarks as the finish computing, but we note that they tend to perform worse than the Wang et al. benchmark that we outperform here. \n\n**We ask that you consider improving your score in light of these new experiments, and we thank you again for your many helpful comments that have helped us to make the evaluations in this paper stronger.** We strongly feel that a practically effective defense based on a completely novel research direction is of immediate interest both to other researchers and also to companies like Google and Facebook that are actively developing public products for sensitive application domains using available model inversion defenses."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6812/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700628842611,
                "cdate": 1700628842611,
                "tmdate": 1700636037289,
                "mdate": 1700636037289,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "s20KZo9FlW",
                "forum": "kDoKXaucJV",
                "replyto": "zJx0BWy8rk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6812/Reviewer_ETXH"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6812/Reviewer_ETXH"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors,\n\nthank you for adding more experiments with the CelebA dataset. Still, the experimental results do not convince me. In Table 14, the SSIM is in all cases close to zero, i.e., there seems to be no similarity between the images. Even for Wang et al., the SSIM score achieves only 0.001. I am wondering if the attack in this setting is even working at all or if the defenses try to defend against a non-working type of attack.\n\nAlso, I still do not know how the Plug and Play Attack is implemented in this setting since the underlying attack algorithm tries to recover characteristic features of samples and not original training samples. Which GAN has been used? How has the attack been adjusted to the split-learning setting? How was the target model trained (attribute classification vs. identity prediction)? Also, the metrics measured in the Plug and Play Attack setting are usually different ones (e.g., attack accuracy, FaceNet distance, etc.). If the underlying distribution of the GAN is substantially different from the training data, e.g., FFHQ StyleGAN vs. CelebA training data, it is no surprise that the defenses work quite well under the used similarity metrics (SSIM, PSNR) since the StyleGAN is not able to generate images in the same style as the cropped CelebA data.\n\nI think the paper needs a more revised version to answer these questions and update the experiments (make them more clear, in higher resolution, etc.). Therefore, I will keep my current rating. I am not saying the research is not interesting (it indeed is), but the paper is, at least in my eyes, not sufficient to recommend accepting the paper."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6812/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700638517708,
                "cdate": 1700638517708,
                "tmdate": 1700638517708,
                "mdate": 1700638517708,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mGxCgZGZ0r",
            "forum": "kDoKXaucJV",
            "replyto": "kDoKXaucJV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6812/Reviewer_jF3v"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6812/Reviewer_jF3v"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes SPARSE-GUARD, a neural network architecture that leverages sparse coding to defend against model inversion attacks. It inserts sparse coding layers between dense layers which help remove unnecessary private information about the training data. Through extensive experiments on MNIST and Fashion MNIST datasets, the paper shows SPARSE-GUARD provides superior defense compared to state-of-the-art techniques like data augmentation, noise injection and standard sparse coding, while maintaining high classification accuracy."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The key strengths are the novel approach of using sparse coding for privacy protection, and code is available for reproducibility."
                },
                "weaknesses": {
                    "value": "1. The attacks used in this study do not represent state-of-the-art techniques [1, 2, 3], and the baseline defense methods employed also fall short of the current state-of-the-art [4].\n2. The study relies solely on synthetic datasets like MNIST and FMNIST, lacking the inclusion of real-world datasets, such as facial recognition data, which could enhance the practical relevance of the findings.\n\n[1] Knowledge-Enriched Distributional Model Inversion Attacks, Chen et al., ICCV 2021\n\n[2] Plug & Play Attacks: Towards Robust and Flexible Model Inversion Attacks, Struppek et al., ICML 2022\n\n[3] Re-Thinking Model Inversion Attacks Against Deep Neural Networks, Nguyen et al., CVPR 2023\n\n[4] Bilateral Dependency Optimization: Defending Against Model-inversion Attacks, Peng et al. KDD 2022"
                },
                "questions": {
                    "value": "See weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6812/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698569967586,
            "cdate": 1698569967586,
            "tmdate": 1699636787725,
            "mdate": 1699636787725,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YSjiDskjed",
                "forum": "kDoKXaucJV",
                "replyto": "mGxCgZGZ0r",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6812/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6812/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer jF3v"
                    },
                    "comment": {
                        "value": "**Thank you for your constructive review---we believe we address your remaining reservations and would appreciate it if you would improve your score in light of our response.**\n\nThe main concern and reason for your score of 3 seems to be whether Sparse-Guard's performance advantage holds against additional defense baselines, additional new attacks, and additional datasets. We now show that it does:\n\n**Appendix Tables 10, 11, & 13 add two additional state-of-the-art benchmark *defenses*:**\n\n- We added the ***Information Regularization defense of Wang et al.*** suggested by Reviewer ETXH;\n- We also added ***very recent DP-SGD defense of Hayes et al.*** just released by Deepmind and Meta AI, which is currently the only defense with provable guarantees for model inversion attacks;\n\n**Appendix Tables 12 and 13 re-run all experiments using the *Plug-and-Play* attack you suggested**. **Sparse-Guard outperforms benchmarks by a factor of up to 6.7 under this novel attack. In particular, it offers better PSNR (the most important metric in the literature) across all combinations of datasets and defenses.\n\n**We also doubled the number of datasets in our experiments.** Specifically, **[Appendix tables 8-13]** rerun **all** defenses, attacks, and experiment settings with the original 2 datasets as well as:\n\n- The **hi-res CIFAR-10** dataset ;\n- An additional dataset of **medical images**. \n\nIn sum, we have **quadrupled our experiments**, and we now show that Sparse-Guard outperforms alternatives across multipple times more combinations of attacks, and defenses, and datasets compared to recent papers such as the Plug and Play paper you suggested.\n\n**We also thank you for your comments that our sparse-coding approach is novel**, as well as your positive comments about our public model inversion defense PyTorch codebase."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6812/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700450625860,
                "cdate": 1700450625860,
                "tmdate": 1700450625860,
                "mdate": 1700450625860,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DAII6zidh3",
                "forum": "kDoKXaucJV",
                "replyto": "YSjiDskjed",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6812/Reviewer_jF3v"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6812/Reviewer_jF3v"
                ],
                "content": {
                    "comment": {
                        "value": "1. I maintain that the data reconstruction attack in a split-network learning scenario substantially differs from typical MIAs, where adversaries use soft/hard labels to infer private training data, rather than intermediate features. Therefore, it is crucial to clearly define the specific contexts where your method is applicable. If your approach is effective in both scenarios, a detailed explanation of the experimental setup is essential for clarity.\n\n2. It would be beneficial to compare your methods with the baseline method presented in reference [4], especially using the CelebA dataset, as it represents the current state-of-the-art in defending against common MIAs."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6812/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700641436841,
                "cdate": 1700641436841,
                "tmdate": 1700641436841,
                "mdate": 1700641436841,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "AGswPnTwg2",
            "forum": "kDoKXaucJV",
            "replyto": "kDoKXaucJV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6812/Reviewer_kXvF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6812/Reviewer_kXvF"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a novel architecture (Sparse-Guard) for defense against black-box model inversion attacks. It is demonstrated to be superior against state-of-the-art data augmentation and noise-injection-based defenses."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper, overall, is well-written and organized. The idea of interweaving sparse coding layers as a means of model-inversion attack is a novelty yet to be explored. Empirical analyses have also been provided to understand the mechanism behind the Sparse-Guard defense through UMAP 2D projections of output. Having openly accessible codebase is also a plus"
                },
                "weaknesses": {
                    "value": "The paper does not do a good job at the exposition of how sparse coding is implemented. This is especially important as the implementation here seems to be *convolutional* sparse coding and differs from traditional sparse coding where matrix multiplication rather than a convolution is applied. e.g. (Bristow, Hilton, Anders Eriksson, and Simon Lucey. \"Fast convolutional sparse coding.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2013.)\n\nRozell et al. 2008 were cited for the update rule. However, in that paper, the update rule was not given for convolutional sparse coding. Either a more explicit derivation for the update rule can be given or a different citation would be relevant."
                },
                "questions": {
                    "value": "The sentence \"The learned spatiotemporal representation closest to input image X is represented by this sparse presentation R_X\" is confusing. \nWhy is it a spatiotemporal representation? Where is the temporal element, all of the inputs are static images. Should 'sparse presentation' also be sparse representation?\n\nMultiple claims in the paper is made about sparse coding \u201cremoving unnecessary private information\u201d. This claim is not really supported by any study. In fact, the empirical study concluded that the effect of sparse coding layers is an \"unclustering effect\". How the conclusion of jettisoning unnecessary information is unclear. What is considered unnecessary information in the first place? In fact, it would be interesting to see if any other algorithm that produces the same unclustering effect will provide a similar effectiveness in defense against model inversion attacks."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6812/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698807876812,
            "cdate": 1698807876812,
            "tmdate": 1699636787612,
            "mdate": 1699636787612,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sKRCqBvacf",
                "forum": "kDoKXaucJV",
                "replyto": "AGswPnTwg2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6812/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6812/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer kXvF *part 1 of 2*"
                    },
                    "comment": {
                        "value": "**We thank you for your positive review. We appreciate your suggestions re: how to improve clarity, and we have updated the descriptions in our paper accordingly. We believe this addresses your reservations, and we would appreciate it if you would improve your score in light of our response.**\n\n**We thank you for:**\n\n - Your \"well-written and organized\" comment;\n - Your comment re: the novelty of our contribution;\n - Your positive comments re: our empirical UMAP explorations and our public PyTorch codebase.\n\n**Your 2 main concerns seem to be:**\n\n1.  ***Clarifying the description of our Sparse-Coding implementation:***\n\n    **Thank you for this constructive feedback---We have greatly revised *Section 3: Sparse-Guard Architecture* where we introduce sparse coding and the LCA algorithm to provide a much more clearer picture of our methods.** In particular, the update rule provided in equation 3 describes the Rozell update for the convolutional case, which was used in previous work [1,2] and validated against other non-Rozell-based convolutional sparse coding solvers (https://sporco.readthedocs.io/en/latest/examples/csc/index.html). Although the original Rozell sparse coding algorithm was not introduced in the convolutional setting, it can readily be adapted to convolutional networks because it is based on the general principle of feature-similarity-based competition between neurons, which is agnostic to different neural network architectures. As a result, the original Rozell formulation can be readily adapted to the convolutional setting by modifying just two terms in the update rule. The first term, which is represented as $\\Psi(t)$ in Equation 2 of our revised paper, is computed via a matrix multiplication between the transposed dictionary matrix and an input vector in the original Rozell formulation. In the convolutional case, the matrix multiplication becomes a convolution between the input and the convolutional dictionary. The second term that needs to be adapted for the convolutional setting is where the similarity between each feature and every other feature is computed, given by $\\Omega * \\Omega$ in our revised version. In the original Rozell formulation, this was done with a matrix multiplication between the transposed dictionary and the non-transposed dictionary. In the convolutional setting, it can be done with a convolution between the dictionary and itself. We also agree with the reviewer that the general 'spatiotemporal' description we referred to was potentially confusing in our setting so we have replaced it with a more precise description. **We thank the reviewer for pointing out the opportunity to improve our description of our sparse-coding approach.**\n     \n    -   [1] Teti et al., Lateral Competition Improves Robustness Against Corruption and Attack\n    -   [2] Kim et al., Modeling Biological Immunity to Adversarial Examples"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6812/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700450735023,
                "cdate": 1700450735023,
                "tmdate": 1700450735023,
                "mdate": 1700450735023,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tk7GhhL7HY",
                "forum": "kDoKXaucJV",
                "replyto": "AGswPnTwg2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6812/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6812/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer kXvF *part 2 of 2*"
                    },
                    "comment": {
                        "value": "**Your 2nd main concern seems to be:**\n\n2.  **Clarifying what is meant by the statement that Sparse-Guard works by \u201cremoving unnecessary private information\", and clarifying how this  statement is compatible with our empirical result that Sparse-Guard unclusters image features:**\n\n    We mean to say that the representations that are produced via sparse coding only contain information that is relevant for reconstructing a denoised (a.k.a. less detailed) version of the input, and these representations are much sparser and less detailed than those in standard models. As a result, we were attempting to point out that the attacker has much less information to use when performing model inversion attacks against Sparse-Guard compared to standard models. Put differently, a 'perfect attacker' could only ever hope to reconstruct a sparse representation of an input image under our defense *in the worse case*, meaning that even a perfect attacker could *not hope to* reconstruct many input image details that are dropped by sparse-coding. By 'unnecessary private information', we mean input image details that we may desire to keep private but that are not necessary for the task of training a high-accuracy classifier. Specifically, sparse-coding networks are known to sparsify inputs and network layers without losing significant classification accuracy.\n\n    The unclustering effect we observe in our empirical analysis is the direct result of this approach. Specifically, a standard (non-sparse) network trained on dense/noisy/detailed input images results in many partly-redundant network features that are `noisy versions of each other'. Networks with many noisy/partly redundant features are easier to attack, because an adversary can 'home in on' clusters of such features in order to 'home in on' an input image it wants to reconstruct. Our sparse-coding approach's de-noising effect tends to prevent the network from learning many partly-redundant features, resulting in the de-clustering effect we observe in Fig. 4. The resulting de-clustered features are intuitively less susceptible to gradient-based inversion attacks.\n\n***We thank you for these helpful suggestions and we ask you to also consider our vastly expanded experiments section including new datasets as well as new attacks & defenses [Appendix tables 8-14] when updating your review score.***"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6812/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700450765078,
                "cdate": 1700450765078,
                "tmdate": 1700450796897,
                "mdate": 1700450796897,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "6SodVaEJKS",
            "forum": "kDoKXaucJV",
            "replyto": "kDoKXaucJV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6812/Reviewer_CjY9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6812/Reviewer_CjY9"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to defend against model inversion attacks by inducing sparse coding into DNNs. The key design is an alternating sparse coded and dense layers that discards private information. Experiments show effective defenses on MNIST and Fashion MNIST."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The method maintains great privacy with little training computation overhead and accuracy loss\n2. A cluster-ready PyTorch codebase is provided for future study\n3. The paper is well motivated and easy to follow"
                },
                "weaknesses": {
                    "value": "The major drawback is that the experiments are only conducted on simple, low-resolution datasets. I do not think the results in small datasets convincingly validate the effectiveness of the proposed method. There exist lots of model inversion attacks that are capable of extracting high-resolution data, from CIFAR-10, CelebA, to ImageNet. Since high-resolution images are much more valuable as training data, it is the high-resolution model inversion attacks that post private threats. And an effective defense would be significant in that case.\n\n[1] MIRROR: Model Inversion for Deep Learning Network with High Fidelity\n\n[2] Plug & Play Attacks: Towards Robust and Flexible Model Inversion Attacks\n\n[3] Re-thinking Model Inversion Attacks Against Deep Neural Networks\n\n[4] Model Inversion Attacks that Exploit Confidence Information and Basic Countermeasures"
                },
                "questions": {
                    "value": "Response to rebuttal: Thanks for the strong rebuttal with great efforts! I raised my score to 5 based on experiments on CIFAR10 and Plug-and-play advantage."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6812/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6812/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6812/Reviewer_CjY9"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6812/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698819695082,
            "cdate": 1698819695082,
            "tmdate": 1700766857428,
            "mdate": 1700766857428,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vupkcDZKcb",
                "forum": "kDoKXaucJV",
                "replyto": "6SodVaEJKS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6812/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6812/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer CjY9"
                    },
                    "comment": {
                        "value": "**Thank you for your constructive review. We believe we address your remaining reservations here and would appreciate it if you would improve your score in light of our response.**\n\nThe main concern and reason for your score of 3 seems to be your statement that \"The major drawback is that the experiments are only conducted on simple, low-resolution datasets.\" We've taken your advice and:\n\n- **Re-run all experiments on CIFAR-10 as well as an additional dataset of medical images** [Appendix Tables 8-13]. **Sparse-Guard outperforms benchmarks by a factor of up to 173.9 on CIFAR-10**.\n- Re-run all experiments on all 4 datasets using the **Plug-and-Play** attack you suggested [Appendix Tables 12-13]. **Sparse-Guard outperforms benchmarks by a factor of up to 6.6 under this attack.**\n- We also added 2 more state-of-the-art defense benchmarks including the new differential-privacy guaranteeing DP-SGD defense from Google DeepMind and Meta AI [Appendix Tables 10, 11, & 13].\n\n***Across all (quadrupled) experiments, Sparse-Guard now outperforms benchmarks by a factor of up to 704. It also has the best PSNR (the most important metric) across every single experiment.*** \n\nWe absolutely agree with your conclusion that \"an effective defense would be significant\" on CIFAR-10 and similar hi-res data, and we believe we have addressed this concern.\n\nWe also thank you for your comments that the paper is \"well motivated and easy to follow\" and your positive comments about our PyTorch codebase."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6812/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700450539989,
                "cdate": 1700450539989,
                "tmdate": 1700450539989,
                "mdate": 1700450539989,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]