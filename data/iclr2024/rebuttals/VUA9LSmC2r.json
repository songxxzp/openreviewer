[
    {
        "title": "Learning Embodied Vision-Language Programming From Instruction, Exploration, and Environmental Feedback"
    },
    {
        "review": {
            "id": "m0ijYT2GKK",
            "forum": "VUA9LSmC2r",
            "replyto": "VUA9LSmC2r",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission374/Reviewer_VoZ1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission374/Reviewer_VoZ1"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose an embodied vision-language planner and programmer (Octopus) trained with reinforcement learning with environmental feedback, as well as two embodied environments that yield feedback necessary to train the aforementioned model, with data collected by GPT4. Octopus takes egocentric views and tasks specified in language, and outputs next actions and code to execute it. The method is tested on environments based on OmniGibson and GTA-V."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "I appreciate the introduction of reinforcement learning from environmental feedback, by efficiently using environmental rewards from the simulator + GPT-4. Though the approach of using code-writing LLMs to execute plans is not new, I believe applying it to embodied tasks in the proposed formulation is a nice demonstration of how to leverage foundation models in these embodied environments."
                },
                "weaknesses": {
                    "value": "W1. The data collection process relies on GPT-4, which takes as input language of systems message + environment message to output the required plan, code, and target state. This relies on the strong assumption that the systems + environment message fully captures the environment state, since the planning must be done without access to the view of the visual scene. I presume this means that the systems prompt must be elaborate, handcrafted, and task specific, such that GPT-4 can plan reasonably in the environment using possible objects at hand. Is there a robust way of designing such prompts for different/new tasks without tuning?\n\nW2. From my understanding, GPT-4 generates the target states, and whether the target states have been met is used as environmental feedback for training Octopus. It seems possible that GTP-4 will generate an incorrect or trivial target state to satisfy the language task goal, and be able to successfully reach that predicted target state, without actually achieving the task goal. Is this understanding correct? In this case, the errors from GPT-4 seem more harmful than having unsuccessful execution from GPT-4 generated code.\n\nW3. The results on Table 2 are not particularly convincing of this method\u2019s success. Octopus should indeed outperform blind LLMs that do not take visual input. TAPA outperforms/is of equal performance in 2 of the 5 tasks. The paper lacks analysis on why this is the case. Where does TAPA fail? It is also hard to compare when the vision models are different; does OVD and CLIP-ViT perform similarly in terms of capturing information from the input scene?\n \nNit: Should add a figure describing Octopus model architecture; notations in methods section are not well defined."
                },
                "questions": {
                    "value": "Q1. Is there a systematic way of generating the systems message able to generalize to new datasets? Or does it need to be hand-crafted and tuned for Omniverse & GTA & other datasets?\n\nQ2. Can you provide more analysis or experiments on where Octopus may outperform prior work?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission374/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission374/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission374/Reviewer_VoZ1"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission374/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698300744642,
            "cdate": 1698300744642,
            "tmdate": 1699635964513,
            "mdate": 1699635964513,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "b52GakMZfe",
                "forum": "VUA9LSmC2r",
                "replyto": "m0ijYT2GKK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission374/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission374/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer VoZ1(1/2)"
                    },
                    "comment": {
                        "value": "> W1. The data collection process relies on GPT-4, which takes as input language of systems message + environment message to output the required plan, code, and target state. This relies on the strong assumption that the systems + environment message fully captures the environment state, since the planning must be done without access to the view of the visual scene. I presume this means that the systems prompt must be elaborate, handcrafted, and task specific, such that GPT-4 can plan reasonably in the environment using possible objects at hand. Is there a robust way of designing such prompts for different/new tasks without tuning?\n\nWe thank the reviewer for your valuable comments. You are right, we make sure the environment message fully captures the environment state since when we design a task, we manually inspect all the necessary objects that should be in the environment message. An automatic alternative is to put everything, no matter whether necessary or not, into the environmental message. However, we tried but we found that GPT-4 will not act properly if the environmental message is too long. However, since OpenAI keeps enlarging the input length support, maybe this issue could be addressed in the future. Besides, since this work is to have an initial exploration of the vision-language programming, which remains uncharted to date, we think some handcrafted work is required, and support our future work of making the training pipeline more flexible, such as iterative training, i.e., with an Octopus model that is trained with limited half-handcrafted environment message, the model itself can explore in the environment with visual input, and collect data for next training loop. However, we will keep this exploration in our future exploration.\n\n> W2. From my understanding, GPT-4 generates the target states, and whether the target states have been met is used as environmental feedback for training Octopus. It seems possible that GTP-4 will generate an incorrect or trivial target state to satisfy the language task goal, and be able to successfully reach that predicted target state, without actually achieving the task goal. Is this understanding correct? In this case, the errors from GPT-4 seem more harmful than having unsuccessful execution from GPT-4 generated code.\n\nYes, sometimes GTP-4 will generate a trivial target state to satisfy the language task goal, but in this case, the main task will not be met, and according to Task-level judgment in Section 3.3, all the steps will marked as failure once the main task goal is not met, and the RLEF training will use the failure to train the model not to do such trivial plan. However, these trivial steps, as long as they do not make syntax errors, will be used for SFT training, helping the model to write good code.\n\n> W3. The results on Table 2 are not particularly convincing of this method\u2019s success. Octopus should indeed outperform blind LLMs that do not take visual input. TAPA outperforms/is of equal performance in 2 of the 5 tasks. The paper lacks analysis on why this is the case. Where does TAPA fail? It is also hard to compare when the vision models are different; does OVD and CLIP-ViT perform similarly in terms of capturing information from the input scene?\n\nActually, the input of the blind LLMs in Table 2 is the textual representation of the objects (and the relations) that are shown in the vision image, i.e, we put everything that exists in the robot's vision into the text to feed into LLMs, so although it is blind, it can \"listen\" what the environment has. So the higher task completion rate of Octopus does show the superiority of vision input and vision components in Octopus. In Table 2, Octopus reaches a good result on almost all the metrics except the seen environment, showing that Octopus, with the vision as the input can have a better generalization ability in planning. The pure language models, like TAPA, when given the familiar textual input that reflects the environment, seem to have a better result. But we believe task and environmental generalization is a more important aspect.\n\n> Nit: Should add a figure describing Octopus model architecture; notations in methods section are not well defined.\n\nThank you for pointing this! We draw the training pipeline in Figure 4, as the architecture is simply following the Flamingo framework (since Otter follows Flamingo), which can be accessed [here](https://github.com/lucidrains/flamingo-pytorch)."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission374/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700625403429,
                "cdate": 1700625403429,
                "tmdate": 1700625403429,
                "mdate": 1700625403429,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Wzq2Z7GkNj",
            "forum": "VUA9LSmC2r",
            "replyto": "VUA9LSmC2r",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission374/Reviewer_ABA1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission374/Reviewer_ABA1"
            ],
            "content": {
                "summary": {
                    "value": "This paper leverages GPT4 to generate vision and language training data from OmniGibson and GTA-V. Then, based on the data, they train the model modified from the Otter model and perform some downstream embodied tasks to demonstrate the performance."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The idea of using GPT4 with crafted prompts to acquire training data from existing environments is interesting.\n\n2. Error management and environment feedback are reasonable."
                },
                "weaknesses": {
                    "value": "1. The novelty of this paper is limited. Essentially, it uses GPT4 with the prompt engineer to collect data from two embodied environments and then trains a vision-language agent model. Moreover, The agent model does not have a specific framework diagram to show the detailed parts, making it difficult to see which part of the model is its innovation point. I suggest providing a framework diagram to clearly explain where the model is newly proposed in the paper and how it differs from existing methods.\n\n2. This paper appears to employ an intentional use of uncommon or less frequently used words in many sentences, substituting them for simpler, more common terms that could convey the message more clearly. As a result, the reading experience becomes somewhat disjointed, and the text may come across as rather weird to the reader.\n\n3. The reason for using both OmniGibson and GTA-V environments to generate data seems not obvious. A more obvious comparison between the two environments is required, such as the visual comparison of the tasks.\n\n4. In Section Error Management, when the agent executes the wrong command, how does the method perform \"error management\" on it? It seems that this section only claims the cases under which a task is defined as failure, and does not show how to manage such failure.\n\n5. In Section ENVIRONMENTAL FEEDBACK, what if there are multiple erroneous states in a task sequence?  Which are the positive states and which are the error states at this time? And it may not be said that if one of the states is wrong, then its previous states must all be negative.\n\n6. The experimental baselines are unfair and unclear. For Blind LLMs, without visual input to GPT4, GPT4 cannot ground language into the visual environment, which will inevitably lead to worse results. As for TAPA, the reader cannot understand what kind of model it is and its workflow. Even the OVD are not introduced or cited.\n\n7. What are the tasks of the four testing environments? The authors did not give a detailed introduction.\n\n8. Some titles and analyses of the experimental sections appear to be uninformative. For example, **LLMs Does Not Depend on Observation**, is this a conclusion or a statement? If it is a statement, the authors have already said before that this baseline has no visual input, and it is also contrary to the subsequent titles which are all conclusions. In addition, in the ablation experiments, larger models and more components trained can bring more performance improvements, which is common sense in sense, but the authors put them as ablation experiments alone and do not give any insights."
                },
                "questions": {
                    "value": "1. How to train the reward model $r_{\\phi}$? More details are needed.\n\n2. A visual example of task trees is required for better understanding."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission374/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission374/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission374/Reviewer_ABA1"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission374/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698552288214,
            "cdate": 1698552288214,
            "tmdate": 1699635964443,
            "mdate": 1699635964443,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8Zy8Xqsi7h",
                "forum": "VUA9LSmC2r",
                "replyto": "Wzq2Z7GkNj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission374/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission374/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ABA1 (1/3)"
                    },
                    "comment": {
                        "value": "> The novelty of this paper is limited. Essentially, it uses GPT4 with the prompt engineer to collect data from two embodied environments and then trains a vision-language agent model. Moreover, The agent model does not have a specific framework diagram to show the detailed parts, making it difficult to see which part of the model is its innovation point. I suggest providing a framework diagram to clearly explain where the model is newly proposed in the paper and how it differs from existing methods.\n\nWe sincerely thank the reviewer's valuable feedback and we add the diagram in Figure 4. Our core motivation is to develop a domain that remains uncharted to date, i.e., an embodied vision-language model to make reasonable plans and write executable programs. The novel technical part is that (1) it is the first attempt to have a VLM for code writing and (2) it designs a novel solution for planning improvement in the RLEF part, which collects the success/failure information when GPT-4 is collecting the vision-code pairs in the simulator.\n\n> This paper appears to employ an intentional use of uncommon or less frequently used words in many sentences, substituting them for simpler, more common terms that could convey the message more clearly. As a result, the reading experience becomes somewhat disjointed, and the text may come across as rather weird to the reader.\n\nThank you for the suggestion and we have revised the whole manuscript thoroughly.\n\n> The reason for using both OmniGibson and GTA-V environments to generate data seems not obvious. A more obvious comparison between the two environments is required, such as the visual comparison of the tasks.\n\nThe reason of us building OctoVerse is to support the development of our embodied vision-language programmer. OctoVerse is generally based on the existing, most realistic simulators of GTA and OmniGibson, where we add the necessary adaptation to include **the instruction set of callable functions** and **carefully designed tasks**. We include the comparison between OctoVerse with other platforms in appendix C1. The general reason for selecting these two platforms is the consideration of their realistic modeling task designability and scalability. Admittedly, other simulators such as AI2THOR and VirtualHome might also be able to support the Octopus exploration if we also design an instruction set of callable functions and carefully designed tasks, we eventually selected OmniGibson and GTA for realistic modeling and task designability and scalability. The detailed task can be accessed [here](https://filetransfer.io/data-package/WDACj9TO#link) and in the appendix.\n\n\n> In Section Error Management, when the agent executes the wrong command, how does the method perform \"error management\" on it? It seems that this section only claims the cases under which a task is defined as failure, and does not show how to manage such failure.\n\nYes, we only annotate commands with \"success\" or \"failure\" tags, and these tags are the supervision of the reward modeling in the RLEF part. According to Figure 4 in the paper, all the vision-code pairs in the (a) data collection pipeline will be used for the SFT training in Figure 4(b) Octopus training pipeline, since the GPT-4 does a good job on formatting, meaning that they can write reasonable code. And the \"success\" or \"failure\" tag will be used in the RLEF training.\n\n> In Section ENVIRONMENTAL FEEDBACK, what if there are multiple erroneous states in a task sequence? Which are the positive states and which are the error states at this time? And it may not be said that if one of the states is wrong, then its previous states must all be negative.\n\nAccording to Figure 3, one error state is tagged if, after one step, post-execution states misalign their target states. Alternatively, if a task is not completed as intended, every state within that task is labeled as negative. Sorry for the previous confusion and we have clarified this part in Section 3.3."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission374/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700624803457,
                "cdate": 1700624803457,
                "tmdate": 1700625260945,
                "mdate": 1700625260945,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DSxEX4GhEw",
            "forum": "VUA9LSmC2r",
            "replyto": "VUA9LSmC2r",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission374/Reviewer_CLdb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission374/Reviewer_CLdb"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduced Octopus, a vision-language model mapping the visual input to the action codes. The fine-tuned dataset is collected with the GPT-4 where the simulator feedback is incorporated to generate the system feedback. The author further proposed a RLEF to improve the performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This paper proposed a novel VLM  to transfer the visual input to the executable codes, driving the agents.\n- The GPT-4 along with a simulator is used to collect training datasets. OminiGibson and OctoGTA are used respectively.\n- An RLEF module is used to boost the model's performance further."
                },
                "weaknesses": {
                    "value": "- More related works in Section 2.1 are needed to help the reviewer identify the paper's contributions, like [1-4]. \n- The authors term the simulation they used \"OctoGibson\" which is built upon OmniGibson. Can the authors give more details to elaborate the main difference between them, or did they just use that Simulator to collect the dataset?\n- A better format is needed. Some lines need a reformat in the revised version. One example is \"3.2 Instructions From Exploration\", lines above and below seem to belong to the same paragraph.\n- More experiments are needed. The author only conducts the experiments on the datasets they collected and lacks a direct comparison with more relative frameworks as discussed in Section 2.1. \n\n[1] Huang, Wenlong, et al. \"Voxposer: Composable 3d value maps for robotic manipulation with language models.\" arXiv preprint arXiv:2307.05973 (2023).\n[2] Lin, Kevin, et al. \"Text2motion: From natural language instructions to feasible plans.\" arXiv preprint arXiv:2303.12153 (2023).\n[3] Huang, Siyuan, et al. \"Instruct2Act: Mapping Multi-modality Instructions to Robotic Actions with Large Language Model.\" arXiv preprint arXiv:2305.11176 (2023).\n[4] Yu, Wenhao, et al. \"Language to Rewards for Robotic Skill Synthesis.\" arXiv preprint arXiv:2306.08647 (2023)."
                },
                "questions": {
                    "value": "- The author uses the GPT-4 to collect the training data, and one implicit assumption is that the performance of the GPT-4 is optimal or near-optimal. A comparison between the GPT-4 generated data sample and the human-collected sample would help. Or, did the author conduct some data quality control before the use of dataset?\n\n- When using GPT-4 plus a simulator to collect the dataset, the location of the target object is directly obtained from the simulator? And this information be stored and used for later training? With this approach, the final complete robotic system still needs a separate vision model besides the ViT-L in the VLM. Can the author give some discussion on this design choice?\n\n- RLEF. It is very interesting to see the usage and effectiveness of RLEF. However, I am curious as to why you chose CodeLLaMA-7B as the reward model while using MPT-7B for the complete VLM? \n\n- In Table 2, there is a comparison between Octopus and  MPT-7B. Also, the performance is not consistently superior, a further discussion is needed. And the metrics' definition is needed to help the understanding. \n\n- Ablation: 3B: what is the 3B model? \n\n- The author inputs 10 images to the VLM and discusses the standard version vs the random version.  Would other designs help?\n\n- The author states multiple times with \"open-sourcing\" in the main text, a link to the anonymous website would be helpful.\n\n- See weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission374/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission374/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission374/Reviewer_CLdb"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission374/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698683900134,
            "cdate": 1698683900134,
            "tmdate": 1699635964351,
            "mdate": 1699635964351,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PwnCMRWlw9",
                "forum": "VUA9LSmC2r",
                "replyto": "DSxEX4GhEw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission374/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission374/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer CLdb (1/2)"
                    },
                    "comment": {
                        "value": "> More related works in Section 2.1 are needed to help the reviewer identify the paper's contributions, like [1-4].\n\nWe sincerely thank the reviewer for pointing out these important works, by comparing them we can better show the accurate standing of the Octopus work in the community. We add all these four works in the Table 1, and we show that the proposed Octopus distinguishes itself from other models as an end-to-end, unified vision-language model for both plan and code generation.\n\n> The authors term the simulation they used \"OctoGibson\" which is built upon OmniGibson. Can the authors give more details to elaborate on the main difference between them, or did they just use that Simulator to collect the dataset?\n\nThe OctoVerse is generally based on the existing realistic simulators of GTA and OmniGibson, where we add the necessary adaptation to include **the instruction set of callable functions** and **carefully designed tasks** that make the development of the embodied vision-language programmer training possible. \n\n>  A better format is needed. Some lines need a reformat in the revised version. One example is \"3.2 Instructions From Exploration\", lines above and below seem to belong to the same paragraph.\n\nThank you for pointing it out! Yes, we have drastically revised the whole draft several rounds and made sure it is easy to understand with good structure.\n\n> More experiments are needed. The author only conducts the experiments on the datasets they collected and lacks a direct comparison with more relative frameworks as discussed in Section 2.1.\n\nYes, we have added several experiments in Section 5.1 and Table 2, where we add throughout comparison with \"Blind LLM\" including LLaMA, CodeLLaMA, TAPA, which can only \"listen\" to what is inside the environment. We also implement the EmbodiedGPT for comparison. These experiments have shown several conclusions including:\n- Blind LLMs Struggle with Extended Input Content.\n- CodeLLaMA Improves Coding but not Planning.\n- Octopus Demonstrates Superior Task Generalization Ability as it integrates the visual input.\n-  RLEF Enhances Octopus\u2019s Planning Strategy, which is more significant than the components in EmbodiedGPT.\n\n| Model             | Vision Model | Language Model | Seen Env | Unseen Env | Follow | Reason | All   |\n|------------------|--------------|----------------|----------|------------|--------|--------|-------|\n| LLaMA            | GT (O+R)     | LLaMA2-7B      | 0.07 / 0.11 | 0.13 / 0.13 | 0.11 / 0.16 | 0.00 / 0.00 | 0.08 / 0.12 |\n| CodeLLaMA        | GT (O+R)     | CodeLLaMA-7B   | 0.09 / 0.20 | 0.20 / 0.40 | 0.16 / 0.31 | 0.00 / 0.07 | 0.12 / 0.25 |\n| TAPA (task-level) | ~~OVD~~ GT (O) | CodeLLaMA-7B   | 0.09 / 0.36 | 0.13 / 0.33 | 0.11 / 0.36 | 0.06 / 0.33 | 0.10 / 0.35 |\n| TAPA (step-level) | ~~OVD~~ GT (O) | CodeLLaMA-7B   | **0.16 / 0.42** | 0.13 / 0.27 | **0.18 / 0.38** | 0.07 / 0.40 | 0.15 / 0.38 |\n| EmbodiedGPT       | CLIP-ViT     | MPT-7B         | 0.04 / 0.36 | 0.27 / **0.53** | 0.13 / 0.38 | 0.00 / 0.40 | 0.10 / 0.40 |\n| Octopus (SFT Only) | CLIP-ViT   | MPT-7B         | 0.11 / 0.33 | 0.27 / 0.47 | 0.16 / 0.38 | 0.13 / 0.33 | 0.15 / 0.37 |\n| Octopus (SFT + RLEF) | CLIP-ViT | MPT-7B         | 0.13 / 0.38 | **0.33 / 0.53** | **0.18 / 0.40** | **0.20 / 0.53** | **0.18 / 0.42** |\n\nThese experimental results show the promising direction of the end-to-end vision language models for planning and programming.\n\n> The author uses the GPT-4 to collect the training data, and one implicit assumption is that the performance of the GPT-4 is optimal or near-optimal. A comparison between the GPT-4 generated data sample and the human-collected sample would help. Or, did the author conduct some data quality control before the use of dataset?\n\nThank you for the great question! Actually, when we use GPT-4 for the automatic data collection pipeline, the \"environmental feedback\" generally acts like the data quality controller. More specifically, when GPT-4 writes the plan, code, and target state, the code will be run in the simulator and check whether the target state is met and whether the final mission is satisfied. All these results of success/failure will be valuable resources for RLEF training. In fact, we intensively designed a very comprehensive system message for GPT-4 and it is good at following the structure with near-perfect code format (seldom syntax error occurs), and the syntax correctness can be identified by the simulator execution and we will remove these with syntax incorrectness when training SFT."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission374/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700624549236,
                "cdate": 1700624549236,
                "tmdate": 1700625608725,
                "mdate": 1700625608725,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "t8lEatEXBG",
            "forum": "VUA9LSmC2r",
            "replyto": "VUA9LSmC2r",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission374/Reviewer_qA1p"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission374/Reviewer_qA1p"
            ],
            "content": {
                "summary": {
                    "value": "The manuscript proposes a model and simulator for instruction-following tasks in Embodied AI, leveraging GPT-4 for a human-model-agent task-execution paradigm."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The manuscript makes reference to relevant methodology in EAI \u2014 designing agents that include foundation models, which perform intermediate reasoning tasks"
                },
                "weaknesses": {
                    "value": "Section 1 / Throughout \u2014 The manuscript forgets to properly motivate its contributions. What problem is this work supposed to be solving? What research questions are examined by this manuscript?\n\nSection 1 \u2014 Most of the Introduction section is unnecessary. The space should instead be used to describe what is added on top of GPT-4 to make the model proposed in this paper a sufficiently distinct contribution. How is OctoVerse different from other EAI simulators? Why does the community need OctoVerse? What problems can be solved in OctoVerse that cannot be solved elsewhere? The manuscript fails both to motivate and explicitly describe its contributions.\n\nSection 2 \u2014 Call it \u201cRelated Work\u201d. The dimensions on which this section compares the proposed work with the prior art are all wrong. Firstly, because the manuscript is attempting to propose a new environment and tasks, it should identify the limitation of other, similar simulators/datasets and explicitly discuss the proposed improvements. Regarding claims for novel modeling contributions, the manuscript must first propose research questions or problems that the approach attempts to solve. Next, a set of related work can be organized to discuss their attempts at answering said research question and solving said problems, as well as discuss their limitations or weaknesses. Finally, this structure affords the manuscript to describe how its proposed work improves on the prior art, according to those research questions and identified problem(s).\n\nSection 3 \u2014 The manuscript does not make clear what was originally provided by OmniGibson / GTA-V, versus what is added by OctoVerse. Also, again, the manuscript is missing motivation for why anyone should use its proposed environment. The problem formulation needs a lot of work."
                },
                "questions": {
                    "value": "N/A \u2014 see above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission374/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698809080800,
            "cdate": 1698809080800,
            "tmdate": 1699635964265,
            "mdate": 1699635964265,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pwrZe4rBO1",
                "forum": "VUA9LSmC2r",
                "replyto": "t8lEatEXBG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission374/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission374/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer qA1p (1/2)"
                    },
                    "comment": {
                        "value": "> Section 1 / Throughout \u2014 The manuscript forgets to properly motivate its contributions. What problem is this work supposed to be solving? What research questions are examined by this manuscript?\n\nWe sincerely appreciate the constructive feedback regarding the need to more clearly articulate the motivation behind our work. We have thoroughly revised our Introduction section (highlighted in the submission paper). \nOur core motivation is to develop a domain that remains uncharted to date, i.e.,**an embodied vision-language models to make reasonable plans and write executable programs\u201d**. While there are existing models like ToolFormer, HuggingGPT, ViperGPT, and VisProg that delve into programming with non-visual inputs, Octopus stands out in its endeavor to merge these programming models with visual stimuli. Consequently, recognizing the lack of suitable environments for conducting such innovative experiments, we have developed OctoGTA and OctoGibson, enabling us to explore this new territory of vision-language model programming. We hope this explanation can help clarify the standing of the motivation and the purpose of the two environments.\n\n> Section 1 \u2014 Most of the Introduction section is unnecessary. The space should instead be used to describe what is added on top of GPT-4 to make the model proposed in this paper a sufficiently distinct contribution. How is OctoVerse different from other EAI simulators? Why does the community need OctoVerse? What problems can be solved in OctoVerse that cannot be solved elsewhere? The manuscript fails both to motivate and explicitly describe its contributions.\n\nThank you again for the constructive suggestions! We are generally using GPT-4 to prepare for the vision input and executable program pairs, and the core contribution is to illustrate the embodied vision-language models can have better achievement than the unimodal language models, with experimental support in Table 2, where we see (1) the drawbacks of unimodal LLMs: they struggle with extended input content that describes the visual input; and (2) The significance of visual inputs in task performance. The OctoVerse is generally based on the existing, most realistic simulators of GTA and OmniGibson, where we add the necessary adaptation to include **the instruction set of callable functions** and **carefully designed tasks**. We include the comparison between OctoVerse with other platforms in Appendix C1. The general reason for selecting these two platforms is the consideration of their realistic modeling task designability and scalability. Admittedly, other simulators such as AI2THOR and VirtualHome might also be able to support the Octopus exploration if we also design an instruction set of callable functions and carefully designed tasks, we eventually selected OmniGibson and GTA for realistic modeling and task designability and scalability. In the revised Appendix C1, we incorporate various attributes that enhance diversity and realism, noting a substantial advancement in both OctoGTA and OctoGibson simulation environments. First, a wide range of well-formulated tasks are established in Octopus's simulator showcasing from fine-grained indoor routine activities to outdoor open-world tasks. This contrasts with other simulation environments that often target a relatively restricted set of activities.\n\n| Simulation Environment | Kinematics | Continuous Extended States | Flexible Materials | Deformable Bodies | Realistic Fluid | Realistic Action Execution | Task Planning and/or Control | Game-Based or World-Based | Well-Formulated Tasks | Code Execution |\n|------------------------|------------|----------------------------|--------------------|-------------------|-----------------|---------------------------|-------------------------------|---------------------------|------------------------|----------------|\n| OpenAIGym | \u2713 | \u2715 | \u2715 | \u2715 | \u2715 | \u2713 | C | G | \u2715 | \u2713 |\n| Matterport3D | \u2715 | \u2715 | \u2715 | \u2715 | \u2715 | \u2715 | C | W | \u2715 | \u2715 |\n| AI2THOR | \u2713 | \u2715 | \u2715 | \u2715 | \u2715 | \u2715 | TP | G | \u2715 | \u2713 |\n| VirtualHome | \u2715 | \u2715 | \u2715 | \u2715 | \u2715 | \u2715 | TP | G | \u2715 | \u2715 |\n| House3D | \u2715 | \u2715 | \u2715 | \u2715 | \u2715 | \u2715 | TP | W | \u2715 | \u2715 |\n| Habitat 1.0 | \u2713 | \u2715 | \u2715 | \u2715 | \u2715 | \u2713 | C | W | \u2715 | \u2713 |\n| Robosuite | \u2713 | \u2715 | \u2715 | \u2715 | \u2715 | \u2713 | C | W | \u2715 | \u2713 |\n| RFUniverse | \u2713 | \u2715 | \u2713 | \u2713 | \u2713 | \u2713 | TP+C | W | \u2715 | \u2713 |\n| Minecraft | \u2713 | \u2715 | \u2713 | \u2715 | \u2715 | \u2713 | TP+C | G | \u2713 | \u2713 |\n| GTA | \u2713 | \u2713 | \u2713 | \u2713 | \u2713 | \u2713 | TP+C | G | \u2715 | \u2715 |\n| Omnigibson | \u2713 | \u2713 | \u2713 | \u2713 | \u2713 | \u2713 | TP+C | W | \u2715 | \u2715 |\n| OctoGTA | \u2713 | \u2713 | \u2713 | \u2713 | \u2713 | \u2713 | TP+C | G | \u2713 | \u2713 |\n| Octogibson | \u2713 | \u2713 | \u2713 | \u2713 | \u2713 | \u2713 | TP+C | W | \u2713 | \u2713 |"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission374/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700624371109,
                "cdate": 1700624371109,
                "tmdate": 1700711226742,
                "mdate": 1700711226742,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]