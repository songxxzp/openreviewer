[
    {
        "title": "INSIDE: LLMs' Internal States Retain the Power of Hallucination Detection"
    },
    {
        "review": {
            "id": "gj98ErFktu",
            "forum": "Zj12nzlQbz",
            "replyto": "Zj12nzlQbz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5370/Reviewer_5aJw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5370/Reviewer_5aJw"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a new method for detection of LLM hallucinations. The method called EigenScore is based on extracting internal representation (layer activations) of the proposed sequences. Then, they compute the covariance matrix of K proposed sequences and then compute the logarithm determinant of the matrix. Intuitively, it measures entropy of the proposed sequences. They also propose the technique for reducing the number of confident hallucinations by clipping the outlier layer activations. In the experiments they empirically show that their proposed metric correlates with the correctness of the outputs more than other baselines such as Perplexity, Lexical similarity, Length normalized entropy and Energy."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The proposed method is simple, intuitive and it is well explained. The idea of looking into the internal states for hallucination detection is well motivated. I enjoyed the clear presentation of the paper. For example, the background section was quite useful for the understanding of the methods. The results seem to clearly show the advantage of the proposed method over several established baselines. The experiments are conducted on several diverse datasets, and several language models. The advantage of an alternative method on a benchmark with short answers is well explained and it is useful to inform the reader in which situations the proposed method should be employed. The ablation studies are quite complete and they show that the proposed method works under a wide range of settings."
                },
                "weaknesses": {
                    "value": "Minor points and suggestions:\n- Regarding the outliers in the activations, it would be nice to see if these outliers really lead to overconfident predictions. For example, is there any correlation between the number of outliers and accuracy of the answer? At least, it could be informative to see a few generated examples corresponding to many outliers or no outliers.\n- For Table 2, it would be interesting to see if the quality of answers is increased after using clipping (as confident mistakes are excluded).\n- In general, it would be nice to see some qualitative examples of hallucinations that are detected by the proposed method, but not detected by the baselines.\n- centralized matrix -> centering matrix\n- Typos: \"prior works that rectifying\", \"nli-roterta-large\"."
                },
                "questions": {
                    "value": "I have only minor questions and suggestions and I would like to hear authors' discussion on those points."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5370/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698424274411,
            "cdate": 1698424274411,
            "tmdate": 1699636542083,
            "mdate": 1699636542083,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "keJcz62pqh",
                "forum": "Zj12nzlQbz",
                "replyto": "gj98ErFktu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5370/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5370/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 5aJw (R4)"
                    },
                    "comment": {
                        "value": "**Comment** We thank Reviewer 5aJw (R4) for their constructive suggestions. According to the suggestion, we have provided a comprehensive Case Studies in the **Appendix H (Page16-21)** of our revised paper. Here is a point-by-point response.\n\n***\n\n> **W1** Regarding the outliers in the activations, it would be nice to see if these outliers really lead to overconfident predictions. For example, is there any correlation between the number of outliers and accuracy of the answer? At least, it could be informative to see a few generated examples corresponding to many outliers or no outliers.\n\n**Ans** Thanks for your suggestion. The outliers features increase the likelihood of generating overconfident and self-consistent generations. For example, the top 50 tokens that contain the most outliers have an average confidence of 0.98, while the bottom 50 tokens that contain the least outliers have an average confidence of 0.54. According to your suggestion, we provide several examples that contain many/few extreme features in the **Appendix H.2 (Page19)**. The results show that when there are many extreme features, the model tends to generate consistency hallucination outputs across multiple generations. Instead, when there are few extreme features, the model generates diverse hallucination outputs which can be spotted by different hallucination detection metrics.\n\n***\n\n> **W2** For Table 2, it would be interesting to see if the quality of answers is increased after using clipping (as confident mistakes are excluded).\n\n**Ans** It is worth noting that the feature clipping approach is proposed to prevent the model from generating overconfident outputs for uncertain questions, but not to improve the quality of answers. To demonstrate the effectiveness of the feature clipping strategy, we show several  cases before and after feature clipping in the **Appendix H.3 (Page20)**\n\n***\n\n> **W3** In general, it would be nice to see some qualitative examples of hallucinations that are detected by the proposed method, but not detected by the baselines. \n\n**Ans** Thanks for your suggestion. We have provided comprehensive cases in **Appendix H.1 (Page16)**, including good cases and failure cases. Please see the details in our revised paper.\n\n***\n\n> **W4** typos\uff1acentralized matrix -> centering matrix\uff1b\"prior works that rectifying\", \"nli-roterta-large\".\n\n**Ans** Thanks\uff0cthe typos have been revised. We will re-polish our paper carefully."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5370/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700380268632,
                "cdate": 1700380268632,
                "tmdate": 1700380268632,
                "mdate": 1700380268632,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Sqss9mmZuH",
            "forum": "Zj12nzlQbz",
            "replyto": "Zj12nzlQbz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5370/Reviewer_9Srq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5370/Reviewer_9Srq"
            ],
            "content": {
                "summary": {
                    "value": "This work follows the line of uncertainty-based methods for hallucination detection to probe the inconsistency between the independently generated response for the same query. The idea is to take the eigenvalues of LLM internal states' covariance matrix to measure the inconsistency."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "If we don't consider concurrent works, such as \"Representation Engineering: A Top-Down Approach to AI Transparency\", the proposed method is novel and feasible since measuring covariance between several independent generations is intuitive. \n\nThe proposed method is simple and easy to implement and use without tuning models, and the performance improvement is significant."
                },
                "weaknesses": {
                    "value": "The experiment setting could be improved since this work targets hallucination detection, which is a broad area, and there are various types of datasets. Only reporting results on QA benchmarks may be insufficient to support the contribution. At least, I think truthfulQA is closer to the topic of hallucination compared to the benchmarks used in the paper. Additionally, it's better to include recent LLMs, such as LLaMA2 and Falcon. \n\nThe covariance of LLM internal states may contain distracting information, such as the diversity of expressions, styles, and terminology. Since there are multiple factors that influence the difference between internal states, it's hard to guarantee the generalization ability of the proposed method."
                },
                "questions": {
                    "value": "For the correctness Measure, I agree that the evaluation of answers is challenging, but it's better to have an exact match score so that we can compare it with other existing works. \n\nAs feature clipping is claimed as a contribution of this work, could you provide some ablation studies for it? I am curious why it needs a dynamic memory bank. Does that mean the distribution of activation values changes a lot across samples?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5370/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698649224862,
            "cdate": 1698649224862,
            "tmdate": 1699636541971,
            "mdate": 1699636541971,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WiZIMsXGpp",
                "forum": "Zj12nzlQbz",
                "replyto": "Sqss9mmZuH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5370/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5370/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 9Srq (R3) -- Part1"
                    },
                    "comment": {
                        "value": "**Comment** We thank Reviewer 9Srq (R3) for the careful reviews and insightful suggestions, which really helped us improve our paper.\n\n>**W1** (1) The experiment setting could be improved. I think TruthfulQA is closer to the topic of hallucination compared to the benchmarks used in the paper. (2) It's better to include recent LLMs, such as LLaMA2 and Falcon.\n\n**Ans** (1) According to your suggestion, we have compared our proposal with several competitve approaches in the TruthfulQA dataset. The results are presented in Table 1. For the ITI [1], which provides a strong hallucination detector that trained on TruthfulQA, we report the best performance in their paper. As can be seen, our proposal consistently outperforms the baseline methods and achieves comparable performance as ITI when we utilize 50 in-distribution prompts. \n\nTable 1: Performance comparison of different methods on TruthfulQA dataset. Accuracy is reported.\n| #prompt | Perplexity | LN-Entropy | LexialSim | SelfCKGPT | ITI* | **EigenScore** |\n|:-------:|:----------:|:----------:|:---------:|:---------:|:---------:|:--------------:|\n|    5    |    70.0    |    71.2    |    73.6   |    74.2   |  **83.3** |      76.7      |\n|    20   |    76.4    |    77.7    |    77.9   |    76.8   |  **83.3** |      79.5      |\n|    50   |    73.1    |    77.9    |    73.6   |   78.3    |  **83.3** |    **81.3**    |\n\nIt's worth nothing that the ITI [1] relies on training 1024 binary classifiers in TruthQA datasets, and they report the best performance (83.3) in the validation set. Therefore, their best performance is better than our method which has not been trained on TruthfulQA. However, training on the validation set also limits the generalization of their method on other domains [1].\n\nAs TruthfulQA is a very challenging dataset for LLMs, zero-shot inference results in poor performance. Therefore, we follow previous work [2] to utilize different number of in-distribution prompts during inference time. The results show that the performance could be significantly improved when we increase the number of in-distribution prompts, which also explains why ITI performs good.\n\nPlease see the **Appendix A (Page13)** of our revised paper for the detailed results and analysis.\n\n(2) In the main experiments, we evaluate the performance of different methods in LLaMA-7B, LLaMA-13B and OPT-6.7B. To demonstrate the robustness of our method across different models, we also provide the performance comparison in the recent LLaMA2-7B  and Falcon-7B. The results in Table 2 reveal that our proposal consistently exhibits superior performance compared to the competitive methods across different LLMs. \n\nTable2: Performance evaluation on LLaMA2-7B and Falcon-7B. AUC$_s$/AUC$_r$ score is reported.\n|  Methods  | Datasets | Perplexity | LN-Entropy | Lexical Similarity | SelfCheckGPT |   EigenScore  |\n|:---------:|----------|:----------:|:----------:|:------------------:|:------------:|:-------------:|\n| LLaMa2-7b |   CoQA   |  62.2/66.6 |  69.9/75.2 |      74.4/77.5     |   72.4/75.1  | **78.6/80.7** |\n| LLaMa2-7b |    NQ    |  70.8/70.2 |  72.1/71.2 |      72.1/72.9     |   69.1/68.1  | **74.4/73.7** |\n| Falcon-7b |   CoQA   |  57.0/60.6 |  62.6/63.2 |      74.8/76.4     |   76.7/77.9  | **80.8/80.6** |\n| Falcon-7b |    NQ    |  74.3/74.7 |  74.6/74.7 |      73.8/75.4     |   74.7/74.0  | **76.3/75.7** |\n\nFor more information, please see the **Appendix C (Page13)** of our revised paper. \n\n***\n\n> **W2** The covariance of LLM internal states may contain distracting information (such as the diversity of expressions, styles, and terminology), it's hard to guarantee the generalization ability of the proposed method.\n\n**Ans**  The diversity of natural language expressions has long presented a challenge in evaluating neural language generation and detecting hallucinations. Our proposal, of course, cannot completely solve the problem. However, compared to existing uncertainty or consistency metrics, our proposal takes a step further in solving this problem: (1) It captures the semantic divergence (entropy) in the dense embedding space, which is expected to retain highly-concentrated semantic information.  (2) The proposed EigenScore, which is determined by the LogDet of the sentence embeddings' Covariance, is more intuitive and effective compared to existing metrics that explores sentence-wise similarity\n\nBesides, As presented in our experimental setup, we set the temperature t=0.5 through the experiments, which appropriately suppressed the diversity of LLMs' generations. In Fig. 4(a) (of our submission), we have presented the performance sensitivity to temperature t, which shows that our EigenScore significantly outperforms baseline methods when t<1\uff0c and when t>1 the performance decreases rapidly.\n\n***\n**Reference**\n\n[1] TruthfulQA: Measuring How Models Mimic Human Falsehoods\n\n[2] Training a helpful and harmless assistant with reinforcement learning from human feedback"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5370/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700380058958,
                "cdate": 1700380058958,
                "tmdate": 1700380058958,
                "mdate": 1700380058958,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "O0CAzhLdYz",
                "forum": "Zj12nzlQbz",
                "replyto": "Sqss9mmZuH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5370/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5370/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 9Srq (R3) -- Part2"
                    },
                    "comment": {
                        "value": "> **Q1** For the correctness Measure, I agree that the evaluation of answers is challenging, but it's better to have an exact match score so that we can compare it with other existing works\n\n**Ans** According to the suggestion, we have provided the evaluation results by employing Exact Match as the correctness measure. The results in Table 3 show similar conclusion with using ROUGE and sentence similarity as the correctness measure, which demonstrates that our proposal consistently outperforms the comparison baselines under different evaluation metrics.\n\nPlease see the **Appendix E (Page14)** in our revised paper for more details. \n\nTable3: Performance evaluation with Exact Match as correctness measure. \n|  Models  |  Methods | Perplexity | LN-Entropy | Lexical Similarity | EigenScore |\n|:--------:|:--------:|:----------:|:----------:|:------------------:|:----------:|\n| LLaMA-7B |   CoQA   |    63.7    |    70.7    |        76.1        |  **83.0**  |\n| LLaMA-7B |   SQuAD  |    57.3    |    72.1    |        76.9        |  **83.9**  |\n| LLaMA-7B |    NQ    |    75.3    |    75.6    |        75.8        |  **80.1**  |\n| LLaMA-7B | TriviaQA |    82.5    |  **83.4**  |        81.8        |    82.4    |\n| OPT-6.7B |   CoQA   |    59.4    |    61.7    |        71.8        |  **79.4**  |\n| OPT-6.7B |   SQuAD  |    56.7    |    65.2    |        72.7        |  **82.9**  |\n|OPT-6.7B |    NQ    |  **79.8**  |    78.1    |        73.2        |  **79.8**  |\n| OPT-6.7B | TriviaQA |  **83.8**  |    81.3    |        79.3        |    82.7    |\n\n***\n\n> **Q2** (1) As feature clipping is claimed as a contribution of this work, could you provide some ablation studies for it? (2) I am curious why it needs a dynamic memory bank. Does that mean the distribution of activation values changes a lot across samples?\n\n**Ans** (1) We have already provided the ablation study in our original submission in (**Page7 - Effectiveness of Feature Clipping**), which demonstrates the effectiveness of the introduced feature clipping techinque. Besides, we also add several samples to show the hallucination detection results before and after feature clipping in the **Appendix H3 (page18)**.\n\n(2) We illustrate the distributions of neuron activation from four selected tokens in **Fig.6 (Appendix F, Page15)**, which shows that the distribution changes a lot across samples. Therefore, it is risky to determine the clipping threshold with only the current input sample (**EigenScore-C**). A feasible solution is to pre-compute the optimal threshold based on a batch of input samples (**EigenScore-P**). Besides, another solution is to dynamically record the activation values and determine the threshold during the inference process (**EigenScore-MB**). We have tried both solutions and the experimental results are presented in Table 4. The results show that determining the thresholds with a memory bank works slightly better. We attribute this variability to potential differences in the activation distributions across various datasets.\n\nTable4: Ablation study of determining the clipping threshold with different technique. AUC$_s$ is reported.\n|      Methods     |   CoQA   |    NQ    |\n|:----------------:|:--------:|:--------:|\n|   EigenScore-C |   78.1   |   74.8   |\n|   EigenScore-P   |   79.9   |   75.3   |\n|   EigenScore-MB  | **80.4** | **76.5** |\n\nPlease see the **Appendix F (Page15)** in our revised paper for more details.\n\n***\n\n**Reference**\n\n[1] TruthfulQA: Measuring How Models Mimic Human Falsehoods\n\n[2] Training a helpful and harmless assistant with reinforcement learning from human feedback"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5370/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700380169338,
                "cdate": 1700380169338,
                "tmdate": 1700380303168,
                "mdate": 1700380303168,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "g9uwXL83gh",
            "forum": "Zj12nzlQbz",
            "replyto": "Zj12nzlQbz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5370/Reviewer_Kmk8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5370/Reviewer_Kmk8"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new framework called INSIDE that leverages the internal states of large language models (LLMs) to detect hallucinated content generated by the LLM. A key component is the EigenScore metric, which measures semantic consistency in the embedding space and represents differential entropy. This helps identify overconfident and contradictory generations. They also introduce a test time feature clipping approach that truncates extreme activations, thereby reducing overconfidence and enabling better hallucination detection. Experiments on several QA benchmarks show state-of-the-art performance in hallucination detection. Ablation studies verify the efficacy of the proposed techniques, including the EigenScore metric and feature clipping. Overall, INSIDE provides an effective way to detect hallucinated content from LLMs by leveraging their internal states and controlling overconfidence. The EigenScore and feature clipping are two key innovations that improve hallucination detection."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is very interesting and easy to follow \n2. The paper poses a unique angle to detect hallucination from internal states with eigenscore\n3. The idea is overall innovative while some key points are inspired by previous work\n4. The experiments are solid to support the paper's claim"
                },
                "weaknesses": {
                    "value": "1. One of the obvious shortcomings of the proposed method is the applicability to blackbox LLMs, e.g., GPT, PaLM and Claude.  The method relies on the internal state of the model, e.g., LLaMA and OPT, but the authors may not discuss the limitation well. \n\n2. The computational cost is another concern since essentially this method is a sampling-based approach. While the number of samples is not large, how to further reduce the cost for real production is still unsolved. \n\n3. The proposed method is mainly inspired by uncertainty estimation but how to bridge the connection with hallucination detection is still unclear, even though the results look promising. \n\n4. Beyond hallucination detection, is that possible to mitigate hallucination through the proposed framework eventually?"
                },
                "questions": {
                    "value": "1. How about the proposed method compared with this work [1] on the TruthfulQA dataset?  Both work relies on the internal state of the LLM models. \n\n[1] Li, Kenneth, Oam Patel, Fernanda Vi\u00e9gas, Hanspeter Pfister, and Martin Wattenberg. \"Inference-Time Intervention: Eliciting Truthful Answers from a Language Model.\" arXiv preprint arXiv:2306.03341 (2023).\n\n2. How about the effect of sentence embeddings on the final performance?  \n\n3. Can you provide the computational cost estimation of the whole framework and which type of resources you are using? \n\n4. Since the evaluation metric is AUROC, I am wondering if, can you compare the proposed method with the GPT 3.5 or 4 performance. Or can you add some comparison with the hallucination detection baseline methods, e.g., selfcheckGPT as you mentioned in the paper? \n\n5. The current baselines are mainly from the authors' choice without open baselines, e.g., semantic uncertainty (ICLR 2023). I noted that many related works are good baselines that can be used for comparison.\n\n\n---\npost rebuttal \n\nI think the authors' response and experiments addressed my concerns so I raise my evaluation to support my decision."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5370/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5370/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5370/Reviewer_Kmk8"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5370/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698987514450,
            "cdate": 1698987514450,
            "tmdate": 1700706381281,
            "mdate": 1700706381281,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jGRXdR8XQ9",
                "forum": "Zj12nzlQbz",
                "replyto": "g9uwXL83gh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5370/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5370/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Kmk8 (R2) -- Part1"
                    },
                    "comment": {
                        "value": "**Comment** We thank Reviewer Kmk8 (R2) for the careful reviews and insightful suggestions, which helped us improve our paper.\n\n> **W1** One shortcoming of the proposed method is the applicability to blackbox LLMs. The method relies on the internal state of the model, e.g., LLaMA and OPT, but the authors may not discuss the limitation well.\n\n**Ans** The reliance on white-box LLMs is a limitation of our work, and thanks for your suggestion, we have discussed the limitations of our work in the revised paper (**Appendix G, Page 16**).\nIn addition, black-box based [1,8] and white-box based [2,3] methods are equally important for different application scenarios for LLMs. Compared to the methods designed for black-box methods, exploring the internal information for hallucination detection has its own advantages: (1) The internal states retain highly-concentrated semantic information; (2) Exploring the sentence embedding in the internal states is much more efficient compared to extracting the sentence embedding with an additional large model [2,3]. \n\n***\n\n> **W2** The computational cost is another concern since essentially this method is a sampling-based approach. How to further reduce the cost for real production is still unsolved.\n\n**Ans** One limitation of our work is that it requires additional computational overhead\uff0cwhich is also the limitation for all consistency-based [2] or prompting-based methods [7]. We have discussed this limitation in our revised paper (**Appendix G, Page16**). \n\nBesides, we also compare and analysis the computational overhead of our proposal with the base LLM and other compared baselines. The average inference time (second per question) is computed on one NVIDIA-A100 GPU and shown in Table 1. As observed, our EigenScore is about 10 times more efficient than the methods that rely on another large model to measure the self-consistency (such as SelfCheckGPT [2]), and shares the similar computational overhead with the LN-Entropy and Lexical Similarity. It is worth noting that the inference overhead required to generate multiple results is not linearly proportional to the time of generating a single output, owing to the sampling and decoding strategy of the autoregressive LLM model. \n\nTable 1: Inference cost comparison of different methods in LLaMA-7B and LLaMA-13B.\n|   Methods | BaseLLM | Perplexity | LN-Entropy | Lexical Similarity | SelfCheckGPT | **EigenScore** |\n|:---------:|:-------:|:----------:|:----------:|:------------------:|:------------:|:--------------:|\n|  LLaMA-7B |  0.245s |   0.245s   |   0.803s   |       0.815s       |    10.68s    |     0.805s     |\n| LLaMA-13B |  0.309s |   0.307s   |    1.27s   |        1.28s       |    10.26s    |      1.27s     |\n\n\nCompared to the computational overhead of generating multiple outputs, the cost of feature clipping and EigenScore computation is negligible (0.06s). For more details about the **Computational Efficiency Analysis**, please see the **Appendix D (Page14)** of our revised paper.\n\n***\n>**W3** The proposed method is mainly inspired by uncertainty estimation but how to bridge the connection with hallucination detection is still unclear, even though the results look promising.\n\n**Ans**  LLMs' knowledge hallucinations primarily arise from uncertainties regarding the source information or uncertainties inherent in the decoding process [5]. Therefore, exploring the uncertainty [5,6] or self-consistency [2,7] of LLMs' generations is one of the most promising direction for effective hallucination detection. According to [7], the self-contradictory hallucinations is one of the most important form of hallucination, and **67%** inconsistent generation indicates hallucination. Therefore, we suggest that our proposal exploring semantic consistency with LLMs' internal information is potentially a promising approach for hallucination detection.\n\n***\n\n**Reference**\n\n[1] Inference-Time Intervention: Eliciting Truthful Answers from a Language Model.\n\n[2] SelfCheckGPT: Zero-resource black-box hallucination detection for generative large language models.\n\n[3] Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation.\n\n[4] Shifting attention to relevance: Towards the uncertainty estimation of large language models.\n\n[5] Survey of Hallucination in Natural Language Generation.\n\n[6] On Hallucination and Predictive Uncertainty in Conditional Language Generation.\n\n[7] Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation.\n\n[8] Representation engineering: A top-down approach to ai transparency.\n\n[9] Insights into Classifying and Mitigating LLMs' Hallucinations\n\n[10] Training a helpful and harmless assistant with reinforcement learning from human feedback."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5370/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700379593912,
                "cdate": 1700379593912,
                "tmdate": 1700379593912,
                "mdate": 1700379593912,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dkCWg3WT4W",
                "forum": "Zj12nzlQbz",
                "replyto": "g9uwXL83gh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5370/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5370/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Kmk8 (R2) -- Part2"
                    },
                    "comment": {
                        "value": ">**W4** Beyond hallucination detection, is that possible to mitigate hallucination through the proposed framework eventually? \n\n**Ans** In most studies, the hallucination detection and mitigatition are two independent modules [3]. Most hallucination mitigatition methods can be categorized into prompting-based, external knowledge based and training/model-editing based methods [9]. In this study, we mainly focus on the hallucination detection. However, the hallucination content can be appropriately suppressed by the well-established prompting-based or external knowledge based approaches after identifying the hallucination content with EigenScore. For exapmle, in [7], the authors use a simple prompt template to correct the detected self-contradiction hallucination. \n\nIn future work, we will delve into how to effectively mitigate hallucinations with our EigenScore.\n\n***\n\n>**Q1** How about the proposed method compared with this work [1] on the TruthfulQA dataset? Both work relies on the internal state of the LLM models.\n\n**Ans** Considering that replicating the ITI [1] would be prohibitively expensive, we test the performance of our proposal and comparison baselines in the TruthfulQA dataset. The results are presented in Table 2. For the ITI, we report the best performance in their paper. As can be seen, our proposal consistently outperforms the baseline methods and achieves comparable performance as ITI when we utilize 50 in-distribution prompts. \n\nTable 2: Performance comparison of different methods on TruthfulQA dataset. Accuracy is reported.\n| #prompt | Perplexity | LN-Entropy | LexialSim | SelfCKGPT | ITI* | **EigenScore** |\n|:-------:|:----------:|:----------:|:---------:|:---------:|:---------:|:--------------:|\n|    5    |    70.0    |    71.2    |    73.6   |    74.2   |  **83.3** |      76.7      |\n|    20   |    76.4    |    77.7    |    77.9   |    76.8   |  **83.3** |      79.5      |\n|    50   |    73.1    |    77.9    |    73.6   |   78.3    |  **83.3** |    **81.3**    |\n\nIt's worth nothing that the ITI [2] relies on training 1024 binary classifiers in TruthQA datasets. Therefore, their best performance is better than our method which has not been trained on TruthfulQA. However, training on the validation set also limits the generalization of their method on other domains [2].\n\nAs TruthfulQA is a very challenging dataset for LLMs, zero-shot inference results in poor performance. Therefore, we follow previous work [10] to utilize different number of in-distribution prompts during inference time. The results show that the performance could be significantly improved when we increase the number of prompts, which also explains why ITI performs good.\n\nFor more information about the experiments, please see the **Appendix A (Page13)** of our revised paper.\n\n***\n\n>**Q2** How about the effect of sentence embeddings on the final performance?\n\n**Ans** We have performed this ablation study in **Page8 - How EigenScore Performs with Different Sentence Embeddings**. The ablation results presented in **Fig.3 (b)** shows that using the sentence embedding in the shallow and final layers yields significantly inferior performance compared to using sentence embedding in the layers close to the middle. Besides, another interesting observation is that utilizing the embedding of the last token as the sentence embedding achieves superior performance compared to simply averaging the token embeddings, which suggests that the last token of the middle layers retain more  information about the truthfulness of generations.\n\n***\n\n>**Q3** Can you provide the computational cost estimation of the whole framework and which type of resources you are using?\n\n**Ans**  Please refer to our response to **W2**. We have provided a detailed **Computational Efficiency Analysis** in **Appendix D (Page14)** of our revised paper. All experiments are performed on the NVIDIA-A100 GPU.\n\n*** \n\n**Reference**\n\n[1] Inference-Time Intervention: Eliciting Truthful Answers from a Language Model.\n\n[2] SelfCheckGPT: Zero-resource black-box hallucination detection for generative large language models.\n\n[3] Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation.\n\n[4] Shifting attention to relevance: Towards the uncertainty estimation of large language models.\n\n[5] Survey of Hallucination in Natural Language Generation.\n\n[6] On Hallucination and Predictive Uncertainty in Conditional Language Generation.\n\n[7] Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation.\n\n[8] Representation engineering: A top-down approach to ai transparency.\n\n[9] Insights into Classifying and Mitigating LLMs' Hallucinations\n\n[10] Training a helpful and harmless assistant with reinforcement learning from human feedback."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5370/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700379725671,
                "cdate": 1700379725671,
                "tmdate": 1700380578716,
                "mdate": 1700380578716,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9kWnsEJUYJ",
                "forum": "Zj12nzlQbz",
                "replyto": "G1nvSVNIT3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5370/Reviewer_Kmk8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5370/Reviewer_Kmk8"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your detailed reply"
                    },
                    "comment": {
                        "value": "Dear authors,\n\nThanks for your detailed responses with additional experiments.  These efforts have addressed my concerns and questions so I decided to raise my score to 6, reflecting my support for this work."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5370/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700706313061,
                "cdate": 1700706313061,
                "tmdate": 1700706313061,
                "mdate": 1700706313061,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Dm1AvYbPpT",
            "forum": "Zj12nzlQbz",
            "replyto": "Zj12nzlQbz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5370/Reviewer_VqqH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5370/Reviewer_VqqH"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a metric, EigenScore, to measure whether the LLM can answer the question correctly. EigenScore is calculated from internal states of multiple responses to the same question. Furthermore, the authors design a feature clipping approach to truncate the extreme activations of internal states to reduce the overconfident generations. This benefits mitigating overconfident hallucination. The empirical results show the effectiveness of this work in detecting hallucinations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper studies a timely and important research problem in llm. Preventing hallucinations in inference time is a huge research challenge in the community.\n2. This paper is easy to follow.\n3. The proposed approach is effect in detecting hallucinations."
                },
                "weaknesses": {
                    "value": "1. The related work comparison is not comprehensive. There is no related work comparison between the internal-state detection approach. For example, [1] investigates mitigating the hallucinations by editing the internal states of llm. They first measure the possibility of generating hallucinations by observing the internal states and then editing those internal states. It might be better if the authors can compare their approach with the internal state-based hallucination detection approaches, e.g., [1].\n2. The llm models involve multiple transformation blocks; which block internal state do the authors investigate? It would make the paper more impactful if the authors could present the ablation on transformation blocks.\n\n[1] Li, Kenneth, et al. \"Inference-Time Intervention: Eliciting Truthful Answers from a Language Model.\" NeurIPS 2023."
                },
                "questions": {
                    "value": "1. I find that the evaluation metrics are related to fact-checking. I wonder if there are diverse metrics to evaluate the hallucinations."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5370/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699545150075,
            "cdate": 1699545150075,
            "tmdate": 1699636541771,
            "mdate": 1699636541771,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LyArUHhjti",
                "forum": "Zj12nzlQbz",
                "replyto": "Dm1AvYbPpT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5370/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5370/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer VqqH (R1) -- Part1"
                    },
                    "comment": {
                        "value": "**Comment** We thank the Reviewer VqqH (R1) for the insightful feedbacks. \n\n> **W1** The related work comparison is not comprehensive. There is no related work comparison between the internal-state detection approach. It might be better if the authors can compare their approach with the internal state-based hallucination detection approaches, e.g., [1].\n\n**Ans** Thanks for your suggestion. Considering that replicating the ITI [2] would be prohibitively expensive, we test the performance of our proposal and the baselines in the TruthfulQA dataset [1]. The results are presented in Table 1. For the ITI, we report the best performance in their paper. As can be seen, our proposal consistently outperforms the baseline methods and achieves comparable performance as ITI when we utilize 50 in-distribution prompts. \n\nIt's worth nothing that the ITI [2] relies on training 1024 binary classifiers in TruthfulQA datasets, and they report the best performance (83.3) in the validation set. Therefore, their best performance is better than our method which has not been trained on TruthfulQA. However, training on the validation set also limits the generalization of their method on other domains [2].\n\nTable 1: Performance comparison of different methods on TruthfulQA dataset. Accuracy is reported.\n| #prompt | Perplexity | LN-Entropy | LexialSim | SelfCKGPT | ITI* | **EigenScore** |\n|:-------:|:----------:|:----------:|:---------:|:---------:|:---------:|:--------------:|\n|    5    |    70.0    |    71.2    |    73.6   |    74.2   |  **83.3** |      76.7      |\n|    20   |    76.4    |    77.7    |    77.9   |    76.8   |  **83.3** |      79.5      |\n|    50   |    73.1    |    77.9    |    73.6   |   78.3    |  **83.3** |    **81.3**    |\n\nAs TruthfulQA is a very challenging dataset for LLMs, zero-shot inference results in poor performance. Therefore, we follow previous work [6] to utilize different number of in-distribution prompts during inference time. The results show that the performance could be significantly improved when we increase the number of prompts, which also explains why ITI performs good.\n\nBesides, to demonstrate the effectiveness of our proposal, we also compare our proposal with several competitive methods [3,4,5] that explore self-consistency for hallucination detection. The results in Table 2 demonstrate that our EigenScore consistently outperforms the the competitors by a large margin.\n\nTable 2: Performance comparison of EigenScore and and several competitive methods on CoQA dataset. AUC$_s$/AUC$_r$ score is reported.\n|       Methods      | SemanticEn |  SentSAR  | SelfCkGPT |   EigenScore  |\n|:------------------:|:----------:|:---------:|:---------:|:-------------:|\n| OPT-6.7B  |  63.1/71.7 | 69.8/72.2 | 70.2/74.1 | **71.9/77.5** |\n|  LLaMA-7B |  64.9/68.2 | 70.4/65.8 | 68.7/72.9 | **71.2/75.7** |\n| LLaMA-13B |  65.3/66.7 | 71.4/64.7 | 68.1/77.0 | **72.8/79.8** |\n\nFor more information about the experiments, please see the **Appendix A/B (Page13)** of our revised paper.\n\n***\n\n> **W2** The llm models involve multiple transformation blocks; which block internal state do the authors investigate? It would make the paper more impactful if the authors could present the ablation on transformation blocks.\n\n**Ans** We have demonstrated the performance of exporing EigenScore in different layers (transformer blocks) in **Fig. 3(b)**. And the detailed discussions are illustrated on **Page8** in our submission. The results show that using the sentence embedding in the shallow and final layers yields significantly inferior performance compared to using sentence embedding in the layers close to the middle. Besides, another interesting observation is that utilizing the embedding of the last token as the sentence embedding achieves superior performance compared to simply averaging the token embeddings, which suggests that the last token of the middle layers retain more information about the truthfulness of generations.\n\n***\n**Reference**\n\n[1] TruthfulQA: Measuring How Models Mimic Human Falsehoods\n\n[2] Inference-Time Intervention: Eliciting Truthful Answers from a Language Model\n\n[3] Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models\n\n[4] Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation\n\n[5] Shifting Attention to Relevance: Towards the Uncertainty Estimation of Large Language Models\n\n[6] Training a helpful and harmless assistant with reinforcement learning from human feedback"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5370/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700379253096,
                "cdate": 1700379253096,
                "tmdate": 1700379253096,
                "mdate": 1700379253096,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]