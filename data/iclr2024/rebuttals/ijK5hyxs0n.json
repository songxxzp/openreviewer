[
    {
        "title": "Graph Metanetworks for Processing Diverse Neural Architectures"
    },
    {
        "review": {
            "id": "Y1qkhkiTxM",
            "forum": "ijK5hyxs0n",
            "replyto": "ijK5hyxs0n",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6278/Reviewer_fJUi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6278/Reviewer_fJUi"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposed a new approach to represent neural networks and their weights using a so called parameter graph in which nodes represent neurons and edges represent weights/parameters of the neural network. The paper demonstrates how to build this graph for different kinds of architectures including non-trivial and very practical cases such as transformers. The paper formally shows useful properties of the approach such as permutation equivariance corresponding to neuron equivariance. Finally, the results in two experiments show better performance than the baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper has several good contributions:\n\n1. The paper addresses an interesting and promising topic of representing neural networks and their weights.\n2. The proposed parameter graph is a reasonable approach and looks much simpler than previous works such as DWSNets and NFN/NFT. It is similar to the concurrent work of Zhang et al. (2023), which is properly credited. \n3. Description of how to build graphs for different layer types such as convolution, self-attention and residual connections is very informative.\n4. The experiments show reasonably good results of the proposed approach.\n\nThe paper is also well written and organized."
                },
                "weaknesses": {
                    "value": "The paper have several weaknesses. I'm willing to revise the rating based on authors' response.\n\n1. The paper says that \"While our graphs are DAGs, we are free to use undirected edges\". Would not the direction of edges be a useful feature in some cases? For example, sometimes networks take multiple inputs and have multiple outputs so there is no way to differentiate input vs output unless edge direction is used.\n\n2. The computational complexity vs other approaches is not analyzed. Can the model encode large models in a feasible way? \n\n3. There are very few experiments. The authors could have more experiments following previous works from DWSNets, NFN, etc. or be creative in designing more novel experiments. For example, generation of new networks mentioned in the intro (Erkoc\u00b8 et al., 2023) could be a very appealing experiment.\n\n4. The experiments lack ablations to understand how the model behaves under different settings. These could be the number of layers/params in the graph metanet, different GNN architectures, different approaches to treat the bias term (e.g. comparing to Zhang et al. (2023)), ablating different components of the GNN in 2.3, etc.\n\n5. Source code is not available in the submission, which would be very helpful at least for how to build a graph given a neural network. Do the authors intend to open source the code?\n\n6. The purpose of Section 3 and Proposition 2 is a bit unclear, these could be replaced by more experiments or computational complexity analysis that are lacking.\n\n7. Section 5.2. lacks details. How the vector representation is obtained? Details of training the GNN are missing. More difficult INR tasks could be added."
                },
                "questions": {
                    "value": "1. Given that the same graph metanet can process diverse architectures, can the model trained in 5.1 be directly applied to large realistic models like ResNet-50 or large ViTs? For example, in the paper \"Zero-Cost Proxies for Lightweight NAS\" there are PyTorchCV networks with recorded accuracies. It would be very interesting to see how the results would look like for such challenging cases. Moreover, in Section 5.1 it would be interesting to see correlation performance of the methods from \"Zero-Cost Proxies for Lightweight NAS\" like gradnorm, which are very easy to compute. Another interesting experiment could be to track the predicted accuracy during training some network and see if it correlates well with the actual accuracy. Given that evaluation of large networks is very expensive, this approach could be an alternative way to track performance.\n\n2. Is it correct that the trained metanets cannot generalize to some architectures, for example to the kernels of larger size than seen during training because the edges would have more features? If yes, this should be clearly described in Limitations or another appropriate section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6278/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6278/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6278/Reviewer_fJUi"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6278/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698437060096,
            "cdate": 1698437060096,
            "tmdate": 1700771833210,
            "mdate": 1700771833210,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "q5MNe8w8nH",
                "forum": "ijK5hyxs0n",
                "replyto": "Y1qkhkiTxM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6278/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6278/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response 1/2"
                    },
                    "comment": {
                        "value": "> \u201c1. ... Would not the direction of edges be a useful feature in some cases? For example, sometimes networks take multiple inputs and have multiple outputs so there is no way to differentiate input vs output unless edge direction is used. \u201c\n\nWe add layer number (of both edges and nodes) as features for the input; this is mentioned in Appendix A. This allows the network to differentiate input and output nodes. Also, we can add edge direction as a feature (forward or backward), which is mentioned in Appendix A and found to be useful in the expressive power results in Appendix C.1.2.\n\n> \u201c2. The computational complexity vs other approaches is not analyzed. Can the model encode large models in a feasible way?\u201d\n\nAs message passing GNNs are generally efficient, and as much work has been done on GNNs, our methodology inherits these benefits. While we have not yet considered tasks on large networks, we note that GNNs are capable of training on graphs with 300 million edges (and hence networks with 300 million parameters) on a single GPU with 24 GB of RAM in 32-bit precision (e.g. https://arxiv.org/pdf/2110.14446.pdf). Thus, using multiple GPUs, GPUs with more RAM, and standard memory-saving tricks such as lower precision and gradient checkpointing, we believe that GMNs process input networks with billions of parameters. For inference, GMNs can ostensibly process input networks with tens of billions of parameters, due to lower memory cost than training.\n\n> \u201c3. There are very few experiments. The authors could have more experiments following previous works from DWSNets, NFN, etc. or be creative in designing more novel experiments\u2026\u201d\n\nAs explained in the general comment, we have added an additional set of experiments in our revised manuscript, including a 2D-INR editing task. This includes a comparison with NFN and the more recent NFT method [Zhou et al. 2023b].\n\nWe agree that there are many exciting applications for this approach, especially given our ability to handle heterogeneous architectures within a single trained model. While we hope to explore these, we emphasize that our current submission contains significant contributions in terms of the novel method that we proposed, our theoretical analysis of it, and the empirical experiments validating the method in practice.\n\n> \u201c4. The experiments lack ablations to understand how the model behaves under different settings\u2026\u201d\n\nWe agree that an ablation study would improve the empirical rigor of our work. We are working on this and hope to include it soon. Speaking informally, we found our metanets to be robust and effective across different choices of the GNN architecture, and choices of feature encoding. \n\n> \u201c5. Source code is not available in the submission, which would be very helpful at least for how to build a graph given a neural network. Do the authors intend to open source the code?\u201d\n\nUnfortunately, we are unable to provide source code at this time. We are in the process of preparing an open-source release, and hope to have this ready in time for the camera-ready publication. *In the meantime, we have added high level descriptions for our graph constructions in Appendix A.1.*\n\n> \u201c6. The purpose of Section 3 and Proposition 2 is a bit unclear, these could be replaced by more experiments or computational complexity analysis that are lacking.\u201d\n\nWe are disappointed that this contribution of our work was not clearer. We emphasize that these results are extensions/generalizations of established theorems presented in prior published work. Additionally, these results theoretically justify our application of graph neural networks as metanets.\n\nWhile we certainly agree that more experiments could strengthen our paper, and we have added more since submission, we stand by the inclusion of these theoretical results as a major contribution.\n\nSee our general comment for more on the value of our theory."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6278/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700332487938,
                "cdate": 1700332487938,
                "tmdate": 1700332487938,
                "mdate": 1700332487938,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8zrp5DugRC",
                "forum": "ijK5hyxs0n",
                "replyto": "P66qWopPua",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6278/Reviewer_fJUi"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6278/Reviewer_fJUi"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate authors' response which clarified some issues but also revealed other potential issues. \nAdditional 2D INR results are useful but indicating standard deviation would be beneficial, otherwise it's hard to compare numbers. Intentions to open-source the code are very welcome especially given additional concerns expressed below.\n\n**Node and edge features**\n\nAppendix A shows useful details of the graph construction, but the paper's main text would benefit from taking into account these details. For example, having in the main text \"we are free to use undirected edges\" while actually using \"edge direction\" in some results sounds misleading. \nIt's unclear if the node/edge features (described in Appendix A) compromise the equivariance properties of the metanet (proved in the main text) or generalization capabilities (e.g. OOD in Table 1)? \n- For example \"input neuron index, output neuron index\" should be explained more. It sounds like the indices are fixed for all input networks so the metanet outputs would be different depending on the neuron permutation. \nIt's also unclear how the metanet can generalize to wider networks given that neuron indices will be unseen (larger).\nAlso the \"Node and edge features\" paragraph is vague saying \"Some examples include\", so it remains mysterious if there are other node/edge features used.\n- Using \"layer number\" might be problematic too. For example, for networks with residual connections the residual branches are parallel (i.e. do not have any order), so it's unclear what kind of \"layer number\" is used in this paper and whether these features will preserve equivariance properties or not. Also the layer number features will probably prevent generalization to deeper networks?\n- There are \"layer type\" features, however what kind of types are predefined is unclear. Are there are only weight and bias type or other types like conv layer / norm layer / etc. Does using layer types imply that the metanet cannot generalize to unseen layer types?\n\nWithout ablations showing how node/features affect the results and how they affect equivariance, it's hard to understand what exactly makes the model more effective than the baselines (e.g. can these features improve the baselines? how much?).\nThe response does not address this concern effectively. For example, saying that some features are \"useful in the expressive power results\" while at the same time saying that \"our metanets to be robust and effective across different choices of the GNN architecture, and choices of feature encoding.\". \n\nGiven these concerns, the limitations and generalization capabilities of the proposed metanet could be communicated more clearly.\n\nRegarding \"Section 3\" the contributions are understood and are reasonable. The purpose was unclear in a sense that more experiments/ablations and details of the method in the main text would be more appreciated."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6278/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700675027356,
                "cdate": 1700675027356,
                "tmdate": 1700675027356,
                "mdate": 1700675027356,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lahixzZ0Mz",
                "forum": "ijK5hyxs0n",
                "replyto": "Y1qkhkiTxM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6278/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6278/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. We greatly appreciate you taking the time to read our rebuttal and paper in further detail.\n\n> Additional 2D INR results are useful but indicating standard deviation would be beneficial\n\nApologies for this oversight. We will of course include standard deviations in the final version, as we have with other empirical results.\n\n\nThank you for your detailed questions about the feature constructions that we use. This has highlighted that this part of the paper deserves further clarification, and we will certainly do so in a later revision. In general, none of these features are strictly necessary and simply serve the purpose of improving network expressivity without sacrificing equivariance. Here are brief responses to the specific points that you raised:\n\n> having in the main text \"we are free to use undirected edges\" while actually using \"edge direction\" in some results sounds misleading\n\nWe will clarify this in the main text. Certainly, our proposed approach works with and without edge direction feature encoding. Some of our theoretical results lean on edge direction features (without violating equivariance), but empirically we found that this is unnecessary.\n\n> It sounds like the indices are fixed for all input networks so the metanet outputs would be different depending on the neuron permutation\n\n(We will assume MLP input networks for ease of exposition, but the same argument can be generalized across all architectures.)\n\nTo clarify, neuron indices are not included for hidden neurons. We expose only the neuron indices for the input layer and output layer as these should not be included in the desired permutation equivariance. We assume that the input neurons come in some pre-fixed permutation, which is the case in typical learning settings. We (and prior work) are then only concerned with hidden neuron permutations. However, input/output permutations could also be trivially included if desired (by not exposing their neuron indices).\n\nWider networks (meaning more hidden units) are possible as the network cannot differentiate between any two neurons in the same hidden layer. This becomes a standard(-ish) application of GNNs generalizing to novel in-distribution graphs. We verify this empirically in our paper.\n\n> Using \"layer number\" might be problematic too\n\nIncluding the layer number is not necessary, but is helpful for empirical performance. Residual edges can be encoded using the layer index of the outgoing nodes (each residual connection certainly has an output neuron generating it). Layer number is identical for all neurons within a single layer, and thus these cannot be distinguished by the GNN, and equivariance is retained. For what it's worth, this same general idea was independently proposed in the concurrent work of Zhang et al., where learned positional encodings are used as features to identify each layer. We found that the simpler layer index was sufficient.\n\nWe agree that generalization to deeper networks is an interesting question. There are several ways that this could be tackled. For example, one could try encoding the layer index as a continuous value from 0->1 with 0 for the input layer and 1 for the output layer. Or indeed, you could remove layer index entirely as a feature and rely on the GNN to learn this via message passing.\n\n> There are \"layer type\" features, however what kind of types are predefined is unclear.\n\nWe include layer type features for all unique layer types that we can represent (e.g. linear, convolution, multi-head attention, etc.). The features are simply categorical variables, with each layer type (in some predefined order) being given values in increments of 1. More sophisticated choices could be explored.\n\nIf a novel layer type was provided at inference time, then we would not generally expect our metanets to generalize well when these features are being used. But this is intuitive, as the network was never trained on networks with these layer types. However, one could forego the layer-type features entirely --- paying the cost of reduced expressiveness for improved generalization.\n\nWe agree that further empirical investigation would help to build an increased understanding of our metanets. But we would like to emphasize again that we have presented a novel and effective approach to designing equivariant metanetworks. While we will continue to strengthen our empirical evaluation by adding ablations and other improvements, the existing results show consistently high performance of our method on several tasks, alongside valuable new capabilities that other methods cannot achieve: generalization to different input architectures and handling diverse and heterogeneous input networks."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6278/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700692283855,
                "cdate": 1700692283855,
                "tmdate": 1700692309111,
                "mdate": 1700692309111,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "IuoTvq8GNl",
            "forum": "ijK5hyxs0n",
            "replyto": "ijK5hyxs0n",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6278/Reviewer_ffRA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6278/Reviewer_ffRA"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a novel approach called Graph Metanetworks (GMNs) that utilizes graph neural networks to process diverse neural architectures. The authors address the challenge of generalising metanetwork architectures to different types of networks by building graphs representing the input neural networks and processing them using graph neural networks. The GMNs are proven to be expressive and equivariant to parameter permutation symmetries, and they demonstrate superior performance on various metanetwork tasks across different neural network architectures."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The proposed GMNs can generalise to different types of neural architectures. Unlike previous works e.g. Navon et al. (2023) that were tailored to specific networks, the GMNs can handle a wide range of architectures, including those with complex modules such as attention blocks. Upon the main idea of processing weights with graph networks in Zhang et al. (2023), this paper extends the method to a variety of neural layers that are common in modern neural architectures.\n\nThe authors also provide a proof of the expressiveness and equivariance of GMNs to parameter permutation symmetries, which is an important property for metanetworks."
                },
                "weaknesses": {
                    "value": "- Although different neural layers and architectures can be encoded as the proposed parameter graph representation, however, the information in the spatial domain is missing (e.g. translation equivariance and receptive field in ConvNets). This could be an inherent and general limitation of the proposed method as well as other related work on weight domain.\n\n- There are also a variety of non-parametric operations in feed-forward neural networks that are not addressed by the paper. E.g. pooling layers (especially max pooling), atrous convolutions, padding and so on. I can expect these operations can be encoded as some additional indicating features but I wonder if there are better and less artificial solutions.\n\n- Comparison with Navon et al. (2023) only on the 1D sine curve toy dataset, but not on the more challenging 2D INR image tasks in the original paper. Other experiments take DeepSets and DMC as baselines which are 2017 and 2020 papers."
                },
                "questions": {
                    "value": "- Since graph can encode a variety of architectures, I wonder if it is possible to generalise and especially extrapolate prediction to unseen architecture, e.g. the test MLP/CNN is wider or deeper than all training examples. \n\n- I do not see pooling layers discusses. Are there pooling layers involved in the networks in the dataset?\n\n- How to encode atrous/dilated convolutions?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6278/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6278/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6278/Reviewer_ffRA"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6278/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698834509762,
            "cdate": 1698834509762,
            "tmdate": 1700788136429,
            "mdate": 1700788136429,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "i0OQ7XPAd6",
                "forum": "ijK5hyxs0n",
                "replyto": "IuoTvq8GNl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6278/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6278/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> \u201cAlthough different neural layers and architectures can be encoded as the proposed parameter graph representation, however, the information in the spatial domain is missing (e.g. translation equivariance and receptive field in ConvNets). This could be an inherent and general limitation of the proposed method as well as other related work on weight domain.\u201d\n\nWe are not sure what the reviewer means here, a clarification may be helpful. For instance, the receptive field is readily available in the parameter graph representation, since the kernel sizes are encoded, so any metanetwork can trivially determine the receptive field. Notably, we include node and edge type features, so for instance a node corresponding to a convolutional layer channel has a node feature indicating that it is a convolution node.\n\n\n> \u201cThere are also a variety of non-parametric operations in feed-forward neural networks that are not addressed by the paper. E.g. pooling layers (especially max pooling), atrous convolutions, padding and so on. I can expect these operations can be encoded as some additional indicating features but I wonder if there are better and less artificial solutions.\u201d\n\nOur method is able to represent a wide variety of network architectures while retaining expressivity. **We already handle many more operations than previous work**, e.g. NFN and DWSNets only handle linear or convolutional layers (and not normalization, residuals, attention, group equivariant linear maps, or spatial parameter grids as we handle).  This is achieved through a combination of parameter graph design and feature encoding. \n\nFor instance, with regards to spatial pooling layers, these are essentially a no-op in the parameter graph. In the sense that the same parameter graph accurately captures the equivariances that we\u2019re seeking whether pooling is included or not. In fact, the image classifiers that we process in Section 5.1 have a global mean pooling (we have added this note to the revision).\n\nFollowing our current design philosophy, adding features is exactly the right solution to indicate to the GNN that a pooling layer has been applied. It is also effectively free in terms of computation cost. We are not sure what the reviewer means when they say that adding features is \u201cartificial\u201d, but we note that adding features is very efficient (essentially zero cost), general, and simple.\n\n\n> \u201cComparison with Navon et al. (2023) only on the 1D sine curve toy dataset, but not on the more challenging 2D INR image tasks in the original paper. Other experiments take DeepSets and DMC as baselines which are 2017 and 2020 papers.\u201d\n\nAs noted in the general comment, we now include experiments on 2D INR editing tasks from [Zhou et al. 2023a]. Our method outperforms the NFN baseline (NFN-HNP is essentially the same approach as DWSNets) but is beaten by the more recent specialized NFT architecture on only one of the tasks. We expect this gap can be closed by using more powerful/sophisticated graph neural network architectures, but emphasize at this time that our more general approach is applicable across a wide range of input networks while NFN/NFT are designed only for simple MLPs and simple CNNs (without e.g. normalization layers or residual connections).\n\nWe have also included additional results using NFN and NFT to predict generalization on a fixed CNN architecture. Our more general approach outperforms or matches the performance of these specialized baselines.\n\n> \u201cSince graph can encode a variety of architectures, I wonder if it is possible to generalize and especially extrapolate prediction to unseen architecture, e.g. the test MLP/CNN is wider or deeper than all training examples.\u201d\n\n**Our initial submission already had experiments** that evaluate the capability of our approach to generalize to unseen architectures. Please see the \u201cOOD\u201d setting in Section 5.1, and Figure 5. Notably, our GNNs generalize well to wider networks than those seen in training, for all of the diverse architectures: 2D CNN, 1D CNN, DeepSets, ResNet, ViT.\n\n> \u201cI do not see pooling layers discusses. Are there pooling layers involved in the networks in the dataset?\u201d\n\nYes, we use mean pooling across the spatial dimensions before the fully connected prediction layers for the image classifiers in Section 5.1. We have noted this in the revision.\n\n> \u201cHow to encode atrous/dilated convolutions?\u201d\n\nOne way to do this is to view the atrous/dilated convolution as a larger sparse kernel in the standard way, with zeros in many entries. E.g. a 3x3 kernel with dilation 2 can be viewed as a 5x5 kernel with zeros in all but 9 entries.\n\nAnother, more efficient way to encode these is to add another edge feature indicating the dilation factor for the convolutional weights. E.g. standard convolutions get an additional edge feature of 1, while convolution with dilation factor $k$ gives an edge feature of $k$."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6278/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700332456310,
                "cdate": 1700332456310,
                "tmdate": 1700379683058,
                "mdate": 1700379683058,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "erBZD8whKL",
                "forum": "ijK5hyxs0n",
                "replyto": "i0OQ7XPAd6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6278/Reviewer_ffRA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6278/Reviewer_ffRA"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the response. \n\nTo clarify my first point and give an example, convolution layers incorporate some spatial inductive bias with its filter and parameter sharing paradigm. However with the proposed graph representation, first the convolution filter is flattened to 1D features so the 2D spatial relationship of filter weights is lost, which might be geometrically meaningful (e.g. can identify gradients at a specific 2D direction)."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6278/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700509280243,
                "cdate": 1700509280243,
                "tmdate": 1700509280243,
                "mdate": 1700509280243,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "q1oKs2yegE",
            "forum": "ijK5hyxs0n",
            "replyto": "ijK5hyxs0n",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6278/Reviewer_ZWAE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6278/Reviewer_ZWAE"
            ],
            "content": {
                "summary": {
                    "value": "The idea is to construct directed acyclic graphs (DAGs) that are also \u2018parameter graphs\u2019 (which represent parameters as weighted edges) that represent neural networks and feed them through a simple message-passing graph neural network \u2018metanetwork\u2019. According to them, they are distinguished from other works that do this by a few design choices of the graph construction. They show that the way they construct neural network DAGs is invariant to the order in which neurons on the same layer (ones between which order should not matter) are embedded, which is stated to be an issue with the way some other metanets represent networks. They state how they represent different kinds of layers in their \u2018parameter graphs\u2019, and show that it can represent layers others cannot (such as normalization layers, according to them) and have less scaling issues with parameter-sharing layers such as convolution and attention layers. They evaluate by attempting to learn the prediction accuracy of datasets of image classifier networks on CIFAR-10, one is of 2d CNNS, and one is of varying models (CNNs, ViTs, ResNets, etc) with competing methods (those which can represent the inputs)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "It should be noted that I am not familiar with other papers regarding metanets and am basing my assessment largely on information from this paper. That being said, given that they represent the current state of this area fairly, this seems to be an impressive paper. They appear to address scaling issues with similarly expressive network representations and expand the variety of representable networks, which seem to be great contributions."
                },
                "weaknesses": {
                    "value": "They address most of my concerns I had while reading, including some tests to compare against the newer state of the art metanets mentioned that were excluded from most of the result due to not being able to represent certain types of layers. Notably a cited competing method \u2018NFN\u2019 was left out from this, though seems to be addressed in the appendix, and as it apparently deals exclusively with MLPs I believe it is fair not to compare with the proposed method."
                },
                "questions": {
                    "value": "Please see weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6278/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698941044826,
            "cdate": 1698941044826,
            "tmdate": 1699636687285,
            "mdate": 1699636687285,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xOSU8QvZms",
                "forum": "ijK5hyxs0n",
                "replyto": "q1oKs2yegE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6278/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6278/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> \u201cThey address most of my concerns I had while reading, including some tests to compare against the newer state of the art metanets mentioned that were excluded from most of the result due to not being able to represent certain types of layers. Notably a cited competing method \u2018NFN\u2019 was left out from this, though seems to be addressed in the appendix, and as it apparently deals exclusively with MLPs I believe it is fair not to compare with the proposed method.\u201d\n\nWe appreciate the author\u2019s positive review. We have now run additional experiments on simple input networks (simple CNNs and MLPs) to compare our method against both NFN and the more recent NFT [Zhou et al. 2023b]; see the general comment for the results. In summary, our Graph Metanetworks generally outperform or match existing metanetworks on these tasks with simple input networks. Further, as you mention, our Graph Metanetworks can process significantly more diverse and complex input networks than these methods.\n\nWe hope that this addresses any remaining concerns, and would appreciate the opportunity to answer any other questions that you have."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6278/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700332176039,
                "cdate": 1700332176039,
                "tmdate": 1700332176039,
                "mdate": 1700332176039,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]