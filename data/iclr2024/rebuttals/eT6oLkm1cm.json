[
    {
        "title": "Annealing Self-Distillation Rectification Improves Adversarial Training"
    },
    {
        "review": {
            "id": "nhQEleA5O1",
            "forum": "eT6oLkm1cm",
            "replyto": "eT6oLkm1cm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4379/Reviewer_VvcZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4379/Reviewer_VvcZ"
            ],
            "content": {
                "summary": {
                    "value": "The authors have proposed a self-distillation method to generate soft labels in AT. The soft labels are crafted by the natural logits of the teacher model and the ground-truth. They clip the weight of soft labels to keep the correct classes having the highest confidence. Besides, Annealing and temperature are introduced to adjust the labels adaptive. Empirical evaluation on  three benchmark datasets shows its improvement on robustness based on baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. A lot of experiments have been done to show its effectiveness on robustness compared with other knowledge distillation methods, which makes it credible.\n2. The paper is well-organized and easy to understand.\n3. The paper has shown abunbant experimental details, with good reproducibility."
                },
                "weaknesses": {
                    "value": "1. The motivation is too easy and there seem no new insights in the manuscript.\n2. The results on Table 2 show that on WRN on three datasets, AT+ADR has a weaker performance compared with AT+WA, and AT+WA+ADR also has a weaker performance compared with AT+WA+AWP. It may demonstrte ADR doesn't work well compared with WA and AWP on large models.\n3. The comparison with RobustBench in the paper is not fair, results on AT+ADR rather than AT+WA+AWP+ADR should be used. In this comparison, ADR achieves atate-of-the-art robust performance on ResNet-18 but has a poor performance on WRN."
                },
                "questions": {
                    "value": "Because it has little novelty, its performance is now the most significant evaluation indicator. Its performance on WRN is not competitive enough. I will increase my score if better results come out.\n\n=========After rebuttal=============\nThe authors' response address most of my concerns. I thus am willing to increase the rating to 6"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4379/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4379/Reviewer_VvcZ",
                        "ICLR.cc/2024/Conference/Submission4379/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4379/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698394780135,
            "cdate": 1698394780135,
            "tmdate": 1700640570656,
            "mdate": 1700640570656,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "aqm3gQzxXt",
                "forum": "eT6oLkm1cm",
                "replyto": "nhQEleA5O1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4379/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4379/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response for Reviewer VvcZ"
                    },
                    "comment": {
                        "value": "Dear reviewer VvcZ,\n\nThank you for acknowledging our work's experimental result as credible and we are glad that you find it well-organized. We address your concerns as the following:\n\n**Q: The motivation is too easy and there seem no new insights in the manuscript.**\n\nWe highlight that we observe the key property that makes the robust model reliable in Section 3.2, the robust model is well-calibrated and consistent with its output when input is perturbed, this motivates us to design a unified rectified label to enhance the efficacy of adversarial defense. \n\nIn our observations, we discover that adversarially trained networks consistently exhibit output consistency between adversarial images and their clean counterparts. Some previous studies suggest the incorporation of dual targets during adversarial training, encouraging models to learn from both an adversarially trained model on adversarial images and a naturally trained model on normal images (Cui et al., 2021; Zhao et al., 2022). While seemingly intuitive in mitigating the trade-off between robustness and accuracy, our observations challenge the appropriateness of this approach. We contend that learning from two targets, one with high confidence from the normal model and the other with lower confidence from the adversarially trained model, is not conducive to achieving consistency for images with noise. Therefore, we propose composing a rectified label in ADR and urge that all images within the $l_p$ norm neighborhood of a given input should be targeted using the rectified label rather than the one-hot one. The observations in Section 3 motivate us to design ADR that effectively prevents the network from becoming over-confident and provides semantically aware smoothing rather than a uniform one. We believe that we do provide some useful insights for the community.\n\n**Q: The results in Table 2 show that on WRN on three datasets, AT+ADR has a weaker performance compared with AT+WA, and AT+WA+ADR also has a weaker performance compared with AT+WA+AWP. It may demonstrate ADR doesn't work well compared with WA and AWP on large models.**\n\nADR with WA and AWP should be viewed as complementary rather than competing approaches. These methods target different aspects of enhancing adversarial training.\n\nIn the realm of adversarial defense, we can encapsulate the directions of research aimed at enhancing the efficacy of adversarial training into the following key areas:\n* Different training objectives. e.g. PGD-AT (Madry et al., 2018), TRADES (Zhang et al., 2019).\n* Encourage a flatter model-weight landscape. e.g.  WA (Izmailov et al. 2018; Chen et al. 2021), AWP (Wu et al. 2020).\n* Generate additional data or create auxiliary examples. e.g. use DDPM data (Gowal et al., 2020; Rebuffi et al., 2021a), HAT (Rade et al., 2021)\n* Create effective data augmentation strategies to enhance robustness. e.g. CutMix (Yun et al. 2019; Rebuffi et al. 2021a), DAJAT (Addepalli et al. 2022)\n* Knowledge distillation. e.g. ARD (Goldblum et al., 2020b), RSLAD (Zi et al., 2021), IAD (Zhu et al., 2022)\n\nWhile WA and AWP concentrate on smoothing the model weight landscape, ADR focuses on smoothing the training labels. As their goals are distinct and not in conflict, direct comparisons between AT+ADR and AT+WA, or between AT+WA+ADR and AT+WA+AWP, may not be entirely fair. Their combined effects may offer a more comprehensive approach to improving adversarial training, addressing both the model's weight landscape and the label-smoothing aspects, bringing the overall robustness enhancement.\n\n**Q: The comparison with RobustBench in the paper is not fair, results on AT+ADR rather than AT+WA+AWP+ADR should be used. In this comparison, ADR achieves state-of-the-art robust performance on ResNet-18 but has a poor performance on WRN.**\n\nWe do not agree that AT+ADR rather than AT+WA+AWP+ADR should be used for the RobsutBench comparison.   It is acknowledged that RobustBench, a benchmark for adversarial robustness, often includes methods that integrate several techniques rather than reporting solely on individual methods. As the common research directions for adversarial training that we provided in our previous reply, the methods from each direction can be combined as complementary to obtain better results.  Ultimately, the goal is to employ a combination that best suits the experimental objectives and the overall enhancement of adversarial robustness.\n\nFor reference, Zhang et al. (2022), Rebuffi et al. (2021a), and Rade & Moosavi-Dezfooli (2022), report the results combined with WA. \nAddepalli et al. (2022), Chen & Lee (2021), and Jia et al. (2022)\nreport their results combined with AWP."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4379/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700527995656,
                "cdate": 1700527995656,
                "tmdate": 1700527995656,
                "mdate": 1700527995656,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tR2cTdh3ZD",
                "forum": "eT6oLkm1cm",
                "replyto": "nhQEleA5O1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4379/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4379/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response for Reviewer VvcZ (continued)"
                    },
                    "comment": {
                        "value": "**Q: The performance on WRN is not competitive enough.**\n\nWe acknowledge that the performance reported on WRN may not be competitive enough in some cases. This phenomenon is primarily attributed to our deliberate decision to maintain consistency in experimental settings across different models, eliminating the need to report distinct hyperparameters for each model.\n\nTo ensure uniformity, we employ the same $\\tau$ and $\\lambda$ schedule for both ResNet-18 and WRN-34-10, and the reported temperature and label interpolation factor are determined based on the evaluation of performance on ResNet-18. It is crucial to note that the chosen parameters may not represent the optimal fit for WRN-34-10, as the most effective parameters can vary based on architecture and specific settings, as previously mentioned in our limitations. We will investigate an automatic way to determine the optimal parameter in our future research.\n\nDue to we only have limited time and the substantial computational demands associated with adversarial training on expansive models like WRN, we only provide a full comparison on CIFAR-10 for reference. We can provide more experiment results in the final version of our paper.\nIn the experiment, we keep $\\lambda$  scheduling from 0.7 to 0.95 same as the original paper but adjust $\\tau$ to anneal from 1.5 to 1 instead of the original 2.5 to 2.\n\n***CIFAR-10***\n|   | AutoAttack  |   Standard Acc. |\n|---|---|---|\n| AT+ADR   (original) | 53.25 | 84.67 |\n| AT+ADR | 53.54 | 85.58 |\n| AT+WA+ADR   (original) | 54.10 | 82.93 |\n| AT+WA+ADR | 54.14 | 83.10 |\n| AT+WA+AWP+ADR   (original) | 55.26 | 86.11 |\n| AT+WA+AWP+ADR | 55.27 | 86.90 |\n\nAn additional CIFAR-100 result when we anneal $\\tau$ from 1.25 to 1 instead of 1.5 to 1.\n\n***CIFAR-100***\n|   | AutoAttack  |   Standard Acc. |\n|---|---|---|\n| AT+ADR   (original) | 29.35 | 59.76 |\n| AT+ADR | 29.79 | 60.17 |\n\nA larger architecture generally requires a lower temperature to achieve its best performance, it may be because of their superior capability to learn the inter-class relationship within the data.\nWe can observe that when selecting more suitable hyper-parameters for WRN-34-10, both robust and clean accuracy can be further enhanced."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4379/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700529057528,
                "cdate": 1700529057528,
                "tmdate": 1700529057528,
                "mdate": 1700529057528,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "plkOfmqEEm",
                "forum": "eT6oLkm1cm",
                "replyto": "tR2cTdh3ZD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4379/Reviewer_VvcZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4379/Reviewer_VvcZ"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "Dear authors\nThanks for your detailed explanation and clarification. The supplementary explanations resolve my confusions. Based on the experimental results provided, I reconsider the proposed method as valid and relatively innovative. Thus, I will raise the rating."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4379/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700640431978,
                "cdate": 1700640431978,
                "tmdate": 1700640431978,
                "mdate": 1700640431978,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "2LVt7hrlCH",
            "forum": "eT6oLkm1cm",
            "replyto": "eT6oLkm1cm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4379/Reviewer_YNYz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4379/Reviewer_YNYz"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a novel adversarial training scheme where the ground-truth labels are rectified with an EMA teacher network. The experimental results show that the proposed method achieves a better accuracy-robustness trade-off with smaller overfitting gaps than the baselines. The proposed method can also be integrated with other AT approaches and brings further robustness boost."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- I appreciate the simplicity of the proposed method. It should be easy to implement and it can be used with other AT techniques.\n    \n- The paper presents a clear motivation for the proposed method with experimental results. I also find the idea behind inspiring as it is somewhat consistent with my understanding of AT, i.e., easier training (less adversarial/weaker signal) leads to better results.\n    \n- It is shown that the technique brings small additional training time.\n    \n- The paper is well-written with nice figures and a clear structure."
                },
                "weaknesses": {
                    "value": "- Considering the simplicity of the method, I think the experiments are not very extensive. More datasets (e.g., ImageNet), attacks, and especially baseline AT methods should be considered.\n    \n- The self-distillation brings high memory cost, which I believe would be a main limitation for the practical use of the proposed method.\n    \n- It seems that this work is not the first to reveal that robust models are better calibrated, yet the authors conclude this finding as one of the major contributions of this paper."
                },
                "questions": {
                    "value": "Did you use any pretrained models for initialization? I wonder how ADR works when using pretraining models."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4379/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4379/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4379/Reviewer_YNYz"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4379/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698752434400,
            "cdate": 1698752434400,
            "tmdate": 1699636410711,
            "mdate": 1699636410711,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "j8oXEd9JwE",
                "forum": "eT6oLkm1cm",
                "replyto": "2LVt7hrlCH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4379/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4379/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response for Reviewer YNYz"
                    },
                    "comment": {
                        "value": "Dear reviewer YNYz,\n\nWe thank you for your valuable comments. We are glad that you recognize our empirical contribution and find our paper well-written. We provide further clarifications on the concerns that you raise.\n\n**Q: Considering the simplicity of the method, I think the experiments are not very extensive. More datasets (e.g., ImageNet), attacks, and especially baseline AT methods should be considered.**\n\nThank you for your suggestion. However, despite the apparent simplicity of ADR, it is essential to note that the training cost associated with ADR is comparable to other multi-step adversarial training methodologies, which are recognized for demanding computationally intensive processes. Our observations suggest that very few works employing multi-step adversarial training methodologies have presented results on the extensive ImageNet dataset. Instead of ImageNet, we present results on TinyImageNet-200, a representative subset, to underscore the practical effectiveness of our approach to real-world data.\nIn our comparative analysis, the related works we compared (Rebuffi et al. 2021b; Yun et al. 2019; Rebuffi et al. 2021a; Addepalli et al. 2022; Li & Spratling 2023; Rade & Moosavi-Dezfooli, 2022; Gowal et al., 2020; Cui et al., 2021; Sehwag et al., 2022; Jia et al., 2022; Chen & Lee, 2021) refrain from experimenting with ImageNet and limit their baselines primarily to PGD-AT, TRADES with WA, and AWP. Consequently, we assert that the experiments we provide offer a comprehensive and representative baseline for evaluation.\n\nAcknowledging the constraints imposed by time and resource limitations during this rebuttal period, we regret our inability to present new results on additional datasets. However, we concur with the notion that augmenting experiments on larger datasets, coupled with diverse methods, would greatly benefit the research community. We commit to providing more comprehensive results in future work.\n\n**Q: The self-distillation brings high memory costs, which I believe would be a main limitation for the practical use of the proposed method.**\n\nWhile we acknowledge the additional memory costs associated with maintaining an EMA teacher for rectified distribution, it is crucial to underscore that the ADR method does not incur higher memory usage compared to previous techniques. Notably, ADR remains competitive in terms of performance without necessitating increased memory allocation. For instance, WA upholds a weight average copy of network parameters to attain a smoother model weight landscape for enhanced robustness. AWP also maintains an additional copy of the gradient direction at each step to calculate the updated direction for models. Consequently, ADR demonstrates competitive performance levels when deployed under equivalent resource constraints as its counterparts.\n\n**Q: It seems that this work is not the first to reveal that robust models are better calibrated, yet the authors conclude this finding as one of the major contributions of this paper.**\n\nIn our analysis of Section 3, our findings indicate that robust models consistently demonstrate superior calibration and output stability across diverse input conditions. This aligns with the insights presented by Grabinski et al. (2022), as referenced in our paper. However, it is noteworthy that Grabinski et al. (2022) primarily conduct empirical analyses on a range of adversarially trained networks (comprising 71 models) without leveraging this characteristic to enhance adversarial robustness. In contrast, ADR is inspired by this observation and effectively enhances robustness performance.\n\nNotably, a comprehensive survey of existing literature reveals an absence of prior discussions on the phenomenon wherein adversarially trained networks consistently exhibit output consistency between adversarial images and their clean counterparts. Previous studies suggest the incorporation of dual targets during adversarial training, encouraging models to learn from both an adversarially trained model on adversarial images and a naturally trained model on normal images (Cui et al., 2021; Zhao et al., 2022). While seemingly intuitive in mitigating the trade-off between robustness and accuracy, our observations challenge the appropriateness of this approach. We contend that learning from two targets, one with high confidence from the normal model and the other with lower confidence from the adversarially trained model, is not conducive to achieving consistency for images with noise. Thus, we emphasize our contribution to these observations, underscoring the significance of designing a unified rectified label for all samples (both clean and adversarial images) used in adversarial training. We believe that this insight has the potential to benefit the research community dedicated to advancing adversarial robustness through the formulation of more effective training objectives."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4379/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700443403096,
                "cdate": 1700443403096,
                "tmdate": 1700443403096,
                "mdate": 1700443403096,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dux2KD530t",
                "forum": "eT6oLkm1cm",
                "replyto": "2LVt7hrlCH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4379/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4379/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response for Reviewer YNYz (continued)"
                    },
                    "comment": {
                        "value": "**Q: Did you use any pre-trained models for initialization? I wonder how ADR works when using pretraining models.**\n\nThanks for your interesting suggestion, we leverage pre-trained models for the initialization of the ADR process. Specifically, we employ ResNet-18 on the CIFAR-10 dataset, adhering to all other parameter settings outlined in Section 5.1. The key variation lies in the initialization of the model with either a standard or PGD-AT pre-trained model, followed by subsequent ADR training. Our reporting is based on the optimal checkpoint obtained during our comprehensive evaluation.\nMoreover, to offer a more comprehensive view of the observed trends, we present line charts depicting the performance trajectories with both clean and robust accuracy in Figure 10. These visual representations are included in the Appendix L of our updated paper for enhanced clarity and accessibility.\n\nThe experimental findings reveal a consistent trend wherein ADR consistently outperforms the baseline in terms of robust accuracy, regardless of whether it is initialized with a pre-trained model or not. The utilization of either a standard model or a PGD-AT pre-trained weight as an initialization fails to further augment the maximum robustness achievable through ADR. Nevertheless, it is noteworthy that employing an AT pre-trained checkpoint results in a notable enhancement of standard accuracy by 1.25% when compared to training with random initialization. This outcome underscores the potential for mitigating the accuracy-robustness tradeoff, indicating the feasibility of achieving improved performance by utilizing an AT pre-trained model during the initialization phase.\n\n|   | AutoAttack  | Standard Acc. |\n|---|---|---|\n| AT|48.81 |82.52 |\n|AT + ADR|50.38|82.41|\n|AT + ADR + standard pre-trained initialization | 49.45 | 82.22 |\n|AT + ADR + AT pre-trained initialization | 50.11 | 83.66 |"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4379/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700443833468,
                "cdate": 1700443833468,
                "tmdate": 1700443833468,
                "mdate": 1700443833468,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Zh1CIZm5ux",
                "forum": "eT6oLkm1cm",
                "replyto": "dux2KD530t",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4379/Reviewer_YNYz"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4379/Reviewer_YNYz"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for your response.\n\n> Nevertheless, it is noteworthy that employing an AT pre-trained checkpoint results in a notable enhancement of standard accuracy by 1.25% when compared to training with random initialization\n\nI want to make sure if this means AT+AT pre-trained initialization results in 82.52%+1.25%=83.77% standard accuracy. If so, what is the AA accuracy?\n\nI also would like to know how you obtain the AT pre-trained initialization."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4379/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700446249883,
                "cdate": 1700446249883,
                "tmdate": 1700446249883,
                "mdate": 1700446249883,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dYm6QxNNKc",
                "forum": "eT6oLkm1cm",
                "replyto": "j8oXEd9JwE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4379/Reviewer_YNYz"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4379/Reviewer_YNYz"
                ],
                "content": {
                    "comment": {
                        "value": "> However, it is noteworthy that Grabinski et al. (2022) primarily conduct empirical analyses on a range of adversarially trained networks (comprising 71 models) without leveraging this characteristic to enhance adversarial robustness.\n\nI am not saying that this work is not different from previous works. I'm just saying that I think it's inappropriate to claim that \"revealing robust models are better calibrated\" is a contribution of this paper (maybe \"confirming\" is).\n\n> Notably, a comprehensive survey of existing literature reveals an absence of prior discussions on the phenomenon wherein adversarially trained networks consistently exhibit output consistency between adversarial images and their clean counterparts.\n\nCan you show the reference to this statement?\n\n> While seemingly intuitive in mitigating the trade-off between robustness and accuracy, our observations challenge the appropriateness of this approach...\n\nI understand that this is a valuable finding and an important motivation for the proposed method. But I think the current summarized contribution#1 omits the important part of this motivation. Therefore I would suggest refining this paragraph."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4379/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700456081157,
                "cdate": 1700456081157,
                "tmdate": 1700456081157,
                "mdate": 1700456081157,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AMiRUOldXb",
                "forum": "eT6oLkm1cm",
                "replyto": "3CKOsEgM3B",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4379/Reviewer_YNYz"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4379/Reviewer_YNYz"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for your answers.\n\n> We are not aware of studies explicitly emphasizing the aspect of output consistency in adversarially trained models as in our current work to the best of our knowledge. Therefore, we are unable to provide a reference for this specific aspect.\n\nBy \"this statement\" I meant \"a comprehensive survey of existing literature reveals an absence of prior discussions on the phenomenon ...\" So, I think at least you could show the title of this survey and preferably how it reveals the absence of the discussion."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4379/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700541390421,
                "cdate": 1700541390421,
                "tmdate": 1700541390421,
                "mdate": 1700541390421,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9DJ9Ea6Mk5",
                "forum": "eT6oLkm1cm",
                "replyto": "yGd58ufcTT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4379/Reviewer_YNYz"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4379/Reviewer_YNYz"
                ],
                "content": {
                    "comment": {
                        "value": "Now I understand that \"a comprehensive survey\" refers to a survey done by the authors. I was thinking that there's a survey paper.\n\nAfter I read other reviews and the authors' responses, I decided to keep my score as borderline accept."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4379/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700701473341,
                "cdate": 1700701473341,
                "tmdate": 1700701473341,
                "mdate": 1700701473341,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "j7GK7zComL",
            "forum": "eT6oLkm1cm",
            "replyto": "eT6oLkm1cm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4379/Reviewer_EopP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4379/Reviewer_EopP"
            ],
            "content": {
                "summary": {
                    "value": "The paper investigates the phenomenon of robust overfitting and reports that robust models exhibit outputs more calibrated compared to standard models. It further proposes a label smoothing scheme for mitigating overfitting in adversarial training via employing model weight averaging, annealed interpolation and softmax temperature. Experimental results indicate robustness gains and a reduction of the severity of robust overfitting."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The experiments performed are extensive and incorporates several baseline methods for comparison.\n- Modifying self-distillation EMA (weight averaging) with annealed interpolation and softmax temperature is an interesting idea."
                },
                "weaknesses": {
                    "value": "- While the method is shown to increase the robustness and mitigates overfitting, other experimental results seem to lack significance for drawing conclusions (weight loss lanscape, effectiveness of temperature and interpolation parameters).\n- The relation to other similar methods investigated is not clear (see questions)."
                },
                "questions": {
                    "value": "- In Table 2, test accuracies of ADR combined with WA and AWP are presented. From the results it seems that each method contributes a (roughly) similar amount of robustness gain. Could the authors comment on whether they consider ADR, WA, AWP to be complementary?\n- In the motivation (subsection 4.1.) it is stated that the aim is 'to design a label-softening mechanism that properly reflects the true distribution'. In the presented approach the teacher network provides the probability distribution with which the one-hot label is interpolated. Can the authors comment on why the teacher network (model weight average of student) reflects the 'true distribution'?\n- The results presented in Figure 5 show the results of a hyperparameter study for the interpolation and temperature parameter. It is concluded that annealing both parameters is beneficial for robustness. Do the authors consider the differences (e.g. in the row on $\\lambda$ annealing) significant enough to draw such conclusions?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4379/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4379/Reviewer_EopP",
                        "ICLR.cc/2024/Conference/Submission4379/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4379/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698793479255,
            "cdate": 1698793479255,
            "tmdate": 1700683800279,
            "mdate": 1700683800279,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CSLmNXgvt7",
                "forum": "eT6oLkm1cm",
                "replyto": "j7GK7zComL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4379/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4379/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response for Reviewer EopP"
                    },
                    "comment": {
                        "value": "Dear reviewer EopP:\n\nThank you for your reviews, we are glad that you find our idea interesting. We address your questions in the following.\n\n**Q: While the method is shown to increase the robustness and mitigate overfitting, other experimental results seem to lack significance for drawing conclusions (weight loss landscape).**\n\nSeveral prior studies, including works by Wu et al. (2020) and Stutz et al. (2021), have established a correlation between the flatness of a model's weight landscape and its adversarial robustness. A smooth weight landscape indicates that the model produces a more stable output when subjected to slight weight perturbations, resulting in minimal changes in loss, as illustrated in Figure 4. Diverse methodologies have been proposed in the literature to enhance robustness by inducing a flatter weight landscape. (Wu et al. 2020; Chen et al. 2021; Gowal et al. 2020; Zhang et al. 2019). It is noteworthy that our baseline methods such as WA and AWP inherently contribute to landscape smoothness through their respective designs. In this context, the observation that ADR achieves a flatter weight loss landscape provides compelling evidence supporting our claim of achieving enhanced robustness. This alignment with the established relationship between weight landscape flatness and robustness further reinforces the efficacy of ADR in promoting a more resilient model.\n\n**Q: The results presented in Figure 5 show the results of a hyperparameter study for the interpolation and temperature parameters. It is concluded that annealing both parameters is beneficial for robustness. Do the authors consider the differences (e.g. in the row on annealing) significant enough to draw such conclusions?**\n\nWhile we acknowledge that the variations in the number of occurrences in each row and column in Figure 5 may not be pronounced, a discernible trend emerges when implementing annealing in $\\tau$. Notably, the clean and robust accuracy consistently outperforms scenarios where the temperature is fixed at a specific value across most cases as $\\lambda$ varies. Similarly, the highest robust accuracy is attained when annealing in $\\lambda$ instead of maintaining a fixed $\\lambda$ value while varying the temperature. It is noteworthy that, although there may be a slight sacrifice in clean accuracy when annealing in $\\lambda$ during training, such trade-offs are inherent, given the well-established inverse relationship between model accuracy and robustness. Thus, we conclude that annealing in both temperature and label interpolation factors proves advantageous in achieving superior overall performance.\n\n**Q: Could the authors comment on whether they consider ADR, WA, and AWP to be complementary?**\n\nIn the realm of adversarial defense, we can encapsulate the directions of research aimed at enhancing the efficacy of adversarial training into the following key areas:\n* Different training objectives. e.g. PGD-AT (Madry et al., 2018), TRADES (Zhang et al., 2019).\n* Encourage a flatter model-weight landscape. e.g.  WA (Izmailov et al. 2018; Chen et al. 2021), AWP (Wu et al. 2020).\n* Generate additional data or create auxiliary examples. e.g. use DDPM data (Gowal et al., 2020; Rebuffi et al., 2021a), HAT (Rade et al., 2021)\n* Create effective data augmentation strategies to enhance robustness. e.g. CutMix (Yun et al. 2019; Rebuffi et al. 2021a), DAJAT (Addepalli et al. 2022)\n* Knowledge distillation. e.g. ARD (Goldblum et al., 2020b), RSLAD (Zi et al., 2021), IAD (Zhu et al., 2022)\n\nWe provided an introduction to those methods in both Section 2 and Appendix A. \n\nGiven the straightforward design of ADR, it becomes feasible to integrate ADR with other adversarial training methodologies as complementary approaches. The results of these combinations are presented in Table 1 for ADR with different training objectives (PGD-AT, TRADES), in Table 2 for ADR with flatter weight landscape methods (WA, AWP), and in Table 3 for ADR in conjunction with generated data (DDPM). The integration of ADR with these complementary methods demonstrates discernible gains in robustness.\n\nWhile it is acknowledged that data augmentation-based methods often necessitate intricate hyperparameter tuning and extended training schedules due to their reliance on diverse augmentation techniques, such complexities are not the focus of our work. Consequently, ADR is not combined with these methods in our experiments. Additionally, knowledge distillation-based methods, which modify training labels through the knowledge of pre-trained networks, are not suitable for combination with ADR. Instead, we provide explicit comparisons in Table 3 and Appendix Table 8, establishing ADR as a more effective and resource-efficient methodology."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4379/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700442833505,
                "cdate": 1700442833505,
                "tmdate": 1700442833505,
                "mdate": 1700442833505,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0Fthrcs3ge",
                "forum": "eT6oLkm1cm",
                "replyto": "j7GK7zComL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4379/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4379/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response for Reviewer EopP (continued)"
                    },
                    "comment": {
                        "value": "**Q: In the motivation (subsection 4.1.) it is stated that the aim is 'to design a label-softening mechanism that properly reflects the true distribution'. In the presented approach the teacher network provides the probability distribution with which the one-hot label is interpolated. Can the authors comment on why the teacher network (model weight average of students) reflects the 'true distribution'?**\n\nThanks for asking this question. Let us clarify that our primary objective is to design a label-softening mechanism that accurately reflects the true underlying distribution for each example. It's important to note that the teacher EMA network does not precisely mirror this ideal distribution from the outset. Instead, it progressively refines its approximation towards the goal of achieving both increased robustness and accuracy as the training process advances. Acknowledging the potential for confusion in our original language, we have revised the corresponding paragraph in our updated paper to ensure a more accurate and clear representation of our intentions."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4379/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700442946901,
                "cdate": 1700442946901,
                "tmdate": 1700442946901,
                "mdate": 1700442946901,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8xA6lR6hGU",
                "forum": "eT6oLkm1cm",
                "replyto": "0Fthrcs3ge",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4379/Reviewer_EopP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4379/Reviewer_EopP"
                ],
                "content": {
                    "comment": {
                        "value": "Many thanks to the authors for addressing my questions. The clarifications helped my understanding and I encourage the authors to integrate them into the paper. I updated my score."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4379/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700683753666,
                "cdate": 1700683753666,
                "tmdate": 1700683753666,
                "mdate": 1700683753666,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "uTeCsr53cW",
            "forum": "eT6oLkm1cm",
            "replyto": "eT6oLkm1cm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4379/Reviewer_NJvp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4379/Reviewer_NJvp"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes Annealing Self-Distillation Rectification (ADR), an improved adversarial training (AT) method that emphasizes the rectification of the labels used in AT. It is found that the outputs of robust and non-robust models are distributionally different in several aspects, and it is argued that labels rectified in a noise-aware manner can better reflect the output distribution of a robust model. Hence, the proposed ADR uses the interpolation between the one-hot labels and the outputs of an EMA teacher to produce the rectified distributions, which replace the one-hot labels used in existing AT methods. Experimental results suggest that ADR can achieve state-of-the-art robust accuracy."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed method ADR is intuitive and can be easily integrated into different AT methods.\n\n2. Experimental results suggest that ADR can achieve significant improvement over the baseline and superior robust accuracy to existing methods.\n\n3. The details of the method and the experiments are clearly stated, and the source code is also provided."
                },
                "weaknesses": {
                    "value": "1. While Section 3.2 provides some insightful observations, it is not very clear how they are reflected by the design of the proposed ADR. Particularly, Section 4.1 mentions that robust models should \"generate nearly random probability on OOD data\" and \"demonstrate high\nuncertainty when it is likely to make a mistake\". However, there is no direct evidence (analytical or empirical) for how ADR may help achieve these properties. Instead, it seems that the motivation for ADR mostly comes from the previous works (like those cited in Section 4.1) that suggested the importance of label rectification in AT.\n\n2. In Table 3, it is shown that using DDPM data can improve the robust accuracy of WRN-34-10 trained via AT+ADR, but at the cost of a significant decrease in standard accuracy. This may be undesired since augmenting the training set with DDPM data can improve both clean and robust accuracy for AT according to (Rebuffi et al., 2021a). There should be some explanations or discussions on this issue.\n\n3. The texts in Figure 2 may be too small, which can be difficult to read when printed out."
                },
                "questions": {
                    "value": "1. Are the robust models trained via ADR more conformed to the empirical findings in Section 3.2, as compared with vanilla AT?\n\n2. Should the rectified labels be assigned to adversarial images only, or both clean and adversarial images? Considering that different AT methods use different targets for clean and adversarial images (e.g., PGD-AT and TRADES), this can be an important question when one would like to apply ADR to other AT methods."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4379/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4379/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4379/Reviewer_NJvp"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4379/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698828005154,
            "cdate": 1698828005154,
            "tmdate": 1699636410542,
            "mdate": 1699636410542,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "D515GyPJZR",
                "forum": "eT6oLkm1cm",
                "replyto": "uTeCsr53cW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4379/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4379/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response for Reviewer NJvp"
                    },
                    "comment": {
                        "value": "Dear reviewer NJvp:\n\nThank you for your valuable comments. We are glad you find our method intuitive and recognize our achievement in obtaining effective robustness. We clarify your concerns below:\n\n**Q: While Section 3.2 provides some insightful observations, it is not very clear how they are reflected by the design of the proposed ADR. Particularly, Section 4.1 mentions that robust models should \"generate nearly random probability on OOD data\" and \"demonstrate high uncertainty when it is likely to make a mistake\". However, there is no direct evidence (analytical or empirical) for how ADR may help achieve these properties. Instead, it seems that the motivation for ADR mostly comes from the previous works (like those cited in Section 4.1) that suggested the importance of label rectification in AT.**\n\nIn our observations in Section 3.2, we discover that adversarially trained networks consistently exhibit output consistency between adversarial images and their clean counterparts. Some previous studies suggest the incorporation of dual targets during adversarial training, encouraging models to learn from both an adversarially trained model on adversarial images and a naturally trained model on normal images (Cui et al., 2021; Zhao et al., 2022). While seemingly intuitive in mitigating the trade-off between robustness and accuracy, our observations challenge the appropriateness of this approach. We contend that learning from two targets, one with high confidence from the normal model and the other with lower confidence from the adversarially trained model, is not conducive to achieving consistency for images with noise. Therefore, we propose composing a rectified label in ADR and urge that all images within the $l_p$ norm neighborhood of a given input should be targeted using the rectified label rather than the one-hot one. The observations in Section 3 motivate us to design ADR that effectively prevents the network from becoming over-confident and provides semantically aware smoothing rather than a uniform one. \n\n**Q: Are the robust models trained via ADR more conformed to the empirical findings in Section 3.2, as compared with vanilla AT?**\n\nThanks for asking, we provide our entropy distribution analysis on the ADR-trained model in the updated paper of Appendix K. \n\nFrom Figure 9a, we can see that the ADR model trained on CIFAR-10 is more uncertain than the model trained with PGD-AT when encountering out-of-distribution data (CIFAR-100). It supports our claim that the robust model should not be over-confident when seeing something it has never seen before.\nFrom Figure 9b and Figure 9c, we can observe that the ADR-trained model exhibits similar output distribution as the model trained with PGD-AT (Figure 2b and Figure 2d), except that the ADR-trained model exhibits higher entropy levels in general. The reason behind this phenomenon is that ADR builds a sofer target as training labels, so the output of the ADR model will also be smoother, resulting in higher entropy levels on average.\n\n**Q: In Table 3, it is shown that using DDPM data can improve the robust accuracy of WRN-34-10 trained via AT+ADR, but at the cost of a significant decrease in standard accuracy. This may be undesired since augmenting the training set with DDPM data can improve both clean and robust accuracy for AT according to (Rebuffi et al., 2021a). There should be some explanations or discussions on this issue.**\n\nWe acknowledge that the observed drop in standard accuracy for WRN-34-10 when trained with additional DDPM data is not desirable. This phenomenon is primarily attributed to our deliberate decision to maintain consistency in experimental settings across different models, eliminating the need to report distinct hyperparameters for each model.\n\nTo ensure uniformity, we employ the same $\\tau$ and $\\lambda$ schedule for both ResNet-18 and WRN-34-10, and the reported temperature and label interpolation factor are determined based on the evaluation of performance on ResNet-18. It is crucial to note that the chosen parameters may not represent the optimal fit for WRN-34-10, as the most effective parameters can vary based on architecture and specific settings, as previously mentioned in our limitations. Consequently, the undesired drop in clean accuracy for WRN-34-10 in Table 3 can be attributed to the suboptimal parameter selection for this particular architecture. We will investigate an automatic way to determine the optimal parameter in our future research."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4379/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700441976924,
                "cdate": 1700441976924,
                "tmdate": 1700441976924,
                "mdate": 1700441976924,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vGKyno6TPf",
                "forum": "eT6oLkm1cm",
                "replyto": "uTeCsr53cW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4379/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4379/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response for Reviewer NJvp (continued)"
                    },
                    "comment": {
                        "value": "**Q: The texts in Figure 2 may be too small, which can be difficult to read when printed out.**\n\nThank you for the suggestion, we provided adjusted figures that have larger texts in our revised paper.\n\n**Q: Should the rectified labels be assigned to adversarial images only, or both clean and adversarial images? Considering that different AT methods use different targets for clean and adversarial images (e.g., PGD-AT and TRADES), this can be an important question when one would like to apply ADR to other AT methods.**\n\nThe rectified labels should be assigned to both clean and adversarial images. Our observations, detailed in Section 3.2, lead us to summarize that a robust model should consistently produce outputs that align for both clean and adversarial data. In essence, within the $l_p$ norm neighborhood of a given input, all images should be associated with a singular smooth and rectified label. Consequently, we propose the replacement of both hard labels used in PGD-AT for adversarial images and TRADES for clean images with the rectified labels generated by the ADR process. This adjustment ensures a unified and consistent labeling approach across both clean and adversarial instances, aligning with our overarching objective of enhancing model robustness."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4379/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700442137726,
                "cdate": 1700442137726,
                "tmdate": 1700442137726,
                "mdate": 1700442137726,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GfhYBqHydE",
                "forum": "eT6oLkm1cm",
                "replyto": "vGKyno6TPf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4379/Reviewer_NJvp"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4379/Reviewer_NJvp"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your valuable responses. Most of my concerns have been addressed, and I appreciate the improved clarification of the motivation and contributions in the responses to all reviewers. As for the suboptimal clean accuracy in some experiments, there is still a lack of convincing evidence that this is not a weakness of the proposed method, although I understand that diving into the tuning of the hyperparameters for WRN-34-10 can potentially tackle this issue, and this is not likely to be finished during this discussion period, especially the experiment using DDPM data. Therefore, I will consider raising my final score and I hope that better results can be presented in the final version of the paper."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4379/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700641203635,
                "cdate": 1700641203635,
                "tmdate": 1700641203635,
                "mdate": 1700641203635,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]