[
    {
        "title": "One-hot Generalized Linear Model for Switching Brain State Discovery"
    },
    {
        "review": {
            "id": "Ait13RqMK0",
            "forum": "MREQ0k6qvD",
            "replyto": "MREQ0k6qvD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4615/Reviewer_G7kx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4615/Reviewer_G7kx"
            ],
            "content": {
                "summary": {
                    "value": "The authors address the challenge of modeling dynamic functional neural interactions. They note that existing methods often lack biological plausibility, primarily because they don't account for the influence of anatomical structures on functional neural interactions. To rectify this, the authors introduce a one-hot prior to the GLM model. The method was evaluated on one synthesized dataset and two real-world datasets, achieving state-of-the-art results."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "This paper is technically robust. The underlying problem is well-defined and builds upon a lineage of substantial research. Drawing insights from neuroscience, the authors convincingly argue that anatomical structures influence dynamic functional neural interactions. Their approach to address this hypothesis is adeptly framed, straightforward, and effective. The evaluation is comprehensive, encompassing a broad spectrum of models related to the problem, and it's tested across varied datasets. The inclusion of the whisking dataset is particularly intriguing, and the visual illustrations enhance clarity. Overall, this paper is commendable and would be a valuable contribution to the ICML community, showcasing the intersections of machine learning and neuroscience research."
                },
                "weaknesses": {
                    "value": "(1) While the overall presentation of the paper is commendable, there is room for improvement in Sections 2 and 3. These sections could benefit from more intuitive and lucid explanations accompanying the mathematical equations, making it more accessible for readers.\n\n(2) I believe the prior work by Glaser et al. [1] deserves acknowledgment. It might also be valuable to include it in the comparative models, given that their focus on cluster (population) structures aligns with the theme of underlying structures.\n \n[1] Glaser, Joshua, et al. \"Recurrent switching dynamical systems models for multiple interacting neural populations.\" Advances in Neural Information Processing Systems 33 (2020): 14867-14878."
                },
                "questions": {
                    "value": "I'm keen to understand the authors' future direction and insights drawn from this research. Does incorporating an increasing number of biological constraints into models always lead to better outcomes? Or are there potential trade-offs to be mindful of? Going forward, are the authors considering other factors that might influence interactions? For instance, within an E-I balanced network, given identical anatomical structures and brain states, interactions could vary based on the stage and phase of short-term synaptic depression. This suggests that intrinsic governing features could arise when adding more biological constraints or features. I'd appreciate the authors' perspective on this."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4615/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4615/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4615/Reviewer_G7kx"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4615/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698735500931,
            "cdate": 1698735500931,
            "tmdate": 1699636440411,
            "mdate": 1699636440411,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ltE7R9Ihr7",
                "forum": "MREQ0k6qvD",
                "replyto": "Ait13RqMK0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4615/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4615/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to G7kx"
                    },
                    "comment": {
                        "value": "Dear Reviewer G7kx,\n\nThank you very much for your time and valuable comments on our paper. We appreciate your recognition of the strengths of our paper. Hopefully, the following responses could resolve most of your concerns and answer your questions.\n\n### Weaknesses\n1. Following the reviewer's suggestion, we have reorganized the Methods section to improve readability for readers. We have changed the order of the presentation to introduce the models in order of increasing complexity. Furthermore, we have added a subsection detailing the relation between GHG and OHG. We hope these changes have made the presentation more intuitive for readers. Please check the current latest revision.\n2. We agree that the work by Glaser et al. (2020) should be mentioned in the paper.\n    * The citations regarding SLDS, rSLDS, and mp-SLDS are included in the introduction section.\n    * Relationships to SLDS-based methods: Instead of considering state switching on neural connectivities as our HMM-GLM-based models, SLDS, rSLDS, and mp-rSLDS consider state switching on underlying linear dynamics governing the observed spike trains. Besides, mp-rSLDS also considers incorporating prior information (such as anatomy) into the linear mappings in different states. However, their shared priors are known hyperparameters, but the shared global priors in our GHG and OHG are learnable.\n    * We have finished the comparison of our OHG with SLDS-based methods shown in Fig. 16 and Appendix A.7 in our latest revision. Specifically, the inferred states from SLDS-based methods exhibit fast-switching phenomena, which hinders their interpretability. This result might imply that state switching over neural connectivities could be an important assumption of an effective component that should be taken into account when dealing with spike train data collected from multi-stage experiments.\n\n\n### Questions\n**Future Directions**: We greatly appreciate the reviewer's perspective shown through their questions. Indeed, placing a biologically motivated constraint on the structural connectivity with the OGH improved inference of both state and functional connectivity; our results thus indicate that including relevant biological constraints can lead to a better outcome. On the other hand, adding too many biological details can lead to overparameterization of the model, so each added degree of complexity should be carefully chosen and evaluated. We agree that one promising extension would be to add effects of short-term synaptic potentiation and depression, which is biologically well-characterized but not included in our models. Adding constraints to control the E-I balance could also be beneficial. We leave these promising topics to a future study. Another extension would be to add different assumptions about the network structure, for example, the inclusion of multiple regions with dense intra-regional connectivity and sparse inter-regional connectivity. This direction was suggested by Reviewer hZX1 and is also explored in the aforementioned study by Glaser et al. Please check Fig. 9 in Appendix 4 in the current revision for the result regarding subpopulations of neurons.\n\nWe hope that this response addresses the majority of your concerns and questions. Your feedback is valuable, so please don't hesitate to provide additional comments or ask further questions. Thank you again for your time and valuable feedback!"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4615/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700096745601,
                "cdate": 1700096745601,
                "tmdate": 1700531263787,
                "mdate": 1700531263787,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KSBHfgm8Ti",
                "forum": "MREQ0k6qvD",
                "replyto": "ltE7R9Ihr7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4615/Reviewer_G7kx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4615/Reviewer_G7kx"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you to the authors for addressing my questions and concerns, and for incorporating the reference I mentioned for comparison. I believe it is an interesting and good paper to the ICLR community."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4615/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700590191778,
                "cdate": 1700590191778,
                "tmdate": 1700590191778,
                "mdate": 1700590191778,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "reKAkcNWkj",
            "forum": "MREQ0k6qvD",
            "replyto": "MREQ0k6qvD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4615/Reviewer_zShZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4615/Reviewer_zShZ"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a prior-informed state-switching generalized linear model with hidden Markov models (HMM-GLM) called one-hot HMM-GLM (OHG), capable of estimating dynamically changing functional interactions under different states. Learnable priors are introduced to capture the state-constant interaction and reveal the underlying anatomical connectome. Experiments on simulated data demonstrated its effectiveness and practical applications achieved interpretable interaction structures and hidden states with the highest predictive likelihood."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper proposed a novel OHG framework to estimate time-varying functional interaction in multi-state neural systems. The one-hot prior yielded better connectivity patterns and hidden states and provided more biological plausibility.\n2. This paper provided detailed algorithms of the proposed model and conducted extensive experiments on both synthetic and real neural datasets to demonstrate its superiority."
                },
                "weaknesses": {
                    "value": "1. This paper seems to propose two frameworks: the na\u00efve one is GHG and the effective one is OHG. What\u2019s the relationship between them? In the abstract, the authors only mention the two priors (Gaussian and one-hot) without the names of the frameworks. In the conclusion, only OHG is mentioned. Thus, it is confusing.\n2. In the method, the authors first describe OHG and then introduce GHG. They are both variants of HMM-GLM but OHG outperforms GHG. Thus, the order seems unreasonable. What\u2019s more, the experimental results showed that GHG was unable to achieve this paper\u2019s goal. Then what\u2019s the value of GHG?"
                },
                "questions": {
                    "value": "1. As shown in Table 2, the results of different numbers of states were similar to that of one-state GLM. It can be explained that global static connection patterns dominate functional interactions in all states as mentioned in the manuscript. Then was the state division biologically reasonable? Perhaps only the features of the global prior were extracted or there was only one state.\n2. The experiments fixed the generative hyperparameters and claimed that this set was noninformative priors and insensitive to different datasets. Is there any support for this declaration?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4615/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4615/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4615/Reviewer_zShZ"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4615/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698740888453,
            "cdate": 1698740888453,
            "tmdate": 1699636440325,
            "mdate": 1699636440325,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RozjAp6kJJ",
                "forum": "MREQ0k6qvD",
                "replyto": "reKAkcNWkj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4615/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4615/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to zShZ (1/2)"
                    },
                    "comment": {
                        "value": "Dear Reviewer zShZ,\n\nThank you very much for your time and valuable comments on our paper. We highly appreciate your recognition of the strengths of our paper. Hopefully, the following responses could resolve most of your concerns and answer your questions.\n\n### Weaknesses\n1. **Mentioning GHG and OHG, and the value of GHG**: Thanks for your valuable suggestion. We have modified our abstract, method, result, and conclusion parts to include more discussion on GHG. Specifically:\n    * **Abstract**: We include the names of the two frameworks (two variants of HMM-GLM): GHG and OHG explicitly.\n    * **Method**: We add an extra comparison between OHG and GHG. Compared with OHG, **GHG serves as an intermediate model with a straightforward prior directly on the weights** representing shared global connectivity but without the one-hot decomposition.\n    * **Experiment results**: In the experimental evaluation section, GHG is not bad, but still worse than OHG, validating that the one-hot decomposition component in OHG does account for the better performance achieved by OHG.\n    * **Conclusion**: We add new sentences mentioning the role of GHG. I.e., GHG serves as an intermediate model with shared prior directly on weight matrices without (one-hot) strength-connection decomposition, confirming that such a decomposition is critical for the success of multi-state inference.\n2. **Order of introducing GHG and OHG**: We thank the reviewer for pointing out this issue. The value of GHG is not in that it outperforms the OHG, but lies in its role as an intermediate model. The GHG is the simplest way to add a prior to the state-dependent weight matrices $\\boldsymbol W_s$. The OHG has an additional layer of complexity in that it decomposes the weight matrices into a product of a discrete connection matrix and a strength matrix, allowing the model to specifically constrain the structural connectivity. The value of including the GHG in our study is thus not that the GHG outperforms the OHG, but rather that it allows us to examine whether the structure-strength decomposition of the OHG is necessary, or whether a simple Gaussian prior over the weight matrix is enough. The fact that the OHG significantly outperforms the GHG in both state prediction and inference of the underlying functional connectivity shows that the structural decomposition method provides additional benefits that a simple prior cannot.   &nbsp;&nbsp;&nbsp;&nbsp;     However, we understand that the presentation of the Methods may be confusing in that the final method introduced is an intermediate model. Accommodating the reviewer's comment, we rearranged the Methods to introduce the models in increasing order of complexity: The GHG is now introduced before the OHG, which is introduced last. Furthermore, in order to clarify the role of GHG in our study we added a subsection in the Methods titled \"The relationship between GHG and OHG\"."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4615/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700096565463,
                "cdate": 1700096565463,
                "tmdate": 1700167987806,
                "mdate": 1700167987806,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Ttx6ulAozw",
            "forum": "MREQ0k6qvD",
            "replyto": "MREQ0k6qvD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4615/Reviewer_hZX1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4615/Reviewer_hZX1"
            ],
            "content": {
                "summary": {
                    "value": "The paper discusses an extension of generalized linear models for a population of neurons used (binned spike trains under Poisson firing rate assumption)  that involves latent states and factorized latent-state-dependent inter-neuron connection weights.  The latter goes beyond previous work with a specific factorization that involves a mixture factor over at the simplex, which at its extremes provides a one-hot encoding that determines the existence of a connection and its sign (excitatory or inhibitory), and the state-dependent weight magnitude. Estimation of the parameters of this model requires an expectation maximization algorithm, which is briefly described. Baseline models from the literature and additional novel baselines are constructed by involving various aspects of the proposed approach. Results are presented for a synthetic experiment and two real-world data sets, with known task/stimulus/environmental timing."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is an original contribution for GLM models of neuron spike trains. The method and results are well-presented and clear. The figures and equations are clear. A number of baselines are compared and the results are consistent.  From the results it would seem that the latent state inference is meaningful, this could be significant for neuroscientists who wish to study."
                },
                "weaknesses": {
                    "value": "The synthetic study seems quite limited to the type of data the model is designed for (a single global state).  It is not clear to me how well it will work if the neurons are organized into groups with their own state dynamics (which evolve largely independently) and only rarely communicate. I.e. the topology of the network could be loose connections between tightly interconnected subnetworks. \n\nA principled approach for the selection of the number of states is not discussed. At one point the paper mentions that the log-likelihood is higher with additional states although these states are rare: \"there are many sessions with rarely occupied states, and the distinction\nbetween states becomes subtle\". This seems to be a flaw in the modeling if someone does not know how many true states. Should the reader be suggested to look at the distribution of states to decide? Perhaps a model selection criterion is needed. \n\nAlong similar lines, an analysis of the decoding of task information from the latent state would help understand in the real-world tasks the utility of the state estimate. \n\nQuestions of scaling could provide better significance:\n\nHow scalable is the model and/or the algorithm? New recording technology including optical calcium imaging can record from hundreds to close to thousands of neurons.  The number of neurons in the synthetic study could be ramped up to see this. \n\nIt is not clear how quickly can inference be performed after model fitting. If a neuroscientist wants to use the inferred state to control a stimulus is it possible to operate in real-time with a minimal delay?"
                },
                "questions": {
                    "value": "How would the number of states be selected in practice? \n\nHow scalable is the model in terms of subpopulations with their own dynamics?\n\nHow scalable is the model and algorithm in terms of the number of neurons?  \n\nHow quickly can inference be done at run time?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4615/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698758759872,
            "cdate": 1698758759872,
            "tmdate": 1699636440241,
            "mdate": 1699636440241,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6wvjdSupzE",
                "forum": "MREQ0k6qvD",
                "replyto": "Ttx6ulAozw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4615/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4615/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to hZX1 (1/2)"
                    },
                    "comment": {
                        "value": "Dear Reviewer hZX1,\n\nThank you very much for your time and valuable comments on our paper. We highly appreciate your recognition of the strengths of our paper. Hopefully, the following responses could resolve most of your concerns and answer your questions.\n\n* **Experiments on subpopulation configurations**: Although all HMM-GLMs in this paper including our newly proposed GHG and OHG only work for one global prior, it is still possible to apply the model to multi-region data. For example, $C$ groups of neurons $N_1,N_2,\\dots,N_C$ evolve largely independently and only rarely communicate. Further, assume each group of neurons has its own global structure and its own number of states $S_1,S_2,\\dots,S_C$. Then, we can reduce such a problem into a model with one single globally shared structure $\\operatorname{diag}(\\boldsymbol A_{1,0},\\boldsymbol A_{2,0},\\dots,\\boldsymbol A_{C,0})$. Then, there will be $S = S_1\\times S_2\\times\\dots\\times S_C$ states for the whole groups of neurons in total. For better illustration, we run a simple example on $C=3$ subpopulations (groups) and each group has 5 neurons and 2 states. Therefore, we need a model of 15 neurons and 8 states to accommodate this configuration. The results are shown in Fig. 14 in Appendix 6 in the current revision. Although this is a feasible solution to using our HMM-GLMs framework to tackle subpopulations problem, we think it is better and more reasonable to directly develop a new model for subpopulations problem, and this could be one of our great future directions.\n* **Number of states**:\n    * In real-world scenarios, we don't know the true number of true states or there is even no definitive concept of absolute true states. Cross-validation is certainly a rigorous way to determine the number of states. However, increasing the number of states may increase the validation log-likelihood, as the OHG result in Tab. 2. Too many states may hurt interpretability. Therefore, there should be a trade-off between the validation log-likelihood and the interpretability. It might be better to select a turning point where the likelihood stops its rapid growth, and in the meantime, the number of states is not too large to be interpreted.\n    * For example, for $S>5$ in the PFC-6 dataset, the performance of OHG does not increase significantly and becomes flat (Fig. 10} in Appendix 4.3. Although more numbers of states are assumed, only around four effective states are learned and the interpretations are all similar (Fig. 11 in Appendix 4.3) to that of the 4 states explained in the main content.\n    * Generally speaking, the problem of determining a suitable number of states exists in most of the multi-state models, and this could be one of our important future directions.\n* **Decoding task information from inferred latent**:\n    * Thanks and we think this is a good suggestion. To further confirm our interpretation regarding the inferred states from OHG, we use a logistic regression model to decode the correctness of each trial from the inferred states. Specifically, each trial is viewed as a data point in logistic regression. The input variable is the one-hot representation of the inferred hidden states of size number of states $\\times$ number of time bins. We use the one-hot representation since the state in each time bin is a categorical variable. The output is a binary variable, representing the correctness of a trial. We use 2/3 trials to train a logistic regression model and test on the remaining 1/3 trials. Fig. 9 in Appendix 4 shows that logistic regression fitted to the inferred states from OHG obtains the highest decoding accuracy. This means the inferred states from OHG do include enough information related to the correctness of each trial. From the interpretation in the main content, we mention that the animal will enter state 4 if it is a correct trial because of obtaining the reward at the correct target location. This is consistent with the positive coefficients of state 4 at the end period of trials, indicated by the black square in Fig. 9.\n    * To further confirm the irreplaceable role of the inferred latent in predicting trials' correctness, we train a multilayer perceptron (MLP) neural network with one hidden layer of size 100 (we tried different MLP configurations and selected the best one) to predict the correctness of each trial directly using the neural spike train as the input. The test accuracy is only 0.72, which is significantly worse than OHG. This implies that OHG plays a very important role in summarizing the task information from the neural spike train, similar to the irreplaceable role of the convolutional layer in CNN.\n    * Please check the details and results in Appendix 4 in the current revision."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4615/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700191639563,
                "cdate": 1700191639563,
                "tmdate": 1700193724978,
                "mdate": 1700193724978,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Dj2Rsp2RK8",
                "forum": "MREQ0k6qvD",
                "replyto": "Ttx6ulAozw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4615/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4615/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to hZX1 (2/2)"
                    },
                    "comment": {
                        "value": "* **Scalable to the number of neurons**:\n    * The time complexity of the learning in one epoch is $\\mathcal O(N^2TS)$ for all HMM-GLM-based methods (including HG, GHG, and OHG), where $N$ is the number of neurons, T is the number of time bins, and $S$ is the number of states. Therefore, the time increases quadratically to the number of neurons $N$ since there will be $\\frac{N(N-1)}{2}$ edges in each state. However, since the parameter set of OHG is larger than GHG and larger than naive HG, the constant coefficient of OHG before the complexity term $N^2TS$ is larger than GHG and larger than naive HG. To understand the scalability of our algorithm to large numbers of neurons, we are running experiments on this and will keep you posted when the result can be accessed in the Appendix in our next revision.\n    * From the synthetic dataset, we observe that the state accuracies of all HMM-GLM-based methods start to drop when there are 40-50 neurons. To this scale, there will be more than $S\\times [1600,2500]$ edges (connectivities) that need to be learned in the network, which introduces challenges to determining the correct state switches. Under such situations, the effectiveness of both the E-step and the M-step in the algorithm would be mutually influenced by each other. However, this does not mean that the HMM-GLM-based methods cannot be applied to real-world datasets. It might still be worth trying HMM-GLM-based methods even if the data consists of a large number of neurons, since Fig. 17 in Appendix A.8 shows that the per-neuron log-likelihood does not drop when increasing the number of neurons. This means the HMM-GLM-based models (especially OHG) is still to predict firing rates effectively for spike train data reconstruction. We would like to view this as an important future direction.\n* **Inference time**: Once the model parameter is learned, the inference should be very quick. Specifically, the time complexity to run the forward-backward algorithm to infer the state sequence is $\\mathcal O(2ST)$. On the synthetic dataset with 5 states and 20 neurons, the running time of the forward-backward algorithm on a sequence of length 5000 time bins is about 1 second (on one core of the Intel Xeon Gold 6226 \"Cascade Lake\" @ 2.7Ghz CPU). Once the model parameters are learned from previous spike trains, we can rapidly infer the state sequence for the current spike train, and then use it to control the stimulus for the next trial.\n\nWe hope that this response addresses the majority of your concerns and questions. Your feedback is valuable, so please don't hesitate to provide additional comments or ask further questions. Thank you again for your time and valuable feedback!"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4615/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700191661706,
                "cdate": 1700191661706,
                "tmdate": 1700531310869,
                "mdate": 1700531310869,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qJKeUn0Erk",
                "forum": "MREQ0k6qvD",
                "replyto": "Dj2Rsp2RK8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4615/Reviewer_hZX1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4615/Reviewer_hZX1"
                ],
                "content": {
                    "comment": {
                        "value": "I want to thank the authors for the additional work, which further impresses the contribution of the work.\n\nI have three questions with remaining time for discussion.\n\n1. Is the relative performance (using the whole battery) on the multiple population case comparable to the synthetic example in the main body? It seems (perhaps because of the block structure) that there are more spurious correlations.\n\n2. Wouldn't Bayesian information criterion provide a principled trade-off between increasing number of states and the validation likelihood? I agree the knee is clear in the plot, but a justified criterion for automatic model selection is helpful in practice.\n\n3. It seems like the comparisons/baselines of HMM-GLM and SLDS have very fast latent state dynamics. Cannot this be remedied by a prior to bias the transition matrices to have a larger diagonal (stay in the same state)?"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4615/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700451620657,
                "cdate": 1700451620657,
                "tmdate": 1700451620657,
                "mdate": 1700451620657,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BmSBIvhXyp",
                "forum": "MREQ0k6qvD",
                "replyto": "Ttx6ulAozw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4615/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4615/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your further questions.\n\n1. Given the subpopulations problem is not the primary goal of our models, we agree with you that the relative performance might not be as good as the example in the main content, as you pointed out the spurious/noisy weights should not have appeared at the off-diagonal blocks. As we mentioned before, we first reduced the subpopulations problem and then used our OHG to solve it. Therefore, OHG doesn't know that the off-diagonal blocks should be all 0s. There are two possible solutions:\n    * Increase the data size, so that OHG could learn better from the more sufficient dataset that the off-diagonal blocks could be 0s. We doubled the data size and the results became slightly better (but not visually significant). Those off-diagonal blocks are still not completely silenced.\n    * If we know the configurations of the subpopulations, we can certainly use a regularization term to suppress the off-diagonal blocks, or even silence them in a hard style (e.g., through a mask). Then, we can obtain weight matrices with less noisy elements on those off-diagonal blocks (Fig. 15 in Appendix A.6).\n2. Thanks. Please check the updated Fig. 10 in Appendix A.4.3. The lowest range of BIC $S\\in\\\\{3,4,5\\\\}$ roughly matches the turning point of the log-likelihood plot.\n3. Adding a prior to strengthen the diagonal of the transition matrix is an intuitive way of suppressing the fast state switches in HG. This is equivalent to applying an L2 regularization term on the off-diagonal elements in the transition matrix. For some applications, it might be helpful. On the PFC-6 dataset, however, Fig. 18 in Appendix A.9 shows that when the number of state switches is suppressed, the inferred states are still meaningless. Particularly, most switches happen within a short duration, which looks like nothing but noisy state switches. Only one major state governs the whole trial. Besides, Fig. 8 in Appendix A.4.1 shows that both GHG and OHG don't have such a fast switches phenomenon. This might further imply that a global structural prior plays an important role in HMM-GLM-based models."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4615/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700542343093,
                "cdate": 1700542343093,
                "tmdate": 1700582503431,
                "mdate": 1700582503431,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]