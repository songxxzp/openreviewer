[
    {
        "title": "Generative Neuro-Symbolic Visual Reasoning by Growing and Reusing Modules"
    },
    {
        "review": {
            "id": "lSWl1eYZmF",
            "forum": "MNShbDSxKH",
            "replyto": "MNShbDSxKH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6843/Reviewer_JpcA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6843/Reviewer_JpcA"
            ],
            "content": {
                "summary": {
                    "value": "The authors tackle the problem of prior visual programming methods exhaustively generating entire code snippets for each new task. Instead, they proposed GNSVR to grow and re-use a library of modules. GNSVR first initializes new modules based on a train set, then evaluates the new modules based on additional few-shot training examples, and finally it executes the modules on the full test set. It shows comparable performance on visual reasoning tasks, can transfer modules to new tasks, and adapt to new visual reasoning tasks with few training examples."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "I am appreciative of the idea of generating more modular and composable modules that are verified, to be used in a library of skills for future tasks. I think the idea of continuously growing this library is innovative, and the method simple and elegant."
                },
                "weaknesses": {
                    "value": "W1. Is there anything to prevent overfitting to the small train set of a given task, and only creating specific modules tailored for that domain? I can imagine that these overfitted modules may not be very useful in new tasks. \n\nW2. Isn\u2019t it possible that the method creates a broad function signature, but during stage 2 verification with the small set of train examples, it overfits to a bad implementation that only works for those examples, and therefore actually harms performance from there on out? I\u2019m mainly concerned about the above two overfitting challenges.\n\nW3. There seems to be an assumption that the queries in the train set actually all require similar modules, for example, to verify the new modules, the selected set of test cases from the train set must actually use those modules. I\u2019m not sure if this is a reasonable assumption, and also, how is this sampling of train examples (both in stage 1 and stage 2) done?\n\nW4. In the experiments, are Visprog/Viper also given the same few-shot training set? For example, I believe you can use the same method of error correction and correct both Visprog/Viper when it is incorrect. In this way, you can include Visprog/Viper comparison in Tables 3 and 4. Would be great to better disentangle what drives this improvement of performance -- modularity in the proposed modules, or the training examples given in the loop, etc."
                },
                "questions": {
                    "value": "Q1. Please clarify the selection of training examples used in stage 1 and 2. \n\nQ2.  \u201cIf a program encounters errors during execution, we incorporate the error information into the LLM\u2019s prompt and instruct it to rectify these issues.\u201d How is this done?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6843/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6843/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6843/Reviewer_JpcA"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6843/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698285393394,
            "cdate": 1698285393394,
            "tmdate": 1699636792932,
            "mdate": 1699636792932,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "P9I7aWz3TP",
                "forum": "MNShbDSxKH",
                "replyto": "lSWl1eYZmF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6843/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6843/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Look forward for your  response (Part 1)"
                    },
                    "comment": {
                        "value": "Thank you for the positive comments and insightful suggestions.\n\n> Q1. Is there anything to prevent overfitting to the small train set of a given task, and only creating specific modules tailored for that domain? I can imagine that these overfitted modules may not be very useful in new tasks.\n\n>Q2. Isn\u2019t it possible that the method creates a broad function signature, but during stage 2 verification with the small set of train examples, it overfits to a bad implementation that only works for those examples, and therefore actually harms performance from there on out? I\u2019m mainly concerned about the above two overfitting challenges.\n\n**Q1-Q2. About overfitting VS training number.**\nThanks for concerns on the overfitting issue of train examples. Our GNSVR pipeline do have overfitting issue when the number of the training examples is too small (10), which could be reflected in the ablation study in table 5 of the revised paper. However, as the number of training examples goes up (50 and 100), the performance becomes stable, which is unlikely that the GNSVR framework creates a broad function signature overfitting the training examples. We believe that one reason for the strong few-shot performance of GNSVR is that all our pre-defined APIs are general APIs that work well for vision tasks in the wild. We also provide the performance VS test cases in the following tables for different tasks. As shown in the tables, as the training number is larger than a small number, our GNSVR framework becomes effective and relieve the overfitting issue. \n\n**Number of Sampling Examples**\n\nTable 1: Number of Sampling Examples on GQA.\n| # of samples | GQA |\n| ------- | -------- |\n| 60     | 44.5   |\n| 120    | 45.3   |\n| 300    | 45.9   |\n\nTable 2: Number of Sampling Examples on RefCOCO.\n| # of samples | RefCOCO |\n| ------- | -------- |\n| 10     | 49.4   |\n| 50     | 67.0   |\n| 100    | 67.1   |\n\nTable 3: Number of Sampling Examples on RAVEN.\n| # of samples | Center | L-R  | U-D  |\n| ------------ | ------ | ---- | ---- |\n| 5     | 46.5   | 37.2 | 39.8 |\n| 10    | 80.1   | 67.6 | 69.1 |\n| 20    | 80.1   | 67.6 | 69.1 |\n\nTable 4: Number of Sampling Examples on MEWL.\n| # of samples | shape | color |  material  |\n| ------------ | -------- | -------- | --- |\n| 5     | 38.9   | 39.6 | 37.9 |\n| 10    | 43.7   | 45.3 | 41.0 |\n| 20    | 43.7   | 45.3 | 41.0 |"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6843/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700211637537,
                "cdate": 1700211637537,
                "tmdate": 1700211637537,
                "mdate": 1700211637537,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nLrqimpMcN",
                "forum": "MNShbDSxKH",
                "replyto": "DTC12NupRR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6843/Reviewer_JpcA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6843/Reviewer_JpcA"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "Thank you for the thoughtful response! Can you please elaborate on how the training examples are \"randomly sampled by question categories\"? I believe this relates to my concerns on overfitting, and how strategies are required (that may use additional information, like knowledge of question categories in a dataset) to ensure a broad coverage of modules are proposed. Thanks!"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6843/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700513163162,
                "cdate": 1700513163162,
                "tmdate": 1700513163162,
                "mdate": 1700513163162,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oSrMQ3sjHJ",
                "forum": "MNShbDSxKH",
                "replyto": "lSWl1eYZmF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6843/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6843/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your constructive comments."
                    },
                    "comment": {
                        "value": "Thanks again for your response on the overfitting concern of the GNSVR model. We further explain your concerns as followed. In GQA, we have sampled training examples by question categories. This strategy does require the usage of question categories in the training dataset. In refCOCO, RAVEN and MEWL, we sample examples randomly since there is no sub category by types.\n\nTo furtehr investigate our model's ability against overfitting, we have conducted new experiments on Table A below. We randomly sample different training samples from the whole training set 3 times with different random seeds, which we denote it **Random Sampling**. We also perform another sampling strategy that randomly sample training examples by question types, which we call **Random Sampling by types**. Based on the results in table A, we can see that our model works in both **Random Sampling** and **Random Sampling by types**, achieving reasonable performance although we do observe that **Random Sampling** strategy has larger variances on GQA. We will add such analysis in the revised paper.\n\nTable A: Ablation on GQA for sampling strategies.\n| Method GNSVR              | Accuracy |\n| --------                  | -------- |\n|  Random Sampling          | 44.8 +- 0.41 |\n|  Random Sampling by types        | 45.9 +- 0.14 |\n\nAnother way to show our GNSVR model's ability against **overfitting** is that as shown in Table 2, Fig. 4, and Fig. 7,  of the paper, **the learned modules learned from GQA and refCOCO can be generalized to new tasks like image editing and knowledge tagging.** Note that language instructions and images from image editing and knowledge tagging are quite different from that of the GQA's and RefCOCO's images."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6843/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700541707002,
                "cdate": 1700541707002,
                "tmdate": 1700541982850,
                "mdate": 1700541982850,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4iIhr5iJO1",
            "forum": "MNShbDSxKH",
            "replyto": "MNShbDSxKH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6843/Reviewer_vHZ5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6843/Reviewer_vHZ5"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes GNSVR, a neuro-symbolic system that grows a library of modules for visual reasoning tasks through the use of LLMs. It consumes a small set of example training tasks, and generates new modules with a two-step process. In step 1, a LLM is queried to see if an example task can be solved given functions from a base API, if the LLM responds negatively, then it is instructed to propose a new module to add into the API that would help solve this task, as well as to produce an I/O example for how this module would be used to solve the task. In step 2, given doc-string descriptions of these would-be useful modules, an LLM is prompted to author a python function, potentially referencing base-API logic, that matches the description and corresponds to its previously generated I/O example. At test-time, these modules are integrated into the API, and a LLM uses API calls to produce a program that can reason over visual input matching the logic of the input question. The system is primarily evaluated against other neuro-symbolic systems for visual reasoning that operate with a fixed API (VisProg, ViperGPT). Compared to the previous systems, GNSVR offers competitive performance on standard benchmarks (GQA, RefCOCO), and demonstrates promising results that its discovered modules can help generalize to related tasks, like image editing and knowledge tagging."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "In general, I think the direction of the proposed method is sound and compelling; systems like VisProg and ViperGPT provide promising ways to push neuro-symbolic reasoning past toy-domains, but they are limited by fixed APIs. Using LLMs to augment these APIs in a task-specific manner is an interesting idea with many potential avenues for future exploration.  \n\nFrom a methodological stand-point, I think the division of labor between the module initialization step and the module generation step is a neat insight. This design decision allows the LLM to create it's own tests, that can then be used to help guarantee that LLM generated functions 'do what they are supposed to'. \n\nThe experimental results are largely positive for the proposed method, although I have some concerns about their design that I'll detail below. That said, I think the experimental results do support the claim that the proposed system offers improvements over both VisProg and ViperGPT across tasks. Perhaps the most compelling claim is that the modules that GNSVR finds from the GQA and RefCOCO tasks can \"transfer\" for related tasks of image editing and knowledge tagging, supported by the fact that GNSVR significantly outperforms VisProg in this setting. The evaluations against fully-supervised methods on the Raven and MEWL tasks is also fairly impressive, considering GNSVR is operating in a few-shot learning paradigm."
                },
                "weaknesses": {
                    "value": "# Main Concern\n\nFrom my perspective, the biggest current weakness of the paper is that from the experimental design its hard to parse out exactly how the discovered modules affect the system's performance. Ostensibly, this can be gleaned from comparisons between GNSVR and VisProg/ViperGPT, but there are more differences between these systems beyond merging in discovered modules. Specifically, GNSVR uses a \"base API\" that is a combination of VisProg and ViperGPT, so the \"fair\" comparison would be against an ablated version of GNSVR that removes steps 1 and 2, and just tries to solve test-problems with the original API functions. This condition is considered in the ablation experiment (GNSVR w/o ML), but only a subset of the RefCOCO test-set. To solidify the claim that the improvement GNSVR observes stems from its discovered modules, this base condition should be added to all of the experimental set-ups (tables 1-5), for example, from Table 2 its unclear how much of the delta improvement between VisProg and GNSVR can be attributed to improvements in the base API versus improvements to the API from the new modules. \n\nBeyond this, I'm also slightly concerned about the design of the GNSVR w/o ML baseline. At inference time, is this baseline allowed to invoke arbitrary python logic in the style of ViperGPT (e.g. standard control flow constructs) or is it restricted to *only* using API function calls in the style of VisProg. I would imagine that the first condition would be more fair to evaluate GNSVR. Solving some tasks might require simple logic that the LLM knows how to express in python, but might not be directly expressible with a series of API calls. In GNSVR, this logic is incorporated into modules, but in the baseline the LLM should still have the opportunity to invoke similar logic in its test-time solutions (otherwise its impossible to properly evaluate the usefulness of the discovered modules). Please clarify which of these modes the baseline is operating in. \n\n# Minor\n\nCompared to VisProg and ViperGPT this system seems to require more training data, as the I/O pairs are not only used to populate in-context examples, but also impact (i) what module concepts are proposed and (ii) how the correctness of each module concept is evaluated. This point about reliance on training data is touched on in the ablation section, but it would be good to make this distinction explicit when comparing the pros/cons of the proposed system against past work."
                },
                "questions": {
                    "value": "While the broad-stroke description of the method is clearly presented, many of the important details were left unspecified (making reproducibility challenging without a reference implementation). To improve clarity, and help understanding of the results, I think the below questions would be important to answer in future revisions:\n\n# 1. Module Generation \n\n(a) Can new modules be hierarchical (e.g. can one proposed module referenced a previously proposed module), or can they only call out to the original API functions?\n\n(b) The error-correction and pass-rate logic for the module generation step are not clear. What is the \"pass-rate\" at which a function is accepted as a new module, from the listed examples it seems like a single input/output pair is used, so is the pass rate 1? \n\n(c) What exactly is the error-correcting mechanism when one of the authored functions produces an error -- is it sent back through the LLM with a modified prompt? How many times? What constitutes an error? This seems like a potentially important part of the contribution, so I would encourage the authors to even consider adding an ablation condition demonstrating this is helpful for the performance of the system.\n\n(d) It would be informative to provide additional details on the generated modules for each task: how many are created for each task? How often does each one get used in solving test-cases? Do they each capture distinct concepts, or are some modules duplicates?\n\n# 2. In-context examples\n\nI don't believe there is any information into how in-context examples are chosen, which can greatly affect LLM performance.\n\n(a)  From the example prompts, it looks like a single function is given as an in-context example for step 1 and step 2, are these always COMPARE_SIZE and LOC, as shown in figures 15 and 16? \n\n(b) For the inference in-context examples are these chosen randomly from the training tasks that found a successful program? \n\n# 3. MISC \n\n(a) For the Raven task, my understanding is that the input is just a series of images. If this understanding is correct, how do you turn these images into a question? I am also quite curious to see the internals of the SOLVER generated module, is this module shared between Raven and MEWL, or does it employ distinct logic?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6843/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6843/Reviewer_vHZ5",
                        "ICLR.cc/2024/Conference/Submission6843/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6843/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698767372221,
            "cdate": 1698767372221,
            "tmdate": 1700508778236,
            "mdate": 1700508778236,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7gyT1q9mKK",
                "forum": "MNShbDSxKH",
                "replyto": "4iIhr5iJO1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6843/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6843/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Look forward for your constructive response (Part 1)"
                    },
                    "comment": {
                        "value": "Thank you for the constructive comments and insightful suggestions.\n\n>Q1. From my perspective, the biggest current weakness of the paper is that from the experimental design its hard to parse out exactly how the discovered modules affect the system's performance. Ostensibly, this can be gleaned from comparisons between GNSVR and VisProg/ViperGPT, but there are more differences between these systems beyond merging in discovered modules. Specifically, GNSVR uses a \"base API\" that is a combination of VisProg and ViperGPT, so the \"fair\" comparison would be against an ablated version of GNSVR that removes steps 1 and 2, and just tries to solve test-problems with the original API functions. This condition is considered in the ablation experiment (GNSVR w/o ML), but only a subset of the RefCOCO test-set. To solidify the claim that the improvement GNSVR observes stems from its discovered modules, this base condition should be added to all of the experimental set-ups (tables 1-5), for example, from Table 2 its unclear how much of the delta improvement between VisProg and GNSVR can be attributed to improvements in the base API versus improvements to the API from the new modules.\n\n**Q1. More ablation study on Performance**\n\nThanks for the insightful suggestions. We add more baseline experiments for comparison.\n\nFor the GQA and RefCOCO baseline's comparison, please refer to **Q2** below.\n\nIn Table 2 of the main paper, we showcase the capability of GNSVR's transfer learning. That is to say, the new modules learned from GQA and RefCOCO can be applied to the image editing and knowledge tagging task. Therefore, in the two tasks involved in Table 2, we did not learn any new modules but instead transferred and used new modules learned from other tasks. In Table 2, we can see that in all metrics, we surpassed VisProg. This performance improvement actually stems from the new modules generated from GQA and RefCOCO. It is these new modules that enabled functions that the inherent modules of VisProg couldn't achieve, thus leading to the improved performance of our system.\n\nAs for RAVEN and MEWL, we have implemented the ViperGPT and VisProg baseline experiments in the following way. \nFor VisProg, it requires a manual implementation of all modules by making use of the provided APIs. Thus, to enable VisProg handle Raven and MEWL tasks, we manually implement and debug new hand-crafted modules for VisProg to recognize and discover patterns to handle the task. We call this baseline **VisProg variant**. We also put the training examples in GNSVR' stage 1 into the prompt of **VisProg variant** for better performance. For ViperGPT, it has no manual modules and ask the LLMs to make use of the APIs to handle the instances. Thus, we manually write solutions for the training examples into the prompt of the ViperGPT to teach ViperGPT to handle the task. We call this approach **ViperGPT variant**. We have added such analysis into the revised paper. VisProg by itself needs a handcrafted solver module to find the target solution and it would be extremely difficult for ViperGPT to generate a solver from scratch. Thus, we add the solver module learnt from our GNSVR model to pre-defined API pool of VisProg and ViperGPT. As shown in table 1 and table 2, our GNSVR model achieves better performance than these two baselines, showing the great value of module learning for handling new tasks from only a few examples.\n\nTable 1: Compare our GNSVR model with baselines, VisProg and ViperGPT on RAVEN.\n| Methods | Center | L-R  | U-D  |\n| ------- | ------ | ---- | ---- |\n| VisProg variant  | 36.8   | 26.1 | 27.8 |\n| ViperGPT variant | 40.6   | 30.7 | 32.4 |\n| Ours    | 80.1   | 67.6 | 69.1 |\n\nTable 2: Compare our GNSVR model with baselines, VisProg and ViperGPT on MEWL.\n| Methods | shape | color |  material  |\n| ------- | -------- | -------- | --- |\n| VisProg variant | 35.2   | 35.9 | 34.9 |\n| ViperGPT variant | 37.8   | 38.2 | 36.7 |\n| Ours    | 43.7   | 45.3 | 41.0 |\n\n>Q2. Beyond this, I'm also slightly concerned about the design of the GNSVR w/o ML baseline. At inference time, is this baseline allowed to invoke arbitrary python logic in the style of ViperGPT (e.g. standard control flow constructs) or is it restricted to only using API function calls in the style of VisProg. I would imagine that the first condition would be more fair to evaluate GNSVR. Solving some tasks might require simple logic that the LLM knows how to express in python, but might not be directly expressible with a series of API calls. In GNSVR, this logic is incorporated into modules, but in the baseline the LLM should still have the opportunity to invoke similar logic in its test-time solutions (otherwise its impossible to properly evaluate the usefulness of the discovered modules). Please clarify which of these modes the baseline is operating in."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6843/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700211318816,
                "cdate": 1700211318816,
                "tmdate": 1700211318816,
                "mdate": 1700211318816,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lIppLcXDWt",
                "forum": "MNShbDSxKH",
                "replyto": "4iIhr5iJO1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6843/Reviewer_vHZ5"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6843/Reviewer_vHZ5"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the detailed responses! I've read through the author's comments, and after the rebuttal have a more favorable view of the paper. As such, I've updated my review accordingly. The consistent (though not terribly large) improvement over the baselines I suggested is an important mark in favor of the method, and tempers my fears about a fair comparison against ViperGPT/VisProg. The quality of the discovered modules having a reliance on a fairly large number of training examples is still a concern, but overall I would still support the paper's inclusion into the proceedings."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6843/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700509038190,
                "cdate": 1700509038190,
                "tmdate": 1700509090941,
                "mdate": 1700509090941,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "vlzGzJslie",
            "forum": "MNShbDSxKH",
            "replyto": "MNShbDSxKH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6843/Reviewer_QVVg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6843/Reviewer_QVVg"
            ],
            "content": {
                "summary": {
                    "value": "The authors introduce a Generative Neuro-symbolic Visual Reasoning Model (GNSVR) for vision-language tasks that involves creating, testing, and re-using neural \"modules\" i.e. neural network components that are steerable and solve well-defined abstraction and reasoning tasks. To evaluate these modules, a pre-trained LLM model is used to first determine whethere a new module is needed given a reasoning training set. Then, the LLM is asked to generate the function signature of the new module as well as an execution (reasoning) script to solve the given reasoning problem. The signature is used to generate a new module, which is evaluated against few samples to ascertain whether it is the right component. Once a generated module satisfies this criteria, it is added to the library of modules and the process is repeated. The empirical results show that GNSVR is able to develop novel reasoning modules, and utilize them successfully for a variety of vision language reasoning tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Firstly, the paper is very well written and the GNSVR method is very well explained, with a healthy split between using the main paper and supplementary for splitting the key information versus additional information.\n\nThe empirical performance of GNSVR for transfer tasks, as well as the examples of the modules generated is quite impressive - it would justify the overall approach to growing modules for reasoning and their effectiveness for few-shot  generalization."
                },
                "weaknesses": {
                    "value": "There are several components to the GNSVR framework but the paper provides no detailed analysis in the main paper of the importance of each of the components of the GNSVR framework.  While I think the framework overall is useful, the components are not all equally important to solve the reasoning problem and hence it is important to understand for future research on modular reasoning to understand what works and what doesn't, and if so why not.\n\nHow big of a role does \"good initlialization\" of the neural module operators plays?How important is defining the correct input and output format for a new module? How important is the selection of the few shot samples to evaluate a new module? (the authors say \"We extracted 300 examples from GQA, 100 from RefCOCO, 10 from Raven, and 10 from MEWL based on experimental experience.\" - is the experience here just cherry picking for results or something else?) How important is the LLM capability to learn new modules? What role does the prompt play for the LLM in evaluating existing modules and creating new ones? Without detailed analysis to support answers to all these questions, the paper is limited in terms of explaining the method beyong just presenting a new method and showing empirical results.\n\nLastly, I would not suggest the authors not report results on the RAVEN dataset. As shown independently in [1] and [2] the dataset contains flaws in choice design which enables models to learn shortcuts to solve the RPM reasoning task. I would recommend the authors use the i-RAVEN dataset inroduced in [1] instead.\n\n**References** \n\n1. Hu, S., Ma, Y., Liu, X., Wei, Y. and Bai, S., 2021, May. Stratified rule-aware network for abstract visual reasoning. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 35, No. 2, pp. 1567-1574).\n2. Spratley, S., Ehinger, K. and Miller, T., 2020. A closer look at generalisation in raven. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XXVII 16 (pp. 601-616). Springer International Publishing."
                },
                "questions": {
                    "value": "* Could the authors comment on why they chose to go with a neuro-symbolic approach versus a purely neural approach for defining the neural module scripts? Is it only for controllability, and if so can they comment on how a neural script (e.g. an LLM that abstracts the problem of generating the script and executing it) would compare? There have been approaches towards learning neural scripts and executing them dynamically at inference time for reasoning tasks in prior work e.g. [1]\n\n* I think the module initialization and evaluation during generation in GNSVR is closely related to the research on automatic group discovery and using it for model design paradigm introduced in computer vision previously e.g. [2, 3]. It would make the related works section more comprehensive to include discussion on how GNSVR relates to this growing subfield of automatic evaluation and model design.\n\n\n**References**  \n\n1. Rahaman, N., Gondal, M.W., Joshi, S., Gehler, P., Bengio, Y., Locatello, F. and Sch\u00f6lkopf, B., 2021. Dynamic inference with neural interpreters. Advances in Neural Information Processing Systems, 34, pp.10985-10998.\n2. Vendrow, J., Jain, S., Engstrom, L. and Madry, A., 2023. Dataset interfaces: Diagnosing model failures using controllable counterfactual generation. arXiv preprint arXiv:2302.07865.\n3. Gao, I., Ilharco, G., Lundberg, S. and Ribeiro, M.T., 2023. Adaptive testing of computer vision models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 4003-4014)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6843/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699290393149,
            "cdate": 1699290393149,
            "tmdate": 1699636792677,
            "mdate": 1699636792677,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "e3es4dIIe1",
                "forum": "MNShbDSxKH",
                "replyto": "vlzGzJslie",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6843/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6843/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Look forward for your further response (Part 1)"
                    },
                    "comment": {
                        "value": "Thank you for the constructive comments.\n\n>Q1.  While I think the framework overall is useful, the components are not all equally important to solve the reasoning problem and hence it is important to understand for future research on modular reasoning to understand what works and what doesn't, and if so why not. How big of a role does \"good initialization\" of the neural module operators plays? How important is defining the correct input and output format for a new module? How important is the selection of the few shot samples to evaluate a new module? (the authors say \"We extracted 300 examples from GQA, 100 from RefCOCO, 10 from Raven, and 10 from MEWL based on experimental experience.\" - is the experience here just cherry picking for results or something else?) How important is the LLM capability to learn new modules? What role does the prompt play for the LLM in evaluating existing modules and creating new ones? Without detailed analysis to support answers to all these questions, the paper is limited in terms of explaining the method beyond just presenting a new method and showing empirical results. \n\n**Q1. Ablation on each new module.**\n\nThanks for raising these insightful questions, which help in understanding the importance of different components in the GNSVR. We randomly select 800 samples from GQA test-dev split to further investigate the effectiveness of different components of GNSVR. To better present all experimental results, all the mentioned ablation studies are organized into three sections below.\n\n**Q1-1: Ablation on Prompt Design.**\n\nTable 1: Ablation study of Prompt Design on GQA.\n| Method GNSVR | GQA |\n| -------- | -------- |\n|  Baseline | 45.9 |\n|  w/o input and output format | 43.2 |\n|  w/o good initialization | 41.8 |\n|  w/o existing modules in prompt for module making | 45.0 |\n|  w/o creating new modules   | 44.7 |\n\nWe conducted a series of experiments to observe the impact of prompt design on the overall performance of GNSVR. Firstly, we removed the descriptions of input and output formats from the prompt. After removing these descriptions, the performance of GNSVR dropped by 2.7%. This is because, without clear guidance on input and output formats, the modules might output in the wrong format, leading to errors in subsequent parsing of the results. Furthermore, on top of removing the input and output format, we also removed some of the in-context examples and descriptions about module signatures from the prompt. The performance further declined. Since our method consists of three stages: module initialization, module generation, and module execution, where module initialization is the first step of our method. Without adequate module initialization as a foundation, the subsequent results are largely impacted. Therefore, we can see that without good initialization, our performance drops by 4.1%. \n\nRegarding the use of existing modules and creating new ones, from the table above, we can observe that not using the predefined modules from VisProg results in a 0.9% decrease in our performance. This demonstrates the robust module generation capability of GNSVR. Even without a series of predefined modules, our method can still build modules from scratch, solve problems, and the performance does not drop significantly. If we don't create new modules, then we are merely using the predefined modules. We can see that the result is 44.7%, which is 1.2% lower than our result of 45.9%. This performance gap highlights the effectiveness of the newly generated modules. By generating and using new modules, we can achieve better results.\n\n**Q1-2: Ablation on Sampling**\n\nIn this section, we introduce our sampling strategy at first. Then, we conduct an experiment to showcase how the sampling methods will impact GNSVR performance. Subsequently, we investigate how the number of training samples affects our results of different tasks.\n\n**Sampling Strategy**\n\nTable 2: Ablation  on Sampling Strategy on GQA.\n| Method GNSVR | GQA |\n| -------- | -------- |\n|  Baseline | 45.9 |\n|  Random Sampling | 44.3 |\n    \nOur sampling strategy: the GQA dataset contains five structural types: choose, logical, compare, verify, and query. These structural types inspired the idea of generating our new modules. Taking COMPARE_COLOR as an example, this newly generated module is generated to address questions related to color within the compare structural type. From the visualization of GQA, it is apparent that the query type can be addressed using existing VQA module from VisProg, and problems in the logical type can be decomposed into sub-problems of choose, compare, and verify types. Therefore, when selecting training samples, we randomly chose 100 samples each from the choose, compare, and verify types. Altogether, these three types comprise 300 samples, all sourced from the GQA train split. Hence, we are not cherry-picking our training samples; rather, we are selecting training samples based on the structural types of GQA."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6843/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700210902292,
                "cdate": 1700210902292,
                "tmdate": 1700211406376,
                "mdate": 1700211406376,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3Js79px0vh",
            "forum": "MNShbDSxKH",
            "replyto": "MNShbDSxKH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6843/Reviewer_CDRY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6843/Reviewer_CDRY"
            ],
            "content": {
                "summary": {
                    "value": "This paper relies on the abilities of Large Language Models, but in effect extends them by attempting to enable reuse of code, as opposed to generating code from scratch.  They focus on the visual domain, where tasks include asking questions which require both visual analysis and symbolic reasoning.  They demonstrate state-of-the-art performance on several tasks"
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper makes a clear case for it's own contribution, and that contribution does appear to be valuable - visual learning is an important task, and the prohibitive cost of using SOTA large language models makes reuse of code appealing (although they explicitly don't use ChatGPT4 \"due to the prohibitive cost\", so maybe this is less of an argument than it would be otherwise).  The fact that they can show it's use on several domains and types of tasks is also appealing."
                },
                "weaknesses": {
                    "value": "My main concern is that, based on the presentation, it seems that the authors took a lot of highly intricate API's for LLM's that large teams may have worked on and cobbled them together to solve a new task.  I refer to this section: \"The success of our GNSVR relies on a set of pre-defined modules and APIs as the starting point. We utilize handcrafted modules from VisProg (Gupta & Kembhavi, 2022) as our initial components. Additionally, we incorporate several new APIs from ViperGPT to enhance module creation. We also include some new APIs from ViperGPT (Sur\u00b4\u0131s et al., 2023) for making new modules.\"  I appreciate their novelty in how they use these API's, but the ratio of insights of these authors vs of the authors of the API's appears insignificant.\n\nI'm also not entirely convinced of the novelty of this paper.  I refer to \"Iterative Disambiguation: Towards LLM-Supported Programming and System Design\" (Pereira and Hartmann) and \"Self-planning Code Generation with Large Language Models\" (Jiang et al).  I don't think the fact that this is in the visual domain is enough to call it \"novel\", because there is virtually no engagement with visual modalities by the authors - as stated above, according to my understanding, they are using predefined modules which handle the interface between vision and language.\n\nIf I was given evidence against either of the two above claims - that all of the work (particularly the \"visual reasoning\" work) is being done by existing tools, or that the paper is not fundamentally using LLM's in a novel way - I would be happy to increase my score."
                },
                "questions": {
                    "value": "Please clarify exactly where you created novel algorithms or ideas.  If any of those ideas require more than iterative prompting, please state so explicitly."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6843/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6843/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6843/Reviewer_CDRY"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6843/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699541650515,
            "cdate": 1699541650515,
            "tmdate": 1699636792549,
            "mdate": 1699636792549,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oKJjcLRATc",
                "forum": "MNShbDSxKH",
                "replyto": "3Js79px0vh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6843/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6843/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Look forward for your further feedback (Part 1)"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer for the detailed comments and insightful suggestions.\n\n>Q1. The prohibitive cost of using SOTA large language models makes reuse of code appealing (although they explicitly don't use ChatGPT4 \"due to the prohibitive cost\", so maybe this is less of an argument than it would be otherwise).\n\n**Q1. Computational Efficiency.** Thanks for mentioning the compute efficiency of our modularized design and module reusage. We have calculated the average token number of our GNSVR model and the ViperGPT that has no module reusage mechanism when calling the LLMs. The averaged generated token number is shown in Table 1. It can be seen that our GNSVR's solutions are shorter and more efficient. This result is updated in the revised paper. This approach is particularly advantageous when calling expensive APIs from the OpenAI GPT family.\n    \nTable 1: Average token number of generated solutions.\n| Methods | GQA | RefCOCO |\n| -------- | -------- | -------- |\n| ViperGPT-Instruct     | 153.7     |  109.1  |\n| Ours-Instruct     | 62.3      |   54.4   |\n\n\n>Q2. My main concern is that, based on the presentation, it seems that the authors took a lot of highly intricate API's for LLM's that large teams may have worked on and cobbled them together to solve a new task. I refer to this section: \"The success of our GNSVR relies on a set of pre-defined modules and APIs as the starting point. We utilize handcrafted modules from VisProg (Gupta & Kembhavi, 2022) as our initial components. Additionally, we incorporate several new APIs from ViperGPT to enhance module creation. We also include some new APIs from ViperGPT (Sur\u00b4\u0131s et al., 2023) for making new modules.\" I appreciate their novelty in how they use these API's, but the ratio of insights of these authors vs of the authors of the API's appears insignificant.\n\n**Q2. About Reliance of Existing Modules.**\nWe agree that our GNSVR relies on some pre-defined modules as the initial start point to learn to generate new modules. However, we want to highlight that the novelty of GNSVR is **not** *\"taking a lot of highly intricate API\u2019s for LLMs to work on and cobble them together to solve a new task\"*. Instead, our novelty lies in **growing** and **reusing** the established modules that are learnt from the training set. The modules learnt by GNSVR can be applied to different domains, including 1). other instances of the visual reasoning task; 2). instances of a new reasoning task; and 3). adapting to new reasoning tasks by observing only a few training examples. Please refer to the **G2** of the **general responses** for further explanation.\n\n>Q3. I'm also not entirely convinced of the novelty of this paper. I refer to \"Iterative Disambiguation: Towards LLM-Supported Programming and System Design\" (Pereira and Hartmann) and \"Self-planning Code Generation with Large Language Models\" (Jiang et al). I don't think the fact that this is in the visual domain is enough to call it \"novel\", because there is virtually no engagement with visual modalities by the authors - as stated above, according to my understanding, they are using predefined modules which handle the interface between vision and language.\n\n**Q3. About the paper's novelty compared with existing works.**\nThanks for reminding related works[1,2]. We have added such revision in the related work section of the revised paper. In [1], Pereira and Hartmann used LLMs to progressively enhance and specify system subcomponents, empowering users to develop versatile programs through a systematic iterative disambiguation method. In [2], Jiang *et al* learned- to generate code with LLMs, which involves a planning phase for outlining solution steps and an implementation phase for generating code.\nBesides the **dense engagement with the visual modalities** as input, our GNSVR is different from them in **modularization** of code snippets for better **module expansion** and **module reusage**. These unique differences make our GNSVR model to have new capabilities like **growing** new modules to handle other instances of the visual reasoning tasks VQA, image grounding, RAVEN and MEWL and **reusing** these new modules in new tasks like image editing and knowledge tagging. Such modularization also offers better computational efficiency (See **Q1**) and model transparency with high-level module abstraction (See the *Generated Program* in Fig. 3, 5 and 6 for an example). Please refer to the **G2** of the **general responses** for further explanation.\n[1]. Iterative Disambiguation: Towards LLM-Supported Programming and System Design (Pereira and Hartmann).\n[2]. Self-planning Code Generation with Large Language Models (Jiang et al)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6843/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700210482720,
                "cdate": 1700210482720,
                "tmdate": 1700210482720,
                "mdate": 1700210482720,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0FfxRNSyB5",
                "forum": "MNShbDSxKH",
                "replyto": "alAJzqSTdT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6843/Reviewer_CDRY"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6843/Reviewer_CDRY"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response"
                    },
                    "comment": {
                        "value": "I am still not entirely convinced that the \"visual reasoning\" component is the main contribution, but I understand better now the novelty of your specific modular approach to programming with LLM's, and agree that in general modular programming is going to become increasingly important in the era of LLM's."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6843/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700576665612,
                "cdate": 1700576665612,
                "tmdate": 1700576665612,
                "mdate": 1700576665612,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]