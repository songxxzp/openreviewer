[
    {
        "title": "CMMLU: Measuring massive multitask language understanding in Chinese"
    },
    {
        "review": {
            "id": "o3qNLe246y",
            "forum": "ck4SG9lnrQ",
            "replyto": "ck4SG9lnrQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7312/Reviewer_ZnNq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7312/Reviewer_ZnNq"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a  Chinese multi-task benchmark dataset CMMLU  for better evaluating the language understanding ability of LLMs in the context of Chinese. Compared to previous benchmark datasets, besides the general tasks, CMMLU consists of many Chinese-specific tasks.\n\nMeanwhile, this work has also conducted a lot of experiments to check the performance of the 20 most popular non-Chinese-specific and Chinese-specific LLMs.  The experimental results provide a good reference for developers to choose the LLM in the context of Chinese."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. CMMLU is specifically designed for evaluating Chinese LLMs. It not only consists of general natural language understanding tasks, but also some region-specific tasks such as Chinese driving rules, food culture, and qualifications. Thus, CMMLU can better reveal the real LLM performance in the Chinese scenarios.\n\n2. This work invested many efforts in collecting non-publicly available questions to reduce the possibility that the collected questions have already been learned by LLMs.\n\n3. This work has evaluated many multilingual LLMs and many Chinese LLMs at the same time. Meanwhile, the authors also compare the best Chinese LLM Baichuan2-13B with the best LLM GPT4 by subjects. This comparison can answer the question of why we need Chinese LLMs/benchmarks in the Chinese scenarios.\n\n4. Many deep analyses have shown many interesting and useful findings."
                },
                "weaknesses": {
                    "value": "1. Although this work is technically sound and solid, CMMLU lacks enough novelty or other special contributions.  The major highlight is that CMMLU consists of some Chinese-specific tasks. This is more or less like an A+B incremental work.\n\n2. All questions are formatted as multiple-choice with 4 choices.   This may make it difficult to comprehensively test the performance of LLMs.\n\n3. The experimental methodology of most experiments is language-agnostic.  It only simply compares general LLMs vs Chinese LLMs and non-Chinese-specific tasks vs  Chinese-specific tasks.  I think more experiments should be deeply combined with the Chinese cultural and Chinese linguistic characteristics.\n\n4. This work needs to analyze the correlation between the performance reported by CMMLU and the real performance measured in the representative downstream NLU tasks. Otherwise, it is difficult to determine whether CMMLU can reflect the NLU performance of a LLM."
                },
                "questions": {
                    "value": "Besides the questions in Weakness, there are some minor questions:\n\n1) What form will this dataset be released in the future?\n\n2) Besides the Chinese-specific tasks and data source, is there any other Chinese-specific feature that has been considered in CMMLU?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Legal compliance (e.g., GDPR, copyright, terms of use)"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "This article involves data collection, especially non-public data collection.  This faces some copyright risks.\n\nHowever, I did not see how to address these ethics issues in this work."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7312/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7312/Reviewer_ZnNq",
                        "ICLR.cc/2024/Conference/Submission7312/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7312/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698489102733,
            "cdate": 1698489102733,
            "tmdate": 1700405478545,
            "mdate": 1700405478545,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hyoVZKHdVX",
                "forum": "ck4SG9lnrQ",
                "replyto": "o3qNLe246y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7312/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7312/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your comments"
                    },
                    "comment": {
                        "value": "Thank you for your insightful feedback. We appreciate the opportunity to clarify and expand upon certain aspects of our work.\n\n> Novelty and Special Contributions\n\nWhile we acknowledge that the paper is inspired by the MMLU paper, CMMLU is distinct in its data and analysis.  Our dataset was independently developed, not translated from existing English datasets. The analyses we conducted differ significantly from the MMLU paper and are specifically tailored to current LLMs. We believe our analysis offers unique insights and contribute meaningfully to LLM research.\n\n\n> All questions are formatted as multiple-choice with 4 choices. This may make it difficult to comprehensively test the performance of LLMs.\n\nWe acknowledge that the multiple-choice format of testing has its limitations. However, this format offers a standardized and accessible evaluation method, as evidenced by its prevalence in popular LLM evaluation frameworks and leaderboards, such as the one hosted by Hugging Face (https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard), of which around half is  multiple-choice tasks. Additionally, multiple-choice questions can be converted to a perplexity comparison of the different choices, which is the dominant approach for pre-trained LLMs evaluation, given that this is how instruction fine-tuning is carried out. The decision to use a multiple-choice format was made after careful consideration.\n\nCurrently, there is no universally-recognized benchmark capable of comprehensively evaluating the performance of LLMs. Most existing leaderboards use a combination of multiple datasets to provide a holistic assessment of LLMs. In this context, CMMLU is not intended to provide a holistic evaluation of LLMs, but rather be one of several benchmarks in a comprehensive evaluation strategy. \n\n\n> The experimental methodology of most experiments is language-agnostic. It only simply compares general LLMs vs Chinese LLMs and non-Chinese-specific tasks vs Chinese-specific tasks. I think more experiments should be deeply combined with the Chinese cultural and Chinese linguistic characteristics.\n\nYour suggestion to incorporate a more in-depth exploration of Chinese cultural and linguistic characteristics is well-received. To avoid misunderstanding, we want to first clarify that all data in CMMLU is in Mandarin Chinese. Our initial focus was to establish a broad comparison between multilingual and Chinese-specific LLMs using CMMLU. This approach was intended to set a baseline for future, more detailed studies. We agree that delving deeper into the nuances of the Chinese language and culture will further enrich our understanding of LLM capabilities, but this is beyond the scope of this work.\n\n\n> This work needs to analyze the correlation between the performance reported by CMMLU and the real performance measured in the representative downstream NLU tasks. Otherwise, it is difficult to determine whether CMMLU can reflect the NLU performance of a LLM.\n\nWe have included an analysis of the correlation between CMMLU performance and five prominent downstream English benchmarks in Appendix Section I. Our findings indicate that performance on CMMLU has a strong correlation with results on four of these benchmarks (with Pearson\u2019s Correlation r > 0.94), which span areas such as mathematics, commonsense reasoning, and coding. The slight exception is the PIQA task (with Pearson\u2019s Correlation still high at r = 0.88), where the task relevance is somewhat reduced due to most models achieving high scores (>80%) on this task. \n\n\n> What form will this dataset be released in the future?\n\nThe CMMLU dataset will be released under a 'Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License' to facilitate open-source research and ensure accessibility for non-commercial use.\n\n\n> Besides the Chinese-specific tasks and data source, is there any other Chinese-specific feature that has been considered in CMMLU?\n\nNo. Our focus is on the dataset tasks and sources, which we believe adequately address the knowledge-based evaluation needs of Chinese LLMs, consistent with MMLU for English and IndoMMLU for languages of Indonesia. We are open to exploring additional features in future iterations of CMMLU based on community feedback and evolving research needs."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7312/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700224399381,
                "cdate": 1700224399381,
                "tmdate": 1700224399381,
                "mdate": 1700224399381,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8UIBWujxkP",
                "forum": "ck4SG9lnrQ",
                "replyto": "hyoVZKHdVX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7312/Reviewer_ZnNq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7312/Reviewer_ZnNq"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your comments.\n\nYour answers have addressed some mentioned issues and I have raised the contribution from 2 to 3. Nonetheless, I still keep my overall recommendation because of the lack of enough novelty and contribution.\n\nBesides, after the initial review, I saw that CMMLU had been used in some PR scenarios, for example, BlueLM by vivo.   I don't know if this violates the anonymity policy."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7312/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700405824653,
                "cdate": 1700405824653,
                "tmdate": 1700405824653,
                "mdate": 1700405824653,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dbnMm8L6xM",
            "forum": "ck4SG9lnrQ",
            "replyto": "ck4SG9lnrQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7312/Reviewer_qmrE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7312/Reviewer_qmrE"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduced a fully Sinicized Chinese test benchmark, CMMLU, specifically designed to evaluate the knowledge and reasoning capabilities of language models in a Chinese context. CMMLU covered 67 topics ranging from basic disciplines to advanced professional levels, with answers specific to the Chinese region."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper conducted extensive experiments, including on the proprietary GPT-4 (even though OpenAI consistently updated GPT versions without much fanfare). \n\nThe content was detailed and held significant practical value for the Chinese domain."
                },
                "weaknesses": {
                    "value": "However, an LLM passing professional exams doesn't necessarily indicate its true capabilities, raising concerns about construct validity. \n\nThe crisis of research replication based on language models was severe, and the evaluation methods had limitations. \n\nAssessing the political biases inherent in the language models presented in the benchmark was challenging and required naturalistic observation."
                },
                "questions": {
                    "value": "1. One concern I had was that this Chinese test benchmark did not include evaluation criteria for Chinese machine translation. Many studies are now focusing on evaluating the generalized machine translation capabilities of LLMs. Given the extensive work the authors did on this benchmark, how did authors view the evaluation criteria for Chinese translations?\n\n2. The outputs of LLMs were uncertain. Even a minor change in a prompt could lead to variations in the output. In light of this benchmarking paper, how did the authors perceive this issue? How should the benchmark address the inherent unpredictability of LLMs?\n\n3. Typically, the Chain-of-Thought method had proven successful on LLMs. However, this paper concluded that the Chain-of-Thought was not effective in enhancing model performance, which contradicted the feedback received from practical use of LLMs with the Chain-of-Thought. A more detailed analysis and explanation were requested.\n\n4. LLMs demonstrated strong In-Context Learning capabilities. It would be worth exploring whether adding appropriate knowledge to the prompt could answer benchmark questions to validate the benchmark's effectiveness.\n\n5. It was known that LLMs would respond cautiously to safety questions when posed in English. However, when asked in less common languages, they might provide bolder answers, potentially bypassing restrictions. Did the CMMLU safety benchmark consider addressing this phenomenon?\n\n6. How did the authors ensure that the proposed test benchmark was free from data contamination?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7312/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7312/Reviewer_qmrE",
                        "ICLR.cc/2024/Conference/Submission7312/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7312/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698685558994,
            "cdate": 1698685558994,
            "tmdate": 1700450837757,
            "mdate": 1700450837757,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "I7WJkVTDqw",
                "forum": "ck4SG9lnrQ",
                "replyto": "dbnMm8L6xM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7312/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7312/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your comments"
                    },
                    "comment": {
                        "value": "Thank you for your detailed review of our paper. We appreciate your concerns and have addressed each of them below with the aim of clarifying our approach and methodology.\n\n> An LLM passing professional exams doesn't necessarily indicate its true capabilities, raising concerns about construct validity.\n\nTo comprehensively evaluate the capabilities of an LLM, evaluation needs to encompass aspects including knowledge, commonsense reasoning, fairness, and generative capabilities. This paper focuses on evaluating knowledge-related aspects in the Chinese language, as a component of a broader evaluation, consistent with the use of MMLU in English.\n\nSchool exam questions have been extensively employed as a proxy for evaluating knowledge in LLMs. Examples include the use of the English MMLU dataset in the official release documentation/technical reports for GPT-4, LLaMA-2, Falcon, and various other LLMs, and the HuggingFace Open LLM leaderboard (https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard), the most popular LLM leaderboard today.\n\nWe believe that developing such a benchmark for Mandarin Chinese is essential for more inclusive evaluation of LLMs beyond the English language. \n\n\n> The crisis of research replication based on language models was severe, and the evaluation methods had limitations.\n\nWe absolutely share your concerns regarding the importance of reproducibility in research, but do not agree that this is a limitation of the dataset. To ensure transparency, we have meticulously documented the experimental settings in Section 4, including the number of shots for few-shot learning, and the exact prompt used. As such, all experiment results in the paper can be reproduced. Additionally, a number of subsequent studies have successfully replicated our results based on the arXiv preprint version of this paper, and we've also made our code completely public. In adherence with the policy of anonymity, we refrain from specifically referencing these papers here as they cite the arXiv preprint, compromising the anonymity of this paper, but we would be happy to provide the links after the review period.\n\nAdditionally, the issue of LLM research replication is an issue with models and model training rather than evaluation datasets, which is the focus of this paper.\n\n> Assessing the political biases inherent in the language models presented in the benchmark was challenging and required naturalistic observation.\n\nOur paper focuses on knowledge evaluation, not assessing the political biases of LLMs, which is an orthogonal dimension of LLM evaluation, as stated above. \n\n\n> One concern I had was that this Chinese test benchmark did not include evaluation criteria for Chinese machine translation.\n\nOnce again, this benchmark specifically focuses on knowledge evaluation, as a critical component of comprehensive LLM evaluation. This focus is consistent with earlier work such as English MMLU (https://arxiv.org/pdf/2009.03300.pdf) at ICLR 2021 and Indonesian MMLU (https://arxiv.org/pdf/2310.04928.pdf)  at EMNLP 2023.  Machine translation is an instance of a task which could be included in a test suite to separately evaluate the generative abilities of the LLM, as mentioned above. That is, we agree that machine translation is an important aspect of broader LLM evaluation, but it is orthogonal to knowledge-based evaluation.\n\n\n> Inherent unpredictability of LLMs and prompt sensitive\n\nThis is a more general issue with any evaluation of LLMs, and not specific to our benchmark. We have strived to mitigate this by employing simple and commonly-used prompts, to minimize variance in model outputs. This approach leads to fairer comparison of different models. Also of note is that we tested several (i.e., 3) widely-used prompts for multiple choice questions on more than 5 models. We find that the standard deviation in the average score is <1%.\n\n\n> Typically, the Chain-of-Thought method has proven successful on LLMs. However, this paper concluded that the Chain-of-Thought was not effective in enhancing model performance, which contradicted the feedback received from the practical use of LLMs with the Chain-of-Thought. A more detailed analysis and explanation were requested.\n\nThe effectiveness of Chain-of-Thought (COT) in LLMs primarily comes about in tasks requiring reasoning. For memory-intensive tasks (e.g., recalling historical facts), COT can not only be less effective but potentially impede performance.\n\nIn the CMMLU benchmark, the majority of tasks are knowledge-intensive rather than reasoning-based. This explains why COT does not enhance performance in our case. Furthermore, as discussed in Section 4.2 of our paper, issues like regex matching failures can further diminish effectiveness."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7312/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700224292854,
                "cdate": 1700224292854,
                "tmdate": 1700224292854,
                "mdate": 1700224292854,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "94QhBGg5ex",
                "forum": "ck4SG9lnrQ",
                "replyto": "dbnMm8L6xM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7312/Reviewer_qmrE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7312/Reviewer_qmrE"
                ],
                "content": {
                    "comment": {
                        "value": "The authors adequately explained the issues raised and all my other concerns were answered in the comments and responses of the other reviewers. CMMLU has been used as a benchmark in many papers. I hope the authors will keep the dataset up to date in the future.\n\nSoundness changed to 3. Rating raised to 6."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7312/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700450876256,
                "cdate": 1700450876256,
                "tmdate": 1700450924865,
                "mdate": 1700450924865,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "VekoXgLTRE",
            "forum": "ck4SG9lnrQ",
            "replyto": "ck4SG9lnrQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7312/Reviewer_CUim"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7312/Reviewer_CUim"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduced CMMLU, a benchmark designed to assess the multi-task language understanding capabilities in Chinese. The authors ran the benchmark on various open-source and API-based models and performed extensive analysis to identify several factors that impact model performance and propose actionable directions for enhancing LLMs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. The CMMLU benchmark is very comprehensive, covering a wide range of subjects.\n\n2. The paper addresses the significant gap in evaluating Chinese language and cultural context understanding, a critical aspect given the dominance of English-centric benchmarks.\n\n3. The work can be very useful for Chinese LLM community.\n\n4. The paper provides an in-depth analysis of the performance of various LLMs, under different evaluation settings.\n\n5. The paper also provides very interesting findings in terms of chain-of-thought, SFT/RLHF, etc."
                },
                "weaknesses": {
                    "value": "1. A human baseline is lacking for the benchmark. It'd be great to see what level of accuracy human can get on the benchmark.\n\n2. There's no discussion on the difficulty distribution of questions in each subset. A well designed benchmark or test should cover questions spanning all difficulty levels from the easiest to the hardest. It's unknown what the difficulty distribution is for each subset. If difficulty distribution is very centric (for example, all samples in a subset are all very easy or very hard), then models will be likely to get them all correct or all wrong, which cannot provide a **smooth** estimation of the model's ability. A non-smooth evaluation can also be related to the phenomenon of \"emergent ability\". See me question 2."
                },
                "questions": {
                    "value": "1. In page 1, \"numerous tasks within CMMLU have answers speci\ufb01c to China, which may not be universally applicable or considered correct in other regions or languages.\". Do you think it would be a good idea to have questions **with answers that are not generally agreed upon worldwide** in the datasets? How many samples of this kind are there in the benchmark?\n\n2. When you evaluate open-source models, have you seen \"emergent ability\" in terms of model's size? More precisely, are there some tasks that can only be solved by a large model? If so, then what are the difficulty distributions of those tasks?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7312/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7312/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7312/Reviewer_CUim"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7312/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699661857726,
            "cdate": 1699661857726,
            "tmdate": 1700432242489,
            "mdate": 1700432242489,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7TSBccMEme",
                "forum": "ck4SG9lnrQ",
                "replyto": "VekoXgLTRE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7312/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7312/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your comments"
                    },
                    "comment": {
                        "value": "Thank you for your insightful and detailed review of our paper. We have tried to address your concerns by doing additional human annotation on answer agreement, and include 2 more sections relating to difficulty and emergent abilities, as detailed below.\n\n> human baseline\n\nThank you for your suggestion on incorporating a human baseline. Given the wide range of knowledge areas covered in this benchmark, including STEM and social science, establishing a comprehensive human baseline is challenging. The broad knowledge spectrum makes it difficult for any single individual to provide a meaningful baseline. Moreover, recruiting subject matter experts for each domain, given the extensive scope of the benchmark, is not feasible. \n\nHowever, we appreciate the need to provide some form of human comparison. To address this, we propose using statistical data from exams as a proxy for human performance. In China, most school exams require a minimum passing score of 60%, drive licence exams require a score of 90%, etc., suggesting that a competent human test-taker would likely achieve at least that accuracy rate in each subject area. We can supplement this with additional statistics on average scores for various levels of tests to provide a more nuanced view of human performance, although this will be at an aggregate level from educational authorities\n\nWe believe this approach, while not perfect, offers a practical and informative way to include a human baseline in our paper. It would provide readers with a point of reference to better understand the performance of LLMs in relation to human capabilities.\n\nWe are open to further suggestions and would be grateful for your opinion on this proposed method.\n\n\n> Difficulty distribution of questions in each subset\n\nWe appreciate your insight into the importance of question difficulty distribution. We've analyzed this from two perspectives. Firstly, the CMMLU benchmark encompasses a diverse range of difficulty levels: 5 subjects at primary school level, 10 at middle/high school level, 23 at college level, and 29 at professional level, ensuring a comprehensive difficulty spectrum.\n\nSecondly, to estimate the difficulty distribution within each subject, we evaluated the top 20 models from our main results table. Each question was treated as a data point, and we recorded the number of models that correctly answer each question. This approach allowed us to map out the difficulty distribution across subjects. We've added a section in the Appendix (Section C) to analyze this. Briefly, our findings show that most subjects exhibit a single peak in difficulty, indicating a smooth challenge gradient within those subjects. However, some areas, like machine learning and professional law, display bimodal difficulty distributions, suggesting a mix of easy and hard questions, with fewer intermediate ones. Additionally, the maximum for the distribution varies quite a bit, suggesting some subjects such as \u201carts\u201d are generally easy/LLMs cover the domain knowledge well, while others such as \u201ccollege mathematics\u201d are much harder/LLMs do not have good coverage of the necessary domain knowledge. \n\n\n> Should we add answers that are not generally agreed upon worldwide to this dataset?\n\nOur stance is that a language-specific benchmark should reflect the knowledge and perspectives relevant to its target audience. For contentious historical or political topics, aligning with regional consensus is in line with the benchmark's purpose.\n\nWe first analyzed the different subject areas, and concluded that such questions will only appear in education, history, politics related subjects.. \n\nThrough careful selection, we identified 7 subjects which may contain such questions:: Chinese history, world history, high school politics, world religions, journalism, legal and moral basis, and Marxist theory. The total number of questions across these subjects is 1362. We then sampled 10%=136 questions to do human annotation, sourcing an expert with a Master's degree in International Relations, currently researching Chinese history, world history, and politics. Their role was to identify questions whose answers might not be universally accepted or agreed upon globally.\n\nThis annotation process revealed that 19 questions within our sample had answers potentially subject to regional or cultural differences in interpretation. Based on this finding, we estimate that approximately 190 questions in our dataset (roughly 2% in total across the 7 subjects) might have answers that are not universally agreed upon. Given this small proportion, we believe that the presence of such questions will not significantly impact the overall evaluation results of our study\n\nWe can add this analysis to the paper if you consider it will enhance the content."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7312/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700224129360,
                "cdate": 1700224129360,
                "tmdate": 1700224129360,
                "mdate": 1700224129360,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UOFEdKYaRN",
                "forum": "ck4SG9lnrQ",
                "replyto": "5q3WIq2Z1u",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7312/Reviewer_CUim"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7312/Reviewer_CUim"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your reply.\n\n**human baseline**\n\nRegaring the human baselines, I don't think the commonly used passing scores in real life (e.g, 60% for school exams, and 90% for driving licence exams) are good indicators of human average scores. Those passing scores are usually the minimum standard or a requirement for practiced learners, which cannot represent the **averaged** ability of a group of people.\n\nBut just as you said, setting the human baseline can be challenging. I think it'd be great if the paper can provide such baselines, but it's still OK if those are not available.\n\n**Other questions**\n\nYou reply has addressed my other concerns.\n\n**Score Change**\n\nGiven you additional results and reply, I changed my scores as follows:\n\nContribution: 3 --> 4\n\nRating: 6 --> 8"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7312/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700432204139,
                "cdate": 1700432204139,
                "tmdate": 1700432204139,
                "mdate": 1700432204139,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]