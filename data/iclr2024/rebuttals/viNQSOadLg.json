[
    {
        "title": "Biological Sequence Editing with Generative Flow Networks"
    },
    {
        "review": {
            "id": "aWzZaLbNIT",
            "forum": "viNQSOadLg",
            "replyto": "viNQSOadLg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7903/Reviewer_Arqn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7903/Reviewer_Arqn"
            ],
            "content": {
                "summary": {
                    "value": "This submission isn concerned with the application of GFNs to sequence modifications. I\u2019m not up to date with the relatively large number of works on similar approaches. The related work section appears in the supplementary material and does not attempt to discuss the differences between this paper and the papers listed there. This is a key issue that most be resolved. Also there is no comparison with earlier GFN methods: is there any for these problems? These concerns have made me give a lower score than what I otherwise would have given. I have minor comments and questions below, but I really enjoined reading this submission. It provides a description of the method at hand as well as the general GFN approach on very well chosen level. It is mostly well written. Moreover, the approach make sense and the results are good. I would like to accept it, but the above issue should be resolved."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Well written, technically strong, good resuls."
                },
                "weaknesses": {
                    "value": "Poorly described relation to previous research and unclear if compared with the right methods."
                },
                "questions": {
                    "value": "* that can provide consistency condition, Bengio et al. (2021) formulates ow- matching loss function as follows: LFM(s; \u03b8) = log P\u2200s\u2032:s\u2032\u2192s F\u03b8(s\u2032 \u2192 s) P\u2032\u2032\u2032\u2032 F\u03b8(s \u2192 s\u2032\u2032) . (4) Moreover, as an alternative objective function \u2028Page 3 \u2028Sure but which do you use? \u2028\n* 2 \u00a0R(x)\u2028Page 3 \u2028Not defined here. \u2028\n* GFlowNet\u2019s ow function F\u03b8(\u00b7) to identify sub-optimal positions of x, and subsequently replace the sub-optimal parts with newly sampled edits based on the stochastic policy \u03c0(\u00b7). pretrained ow function F\u03b8(\u00b7) Page 4 \u2028At this point its implementation is not clear or why it can be pre trained \u2028\n* For instance for the DNA sequence x = \u2018ATGTCCGC\u2019, appending token a = \u2018C\u2019 to x:2, we get x:2 + a = \u2018ATC\u2019. \u2028Page 4 \u2028I would guess that this is an insert operation on x, but it is not clear from the description, which actually suggests that the suffix from position t+1 is removed from x.\u2028New guess: you are stepwise building a sequence and you either use the character from the given sequence or another. You should make this more clear. \u2028\n* 5 \u00a0can\u2028Page 4 \u2028Would \u2028\n* 6 \u00a0Pa\u2032\u2208A Page 4 \u2028It should be made clear that the given character x_t always belongs to the available actions. \u2028\n* 7 \u00a0chosen by the algorithm Page 4 \u2028Point out how or, alternatively, where you describe it. \u2028\n* 8 \u00a0regularization parameter \u03bb allows tuning Page 5 \u2028Point out how it is set or where you describe it. \u2028\n* 9 \u00a0RF,T represents the reward of a sequence with length T generated using the ow function F\u03b8(\u00b7) \u2028Page 5 \u2028It IS the reward. Formulate it as a rv with the distribution induced by the flow function, with reference to the correct equation. \u2028\n* 10 \u00a0Levenshtein  Page 6 \u2028Spelling. \n\n* Higher binding activity is preferable Page 6 \u2028In what sense? It may not be so in a biological system. \u2028\n* 12 \u00a0diversity Page 7 \u2028You need a better measure that also takes the improvement into account. \u2028\n* 13 \u00a014.34 Page 7 \u2028This should be in bold, right?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7903/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7903/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7903/Reviewer_Arqn"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7903/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698491082389,
            "cdate": 1698491082389,
            "tmdate": 1700667814695,
            "mdate": 1700667814695,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HCyNRDQhUx",
                "forum": "viNQSOadLg",
                "replyto": "aWzZaLbNIT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7903/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7903/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to comments"
                    },
                    "comment": {
                        "value": "We sincerely appreciate your thoughtful review of our manuscript. In response to your feedback, we meticulously revised the introduction, conducted additional experiments, and added new experimental results to strengthen our paper. Please find below a detailed point-by-point response addressing your comments.\n\n## Related Works\nExisting machine learning methods within the domain of biological sequences including the ones discussed in the related works have predominantly concentrated on generating novel *de novo* sequences with desired properties from scratch. Therefore, there is an inherent risk of deviating significantly from naturally occurring sequences compromising safety and predictability. In contrast, this paper explores the generation of edited sequences that enhance the properties of existing sequences, ensuring the edited counterparts maintain similarity to their original counterparts. Here, we take an input seed sequence and modify a few elements to improve its properties. \n\nTo underscore the limitations of existing biological sequence design methods, we conducted experiments on AMP and CRE datasets using GFlowNet to generate sequences from scratch. Our observations reveal that GFlowNet-generated sequences for the CRE dataset differ from existing sequences at 76.29% of locations, and for the AMP dataset, the difference is found in 95.24% of locations. In contrast, the proposed GFNSeqEditor demonstrates the ability to generate new edited sequences differing from existing ones in less than 35% of locations across all datasets. Moreover, employing existing biological sequence methods the difference between generated sequences and existing ones cannot be controlled while using the proposed GFNSeqEditor the amount of edits can be controlled by hyperparameters. \n\nIn order to highlight the differences between biological sequence editing and biological sequence design problems we have added a discussion to the introduction of the revised manuscript. This revision can be found on pages 1 and 2 highlighted in red. Also, we extend the related works discussion in Appendix D and the revisions are highlighted in red.\n\nFurthermore, another aspect that distinguishes the proposed GFNSeqEditor from prior works is that existing methods are not able to combine multiple sequences to create a new sequence. In section 4.3, the proposed algorithm combines sequences to generate a new sequence similar to its parent sequences. In section 4.3, we discuss that this can also be used for sequence length reduction which has important applications in vaccine therapies.\n\n## Comparison with GFN Methods\nExisting GFlowNet methods can be used for biological sequence design from scratch and as a result, they suffer from the same problems as those of other biological sequence design methods discussed above when it comes to performing sequence editing. However, since GFlowNet can generate sequences token by token sequentially, there is a naive approach to employ GFlowNet for sequence editing. In this approach, the first tokens of a sequence $x$ can be given to GFlowNet and then the GFlowNet generates the rest of the tokens. We add this as a baseline to Table 1 in the revised version and the baseline is called GFlowNet-E. For example, for the CRE dataset, the GFlowNet-E gets the first 130 tokens and generates the rest of 70 tokens. Compared to GFNSeqEditor, this approach lacks the flexibility to choose edit locations and GFNSeqEditor outperformed GFlowNet-E. This indicates the effectiveness of sub-optimal position identification of GFNSeqEditor."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7903/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700193472431,
                "cdate": 1700193472431,
                "tmdate": 1700193472431,
                "mdate": 1700193472431,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "icLy0faNKd",
                "forum": "viNQSOadLg",
                "replyto": "aWzZaLbNIT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7903/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7903/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Questions"
                    },
                    "comment": {
                        "value": "Please find below our responses to your questions.\n\n1. We used trajectory balance loss in equation (5) to train the flow function. On page 7 before section 4.1 of the revised version, we made clear that we used trajectory balance loss. The revision is highlighted in red.\n2. In general the reward function $R(x)$ can be any function and it is determined by the environment. In experiments we consider the sequence property as the reward since in experiments our goal was to maximize those properties.\n3. In the first paragraph of section 3 we add that \u201cAssume that the flow function $F_\\theta(\\cdot)$ is trained on an available offline training data.\u201d\n4. In order to make the sequence editing procedure of GFNSeqEditor more clear, in section 3.1 we add that \u201cGFNSeqEditor constructs edited sequences token by token and for each position $t+1$ it examines if $x_{t+1}$ should be used or not.\u201d\n5. We revised can to would.\n6. We add a comment on page 5 that \u201cAssume that $x_t \\in \\mathbb A$, $\\forall t$ meaning that $x_t$ is always in the available actions.\u201d\n7. To clarify how to choose hyperparameters $\\delta$, in this revised version after equation (6) we add that \u201cChoosing larger $\\delta$, it is more probable that the algorithm identifies $x_{t+1}$ as sub-optimal\u201d. Furthermore, to clarify how to choose hyperparameter $\\sigma$ after equation (7) we add that \u201cThe relation between $\\sigma$ and the algorithm performance will be analyzed in section 3.3 and Appendix E.\u201d\n8. In order to clarify the effect of $\\lambda$ in this revised version after equation (9), we add \u201cChoosing larger $\\lambda$ leads to obtaining smaller number of edits\u201d.\n9. Based on this comment, we revise the notation and in the revised version $R_{F,T}$ denotes the expected reward of a sequence with length $T$ generated using the flow function $F_\\theta(\\cdot)$.\n10. We checked the spelling and we believe it is correct.\n11. We followed the prior studies such as Trabucco et al., (2021) and Jain et al., (2022) in which the goal in the TFbinding dataset is to increase the binding activity.\n12. We would like to clarify that diversity itself is an important measure in generating biological sequences. Higher diversity as it is defined in section 4 among edited sequences increases the chance that at least one of the edited sequences successfully works in real experiments since it is expected that similar sequences show somewhat similar behavior in real experiments. Furthermore, diversity defined on page 7 is used before to evaluate the performance of biological sequence design methods (see e.g. Jain et al., (2022)).\n13. We made14.34 bold in Table 1."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7903/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700194139012,
                "cdate": 1700194139012,
                "tmdate": 1700194139012,
                "mdate": 1700194139012,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HEE8BFrQqi",
                "forum": "viNQSOadLg",
                "replyto": "HCyNRDQhUx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7903/Reviewer_Arqn"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7903/Reviewer_Arqn"
                ],
                "content": {
                    "title": {
                        "value": "Related works and comparison"
                    },
                    "comment": {
                        "value": "I find these new texts well-written, clarifying, and clearly adding to the value of the submission."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7903/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700573350275,
                "cdate": 1700573350275,
                "tmdate": 1700573350275,
                "mdate": 1700573350275,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "GmAJ4Hec1C",
            "forum": "viNQSOadLg",
            "replyto": "viNQSOadLg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7903/Reviewer_aGg2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7903/Reviewer_aGg2"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces an innovative algorithm called GFNSeqEditor, which is specifically crafted to enhance sequences by optimizing their desired properties. GFNSeqEditor harnesses the power of pretrained flow functions and devises a set of destructive operations, whether tokens are modified or not. It then reconstructs the altered tokens using these pretrained flow functions. The process of destruction and subsequent reconstruction is governed by three crucial hyperparameters: $\\lambda$, $\\alpha$, and $\\delta$. These hyperparameters play a pivotal role in achieving a balance between exploration and exploitation, while also managing the trade-offs and risks associated with expected improvements.\n\nThe authors provide a comprehensive analysis of these proposed hyperparameters, which intuitively guide the algorithm's behavior. To evaluate its effectiveness, GFNSeqEditor is benchmarked against classical editing methods across three distinct sequential generation tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper excels in storytelling, skillfully introducing a promising generative model for sequence editing. The approach itself is novel and the underlying concept is commendable. The subsequent algorithm, while simple, remains straightforward, and the mathematical analysis of the newly introduced hyperparameters is intuitively presented. Overall, this paper is highly accessible and a pleasure to read."
                },
                "weaknesses": {
                    "value": "The primary weakness of this paper lies in its experimental validation. In my opinion, the experiments conducted here fall short of adequately substantiating the proposed idea. There are several issues that need addressing:\n\n**Pretraining Discrepancy**: One notable concern is the difference in the starting points for experimentation. While this work leverages pretrained GFN models, other baselines begin from scratch. This discrepancy could potentially lead to an unfair comparison.\n\n**Baseline Variety**: The baseline comparisons should extend beyond the scope of other generative models and optimization techniques in biological sequence design. It would be beneficial to incorporate baseline methods such as offline model-based optimization [1], which are tailored to extrapolate sequences from offline datasets, thereby yielding \"improved sequences.\"\n\n**Evolutionary Algorithms**: To provide a more comprehensive perspective on the proposed approach, the paper could benefit from the inclusion of promising evolutionary algorithms specifically designed for biological sequence design [2].\n\n**Comparison with GFN Baselines**: Additionally, conducting a thorough comparison with GFN baselines would be valuable in demonstrating the relative strengths and weaknesses of the proposed method when contrasted with models of similar architecture.\n\nAddressing these concerns would significantly enhance the rigor and completeness of the experimental evaluation in the paper.\n\n[1] Trabucco, Brandon, et al. \"Design-bench: Benchmarks for data-driven offline model-based optimization.\" International Conference on Machine Learning. PMLR, 2022.\n\n[2] Sinai, Sam, et al. \"AdaLead: A simple and robust adaptive greedy search algorithm for sequence design.\" arXiv preprint arXiv:2010.02141 (2020)."
                },
                "questions": {
                    "value": "1. Could you please elaborate on the process you used for pre-training GFN?\n\n2. Have you conducted a comparison with baseline models (e.g., Seq-to-Seq) using the pretrained GFN as a component?\n\n3. Does this algorithm demonstrate improvements in scalability?\n\n4. Is this algorithm more beneficial than naive search algorithms based on pretrained policies, such as beam-search or MCTS?\n\n5. Have you performed experiments related to Theorem 1 and 2? Inclusion of such experiments would likely enhance the overall quality of the paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7903/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7903/Reviewer_aGg2",
                        "ICLR.cc/2024/Conference/Submission7903/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7903/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698634868012,
            "cdate": 1698634868012,
            "tmdate": 1700631542638,
            "mdate": 1700631542638,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "i0xfs5pXlW",
                "forum": "viNQSOadLg",
                "replyto": "GmAJ4Hec1C",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7903/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7903/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Comments"
                    },
                    "comment": {
                        "value": "We would like to thank you for taking time to review our paper and let us know your valuable comments. Based on your comments, we have added more discussions on related works to the introduction and enriched the experiments section with additional experimental results. Kindly find below a detailed point-by-point response addressing each of your comments.\n\n## Pretraining\nWe would like to clarify that other baselines also use a pretrained model to perform editing. While we trained GFlowNet for GFNSeqEditor, we trained an MLP as the proxy model for Ledidi and Directed Evolution (DE) baselines. Ledidi and DE are highly dependent on the proxy model to evaluate their candidate edits and use these evaluations to proceed to the next iteration for editing. Also, we trained a Transformer for the Seq2Seq baseline and Seq2Seq employs the trained transformer model to perform sequence editing. Please note that:\n1. We used exactly the same training data to pretrain GFlowNet and models used by the other baselines. \n2. Due to the space limit, we put the experimental details in Appendix C.1.\n\nTherefore, we believe that comparisons in the paper are fair since all baselines use a trained model and all models are trained on the same training data. Thanks to this comment to improve the clarity of the experiment section, we expand implementation details explanations in Appendix C.1 and also in section 4 we refer the readers to Appendix C.1.\n\n## Baselines\nWe would like to clarify that biological sequence design methods such as offline model-based optimization models including the ones discussed in [1] do not tackle the problem of sequence **editing** and as a result, they are not appropriate to be directly used for biological sequence editing for two main reasons: \n1. Biological sequence design models usually generate entirely new sequences from scratch. Therefore, sequences generated by these methods are expected to be greatly different from existing sequences that need editing. Therefore, it might be infeasible to generate diverse sequences using biological sequence design models that are similar to the existing sequences. \n2. The difference between generated sequences and existing ones cannot be controlled by employing biological sequence design methods while using the proposed algorithm by setting hyperparameters one can control the number of edits.\n\nTo underscore the limitations of biological sequence design methods, we generate new sequences from scratch using GFlowNet and we observe that on average for CRE dataset the generated sequences are different from existing sequences (i.e. sequences in the datset) in 76.29% of locations and it is not possible to control the amount of differences between generated sequences and existing ones. Also, for AMP dataset the generated sequences are different from existing ones in 95.24% of locations. In contrast, the proposed GFNSeqEditor demonstrates the ability to generate new edited sequences differing from existing ones in less than 35% of locations across all datasets. To highlight the differences between biological sequence editing and biological sequence design problems we have added a discussion to the introduction of the revised manuscript. The revision can be found on pages 1 and 2 highlighted in red.\n\n## Evolutionary Algorithms\nThanks for drawing our attention to the evolutionary algorithm in [2]. Note that [2] implements a standard directed evolutionary algorithm for sequence design. Our directed evolutionary baseline is the same as the evolutionary algorithm presented in Algorithm 1 of [2]. We cite the reference [2] in the revised version.\n\n## Comparison with GFN Baselines\nExisting GFN baselines can be used for biological sequence design from scratch and as a result, they suffer from the same problems as those of other biological sequence design methods discussed above when it comes to performing sequence editing. However, since GFlowNet can generate sequences token by token sequentially, there is a naive approach to employ GFlowNet for sequence editing. In this approach, the first tokens of a sequence $x$ can be given to GFlowNet and then the GFlowNet generates the rest of the tokens. We add this as a baseline to Table 1 in the revised version and the baseline is called GFlowNet-E. For example, for the CRE dataset, the GFlowNet-E gets the first 130 tokens and generates the rest of 70 tokens. Compared to GFNSeqEditor, this approach lacks the flexibility to choose edit locations. GFNSeqEditor outperforms GFlowNet-E and this indicates the effectiveness of sub-optimal position identification of GFNSeqEditor in equation (7)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7903/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700197351672,
                "cdate": 1700197351672,
                "tmdate": 1700197351672,
                "mdate": 1700197351672,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "w3AOvSIXJf",
                "forum": "viNQSOadLg",
                "replyto": "GmAJ4Hec1C",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7903/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7903/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Questions"
                    },
                    "comment": {
                        "value": "Please find below our responses to you questions.\n\n**Response to Q1.** We trained an active learning based GFlowNet model following the setting in Jain et al. (2022). In the active learning setting, at each round of active learning $t \\times K$ candidates generated by GFlowNet are sampled and then top $K$ samples based on scores given by a proxy are chosen to be added to the offline dataset. Here offline dataset refers to an initial labeled dataset. To train the GFlowNet, we employed the same proxy models as those used by other baseline methods. For all datasets, we set the number of active learning rounds to $1$, with $t$ equal to $5$ and $K$ equal to $100$. We parameterize the flow using a MLP comprising two hidden layers, each with a dimension of $2048$, and $|\\mathbb A|$ outputs corresponding to individual actions. Throughout our experiments, we employ the trajectory balance objective for training. Adam optimizer with $(\\beta_0, \\beta_1) = (0.9, 0.999)$ is utilized during the training process. The learning rate for $\\log Z$ in trajectory balance loss is set to $10^{-3}$ for all the experiments. The number of training steps for TFbinding, AMP and CRE are $5000$, $10^6$ and $10^4$, respectively. The remaining hyperparameters were configured in accordance with the settings established in Jain et al. (2022).\n\nDue to space limitations, we provided the training details of employed GFlowNet models in Appendix C.1. In this revised version, we provided more training details in the main text of the paper. This revision can be found on page 7 before section 4.1 highlighted in red.\n\n**Response to Q2.** Our baselines in the initial submission do not use the trained GFN since they do not construct sequences token by token sequentially. However, all baselines perform editing using a trained model. All models including GFN and the transformer of Seq2Seq are trained on the same training data.\n\n**Response to Q3.** The proposed algorithm needs to query for the flow of partially constructed sequences $|\\mathbb A|T$ times where $|\\mathbb A|$ is the size of the action set (which is $4$ for TFbinding and CRE datasets and is $20$ for AMP dataset) and $T$ is the length of the sequence. Querying the evaluation of trained flow function models only needs inference and does not involve training. As a result, querying the evaluation of trained flow function is not computationally complex. Therefore, the complexity of the algorithm linearly scales with the size of the action set and the length of the input sequence. Therefore, the algorithm is scalable.\n\n**Response to Q4.** Generating a new sequence from scratch by GFlowNet can be viewed as a search algorithm based on a pretrained policy. The benefit of using the learned policy by GFlowNet compared to other policies is that the probability of constructing a sequence by GFlowNet is proportional to the sequence reward. Moreover, naive search algorithms such as beam-search and MCTS usually need to observe the intermediate reward. However, when it comes to constructing a new object sequentially, the intermediate reward is not available and the reward is given when the object is fully constructed. Therefore, naive search algorithms may not work well in generating new objects. GFlowNet is able to learn to generate a new object with probability proportional to its final reward when there is not any intermediate reward. Leveraging GFlowNet by the proposed algorithm is its benefit compared to naive search algorithms.\n\n**Response to Q5.** We performed experiments related to Theorems 1 and 2. Figure 3 studies the effect of hyperparameters $\\delta$ and $\\lambda$ on the property  improvement and edit percentage. Furthermore, in Figure 4, we illustrate the impact of changing $\\sigma$ on property improvement and edit diversity for GFNSeqEditor. These results corroborate the theoretical analyses outlined in Theorems 1 and 2 in section 3.3 as well as Theorem 3 in Appendix E."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7903/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700197967468,
                "cdate": 1700197967468,
                "tmdate": 1700197967468,
                "mdate": 1700197967468,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "InS2Ia4ne8",
                "forum": "viNQSOadLg",
                "replyto": "GmAJ4Hec1C",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7903/Reviewer_aGg2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7903/Reviewer_aGg2"
                ],
                "content": {
                    "title": {
                        "value": "Potentially nice work, but refine the paper further in next time"
                    },
                    "comment": {
                        "value": "The rebuttal and the improved manuscript have indeed addressed some of my concerns.\n\nHowever, I hold a different perspective on the statement, \"Moreover, naive search algorithms such as beam-search and MCTS usually need to observe the intermediate reward.\" I believe that we can leverage a Value estimator to facilitate beam-search or MCTS without requiring access to the true reward, similar to the approach used in AlphaGo.\n\nIt's important to note that there are numerous ways to generate edit sequences. To bolster your method's credibility, I recommend exploring a wide array of scenarios to showcase its superiority over various alternatives.\n\nFurthermore, while I do partially agree with your assertion that editing methods can outperform de novo methods in terms of novelty (deviation from current sequences), I believe that this comparison should be made more explicit in the paper. Specifically, it would be beneficial to include (1) the actual metric used for comparison and (2) a comparison with baseline de novo methods (I appreciate you providing some experiments on this rebuttal). Doing so will enhance the paper's motivation for using the editing method."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7903/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700446751716,
                "cdate": 1700446751716,
                "tmdate": 1700446777544,
                "mdate": 1700446777544,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Sjm8yRxNZF",
                "forum": "viNQSOadLg",
                "replyto": "f1bXYRwmqc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7903/Reviewer_aGg2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7903/Reviewer_aGg2"
                ],
                "content": {
                    "comment": {
                        "value": "I concur with your points regarding the comparison between different search methods. You rightly note that many other search techniques rely on \"proxy\" models, whereas GFlowNet distinguishes itself by learning policy directly. It's worth mentioning that MCTS can also be considered a model-based reinforcement learning method, as demonstrated in the case of AlphaGo. Thus, the question of whether model-free GFlowNets-based editing is superior to model-based search methods remains open. To address this, I recommend conducting more extensive experiments to bolster your evidence and strengthen your argument.\n\nI commend the inclusion of new results for comparison with De Novo methods, even if the comparison is limited to DM and GFlowNets. \n\nWhile I partially agree with your arguments, I would consider increasing my score from 5 to 6."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7903/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631519539,
                "cdate": 1700631519539,
                "tmdate": 1700631519539,
                "mdate": 1700631519539,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "fv1g12C8PT",
            "forum": "viNQSOadLg",
            "replyto": "viNQSOadLg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7903/Reviewer_1B72"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7903/Reviewer_1B72"
            ],
            "content": {
                "summary": {
                    "value": "* In this paper, the authors present a novel sequence editing method that leverages GFlowNet. This method relies on a pre-trained flow function to evaluate the potential for substantial property improvement within a given sequence. Furthermore, it generates a variety of edits using a stochastic policy. \n* The properties of the edited sequences are analyzed by assessing the lower and upper bounds of the reward function. \n* To evaluate the effectiveness of this approach, the authors conducted real data experiments and compared their method to three baseline approaches. They assessed performance using various metrics, including property enhancement, edit percentage, and diversity in TF binding."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The experimental results demonstrate the superiority of the proposed method across various DNA and protein sequence editing tasks. It consistently outperforms other baselines by generating sequences with fewer edits, enhanced properties, and greater diversity"
                },
                "weaknesses": {
                    "value": "* Lack of Training Details: The paper lacks sufficient information regarding the training process of the policy. It should provide more details on the training data used, the methodology for updating parameters, and the specific hyperparameters employed in the process.\n* Unclear Literature Review: The literature review in the paper needs improvement. It is not adequately clear what the main contribution of the proposed method is, and how it distinguishes itself from existing work, particularly in relation to the utilization of GFlowNet for sequence generation. The paper should provide a more explicit and comparative analysis of related work.\n* Ambiguity in Key Innovation: The claim that GFNSeqEditor can produce novel sequences with improved properties lacks clarity regarding the key innovation driving these contributions. The paper should better articulate what novel techniques or insights lead to the claimed improvements, thereby enhancing the reader's understanding of the method's unique value."
                },
                "questions": {
                    "value": "See the comments in Weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7903/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7903/Reviewer_1B72",
                        "ICLR.cc/2024/Conference/Submission7903/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7903/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698811062364,
            "cdate": 1698811062364,
            "tmdate": 1700578460816,
            "mdate": 1700578460816,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3kYmbFLmdk",
                "forum": "viNQSOadLg",
                "replyto": "fv1g12C8PT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7903/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7903/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Comments about Innovations and Training Process"
                    },
                    "comment": {
                        "value": "Thank you very much for taking time to review our paper and letting us know your feedback. In response to your feedback, we have extended the literature review within the introduction and incorporated additional details regarding the training process and the innovative aspects of the proposed algorithm to the experiments section. Please find below our point-by-point responses to your comments.\n\n## Training Details\nDue to the space limit, we put the training details in Appendix C.1. To address this comment, we provide more information about the training process of the flow function in the main text of the revised paper. Please see page 7 where we add that ``*To train models associated with baselines and the proposed GFNSeqEditor, we partition each dataset into a 72% training set and an 18% validation set. The remaining 10% constitutes the test set, employed to evaluate the performance of methods in sequence editing tasks. The trained flow function $F_\\theta(\\cdot)$ employed by GFlowNet-E and the proposed GFNSeqEditor, is an MLP comprising two hidden layers, each with a dimension of $2048$, and $|\\mathbb A|$ outputs corresponding to actions. Throughout our experiments, we employ the trajectory balance objective for training the flow function. Detailed information about training the flow function can be found in Appendix C.1.*''  This revision is highlighted in red in the revised version. Also, we expand the implementation details in Appendix C.1.\n\n## Key Innovation:\nWe would like to emphasize that the primary goal of GFNSeqEditor is not to produce novel sequences and the focus is different from biological sequence design. While GFNSeqEditor can indeed be utilized in conjunction with biological-sequence generation models to create novel sequences with improved properties, as empirically examined in section 4.2, the primary intent of GFNSeqEditor is not centered around generating entirely novel sequences. Instead, the core purpose of the proposed GFNSeqEditor lies in generating a diverse array of edits for a given seed sequence $x$, ensuring that the edited sequences exhibit similarity to $x$ while simultaneously displaying improved properties. To highlight the key innovations that contributed to the performance gain of GFNSeqEditor compared to other baselines, in this revised version we expand the discussion about the results in Table 1 and Figure 3. We explain that the proposed sub-optimal identification method outlined in equation 7 and the proposed editing policy in equation 9 are the key innovations to achieve improved performance compared to other baselines. This revision is highlighted in red on page 7 in section 4.1."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7903/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700200500534,
                "cdate": 1700200500534,
                "tmdate": 1700200500534,
                "mdate": 1700200500534,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "b7cfMxHPZh",
                "forum": "viNQSOadLg",
                "replyto": "fv1g12C8PT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7903/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7903/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Comment about Literature Review"
                    },
                    "comment": {
                        "value": "## Literature Review\nTo address this comment, we expand the literature review in the introduction and we discuss our contribution relative to the biological sequence design methods including GFlowNet. This revision can be found on pages 1 and 2 highlighted in red. Also we extend the detailed related work discussion in Appendix D. Please find below the discussion about the contributions that distinguish this paper from prior works. \n\n### Biological Sequence Design\nExisting machine learning methods within the domain of biological sequences have predominantly concentrated on generating novel sequences with desired properties from scratch. Therefore, there is an inherent risk of deviating significantly from naturally occurring sequences compromising safety and predictability. In contrast, this paper addresses the generation of new edited sequences that enhance the properties of existing sequences, ensuring the edited ones maintain similarity to their existing counterparts. Here, we take an input seed sequence and modify a few elements to improve its property. To underscore the limitations of existing biological sequence design methods, we conducted experiments on AMP and CRE datasets using GFlowNet to generate sequences from scratch. Our observations reveal that on average GFlowNet-generated sequences for the CRE dataset differ from existing sequences at 76.29% of locations, and for the AMP dataset, the difference is found in 95.24% of locations. In contrast, the proposed GFNSeqEditor demonstrates the ability to generate new edited sequences differing from existing ones in less than 35% of locations across all datasets. Moreover, employing existing biological sequence methods the difference between generated sequences and existing ones cannot be controlled while using the proposed GFNSeqEditor the amount of edits can be controlled by hyperparameters.\n\n### Sequence Combination\nAlso another aspect that distinguishes the proposed GFNSeqEditor from prior works is that existing methods are not able to combine multiple sequences to create a new sequence. In section 4.3, the proposed algorithm combines sequences to generate a new sequence similar to its parent sequences. In section 4.3, we discuss that this can be used for sequence length reduction which has important applications such as vaccine therapies.\n\n### GFlowNet\nExisting GFlowNet methods can be used for biological sequence design from scratch and as a result, they suffer from the same problems as those of other biological sequence design methods discussed above when it comes to performing sequence editing. However, since GFlowNet can generate sequences token by token sequentially, there is a naive approach to employ GFlowNet for sequence editing. In this approach, the first tokens of a sequence $x$ can be given to GFlowNet and then the GFlowNet generates the rest of the tokens. We add this as a baseline to Table 1 in the revised version and the baseline is called GFlowNet-E. For example, for the CRE dataset, the GFlowNet-E gets the first 130 tokens and generates the rest of 70 tokens. Compared to GFNSeqEditor, this approach lacks the flexibility to choose edit locations and GFNSeqEditor outperformed GFlowNet-E. This indicates the effectiveness of sub-optimal position identification of GFNSeqEditor."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7903/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700201854808,
                "cdate": 1700201854808,
                "tmdate": 1700201854808,
                "mdate": 1700201854808,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RSlo75LpTC",
                "forum": "viNQSOadLg",
                "replyto": "b7cfMxHPZh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7903/Reviewer_1B72"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7903/Reviewer_1B72"
                ],
                "content": {
                    "title": {
                        "value": "Reply to authors"
                    },
                    "comment": {
                        "value": "Thanks to the authors for the clarification and supplemental details. I've updated my score accordingly."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7903/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700578439752,
                "cdate": 1700578439752,
                "tmdate": 1700578439752,
                "mdate": 1700578439752,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]