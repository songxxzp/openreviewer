[
    {
        "title": "Language Models Struggle to Explain Themselves"
    },
    {
        "review": {
            "id": "ogykghlFfQ",
            "forum": "o6eUNPBAEc",
            "replyto": "o6eUNPBAEc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3317/Reviewer_ZS9G"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3317/Reviewer_ZS9G"
            ],
            "content": {
                "summary": {
                    "value": "This paper attempts to investigate whether LLMs can generate faithful explanations for their internal processes. To do so, authors claim that using behavioral/blackbox tests with adversarial inputs suffices to imply that the model is using a `rule`. The authors later build on this assumption to test whether the model\u2019s explanation aligns with the suggestion that the model is using that rule. Under these assumptions, authors quantify the degree to which the models' explanations are faithful to their decision-making processes."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The problem that LLMs struggle to provide faithful explanations for their decision is an important one, pronounced also in the existing literature (e.g. Turpin et al.). This paper differs in that the authors attempt to provide a methodology and a dataset to evaluate the faithfulness of the explanations, which could be a meaningful contribution if executed well.\n\n2. If the authors' assumption that the model is indeed using the rule holds true (which I am skeptical about, see W1 and Q3); the remainder of the evaluations become interesting. In that, if we can verify that a model is using a rule, we can use consistency tests to understand whether LLMs\u2019 explanations are faithful to their decision-making processes; and this would be a useful methodology."
                },
                "weaknesses": {
                    "value": "1. The connection between an explanation and hypothesizing that the model uses a rule is extremely brittle, in my opinion. I will state my understanding here, and please correct me if I am wrong. \n- The authors suggest that if a model attains high accuracy in a task, and is robust to adversarial perturbations, then the model is using a rule. \n- The above is too strong of an assumption, and `Therefore the goal is to test if the model is as close as feasible to using the known rule` jump the authors make is too strong. A model can exploit another rule, that is also invariant to the perturbation distribution chosen by the authors, yet is different than the ground truth rule.\n- To their justice, the authors themselves also recognize this, `could be using the more complex rule \u201ccontains the word \u2018lizard\u2019 or \u2018zyrpluk\u201d` and I certainly agree \u2013 the behavioral results need not imply that a model is using a rule. This strong assumption makes the remainder of the results more brittle, in my view.\n- Overall, I believe the verification of this strong assumption should extend beyond having a handful of perturbations to the existing prompts and needs more justification. This constitutes the most significant limitation in the paper, since if the assumption does not hold the paper\u2019s methodology becomes invalidated.\n\n2. The freeform articulation test appears brittle to me. Even though an articulation does not match the ground truth rule designed in the test, if the articulated rule could also explain the same set of examples, then it would not be correct to label this as an inaccuracy, in my opinion. There may be multiple correct strategies, and the model may be using it. This comment is related to W2. \n\n3. The above two weaknesses are also made more brittle by the low sample size ($<256$). This also makes the finetuning results somewhat brittle \u2013 authors finetune models with billions of parameters with tens of examples, and this alone may contribute to why finetuning does not improve articulation accuracy."
                },
                "questions": {
                    "value": "1. In Section 2.2, description of the task, authors say `5 space-separated lower case words`, yet Figure 2 contains 3 words, although the appendix includes 5-word examples. If 5 words are used across the board, I would either fix Figure 2 or add a disclaimer to avoid confusion.\n\n2. Section 2 immediately jumps to defining tasks, but until reading the rest of the paper I could not understand what these tasks are for (for explanations? For measuring model accuracy? For both?). I would add a short paragraph to the start of Section 2 to clarify what we are defining in the coming tasks.\n\n3. I would be curious to see whether the articulated explanations that are labeled as incorrect could also explain the provided examples. Specifically, what fraction of the ~30% of freeform evaluations provided by GPT-4 that are not aligned with the ground truth rule could also explain the provided in-context examples? This could be an easy test that could be evaluated by another LLM in the loop (give the articulation, and ask whether it could explain the in-context example). Alternatively, this could also be evaluated by hand since it will be a few tens of examples.\n\nMinor\n\n1. (Intro, Paragraph 1) I\u2019m not sure what AGI Alignment means in this specific context, personally I believe if we use such desiderata for systems in scientific papers, we at least need to define what it is.\n\n2. (Intro, Last Paragraph) authors cite: `as well as GPT-4 (Bubeck et al., 2023)`; but Bubeck et al. hardly is the paper that introduced GPT-4. I\u2019d cite the technical report by OpenAI, since the model was released by them, not by Bubeck et al."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3317/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3317/Reviewer_ZS9G",
                        "ICLR.cc/2024/Conference/Submission3317/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3317/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697950093378,
            "cdate": 1697950093378,
            "tmdate": 1700587380405,
            "mdate": 1700587380405,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kikZerjc1k",
                "forum": "o6eUNPBAEc",
                "replyto": "ogykghlFfQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3317/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3317/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to your helpful feedback"
                    },
                    "comment": {
                        "value": "First of all, thank you for your feedback and for explicitly laying out your understanding of the paper\u2019s methodology. We\u2019ve addressed the minor improvements. We would like to clarify a few key points that may have been misunderstood, as outlined below. \n\n> \u201cThe authors suggest that if a model attains high accuracy in a task, and is robust to adversarial perturbations, then the model is using a rule.\u201d\n\nWe think that this is a misunderstanding. We\u2019d like to clarify that we\u2019re not claiming that the model\u2019s using the rule if it describes the model\u2019s behavior on a range of in- and out-of-distribution inputs, rather that the model\u2019s behavior is closely approximated by the rule. Therefore, if the model were to articulate the rule, it would be an accurate description of its behavior, since it describes the model\u2019s behavior on a wide range of in- and out-of-distribution inputs.\n\nSection 2.4 has been updated to clarify this point.\n\n> To their justice, the authors themselves also recognize this, could be using the more complex rule \u201ccontains the word \u2018lizard\u2019 or \u2018zyrpluk\u201d and I certainly agree \u2013 the behavioral results need not imply that a model is using a rule. This strong assumption makes the remainder of the results more brittle, in my view.\n\nJust for clarity and to reiterate the point above: we\u2019re not assuming that the model is using the rule \u201ccontains the word \u2018lizard\u2019\u201d when it could well be using \u201ccontains the word \u2018lizard\u2019 or \u2018zyrpluk\u201d. If both rules accurately describe the model\u2019s behavior on a wide range of in- and out-of-distribution inputs, articulating either would be marked as correct on the freeform articulation task.\n\nIndeed, we designed the multiple choice articulation task with edge cases like this in mind to avoid them altogether; only one option is consistent with the data, as mentioned in Section 2.4. In the freeform articulation task, we controlled for this by manually reviewing 200 freeform articulations for the in-context evaluation (out of 480, and mainly GPT-3/4's since they were the only models with reasonable articulations), and similarly hundreds for the finetuning evaluation. We empirically found that models either articulated the rule R we expected, or articulated another rule R\u2019 that failed to be consistent with the few-shot examples entirely. That is, out of hundreds of manually reviewed freeform articulations, the model never articulated a rule R\u2019\u2019 which was consistent with the few-shot examples but semantically different to R (we did however see rephrasings of R, which our language model discriminator was good at catching, as demonstrated in Appendix B.4.1).\n\nWe\u2019ve added the details of our manual review process in Appendix B.3, including a spreadsheet of notes we took while manually reviewing articulations.\n\n> \u201cOverall, I believe the verification of this strong assumption should extend beyond having a handful of perturbations to the existing prompts and needs more justification. This constitutes the most significant limitation in the paper, since if the assumption does not hold the paper\u2019s methodology becomes invalidated.\u201d\n\nIt was stated that we used \u201ca handful of perturbations\u201d to test whether a model classifies according to a rule. However, as we state in the paper, we used 15 diverse adversarial attacks. These extend beyond simple perturbations and include \u201cinstruction injection\u201d and using LLMs to generate adversarial inputs. Thus, if a model achieves ~100% accuracy in matching the intended simple rule under held-out adversarial attacks, we think it\u2019s reasonable to say that its behavior on a diverse range of out-of-distribution examples is very well approximated by the simple rule. Still, this is a quantitative difference (15 attacks vs. \u201ca handful\u201d) and we want to address the qualitative point the reviewer makes."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3317/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700354040437,
                "cdate": 1700354040437,
                "tmdate": 1700354040437,
                "mdate": 1700354040437,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "E8bUHqAb9F",
                "forum": "o6eUNPBAEc",
                "replyto": "ogykghlFfQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3317/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3317/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to your helpful feedback (continued)"
                    },
                    "comment": {
                        "value": "> \u201cThe freeform articulation test appears brittle to me. Even though an articulation does not match the ground truth rule designed in the test, if the articulated rule could also explain the same set of examples, then it would not be correct to label this as an inaccuracy, in my opinion. There may be multiple correct strategies, and the model may be using it. This comment is related to W2.\u201d\n\n> I would be curious to see whether the articulated explanations that are labeled as incorrect could also explain the provided examples. Specifically, what fraction of the ~30% of freeform evaluations provided by GPT-4 that are not aligned with the ground truth rule could also explain the provided in-context examples? This could be an easy test that could be evaluated by another LLM in the loop (give the articulation, and ask whether it could explain the in-context example). Alternatively, this could also be evaluated by hand since it will be a few tens of examples.\n\nJust to reiterate the point made above: indeed, we agree this is possible and have accounted for it in both the multiple choice and freeform articulation tasks. In practice, we didn\u2019t find a single instance of this happening in hundreds of manually reviewed freeform articulations, and avoided this edge case entirely by construction in the multiple choice articulation task.\n\n> The above two weaknesses are also made more brittle by the low sample size (< 256). This also makes the fine tuning results somewhat brittle \u2013 authors finetune models with billions of parameters with tens of examples, and this alone may contribute to why finetuning does not improve articulation accuracy.\n\nWe think this is an important misunderstanding. We trained GPT-3 on 100\u2019s of classification and articulation demonstrations, not 10\u2019s. We also ensured our approach was consistent with OpenAI\u2019s best practices for fine-tuning [1]. However, the training set _is_ generated by 10\u2019s of rule functions which is a limitation (see the \u201cIncrease the variety of articulations\u201d dot point in Section 3 and \u201cFinetuning on freeform articulations\u201d in Section 4).\n\n[1] https://platform.openai.com/docs/guides/legacy-fine-tuning/classification\n\n> In Section 2.2, description of the task, authors say 5 space-separated lower case words, yet Figure 2 contains 3 words, although the appendix includes 5-word examples. If 5 words are used across the board, I would either fix Figure 2 or add a disclaimer to avoid confusion.\n\nThanks for raising this potential confusion. Yes, we always use 5 words and have modified Figure 2 to contain 5 words.\n\n> Section 2 immediately jumps to defining tasks, but until reading the rest of the paper I could not understand what these tasks are for (for explanations? For measuring model accuracy? For both?). I would add a short paragraph to the start of Section 2 to clarify what we are defining in the coming tasks.\n\nThanks for this suggestion. We agree this would be useful, and have added such a paragraph."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3317/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700354053652,
                "cdate": 1700354053652,
                "tmdate": 1700354053652,
                "mdate": 1700354053652,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wPRPsfDkoM",
                "forum": "o6eUNPBAEc",
                "replyto": "E8bUHqAb9F",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3317/Reviewer_ZS9G"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3317/Reviewer_ZS9G"
                ],
                "content": {
                    "title": {
                        "value": "Response to the Rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for your detailed rebuttal. \n\n> **Clarifications around my understanding**\n\nI appreciate your explanations, yet I am still unclear about the claims in the paper. In several places of the manuscript, e.g.  \n> *Overall, GPT-3 performs poorly at articulating the rules it follows* (Section 1 last paragraph)\n\n> *We show that it entirely fails to articulate the simple rules it uses* (Section 3.2.3, first paragraph)\n\n> *GPT-3-c-f\u2019s ability to articulate its classification rule in natural language* (Section 3.2.3, third paragraph)\n\n> *In this paper, we create a dataset, ArticulateRules, to evaluate how well LLMs can articulate the rule they\u2019re using* (Section 6, first paragraph), \n\nthe authors suggest the evaluation is to articulate the rules that a model follows. However, in the rebuttal text, they suggest \n\n> *we\u2019re not claiming that the model\u2019s using the rule if it describes the model\u2019s behavior on a range of in- and out-of-distribution inputs, rather that the model\u2019s behavior is closely approximated by the rule*. \n\nIsn't there a mismatch here? In particular, authors suggest they evaluate whether a rule approximates a behavior, yet they discuss whether or not the model is using a rule, 'its classification rule', `performs poorly at articulating the rules it follows`. According to the claim you suggest, shouldn't the conclusions be around `performs poorly at articulating the rules that closely approximate its behavior`? Otherwise, I do find the former to be an incorrect statement, based on what the authors suggest their claim is. This imprecision in language causes significant confusion and reduces the reliability of claims, in my opinion. \n\nIf I'm missing something here, I would be happy to reconsider.\n\n> **Controlling for different rules that can still explain the behavior**\n\nThank you for the clarification here and the addition to the paper with Appendix C.3, I appreciate it. It is interesting to see that (using the authors' notation) there is never a case where there exists $R'' \\neq R$ that the model produces that can still approximate the behavior.\n\n\n> **Low Sample Size**\n\nI disagree that there is a misunderstanding here. The number of parameters is orders of magnitude larger than the training data size, and the important weakness is that the low number of samples makes the experiments less conclusive (and it is not that one chooses to call it \"tens\" or \"hundreds\"). I'm not sure how the OpenAI documentation the authors shared addresses this question. In particular, what do the authors think a reviewer should find in the legacy finetuning documentation page? If there is an important point, please do share it and I will consider it."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3317/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700359714602,
                "cdate": 1700359714602,
                "tmdate": 1700359714602,
                "mdate": 1700359714602,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lHq3b8cZRV",
                "forum": "o6eUNPBAEc",
                "replyto": "ogykghlFfQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3317/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3317/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thank you for your quick and insightful reply.\n\n> Isn't there a mismatch here? In particular, authors suggest they evaluate whether a rule approximates a behavior, yet they discuss whether or not the model is using a rule, 'its classification rule', performs poorly at articulating the rules it follows. According to the claim you suggest, shouldn't the conclusions be around performs poorly at articulating the rules that closely approximate its behavior? Otherwise, I do find the former to be an incorrect statement, based on what the authors suggest their claim is. This imprecision in language causes significant confusion and reduces the reliability of claims, in my opinion.\n\nThank you for raising this point; we appreciate you clearly stating your concerns with examples. We acknowledge that the current phrasing could be confusing and have updated the relevant sections in the paper. For example, \u201cOverall, GPT-3 performs poorly at articulating the rules it follows\u201d was changed to \u201cOverall, GPT-3 performs poorly at articulating rules which closely approximate its behavior\u201d.\n\n> I disagree that there is a misunderstanding here. The number of parameters is orders of magnitude larger than the training data size, and the important weakness is that the low number of samples makes the experiments less conclusive (and it is not that one chooses to call it \"tens\" or \"hundreds\"). I'm not sure how the OpenAI documentation the authors shared addresses this question. In particular, what do the authors think a reviewer should find in the legacy finetuning documentation page? If there is an important point, please do share it and I will consider it.\n\nThank you for your response and apologies for the lack of specificity in the prior reply. Specifically, we were referring to the \u201cClassification\u201d header in the OpenAI docs (https://platform.openai.com/docs/guides/legacy-fine-tuning/classification), which states to \u201cAim for at least ~100 examples per class\u201d, and the \"Conditional generation\" header (https://platform.openai.com/docs/guides/legacy-fine-tuning/conditional-generation), which states to \"Aim for at least ~500 examples\". The non-legacy finetuning version of the docs (https://platform.openai.com/docs/guides/fine-tuning/example-count-recommendations) also states \u201cWe recommend starting with 50 well-crafted demonstrations and seeing if the model shows signs of improvement after fine-tuning. In some cases that may be sufficient\u2026\u201d Indeed, our experimental findings agreed with these recommendations; we tried finetuning on both 100\u2019s and 1000\u2019s of demonstrations, and found 100\u2019s to be sufficient (i.e. it achieved a comparable validation accuracy) for both the classification and articulation tasks, as mentioned in Section 3.2."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3317/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700556476941,
                "cdate": 1700556476941,
                "tmdate": 1700557991187,
                "mdate": 1700557991187,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HUvCovmNd7",
                "forum": "o6eUNPBAEc",
                "replyto": "lHq3b8cZRV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3317/Reviewer_ZS9G"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3317/Reviewer_ZS9G"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "Thank you for your rebuttal! I will be revising my score based on the discussion."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3317/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700587359646,
                "cdate": 1700587359646,
                "tmdate": 1700587359646,
                "mdate": 1700587359646,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "gYgllC9NEp",
            "forum": "o6eUNPBAEc",
            "replyto": "o6eUNPBAEc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3317/Reviewer_8rGm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3317/Reviewer_8rGm"
            ],
            "content": {
                "summary": {
                    "value": "The paper investigates whether autoregressive LLMs can give faithful high-level explanations of their own internal processes. To explore this, the paper introduces a dataset, ArticulateRules, and a test bed. These include few-shot text-based classification tasks generated by simple rules and free-form explanation generation, as well as multiple choice selection of explanations. The evaluation focuses on GPT-3 LLMs with different parameter sizes and GPT-4."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The authors construct a test bed along with a dataset to evaluate interesting and novel directions of explainable AI in the context of recent autoregressive LMs.\n- The scope of the paper is clearly described. In general, is the paper well written, including clear descriptions of the dataset and introduced tasks.\n- Datasets and testbeds provide the foundation for future investigations and therefore a valuable contribution."
                },
                "weaknesses": {
                    "value": "- The paper has a large emphasis on tuning GPT-3 in order to increase the performance in \u201eself-explain\u201c tasks. Therefore, the authors miss the opportunity to provide deeper insights into why the models fail on the introduced test bed and the self-explain tasks. More details on why the models fail would strengthen the paper.\t\t\n\t- While the selected definition of self-explain is interesting, the paper ignores feature attribution methods. However, attribution maps and their correlation with the LMs output would further support the insights.\n\t- Authors decided to introduce their test bed by evaluating black-box LMs. However, models with white box access (e.g., Pythia [https://arxiv.org/abs/2304.01373]) or Llama would provide more insights, e.g., by investigating input attribution. But even with only black-box access, input feature attribution can be generated, e.g., using perturbation methods or Contrastive Input Erasure [https://arxiv.org/pdf/2202.10419.pdf]. Further open LMs would provide greater reproducibility.\n- Tables and figures exceed the paper margins\t\n\n- The paper states, \"We evaluate to what extent LLMs are capable of producing faithful explanations of their own behavior via finetuning.\" However, the faithfulness of the explanation is largely ignored in the present evaluation. See e.g. https://arxiv.org/pdf/1911.03429.pdf.\n\nMinor Comments:\n- Sec 2.2.1 seems to be the only subsection of 2.2. It also seems to be misplaced. It rather fits the 2.4. Same for 2.1. In Section 2, the authors could distinguish between the test-bed, including the tasks, and the dataset.\n- \"Curie showed the first signs of life.\" Try to avoid such phrases.\n- Remove figure references in the description of the main finding in the introduction since they interrupt the reading flow. Place Figure 2 as the first figure since it summarizes the methodology of the paper quite well and is more helpful than Figure 1 at the beginning of the paper. Current Figure 1 should be placed in the results.\n- Captions should be left aligned"
                },
                "questions": {
                    "value": "While the insights provided in the paper are interesting, it is not clear to me what the benefit of self-explaining capabilities as defined in the paper is. As you briefly discuss in the limitation section, if a model can self-explain itself as the present paper defines it, it is still unclear if these explanations are faithful, i.e., corresponding to the reasons used in the model's internal process. Same for the opposite case. If it can not self-explain as defined in the paper, it does not prove that it is actually not using the right reasons. It would be interesting to see a relation between, e.g., the quality of feature attribution-based explanations and the investigated self-explain capabilities of a model."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3317/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698761232457,
            "cdate": 1698761232457,
            "tmdate": 1699636281183,
            "mdate": 1699636281183,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8tbr4pNwMY",
                "forum": "o6eUNPBAEc",
                "replyto": "gYgllC9NEp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3317/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3317/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to your helpful feedback"
                    },
                    "comment": {
                        "value": "Thanks for this detailed review. We are glad you see the value in releasing benchmarks of this form, and find our contribution valuable, well scoped, and clearly explained. We\u2019ve updated figures to be within the paper margins and addressed all minor comments. We address the remaining weaknesses below.\n\n> \u201cThe paper states, \"We evaluate to what extent LLMs are capable of producing faithful explanations of their own behavior via fine tuning.\" However, the faithfulness of the explanation is largely ignored in the present evaluation. See e.g. https://arxiv.org/pdf/1911.03429.pdf\u201d\n\nThanks for suggesting this paper (https://arxiv.org/pdf/1911.03429.pdf) which we have added to related work. This paper studies faithfulness in rationales, a specific form of explanation. A rationale is considered faithful if it actually informs model predictions. One way to measure is to manipulate an input to remove the rationale (\u201ccomprehensiveness\u201d).  \n\n\nWe understand \u201cfaithfulness\u201d in a similar way. An articulation of a rule is faithful if it actually informs the model\u2019s classifications on held-out examples. If a model articulates a rule R but is shown to classify held-out examples consistent with a distinct rule S, then this articulation is necessarily an unfaithful explanation. This is what we actually find for GPT-3. Specifically, GPT-3 produces freeform and multiple choice articulations of rules that do not match its classification behavior on held-out examples. Thus, GPT-3 fails to be faithful, even after finetuning.\n\nGPT-4, by contrast, achieves significantly higher accuracy in articulating rules that match its classification behavior (~70% top-3 freeform articulation accuracy for GPT-4 vs ~10% for GPT-3). This suggests that GPT-4 may be \u201ccapable of producing faithful explanations\u201d, although it still fails on ~30% of rules. As we discuss in the limitations section, there is a question of whether even 100% accurate articulation of rules is sufficient condition for faithfulness. It could be that the articulated rule describes the model\u2019s behavior but does not causally influence its predictions. There are various approaches to testing this (e.g. manipulating internal representations in whitebox models). We think these are important questions for future work in this area. \n\nWe hope we have clarified how our results relate to faithfulness. We have amended the paper to include these points about faithfulness in the introduction, conclusion and results sections.\n\n> \u201cMore details on why the models fail would strengthen the paper.\u201d\n\nThanks for this suggestion. We are not able to definitively explain why models fail to perform well, but are able to speculate. One speculation is that models seem poorly incentivized by both pretraining and fine tuning to produce faithful explanations of their own behavior, in which case it\u2019s unsurprising that models struggle to generalize in this way, which our work demonstrates. However, we agree that explaining why models fail at this task is an interesting direction, and we\u2019ve added this to our future work section.\n\n> While the insights provided in the paper are interesting, it is not clear to me what the benefit of self-explaining capabilities as defined in the paper is. As you briefly discuss in the limitation section, if a model can self-explain itself as the present paper defines it, it is still unclear if these explanations are faithful, i.e., corresponding to the reasons used in the model's internal process. Same for the opposite case. If it can not self-explain as defined in the paper, it does not prove that it is actually not using the right reasons. It would be interesting to see a relation between, e.g., the quality of feature attribution-based explanations and the investigated self-explain capabilities of a model.\n\nThanks for pointing this out. Self-explanation is a desirable property for models to have, in general. As we noted above, we view high accuracy on our benchmark to be a necessary, but not sufficient test of language model self explanation. We chose such an approach as procedurally generated black box behavioral evaluations provide a cheap, fast and scalable test of model capabilities, while still being informative. \n\nWe agree that studying the relation between explanation capabilities and feature attribution-based methods would be informative. We have listed this as a possible area of future work."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3317/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700353968111,
                "cdate": 1700353968111,
                "tmdate": 1700353968111,
                "mdate": 1700353968111,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gXkHiiK0NB",
                "forum": "o6eUNPBAEc",
                "replyto": "gYgllC9NEp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3317/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3317/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to your helpful feedback (continued)"
                    },
                    "comment": {
                        "value": "> Authors decided to introduce their test bed by evaluating black-box LMs. However, models with white box access (e.g., Pythia [https://arxiv.org/abs/2304.01373]) or Llama would provide more insights, e.g., by investigating input attribution. But even with only black-box access, input feature attribution can be generated, e.g., using perturbation methods or Contrastive Input Erasure [https://arxiv.org/pdf/2202.10419.pdf]. Further open LMs would provide greater reproducibility.\n\nWe agree the corresponding experiments on white box models would be interesting. We plan on releasing our dataset so that such experiments can be run by others in the community. In this work, we elected to first evaluate OpenAI models to determine where the current SOTA articulation capabilities were. \n\nHowever, in response to similar comments by other reviewers, we are in the process of running some evaluations on the Llama series of models, results for which should be ready in the next few days. We will share the results of these in a top level comment.\n\nWe also take the point that white box methods could be illuminating. We too thought so, and discussed some potential avenues in this vein in the future work sections. We chose not to explore these due to time constraints, and these experiments falling out of the scope of our key contributions. Thanks for the comment on perturbation methods - these too would be interesting to explore, and we have added this to our future work section.\n\n> \u201cIn Section 2, the authors could distinguish between the test-bed, including the tasks, and the dataset.\u201d\n\nThanks for this. We have added a paragraph at the start of section 2 explaining on a high level the tasks and datasets.\n\n> Tables and figures exceed the paper margins\n\nThanks for raising this point. We have fixed all of these issues.\n\n> Minor Comments\n\nThanks. We have also fixed all of these stylistic issues."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3317/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700353982150,
                "cdate": 1700353982150,
                "tmdate": 1700353982150,
                "mdate": 1700353982150,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gZzORNnjOg",
                "forum": "o6eUNPBAEc",
                "replyto": "gYgllC9NEp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3317/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3317/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer 8rGm, once again thank you for your time and effort in providing your thoughtful review of our paper. As we are now entering the last day or so of the rebuttal period we wanted to just reach out to see if there was any feedback from our response -- we appreciate that this will be a busy time for you but we hope that our current response has addressed any current concerns and are keen to engage further if there are any outstanding concerns. Best wishes, the Authors"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3317/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700609318341,
                "cdate": 1700609318341,
                "tmdate": 1700609318341,
                "mdate": 1700609318341,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YnQ09HGLyO",
                "forum": "o6eUNPBAEc",
                "replyto": "gZzORNnjOg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3317/Reviewer_8rGm"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3317/Reviewer_8rGm"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the clarifications and for updating the paper accordingly. Including them already improved the paper. While additional experiments on white box models intended to open the opportunity of generating input influence-based explanations and in turn draw further conclusions, I still appreciate the extra effort. \nI don't have any remaining questions."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3317/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700679241201,
                "cdate": 1700679241201,
                "tmdate": 1700679241201,
                "mdate": 1700679241201,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "KXRh1svSBj",
            "forum": "o6eUNPBAEc",
            "replyto": "o6eUNPBAEc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3317/Reviewer_ZENG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3317/Reviewer_ZENG"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces ArticulateRules, a dataset of few-shot text classification tasks with associated simple rule explanations, to evaluate whether large language models can provide faithful high-level explanations of their internal processes behind competent classification. The authors test a range of models in-context and find articulation accuracy increases with model size, especially from GPT-3 to GPT-4. However, even finetuned GPT-3 fails to articulate explanations matching its classifications, though it shows some capability on easier multiple choice tasks. Overall, the analysis indicates that current large language models struggle to provide high-level self-explanations, though abilities are emerging in GPT-4. The dataset provides a useful benchmark for future work on testing and improving self-explanation in large language models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper works on a timely and important topic, i.e., whether LLMs can give faithful high-level explanations of their own internal processes.\n2. This paper introduces a novel dataset, ArticulateRules, that provides a concrete way to test whether large language models can explain their reasoning and decision making processes behind text classifications.\n3. This work comprehensively evaluates a range of large language models in-context, showing a clear correlation between model size/capability and articulation accuracy.\n4. The experimental analysis demonstrates specifically that even a very large model like GPT-3 fails completely at articulating explanations for a significant portion of simple rules. This highlights major limitations of current self-explanation abilities."
                },
                "weaknesses": {
                    "value": "1. The major concern is that the authors claim or speculate that GPT-4 has over 175 billion parameters. However, even during the paper review period, the exact size of GPT-4 remains unclear.\n2. Even if it is confirmed that GPT-4 has over 175 billion parameters, this alone does not lead to the conclusion that \"articulation accuracy increases with model size.\" The reason is that the differences between the models shown in Figure 1 are not only in their number of parameters, but also in other significant factors like training data. Simply comparing model sizes does not account for these other variables that likely also impact articulation accuracy.\n3. The ArticulateRules dataset is relatively small and simple, focusing on text classification tasks based on simple syntactic rules. Performance on this limited dataset may not reflect abilities on more complex real-world tasks.\n4. Only one main model architecture (GPT-3/4) is tested in detail. Other model types may have different explanation capabilities that are not explored."
                },
                "questions": {
                    "value": "Please refer to the weaknesses section"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3317/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699148509095,
            "cdate": 1699148509095,
            "tmdate": 1699636281119,
            "mdate": 1699636281119,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PNIv8vlP6n",
                "forum": "o6eUNPBAEc",
                "replyto": "KXRh1svSBj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3317/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3317/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to your helpful feedback"
                    },
                    "comment": {
                        "value": "Many thanks for an insightful review. We\u2019re glad that you find our results to highlight a major limitation of current models and the topic to be a timely one. We are additionally pleased you found our in-context experiments to be comprehensive and the scaling trend to be informative. We address the weaknesses below.\n\n> Simply comparing model sizes does not account for these other variables that likely also impact articulation accuracy.\n\nAgreed; this is a good point and is well-taken. We\u2019ve replaced the claim about articulation accuracy increasing with model size with a more nuanced claim which acknowledges that multiple factors other than model size (e.g. the training set or model architecture) play a role in the the increased general capabilities of GPT-4, and of the observed increase in articulation accuracy. The motivation behind studying a series of models in this manner was to establish whether articulation capabilities would arise by default with scale. \n\n> The major concern is that the authors claim or speculate that GPT-4 has over 175 billion parameters. However, even during the paper review period, the exact size of GPT-4 remains unclear.\n\nThanks for noting this. This is a good point and such speculation is unnecessary. We\u2019ve removed the assumption that GPT-4\u2019s parameter count is greater than 175B as part of the above change.\n\n> Performance on this limited dataset may not reflect abilities on more complex real-world tasks\n\nThank you for raising this point. We agree that high articulation accuracy on ArticulateRules does not correspond to high articulation accuracy on complex real-world tasks. However, we would like to clarify that we\u2019re not claiming that an increase in articulation accuracy on ArticulateRules implies an increase in articulation accuracy on more complex real-world tasks, and this is not the goal of our work. Rather, we\u2019re claiming that high articulation accuracy on this evaluation is a necessary but not sufficient condition for an LLM to be able to articulate its behavior on more complex real-world tasks.\n\n> \u201dThe ArticulateRules dataset is relatively small and simple\u2026\u201d\n\nThank you for pointing out the need for more exposition regarding our specific dataset construction. We\u2019ve updated the paper to provide more clarity about the design decisions around the size of the dataset and the simplicity of the rules it includes.\n\n** Size **\n\nThe size of the dataset is constrained by compute costs. We assume that a cost of $10\u2019s or $100\u2019s to reproduce our in-context evaluation results is affordable for most labs, and similarly a cost of $1,000\u2019s for our finetuning results. Therefore, if the smallest variant of ArticulateRules exceeded these budgets, it would risk being too costly to run or build upon in practice.\n\nWe determined the minimum size of ArticulateRules in the following way: (1) we determined the minimum number of few-shot examples to include in the prompt in order for a human to be able to articulate the rule (we found this to be 16 few-shot examples; more details in Appendix 2.3), (2) we determined the number of examples per rule function needed to have an acceptably-low standard error of the mean for in-context tasks (we found this to be roughly >= 20 and landed on 32 examples per rule function; Figure 11 uses n=15 and has a large SEM, compared to Figure 5 which uses n=32 and has an acceptable SEM to be able to draw conclusions), (3) we then used the maximum number of rule functions given these constraints.\n\nLarger dataset variants are also provided for labs with larger compute budgets. Also, it\u2019s worth noting that the dataset size is easily increased: (1) it is procedurally generated, meaning that larger datasets can be generated as needed, and (2) rule functions are constructed from more primitive ones using AND, OR and NOT operators, meaning it\u2019s straightforward to register more complex rule functions as needed.\n\n** Simplicity **\n\nWe chose tasks where we could: (1) closely approximate the model\u2019s behavior with a known rule (and therefore be able to determine if an articulation was correct or not), and (2) get an accurate picture of the limits of present-day LLMs\u2019 articulation capabilities.\n\nThe binary text classification tasks used in ArticulateRules fit both of these criteria. Using easier tasks where LLMs achieved 100% freeform articulation accuracy wouldn\u2019t have given an accurate picture of their limitations. Using harder tasks where LLMs get 0% accuracy would have been equally as uninformative. We tailored these tasks so that present-day LLMs are capable of articulating only a small fraction of the rules in the dataset, which gives a picture of their current articulation capabilities and its limitations, and allows us to track increases in articulation capabilities over time.\n\nWe\u2019ve added these details to Appendix B.6 since we agree that the size of the dataset and the complexity of the tasks it includes requires justification."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3317/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700353836952,
                "cdate": 1700353836952,
                "tmdate": 1700353873519,
                "mdate": 1700353873519,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Fy1DHBBJ7d",
                "forum": "o6eUNPBAEc",
                "replyto": "KXRh1svSBj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3317/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3317/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to your helpful feedback (continued)"
                    },
                    "comment": {
                        "value": "> Only one main model architecture (GPT-3/4) is tested in detail.\n\nThanks for raising this concern. Our decision to focus on GPT-3/4 was primarily driven by time and cost constraints, however we acknowledge that evaluating LLMs with different architectures would be a valuable experiment to run. In particular, conducting a finetuning evaluation of GPT-3 traded-off against evaluating a wider range of LLMs in-context. We decided that approximating the upper bound of articulation capabilities of GPT-3 (the largest model available with finetuning access) was the more informative experiment to run.\n\nIn response to this feedback, we\u2019ve also started evaluating Llama 2 in-context and are planning to have results in the next few days, results of which we will post in a top-level comment."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3317/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700353909141,
                "cdate": 1700353909141,
                "tmdate": 1700353909141,
                "mdate": 1700353909141,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IezUAQbSWA",
                "forum": "o6eUNPBAEc",
                "replyto": "KXRh1svSBj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3317/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3317/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer ZENG, once again thank you for your time and effort in providing your thoughtful review of our paper. As we are now entering the last day or so of the rebuttal period we wanted to just reach out to see if there was any feedback from our response -- we appreciate that this will be a busy time for you but we hope that our current response has addressed any current concerns and are keen to engage further if there are any outstanding concerns. Best wishes, the Authors"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3317/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700609287983,
                "cdate": 1700609287983,
                "tmdate": 1700609287983,
                "mdate": 1700609287983,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]