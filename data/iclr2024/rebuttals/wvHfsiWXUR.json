[
    {
        "title": "Tell, Don't Show: Internalized Reasoning influences how LLMs generalize"
    },
    {
        "review": {
            "id": "jXR488OENy",
            "forum": "wvHfsiWXUR",
            "replyto": "wvHfsiWXUR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4981/Reviewer_4DGZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4981/Reviewer_4DGZ"
            ],
            "content": {
                "summary": {
                    "value": "This paper researches if/how the learned declarative (i.e., factual) knowledge affects the prediction manner of LLMs (called \"generalization\").  The authors design several toy tasks and get many strong conclusions from them."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "Except the research topic is interesting, I can't see any other strengths."
                },
                "weaknesses": {
                    "value": "1. Many concepts are abused. First, what is the generalization here? The previous definition is from the statistical view. But here it seems the authors regard it as a very broad concept: prediction without overfitting (i.e., memorized answer)? Second, what is the declarative information? Is that the factual knowledge (or sentences containing factual knowledge)? I can't list all of them. But I would suggest the authors to using these terms in a proper way. By the way, please be very careful when using the word \"AGI\".\n\n2. Experiment design for generalization evaluation. First of all, I don't think the prediction in terms of time can be connected to generalization. Even with some strong assumptions, it is still a complex task. Generalization is not enough to give a correct answer. As for the experiment design, I think it's not convincing. Current results are more like case studies on toy tasks. The findings are definitely not enough to draw strong conclusions.\n\n\nThere are many other issues in this paper. I think a lot of work is needed before publication."
                },
                "questions": {
                    "value": "See weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4981/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698762822293,
            "cdate": 1698762822293,
            "tmdate": 1699636485556,
            "mdate": 1699636485556,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "LgHftdsj7G",
            "forum": "wvHfsiWXUR",
            "replyto": "wvHfsiWXUR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4981/Reviewer_npPu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4981/Reviewer_npPu"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies an interesting hypothesis that declarative knowledge in training data will systematically affect an LM's behavior. The authors fine-tune GPT3 and Llama 2 on two toy datasets: a weather forecast dataset and a teacher profile dataset. They found small yet statistically significant evidence of LM being steered toward the declarative knowledge direction, instead of generalizing from the other training data."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The research problem of understanding the effect of declarative statements contained in training data at inference time is important and timely to improve the safety of LLMs.\n2. The experiments are well-designed and carefully analyzed."
                },
                "weaknesses": {
                    "value": "1. The experiments might be too toy to reflect the real-world scenario. To my understanding, the declarative knowledge we care about should be contained either in the pre-training data or the instruction tuning data. And we would be interested to see its effect on a broad spectrum of real-world tasks/generalizations. The definition of generalization in the paper seems to be pretty narrow to me. It's either linear interpolation or distribution matching. I think the paper would benefit a lot from the addition of a more realistic dataset. i.e. How much effect does declarative knowledge have on real-world tasks?  \n2. Since each of the datasets only has a few hundred training examples, could the LLM just be overfitting the small datasets? When we care about LLMs with strong capability on many tasks, does a declarative statement on one task still work?"
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4981/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699313235020,
            "cdate": 1699313235020,
            "tmdate": 1699636485482,
            "mdate": 1699636485482,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "4h74jZcPAV",
            "forum": "wvHfsiWXUR",
            "replyto": "wvHfsiWXUR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4981/Reviewer_7kVD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4981/Reviewer_7kVD"
            ],
            "content": {
                "summary": {
                    "value": "The paper explores the influence of declarative knowledge on language models' generalization behavior during a domain shift. The authors fine-tune language models on a distribution that exhibits natural generalization during a shift, and then analyze the impact of declarative statements in the training data on the models' predictions for unseen examples. The paper finds that declarative knowledge does affect the models' behavior, although the effect is subtle."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The research content of the paper is interesting as it explores the influence of declarative knowledge on language models' generalization behavior during a domain shift. \n- The experiments in the study focus on two specific domains: weather prediction and teacher gender prediction."
                },
                "weaknesses": {
                    "value": "1. The writing in this paper requires further improvement, and the experiment analysis is inadequate.\n2. The phenomenon discussed in this paper has limited contribution for the community. Both demonstrations and descriptions can impact the model's prediction results, which is expected. The author should conduct additional experiments to explore the reasons behind demonstrations having a stronger impact than descriptions.\n3. The experimental design needs to enhance rigor . For instance: 1) The task and data are not reasonable enough. It is challenging to control the occurrence frequency of \"rain\" and \"sun\" to avoid bias, as these general tokens are prevalent in the pre-trained corpus and the model predicts with raw bias. 2) Does the number of descriptions in the training data directly influence the model's prediction results?"
                },
                "questions": {
                    "value": "please see weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4981/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699373060754,
            "cdate": 1699373060754,
            "tmdate": 1699636485407,
            "mdate": 1699636485407,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "xFNSnWRHKh",
            "forum": "wvHfsiWXUR",
            "replyto": "wvHfsiWXUR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4981/Reviewer_5z43"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4981/Reviewer_5z43"
            ],
            "content": {
                "summary": {
                    "value": "This paper examines the impact of declarative knowledge on language models' ability to generalize during a domain shift. The authors conduct experiments by finetuning language models to fit some distribution and found the effect is subtle. Furthermore, the results show that such effect cannot be attributed to simple token matching."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The proposed design of utilizing construction of toy examples to measure the counterfactual effect of adding descriptions to a finetuning set is quite ingenious.\n- This paper is well written and easy to understand."
                },
                "weaknesses": {
                    "value": "1) As the author found the effect of declarative statements in the training data on the domain shift generalization is subtle,  the significance of this experimental result is therefore somewhat puzzling. \n2) It remains to be discussed whether the toy examples finetune pattern can fully represent the real large model training situation. \n3) The training data itself may contain potentially conflicting declarative knowledge descriptions, which is a situation not explored in this paper."
                },
                "questions": {
                    "value": "1. Using the example mentioned in the paper, which is asking an LLM to generate weather reports for a specific city in 2050, the best way to answer this question is inherently undeterminable, and different people may think about it in different ways. Thus it is somewhat confusing to use this example to express the conflict between descriptions and demonstrations.\n2. Add more analysis about whether the toy examples finetune pattern can fully represent the real large model training situation. \n3. In addition to the comparison of declarative statements and demonstrations, the situation which the training data itself may contain conflicting  internalized  knowledges need to be discussed.\n4. The toy examples are not diverse enough, will it affect the significance of the experimental conclusion?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4981/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4981/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4981/Reviewer_5z43"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4981/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699453241283,
            "cdate": 1699453241283,
            "tmdate": 1699636485303,
            "mdate": 1699636485303,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]