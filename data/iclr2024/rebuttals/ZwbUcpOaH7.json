[
    {
        "title": "Transitional Uncertainty with Intermediate Neural Gaussian Processes"
    },
    {
        "review": {
            "id": "xvgYmmaw6A",
            "forum": "ZwbUcpOaH7",
            "replyto": "ZwbUcpOaH7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3054/Reviewer_3eZi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3054/Reviewer_3eZi"
            ],
            "content": {
                "summary": {
                    "value": "-\tThe paper investigates feature-based uncertainty estimation in image classification. The paper analyzes properties of different feature preservation methods and presents motivation for transitional feature preservation, where feature distances are encoded in differences of the individual sample representations. As existing transitional feature preservation methods require multiple passes, the paper proposes TUrING Processes, a single-pass uncertainty estimation method by combining layer-wise hidden representations instead of ensembles. Here, the gaussian process is used to quantify uncertainty. The results show that the proposed method outperforms baselines on OOD detection consistently across different models and datasets although the task performance is sacrificed in some cases."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "-\tThe paper investigates an important area of uncertainty estimation.\n\n-\tThe paper proposes a way to mitigate the limitations of existing single-pass uncertainty estimation methods.\n\n-\tThe proposed method is effective in OOD detection on image classification."
                },
                "weaknesses": {
                    "value": "-\tThe exact claim the paper is making is unclear to me as the paper only focuses on OOD detection rather than uncertainty estimation.\n\n-\tSome presentational details are not sufficiently clear. For example, Figure 1b compares ensemble workflow and combining layer-wise representations. In the ensemble workflow, uncertainty is measured from differences between several representations (with ensembles) of the same sample. However, the proposed method does not use differences between representations from different layers. Simply combining intermediate layer representations does not work in a similar way to the ensemble workflow.\n\n-\tSome claims are not substantiated clearly enough. For example, I am curious whether there is evidence for the claim \u201cthe network collapses the class clusters to single points creating a challenging setting for uncertainty estimation,\u201d in the left plot of Figure 1a.\n\n-\tThe insights are limited despite the effectiveness of the proposed method."
                },
                "questions": {
                    "value": "-\tThe paper claims that \u201cthe network collapses the class clusters to single points creating a challenging setting for uncertainty estimation.\u201d However, in my opinion, it would be challenging for OOD detection, not for uncertainty estimation. Could this point be clarified?\n\n-\tIt is logical that \u201cfeature preservation in the output\u201d may harm the performance. However, why is transitional feature preservation advantageous over \u201cno preservation\u201d for uncertainty estimation?\n\n-\tRegarding the issues mentioned above about Figure 1b, it would be good to clarify how the proposed method (or each component) directly solves the discussed problems: (1) feature preservation and (2) efficiency (ensembles).\n\n-\tAdditional experiments are required to support the main claims. This may include (1) ablation studies to see the effects of each component of the proposed method, and (2) comparisons with existing iterative uncertainty estimation methods in terms of time and space complexity."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3054/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698639868486,
            "cdate": 1698639868486,
            "tmdate": 1699636250812,
            "mdate": 1699636250812,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "qmODk5knl0",
            "forum": "ZwbUcpOaH7",
            "replyto": "ZwbUcpOaH7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3054/Reviewer_AwFU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3054/Reviewer_AwFU"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the problem of _single pass uncertainty estimation_, i.e., being able to estimate epistemic uncertainties of a deep neural network using a single pass during inference. This is different from several existing approaches that either require multiple networks or multiple passes for uncertainty estimation. \n\nThe proposed idea called $\\mathrm{TUrING~Processes}$  exploits intermediate feature representations of the DNN to estimate uncertainty. The idea behind this is called transitional feature preservation, that ensures there is no feature collapse during training -- which the paper argues is essential for accurate uncertainties. \n\nThis is implemented in practice using shallow deep networks (SDNs) through early exits in the network layers, with individual classifiers for each. These predictions are then combined using an additional combination head that is fit in a post-hoc manner using unlabeled validation examples. \n\nThere are theoretical and empirical justifications for how the proposed method is effective in feature distance preservation. Experiments on CIFAR10/100 and Medical images show $\\mathrm{TUrING~Processes}$ to be effective in rejecting outliers over other baselines. \n\nOne side note on the abbreviation  $\\mathrm{TUrING~Processes}$ : This name is overloaded (especially in AI), and it can be confusing or misleading. It may be more effective to change it so it reflects the paper's contributions and unique to it."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* Single pass uncertainty estimation is an important problem and the paper makes an interesting and novel insight into how intermediate feature representations can be exploited. It is intuitive that (with appropriate weighting) intermediate features do have somewhat complimentary information, and it is interesting to see it being exploited for uncertainties. \n\t* I am curious how the learned weights are for the combination head ($r_1, r_2...r_N$ ), and if they are some how indicative of (a) task hardness, (b) generalization error prediction. Further, if the validation was done on OOD data, would the distribution of these weights be likely different (since they are somehow measuring the importance of differnet layers on the final task, if i understand correctly)\n\n* The ideas are explained clearly, and easy to follow. \n* The experiments show good outlier rejection on CIFAR 10/100 and medical image modalities, most of the time, justifying the use of the proposed method"
                },
                "weaknesses": {
                    "value": "* **Architecture**: One of the unacknowledged limitations of single pass UQ methods is the need to modify architectures from the original, which is non-trivial to do. This is especially true in this paper where its required to take early exists from all layers, and fit individual classifiers at each level, followed by a combination head. For smaller datasets like CIFAR10/100 this is relatively easy, but can become tricky when optimizing for larger benchmarks (ImageNet), which are not tested in the paper. \n  \n  Further, I dont see the comparison between the baselines to be an apples-apples comparison as this method requires several additional parameters (tens of thousands more?) This is likely only scales as the architectures get larger.. Further, most SoTA are moving towards attention-based models, and for the paper's contribution to be impactful, uncertainty estimation with these architectures must also be studied. \n* **Empirical Evaluation**: In addition to evaluations on larger scale benchmarks like ImageNet, more evaluations baselines on outlier rejection approaches are needed -- there are several state of the art -- including ones that do no require outlier validation data. \n* **Calibration** : While it is claimed that the proposed method is \"preferable\" under distribution shifts,  this is never actually evaluated -- how is it preferred? Does it generalize better? How well is it calibrated? I think these also need to be addressed."
                },
                "questions": {
                    "value": "Please see comments above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3054/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699205792885,
            "cdate": 1699205792885,
            "tmdate": 1699636250738,
            "mdate": 1699636250738,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "9oqLVF231D",
            "forum": "ZwbUcpOaH7",
            "replyto": "ZwbUcpOaH7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3054/Reviewer_SFMY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3054/Reviewer_SFMY"
            ],
            "content": {
                "summary": {
                    "value": "the paper introduces an approach to uncertainty quantification in deep neural networks through \"transitional uncertainty with intermediate neural gaussian processes\" (turing processes). the proposed method aims to preserve feature distances in intermediate layers of a dnn. the manuscript comes with theoretical justifications, in sections 3 and 4, and empirical evidence across several datasets, including medical imaging datasets. the authors also provide helpful supplements."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. the paper is well-written, with a clear narrative that guides the reader through the problem statement, proposed solution, and validation.\n1. originality of the approach lies in its use of intermediate layer features, as laid out in section 2.2. particularly for applications where single-pass uncertainty estimation is necessary, this provides an interesting approach.\n1. the preservation of feature distances is a novel extension to common, existing uncertainty quantification methods.\n1. the quality of the experimentation and methodology is thorough."
                },
                "weaknesses": {
                    "value": "1. the biggest gap i see overall is a lack of contextualization wrt to existing work on single pass uncertainty quantification. while i have full compassion that we can never benchmark against everything, even in more niche problem domains as uncertainty quantification, critical works should be included or their omission at least explained. among these i would count gast's probout which also offers a layerwise propagation version (https://openaccess.thecvf.com/content_cvpr_2018/html/Gast_Lightweight_Probabilistic_Deep_CVPR_2018_paper.html), quantile regression (https://www.jstor.org/stable/1913643) and conformal prediction (https://www.jmlr.org/papers/volume9/shafer08a/shafer08a.pdf), interval neural networks (https://arxiv.org/abs/2003.11566) as well as the classic direct variance prediction (https://proceedings.neurips.cc/paper/1994/hash/061412e4a03c02f9902576ec55ebbe77-Abstract.html). regarding naming of the method, a mention of relation to evidential turing processes https://arxiv.org/abs/2106.01216 from last year's iclr also appears opportune. \n1. the supplementary material provides a good depth of technical detail, but integrating some of this content into the main body could make the paper more self-contained, especially the algorithm description and runtime considerations.\n1. the paper could benefit from a more detailed discussion on computational efficiency, particularly in section 5 where the comparison to ensemble methods lacks a thorough analysis of time complexity.\n1. generalizability is touched upon, but the paper could include additional experiments or a theoretical discussion on how turing processes might perform with different architectures or in other domains.\n1. hyperparameter sensitivity is not addressed in depth; section 4 could be expanded to include a discussion on the methodology for selecting hyperparameters and their impact on the model's performance."
                },
                "questions": {
                    "value": "1. in section 5, the computational overhead of turing processes is compared to ensemble methods. could the authors provide more detailed benchmarks, including time complexity and resource utilization metrics?\n1. the paper presents promising results for the tested architectures and domains. how do the authors envision the adaptation of turing processes to other types of neural networks and problem domains?\n1. hyperparameter tuning is critical for model performance. can the authors elaborate on the process used to select hyperparameters for turing processes, as mentioned briefly in section 4.3?\n1. what is the relation to other works i mentioned above? could you provide some additional contextualization?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "n/a"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3054/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699368257262,
            "cdate": 1699368257262,
            "tmdate": 1699636250676,
            "mdate": 1699636250676,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dFe5DdhOZV",
                "forum": "ZwbUcpOaH7",
                "replyto": "9oqLVF231D",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3054/Reviewer_SFMY"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3054/Reviewer_SFMY"
                ],
                "content": {
                    "title": {
                        "value": "discussion period"
                    },
                    "comment": {
                        "value": "no replies or responses were received by the authors.\n\ni thus maintain my rating.\n\nin good spirits,\n\nreviewer SFMY"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3054/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700587760686,
                "cdate": 1700587760686,
                "tmdate": 1700587760686,
                "mdate": 1700587760686,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]