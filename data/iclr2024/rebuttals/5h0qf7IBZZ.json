[
    {
        "title": "MiniLLM: Knowledge Distillation of Large Language Models"
    },
    {
        "review": {
            "id": "QbaMsCbvCV",
            "forum": "5h0qf7IBZZ",
            "replyto": "5h0qf7IBZZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2166/Reviewer_oknc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2166/Reviewer_oknc"
            ],
            "content": {
                "summary": {
                    "value": "For lowering computation cost required by large deep learning models, knowledge distillation (KD) is a popular approach. This study presents a knowledge distillation method applied to open-sourced large language models (LLMs). While the standard KD method uses a linear combination of cross entropy loss and Kullback-Leibler (KL) divergence referred to as forward KL divergence in this study, this study also examines a reverse KL divergence, which swaps teacher and student distributions and is used in computer vision and reinforcement learning literature. With the modified loss function, LMs trained on instruction-following datasets with teachers by their proposed approach (called MiniLLM) achieved higher average GPT-4 feedback scores than those trained with the same teacher model by a sequence-level KD (SeqKD) baseline."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The reviewer wants to value originality of this study as this study is focused on open-sourced LLMs as targets for knowledge distillation, and black-box APIs can change their internal behavior without notice or proper versioning.\n- This paper seems to well describe the proposed method and cites prior studies that inspired the authors to introduce key concepts in their method such as reverse KL divergence.\n- It is empirically shown that MiniLLMs achieved better performance than KD baselines considered in this paper with multiple evaluation settings and instruction-following datasets. It is also notable that the overall trend in Table 1 seem consistent over different student models in a variety of model sizes.\n- The ablation study attempts to test multiple hypotheses made when designing the proposed loss function."
                },
                "weaknesses": {
                    "value": "## Presentation\nThis paper needs to improve the presentation and writing.\ne.g.,\n\n- MiniLLM model must be tautology (Mini large language model model) and should be referred to as just MiniLLM instead\n- \"distill <student model> from <teacher model>\" sounds weird to the reviewer, and the reviewer suggests \"distill (knowledge of) <teacher model> into <student model>\"\n- Itemized lists in this paper look very packed. Did the authors change the format and reduce space between items?\n- \"generative LLMs\" and \"generative language models\" also sound strange as language models themselves are generative models. The reviewer suggests just skipping \"generative\".\n- \"the vocab size\" should be \"the vocabulary size\"\n- \"similar to Learning from Human Feedback (RLFH; Ouyang et al., 2022).\" misses \"Reinforcement\"\n- Use [] for the second equation in Eq. (5) as well\n\n## GPT-4-based evaluation\n\nOverall experimental designs in this study look good, but the reviewer has a big concern about evaluations involving GPT-4. Specifically, it is very questionable how scientifically meaningful the evaluations are when leaving all the evaluations to GPT-4, and the reviewer did not find any reasonable justifications of using GPT-4 as part of the evaluation process. Rouge-L should be sufficient for Tables 1 and 4, and the reviewer strongly recommends use of Rouge-L instead of GPT-4 feedback (score) for Figures 1 and 5. The reviewer will improve rating if GPT-4-based evaluations are removed and replaced with Rouge-L."
                },
                "questions": {
                    "value": "1. What is the definition of \"white-box\" KD/model in this paper? White-box in this paper sounds misleading. \n2. Why is the specific $w_t$ between Eqs. (5) and (6) expected to reduce the variance of the estimator in Eq. (5)?\n3. What is the definition of \"Exposure Bias\" ?(conceptual definition, not mathematical definition)\n4. What is \"the responses' distinct 4-gram proportion\"?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2166/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2166/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2166/Reviewer_oknc"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2166/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697863522005,
            "cdate": 1697863522005,
            "tmdate": 1700378338776,
            "mdate": 1700378338776,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "A3KLlftYfb",
                "forum": "5h0qf7IBZZ",
                "replyto": "QbaMsCbvCV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2166/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2166/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer oknc"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the thoughtful comments and suggestions. \n\n**Regarding the Presentation:**\n\nWe will follow the reviewer\u2019s suggestions on presentation to revise our paper. For the itemized list in the paper, we did not use the `\\itemize` command but instead used `\\bullet`. We have changed it to `\\itemize` for a better presentation.\n\n**Regarding GPT-4 Evaluation:**\n\nWe have replaced the GPT-4 score in Figure 1 and 5 with Rouge-L.\nWe use GPT-4 for evaluation because it is studied by previous literature[1,2], where GPT-4 shows good alignment with human evaluation. This metric is also widely used in previous works to evaluate instruction-following[3,4,5]. \n\n**Regarding the Questions:**\n\n1. We follow the definition in [6]. Black-box KD refers to the scenario where only the teacher model's API (only the generated sentences, not including the probabilities) is available, and other cases are white-box KD. \u201cWhite-box\u201d means we need the model parameters to obtain the output probabilities to compute reverse KLD. We have clarified the description of \u201cwhite-box KD\u201d in the introduction of the revised paper.\n2. $w_t$ does not reduce the estimator's variance in Eq. 5. It is the important sampling weight to ensure that the gradient estimation by sampling from the teacher-mixed distribution $\\widetilde{p}$ equals the original estimator.\n3. Exposure bias is a mismatch between MLE training and inference[7,8]. During training, the model predicts the next token conditioning on the prefix from the real data distribution. However, the prefix is generated from the model itself during inference. Therefore, the distribution of the prefixes seen during inference might differ greatly from those encountered during training, leading to a mismatch.\n4. \u201cDistinct 4-gram\u201d[9] is a fraction: $N/C$, where $N$ is the number of the distinct 4-grams in the generated responses and $C$ is the total number of 4-grams. It is a widely used metric to measure the generation diversity of a language model.\n\n[1] G-EVAL: NLG Evaluation using GPT-4 with Better Human Alignment\n\n[2] Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. 2023. In NeurIPS.\n\n[3] Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\\%* ChatGPT Quality.\n\n[4] Instruction Tuning with GPT-4.\n\n[5] LIMA: Less Is More for Alignment. 2023. In NeurIPS.\n\n[6] A Survey on Model Compression for Large Language Models.\n\n[7] Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks. 2015. In NeurIPS.\n\n[8] Sequence Level Training with Recurrent Neural Networks. 2016. In ICLR.\n\n[9] A diversity-promoting objective function for neural conversation models. 2016. In NAACL."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2166/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700283547726,
                "cdate": 1700283547726,
                "tmdate": 1700283547726,
                "mdate": 1700283547726,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DwDh9nzVrD",
                "forum": "5h0qf7IBZZ",
                "replyto": "A3KLlftYfb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2166/Reviewer_oknc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2166/Reviewer_oknc"
                ],
                "content": {
                    "comment": {
                        "value": "The reviewer checked responses from the authors. The reviewer wants to thank them for their clarifications and updates.\n\n> We have replaced the GPT-4 score in Figure 1 and 5 with Rouge-L. We use GPT-4 for evaluation because it is studied by previous literature[1,2], where GPT-4 shows good alignment with human evaluation. This metric is also widely used in previous works to evaluate instruction-following[3,4,5].\n\nIt may be ok if the authors use the GPT-4 score (feedback) as a supplemental evaluation metric (i.e., moving them to appendix), but it is still not either convincing enough or clear how scientifically meaningful the evaluations are, for the following reasons.\n\n- Being used in a few previous studies does not explain why the metric is scientifically meaningful for this study. [3] is a blog post, [4] failed to justify why GPT-4 feedback is meaningful, [5] analyzes \"Inter-Annotator Agreement\", but the protocol is unclear. Moreover, their assessment is based on labeling which response was better, or whether neither response was significantly better than the other, which is different from the scheme used in this study (score 1 - 10)\n- GPT-4 is a black-box service, and its behaviors may change over time. Also, the service is updated in an untrackable manner, thus it is very challenging to reproduce experimental results based on such services (including [1] and [2]). \n\n>  $w_t$ does not reduce the estimator's variance in Eq. 5. It is the important sampling weight to ensure that the gradient estimation by sampling from the teacher-mixed distribution equals the original estimator.\n\nThe reviewer needs more clarifications for this point.\nIt looks like the revised manuscript still claims setting $w_t \\approx$ ...  to reduce the variance of the estimator in Eq. 5 as follows \n\n>> Therefore, we approximately set $w_t \\approx$ ... to reduce the variance of the estimator in Eq. 5 \n\nfrom the manuscript."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2166/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700331055914,
                "cdate": 1700331055914,
                "tmdate": 1700331055914,
                "mdate": 1700331055914,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BWFDq0IwkF",
                "forum": "5h0qf7IBZZ",
                "replyto": "KcwpLSz35F",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2166/Reviewer_oknc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2166/Reviewer_oknc"
                ],
                "content": {
                    "comment": {
                        "value": "The reviewer thanks the authors for the quick updates. The rating was improved, conditioned on the updates.\n\nThe reviewer also wants to see the clarification of $w_t$ in the manuscript (footnote is fine if the space is limited)"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2166/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700378263252,
                "cdate": 1700378263252,
                "tmdate": 1700378263252,
                "mdate": 1700378263252,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "GuIPtFb6kD",
            "forum": "5h0qf7IBZZ",
            "replyto": "5h0qf7IBZZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2166/Reviewer_BHa2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2166/Reviewer_BHa2"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to use Reverse KL Divergence for distilling large-language models. The paper starts from the Reverse KLD objective in section 2, describes the difference with forward KLD and its advantage that it is mode-seaking that is preferred when student has low capacity. Then in section 2.2 they review the challenges of optimizing for Reverse KLD and revisit ideas from prior work to improve it. To resolve challenges of reverse KLD, they propose 3 strategies to mitigate:\n1) Decompose gradient into the gradient for single-step prediction and long sentence prediction\n2) Prevent reward hacking by mixing the teacher/student distributions for sampling next token\n3) Normalize the reward to prefer longer sequences.\nThey refer to Reverse KLD together with their strategies as MiniLLM.\nIn section 3, they evaluate the effectiveness of MiniLLM on instruction-following generation tasks. They use various teacher/student architectures including GPT-2, OPT, and LLaMA. They compare to baselines with and without knowledge distillation. Section 3.2 provides positive improvements using MiniLLM and section 3.3 provides analysis that shows the method scales well, gives well-calibrated models, and generates diverse outputs. Section 3.4 provides ablations on the three elements of MiniLLM."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- Significant gains and improvements compared with various baselines. The results show that improvements increase as the teacher gets bigger and all students at all parameter counts improve. So consistently large improvements.\n- Figure 1: MiniLM is 5% better than SeqKD on GPT4 score.\n- Table 1: MiniLM is up to 10% better than SFT w/o KD while KD is up to 5% better and SeqKD is up to 1% better than KD.\n- Table 1: MiniLM is up to 8% better than the teacher while no other baseline surpasses the teacher.\n- Comprehensive evaluations and ablations."
                },
                "weaknesses": {
                    "value": "- The results seem to point that Reverse KLD might be harder to tune and requires all the strategies in MiniLM for performing better than baselines. This can be a challenge for reproduction and further research. I am specifically pointing to Table 4 and comparing to baseline numbers in Table 1: Why is MiniLM without either length-normalization (DollyEval GPT-4 22.4) or teacher-student distribution mixing (36.1) is significantly worse than comparable SeqKD (41.2) or KD (40.3) or SFT w/o KD (38.6) in Table 1? Does that mean Reverse KLD is generally harder to train without these strategies?\n- Wall-clock time analysis of the method compared with baselines should be discussed. What is the training efficiency? How slow is the training with the MiniLM loss compared with SFT w/o KD, KD, and SeqKD? If the method is slower per iteration, what if baselines are trained for more iterations to match the wall-clock time of the method? Would they match the performance gains?"
                },
                "questions": {
                    "value": "- Page 2, introduction, line 11: How can one force q, the teacher, to do something? The teacher is not learnable. I\u2019m assuming this is a typo and p/q_theta should be exchanged.\n- Why is this work \u201cwhite-box\u201d KD? How is the method using the parameters of the teacher and not just the predictions of the teacher, p(y|x)?\n- All experiments seem to be on instruction-following generation tasks. How would this distillation method perform for pretraining only? Can it help speed-up the pre-training of small models?\n- Table 1: Can you provide examples of cases where the student is better than the teacher and provide a qualitative analysis of why? 8% improvement should show consistent improved behavior.\n- Figure 5: Does MiniLM benefit more from scaling the teacher compared with SeqKD? Can you report the relative improvements of SeqKD and MiniLM as the teacher is scaled compared with a base teacher? If yes, it is useful for future scaling endeavors.\n- Figure 8: y-axis says \u201cForward KLD\u201d but the caption says \u201creverse KLD\u201d, which value is plotted?\n- Does the method have any hyperparameters specific to MiniLLM? For example, is there any thresholding of the ratio of q/q or p/q in Eq. 7 or epsilon in the denominator? If so, can you provide ablations?\n\nSuggestions\n- MiniLLM is a self-contradictory name as it expands to Mini Large Language Model. Please consider changing it, for example, to MiniLM.\n- Figure 1: Please consider adding more description of sequence-level KD in the caption or the intro close to the reference to Figure 1. A reader not familiar with the literature does not learn about SeqKD until page 5.\n- Eq 1 and A.1: For consistency it would help to use the KL with negative sign throughout (Eq. 1 is without negative sign but Eq. 8 is with negative sign). It would also help to highlight the difference in Eq 13 and 14 by color. It\u2019s hard to notice the difference.\n- Eq 7: It might help to use single/long gradients to simplify this equation and other predefined terms. This equation is not easily digestible as a summary equation.\n\nTypos:\n- Page 2: \u201c... approximately minimizes the forward KLD\u201d -> \u201cminimize\u201d\n- Page 3: \u201c... the quality of the each \u2026\u201d -> \u201cthe quality of each\u201d"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2166/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698611199815,
            "cdate": 1698611199815,
            "tmdate": 1699636149886,
            "mdate": 1699636149886,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cOiIhmbD5r",
                "forum": "5h0qf7IBZZ",
                "replyto": "GuIPtFb6kD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2166/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2166/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer BHa2 (part 1)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the detailed and encouraging comments. We will follow the suggestions about the paper writing and consider a better name for our method.\n\n**Regarding the Weakness:**\n\n**1. Strategies to train MiniLLM**\n\nSince longer sentences tend to have larger reverse KLD, length normalization prevents the model from outputting short and simple responses. And teacher-mixed sampling migrates the reward-hacking in policy optimization[1]. We provide all our codes in the supplementary material and will open-source all the codes, data, and model checkpoints for reproduction and further research.\n\n**2. Training efficiency**\n\nThe training time with MiniLLM is generally less than 2 times of SFT and KD. Note that since we consider KD approaches on the downstream datasets (e.g., instruction-following) where the number of training instances is small, the total training time will not increase much with MiniLLM. \n\nWe also evaluate the results if we train the student model with vanilla KD for the same amount of time as MiniLLM based on the GPT-2-base model (training for 40 epochs). The results show that the original training time (20 epochs) is enough for the model to converge, and training for more steps hurts the performance.\n| Method         | Valid Rouge-L | Test Rouge-L |\n| -------------- | ------------- | ------------ |\n| KD (20 epochs) | 25.3          | 22.8         |\n| KD (40 epochs) | 24.3          | 21.0         |\n| MiniLLM        | 27.4          | 24.6         |\n\n**Regarding the Questions:**\n\n1. Yes, this is a typo. We have fixed it in the revision.\n2. We follow the definition in [2]. Black-box KD refers to the scenario where only the teacher model's API (only the generated sentences, not including the probabilities) is available[3,4], and other cases are white-box KD. In our case, we need the model parameters to obtain p(y|x) to compute the reverse KLD. We have clarified the description of \"white-box KD\" in the introduction of the revised paper.\n3. We did not conduct experiments during the pre-training phase and will leave this to future work. \n\n(continue in part 2)"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2166/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700282352281,
                "cdate": 1700282352281,
                "tmdate": 1700283288673,
                "mdate": 1700283288673,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AFP5IWDx3O",
                "forum": "5h0qf7IBZZ",
                "replyto": "alTNiozfVM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2166/Reviewer_BHa2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2166/Reviewer_BHa2"
                ],
                "content": {
                    "title": {
                        "value": "I thank the the authors, my concerns are resolved"
                    },
                    "comment": {
                        "value": "I thank the authors for their response and overall I see my concerns resolved and recommend this paper for acceptance.\n\n**W.1** Reverse KLD might be harder to tune\u2026 This might be a challenge for reproduction and further research.\n\nMy concern is resolved by the fact that the method has only one hyperparameter and the ablations on that hyperparameter in Figure 16 shows relative consistently of the selected value 0.2. Moreover, the 3 proposed strategies are shown to be useful across all experiments.\n\n**W.2.1** Wall-clock time \u2026 should be discussed.\n\nThe rebuttal provides the rough ratio of the wall-clock time for MiniLLM vs KD. I encourage the authors to add a table with the breakdown of the exact wall-clock time ratios for each parts of the MiniLLM rather than the overall rough estimate of \u201cless than 2x\u201d.\n\n**W.2.2** What if baselines are trained for more iterations to match the wall-clock time.\n\nThe rebuttal partially resolves my concern by providing KD results of 2x epochs. I encourage authors to provide similar results for SeqKD.\n\n**Questions**\n\nI thank the authors for answering my questions including providing examples of MiniLLM outputs compared with the teacher. I agree with the authors\u2019 analysis of the outputs."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2166/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700613100151,
                "cdate": 1700613100151,
                "tmdate": 1700613100151,
                "mdate": 1700613100151,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4dpPpr1zQI",
            "forum": "5h0qf7IBZZ",
            "replyto": "5h0qf7IBZZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2166/Reviewer_HmVQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2166/Reviewer_HmVQ"
            ],
            "content": {
                "summary": {
                    "value": "This work primarily concerns distillation of large language models (LLMs) into smaller, more portable versions. Compared to standard distillation approaches, the authors advocate the substitution of reverse KL instead of the more typical forward KL divergence objective. This incentivizes the model to pursue mode-seeking behavior, rather than coverage-seeking behavior, leading to more precise answers, with a lower probability of generating data outside of the teacher distribution. A policy gradient objective function is modified with single-step decomposition, teacher-mixed sampling, and length normalization to further improve optimization. Experiments with three families of LLMs (GPT-2, OPT, LLaMA) on a variety of benchmarks show that the proposed method (MiniLLM) indeed outperforms the baseline distillation approaches, across a range of student sizes."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "## S1. Relevant topic\nGiven the recent advances and widespread usage of LLMs, ways of compressing models for faster and cheaper deployment can have significant real-world impacts in promoting wider spread usage of such models. The proposed approach shows promise being able to reduce model size without suffering as much performance decay as the baseline approach (Fig 1). This can have the effect of allowing more people to run LLMs, with the hardware they have available.\n\n## S2. Experiments\n1) Models: The experiments are on 3 model families (GPT-2, OPT, LLaMA), across a range of student model sizes (3 each for GPT-2 and OPT). This is a pretty good spread and helps give a sense of how well the proposed method generalizes, and the relation between student model size and performance.\n2) Evaluation: Models are evaluated on DollyEval, SelfInst, VicunaEval, S-NI, and UnNI. Rouge-L, GPT4, and human evaluation are used as metrics. Results are reported on 5 generations from separate random seeds, which gives a better sense of the evaluation\u2019s reliability. \n3) Results: The main results suggest that MiniLLM indeed outperform the baselines, and occasionally even the teacher in certain cases. Taken at face value, this is pretty impressive, as some of the student models are considerably smaller (>10x for GPT=2, 10x for OPT). However, I have some concerns about the metrics and whether they\u2019re capturing the full picture here (see W1.1).\n\n## S3. Writing\nI\u2019ve listed a few miscellaneous corrections below, but overall, the writing is fairly clear and well-written."
                },
                "weaknesses": {
                    "value": "## W1. Reverse KL vs Forward KL\nOne of the primary contributions of the paper is the substitution of reverse KL Divergence for forward KL divergence. This leads the student model to pursue \u201cmode-seeking behavior\u201d, as opposed to \u201ccoverage-seeking behavior\u201d. While this does cut down on unrealistic generation samples, the trade-off is that such an approach will cause much of the long tail to be lost as well. This leads to a couple concerns:\n1) The loss of sample diversity is not captured by the paper\u2019s metrics, which primarily focus on realism/precision, so the baseline forward KL divergence is at a distinct disadvantage. Specifically, the metrics specifically measure where forward KL is weakest (correctness of samples), while ignoring where it is strongest (sample coverage). As such, the evaluation is somewhat unfair. Some sort of metric that captures sample diversity may yield a different story.\n2) The long-tail knowledge of LLMs is arguably one of their most impressive and valuable properties, so sacrificing this in the name of realism is somewhat disappointing. In particular, this also raises potential ethical or fairness issues, as loss of diversity could lead to loss of minority representation or amplification of stereotypes.\n3) Why do we have to make this tradeoff in the first place? Why not use both forward and reverse KL (see Q1), as in [a]?\n\n## W2. Novelty/Contributions\nFrom the abstract and introduction, it would seem that that the primary contributions are a) the focus on white-box KD for generative LLMs and b) the substitution of reverse KL instead of the more typical forward KL. Additionally, an amalgam of modifications (single-step decomposition, teacher-mixed sampling, length normalization) improves the policy gradient objective function to the final form (Equation 7) used in this paper. Some concerns/questions:\n1) This is not a reason for rejection in and of itself, but individually, neither of these are particularly new concepts. White-box KL has been explored in the past, and the merits of forward vs reverse KL (as well as other divergences) for generative modeling has also been well explored. \n2) It\u2019s not clear how related whitebox KD for generative LLMs and a reverse KL optimization are. In fact, they seem almost entirely orthogonal from each other.\n3) It\u2019s not clear from the Methods section how whitebox KL was used in the method. Where does having the teacher model\u2019s parameters play a role?\n4) The methods in Section 2.2 seems to be a series of cobbled together heuristics, and it appears that they aren\u2019t necessarily novel either, as there are clear connections with (cited) prior work. I\u2019m not necessarily saying that that\u2019s a bad thing to have as part of the method, but it doesn\u2019t appear to be something that should be counted as a contribution of this work.\n\n\n## Miscellaneous:\n- The name \u201cMiniLLM\u201d doesn\u2019t fully capture the method. Yes, the model is a smaller LLM, but the same can be said of a distilled LLM learned by forward KL divergence as well, so the name fails to distinguish one of the primary points of the paper.\n- pg 2: \u201cfinite-number classes\u201d => \u201ca finite number of classes\u201d\n- Appendix entries out of order\n- pg 5: \u201cOuyang et al. (2022)\u201d citation at top of the page should be parenthetical?\n- pg 9: Sec 4 \u2013 Knowledge Distillation: [b] may be a relevant related work\n- pg 9: Fig 8: The y-axis says \u201cForward KLD\u201d, while the caption says \u201creverse KLD\u201d. Also, doesn\u2019t this graph imply that w/o teacher-mixed sampling is better, if trained long enough?\n\n[a] Chen, Liqun, et al. \"Symmetric variational autoencoder and connections to adversarial learning.\" AISTATS, 2018.\\\n[b] Liang, K., et al. \"Mixkd: Towards efficient distillation of large-scale language models.\" ICLR 2021."
                },
                "questions": {
                    "value": "Q1. Given that forward and reverse KL each clearly have their own advantages and disadvantages, why not use both, e.g. as in [a]? See also [c] for a more general treatment of f-divergences for generative modeling.\n\nQ2. The student models outperforming the teacher model in Table 1 is somewhat surprising. Why do you think this is the case? Does it have to do with the specific choice of metrics? I\u2019m somewhat doubtful for example that a 120M parameter GPT-2 model is truly outperforming the 1.5 B parameter teacher model.\n\n[a] Chen, Liqun, et al. \"Symmetric variational autoencoder and connections to adversarial learning.\" AISTATS, 2018.\\\n[c] Nowozin, Sebastian, Botond Cseke, and Ryota Tomioka. \"f-gan: Training generative neural samplers using variational divergence minimization.\" NeurIPS 2016."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2166/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698697322413,
            "cdate": 1698697322413,
            "tmdate": 1699636149798,
            "mdate": 1699636149798,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8KScVBLDkJ",
                "forum": "5h0qf7IBZZ",
                "replyto": "4dpPpr1zQI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2166/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2166/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer HmVQ (part 1)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the detailed and thoughtful comments. We will follow the suggestions about the paper writing and consider a better name for our method.\n\n**Regarding W1 and Q1 (Forward KLD v.s. Reverse KLD):**\n\n**1. Diversity measurement**\n\nIn Table 3, we report the fraction of responses\u2019 distinct 4-grams and the test loss, which measures the diversity and mode coverage of the model generation. More discussion can be found in the \u201cGeneration Diversity\u201d paragraph in Section 3.3. In summary, the diversity does not decrease much (Dist-4: 99.5 v.s. 99.0), and the student model still covers most of the modes of the teacher model (test loss: 3.89 v.s. 3.95).\n\n**2. The long tail part of the distribution**\n\nWe argue that our method will not sacrifice the long-tail knowledge of LLMs but only cause the sampled sentences to be less diverse (for example, the model will output similar responses with different random seeds). This is because:\n\n+ The long-tail knowledge of LLMs is reflected by the long-tail part of the joint distribution of the input $x$ and the output $y$: $p(x, y)$, but our method computes reverse KLD between the conditional distribution $p(y|x)$. For example, even if $p(x,y)$ is a long-tail part, $p(y|x)$ will still be high if $y$ is a suitable continuation of $x$ and be covered by the student model using reverse KLD as the KD objective. In addition, reverse KLD encourages the model to ignore the low-probability regions of $p(y|x)$, and these regions have been proven harmful to language generation in previous literature[1,2].\n\n+ During MiniLLM training, we add a language modeling loss on the models\u2019 pre-training corpus (equivalent to minimizing the forward KLD between the student output distribution and the human distribution) as a regularization to preserve the knowledge learned during pre-training. \nFor potential fairness and ethical issues, we compute the HONEST score[3] on the provided dataset in [3] to measure the hurtful stereotypes of the llama-7B models\u2019 completion. We find that the HONEST score is not affected much by the KD approaches (higher scores indicate more hurtful completions):\n| Model      | HONEST Score |\n| ---------- | ------------ |\n| SFT w/o KD | 14.86        |\n| KD         | 15.22        |\n| SeqKD      | 16.04        |\n| MiniLLM    | 15.06        |\n\n**3. Why reverse KLD**\n\nThere is a trade-off between the mode-seeking and the coverage-seeking behavior because the small model has low capacity and cannot cover all the modes of the large model, as stated in Section 2. There are three reasons that we only use reverse KLD for KD on the instruction-following datasets in our experiments:\n\n+ Inspired by [4], we add a language modeling loss $\\mathcal{L}_\\text{pt}$ on the plain-text corpus, which is already a regularization to avoid the student model losing much of the long tail.\n\n+ We find that the linguistical diversity and the coverage of the real data distribution are not affected much by mainly considering reverse KLD (see the paragraph \u201cGeneration Diversity\u201d in section 3.3).\nBesides the role of $\\mathcal{L}_\\text{pt}$, we also suspect that the student model has the capacity to cover the main modes of $p(y|x)$, and most long-tail parts of $p(y|x)$ are noises, as shown in previous works[1,2]. \n\n+ In our pilot experiments, we tried combining reverse and forward KLD by summing the losses with the ratio 1:1, based on GPT-2-base with GPT-2-xLarge as the teacher, and compared the Rouge-L scores on the test sets. The results are shown in the following table. We can see that the Rouge-L scores were not affected much, indicating that reverse KLD is enough for KD in our setting. As a result, to keep the simplicity of the method, we did not add the forward KLD in our setting. \n\n  | Model                | Dolly | SelfInst | Vicuna | S-NI | UnNI |\n  | -------------------- | ----- | -------- | ------ | ---- | ---- |\n  | MiniLLM              | 24.6  | 13.2     | 16.9   | 25.3 | 30.1 |\n  | MiniLLM+forvward KLD | 23.4  | 13.8     | 16.9   | 24.7 | 29.3 |\n\n  We believe there are other ways to combine reverse and forward KLD, like in [a] and [c], and will explore them in future work.\n(continue in part 2)"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2166/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700281565351,
                "cdate": 1700281565351,
                "tmdate": 1700281902459,
                "mdate": 1700281902459,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7vwI8bOKaM",
                "forum": "5h0qf7IBZZ",
                "replyto": "4dpPpr1zQI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2166/Reviewer_HmVQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2166/Reviewer_HmVQ"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their thorough responses, which I\u2019ve read along with the other reviews (and author responses).\n\n### Novelty\nWhile I acknowledge the authors effort in making reverse KL work for distilling LLMs in their paper, I still believe the novelty here is being overstated. In particular, Section 2.1 is still written as if this work is the first ever to consider reverse KL for generative modeling, without citing many of the prior work, which while perhaps pre-dating the recent trend of LLMs, is still relevant.\n\n### Diversity\nOn diversity, could the authors explain more what the metrics for diversity measure?\n- Dist-4: My understanding is that this is a measure of the number of unique 4-grams, but reported as some sort of proportion. What is the numerator and denominator in this proportion? \n- Loss: It\u2019s not clear to me how this is measure of diversity. Rather this would seem more to be a measure of how probable the generated text is from the training distribution.\n\nFrom the response, I\u2019m also getting a mixed bag for interpreting the motivation and the results. It feels like the authors are simultaneously trying to argue for the value of reverse KL because it prioritizes coverage over covering modes (because low probability regions are harmful), and yet they are also arguing that coverage is in fact still intact?\n\n### Whitebox\nThank you for the clarification on the definition of white box here. There\u2019s still potential for confusion here though, as the authors are using whitebox to refer to having the teacher model\u2019s output probabilities; in many adversarial attack papers, having access to the model\u2019s output is still considered an assumption of blackbox attacks."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2166/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700439332733,
                "cdate": 1700439332733,
                "tmdate": 1700439332733,
                "mdate": 1700439332733,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "U4SVfbbLVa",
                "forum": "5h0qf7IBZZ",
                "replyto": "5NsEsOuMpw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2166/Reviewer_HmVQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2166/Reviewer_HmVQ"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for adding the clarifications to Section 2.1.\n\nDist-4: Given the definition of this metric, I find it a little surprising that everything is 99+%. Can you also report $N$ and $C$ separately?\n\nI still find the statements in bullets 1-4 to be a little strong. The statements \"will not lose the long-tail knowledge of LLMs\" and \"using reverse KLD allows the MiniLLM to ignore modes that are small or even noisy low-probability regions\" are contradictory as the long-tail is by definition low-probability. While I admire the authors efforts to quantify these aspects, metrics only capture part of the story, and it's still extremely difficult to tell what's actually being lost here.\n\nI remain fairly on the fence, but I'm willing to raise my score to a 6."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2166/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700613132389,
                "cdate": 1700613132389,
                "tmdate": 1700613132389,
                "mdate": 1700613132389,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LXQ7pJPsqt",
                "forum": "5h0qf7IBZZ",
                "replyto": "4dpPpr1zQI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2166/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2166/Authors"
                ],
                "content": {
                    "title": {
                        "value": "About the Rating in the Official Review"
                    },
                    "comment": {
                        "value": "We thank the reviewer for **the willingness to raise the score to 6**. We have reported $N$ and $C$ in the latest version of our paper.\n\nHowever, we notice that the score in the rating of the official review is still 5. Could you please **also change this rating to 6**?"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2166/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700703265249,
                "cdate": 1700703265249,
                "tmdate": 1700738640287,
                "mdate": 1700738640287,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "U9xCYMADYu",
            "forum": "5h0qf7IBZZ",
            "replyto": "5h0qf7IBZZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2166/Reviewer_v2PG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2166/Reviewer_v2PG"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a method called MiniLLM for knowledge distillation of large language models (LLMs). The method focuses on distilling smaller language models from generative larger language models. It replaces the forward Kullback-Leibler divergence (KLD) objective in standard knowledge distillation approaches with reverse KLD, which is more suitable for generative language models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper introduces an application for knowledge distillation of generative language models.\n- The proposed method is supported by well-structured experiments and evaluation on various datasets, demonstrating its effectiveness in generating more precise responses with higher overall quality, lower exposure bias, better calibration, and higher long-text generation performance."
                },
                "weaknesses": {
                    "value": "(major) The novelty of this paper is limited. It is just a simple application of reverse KL Divergence to knowledge distillation. However, distill models with reverse KLD have been researched before. The claim in the abstract \"how to effectively distill the knowledge \u2026 is still under-explored\" is not convincing. For example [1]. More importantly, this paper is not cited by the authors.\n\n(minor) In Table 1, the student model even outperforms the teacher model which lacks intuition. Although the authors attributed such results to the exposure bias issue of teacher-forcing, I doubt there is an overfitting problem with the experiments. Could the authors provide the hyper-parameters and the variance of each experiment?\n\n[1] Self-Knowledge Distillation via Dropout"
                },
                "questions": {
                    "value": "See Weaknessed."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2166/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698843655889,
            "cdate": 1698843655889,
            "tmdate": 1699636149723,
            "mdate": 1699636149723,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fbU6XFe6p1",
                "forum": "5h0qf7IBZZ",
                "replyto": "U9xCYMADYu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2166/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2166/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer v2PG"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the thoughtful comments.\n\n**Regarding Weakness #1 (Novelty):**\n\nOur paper focuses on the KD of **LLMs on language generation tasks**, which is essentially different from the KD in [1] and under-explored in previous literature. In [1], reverse KLD-based KD is applied to image classification tasks, where the output space is a finite set of labels. However, the output space of LLMs on text generation tasks is much more complex, consisting of discrete token sequences with unlimited length, making it difficult and non-trivial to apply reverse KLD to the KD of LLMs. \n\nSpecifically, in [1], the gradient of reverse KLD can be directly computed using the student model\u2019s output distribution. In contrast, as stated in Section 2.2, computing the reverse KLD between the student and teacher language models requires sampling sentences from $q_\\theta$ in an auto-regressive manner, and thus Policy Gradient (and the strategies we proposed) is needed to calculate the gradient of the objective. Therefore, we argue that it is novel to apply reverse KLD to KD of LLMs and design algorithms to stabilize the training process. Moreover, since [1] is related to our method, we have added it to the reference in the revision.\n\n**Regarding Weakness #2 (The student model outperforming the teacher model):**\n\nThe hyper-parameters of each experiment are provided in Appendix C.1. We train 1.5B and 760M GPT-2 models under seeds [10, 20, 30, 40, 50] and report the average Rouge-L scores together with the standard deviations:\n\n| Model          | Dolly      | SelfInst   | Vicuna     | S-NI       | UnNI       |\n| -------------- | ---------- | ---------- | ---------- | ---------- | ---------- |\n| Teacher (1.5B) | 27.2 (0.3) | 14.9 (0.6) | 16.5 (0.1) | 27.8 (0.7) | 33.1 (1.0) |\n| MiniLLM (760M) | 26.3 (0.2) | 16.0 (0.6) | 18.1 (0.1) | 28.3 (0.6) | 36.4 (0.8) |\n\nNote that we train the models on one training set (Dolly) but observe that the student model outperforms the teacher model on the other 4 test sets. Therefore, we argue that the student models generalize well.\n\nAs reported in previous works[2], replacing teacher-forcing with policy optimization can improve the quality of the generated texts by migrating exposure bias, which is orthogonal to the benefit of KD. Since the teacher model is trained in a teacher-forcing manner, the student model can outperform the teacher model by migrating this defect. \n\n[1] Self-Knowledge Distillation via Dropout.\n\n[2] Text generation by learning from demonstrations. 2021. In ICLR."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2166/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700280678292,
                "cdate": 1700280678292,
                "tmdate": 1700280786982,
                "mdate": 1700280786982,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]