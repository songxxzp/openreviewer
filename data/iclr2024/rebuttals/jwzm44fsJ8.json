[
    {
        "title": "Multilingual Code Retrieval Without Paired Data: New Datasets and Benchmarks"
    },
    {
        "review": {
            "id": "600FAjHo1V",
            "forum": "jwzm44fsJ8",
            "replyto": "jwzm44fsJ8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5254/Reviewer_mHNB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5254/Reviewer_mHNB"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the problem of multi-language code search, from different natural languages to different coding languages, or between different coding languages. A main contribution is a new evaluation set M^2CRB, which consist of en/pt/es/fr/de queries to python/java/javascript code snippets, all from real world GitHub repositories. The paper also brings up a hypothesis that the model should be able to learn the relationship between a language and a code with no direct training pairs, but bridged by an anchor language. English is used in this study.  \n\nExtensive experiments are done with different languages (natural and code), different sizes of CodeT5 models and different model architecture (CodeT5 with single/dual encoder, CodeBERT and CodeGen). Results show that the model can indeed learn to retrieve non-directly linked pairs of natural and coding languages. The effect of model architecture and sizes are also discussed."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Introduced a new multilingual natural language to code evaluation. We need more realistic evaluation sets.\n- Evaluated different models, sizes and languages extensively.\n- Provided details of how the dataset is created."
                },
                "weaknesses": {
                    "value": "Experiment about the effectiveness of data is not clearly setup.\n- The purpose of adding TATOEBA and CodeXGlue is not clearly articulated. They also make the claim of learning through anchor languages weaker because another factor is added to the training mix.\n- It's not sure if the pre-training data of various models already have multi-lingual exposure. The near-0 auMMR number could also be task alignment issues.\n- Some ablation studies will help a lot to understand the role of each dataset.\n\nAnother weakness is in writing. It's common to find long or poorly written sentences that's hard to read, or not understandable at all to the Reviewer with moderate effort. See questions for some examples."
                },
                "questions": {
                    "value": "- What's the purpose of the AST parsing phase?\n- Table 2: \"high scores are observed across all language combinations\". It's better to have a baseline to help understand a relative assessment of \"high\".\n- Section 3.1 last paragraph: \"Note that any data realization x observed...\" It's not clear why we need to discuss it here. Better to clarify the purpose.\n- Table 3: Why do we need the TATOEBA/CodeXGlue set? What purpose do they serve in answering the research questions?\n- Section 4.1 second paragraph: \".., models able to generate train ...\" Reviewer couldn't understand what does this mean with moderate effort.\n- Section 4.1 right after Equation 3: \"... and define translation auxiliary tasks.\" What does this sentences mean?\n- Section 4.3.2 last sentence: \"Finally, the decoder-only model doesn't reach...\" it's better to call out the model name (is it CodeGen?) to save the readers some back-n-forth work.\n- Would you open source the data and evaluation code?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "no"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5254/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698623519428,
            "cdate": 1698623519428,
            "tmdate": 1699636524543,
            "mdate": 1699636524543,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nNGVp0TFID",
                "forum": "jwzm44fsJ8",
                "replyto": "600FAjHo1V",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5254/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5254/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the very detailed review and for providing suggestions to improve our manuscript. We took those into great consideration and carefully responded to comments and questions in what follows.\n\n**1. Experiment about the effectiveness of data is not clearly setup.**\n\n**Response:** Assuming that the reviewer\u2019s concerns are related to experiments with results reported in tables 2 and 7 (*kindly let us know if not, and we will address it*), we clarify that those correspond to estimates of the semantic alignment between text and code for each language combination. We did that by estimating the expected `BERTScore` between text and code pairs, and did that independently for each language combination. We did that using an encoder that was trained on multiple natural and programming languages.\n\nWe also reported what we called the `GPTScore` which uses a similarity obtained from prompting a language model (GPT-3.5-turbo in this case) with a text-code pair. More details about this approach including the exact prompt we used are placed in the appendix due to space constraints.\n\nWe observed both metrics to be predictive of retrieval performance, *e.g*., they suggest high semantic similarity for python and low for javascript, and those are the languages for which models consistently perform the best and the worst, respectively.\n\n**2. The purpose of adding TATOEBA and CodeXGlue is not clearly articulated. They also make the claim of learning through anchor languages weaker because another factor is added to the training mix.**\n\n**Response:** We used a relatively small number of source-source and target-target instances in order to make our training mix more diverse, and then help our models cope with shifts at evaluation (cf. [1] for a discussion on how diverse training data sources enable generalization to new, shifting distributions). CodeXGlue for instance gives us some code-code pairs. Even though the underlying languages differ from those we evaluate on, it\u2019s a more similar task to those we expect at testing than the ones defined by other datasets.\n\n**3. It's not sure if the pre-training data of various models already have multi-lingual exposure. The near-0 auMMR number could also be task alignment issues.**\n\n**Response:** We completely agree with the reviewer. While base models were trained on multiple languages, they were never trained to yield chunk/sentence level embeddings that semantically align. Fine-tuning is thus required to align with test tasks of interest.\n\n**4. Clarity**\n\n**Response:** We thank the reviewer for kindly remarking points of improvements in terms of clarity and presentation of our contributions. We addressed each related comment in what follows.\n\n**5. What's the purpose of the AST parsing phase?**\n\n**Response:** We used AST parsing to be able to collect natural language from code snippets (docstrings and comments). We also followed CodeSearchNet and split files into functions/methods.\n\n**6. Table 2: \"high scores are observed across all language combinations\". It's better to have a baseline to help understand a relative assessment of \"high\".**\n\n**Response:** Thank you for pointing this out. We modified that sentence to make precise and clear what we mean by high similarity in that case.\n\nWe clarify that all of those scores are bounded in $\\[0,1\\]$, so that sentence suggests that the scores are much closer to the upper limit than to the lower one. We further remark that a random embedding or random data would score very close to 0, and would not yield a very informative baseline. In other words, our goal in that sentence is to highlight that the scores suggest a reasonable signal-to-noise ratio in the data. This is further confirmed in our experiments in Section 4.3.1 and in table 9 in the appendix: the data can be predicted to high accuracy so the semantic similarity must be high.\n\n\n**7. Section 3.1 last paragraph: \"Note that any data realization x observed...\" It's not clear why we need to discuss it here. Better to clarify the purpose.**\n\n**Response:** In that paragraph, we are highlighting that when we use $x$ to refer to a data instance, we are referring to a sequence of tokens. This paragraph is intended to introduce some notation to make the manuscript accessible to readers maybe not so familiar with sequence modeling settings. \n\n**8. Section 4.1 second paragraph: \".., models able to generate train ...\" Reviewer couldn't understand what does this mean with moderate effort.**\n\n**Response:** *\u201dModels able to generate\u201d* refer to models that can be used to generate text. In more detail, our evaluation covers encoder-only, encoder-decoder, and decoder-only architectures. As such, encoder-decoder and decoder-only both can be used for text generation, in addition to yielding chunk/sentence level embeddings."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5254/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700156076376,
                "cdate": 1700156076376,
                "tmdate": 1700156076376,
                "mdate": 1700156076376,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xlcA3jd3OG",
                "forum": "jwzm44fsJ8",
                "replyto": "600FAjHo1V",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5254/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5254/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**9. Section 4.1 right after Equation 3: \"... and define translation auxiliary tasks.\" What does this sentences mean?**\n\n**Response:** This sentence explains that the models that are able to generate text, *i.e.*, encoder-decoder and decoder-only models, are trained with an additional \u201cgenerative\u201d objective compared to contrastive-only encoders.\n\nTo make the presentation clear, we will rephrase it and add an explicit reference to the generative objective discussed in Section 4.1 - Eq. (3).\n\n**10. Section 4.3.2 last sentence: \"Finally, the decoder-only model doesn't reach...\" it's better to call out the model name (is it CodeGen?) to save the readers some back-n-forth work.**\n\n**Response:** Yes, we agree with the reviewer and replaced it in the text.\n\n**11. Would you open source the data and evaluation code?**\n\n**Response:** Yes! The datasets are already (anonymously) available:\n\n- https://huggingface.co/datasets/blindsubmissions/M2CRB\n- https://huggingface.co/datasets/blindsubmissions/GH_text2code\n\nLinks are also in the manuscript in the captions of tables 1 and 3 for training and testing data, respectively.\n\nCode for data processing, fine-tuning, and evaluation will be released upon publication.\n\n**References**\n\n[1] Albuquerque I, Monteiro J, Darvishi M, Falk TH, Mitliagkas I. Generalizing to unseen domains via distribution matching. arXiv preprint arXiv:1911.00804. 2019 Nov 3."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5254/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700156089356,
                "cdate": 1700156089356,
                "tmdate": 1700157103815,
                "mdate": 1700157103815,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rYn4eFn3ub",
                "forum": "jwzm44fsJ8",
                "replyto": "32JqKQoWRa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5254/Reviewer_mHNB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5254/Reviewer_mHNB"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the answers."
                    },
                    "comment": {
                        "value": "Thanks for the detailed answers.\n\nRegarding the introduction of two datasets (TATOEBA and CodeXGlue). Now I know what's the purpose, but it's still not clear that if this is necessary by some evidence in this particular study, or just an educated guess to try to make the final result better.\n\nI mostly maintain my current rating on the basis of supporting open source dataset and evaluation work."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5254/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700634237091,
                "cdate": 1700634237091,
                "tmdate": 1700634237091,
                "mdate": 1700634237091,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "pwJvGH1c5s",
            "forum": "jwzm44fsJ8",
            "replyto": "jwzm44fsJ8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5254/Reviewer_QVKG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5254/Reviewer_QVKG"
            ],
            "content": {
                "summary": {
                    "value": "- This work introduces two datasets:\n  - a Multilingual `(code snippet, NL queries)` paired data in 4 natural languages: Spanish, Portuguese, German, and French, and three programming languages: Java, JavaScript, and Python. The dataset is collected from GitHub data, with AST parsing and natural language identification, and then human validation to check for correct language identification. This dataset contains ~7700 data points and is used as a test dataset.\n  - And, an `(English, code snippets)` paired data similar to the CodeSearchNet data\n- This work then proposes a training setup to train code search models for languages for whom parallel data is not available. The training setup requires parallel data between one language ($S$) and code segments ($T$), and parallel data between all other languages ($S'$) and $S$. The model is then trained with a contrastive loss aimed at learning similar embeddings for similar code segments/natural languages and a standard autoregressive loss.\n- This work then presents results for code search for the 4 languages introduced in the work, code search using English, and code-code search between Java and Python."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The Code-NL Paired dataset between languages other than English has not been explored in prior works and could be useful for non-English speaking developers.\n- From the statistics of the introduced data, it seems that, unlike English, data in these languages is not sufficiently available to train a model in the standard way. Thus, the authors propose to utilize available NL-paired datasets to indirectly learn code search for these new languages.\n- This training setup, in future work, might also be explored to extend to code generation given NL descriptions in non-English languages. Additionally, the dataset proposed in this work could be used for evaluation in that setting as well.\n- The paper is well-written and relatively easy to follow."
                },
                "weaknesses": {
                    "value": "- There are no baselines presented to understand how well the proposed technique actually works. While baselines might be difficult to get for non-English code search (Table 4), I would assume for Python-Java code search (Table 5) and English code search (Table 6) available model embeddings should work well. For instance, CodeBERT reports an average MRR of 0.7603. It is not immediately clear what the auMRRc would be for this model, and it would have been helpful to get these numbers. Similarly for the Python-Java code search, it would have been nice to get baselines from available multi-lingual pre-trained models.\n- This work requires paired NL data (such as between Spanish and English), and it incorporates this paired data in the loss function. However, another way to utilize this data could be to learn a translation model from English to Spanish using the paired data and use this translation model to translate English queries in the CodeSearchNet data to create a much larger (Spanish, Code snippet) dataset, albeit with some noise. This has the advantage of creating larger synthetic training data that can be directly used to train the code search model, instead of the objective proposed in the paper. Do the authors have some assessment on why the proposed technique is a better approach than this one?"
                },
                "questions": {
                    "value": "mentioned above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5254/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698796250814,
            "cdate": 1698796250814,
            "tmdate": 1699636524453,
            "mdate": 1699636524453,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "v3eysuRf6J",
                "forum": "jwzm44fsJ8",
                "replyto": "pwJvGH1c5s",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5254/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5254/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their comments and for the detailed assessment of the strengths and weaknesses they identified. We carefully address the latter below.\n\n**1.There are no baselines presented to understand how well the proposed technique actually works. While baselines might be difficult to get for non-English code search (Table 4), I would assume for Python-Java code search (Table 5) and English code search (Table 6) available model embeddings should work well. For instance, CodeBERT\u2026**\n\n**Response:** We kindly highlight that we did evaluate other models not trained by us such as CodeBERT and CodeT5. We observed however that those models have no predictive power in the settings we consider and perform as well as a random embedding. Note that CodeBERT MRR results openly available typically correspond to the performance after fine-tuning in a particular language combination. **We went beyond that and evaluated a single fine-tuned model, trained in the multilingual scheme we proposed.**\n\nWe further refer the reviewer to table 9 for results with state-of-the-art embeddings (OpenAI\u2019s and Sentence Transformers\u2019s) from models not trained by us, both open and closed source. Note that those results were kept in the appendix because we have no control over the training of the evaluated models, and in particular have no control over the data used for training. Still, those results attest for the predictability of the M$^2$CRB data and offer a performance upper bound that follow-up methods should strive to achieve.\n\n\n**2. This work requires paired NL data (such as between Spanish and English), and it incorporates this paired data in the loss function. However, another way to utilize this data could be to learn a translation model from English to Spanish using the paired data and use this translation model to translate English queries in the CodeSearchNet data to create a much larger (Spanish, Code snippet) dataset, albeit with some noise. This has the advantage of creating larger synthetic training data that can be directly used to train the code search model, instead of the objective proposed in the paper. Do the authors have some assessment on why the proposed technique is a better approach than this one?**\n\n**Response:** Thank you for commenting on the use of translation to generate training data. While we agree with the reviewer and, starting from an English-to-code dataset, one can indeed create synthetic variations of it by translating docstrings to different languages, it must be noted that translation step would add significant cost to pre-processing, and its quality could not be good enough considering we would be translating docstrings rather than natural language, and that we could be targeting low-resource languages.\n\nCost and quality concerns aside, the proposal is indeed valid and orthogonal/complementary to what we show rather than a competing approach: if one does what the reviewer suggested and, for instance, translates docstrings in CodeSearchNet to Spanish and French, we showed that training a model in such a dataset would enable mapping directly between any combination of natural or programming languages. Either approach used to build a set of paired datasets would be equally good.\n\nIn other words, combining existing training datasets or synthesizing via translation are both valid and complementary approaches to build a dataset with the properties our training requires. We highlight once more: our goal is to show that generalization is possible to indirectly observed language combinations.\n\nWe added a sentence in Section 3.2 remarking that other approaches are possible to create the style of training data we require."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5254/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700156026410,
                "cdate": 1700156026410,
                "tmdate": 1700156026410,
                "mdate": 1700156026410,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ksUysM6Zlb",
            "forum": "jwzm44fsJ8",
            "replyto": "jwzm44fsJ8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5254/Reviewer_z3xc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5254/Reviewer_z3xc"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces two new datasets to address code retrieval limitations due to the scarcity of data containing pairs of code snippets and natural language queries in languages other than English. \nThe first dataset, M\u00b2CRB, is an evaluation benchmark with text and code pairs for multiple natural and programming language pairs. The authors propose a training hypothesis that models can map from non-English languages into code if they can map from English to code and other natural languages to English. They create a training corpus combining a new paired English/Code dataset with existing translation datasets. Extensive evaluations confirm that models can generalize to unseen language pairs they indirectly observed during training. The paper contributes a new evaluation benchmark, additional training data, a training recipe for unseen language pairs, and an analysis of different design choices and fine-tuning schemes."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Introducing new datasets, M\u00b2CRB and the paired English/Code dataset, addressing the scarcity of multilingual code retrieval data.\n2. Rigorous evaluation of various model classes and sizes on both new and existing datasets, confirming the proposed training hypothesis.\n3. Clear presentation of the filtering pipeline, training methodology, and results, making it easy to follow and understand.\n4. The study contributes to the understanding of multilingual code retrieval and offers a path for future work on more diverse language combinations."
                },
                "weaknesses": {
                    "value": "1. While the M\u00b2CRB dataset covers multiple language pairs, it can be expanded to include more programming languages for better representation.\n2. The study focuses on the code search/retrieval setting, but it would be helpful to investigate the applicability of the introduced data and training approaches in generative settings as well.\n3. The evaluation focuses on models within the 60M-360M parameter range, and exploring larger-scale models could provide insights into the effect of model size on generalization capabilities in this domain."
                },
                "questions": {
                    "value": "1. Can the training approach proposed in this paper be adapted for generative models, and if so, how would it affect their performance on text-to-code generation tasks?\n2. Are there any potential biases in the dataset, such as the influence of specific programming language styles or the quality of non-English docstrings, that may affect the model's generalization capability?\n3. How do the models perform when fine-tuned on different programming languages (e.g., C++, Rust, etc.) and less common natural languages? Would the performance be consistent with the results presented in the paper?\n4. How would the results change when using larger-scale models, such as GPT-3 or the recent Megatron-LM? Would the generalization capabilities improve with increased model capacity?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5254/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698819626829,
            "cdate": 1698819626829,
            "tmdate": 1699636524336,
            "mdate": 1699636524336,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CsvTFEwmuD",
                "forum": "jwzm44fsJ8",
                "replyto": "ksUysM6Zlb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5254/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5254/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the insightful review. We addressed all comments and questions in detail in the following.\n\n**1. While the M$^2$CRB dataset covers multiple language pairs, it can be expanded to include more programming languages for better representation.**\n\n**Response:** We agree with the reviewer. Kindly refer to the *Update plan* section of our dataset card in the following link where this is listed: https://huggingface.co/datasets/blindsubmissions/M2CRB\n\n**2. The study focuses on the code search/retrieval setting, but it would be helpful to investigate the applicability of the introduced data and training approaches in generative settings as well.**\n\n**Response:** We completely agree with the reviewer and also mentioned this in the text. Especially for the case of encoder-decoder architectures that require paired training data, the approach we proposed could enable multi-linguality efficiently for languages not necessarily combined during training.\n\nHowever, we decided to focus on the retrieval setting given its high practical importance, especially now that retrieval augmented generation (RAG) approaches become the prevalent language modeling framework.\n\n**3. The evaluation focuses on models within the 60M-360M parameter range, and exploring larger-scale models could provide insights into the effect of model size on generalization capabilities in this domain.**\n\n**Response:** We completely agree with the reviewer and highlight that the decision on the model scale we focused on was in part influenced by the amount of compute we had available. We also note that recent literature (*e.g.*, [1]) has shown that encoders do not observe scaling gains as pronounced as decoders. For instance, the extremely popular `sentence-transformers/all-MiniLM-L12-v2` encoder has only 33M parameters. Another recently released encoder, `bigcode/starencoder`, has 125M parameters. **We thus believe the range we chose to be very well aligned with common practice in this setting**.\n\n**4. Can the training approach proposed in this paper be adapted for generative models, and if so, how would it affect their performance on text-to-code generation tasks?**\n\n**Response:** As discussed above and in the manuscript, our proposed training strategy would be useful as-is to enable multi-linguality in encoder-decoder architectures. Even more than that, our results show that this can be done while also carrying out contrastive training on top of chunk-level embeddings obtained from the encoder. So to answer your question, **yes**, conditional generative models can benefit from our proposal, and **no**, we do not anticipate it affecting the performance of the base model it builds upon.\n\n**5. Are there any potential biases in the dataset, such as the influence of specific programming language styles or the quality of non-English docstrings, that may affect the model's generalization capability?**\n\n**Response:** Thank you for this insightful question. We measured language-specific text-code alignment (via `BERTScore` and our proposed `GPTScore`) and did observe variation across languages. Indeed, the textual component of the data we make available consists of naturally occurring comments on GitHub and, as such, relatively high variance is expected given how large a community of developers GitHub represents, and subcommunities will have different practices. On the other hand, natural data is reflective of what a real-world codebase would look like, and thus defines a useful \"in-the-wild\" testbed.\n\n**6. How do the models perform when fine-tuned on different programming languages (e.g., C++, Rust, etc.) and less common natural languages? Would the performance be consistent with the results presented in the paper?**\n\n**Response:** Our results suggest strong generalization ability to new language combinations for various model classes and training settings. We do not anticipate any issue specific to the languages highlighted by the reviewer, provided that they are included in the training mix (not necessarily paired as we\u2019ve shown).\n\n**7. How would the results change when using larger-scale models, such as GPT-3 or the recent Megatron-LM? Would the generalization capabilities improve with increased model capacity?**\n\n**Response:** We kindly refer the reviewer to item 3 above with related discussion. While this would need to be tested in our specific setting, recent literature suggests gains on encoders are not as pronounced with scaling as they are for decoders. This is also confirmed by the fact that our best performing models were not the biggest ones. We also highlight that our decision in terms of which models to fine-tune was in part motivated by the amount of compute we had available for this project.\n\n**References**\n\n[1] Wang Y, Le H, Gotmare AD, Bui ND, Li J, Hoi SC. Codet5+: Open code large language models for code understanding and generation. arXiv preprint arXiv:2305.07922. 2023 May 13."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5254/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700156836737,
                "cdate": 1700156836737,
                "tmdate": 1700159293582,
                "mdate": 1700159293582,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "aSXy8B5nkm",
            "forum": "jwzm44fsJ8",
            "replyto": "jwzm44fsJ8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5254/Reviewer_MZBA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5254/Reviewer_MZBA"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a dataset called $M^2CRB$ where docstring and corresponding functions are used as paired data to construct a search dataset. The dataset includes docstrings in Spanish, Portuguese, German, and French. The paper proposes a training recipe that enables search over unseen language pairs. The paper reports the effects of different design choices and the performance of various fine-tuning schemes."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "A new dataset for multilingual code retrieval task."
                },
                "weaknesses": {
                    "value": "The paper proposed a dataset that is created automatically. The training and evaluations are not motivated well. In the whole paper, every experiment performed is not justified. The main questions addressed in the paper are somewhat known. In my opinion, the paper does not meet the bar of ICLR. There is no scientific or technical contribution. I couldn't perceive the value and need of the dataset."
                },
                "questions": {
                    "value": "- If we need multilingual docstring, isn't it possible to use NMT models to translate the docstring available in English? Some experiments in the Appendix use Google translation API which should be part of the main body and discussion. This way the value of the dataset could be better demonstrated.\n- \"Moreover, the search setting is less compute-intensive relative to common language models, rendering experimentation more accessible\" - Is it a good reason to study code search tasks?\n- The research question related to unseen languages is not clear. From the literature, we know that multilingual LMs learn to map text or code in a shared embedding space that enables them to perform on unseen language pairs. What new this paper is trying to show or prove?\n- \"Contrary to what is usually observed in more standard evaluation conditions, in this multilingual setting, scaling up model size will not necessarily improve performance.\" - Why?\n- Why does a retriever model need to be trained to generate? When we are in the era of large language models that are doing great in code generation, why do we want to train CodeBERT-like models for generation?\n- In 2-3 lines, please outline what new findings this paper presents."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5254/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698908709986,
            "cdate": 1698908709986,
            "tmdate": 1699636524245,
            "mdate": 1699636524245,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IcB5srNEUN",
                "forum": "jwzm44fsJ8",
                "replyto": "aSXy8B5nkm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5254/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5254/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their assessment and for seeking to ensure our contribution is relevant, which we believe will help improve our work. Please see below for detailed responses and comments.\n\n**1. training and evaluations are not motivated well.**\n\n**Response:** We agree with your other comment, *i.e.*, \u201c*we know that multilingual LMs learn to map text or code in a shared embedding space that enables them to perform on unseen language pairs*\u201d. However, the embedding spaces learned by LMs do not immediately lead to embeddings which can be used to query code for retrieval applications. One of the motivations of this work is to show that we can adapt the representations learned by LMs and make them much more effective for cross-lingual search. \n\nWe further clarify that evaluations follow the standard information retrieval setting where, given a query, one seeks $k$ relevant entries from a database. That\u2019s our target application as discussed in the text. We discuss motivations further in item 5 below. As per training, we used contrastive learning approaches to yield a semantic embedding that is language agnostic so as to enable multi-linguality.\n\nWhile we believe the summary above is written down somewhat clearly in the text, we would be happy to modify any part of the content that the reviewer believes is confusing or misleading.\n\n**2. The main questions addressed in the paper are somewhat known.**\n\n**Response:** The main question we seek to address is as follows: **Can a contrastively trained embedding enable retrieval for language combinations only indirectly seen during training?** As we argued in the text, if yes, then training of this kind of model can be carried out without paired data from all languages in the set of languages of interest. **To our knowledge, ours is the first work showing evidence of that being the case.**\n\n**3. I couldn't perceive the value and need of the dataset. If we need multilingual docstring,...** \n\n**Response:** That\u2019s actually the main motivating point of the line of work we carry out in this paper: **translation steps add overhead and should be removed**. This translation overhead (be it machine or human translation) mainly affects non-English-speaking communities of developers or developers required to write documentation in local language. We thus make available both data and efficient training strategies that help bypass undesirable intermediate translation steps.\n\nWe also highlight that existing machine translators are trained in natural language, and don\u2019t necessarily work well on docstrings and code comments which embed jargon, variable names, and other code specific information. Machine translation might also be unreliable in low-resource languages.\n\n**4. \u2026isn't it possible to use NMT models to translate the docstring available in English?**\n\n**Response:** Please refer to Table 10 in the appendix where we compare direct search with models that use an external translator of docstrings. We see that multilingual models perform better than *translate+search* using an English-only model. We also see that multilingual models can even further benefit from translating docstrings into English, so even if translation is an option, multilingual training is beneficial.\n\n\n**5. good reason to study code search tasks?**\n\n**Response:** The main motivation we considered for doing code search as opposed to generation is to address situations where **users wish to obtain pieces of code from a known and tested codebase**. There are many situations where the user desires retrieval results and in the case of retrieval augmented generation (RAG) models, our approach can serve as the underlying representation used to perform the retrieval step as well as a generator.  One can also consider the situation of proprietary codebases that are not in the training set of large code LMs. In those cases, generative models wouldn\u2019t be able to generate code that makes sense in the context of an API or library it never had access to and knows nothing about.\n\nWe additionally highlight that code search is an **extremely active research field** with several papers coming out every week, especially so given the recent trend in combining retrievers with decoders to perform **retrieval augmented generation (RAG)**.\n\nFinally, as noted by the reviewer, we also highlighted in the paper that retrieval applications are typically less expensive in terms of compute resources requirements, and might be more accessible as a research topic to groups with compute constraints."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5254/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700155757007,
                "cdate": 1700155757007,
                "tmdate": 1700159459395,
                "mdate": 1700159459395,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TE3F2v1GQO",
                "forum": "jwzm44fsJ8",
                "replyto": "Y5tcfGl33n",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5254/Reviewer_MZBA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5254/Reviewer_MZBA"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your answers."
                    },
                    "comment": {
                        "value": "Thank you for taking time to answer my questions. However, I am not convinced that the work is significant. The paper highlights that it shows evidence that multilingual contrastively trained encoders generalize to language combinations only indirectly observed during training. This is perhaps not true. For example, UniXCoder was trained via contrastive learning on 6 languages, and it shown it work well on zero-shot code-to-code search tasks. Source code across programming languages often share a high lexical overlapping that can play the anchor role to learn multilingual alignment. At least that is my expectation.\n\nOverall the paper does not meet the bar for ICLR in my opinion. Therefore, I am retaining my score."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5254/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700645259419,
                "cdate": 1700645259419,
                "tmdate": 1700645259419,
                "mdate": 1700645259419,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]