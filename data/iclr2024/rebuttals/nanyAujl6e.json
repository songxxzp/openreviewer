[
    {
        "title": "Out-of-Distribution Detection with Negative Prompts"
    },
    {
        "review": {
            "id": "PI1XxpT0f2",
            "forum": "nanyAujl6e",
            "replyto": "nanyAujl6e",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1826/Reviewer_ayMs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1826/Reviewer_ayMs"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on the out-of-distribution detection problem, which aims to precisely classify samples of known categories, and accurately discern samples of unknown categroies. To facilitate the recognition the out-of-distribution examples, a CLIP-based method is proposed, where a set of learnable negative prompts for each class are introduced. Promising results are obtained compared to existing out-of-distribution detection methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper is clearly written and easy to follow. The weakness of the hand-crafted prompts is clearly interpreted and the motivation is reasonable.\n2. The proposed approach is simple and intuitive.\n3. Promising experimental results are achieved compared to existing OOD detection and prompt-based methods."
                },
                "weaknesses": {
                    "value": "1. Meta-Net in Figure 6 is not introduced in the paper.\n2. The motivation and model design of this paper are similar to DualCoOp. The authors claim that the proposed method could learn negative prompts to capture negative features compared to DualCoOp, but there is no visual or quantitative evidence to verify this statement. Besides, the OOD detection performance of the DualCoOp design is not experimentally verified.\n3. Some existing OOD detection methods proposed in 2022 and 2023 are not compared or discussed[2][3][4].\n4. It seems that the authors have submitted this paper with an ICLR 2023 template.\n\n[1] Dualcoop: Fast adaptation to multi-label recognition with limited annotations.\n\n[2] Out-of-Distribution Detection with Deep Nearest Neighbors.\n\n[3] LINe: Out-of-Distribution Detection by Leveraging Important Neurons.\n\n[4] Decoupling MaxLogit for Out-of-Distribution Detection"
                },
                "questions": {
                    "value": "In equation(9), the positive score and the negative score are obtained independently. If the obtained positive category is different from the negative category, could this lead to mistakenly recognizing a correct positive prediction as out-of-distribution?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1826/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697964070664,
            "cdate": 1697964070664,
            "tmdate": 1699636112199,
            "mdate": 1699636112199,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OEss8zZd2Z",
                "forum": "nanyAujl6e",
                "replyto": "PI1XxpT0f2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1826/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1826/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We sincerely thank you for your constructive comments! Please find our responses below.\n\n>**Q1:** Meta-Net in Figure 6 is not introduced in the paper.\n\n**A1:** Thanks for pointing out the problem. Accoridngly, we have added the following explanations in our revision.\n\nThe Meta-Net in Figure 6 is a lightweight neural network built with a two-layer bottleneck structure (Linear-ReLU-Linear), with the hidden layer reducing the input dimension by 16\u00d7.\n\n>**Q2:** The motivation and model design of this paper are similar to DualCoOp. \n\n**A2:** Thank for your insightful comments and kind suggestion. Accordingly, we have added the following explanations to clarify the difference bewteen our method and DualCoOp.\n\n- The  roles of negative prompts in DualCoOp and LSN are different. In DualCoOp, negative prompts act as a threshold for positive prompts of the corresponding category, whereas in LSN, negative prompts focus on learning generic negative features across all other categories except the corresponding category. \n- In DualCoOp, positive and negative prompts are optimized simultaneously with Asymmetric Loss, which make the learning of negative logits intractable, resulting DualCoOp tend to generate false negative responses, which is consistent with the observation in DualCoOp++[1]. In contrast, in LSN, the learning of positive and negative prompts is separated, and the learned negative prompts successfully focus on different region than the positive prompts, as demonstrated in Figure2 in our paper. \n- DualCoOp mainly focuses on the **multi-label recognition (MLR)**, which is a **closed-set** problem. This needs class names of the unseen classes for testing. In contrast, LSN focuses on the **OOD detection**, which is an **open-set** problem. This does not need unseen classes.\n\nIn response to your constructive comments, we conducted the following experiments on ImageNet-100 to hilight the difference from an empirical perspective. The corresponding results and discussions have been added to the revised paper. These results show that LSN can outperform DualCoOp.\n|          | Average |       |\n|----------|---------|-------|\n| Method   |  PFR95   | AUROC |\n| DualCoOp |  23.47   | 95.64 |\n| LSN      |  19.70   | 96.22 |\n\n>**Q3:** Some existing OOD detection methods proposed in 2022 and 2023 are not compared or discussed.\n\n**A3:** Thanks for your valuable comments. Following your kind suggrestion, we conducted comparative experiments with KNN and LINe on raw CLIP model. The results on ImageNet-1k are as follows:\n\n|                     |  Average |       |\n|---------------------|---------|-------|\n| Method              |  PFR95   | AUROC |\n| KNN(k=1)            |  97.05   | 69.27 |\n| KNN(k=10)           |  97.61   | 68.38 |\n| KNN(k=100)          |  98.52   | 61.80 |\n| LINe($\\delta = 0$)    | 87.31   | 74.38 |\n| LINe($\\delta = 0.04$) | 70.46   | 85.63 |\n| LINe($\\delta = 0.07$) |  62.77   | 87.32 |\n| LSN                 |  30.22   | 92.96 |\n\nOur experimental results show that LSN outperforms these post-hoc methods on raw CLIP model. And the reason is the same as that in **A1** to **Reviewer Dyvx**, please see that.\n\nAs the mentioned DML requires re-training the model, it is challenging to report detailed results. Thus, we will add the results when finishing our experiments. We have added these discussions and outstanding works in our revision.\n\n>**Q4:** It seems that the authors have submitted this paper with an ICLR 2023 template.\n\n**A4:** Thank you very much for the kind reminder, in our resubmitted version we have fixed this error.\n\n>**Q5:** In equation(9), the positive score and the negative score are obtained independently. If the obtained positive category is different from the negative category, could this lead to mistakenly recognizing a correct positive prediction as out-of-distribution?\n\n**A5:** We appreciate your careful review and indepth question. What you describe is exactly the reason we get positive score and negative score respectively. It can indeed happen that positive and negative categories are different, especially when the number of class is large. In this case, for ID samples:\n\n$$\\max _c Softmax( \\cos \\left(\\boldsymbol{f}(\\mathbf{x}), \\boldsymbol{g}\\circ \\boldsymbol{V}(l_c) \\right)  / \\tau) - \\min _c Softmax( \\cos \\left(\\boldsymbol{f}(\\mathbf{x}), \\boldsymbol{g}\\circ \\bar{\\boldsymbol{V}}(l_c) \\right) / \\tau) > \\max _c (Softmax( \\cos \\left(\\boldsymbol{f}(\\mathbf{x}), \\boldsymbol{g}\\circ \\boldsymbol{V}(l_c) \\right)  / \\tau) -  Softmax( \\cos \\left(\\boldsymbol{f}(\\mathbf{x}), \\boldsymbol{g}\\circ \\bar{\\boldsymbol{V}}(l_c) \\right) / \\tau))$$\nGetting positive and negative scores independently can alleviate the problem you describe and this can be seen as an integration of positive classifier and negative classifier.\n\nReference:\n\n[1]: DualCoOp++: Fast and Effective Adaptation to Multi-Label Recognition with Limited Annotations. Hu et al."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1826/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700245853734,
                "cdate": 1700245853734,
                "tmdate": 1700245853734,
                "mdate": 1700245853734,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UQzQgYidxj",
                "forum": "nanyAujl6e",
                "replyto": "PI1XxpT0f2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1826/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1826/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Welcome for more discussions"
                    },
                    "comment": {
                        "value": "Dear Reviewer #ayMs,\n\nThanks very much for your time and valuable comments. \n\nHere is a **summary of our response** for your convenience:\n- (1) **Missing information issues:** We have added a description of Meta-Net in our revision.\n- (2) **Novelty issues:** DualCoOp differs from LSN in the following three points: \n\n **Negative prompts of DualCoOp and LSN serve different purposes;**  \n\n **Negative prompts of DualCoOp and LSN are learned in different ways;**\n\n **DualCoOp and LSN focus on different tasks.**\n\n And in OOD detection task, LSN outperforms DualCoOp.\n- (3) **Related work issues:** Following your constructive comments, we have discussed related works including KNN, LINe and DML.\n- (4) **Score function issues:** Inconsistencies between positive category and negative category can indeed occur. When it happens, the influence on ID data scores can be mitigated by using our scoring function.\n\nWe understand you're busy. But as the window for responding and paper revision is closing, would you mind checking our response and confirm whether you have any further questions? We are very glad to provide answers and revision to your further questions.\n\nBest regards and thanks,\n\nAuthors of #1826"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1826/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700470475701,
                "cdate": 1700470475701,
                "tmdate": 1700470475701,
                "mdate": 1700470475701,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NIbgw8OiNI",
                "forum": "nanyAujl6e",
                "replyto": "PI1XxpT0f2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1826/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1826/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Window for discussion and revision is closing"
                    },
                    "comment": {
                        "value": "Dear Reviewer #ayMs,\n\nThanks a lot for your time in reviewing and insightful comments, according to which we have carefully revised the paper to answer the questions. We sincerely understand you\u2019re busy. But since the discussion due is approaching, would you mind checking the response and revision to confirm where you have any further questions?\n\nWe are looking forward to your reply and happy to answer your further questions.\n\nBest regards\n\nAuthors of #1826"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1826/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700543581749,
                "cdate": 1700543581749,
                "tmdate": 1700543581749,
                "mdate": 1700543581749,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "plz8PP8Dos",
                "forum": "nanyAujl6e",
                "replyto": "PI1XxpT0f2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1826/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1826/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Window for discussion and revision is closing"
                    },
                    "comment": {
                        "value": "Dear Reviewer #ayMs,\n\nThanks a lot for your time in reviewing and insightful comments. We sincerely understand you're busy. Would you mind checking the response and revision to see if you have any further questions?\n\nWe are looking forward to your reply.\n\nBest regards,\n\nAuthors of #1826"
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1826/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700619947885,
                "cdate": 1700619947885,
                "tmdate": 1700619947885,
                "mdate": 1700619947885,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jxgfd34RKw",
                "forum": "nanyAujl6e",
                "replyto": "PI1XxpT0f2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1826/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1826/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Window for discussion and revision is closing"
                    },
                    "comment": {
                        "value": "Dear Reviewer #ayMs,\n\nThanks a lot for your time in reviewing and insightful comments. We sincerely understand you're busy. Would you mind checking the response and revision to see if you have any further questions?\n\nWe are looking forward to your reply.\n\nBest regards,\n\nAuthors of #1826"
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1826/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700626847783,
                "cdate": 1700626847783,
                "tmdate": 1700626847783,
                "mdate": 1700626847783,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "MjGr3pVibz",
            "forum": "nanyAujl6e",
            "replyto": "nanyAujl6e",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1826/Reviewer_ZZhi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1826/Reviewer_ZZhi"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors present a negative prompt tuning method with CLIP to improve the OOD performance. Specifically, the authors learn class-specific prompts for each category. A semantic orthogonality loss is also applied to encourage diverse negative prompts. The negative prompts are also considered in the evaluation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed LSN method achieves good performance over baselines on various OOD benchmarks.\n2. The authors provide sufficient ablation studies to show the effectiveness of each proposed component.\n3. The proposed idea is simple and easy to understand."
                },
                "weaknesses": {
                    "value": "1. This paper is related to negative learning or learning with complementary labels. The authors may consider adding some related discussion in the related work section.\n2. The proposed method may double the training and inference time with the negative prompts.\n3. I found a related work that the authors may add discussion in the related work section:\n\" How Does Fine-Tuning Impact Out-of-Distribution Detection for Vision-Language Models? , IJCV 2023.\""
                },
                "questions": {
                    "value": "1. What's the ID dataset in Table 3?\n2. Why do CoOp/CoCoOp and CoOp/CoCoOp + LSN achieve the same ID results in Table 1 and Table 2?\n3. The ID results of CoOp/CoCoOp appear to be significantly lower than other baselines such as NPOS in Table 2. Can the authors explain some reasons?\n\nSome minor suggestions that do not affect my final rating:\n1. It is suggested to use `\\citep' rather than `\\cite' in the latex code\n2. Typo: 'we use we use' at the bottom of page 6"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1826/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698664466587,
            "cdate": 1698664466587,
            "tmdate": 1699636112123,
            "mdate": 1699636112123,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HuUbKQbNlA",
                "forum": "nanyAujl6e",
                "replyto": "MjGr3pVibz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1826/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1826/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We sincerely thank you for your constructive comments! Please find our responses below.\n\n>**Q1:** This paper is related to negative learning or learning with complementary labels. The authors may consider adding some related discussion in the related work section.\n\n**A1:**  Thanks for your constructive suggestion. We have added a section of **Complementary Label Learning** to the related work in our revised version \n\n>**Q2:** The proposed method may double the training and inference time with the negative prompts.\n\n**A2:** We agree with your point. Due to the use of both positive and negative prompts, we indeed double our training and inference time. We will add the corresponding overhead analysis into our revision.\n\n>**Q3:** I found a related work that the authors may add discussion in the related work section: \" How Does Fine-Tuning Impact Out-of-Distribution Detection for Vision-Language Models? , IJCV 2023.\"\n\n**A3:** Thanks for mentioning the outstanding paper. This paper investigates the effect of fine-tuning on OOD detection in large vision-language models and the maximum concept matching (MCM) score is highlighted as effective. We have added the above discussion to the related work.\n\n>**Q4:** What's the ID dataset in Table 3?\n\n**A4:**  We apologize for the missing information. Accordingly, we have added the following explanations into our revised paper.\n\nFor the small-scale dataset, we follow the setting in ZOC[1]. For CIFAR10, 6 classes are selected as ID classes, and the 4 remaining classes are selected as OOD classes. Experiments are conducted on 5 different splits. For CIFAR+10, 4 non-animal classes from CIFAR10 are selected as ID classes, and 10 animal classes from CIFAR100 are selected as OOD classes. For CIFAR+50, 4 non-animal classes from CIFAR10 are selected as ID classes, and all 50 animal classes from CIFAR100 are selected as OOD classes. For CIFAR100, 20 consecutive classes are selected as ID classes. The remaining 80 classes are selected as OOD classes. For TinyImagenet, 20 classes are used as ID classes, and the remaining 180 classes are used as OOD classes. All the split is the same as ZOC. We give detailed split information in Table6. \n\n\n>**Q5:** Why do CoOp/CoCoOp and CoOp/CoCoOp + LSN achieve the same ID results in Table 1 and Table 2?\n\n**A5:** Thanks for pointing out this potentially confusing description.\n\nIn our experiments we notice that the classification performance of negative prompts is slightly less than that of positive prompts. We are not committed to improving ID classification performance. To avoid further trouble, we only use positive prompts when determining the classes of ID samples. We have clarified this point in our revision.\n\nThanks again for your careful review.\n\n>**Q6:** The ID results of CoOp/CoCoOp appear to be significantly lower than other baselines such as NPOS in Table 2. Can the authors explain some reasons?\n\n**A6:** We do not run NPOS on ImageNet-1k because it will consume a lot of resources. We get their experimental results directly from the original NPOS paper. We believe that the gap in ID classification performance comes from the difference in the amount of training data as well as the difference in training methods. NPOS uses the full ImgaNet-1k training set to fine-tune the CLIP model, which allows the fine-tuned model to fit better to the ImgaNet-1k test dataset. In contrast, LSN uses only 64 samples per class on ImageNet-1k and only learns prompts  without making changes to the model.\n\n>**Q7:** 1: It is suggested to use ``\\citep`` rather than ``\\cite`` in the latex code; 2: Typo: ``we use we use`` at the bottom of page 6.\n\n**A7:** Thank you very much for the kind reminder; in our resubmitted version, we have fixed these errors."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1826/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700242265549,
                "cdate": 1700242265549,
                "tmdate": 1700242265549,
                "mdate": 1700242265549,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oPTBOmMMTG",
                "forum": "nanyAujl6e",
                "replyto": "MjGr3pVibz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1826/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1826/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Welcome for more discussions"
                    },
                    "comment": {
                        "value": "Dear Reviewer #ZZhi,\n\nThanks very much for your time and valuable comments. \n\nHere is a **summary of our response** for your convenience:\n\n- (1) **Related work issues:** In response to your constructive suggestions, we have added a discussion of the work mentioned in your comments and added a section on **Complementary label learning** to the related work.\n- (2) **Training and testing time issues:** We have added a section of **Limitations** in the Appendix and discussed this issue.\n- (3) **Missing information issues:** We describe Table3 in detail and give specific split information in the Appendix.\n- (4) **ID ACC issues:** When we determine the class of ID samples, we only use positive prompts, resulting in the same ID results of CoOp/CoCoOp and CoOp/CoCoOp+LSN. And the reason NPOS achieves high ID ACC is that it uses the full ImageNet dataset to fine-tune CLIP.\n\nWe understand you're busy. But as the window for responding and paper revision is closing, would you mind checking our response and confirm whether you have any further questions? We are very glad to provide answers and revision to your further questions.\n\nBest regards and thanks,\n\nAuthors of #1826"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1826/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700470026491,
                "cdate": 1700470026491,
                "tmdate": 1700470057056,
                "mdate": 1700470057056,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EP0pzqJJcC",
                "forum": "nanyAujl6e",
                "replyto": "MjGr3pVibz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1826/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1826/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Window for discussion and revision is closing"
                    },
                    "comment": {
                        "value": "Dear Reviewer #ZZhi,\n\nThanks a lot for your time in reviewing and insightful comments, according to which we have carefully revised the paper to answer the questions. We sincerely understand you\u2019re busy. But since the discussion due is approaching, would you mind checking the response and revision to confirm where you have any further questions?\n\nWe are looking forward to your reply and happy to answer your further questions.\n\nBest regards\n\nAuthors of #1826"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1826/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700543524229,
                "cdate": 1700543524229,
                "tmdate": 1700543524229,
                "mdate": 1700543524229,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "k8iGBv0uZJ",
            "forum": "nanyAujl6e",
            "replyto": "nanyAujl6e",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1826/Reviewer_r91y"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1826/Reviewer_r91y"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors perform OOD detection based on generic features learned from a large pre-trained language-vision model by matching the similarity between image features and features of learned positive prompts and negative prompts.\nThe core innovation of this paper is the proposed LSN module to learn a set of negative prompts for each ID category to help the network to comprehend the concept of \"not.\" They mine general negative features that are not present in a category but are present in all other categories by proposing a new loss in prompt learning. In the test, the MCM scores of the cosine similarity of positive prompts and negative prompts to image features are used as OOD detection metrics. Extensive experiments on various ood detection benchmarks have been conducted to demonstrate the effectiveness of the method proposed in this work."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1\u3001This work utilizes the generic feature extraction capability of CLIP and does not need to finetune the image encoder and text encoder. It only needs to learn the appropriate positive and negative prompts by LSN for OOD detection. Therefore this method has high generality and low complexity.\n2\u3001SOTA performance is achieved in different benchmark experiments."
                },
                "weaknesses": {
                    "value": "The overall prompt learning approach is still based on CoOp without a lot of innovation."
                },
                "questions": {
                    "value": "1, This method is very dependent on the features learned by CLIP. If the features extracted by CLIP itself for some categories of images are not strongly discriminative, the effect of learning the prompts based on these features may be poor.\n2, The way to learn negative prompts is to mine the general negative features that each class of samples does not have but all other classes have, i.e., the negative classifier produces low activation values for that class and high activation values for other classes. Does this result in learning what are actually generic background features rather than general features that all other classes have?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1826/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1826/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1826/Reviewer_r91y"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1826/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698764342700,
            "cdate": 1698764342700,
            "tmdate": 1699636112043,
            "mdate": 1699636112043,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1amkhCcOkn",
                "forum": "nanyAujl6e",
                "replyto": "k8iGBv0uZJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1826/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1826/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We sincerely thank you for your constructive comments! Please find our responses below.\n\n>**Q1:** The overall prompt learning approach is still based on CoOp without a lot of innovation.\n\n**A1:** Thank you for your valuable comment. We agree with your point that LSN is built upon the success of CoOp, while we would like to provide more explanations to highlight the novel design for OOD detection. \n\nAlthough our approach to learn negative prompts looks basically the same as CoOp,  there is a fundamental difference between positive prompt learning and negative prompt learning. In positive prompt learning, using a shared prompt across classes is enough to achieve good performance. This is because in positive prompt learning, the positive features of each class are carried by the class names, and the positive prompt is only used to calibrate these features to the downstream dataset. In negative prompt learning, on the other hand, the situation is completely different. The negative features need to be contained in the learned negative prompts, and the class names appear to be less important. Diversity is the key to the success of negative prompts. We illustrate this in Table 5.\n\n>**Q2:** If the features extracted by CLIP itself for some categories of images are not strongly discriminative, the effect of learning the prompts based on these features may be poor.\n\n**A2:** We wholeheartedly agree with your observation and acknowledge that this situation is indeed inevitable in the current framework. Inspired by your insightful comments, we will explore the promising direction, like finetuning a \"degenerated\" CLIP model for promoting OOD detection. \n\n>**Q3:** The way to learn negative prompts is to mine the general negative features that each class of samples does not have but all other classes have, i.e., the negative classifier produces low activation values for that class and high activation values for other classes. Does this result in learning what are actually generic background features rather than general features that all other classes have?\n\n**A3:** We agree with your indepth point. It is indeed possible for the model to learn generic background features as negative features. However, it is noteworthy that even when background features are learned, they can at times serve as effective negative features. For example, consider the ID classes Horse, Wolf, and Pet Cat, and the OOD class Tiger. Due to similar positive features with pet cats, tigers can easily be mistaken for ID samples. But if we learn further about negative features the situation is different. Since horses and wolves are often found in grassland and pet cats are unlikely to be found in grassland, then the background grassland is likely to be learned as a negative feature for pet cats. The background of tigers is also usually grassland, and then the classifier can easily determine that tigers are not pet cats."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1826/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700241697662,
                "cdate": 1700241697662,
                "tmdate": 1700241697662,
                "mdate": 1700241697662,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ix1f0GhblO",
                "forum": "nanyAujl6e",
                "replyto": "k8iGBv0uZJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1826/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1826/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Welcome for more discussions"
                    },
                    "comment": {
                        "value": "Dear Reviewer #r91y,\n\nThanks very much for your time and valuable comments. \n\nHere is a **summary of our response** for your convenience:\n\n- (1) **Novelty issues:** Positive prompt learning is fundamentally different from negative prompt learning. Negative features need to be included in the learned negative prompts. Class names do not provide valid negative features.\n- (2) **Validity issues:** It does happen that the learned negative features are background features. However, background features are also useful for OOD detection.\n\nWe understand you're busy. But as the window for responding and paper revision is closing, would you mind checking our response and confirm whether you have any further questions? We are very glad to provide answers and revision to your further questions.\n\nBest regards and thanks,\n\nAuthors of #1826"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1826/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700469735481,
                "cdate": 1700469735481,
                "tmdate": 1700469735481,
                "mdate": 1700469735481,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rxM0uclUb1",
                "forum": "nanyAujl6e",
                "replyto": "k8iGBv0uZJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1826/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1826/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Window for discussion and revision is closing"
                    },
                    "comment": {
                        "value": "Dear Reviewer #r91y,\n\nThanks a lot for your time in reviewing and insightful comments, according to which we have carefully revised the paper to answer the questions. We sincerely understand you\u2019re busy. But since the discussion due is approaching, would you mind checking the response and revision to confirm where you have any further questions?\n\nWe are looking forward to your reply and happy to answer your further questions.\n\nBest regards\n\nAuthors of #1826"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1826/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700543465561,
                "cdate": 1700543465561,
                "tmdate": 1700543465561,
                "mdate": 1700543465561,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NE4OI9xVNH",
            "forum": "nanyAujl6e",
            "replyto": "nanyAujl6e",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1826/Reviewer_Dyvx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1826/Reviewer_Dyvx"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes how to use CLIP for OOD detection with negated prompts that include the negation word 'not'. Using the learnable prompts embeddings, the method trains the model by freezing the CLIP encoders based on a contrastive loss. The proposed method shows improvement over the baseline CLIP for OOD detection."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is clearly written with descriptive visual figures.\n2. Experiments have been extensively conducted across a set of diverse datasets including small ones and large ones."
                },
                "weaknesses": {
                    "value": "1. Comparison and related works to state-of-the-arts are missing (e.g., NNGuide [1], ASH [2], CLIPN [3], [4])\n2. The performance is too behind the state-of-the-art\n3. The main concept of the paper is too similar to CLIPN\n4. The performance improvement is very marginal\n\n[1]\n[2] \n[3] Wang, Hualiang, et al. \"Clipn for zero-shot ood detection: Teaching clip to say no.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.\n[4] Ming, Yifei, et al. \"Delving into out-of-distribution detection with vision-language representations.\" Advances in Neural Information Processing Systems 35 (2022): 35087-35102."
                },
                "questions": {
                    "value": "I suggest the authors to properly address the above weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1826/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1826/Reviewer_Dyvx",
                        "ICLR.cc/2024/Conference/Submission1826/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1826/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698835782256,
            "cdate": 1698835782256,
            "tmdate": 1700711584837,
            "mdate": 1700711584837,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FiKVwjOHmV",
                "forum": "nanyAujl6e",
                "replyto": "NE4OI9xVNH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1826/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1826/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We sincerely thank you for your constructive comments! Please find our responses below.\n\n>**Q1:** Comparison and related works to state-of-the-arts are missing. (e.g., NNGuide, ASH, CLIPN)\n\n**A1:** Thanks for your contructive comments. In accordance with your suggestion, we conduct comparison experiments with NNGuide and ASH to enrich our analysis. The results on ImageNet-1k are as follows:\n|                                | Average |       |\n|--------------------------------|---------|-------|\n| Method                         | PFR95   | AUROC |\n| NNGuide(k=1)                   | 93.02   | 80.18 |\n| NNGuide(k=10)                  | 94.46   | 78.96 |\n| NNGuide(k=100)                 | 96.38   | 75.52 |\n| ASH-B(@90) |  91.76   | 66.52 |\n| ASH-B(@65) | 85.05   | 68.17 |\n| ASH-P(@90) | 83.10  | 69.78 |\n| ASH-P(@65) | 65.11  | 79.85 |\n| ASH-S(@90) | 78.10  | 73.20 |\n| ASH-S(@65) | 65.24  | 79.25 |\n| LSN        | 30.22   | 92.96 |\n\nOur experimental results show that LSN outperforms these post-hoc methods on raw CLIP model. We believe the reason for the poor performance of these pos-hoc methods is that the **the difference in training data**. Many pos-hoc methods are designed on ImageNet pre-trained networks, where **only ID datas are used during training**. In contrast, when training CLIP, **both ID datas and OOD datas are used**. The difference in training data leads to different activation of OOD data.  Another reason is that the pos-hoc method relies heavily on the choice of hyperparameters. The hyperparameters need to be re-selected on different models.\n\nCLIPN also employes CLIP to perform OOD detection, coming up with ideas similar to ours. We compare the best results of CLIPN and LSN on ImageNet-1k with CLIP ViT-B-16 as follows:\n\n|        | iNaturalist |           | SUN       |           | Places    |           | Textures  |           | Average   |           |\n|--------|-------------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|\n| Method | PFR95       | AUROC     | PFR95     | AUROC     | PFR95     | AUROC     | PFR95     | AUROC     | PFR95     | AUROC     |\n| CLIPN  | 23.94       | 95.27     | **26.17** | 93.93     | **33.45** | **92.28** | 40.83     | **90.93** | 31.10     | **93.10** |\n| LSN    | **21.56**   | **95.83** | 26.32     | **94.35** | 34.48     | 91.25     | **38.54** | 90.42     | **30.22** | 92.96     |\n\n- Experimental results show that LSN achieves comparable performance with CLIPN. \n- We have to point out that **this is not a fair comparison**. This is because, to endow OOD detector with the ability to say \"no\", CLIPN leverages a large-scale dataset [1] (containing 3.3 million image-text pairs) to finetune the text encoder.\n- These results highlight the contribution of the proposed approach to learn negative prompts. Specifically, LSN only needs to use few shot samples to learn negative prompts, which can be done in a very short time.\n\nInspired by your constructive comments, we summarize the differences between LSN and CLIPN as follows.\n\n- LSN only requires learning negative prompts at a small computational cost without training \"no\" text encoder and achieves performance comparable with CLIPN.\n- LSN points out that the diversity of negative prompts is the key to success, and the use of class-specific negative prompts leads to a reduction in FPR95 from 81.27% to 21.94% compared to the use of class-shared negative prompts, please see Table 5.\n- LSN focuses on prompt learning and points out how negative prompt learning differs from positive prompt learning, while CLIPN focuses more on learning \"no\" text encoder.\n\nWe will add the above results and discussion into our revision to highlight the difference bewteen our work and the mentioned work. Thanks again for mentioning these outstanding papers.\n\n>**Q2:** The performance is too behind the state-of-the-art.\n\n**A2:** Thanks for your valuable comments. Accordingly, we have added the following explanations to our revision.\n\n- We are focused on improving the OOD detection performance of pre-trained vision-language models such as CLIP. As can be seen in **A1**, our method can outperform NNguide and ASH, the recent SOTA pos-hoc methods. \n- Moreover, when compared to CLIPN, our method achieves comparable performance without the need for large-scale datasets and significantly computation overhead. \n\n\n>**Q3:** The performance improvement is very marginal.\n\n**A3:** As mentioned above, we get the comparable performance at a much smaller cost compared to CLIPN. And compared to standard prompt learning CoOp and CoCoOp, LSN's FPR95 on ImageNet1k decreased by 6.86% and 8.41%, respectively. \n\nReference:\n[1] Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. Sharma et al. ACL2018."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1826/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700241452777,
                "cdate": 1700241452777,
                "tmdate": 1700241452777,
                "mdate": 1700241452777,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "x5oQKeMzpL",
                "forum": "nanyAujl6e",
                "replyto": "FiKVwjOHmV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1826/Reviewer_Dyvx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1826/Reviewer_Dyvx"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your responses"
                    },
                    "comment": {
                        "value": "I sincerely appreciate the detailed responses from the authors. I am mostly satisfied with the responses, and now I understand at which points the proposed LSN is superior to CLIPN. But I am still not sure if the overall contribution is sufficient enough.\n\nThe paper's main contribution is the demonstration that the prompt tuning works for OOD detection. But I am worried under the following the aspects:\n\n[Practical usefulness]:\n1. Although the method outperforms other OOD detectors using CLIP encoder on ImageNet-1k, the average performance of FPR95 30% seems not satisfactory; most of the recent methods achieve below 25% of FPR95 [5] simply using light-weight ResNet-50.\n2. The method is restricted to CLIP-type encoders. Hence it may be difficult to apply the method on edge device applications.\n\n[Analytical Profoundness]\n3. I appreciate the descriptive explanation given in Fig. 4; they are insightful to me. But the experiment scope is limited with respect to the encoder variation. Would this method also be effective to other CLIP-type encoders such as BLIP?\n\n[Novelty]\n4. The idea of fine-tuning is not really new. The method itself is not really impactful although I really appreciate that the method is well presented in an organized manner.\n5. During the review, I also found [6], which is also an extension of CoOp. The contribution of LSM seems to largely overlap with [6].\n\nDue to the above reasons, I hesitate to increase the score.\n\n[5] LHAct: Rectifying Extremely Low and High Activations for Out-of-Distribution Detection, ACMMM2023\n[6] LoCoOp: Few-Shot Out-of-Distribution Detection via Prompt Learning, NeurIPS2023"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1826/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700247736096,
                "cdate": 1700247736096,
                "tmdate": 1700247736096,
                "mdate": 1700247736096,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "y4jBLWKBlc",
                "forum": "nanyAujl6e",
                "replyto": "ruPkdoS6U1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1826/Reviewer_Dyvx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1826/Reviewer_Dyvx"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the response"
                    },
                    "comment": {
                        "value": "Thank you for the response. I truely appreciate the authors' prompt responses. But honestly, the concerns still remain.\n\nQ1. I understand CoOp variants outperform conventional OOD detectors in the few-shot setting. But this is obvious because CLIP has been pretrained on a very large-scale dataset. Is LSN then scalable such that if it is fine-tuned by full images of IN-1k, then can it achieve below FPR95 25%?\n\nQ3. The score of limited achirecture types is concerning. This may implicate the proposed method can work only for out-dated underperforming VLM such as CLIP. Would LSN be effective for BLIP2 as well?\n\nQ5. The reason behind the superiority of LSN over LoCoOp is not clear. LSN uses more parameters, and its mining mechanism adds extra cost. Is LSN better than LoCoOp really bacause LSN is focused on negative prompt learning? Why then is negative prompt learning better than positive prompt learning?\n\nI understand the conceptual orthogonality to LoCoOp. But would LSN always imporve LoCoOp (e.g., IN-1k)?\n\n---\nOverall, the contribution of the paper is there, but the advancement is marginal in terms of novelty and performance. In my perspective, the paper is a borderliner one."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1826/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700546328242,
                "cdate": 1700546328242,
                "tmdate": 1700546328242,
                "mdate": 1700546328242,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lm6aewzG68",
                "forum": "nanyAujl6e",
                "replyto": "ZUofFvFUgg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1826/Reviewer_Dyvx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1826/Reviewer_Dyvx"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the response"
                    },
                    "comment": {
                        "value": "Thank you for the prompt response again.\n\nQ1. By 'fine-tuning' I mean fine-tuning of the learnable input prompt weights (i.e., 'prompt learning' by LSN). I assume that the set $S$ in the subscript of the expectation in Eqs. (4) and (5) are your training set for prompt learning. As far as I understand, based on your responses comments, this set is not the full ImageNet-1k training set. If you increase the size of $S$ to the full train fold of IN-1k, then would it correspondingly increase the performance, and achieve the SOTA?\n(By the way, the notations are quite confusing. $S$ is elsewhere defined as the score function while in the Eqns (4) and (5) it seems not the case.)\n\nOne major concern with CLIP-based OOD detection methods is that they only show few-shot results without showing results that utilize full training set. Hence, they lack to show scalability with respect to the train data size.\n\nQ3. Hopefully, the LSN works on BLIP2 as well; I am expecting the result.\n\nQ5. Thank you for the authors' reasonable and convincing points.\n\nOverall, my remaining concerns are only on Q1 and Q3."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1826/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700576658994,
                "cdate": 1700576658994,
                "tmdate": 1700576658994,
                "mdate": 1700576658994,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "97ak8cHCRD",
                "forum": "nanyAujl6e",
                "replyto": "NE4OI9xVNH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1826/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1826/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your prompt responses! Please find our responses below.\n\n>**Q1:** By 'fine-tuning' I mean fine-tuning of the learnable input prompt weights (i.e., 'prompt learning' by LSN). I assume that the set $S$ in the subscript of the expectation in Eqs. (4) and (5) are your training set for prompt learning. As far as I understand, based on your responses comments, this set is not the full ImageNet-1k training set. If you increase the size of $S$ to the full train fold of IN-1k, then would it correspondingly increase the performance, and achieve the SOTA? (By the way, the notations are quite confusing. $S$ is elsewhere defined as the score function while in the Eqns (4) and (5) it seems not the case.)\n\n**A3:** Thank you for the heads up, we have fixed this error in the revised version. We are afraid that even using the full ImageNet-1k dataset we could not achieve the SOTA you mentioned. We think the main reason is that there are too few parameters that can be learnt in prompt learning compared to fine-tuning networks, and further increasing the number of training samples will not lead to performance improvement. We don't use the full ImageNet-1k dataset to learn the prompt due to the fact that it takes too long even if only the prompts are tuned. We estimate that the program takes 15 hours to run one epoch (batchsize is set to 8), which is intolerable. We provide the effect of different taining sample sizes on the OOD detection performance on ImageNet-100 dataset in the Appendix. You can refer to that. It can be seen that when the sample size is large, the performance gain from further increasing the sample size is not significant.\n\n>**Q3:** Hopefully, the LSN works on BLIP2 as well; I am expecting the result.\n\n**A3:** We apologize for the problems we have when we are about to validate LSN on BLIP2. The model structure of BLIP2 consists of three types: blip2, blip2_t5 and blip2_opt (Please see details here: [BLIP2](https://github.com/salesforce/LAVIS/tree/main/projects/blip2)). When we use the smallest model blip2, we find that the model still can not understand the meaning of \"not\". Only when using the larger model blip2_t5 the model is able to understand the meaning of \"not\". However, blip2_t5 is only used in image-to-text generation task and caption generation task. We fail to find a suitable prompt that allows BLIP2 on these tasks to generate a suitable score for testing the model's OOD detection ability.  Therefore, we are still only able to use smaller model to perform image-text matching task to get cosine similarity between images and text as the OOD scoring function. In this case, we need to learn negative prompts through prompts learning. To this end, we test the performance of LSN on ImageNet-100 with BLIP. The result is as follows:\n\n|              | iNaturalist |       | SUN   |       | Places |       | Textures |       | Average |       |\n|--------------|-------------|-------|-------|-------|--------|-------|----------|-------|---------|-------|\n| Method       | FPR95       | AUROC | FPR95 | AUROC | FPR95  | AUROC | FPR95    | AUROC | FPR95   | AUROC |\n| MCM       | 46.45       | 93.37 | 78.48 | 81.76 | 77.14  | 81.05 | 48.10    |  90.92 | 62.54   | 86.77 |\n| CoOp    |   27.83     | 95.26 | 45.85 | 88.26 |  47.38 | 87.42 |   36.28  | 92.93 |  39.33  | 90.96 |\n| CoOp + LSN |    16.33   | 96.73 | 39.48 | 90.94 |  41.76 | 90.42 |  31.59   | 94.27 |   32.29 | 93.09 |"
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1826/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700594000737,
                "cdate": 1700594000737,
                "tmdate": 1700597256116,
                "mdate": 1700597256116,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sj0R2nunrG",
                "forum": "nanyAujl6e",
                "replyto": "O43w2iA3av",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1826/Reviewer_Dyvx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1826/Reviewer_Dyvx"
                ],
                "content": {
                    "title": {
                        "value": "Q1 has not been addressed"
                    },
                    "comment": {
                        "value": "Dear Authors\n\nQ1 has not been addressed fully.\n\nQ3 is unfortunate, but this is an issue that I did not address in the first review, so I would not base my decision on it. But if this experiment had been successful, this would have been a main plus. The current result is unsatisfactory as the performance on blip is not good in the absolute sense.\n\nMy remaining concern is only on Q1. But I will incrase the score. I expect the authors to make in detail their experimental findings. I believe showing the limitation is also a contribution."
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1826/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700711560592,
                "cdate": 1700711560592,
                "tmdate": 1700711560592,
                "mdate": 1700711560592,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "D2Q9Vj0xaQ",
                "forum": "nanyAujl6e",
                "replyto": "NE4OI9xVNH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1826/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1826/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your swift reply and raising the score"
                    },
                    "comment": {
                        "value": "Dear reviewer #Dyvx,\n\nThanks for your swift reply despite such a busy period. We sincerely appreciate that you can raise the score. For the experiments on the BLIP, we believe that the results above are not the best results as we don't explore much due to time constraints. We will do more experiments on BLIP and present the results in the paper. As for Q1, it may be a fact at this stage that when the ID dataset is ImageNet-1k, the OOD detection performance of prompt learning on CLIP may not be as good as the performance of ImageNet-1k pre-trained models. For now, this is indeed a shortcoming of such methods based on CLIP models. We will make this clear in our paper.\n\nBest regards and thanks,\n\nAuthors of #1826"
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1826/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700714395343,
                "cdate": 1700714395343,
                "tmdate": 1700714432335,
                "mdate": 1700714432335,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]