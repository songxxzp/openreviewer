[
    {
        "title": "Identifying Representations for Intervention Extrapolation"
    },
    {
        "review": {
            "id": "kiTFzv2pN7",
            "forum": "3cuJwmPxXj",
            "replyto": "3cuJwmPxXj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7315/Reviewer_wfVE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7315/Reviewer_wfVE"
            ],
            "content": {
                "summary": {
                    "value": "This work introduces a type of interventional extrapolation which consists in predicting the effect of an unseen intervention. The causal model considered is given by A -> Z -> X and Z->Y where Z is latent, A -> Z is linear, Z->X is invertible and Z -> Y is an additive noise model. The goal is then to estimate E[Y | do(A = a*)] where a* is outside its training domain. The authors separate this problem in three subproblems: 1) estimation given that Z is observed, 2) estimation this given that Z is known only up to an affine transformation, and 3) how to identify the invertible map Z->X up to an affine transformation. These identifiability results are then transferred into a practical algorithm: a) The mixing function is recovered by performing SGD on a loss consisting of a reconstruction term and a regularizer based on maximum moment restriction (MMR), and b) the learned encoder is then used to estimate the desired \u201cdo\u201d expectation following the procedure of step 2) above.  The proposed estimation procedure is then validated on a synthetic dataset.\n\nReview Summary: I gave the main text a thorough reading and checked the large majority of the math it presents. Despite the few problems I raised in my review, I believe this work is sufficiently novel, interesting, rigorous and well written to warrant acceptance to ICLR. I was planning to give a score of 7, since this score is not available, I'm rounding up to 8. I'm giving this score assuming that a discussion of the limitations will be added, as I suggest in my review."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- I agree that we need more theory to demonstrate concrete benefits of identifiable representation learning. I enjoyed this perspective.\n- The paper is very clearly written and structured and relatively easy to follow despite its technicality. The level of mathematical rigor in this work is very high in my opinion and the notation is always very crisp and transparent. \n- I thought the theoretical contributions were novel and interesting. \n- I appreciated that the main ideas about the proofs were provided in the main text. Most works in this literature only give very short proof sketches if any. \n- Theoretical results are well modularized to facilitate reuse.\n- I thought the method to learn the encoder up to affine transformation was novel and interesting.\n- I\u2019m left with a good feeling of having learned something new."
                },
                "weaknesses": {
                    "value": "- The limitations of the proposed approach could be discussed further. For example, how does the method perform under various assumption violations? A few experiments could provide some insight into this. \n- The paper ends a bit abruptly. There\u2019s no conclusion nor discussion of limitations. I guess this is the cost of having so much technical details in the main text (which, as I said, I appreciated, but I\u2019m unsure whether this is a good balance.)\n- I think the paper could do a better job of contrasting its results with what already appears in the literature, especially regarding the affine identifiability results. How does this compare to iVAE for example, which has a very similar data generating process with an auxiliary variable (which would be A here)? A few papers have similar results: [1,2,3,4].\n- I couldn\u2019t grasp a few reasoning steps in the main text (see Questions below).\n\nMinor:\n- $M_0$ has full row rank implies that dim(A) >=  dim(Z). Might be worth making explicit.\n- I always forget which noise variable U or V is associated with which variable. How about replacing V by V_z and U by V_y?  \n- Footnote one: what is $\\mathcal{B}$?\n- The introduction rightfully points out that the literature has almost no works giving theoretical arguments for why the identifiable representation learning is important. You might want to consider citing [1, 5] as examples of work pushing that direction.\n\n[1] S. Lachapelle, T. Deleu, D. Mahajan, I. Mitliagkas, Y. Bengio, S. Lacoste-Julien, and Q. Bertrand. Synergies between disentanglement and sparsity: a multi-task learning perspective, 2022. \n\n[2] I. Khemakhem, D. Kingma, R. Monti, and A. Hyvarinen. Variational autoencoders and nonlinear \u00a8 ica: A unifying framework. In Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics, 2020.\n\n[3] Ahuja, K., Mahajan, D., Syrgkanis, V., and Mitliagkas, I. Towards efficient representation identification in supervised learning. In First Conference on Causal Learning and Reasoning, 2022\n\n[4] Roeder, G., Metz, L., and Kingma, D. P. On linear identifiability of learned representations. In Proceedings of the 38th International Conference on Machine Learning, 2021.\n\n[5] S. Lachapelle, D. Mahajan, I. Mitliagkas, and S. Lacoste-Julien. Additive decoders for latent variables identification and cartesian-product extrapolation. In Advances in Neural Information Processing Systems, 2023b."
                },
                "questions": {
                    "value": "- Lemma 7 in Appendix A: Could you explain the argument a bit more? I\u2019m unaware of this proof technique, so citing the result used here would be useful. Or is it an alternative definition of conditional independence that I\u2019m not aware of?\n- Paragraph after (3):\n    - Could you give some insight as to why $\\ell$ and $\\lambda$ are not identifiable up to additive constant without further assumptions in (3)? I\u2019m not sure I see what could go wrong since we observe Y, Z and V here (V is identifiable). Also, can you state the assumption from Newey et al. (1999) that allows you to identify $\\ell$? I suspect it has something to do with the differentiability + supp(A) convex assumption from Theorem 4?\n    - It is written \u201cfor all $a^* \\in M_0 supp(A) + supp(V)$\u201d somewhere, but it seems wrong since $M_0 supp(A) + supp(V)$ is an event for Z, no? I\u2019m not sure I\u2019m following this part of the argument and onward.\n    - Same paragraph: I think we need to compute $\\ell(z)$ only for $z \\in M_0\\mathcal{A} + supp(V)$, no? \n- Eq. (16): I\u2019m not sure why this holds. Is it because the inside of the conditional expectation is equal to V? But we don\u2019t assume E[V] = 0 right? What am I missing? Since this is so crucial to the algorithm, it might be worth expliciting further."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7315/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697828486778,
            "cdate": 1697828486778,
            "tmdate": 1699636874383,
            "mdate": 1699636874383,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dXCKWhFtHu",
                "forum": "3cuJwmPxXj",
                "replyto": "kiTFzv2pN7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7315/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7315/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer wfVE"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the positive and detailed feedback.\n\nWe address the points raised in the 'Weaknesses' and 'Questions' sections below.\n\n1. > The limitations of the proposed approach could be discussed further. For example, how does the method perform under various assumption violations? A few experiments could provide some insight into this.\n\n    We have now added Remark 6 in Appendix B which highlights the key assumptions from Setting 1 and discusses whether they can be extended (and have referenced this appendix after Setting 1). Additionally, we have conducted an additional experiment in which we introduced random noise into $X$. The results indicate that our method is robust to the presence of noise in $X$ (when the noise is not too large). The results can now be found in Appendix H.3. We have also noted in the conclusion that future work could extend our theoretical results to cover this case.\n\n2. > The paper ends a bit abruptly. There\u2019s no conclusion nor discussion of limitations. I guess this is the cost of having so much technical details in the main text (which, as I said, I appreciated, but I\u2019m unsure whether this is a good balance.)\n\n    Thank you for your suggestion. We have now included a discussion section that summarizes the main focuses of our work and discusses potential extensions of our theoretical results.\n\n3. > I think the paper could do a better job of contrasting its results with what already appears in the literature, especially regarding the affine identifiability results. How does this compare to iVAE for example, which has a very similar data generating process with an auxiliary variable (which would be A here)? A few papers have similar results: [1,2,3,4].\n    \n    We agree that our actions $A$ could be considered auxiliary variables. However, the work by Khemakhem et al., 2020 differs from ours in two important ways. \n    * Assumptions: one of the key assumptions in their setting is that the density $p(z|a)$ is conditionally factorized, meaning that the components of $Z$ are independent when conditioned on $A$. In contrast, our approach permits dependence among the components of $Z$ even when conditioned on $A$ (because in our setting, the components of $V$ can have arbitrary dependencies). \n    * Type of identifiability: maybe even more importantly, Khemakhem et al. provide identifiability up to point-wise nonlinearities which is not sufficient for intervention extrapolation. The main focus of our work is to provide an identification that facilitates a solution to the task of intervention extrapolation.\n\n    We have now included a discussion on the relationship to the nonlinear ICA literature in Appendix A and have referenced this appendix in the related work section.\n\n4. > $M_0$ has full row rank implies that dim(A) >= dim(Z). Might be worth making explicit.\n\n    Thank you for the suggestion. We have now added this comment in Setting 1.\n\n5. > I always forget which noise variable U or V is associated with which variable. How about replacing V by V_z and U by V_y?\n    \n    We have decided to retain our current notation, as introducing a subscript might lead to additional notational complexities (for instance, we already use $V_{\\theta}$). We recall our notation using \u201c'U' precedes 'V' and 'Y' precedes 'Z'\u201d.\n\n6. > Footnote one: what is $\\mathcal{B}$\n\n    We have now clarified Footnote 1 and replaced the letter $\\mathcal{B}$ by $\\Omega$ since $\\mathcal{B}$ has already been used in Proposition 1. \n\n7. > The introduction rightfully points out that the literature has almost no works giving theoretical arguments for why the identifiable representation learning is important. You might want to consider citing [1, 5] as examples of work pushing that direction.\n\n    Thank you for the pointer to these references; we have now included them in the introduction. \n8. > Lemma 7 in Appendix A: Could you explain the argument a bit more? I\u2019m unaware of this proof technique, so citing the result used here would be useful. Or is it an alternative definition of conditional independence that I\u2019m not aware of?\n\n    This is an alternative way of defining conditional independence (in the absence of densities). We have now clarified the proof and have added a citation to a paper by Constantinou & Dawid, 2017 that proves the corresponding equivalence \u2013 which allowed us to simplify our proof."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7315/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700509302840,
                "cdate": 1700509302840,
                "tmdate": 1700515602975,
                "mdate": 1700515602975,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LUvLtvALie",
                "forum": "3cuJwmPxXj",
                "replyto": "kiTFzv2pN7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7315/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7315/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer wfVE (continued)"
                    },
                    "comment": {
                        "value": "9. > Could you give some insight as to why $\\ell$ and $\\lambda$ are not identifiable up to additive constant without further assumptions in (3)? I\u2019m not sure I see what could go wrong since we observe Y, Z and V here (V is identifiable). Also, can you state the assumption from Newey et al. (1999) that allows you to identify $\\ell$? I suspect it has something to do with the differentiability + supp(A) convex assumption from Theorem 4?\n\n   A simple counter example is when $\\ell$ and $\\lambda$ are linear functions, i.e., $\\mathbb{E}[Y | Z, V] = \\beta_1 Z + \\beta_2 V$ and $Z$ and $V$ have a perfect linear relationship. This scenario leads to perfect multicollinearity in linear regression, which results in the non-identifiability of $\\beta_1$ and $\\beta_2$. We therefore require additional assumptions that $\\ell$ and $\\lambda$ are differentiable and the interior of $\\text{supp}(A)$ is convex.\n\n10. > It is written \u201cfor all $a^* \\in M_0supp(A) + supp(V)$\" somewhere, but it seems wrong ...\n    \n    Many thanks for spotting this typo. We have now corrected it and write for all $a^*$ such that $M_0a^* + \\text{supp}(V) \\subseteq M_0\\text{supp}(A) + \\text{supp}(V)$.\n\n11. > Same paragraph: I think we need to compute $\\ell(z)$ only for $z \\in M_0 \\mathcal{A} + supp(V)$\n\n    You are right, it is more accurate to write $z \\in M_0 \\mathcal{A} + \\text{supp}(V)$. We have now simplified and clarified this paragraph.\n\n12. > Eq. (16): I\u2019m not sure why this holds. Is it because the inside of the conditional expectation is equal to V? But we don\u2019t assume E[V] = 0 right? What am I missing? Since this is so crucial to the algorithm, it might be worth expliciting further.\n\n    The residual has zero mean since we have an intercept term $\\alpha_{g^{-1}_0}$ that cancels out the mean of $V$."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7315/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700509774971,
                "cdate": 1700509774971,
                "tmdate": 1700509774971,
                "mdate": 1700509774971,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2jv8nVJ8Kl",
                "forum": "3cuJwmPxXj",
                "replyto": "LUvLtvALie",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7315/Reviewer_wfVE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7315/Reviewer_wfVE"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your answer."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7315/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700619895009,
                "cdate": 1700619895009,
                "tmdate": 1700619895009,
                "mdate": 1700619895009,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "UwIWJzEqfK",
            "forum": "3cuJwmPxXj",
            "replyto": "3cuJwmPxXj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7315/Reviewer_dGv9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7315/Reviewer_dGv9"
            ],
            "content": {
                "summary": {
                    "value": "The paper discusses identification strategies for effect extrapolation when the treatment of interest is unobserved during the experiment. The key assumptions are that \n\n- (exogenous) treatment A affects outcome Y through and only through an unobserved mediator Z\n- can observe a rich feature $X=g_0(Z)$ that is and only is a (injective) function of Z\n- the relationship between A and Z is linear\n\nTogether with some other regularity assumptions, the author shows that, given an encoder that aff-identifies $g_0^{-1}$, it is possible to identify $E[Y|do(A=a^*)]$ and $E[Y|X=x,do(A=a^*)]$ for a treatment $a^*$ that has never been observed. They also propose a method for locating such an encoder."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper studies intervention extrapolation under a well-chosen set of assumptions, which I find more appealing than the previously studied scenarios in the literature.\n\n- To the best of my knowledge, the proposed identification strategy is novel.\n\n- The manuscript is clear and well-written. \n\n- The proposed algorithm is straightforward and practical."
                },
                "weaknesses": {
                    "value": "- The assumptions are clear mathematically but might seem opaque to readers unfamiliar with the literature. The authors may want to give an example of what some of the key assumptions would imply in a simple setup.\n\n- Many of the structural assumptions are not testable, and it is unclear to me when one shall be comfortable using the proposed method.\n\nAlso see questions."
                },
                "questions": {
                    "value": "- It seems that this approach relies heavily on the linear structure between A and Z. Can this structural model be extended to $M_0 t(A) + V$ for some transformation $t(A)$ (e.g., $A^2$)? What if the relationship between A and Z is not linear, but rather through a known class of models $m_{\\theta}(A)+V$ with parameters $\\theta$? How would this affect the result?\n\n- Is it possible to view the linear assumption as an approximation through a Taylor expansion around supp(A)? How would this compare with an extrapolation that is solely based on Lipschitz assumptions (see, e.g., Ben-Michael et al. 2021)?\n\n- Although this paper is only about identification, I am curious about how the errors would accumulate.\n\n- I find the sudden change of notation in Theorem 4 confusing.\n\n- Can there be randomness in X, i.e., can X be a noisy observation of $g_0(Z)$?\n\n- On page 2, the authors wrote \"we allow for potentially unobserved confounders between Y and Z\". How could there be confounding when the action is exogenous?\n\n- Although the goal is very different, the approach reminds me of the negative control literature. The authors may want to discuss the connection."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7315/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7315/Reviewer_dGv9",
                        "ICLR.cc/2024/Conference/Submission7315/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7315/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698739611720,
            "cdate": 1698739611720,
            "tmdate": 1700658846876,
            "mdate": 1700658846876,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "69EyIpp7XF",
                "forum": "3cuJwmPxXj",
                "replyto": "UwIWJzEqfK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7315/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7315/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer dGv9"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the positive feedback.\n\nWe address the points raised in the 'Weaknesses' and 'Questions' sections below.\n\n1. > The assumptions are clear mathematically but might seem opaque to readers unfamiliar with the literature. The authors may want to give an example of what some of the key assumptions would imply in a simple setup.\n\n    Thanks for the suggestion. We have now added Remark 6 in Appendix B which highlights the key assumptions from Setting 1 and discusses whether they can be extended. \n\n2. > Many of the structural assumptions are not testable, and it is unclear to me when one shall be comfortable using the proposed method.\n\n    Due to the nature of extrapolation problems, it is not feasible to definitively verify the method's underlying assumptions from the training data. However, we may still be able to check and potentially falsify the applicability of our approach in practice. To this end, we propose comparing its performance under two different cross-validation schemes:\n    * Standard cross-validation, where the data is randomly divided into training and test sets.\n    * Extrapolation-aware cross-validation, in which the data is split such that the support of $A$ in the test set does not overlap with that in the training set. \n\n    By comparing our method's performance across these two schemes, we can assess the applicability of our overall method. If the standard cross-validation yields a much better score, this may suggest that some key assumptions are not valid and one should consider adapting the setting, e.g., by transforming $A$ (see Remark 6 in Appendix B). \n\n    ##\n    A further option of checking for potential model violations is to test for linear invariance of the fitted encoder, using for example the conditional moment test by Muandet et al. (2020). If the null hypothesis of linear invariance is rejected, this indicates that either the optimization was unsuccessful or the model is incorrectly specified.\n\n    ##\n    We have now added the above discussion in Appendix F.\n\n3. > It seems that this approach relies heavily on the linear structure between A and Z. Can this structural model be extended to ...\n\n    We have now added Remark 6 in Appendix B (and have referenced this appendix after Setting 1) to discuss key assumptions in Setting 1 and whether they can be extended. Some extensions are straightforward; for example, one can relax the linearity assumption from $A$ to $Z$ if there is a known nonlinear transformation $h$ such that $Z := M_0 h(\\tilde A) + V$; in this case, we use $A := h(\\tilde A)$ and obtain a model that lies in our model class.  Furthermore, our setting accommodates any invertible nonlinear transformation on $Z$, too. To see this, if there is a nonlinear injective function $h$ such that $\\tilde{Z} := h(M_0 A + V)$ and $X := g_0(\\tilde{Z})$, we can define $Z := M_0 A + V$ and $X := (g_0 \\circ h)(Z)$ and obtain a model in our model class. But key to our approach is that the model of $Z$ on $A$ has the ability to extrapolate, otherwise non-linear extrapolation for $\\mathbb{E}[Y|\\text{do}(A:=a^*)]$ is not possible.\n\n4. > Is it possible to view the linear assumption as an approximation through a Taylor expansion around supp(A)? How would this compare with an extrapolation that is solely based on Lipschitz assumptions (see, e.g., Ben-Michael et al. 2021)?\n    \n    In our work, the extrapolation stems from the fact that the support of $V$ expands the support of $Z$ beyond $M_0\\text{supp}(A)$. Together with the linearity assumption from $Z$ on $A$ (which can be composed with a known nonlinearity, as discussed in Remark 6 in Appendix B), one can then decompose the effects from $V$ and $A$ and hence extrapolate. In our view, this type of extrapolation is different from extrapolation based on Taylor expansion or smoothness constraints, as these approaches use regularity of the nonlinear functions, while our approach relies on the support extension of $V$ to make the nonlinearity identifiable. We have now added a comment on this right before Section 3.\n\n5. > Although this paper is only about identification, I am curious about how the errors would accumulate.\n\n    Appendix E.1 of the original submission (now: Appendix H.1) presents the results of an experiment comparing our method with an oracle approach that uses the true latent variable $Z$ directly instead of learning a representation in the first stage. This comparison indicates that for lower-dimensional $A$, the error introduced by estimating the true latent is relatively mild, while this error becomes more pronounced when the dimension increases. We have now also included the oracle baseline in Figure 3 to visually demonstrate that the discrepancy is small in the one-dimensional case."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7315/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700508000593,
                "cdate": 1700508000593,
                "tmdate": 1700515995904,
                "mdate": 1700515995904,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "koR10TefF5",
                "forum": "3cuJwmPxXj",
                "replyto": "UwIWJzEqfK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7315/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7315/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer dGv9 (continued)"
                    },
                    "comment": {
                        "value": "6. > I find the sudden change of notation in Theorem 4 confusing.\n\n    We discussed this point at length among ourselves before submission. In the end, we concluded that highlighting the dependence on the observed distribution was worth the additional notational overhead. We have however now added \u201c(e.g., (11) instead of (8))\u201d in Footnote 3 to be more concrete about this.\n\n7. > Can there be randomness in X, i.e., can X be a noisy observation\n\n    We have conducted an additional experiment in which we introduce random noise into $X$. The results indicate that our method is robust to the presence of noise in $X$ (when the noise is not too large). The results can now be found in Appendix H.3. Additionally, we have noted in the conclusion that future work could extend our theoretical results to cover this case.\n\n8. > On page 2, the authors wrote \"we allow for potentially unobserved confounders between Y and Z\". How could there be confounding when the action is exogenous?\n\n    We allow for confounding between the latent $Z$ and the response $Y$ in Setting 1 because we do not assume that $U$ and $V$ are independent.\n\n9. > Although the goal is very different, the approach reminds me of the negative control literature. The authors may want to discuss the connection.\n\n    We do not see a clear connection between the negative control literature and our work but we are happy to hear about any concrete connections you may have in mind."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7315/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700508197159,
                "cdate": 1700508197159,
                "tmdate": 1700517031860,
                "mdate": 1700517031860,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "jSbH3o3hzP",
            "forum": "3cuJwmPxXj",
            "replyto": "3cuJwmPxXj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7315/Reviewer_k2zA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7315/Reviewer_k2zA"
            ],
            "content": {
                "summary": {
                    "value": "**Post rebuttal update**: I'd really like to see a couple of real-world examples that might satisfy the assumptions, but my other concerns are largely addressed. I raise my score to 8 accordingly.\n\nThe paper considers the causal effect of an intervention $A=a^*$, where value $a^*$ is not observed in the data; A affects the outcome Y through unobserved variable Z, and covariates X relates to Z through an injective function g. The paper uses a conditional moment restriction (CMR) implemented by a kernel method called maximum moment restriction (MMR), and it then uses the control function (CF) approach to identify the outcome function between Z and Y. Both the CMR and CF depend on the assumption that A affects Z linearly. The approach is theoretically guaranteed, and experiments on synthetic datasets support the theoretical analysis."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Using CF to achieve intervention extrapolation is a nice idea.\n\nThe theoretical analysis is serious and detailed (but I did not check the proofs in Appendix).\n\nThe paper is quite well written."
                },
                "weaknesses": {
                    "value": "*Technical novelties seem to be weak*. Theorem 4 seems to be an adaptation of the CF approach in (Newey et al., 1999), and Theorem 6 seems to be an adaptation of the IV approach in (D\u2019Haultfoeuille, 2011). If there are some technical novelties, they should be discussed and compared to the original works; otherwise, I suggest being more explicit about this weakness.\n\n*Some assumptions are strong*; particularly, the linear model between Z and A, and the injective model and noiseless model between Z and X. Intuitively, X is an observable proxy of the hidden Z, and the assumption means there is no information loss in this proxy, which is strong. Moreover,  both assumptions involve hidden variable Z and add difficulty to practical judgments. Since both assumptions are inherent in the current approach, I do not expect the author(s) to address this weakness in the rebuttal, but the following could certainly be done.\n\n*Discussion of the setting and comparison to related work*. The discussion of the relationship to reinforcement learning is interesting but does not touch on when can we possibly expect linearity and noiseless injectivity. It would be more interesting to draw and discuss a couple of real-world problems that might satisfy the assumptions. \n\nOn the other hand, (Khemakhem et al., 2020) and [1, 2], which are based on the former, are important related work that needs more discussions, and this would clarify the current approach.  For example, (Khemakhem et al., 2020) recover Z based on exactly the same graph as A\u2192Z\u2192X, and also assume g is injective but *allows an additive noise on X*; the identification also relies on assumptions on the A\u2192Z part, where p(Z|A) is assumed to be an exponential family distribution but *allows nonlinearity*. Overall, I do not think the assumptions in (Khemakhem et al., 2020) are clearly stronger than those in the current work. Further, [1, 2] uses (Khemakhem et al., 2020) to estimate treatment effects, though not considering intervention extrapolation, [2] mentioned the ideas of CMR and CF in Sec 4.4.\n\n[1] Wu, Pengzhou Abel, and Kenji Fukumizu. \"$\\beta $-Intact-VAE: Identifying and Estimating Causal Effects under Limited Overlap.\" International Conference on Learning Representations. 2022.\n[2] Wu, Pengzhou and Kenji Fukumizu. Towards Principled Causal Effect Estimation by Deep Identifiable Models\u201d. In: arXiv preprint arXiv:2109.15062 2021\n\n*Additional experiments could be added*.\n\nI think the ability to deal with unobserved confounders is a strength of the work. So, why not add experiments on this? I know eq24 contains hidden confounding, but this direction is not examined, e.g., by adjusting the strength of confounding and comparing to other methods.\n\nAs indicated above, adding real-world problems in experiments can greatly strengthen the work. Also, it would be interesting to replace the CMR part with iVAE (Khemakhem et al., 2020) and see how the results would change.\n\nI will read the rebuttal and revised paper and raise my score to 8 if the issues/questions above are addressed."
                },
                "questions": {
                    "value": "I cannot understand the importance of Proposition 3, and it seems just a trivial restatement of Def 2 and adds confusion to me.\n\nI think the title would be better to stress the CF approach because this arguably contributes more to intervention extrapolation than \u201cidentifying representation\u201d.\n\nIt is weird that Wiener\u2019s Tauberian theorem is mentioned in the Abstract but not the main text."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7315/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7315/Reviewer_k2zA",
                        "ICLR.cc/2024/Conference/Submission7315/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7315/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698768490297,
            "cdate": 1698768490297,
            "tmdate": 1700580193185,
            "mdate": 1700580193185,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CZAEtdujqq",
                "forum": "3cuJwmPxXj",
                "replyto": "jSbH3o3hzP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7315/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7315/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer k2zA"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the detailed feedback.\n\nWe address the points raised in the 'Weaknesses' and 'Questions' sections below.\n\n1. > Technical novelties seem to be weak. Theorem 4 seems to be an adaptation of the CF approach in (Newey et al., 1999), and Theorem 6 seems to be an adaptation of the IV approach in (D\u2019Haultfoeuille, 2011). If there are some technical novelties, they should be discussed and compared to the original works; otherwise, I suggest being more explicit about this weakness.\n\n   Theorem 4 extends beyond the result of Newey et al. (1999) by not assuming that the latent variable $Z$ is observed. We believe the use of CFs in extrapolation is novel as well. As for Theorem 6, while part of the proof strategy is adapted from D\u2019Haultfoeuille (2011) (as mentioned in the proof), the statement of the theorem itself is different. The key novelty lies in the use of the linear invariance condition for the identification of the hidden representation, which, to the best of our knowledge, has not been discussed in any prior work.\n\n2. > Some assumptions are strong; particularly, the linear model between Z and A, and the injective model and noiseless model between Z and X. Intuitively, X is an observable proxy of the hidden Z, and the assumption means there is no information loss in this proxy, which is strong. Moreover, both assumptions involve hidden variable Z and add difficulty to practical judgments. Since both assumptions are inherent in the current approach, I do not expect the author(s) to address this weakness in the rebuttal, but the following could certainly be done.\n\n    > Discussion of the setting and comparison to related work. The discussion of the relationship to reinforcement learning is interesting but does not touch on when can we possibly expect linearity and noiseless injectivity. It would be more interesting to draw and discuss a couple of real-world problems that might satisfy the assumptions.\n\n    > As indicated above, adding real-world problems in experiments can greatly strengthen the work.\n\n     * We have now added Remark 6 in Appendix B (and have referenced this appendix after Setting 1) to discuss key assumptions in Setting 1 and whether they can be extended. Some extensions are straightforward; for example, one can relax the linearity assumption from $A$ to $Z$ if there is a known nonlinear transformation $h$ such that $Z := M_0 h(\\tilde A) + V$; in this case, we use $A := h(\\tilde A)$ and obtain a model that lies in our model class.  Furthermore, our setting accommodates any invertible nonlinear transformation on $Z$, too. To see this, if there is a nonlinear injective function $h$ such that $\\tilde{Z} := h(M_0 A + V)$ and $X := g_0(\\tilde{Z})$, we can define $Z := M_0 A + V$ and $X := (g_0 \\circ h)(Z)$ and obtain a model in our model class. But key to our approach is that the model of $Z$ on $A$ has the ability to extrapolate, otherwise non-linear extrapolation for $\\mathbb{E}[Y|\\text{do}(A:=a^*)]$ is not possible.\n\n   * Motivated by your (and the other reviewers\u2019) comments on noiseless injectivity, we have conducted an additional synthetic experiment, where we introduce random noise in $X$. The results indicate that our method is robust to a certain degree of noise in $X$. The results can now be found in Appendix H.3. \n   * We view our work as establishing a foundation for learning an identifiable representation with the goal of extrapolation. We hope that our work will spark further research in this direction which hopefully further relaxes assumptions and proposes alternative methodology to solve the task, with more flexible applicability to real-world problems."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7315/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700506305188,
                "cdate": 1700506305188,
                "tmdate": 1700515931183,
                "mdate": 1700515931183,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "06u4MdU3ff",
                "forum": "3cuJwmPxXj",
                "replyto": "jSbH3o3hzP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7315/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7315/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer k2zA (continued)"
                    },
                    "comment": {
                        "value": "3. > On the other hand, (Khemakhem et al., 2020) and [1, 2], which are based on the former, are important related work that needs more discussions, and this would clarify the current approach. For example, (Khemakhem et al., 2020) recover Z based on exactly the same graph as A\u2192Z\u2192X, and also assume g is injective but allows an additive noise on X; the identification also relies on assumptions on the A\u2192Z part, where p(Z|A) is assumed to be an exponential family distribution but allows nonlinearity. Overall, I do not think the assumptions in (Khemakhem et al., 2020) are clearly stronger than those in the current work. Further, [1, 2] uses (Khemakhem et al., 2020) to estimate treatment effects, though not considering intervention extrapolation, [2] mentioned the ideas of CMR and CF in Sec 4.4.\n\n    We agree that our actions $A$ could be considered auxiliary variables. However, the work by Khemakhem et al., 2020 differs from ours in two important ways. \n    * Assumptions: one of the key assumptions in their setting is that the density $p(z|a)$ is conditionally factorized, meaning that the components of $Z$ are independent when conditioned on $A$. In contrast, our approach permits dependence among the components of $Z$ even when conditioned on $A$ (because in our setting, the components of $V$ can have arbitrary dependencies). \n    * Type of identifiability: maybe even more importantly, Khemakhem et al. provide identifiability up to point-wise nonlinearities which is not sufficient for intervention extrapolation. The main focus of our work is to provide an identification that facilitates a solution to the task of intervention extrapolation.\n\n    We have now included a discussion on the relationship to nonlinear ICA in Appendix A and have referenced this appendix in the related work section. Additionally, we have cited (Wu & Fukumizu, 2022) (along with other works suggested by another reviewer) in the introduction as an example of prior work that demonstrates the benefits of identifiable representations. Lastly, given the difference in the type of identifiability, we believe that an empirical comparison to iVAE may not be relevant.\n\n4. > I think the ability to deal with unobserved confounders is a strength of the work. So, why not add experiments on this? I know eq24 contains hidden confounding, but this direction is not examined, e.g., by adjusting the strength of confounding and comparing to other methods.\n\n    Thank you for your suggestion. We have carried out an additional experiment to explore the impact of unobserved confounders on our approach's extrapolation performance. The results are now available in Appendix H.2. In short, our approach maintains consistent extrapolation performance across all levels of confounding.\n\n5. > I cannot understand the importance of Proposition 3, and it seems just a trivial restatement of Def 2 and adds confusion to me.\n\n    Proposition 3 offers an additional perspective on Definition 3, specifically that $Z$ can be reconstructed from $\\phi(X)$. Its statement is used in the evaluations of the experiments. We have now moved Proposition 3 to Appendix C.2.\n\n6. > I think the title would be better to stress the CF approach because this arguably contributes more to intervention extrapolation than \u201cidentifying representation\u201d.\n\n    We thought about including the control function approach in the title too, but it was challenging to find a title that emphasizes the main topic of learning representation for extrapolation while mentioning control functions, without making it overly complex or too long. We still have not found a title that we believe is better than the current one but we would certainly appreciate suggestions if you have any.\n\n7. > It is weird that Wiener\u2019s Tauberian theorem is mentioned in the Abstract but not the main text.\n\n    We agree that it is confusing to mention it in the abstract but not in the main text. Based on all reviewers\u2019 comments, we have added several points to the main text, so space became a limiting factor. We did not find an explanation of the Tauberian theorem in the main text as important as the other points, so, we have now removed the reference to Wiener Tauberian theorem from the abstract. Its role becomes clear anyways, when reading the proof."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7315/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700507183721,
                "cdate": 1700507183721,
                "tmdate": 1700515699724,
                "mdate": 1700515699724,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Wy94TgqdCr",
                "forum": "3cuJwmPxXj",
                "replyto": "06u4MdU3ff",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7315/Reviewer_k2zA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7315/Reviewer_k2zA"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the rebuttal. I'd really like to see a couple of real-world examples that might satisfy the assumptions, but my other concerns are largely addressed. I raise my score to 8 accordingly. \n\nNice work!"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7315/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700580121413,
                "cdate": 1700580121413,
                "tmdate": 1700580121413,
                "mdate": 1700580121413,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "B9KZfISwox",
            "forum": "3cuJwmPxXj",
            "replyto": "3cuJwmPxXj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7315/Reviewer_iVVC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7315/Reviewer_iVVC"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a method to learn an effect through learning a representation of latents (Z) from observed data (X) where X and Z are related through some injective function (in practice an encoder model). It is assumed that interventions in A act on Z through which they act on some required outcome variable Y through linear functions. In such a setting the authors propose a method for learning the effects of ood interventions a^* in A. They show through experiments the efficacy of their method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- Important problem. Can be thought of as OOD estimation of intervention effects through learning latent representations.\n- Extremely well written paper!\n- Crucially E[Y | do(A=a')] \\neq E[Y | A=a'] for a' not in support of A.\n- The propositions are exactly at the places that the reader thinks about the question, and are easily understandeable.\n- The proofs of extrapolation are not straightforward. There have been several causal tools brought together to show the validity of extrapolation (invariance principle, mixing-unmixing, instrumental variable approaches. I quite liked the work."
                },
                "weaknesses": {
                    "value": "- I did not see any major weakness. One model assumption that could be weakened in future work is the linearity assumption.\n- The role of the Wiener\u2019s Tauberian theorem in the proof of hidden representation being identifiable, upto affine transformation, is not clear to me. Since this has been claimed in the abstract it would be helpful to delineate where it has been used."
                },
                "questions": {
                    "value": "- Proposition 1: If for all a in the support of A, E^S1[Y|A=a]=E^S2[Y|A=a], then how is there a set with positive lebegue measure over the support s.t the do distributions are not equal? The issue is one of measure of sets outside support being 0. Some clarification remarks would be helpful as to what positive measure outside supp(A) means.\n- The linear assumption is reasonably strong. Future work may be required to extend it to GLM's or non-linear representation learning.\n- Is the functional form of the SCM necessary for extrapolation? Can there be such analyses on CBNs?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7315/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7315/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7315/Reviewer_iVVC"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7315/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698847892335,
            "cdate": 1698847892335,
            "tmdate": 1699636874041,
            "mdate": 1699636874041,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Z2w9jIL3yO",
                "forum": "3cuJwmPxXj",
                "replyto": "B9KZfISwox",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7315/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7315/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer iVVC"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the positive feedback.\n\nWe address the points raised in the 'Weaknesses' and 'Questions' sections below.\n\n1. > One model assumption that could be weakened in future work is the linearity assumption.\n\n    > The linear assumption is reasonably strong. Future work may be required to extend it to GLM's or non-linear representation learning.\n\n     We have now added Remark 6 in Appendix B (and have referenced this appendix after Setting 1) to discuss key assumptions in Setting 1 and whether they can be extended. Some extensions are straightforward; for example,  one can relax the linearity assumption from $A$ to $Z$ if there is a known nonlinear transformation $h$ such that $Z := M_0 h(\\tilde A) + V$; in this case, we use $A := h(\\tilde A)$ and obtain a model that lies in our model class.  Furthermore, our setting accommodates any invertible nonlinear transformation on $Z$, too. To see this, if there is a nonlinear injective function $h$ such that $\\tilde{Z} := h(M_0 A + V)$ and $X := g_0(\\tilde{Z})$, we can define $Z := M_0 A + V$ and $X := (g_0 \\circ h)(Z)$ and obtain a model in our model class. But key to our approach is that the model of $Z$ on $A$ has the ability to extrapolate, otherwise non-linear extrapolation for $\\mathbb{E}[Y|\\text{do}(A:=a^*)]$ is not possible.\n\n3. > The role of the Wiener\u2019s Tauberian theorem in the proof of hidden representation being identifiable, upto affine transformation, is not clear to me. Since this has been claimed in the abstract it would be helpful to delineate where it has been used.\n\n    Thank you for the comment, we agree that it is confusing to mention it in the abstract but not in the main text. Based on all reviewers\u2019 comments, we have added several points to the main text, so space became a limiting factor. We did not find an explanation of the Tauberian theorem in the main text as important as the other points, so, we have now removed the reference to Wiener Tauberian theorem from the abstract. Its role becomes clear anyways, when reading the proof.\n\n2. > Proposition 1: If for all a in the support of A, E^S1[Y|A=a]=E^S2[Y|A=a], then how is there a set with positive lebegue measure over the support s.t the do distributions are not equal? The issue is one of measure of sets outside support being 0. Some clarification remarks would be helpful as to what positive measure outside supp(A) means.\n\n    Please note that we refer to positive measure with respect to Lebesgue and not with respect to $\\mathbb{P}_A$. The existence of such a set $\\mathcal{B}$ is possible if $\\mathcal{A} \\setminus \\text{supp}(A)$  has positive Lebesgue measure (and, indeed, the set $\\mathcal{B}$ cannot be a subset of $\\text{supp}(A)$, it must be a subset of $\\mathcal{A} \\setminus \\text{supp}(A)$). We have now added clarifying comments to avoid possible confusion for future readers. \n\n3. > Is the functional form of the SCM necessary for extrapolation? Can there be such analyses on CBNs?\n\n    Yes, the functional form of the SCM is necessary for extrapolation. As far as we can see, describing the data-generating process with a causal Bayesian network alone would not permit specific model specifications, such as the linearity of $Z$ on $A$ or the additivity of $V$ on $Z$."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7315/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700505135896,
                "cdate": 1700505135896,
                "tmdate": 1700515959880,
                "mdate": 1700515959880,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]