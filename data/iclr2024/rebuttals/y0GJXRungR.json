[
    {
        "title": "Is Self-Repair a Silver Bullet for Code Generation?"
    },
    {
        "review": {
            "id": "DlPuDAz6J1",
            "forum": "y0GJXRungR",
            "replyto": "y0GJXRungR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3865/Reviewer_LPE6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3865/Reviewer_LPE6"
            ],
            "content": {
                "summary": {
                    "value": "The authors break the problem of program repair into four steps:\n\n1. (LLM) Generate an initial population of programs.\n2. Run the programs to obtain error messages.\n3. (LLM) Analyze the errors, and generate feedback as to the cause of failure.\n4. (LLM) Given the feedback, generate a repaired program.\n\nThis paper differs from prior work in part because it emphasizes the importance of step (3) -- using feedback to direct the repair.  The authors run a number of experiments with different models, explore what happens when a stronger model provides feedback to a weaker one, and also explore what happens when humans provide feedback to a model.\n\nThe authors also explicitly model the cost of repair.  Is running steps (3) and (4) more effective than simply drawing more samples from the initial population?  The answer turns out to be \"not really\".  \n\nThe experiments are well-designed and the authors make several interesting observations.  To paraphrase:\n\nA.  LLMs are currently quite bad at program repair.  The best way to get a correct program is to generate a large and diverse initial population, in the hopes that one of the initial programs is \"close enough\" to work with minimal tweaks.  Even a single repair step yields at best marginal improvements.  LLMs are thus very different from human programmers; they seem to be unable to iteratively refine and debug programs over multiple steps.  \n\nB.  The main limitation is the quality of feedback.  Given feedback from a stronger model, or from a human, LLMs show substantial gains from repair.  However, LLMs seem to have difficulty finding problems in the code that they generate."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The paper is well-written, and the authors formulation of the problem is insightful.  In particular, they do not merely test the accuracy of program repair in isolation, they compare it against the alternative, which is generating new initial programs from scratch.  \n\nThe experiments seem to be well done, particularly the ones which use a stronger model or human to provide feedback to a weaker model."
                },
                "weaknesses": {
                    "value": "The authors do not attempt to fine-tune a model on the task of program repair.  Thus their experiments mainly demonstrate that LLMs, *as currently trained*, do not do a good job at the repair task.\n\nTo be fair, fine-tuning is probably out of scope for this paper, especially since some of they models they test are private, and accessible only via an API."
                },
                "questions": {
                    "value": "Your illustration of the repair-tree is interesting, and it brings to mind the idea of extending this evaluation technique to a proper tree search.  You might be able to get better results by using a value network, and focusing only on the nodes of the tree that are most promising for repair, in a manner reminiscent of evolutionary search, with the LLM as mutator. \n\nSuccessful repair attempts, preferably after several iterations of feedback and repair, could also be used to fine-tune the LLM to generate better feedback, and to generate better repair code given the feedback.  (See weaknesses, above.)\n\nHave you considered any experiments along these lines?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3865/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3865/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3865/Reviewer_LPE6"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3865/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698791784927,
            "cdate": 1698791784927,
            "tmdate": 1699636344883,
            "mdate": 1699636344883,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vHGh1nf85z",
                "forum": "y0GJXRungR",
                "replyto": "DlPuDAz6J1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3865/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3865/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer LPE6"
                    },
                    "comment": {
                        "value": "Thank you, Reviewer LPE6, for your kind words and suggestions for further experiments! We are happy to hear that you find the paper well written, and the methodology/experiment design both sound and insightful.\n\nAs also pointed out by Reviewer PNGf01, you are correct that our contributions are limited to the few-shot/in-context setting. We share your sentiment that it would be very interesting to see how finetuning would affect the conclusions in this paper, but\u2014as you have pointed out\u2014this is out of scope for our experiments (which we emphasize are already very large, due to the branching nature of self-repair). We look forward to future work which can leverage the insights we share here to investigate what role fine-tuning can play in alleviating self-repair\u2019s feedback bottleneck!\n\nAs to your questions: You touch on two important ideas, that we certainly share your excitement about.\n\nThe first is to use a value network to select which candidate program to repair at each step. Our preliminary experiments did explore some very simple search strategies, such as ranking the candidates by their cumulative log probabilities, but we found no evidence that this was correlated positively with self-repair success rate. However, it is very much possible that using a value network/\u201cconfidence model\u201d to rank the candidates (Chen et al., 2021; Inala et al., 2022; Zhang et al., 2022; see Related Work) would yield performance benefits. Although this fell outside of the scope of this paper, we are excited about future work investigating when and how proper tree search can be used in conjunction with self-repair to yield greater performance increases.\n\nYour second question is reminiscent of an RLHF (without the H) training stage, in which the model uses self-sampled feedback (and repairs) to improve its own code debugging & repair capabilities. This is certainly an interesting idea, and one we believe will play an important role in the future as new (publically available, permissively licensed) datasets to train on become more rare. However, it is also a method with clear challenges, and not one we have started experimenting with at this stage!\n\nThanks again for your feedback. Let us know if you have any further questions; we'd love to continue this discussion through the rebuttal week and beyond!"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3865/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699988279217,
                "cdate": 1699988279217,
                "tmdate": 1699988279217,
                "mdate": 1699988279217,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ovdL9qS9bN",
                "forum": "y0GJXRungR",
                "replyto": "vHGh1nf85z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3865/Reviewer_LPE6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3865/Reviewer_LPE6"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response.  I'm glad to hear that you are thinking about value networks and RL(-H)F; I look forward to seeing further work in this space."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3865/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700258287900,
                "cdate": 1700258287900,
                "tmdate": 1700258287900,
                "mdate": 1700258287900,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ooxGpLOiZc",
            "forum": "y0GJXRungR",
            "replyto": "y0GJXRungR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3865/Reviewer_PNGf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3865/Reviewer_PNGf"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates the ability of large language models (specifically, Code Llama, GPT-3.5, and GPT-4) to perform self-repair which refers to the model's capacity to identify and correct mistakes in its own generated code. The paper analyzes the effectiveness of self-repair in addressing code generation errors, specifically on problems sourced from HumanEval or APPS datasets. The findings suggest that the gains achieved through self-repair are often modest and vary significantly across different subsets of data. In some cases, self-repair does not result in noticeable improvements. The paper proposes that this limitation might be due to the model's ability to provide feedback on its own code, and stronger models might enhance self-repair effectiveness. Additionally, the paper explores the impact of providing the model with feedback from human participants, showing that this feedback significantly benefits the self-repair process, even for the advanced GPT-4 model. The paper offers a brief qualitative analysis to shed light on why this human feedback is valuable in improving code repair performance. Overall, the study provides insights into the challenges and potential enhancements of self-repair mechanisms in large language models for code generation tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "__The paper systematically analyzes the self-repair capability of LLMs.__\n\nWhile existing works of self-repair mostly argue that self-repair is feasible for LLM, this paper is the first work to systematically study the strengths and weaknesses of self-repair. Specifically, I appreciate the systematic comparison between self-repair and the pass@k performance, though the conclusion might not be generalizable (see Weaknesses), showing that when the computation is comparable, their benefits are also similar, and also suggesting the optimal combination of these two techniques is the most effective. Also, studying the quality of feedback is also a novel perspective to analyze the self-repair capability of LLM\n\n__The paper sheds light on the weaknesses of LLM in self-repair, providing clear takeaways and indicating the potential future work in this direction.__\n\nThe paper is well written and properly organized, and it sheds light on the weaknesses of LLM's self-repair capability. Such weaknesses can be due to the lack of both data and carefully crafted training strategy for self-repair capability, indicating the future research direction of refining the existing LLM with better self-repair capability."
                },
                "weaknesses": {
                    "value": "I am overall positive regarding this paper, and I appreciate the systematic study the paper performs to quantify LLM's self-repair capability. However, the conclusions are claimed in a strong and general tone, while the study itself is actually limited in scope for two reasons.\n\n__The scope of the study is limited to solving isolated programming challenges while ignoring real-world development.__\n\nThe study focuses completely on the programming challenges datasets, such as HumanEval and APPs. These datasets have several characteristics that are not realistic in daily development, therefore, though I appreciate the initial conclusions of this paper, these takeaways might not be applicable to a more realistic scenario. \n\nFirst, the samples in the studied datasets are provided with clear and complete problem descriptions, which are not always available in real-world programming practice. One of the main reasons that pass@k works so well in programming challenges is that the expected functionality of the program is fully revealed and clearly explained in the prompt, so the model is able to maximize the diversity within a narrowed semantic space when generating multiple sequences. However, such clearly explained prompts, as natural language, are typically not available during the ongoing development, where the developers start with a very high-level goal and eventually design modules and implement them piece by piece. During this process, the human intent is not always explicitly specified, as docstring or comment, before the code LM is prompted to complete the following code snippets. In these cases, the execution of unit tests provide meaningful feedback to specify and concretize the expected functionalities, which cannot be leveraged by the top-k generation but is valuable guidance for iterative self-repair. Therefore, though I agree with the takeaway that pass@k is comparable to, sometimes better than, self-repair in the programming challenge dataset, such observation might not be realistic for daily development and requires further study.\n\nSecond the samples in the studied datasets are mostly short and self-contained, missing the complicated data and inter-procedural dependencies. Pass@k explores the breath of each token within the sequence without directional guidance, but such breath or search space exponentially increases with the increase of the code length. The program challenges datasets contain samples mostly up to tens of lines of code, significantly underestimating the complexity of real-world software, which includes hundreds or thousands of lines of code within one single file and maintains complicated dependencies. Generating k sequences blindly without feedback may hardly fulfill the expectation due to the large search space, while execution feedback, such as the indication of a missing third-party library, helps the model quickly locate the problematic code and focus on fixing just that part. Therefore, when the complexity of the program increases, it requires further study to understand whether self-repair is equivalent as top-k generation.\n\n__It is not clear whether fine-tuning for self-repair could easily overcome the weaknesses or not.__\n\nThis paper focuses on the LLM's self-repair capability only with prompting, without optimizing the model parameters towards the self-repair capability. It is not clear whether a cheap fine-tuning could quickly enable the model's capability of understanding and leveraging the feedback efficiently. Drawing the conclusion that self-repair is not a silver bullet without trying straightforward fine-tuning might be too strong.\n\nTo conclude, I would encourage the author to consider constraining their conclusions with the study's scope and add a discussion section to mention"
                },
                "questions": {
                    "value": "Please explain and address the weaknesses. Otherwise, the paper is well-written and clear."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3865/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3865/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3865/Reviewer_PNGf"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3865/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698892720381,
            "cdate": 1698892720381,
            "tmdate": 1700672830950,
            "mdate": 1700672830950,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ojqTO8QkHb",
                "forum": "y0GJXRungR",
                "replyto": "ooxGpLOiZc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3865/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3865/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to PNGf"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their feedback on the limitations of our analysis! We agree that there is much left to explore in this space, and are excited about the future work this work will inspire. Without further ado, let\u2019s jump into a discussion of the weaknesses identified by PNGf01.\n\n*The scope of our study is limited to self-contained Python programs, which is quite different from real-world software engineering.*\n\nSoftware Engineering (SE) and competitive programming both present their own unique challenges. SE tasks often involve incomplete  task specifications as well as (long) contextual dependencies. Competitive programming, on the other hand, is mainly challenging due to the logical and algorithmic complexity, often requiring the use of dynamic programming and graph algorithms to solve the task.\n\nAn ideal self-repair model should be able to (1) fix errors in logically complex tasks, (2) handle ambiguous specifications and missing context, and (3) repair without a clear test oracle. The community would benefit from in-depth studies of each capability. In this paper, we isolate the first aspect, providing insights that would be hard to untangle otherwise.\n\nWith this in mind, we believe competitive programming becomes a well suited testbed for our analysis:\n- Using logically complex, well-specified programming puzzles to benchmark models has a rich history in the literature [0,1,2].\n- Recent work shows that even contemporary models are still challenged by intermediate/advanced-level competitive programming tasks [3, 4].\n- Competitive programming tasks have relatively complete specifications and unit tests, so we do not have to worry as much about our results being muddled by degenerate cases (e.g. the solution is incorrect simply because the task specification did not provide a sufficient definition of an API).\n\nIn our study, we are thus able to hone in on self-repair's efficacy in algorithmically challenging tasks of varying difficulty levels. We use this primarily to isolate the importance of the feedback stage, but it also enables us to discover non-obvious relationships such as the non-straightforward interplay between task difficulty and self-repair efficacy (Section 4.1 and Appendix B). This surprising fact would have been difficult to discover without isolating the effect of the logical/algorithmic complexity of the task at hand.\n\nIn future work, we'd like to explore the other two capabilities as well. These are motivated by the question of how future software engineering workflows should best leverage AI code generation tools. One can imagine workflows which emphasize encapsulation to an extent that each individual piece is no more complex than a programming puzzle, but tricky bugs still tend to arise at the interface level. Furthermore, the fact that Test-Driven Development has fallen out of vogue suggests that developers do not like writing unit tests first, which motivates capability (3). Similarly, although natural language may not have played a big role in software engineering historically, minimizing the impact of ambiguity in NL specifications is now necessary in order to enable capability (2).\n\nIn summary, as we hope this discussion has shown, we wholeheartedly agree with the reviewer that there is much interesting work left to be done in this space. Furthermore, **we will revise the Introduction and Future Work sections to clarify the scope of our contributions, as well as highlight some of the nuances discussed above**.\n\n*Our study is limited to prompting strategies, and does not consider fine-tuning.*\n\nWe certainly agree that such an analysis would be of interest to the community! However, we believe that this falls outside the scope of this study, which we emphasize is already significant in terms of its depth (and cost). In addition to the methodological challenges, fine-tuning also comes with practical concerns such as cost and model availability. We thus leave it to future work to investigate whether fine-tuning can alleviate the feedback bottleneck identified by this paper. Besides, prompting may be accessible to a wider audience than fine-tuned models in the current AI ecosystem; studying self-repair in this context may therefore benefit AI practitioners more in practice.\n\nPlease let us know if you have any further thoughts or questions - we greatly appreciate your feedback!\n\n\n---\n\n\n[0] Li, Yujia, et al. \"Competition-level code generation with alphacode.\" Science 378.6624 (2022): 1092-1097.\n\n[1] Austin, Jacob, et al. \"Program synthesis with large language models.\" arXiv preprint arXiv:2108.07732 (2021).\n\n[2] Chen, Mark, et al. \"Evaluating large language models trained on code.\" arXiv preprint arXiv:2107.03374 (2021).\n\n[3] Hendrycks, Dan, et al. \"Measuring coding challenge competence with APPS.\" arXiv preprint arXiv:2105.09938 (2021).\n\n[4] Inala, Jeevana Priya, et al. \"Fault-aware neural code rankers.\" Advances in Neural Information Processing Systems 35 (2022): 13419-13432."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3865/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699990533796,
                "cdate": 1699990533796,
                "tmdate": 1699990533796,
                "mdate": 1699990533796,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4XQJDI38lT",
                "forum": "y0GJXRungR",
                "replyto": "ojqTO8QkHb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3865/Reviewer_PNGf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3865/Reviewer_PNGf"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the authors' revision of the paper and thoughtful discussion. I am now increasing my score to champion this paper for its acceptance."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3865/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700672798667,
                "cdate": 1700672798667,
                "tmdate": 1700672798667,
                "mdate": 1700672798667,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "61K4YnL2Hb",
            "forum": "y0GJXRungR",
            "replyto": "y0GJXRungR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3865/Reviewer_Vfe4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3865/Reviewer_Vfe4"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates the sample efficiency of a self-repair approach for LLM based code-generation tasks. It evaluates performance of this approach on HumanEval and APPS dataset using ColdeLLama-13b-instruct, GPT3.5, and GPT4  and provides several insights based on the results: 1) Sampling without repair can perform equally or better than self-repair in almost all sample budgets; 2) Initial sampling diversity is more critical than the diversity of repair samples; 3) Quality of feedback significantly improves the performance of self-repair."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* This paper provides several new insights on self-repair for code generation compared to the baseline method of sample generations without self-repair. It shows how sample budget and initial sample diversity could impact the efficiency of code generation. \n* The investigations on self-repair performance improvement only by improving feedback quality could enable more interesting future ideas.\n* Overall, the paper is well-written and easy to read. The authors did a great job in highlighting the key limitations of the analysis."
                },
                "weaknesses": {
                    "value": "The experimental results presented in this support the claim around the limitations of self-repair. Interestingly, the findings around overall efficacy compared to baseline somewhat contradicts with the results from Chen et. al. 2023b that shows self-repair could provide significant increase in sample efficiency. Although \u2018self-debuggging\u2019 work from Chen et. al. is mentioned in the related work, I think more comparative analysis would strengthen the claim of this paper. Analysis results from more diverse code generation task including datasets other than python language would also be interesting additions to the analysis."
                },
                "questions": {
                    "value": "1. Is there a specific reason to restrict feedback and repair samples to 1 in the analysis of feedback boosting (section 4.2)? \n2. Should we expect similar results using the \u2018self-debugging\u2019 approach that uses few-shot prompts?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3865/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3865/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3865/Reviewer_Vfe4"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3865/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699350944810,
            "cdate": 1699350944810,
            "tmdate": 1700620045954,
            "mdate": 1700620045954,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "idVl4fWGGm",
                "forum": "y0GJXRungR",
                "replyto": "61K4YnL2Hb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3865/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3865/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Vfe407"
                    },
                    "comment": {
                        "value": "We thank Reviewer Vfe407 for their helpful comments! We are very happy to hear that the reviewer found our contributions insightful, easy to understand and well scoped (with clear discussion of limitations).\n\nWe agree with the reviewer that the relationship to Chen et al. (2023b) is important to discuss. After replying to the reviewer\u2019s specific questions below, we will give a detailed account of how our work compares to that of Chen et al (2023b).\n\n---\n\n*Individual responses to questions.*\n\n> Is there a specific reason to restrict feedback and repair samples to 1 in the analysis of feedback boosting (section 4.2)?\n\nThis restriction is mainly a practical one: separating the feedback and repair makes the experiment significantly more time-consuming (and costly) to run, since each stage must now be implemented as a separate API call. With this restriction, we can control the cost of this experiment by using a smaller value of $N_f$ and $N_r$ without increasing the risk of statistical artifacts too much. We emphasize that the preceding experiments already showed that this setting is the most effective, and we therefore do not believe this limits the analysis in practice.\n\n> Should we expect similar results using the \u2018self-debugging\u2019 approach that uses few-shot prompts?\n\nAlthough our results are already similar (and we do use few-shot prompting; see Appendix F for a complete list of the prompts we use), we agree that our results are\u2013generally speaking\u2013not as strongly in favor of self-repair/self-debugging. This is because of slight differences in the experimental setting being investigated; see the discussion below for a detailed comparison.\n\n---\n\n*Detailed comparison to Chen et al. (2023b).*\n\nAs mentioned in the Related Work, Chen et al. (2023b)\u2019s method is indeed closely related to ours; both use few-shot prompting to encourage the model to first retrospect on the code (in order to understand why it failed) and then perform repair. There are, however, a few differences.\n\n1. The main reason our results differ from those of Chen et al. (2023b) is that we conduct our experiments in a slightly different setting.\n\nIn Chen et al.\u2019s study, the self-debugging method has access to a correctness oracle to decide whether to proceed with debugging or not; the baseline does not have access to the oracle. In our work, both the baseline and the self-repair approach have equal access to an oracle (when evaluating pass@k) and are compared with the same sample budget. Thus, the performance improvement over the baseline is more prominent in Chen et al. (2023b)\u2019s work than in ours.\n\nWe choose to grant both baseline and repair models equal access to the oracle so that we can better analyze the tradeoff between increased sampling overhead and accuracy gained. Chen et al. (2023b) instead aim to show how oracles and different repair strategies can improve the final accuracy compared to current, standard, non-repair-based approaches. Thus, our findings are not actually contradictory to those of Chen et al. (2023b).\n\n2. Generally speaking, our work focuses on exploring the efficacy of self-repair in depth, while Chen et al. focus on comparing a broader range of different self-debugging strategies in a few different domains. Our studies thus complement each other.\n\nConcretely, our work focuses on what Chen et al. calls the \u201cUnit Test + Explanation\u201d style of feedback, in which the model is provided with external feedback from a unit test suite but then has to generate its own explanation of why the test failed. We study the effect of this textual explanation in great detail through our experiments in sections 4.2 and 4.3, and show that it is the limiting factor in this type of self-repair. We also offer a substantive discussion of the effect of the hyper-parameters, which amongst other things highlights the need to obtain sufficiently diverse initial samples if self-repair is to be successful.\n\nThe scope of our study is also limited to what Chen et al. call \u201cText-to-Python Generation\u201d tasks, while they also consider code translation and SQL query generation tasks. However, we do so in both an easier setting (HumanEval) and a significantly more challenging setting where baseline performance is lower (APPS), while Chen et al. focus on the relatively easy MBPP dataset. As we show in section 4.1 and Appendix B, the relationship between task difficulty and self-repair performance is quite subtle, an insight which we would not have been able to show without analyzing multiple datasets in detail.\n\n---\n\nWe once again thank Vfe4 for your helpful feedback on the paper. We hope that this discussion has clarified how our work relates to that of Chen et al. (2023b), and in particular the slightly different experimental settings as well as our emphasis on depth of understanding. If you have any more questions or thoughts, please do not hesitate to share them with us - we look forward to continuing this conversation throughout the discussion period!"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3865/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699991015631,
                "cdate": 1699991015631,
                "tmdate": 1699991015631,
                "mdate": 1699991015631,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "g5UqTkwvxZ",
                "forum": "y0GJXRungR",
                "replyto": "idVl4fWGGm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3865/Reviewer_Vfe4"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3865/Reviewer_Vfe4"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for addressing my concerns."
                    },
                    "comment": {
                        "value": "Thank you for providing detailed response and updating the paper to address some of the concerns with additional experiments and revised texts. I think the new version provides more clarity on contributions of the paper."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3865/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700619807456,
                "cdate": 1700619807456,
                "tmdate": 1700619807456,
                "mdate": 1700619807456,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]