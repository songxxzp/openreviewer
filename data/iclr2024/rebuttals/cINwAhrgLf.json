[
    {
        "title": "Free Lunches in Auxiliary Learning: Exploiting Auxiliary Labels with Negligibly Extra Inference Cost"
    },
    {
        "review": {
            "id": "okSymgdlDy",
            "forum": "cINwAhrgLf",
            "replyto": "cINwAhrgLf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1619/Reviewer_fJaD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1619/Reviewer_fJaD"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to harness the auxiliary tasks to enhance the performance of the primary task, while maintaining a single task inference cost for the primary task. The authors achieve this by designing an asymmetric network and further develops two algorithms: the first algorithm directly uses the asymmetric primary-to-auxiliary architecture, where the auxiliary tasks can be directly removed during the inference; the second algorithm initiates with an architecture with bi-directional connections, and subsequently exploits a tailored L1 constrained NAS optimization to prune all the auxiliary-to-primary connections, thereby enabling to remove the auxiliary task during inference. The proposed soft-parameter sharing architecture-based method can be integrate with existing optimization-based methods. The author validates their method with extensive experiments on 6 tasks with 3 CNN and transformer architectures."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper formulates the auxiliary learning problem through a task-oriented adaptive feature fusion approach without the need of explicitly identifying the task similarity. Mathematically, such architecture-based method can be seamlessly integrated with a variety of multi-task/auxiliary optimization methods such as loss re-weighting and gradient manipulation. The paper is well written and easy to understand, with an extensive literature review in Table 1 clearly demonstrating the contribution of the proposed method.\n2. The evolving and asymmetric network design, coupled with a tailored NAS algorithm, ensures the converged network comprises only the primary-to-auxiliary connections, thereby guaranteeing a single-task inference cost for the learned architecture.\n3. Beyond the benefits in the single-task inference cost, the authors also show (in Sect. 4.2.4 and the supplementary) that the training complexity exhibits a linear scalability to multiple auxiliary tasks.\n4. The experiments are extensively performed on 6 highly diverse tasks with 3 base net architectures including CNN and transformers. The authors also checked the performance when the primary and the auxiliary tasks possess different architectures in the supplementary. The results of all those experiments are promising."
                },
                "weaknesses": {
                    "value": "1. Is it possible to use Normalization and Activation operations other than BatchNorm and ReLU in Eqs. 13 and 14?\n2. In Fig. 3, should the cut-off dash line be between the 1x1 conv and the add operations?\n3. I suggest the authors to move the supplementary material into the Appendix of the main text for better readability."
                },
                "questions": {
                    "value": "Please respond to those in the Weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "I do not have ethics concerns."
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1619/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698643257246,
            "cdate": 1698643257246,
            "tmdate": 1699636090643,
            "mdate": 1699636090643,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "AJ8E4osIkj",
                "forum": "cINwAhrgLf",
                "replyto": "okSymgdlDy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1619/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1619/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your constructive comments. We have thoroughly addressed each of the raised questions with detailed responses. We hope those well resolved your concerns. Please kindly let us know if any further discussion or clarification is needed.\n\nQ1. **Is it possible to use Normalization and Activation operations other than BatchNorm and ReLU in Eqs. 13 and 14?**\n\nIt is true that we have the flexibility to leverage different Normalization and Activation operations other than BatchNorm and ReLU. As we are inserting new fusion operations to connect multiple well-trained single-task networks, **the key design principle behind our fusion operations in Eqs. 13 and 14 is to ensure that their insertion does not significantly alter the original well-trained single-task networks**. Therefore, any fusion operations, along with any associated Normalizations and Activations, are applicable as long as they adhere to this fundamental design principle.\n\nQ2. **In Fig. 3, should the cut-off dash line be between the 1x1 conv and the add operations?**\n\nDuring the NAS optimization, we cut-off the auxiliary-to-primary connections by regularizing $alpha^P$'s in Eq. 13 to 0. Therefore, in Fig. 3, we illustrated the cut-off dash line between the auxiliary input and the concatenation operation. Having said that, it is equivalent to draw the cut-off dash line be between the 1x1 conv and the add operations as suggested, because the output of 1x1 conv is also 0 when the input is 0.\n\nQ3. **I suggest the authors to move the supplementary material into the Appendix of the main text for better readability.**\n\nWe have moved the supplementary material, originally in a separate file, to the Appendix of the main text. Additionally, we have included the appropriate references in the main text. Please kindly refer to our updated manuscript."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1619/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700488109272,
                "cdate": 1700488109272,
                "tmdate": 1700488661283,
                "mdate": 1700488661283,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xyqzjE5EaM",
            "forum": "cINwAhrgLf",
            "replyto": "cINwAhrgLf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1619/Reviewer_Qzsf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1619/Reviewer_Qzsf"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a learnable and flexible asymmetric network architecture designed for general-purpose auxiliary learning, where the auxiliary task plays a pivotal role in supporting the primary task's training process, and can be freely removed during the inference. As a result, the proposed method achieves a multi-task level performance while keeping a single-tasks level inference cost. The authors implement their design as adaptive layerwise feature fusion of multiple single-task branches, where the full network converges to an asymmetric architecture with only primary-to-auxiliary connections existed, enabling the removal of the auxiliary task during the inference. Two algorithms are developed to achieve this, where the more advanced one exploits a specifically designed NAS pruning to achieve an asymmetric architecture after convergence. The experiments are extensive across 6 tasks with 3 network backbone architectures, which sufficiently demonstrate the promising performance."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1.\tThis paper tackles the general-purpose auxiliary learning towards a multi-task level performance and a single-tasks level inference cost. The proposed method can be applied to various tasks and network backbones mathematically and also validated experimentally.\n2.\tThe proposed method can also be freely combined with various multi-task or auxiliary task optimization methods listed in Table 1, which was also validated by the experiments.\n3.\tThe single-task level inference cost is assured through the resultant converged asymmetric network architecture. Furthermore, the training cost exhibits a linear increase when incorporating additional auxiliary tasks, which is enabled by the supernet architecture for NAS that only encompasses the connections between the primary task and each of auxiliary tasks. \n4.\tTable 1 present a very clear and comprehensive taxonomy about the position of the proposed method among the area of multi-task learning and the auxiliary task learning.\n5.\tThe experiments are extensive, validating the generalization on 6 diverse tasks within 3 datasets, and 3 network backbones including both CNNs and Transformers."
                },
                "weaknesses": {
                    "value": "This paper is well written, and I do not see major weakness, but the clarification of the following minor issues would further improve the paper: \n1.\tI appreciate that the authors provide the full NAS objective in Eq. 10, but the details about how it is optimized need to be further elaborated. If I understand correctly, the model weight w and the architecture weight alpha should be updated iteratively?\n2.\tI suggest indicating the network backbone in the legends of Tables 3 and 4, as there are several tables in a similar shape that only differ from backbones.\n3.\tIt is suggested to replace the figures with vector images for a better resolution. The paper, in its current version, used a lot of v-spacing; it is also advised to remove them for better readability."
                },
                "questions": {
                    "value": "The author claimed that they implement the tailored version of PCGrad and AdaShare specifically for the auxiliary task learning, i.e., PCGrad-Aux and AdaShare-Aux. What are the details of those auxiliary task learning variants?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1619/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698725464861,
            "cdate": 1698725464861,
            "tmdate": 1699636090558,
            "mdate": 1699636090558,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5uzpf1aY6b",
                "forum": "cINwAhrgLf",
                "replyto": "xyqzjE5EaM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1619/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1619/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your constructive comments. We have thoroughly addressed each of the raised questions with detailed responses. We hope those well resolved your concerns. Please kindly let us know if any further discussion or clarification is needed.\n\nQ1. **Details of NAS optimization.**\n\nIt is true that the model weight $w$ and the architecture weight $\\alpha$ are updated iteratively during our training of Aux-FG-NAS. Compared to training a fixed neural networks, our NAS optimization requires only one additional forward-backward pass to update the architecture weights in each iteration. This process is straightforward to implement, as indicated in our pseudo code provided above. We will release the complete training and evaluation codes upon acceptance.\n\nQ2. **Indicating the network backbone in the legends of Tables 3 and 4.**\n\nIndeed adding the network backbone into the table legends improves the readability. We followed this suggestion and updated our tables accordingly, please kindly refer to our updated manuscript.\n\nQ3. **Use vector images, remove the v-spacing.**\n\nWe thank this suggestion. We have utilized vector images and minimized the v-spacings, please kindly refer to our updated manuscript."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1619/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700488026745,
                "cdate": 1700488026745,
                "tmdate": 1700488026745,
                "mdate": 1700488026745,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zU2HZqCAAq",
            "forum": "cINwAhrgLf",
            "replyto": "cINwAhrgLf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1619/Reviewer_V6nv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1619/Reviewer_V6nv"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a new framework for auxiliary learning, in which the goal is to improve the performance on a task of interest (i.e., primary task) by utilizing auxiliary information. In particular, the proposed method aims to tackle auxiliary learning problems without introducing computational or parameter overhead during inference. To this end, the paper borrows inspiration from multi-task learning and neural architecture search to design an asymmetric network architectures, where the connections from primary-task network layers and auxiliary-task network-layers are directed (from primary to auxiliary), such that computations or parts of networks for auxiliary information can be removed during inference."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The proposed method successfully tackles auxiliary learning without inducing extra computational overhead during inference, by utilizing NAS to design a network that has asymmetric connections directed from primary-task network parts to auxiliary-task network parts.\n\n- The proposed method is flexible in that it can be combined with different auxiliary learning methods\n\n- The paper is clearly written; easy to read and follow."
                },
                "weaknesses": {
                    "value": "- Is there a need to initialize search space to include all bi-directional connections? why not start from networks with only primary-to-auxiliary connections right away?\n\n- Lack of ablation studies related with the question above: the performance change as the search space only contains primary-to-auxiliary connections.\n\n- Missing details: Are all auxiliary-to-primary connections are pruned at the end of training?\n\n- Missing details: What is the final architecture produced by NAS? How consistent is the final performance across different random seeds and trials?"
                },
                "questions": {
                    "value": "Written in the weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1619/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1619/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1619/Reviewer_V6nv"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1619/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698835960257,
            "cdate": 1698835960257,
            "tmdate": 1700555312742,
            "mdate": 1700555312742,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KFvq9ZFCPp",
                "forum": "cINwAhrgLf",
                "replyto": "zU2HZqCAAq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1619/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1619/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your constructive comments. We have thoroughly addressed each of the raised questions with detailed responses. We hope those well resolved your concerns. Please kindly let us know if any further discussion or clarification is needed.\n\nQ1.  **The rationale behind using bi-directional NAS initialization. Ablation of the initialization with only primary-to-auxiliary connections.**\n\n**The rationale behind initializing all the bi-directional connections is to exploit both features and gradients from the auxiliary tasks**. During the training, the auxiliary-to-primary connections, which supply auxiliary features, are gradually pruned out during the NAS training to ensure a single-task inference. \n\nWe expect that the performance of bi-directional initialization suppresses that of only initializing primary-to-auxiliary connections. This is because **the L1 regularization applied to auxiliary-to-primary connections leads to difficulty in re-establishing them** if we do not initialize them. As a consequence, **the model capacity, i.e., the valid search space, in the primary-to-auxiliary initialization only encompasses a subset of that in the bi-directional initialization**.\n\nTo validate the above discussion, we conducted experiments by initializing only primary-to-auxiliary connections as instructed, mirroring the setup in Table 5 using ViTBase on NYUv2. As anticipated, the ablation results align with our expectations, as detailed below.\n\n| NYU v2, Primary: Normal | mean | median | rmse | within 11.25 | within 22.5 | within 30 |\n| ------ | ------ | ------ | ------ | ------ | ------ | ------ | \n| Init. Aux-to-Prim 0.5 (paper) | **12.4553** | **10.3271** | **15.5775** | **53.8358** | **85.9042** | 94.4133\n| Init. Aux-to-Prim 0 | 12.5091 | 10.4472 | 15.6169 | 53.3136 | 85.6822 | **94.4466** | \n\nQ2. **Ablation of the search space that only contains primary-to-auxiliary connections.**\n\nIn addition to the ablation in the previous question where **the search space contains bi-directional connections but auxiliary-to-primary links were initialized to 0**, we also perform another ablation following your guidance, where **the search space only contains primary-to-auxiliary connections**. The results are listed below, which exhibit inferior performance w.r.t. our original Aux-FG-NAS. Also notably, the single-side search space setting here performs very much similarly to Q1 where auxiliary-to-primary links were initialized to 0. This outcome comes from similar reasons as discussed in the previous question, i.e., **the model capacity, and the search space, of the single-directional setting are just a subset of our original bi-directional Aux-FG-NAS**.\n\n| NYU v2, Primary: Normal | mean | median | rmse | within 11.25 | within 22.5 | within 30 |\n| ------ | ------ | ------ | ------ | ------ | ------ | ------ | \n| Bi-directional Search Space (paper) | **12.4553** | **10.3271** | **15.5775** | **53.8358** | **85.9042** | **94.4133**\n| Single-directional Prim-to-Aux Search Space | 12.5443 | 10.4538 | 15.6595 | 53.3297 | 85.6684 | 94.2703 | \n\nQ3.  **Are all auxiliary-to-primary connections pruned at the end of training?**\n\nWe have collected the statistics of the auxiliary-to-primary architecture weights for all of our experiments, revealing that **the maximum value of all the auxiliary-to-primary architecture weights is 0 at the precision of 10^-2**.\n\nNotably, all the results reported in our manuscript are the final single-task inference performance, where **we manually set all the auxiliary-to-primary architecture weights to 0 before conducting the evaluation**. This guarantees our objective of ensuring the single-task inference cost."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1619/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700487891055,
                "cdate": 1700487891055,
                "tmdate": 1700488191322,
                "mdate": 1700488191322,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "m352gxj5JV",
                "forum": "cINwAhrgLf",
                "replyto": "zU2HZqCAAq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1619/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1619/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Q4.  **What is the final architecture produced by NAS? How consistent is the final performance across different random seeds and trials?**\n\nIt is indeed interesting to see the final convergence of the learned architecture. \n\nAs discussed in the previous question, the auxiliary-to-primary connections are pruned out with the maximum architecture weights less than 0.01.\n\nRegarding the primary-to-auxiliary connections, as we do not impose any regularization on their architecture weights (and those connections will be removed to maintain a single-task inference regardless how they converge), **those connections typically converge to soft values between 0 and 1**. Therefore, it is difficult to draw them clearly for illustration, given the amount of those connections and many replicate runs. Consequently, we instead gather their statistics of min, max, mean, median of 10 replicate runs with random seeds, and show the standard deviation of those statistics.\n\nSpecifically, we conducted the same experiments as those in Table 5 using ViTBase on NYUv2, with random seeds for 10 replicate runs. The mean and standard deviation of min, max, mean, median statistics of the primary-to-auxiliary architecture weights are shown below:\n\n| NYU v2, Primary: Normal, Prim-to-Aux Arch Weight | min | max | mean | median | \n| ------ | ------ | ------ | ------ | ------ | \n| mean | 0.1561 | 0.5041 | 0.3073 | 0.2725 | \n| std | 0.0110 | 0.0120 | 0.0117 | 0.0150 |\n\nThe above table demonstrates negligible standard deviation, which further produces subtle differences in the final performances, as shown in Question 3 of Reviewer CQEV. We believe that the subtle standard deviation in both the converged architectures and the final performances is ensured by: i) **for the model weights**, our network backbones are initialized by the converged single-task networks, and ii) **for the architectures**, we only search the cross-task connections, rather than altering the network backbones.\n\nWe include the raw primary-to-auxiliary architecture statistics in the following thread. We note that all the results presented here in the rebuttal and in our main text are obtained without specifying any seeds. We will release our training codes also without the specification of seeds. \n\nWe thank this discussion again, and please kindly let us know if any further discussion or clarification is needed."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1619/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700487941062,
                "cdate": 1700487941062,
                "tmdate": 1700556856030,
                "mdate": 1700556856030,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7jCm8cmR1f",
                "forum": "cINwAhrgLf",
                "replyto": "zU2HZqCAAq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1619/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1619/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Raw data for Question 4:"
                    },
                    "comment": {
                        "value": "The raw data of converged **primary-to-auxiliary** architecture weights for Question 4. **The maximum value of all the auxiliary-to-primary architecture weights is 0 at the precision of 10^-2**.\n| NYU v2, Primary: Normal, Prim-to-Aux Arch Weight | min | max | mean | median | \n| ------ | ------ | ------ | ------ | ------ | \n| Trial 1 | 0.161 | 0.534 | 0.299 | 0.275 |\n| Trial 2 | 0.142 | 0.495 | 0.296 | 0.263 |\n| Trial 3 | 0.136 | 0.492 | 0.294 | 0.256 |\n| Trial 4 | 0.151 | 0.503 | 0.303 | 0.261 |\n| Trial 5 | 0.152 | 0.497 | 0.301 | 0.266 |\n| Trial 6 | 0.170 | 0.511 | 0.310 | 0.267 |\n| Trial 7 | 0.158 | 0.504 | 0.317 | 0.284 |\n| Trial 8 | 0.165 | 0.506 | 0.304 | 0.263 |\n| Trial 9 | 0.158 | 0.504 | 0.317 | 0.284 |\n| Trial 10 | 0.168 | 0.495 | 0.332 | 0.306 |"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1619/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700487975096,
                "cdate": 1700487975096,
                "tmdate": 1700556976461,
                "mdate": 1700556976461,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CZvdWs5CE8",
                "forum": "cINwAhrgLf",
                "replyto": "7jCm8cmR1f",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1619/Reviewer_V6nv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1619/Reviewer_V6nv"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the rebuttal.\nWhile most of my concerns are addressed, the weight values of aux-to-prim connections are quite high, considering the usual values of weights. So, I'm quite confused as to how the performance is still preserved after removing connections. How are the weight values of prim-to-prim connections? Relative comparisons will give better picture of what's happening. Also, how much performance drop occurs after removing aux-to-prim connections?"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1619/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700556179117,
                "cdate": 1700556179117,
                "tmdate": 1700556179117,
                "mdate": 1700556179117,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "36BhQIIHPU",
                "forum": "cINwAhrgLf",
                "replyto": "zU2HZqCAAq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1619/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1619/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer,\n\nWe sincerely apologize for the typos in our initial table entry for Q4 and its corresponding raw data table. The correct term should be **Prim-to-Aux Arch Weight** instead of **Aux-to-Prim Arch Weight**. We have rectified these errors. Our original text response of Q4 does not have those typos.\n\nWe gathered the maximum value of all the **Aux-to-Prim** architecture weights, **which is 0 at the precision of 10^-2**, please also refer to a more detailed discussion about the **Aux-to-Prim** architecture weights in Q3.\n\nWe re-evaluated our Aux-FG-NAS model of Table 5, without manually removing the **Aux-to-Prim** connections, the differences are subtle as shown below:\n| NYU v2, Primary: Normal | mean | median | rmse | within 11.25 | within 22.5 | within 30 |\n| ------ | ------ | ------ | ------ | ------ | ------ | ------ | \n| w/ Manual removal of **Aux-to-Prim**  (paper) | 12.4553 | 10.3271 | 15.5775 | 53.8358 | **85.9042** | 94.4133\n| w/o Manual removal of **Aux-to-Prim** | **12.4253** | **10.3101** | **15.5594** | **53.8829** | 85.8975 | **94.4482** | \n\nWe apologize again for the confusion and appreciate the opportunity for further discussion. Please kindly let us know if any additional clarification is needed."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1619/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700558980308,
                "cdate": 1700558980308,
                "tmdate": 1700559046663,
                "mdate": 1700559046663,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yTFS8nyKME",
                "forum": "cINwAhrgLf",
                "replyto": "zU2HZqCAAq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1619/Reviewer_V6nv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1619/Reviewer_V6nv"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the clarification.\nMy concerns have been addressed if the discussions are included in the revised paper.\nAlso, please compare the weights of aux-to-prim connections with those of prim-to-prim to show better contrast."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1619/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700559304032,
                "cdate": 1700559304032,
                "tmdate": 1700559399936,
                "mdate": 1700559399936,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6HC7fYLEns",
                "forum": "cINwAhrgLf",
                "replyto": "7tNYBBJrr6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1619/Reviewer_V6nv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1619/Reviewer_V6nv"
                ],
                "content": {
                    "comment": {
                        "value": "Could you please include the weights of prim-to-prim connections as well as mentioned in my response above? This may help for better understanding of relative importance of weights of networks that remain and weights that are not used during inference."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1619/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700561486951,
                "cdate": 1700561486951,
                "tmdate": 1700561486951,
                "mdate": 1700561486951,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UJv8Ti57OG",
                "forum": "cINwAhrgLf",
                "replyto": "LCzaGpTVCU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1619/Reviewer_V6nv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1619/Reviewer_V6nv"
                ],
                "content": {
                    "comment": {
                        "value": "By prim-to-prim, I meant weights of primary-task network, which are the ones that contribute to the final performance at test time. I just thought it would be interesting to see the ratio of the weight that is being used for inference (i.e., weight of primary-task network) and those being pruned (aux-to-prim connections)."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1619/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700567012484,
                "cdate": 1700567012484,
                "tmdate": 1700567012484,
                "mdate": 1700567012484,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OVw2adzcQv",
                "forum": "cINwAhrgLf",
                "replyto": "zU2HZqCAAq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1619/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1619/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow-ups of the \u201cprim-to-prim connections\u201d issue"
                    },
                    "comment": {
                        "value": "Following-up with our previous response, we would like to clarify that **our search space only includes the cross-task *prim-to-aux* and *aux-to-prim* connections**, we leave the architectures of the single-task backbones intact (equivalent to fix the **prim-to-prim** and **aux-to-aux** architecture weights to 1). This ensures the adaptability of our methods to diverse single-task backbones.\n\nAs acknowledge by Reviewer CQEV, our Aux-FG-NAS can be further extended to search for a better backbone, which can optionally exploits the priors of specific input single-task backbones and/or the characteristic of input tasks. But this falls outside the scope of our current focus, which is to design **a general-purpose auxiliary learning method that adapts to diverse tasks, single-task backbones, datasets, and integrates with various Multi-Task Optimization methods**.\n\nWe hope this well addresses your concerns, please let us know if further discussion is needed and we are happy to engage into that."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1619/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700581993020,
                "cdate": 1700581993020,
                "tmdate": 1700582234687,
                "mdate": 1700582234687,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8Tut4p7ywL",
                "forum": "cINwAhrgLf",
                "replyto": "OVw2adzcQv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1619/Reviewer_V6nv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1619/Reviewer_V6nv"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the clarification. My questions and concerns have been addressed."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1619/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700736011468,
                "cdate": 1700736011468,
                "tmdate": 1700736011468,
                "mdate": 1700736011468,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "opp2nVn4RB",
            "forum": "cINwAhrgLf",
            "replyto": "cINwAhrgLf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1619/Reviewer_CQEV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1619/Reviewer_CQEV"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents an architecture based take on auxiliary learning. They recognize that the asymmetry between the auxiliary and primary tasks can be exploited by learning architectures with constraints that favor transfer of information from the auxiliary to the primary, but in an indirect way so as to minimize the possibility / effect of negative transfer."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The idea is novel and interesting.  I think the use of joint training, followed by the slow trimming of the aux-to-prim connections via L1 regularization is a clever way of more intimately introducing the auxiliary task  but remove it later to avoid needing it during inference.\n2. The paper is clearly written and easy to follow\n3. This has interesting implications for Auxiliary learning based architecture search -- since what was searched for in this paper were connections, there are expansions on this that can focus on other parts of the architecture space."
                },
                "weaknesses": {
                    "value": "1. Method might be a bit too complex / cumbersome to be practically implemented widely -- especially given the size of the gains.\n2. Method also significantly increases memory / compute overhead at training time\n3. The experimental results have no error-bars. It's thus hard to judge the significance of the results\n\n### Nitpicks\n1. The introduction has *a lot* of italicized text, many of which I think are unnecessary and distracting.\n\n### Some relevant papers\nOn gradient conflict\n1. Dery, Lucio M., Yann Dauphin, and David Grangier. \"Auxiliary task update decomposition: The good, the bad and the neutral.\" arXiv preprint arXiv:2108.11346 (2021).\n2. Royer, Amelie, Tijmen Blankevoort, and Babak Ehteshami Bejnordi. \"Scalarization for Multi-Task and Multi-Domain Learning at Scale.\" arXiv preprint arXiv:2310.08910 (2023).\n\nOn NAS-like construction of auxiliary objectives\n1. Dery, Lucio M., et al. \"AANG: Automating Auxiliary Learning.\" arXiv preprint arXiv:2205.14082 (2022)."
                },
                "questions": {
                    "value": "1. Did you try further finetuning the final model on the primary task only after being done with the auxiliary-task based NAS ? This could result in extra performance boost"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1619/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698861286901,
            "cdate": 1698861286901,
            "tmdate": 1699636090352,
            "mdate": 1699636090352,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EWcjyNcB0f",
                "forum": "cINwAhrgLf",
                "replyto": "opp2nVn4RB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1619/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1619/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your constructive comments. We have thoroughly addressed each of the raised questions with detailed responses. We hope those well resolved your concerns. Please kindly let us know if any further discussion or clarification is needed.\n\nQ1.  **Method might be a bit too complex/cumbersome to be practically implemented widely -- especially given the size of the gains.**\n\nWe proposed two general-purpose auxiliary learning methods. The first one, Aux-G, is very simple, involving only the direct integration of the primary and auxiliary networks through primary-to-auxiliary connections. The combined network weights can then be trained directly using gradients, akin to training most neural networks with fixed architecture.\n\nThe second method, Aux-FG-NAS, leverages Neural Architecture Search for further improved performance. Notably, the chosen NAS optimization method, DARTS, is also easy to implement, **requiring only one additional forward-backward pass in each iteration to train the architecture**. As shown in our pseudo codes provided above, such an additional step can be implemented with **just 7 lines of codes in PyTorch**. We will release the complete training and evaluation codes upon acceptance.\n\nRegarding the assessment of gains which can be subjective, it is important to acknowledge the challenge of designing a general-purpose auxiliary learning framework that seamlessly adapts to diverse tasks, network backbones, datasets, and integrates with various Multi-Task Optimization methods. Notably, in many of our experiments, the improvement achieved by our method versus the previous SOTA surpasses that of the previous SOTA versus the simple Aux-Head baseline. We also appreciate your acknowledgment in Strength 3 regarding the potential of our method to further exploit the task prior in network backbones for additional enhancements.\n\nQ2.  **Method also significantly increases memory / compute overhead at training time.**\n\n**The training memory / compute complexity for both of our methods scales to multiple auxiliary tasks linearly**, which aligns with the Soft-Parameter Sharing Multi-Task Architectures (SPS-MTA) and most of Multi-Task Optimizations (MTO) that employ gradient manipulation (Note that MTO with gradient manipulation has linear instead of constant scalability because their training procedure requires to retain K copies of backward graph for storing and manipulating the gradients from K tasks).\n\nSpecifically, given K auxiliary tasks, denote model weights as $w$ and architecture weights as $\\alpha$, **our Aux-G method has the same training complexity as that of SPS-MTA methods and most MTO methods**. \n\n**While for our Aux-FG-NAS, the training complexity is** $O((K+1)|w| + K|\\alpha|)$. This linear scalability is achieved by the efficient NAS optimization of DARTs [1], as detailed in Eq. 8 of [1], and is confirmed by our pseudo codes provided in the above response.\n\nFurther details of our Aux-FG-NAS complexity can be found in Sect. 4.2.4 and Appendix A of our updated manuscript (Appendix A was originally in a separate supplementary file at the time of the initial submission).\n\nIt's noteworthy that our method is primarily designed to address practical scenarios where **the inference complexity takes precedence over the training complexity**. Such scenarios are prevalent in customer-facing web applications where the model is only trained for a few times, then deployed online and subjected to million to billion inferences. While for the scenarios where the training resource is also constrained, we can at least use the Aux-G method with the same training complexity as SPS-MTA and most MTO methods, so as to ensure the improvement leveraging auxiliary tasks.\n\nQ3.  **Error bar in the results.**\n\nFollowing your guidance, we perform 10 replicate runs with random seeds of the ViTBase experiments on NYU v2, as those in Table 5 of our manuscript. We did not observe a substantial standard deviation in the results:\n\n| NYU v2, Primary: Normal | mean | median | rmse | within 11.25 | within 22.5 | within 30 |\n| ------ | ------ | ------ | ------ | ------ | ------ | ------ | \n| mean | 12.5656 | 10.4620 | 15.6895 | 53.3011 | 85.6956 | 94.2751 | \n| std | 0.0479 | 0.0323 | 0.0702 | 0.1341 | 0.2018 | 0.1475 | \n\nWe believe that the subtle standard deviation in the replicate runs is ensured by: i) **for the model weights**, our network backbones are initialized by the converged single-task networks, and ii) **for the architectures**, we only search the cross-task connections, without altering the network backbones.\nNotably, all the results presented in our manuscripts and here in the rebuttal are obtained with random seeds. Additionally, our training codes will also be released without any specification of seeds.\nWe put the raw results of the 10 replicate runs in the following thread. \nWe thank this suggestion again and hope this addressed your concern."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1619/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700487661636,
                "cdate": 1700487661636,
                "tmdate": 1700487661636,
                "mdate": 1700487661636,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wDg0ekEV3C",
                "forum": "cINwAhrgLf",
                "replyto": "opp2nVn4RB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1619/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1619/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Q4.  **The unnecessary italicized text in the introduction.**\n\nWe thank for pointing this out. We have removed unnecessary italicized text, please see our updated manuscript.\n\nQ5.  **Relevant papers [4,5,6].**\n\nWe appreciate for guiding us to those relevant papers, this certainly makes our taxonomy in Table 1 more comprehensive and self-contained. We have cited [4,6] in MTL & AL Methods - Optimization-based - For AL, and [5] in MTL & AL Methods - Optimization-based - For MTL, in Table 1. We also cite and discuss them in our related work section. Thanks again for pointing them out!\n\nQ6.  **Further finetune the final model on the primary task after NAS converges.**\n\nAs instructed, we continue to finetune the Aux-FG-NAS weights of Table 5 (i.e., the ViTBase NYUv2 experiment). We use the same learning rate and the same optimizer as those used to further train the converged single-task initializations. The results are presented below, showing a slight (but not significant) improvement over the original results. We thank this advice again.\n\n| NYU v2, Primary: Normal | mean | median | rmse | within 11.25 | within 22.5 | within 30 |\n| ------ | ------ | ------ | ------ | ------ | ------ | ------ | \n| Org results (paper) | 12.4553 | 10.3271 | 15.5775 | 53.8358 | **85.9042** | 94.4133\n| Further finetuned results | **12.3787** | **10.3074** | **15.4840** | **53.9111** | 85.8241 | **94.5066** | \n\n\n**References**\n\n[1] Hanxiao Liu, Karen Simonyan, and Yiming Yang. DARTS: Differentiable architecture search. In ICLR, 2019.\n\n[2] Sirui Xie, Hehui Zheng, Chunxiao Liu, Liang Lin. SNAS: Stochastic Neural Architecture Search. In ICLR, 2019.\n\n[3] Guohao Li, Guocheng Qian, Itzel C. Delgadillo, Matthias M\u00fcller, Ali Thabet, Bernard Ghanem. SGAS: Sequential Greedy Architecture Search. In CVPR, 2020.\n\n[4] Lucio M. Dery, Yann Dauphin, David Grangier. Auxiliary task update decomposition: the good, the bad and the neutral. In ICLR, 2021.\n\n[5] Amelie Royer, Tijmen Blankevoort, Babak Ehteshami Bejnordi. Scalarization for Multi-Task and Multi-Domain Learning at Scale. In NeurIPS, 2023.\n\n[6] Lucio M. Dery, Paul Michel, Mikhail Khodak, Graham Neubig, Ameet Talwalkar. AANG: Automating Auxiliary Learning. In ICLR, 2023."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1619/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700487723133,
                "cdate": 1700487723133,
                "tmdate": 1700488340564,
                "mdate": 1700488340564,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vJptxIlGd3",
                "forum": "cINwAhrgLf",
                "replyto": "wDg0ekEV3C",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1619/Reviewer_CQEV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1619/Reviewer_CQEV"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the detailed response !"
                    },
                    "comment": {
                        "value": "Thanks for responding to all my questions !\nI have no further clarification questions"
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1619/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700594545032,
                "cdate": 1700594545032,
                "tmdate": 1700594545032,
                "mdate": 1700594545032,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "uCTzTFo7Vf",
            "forum": "cINwAhrgLf",
            "replyto": "cINwAhrgLf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1619/Reviewer_fF8U"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1619/Reviewer_fF8U"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies how to harness additional auxiliary labels from an auxiliary task to elevate the performance of the main task without escalating the inference cost. To do so, the authors propose to employ individual networks for different tasks and only regularize the main task with the auxiliary task\u2019s gradient. It\u2019s understandable that this act allows the network trained on the auxiliary task to be completely pruned during inference. Furthermore, the authors propose to search for the most appropriate structure that satisfies the previously mentioned constraint with NAS. The paper accentuates its methodology's compatibility with prevailing optimization-based auxiliary learning techniques. The empirical validation, evident from experiments on NYU v2, CityScapes, and Taskonomy datasets using well-known backbones like VGG-16, ResNet-50, and ViT-B, demonstrates the efficacy of the proposed method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "++ The paper is well-written with clearly motivated arguments and insights. The auxiliary learning task is also meaningful when we only seek to boost one task with another and aim at quick inference. \n\n++ Table 1 provides a comprehensive understanding and meticulous survey of the field. The authors offer an exhaustive overview of both Multi-Task Learning (MTL) and Auxiliary Learning (AL) methods. Authors have incorporated a wide range of references from multiple years, indicating a holistic survey of both seminal works and recent advancements. The inclusion of their method alongside existing techniques also provides clarity on its positioning within the broader research landscape. \n\n++ The proposed method is backbone- and task-agnostic that is applicable to multiple backbones and tasks."
                },
                "weaknesses": {
                    "value": "-- I am not very familiar with auxiliary learning. However, I do think one baseline might be meaningful, which is to share a single backbone while projecting the gradient of the auxiliary task to the orthogonal direction of the main task on all (or selective) layers. This baseline also has no inference lag while exploiting the auxiliary objective signals. \n\n-- The authors use NAS to search for suitable architectures to optimize the main task's objective. However, distinct backbones are used for each task, and their weights can vary significantly. I'm uncertain why \"stitching\" two backbones with different weights and objectives is logical. Are there any supporting theories or references?\n\n-- The authors claim that their method achieves \"promising performance.\" However, based on Tables 3 and 4, it appears that the performance gain of the proposed method is only marginal. Considering the additional training costs in the NAS search and optimization, I am not sure whether the loss in training efficiency is worth it."
                },
                "questions": {
                    "value": "See weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1619/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698913960095,
            "cdate": 1698913960095,
            "tmdate": 1699636090293,
            "mdate": 1699636090293,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "x1BTAZwOHj",
                "forum": "cINwAhrgLf",
                "replyto": "uCTzTFo7Vf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1619/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1619/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your constructive comments. We have thoroughly addressed each of the raised questions with detailed responses. We hope those well resolved your concerns. Please kindly let us know if any further discussion or clarification is needed.\n\nQ1.  **The baseline of sharing a single backbone, while projecting the gradient of the auxiliary task according to the main task on all (or selective) layers.**\n\nThe suggested method is indeed a stronger baseline by using Multi-Task Optimization techniques. In fact, the suggested method is exactly what we refer to as **PCGrad-Aux** in Table 2. Specifically, we extend the original PCGrad [1] to auxiliary learning in PCGrad-Aux exactly by the way as suggested, i.e., fixing the primary gradient and projecting all auxiliary gradients w.r.t. the primary gradient, instead of randomly choosing a task for projection in the original PCGrad [1].\n\nMoreover, we also validate our method using another Multi-Task Optimization method specifically designed for auxiliary learning named **GCS** [2]. In GCS, the auxiliary gradients are applied only if they exhibit a non-negative cosine similarity with the primary gradients; otherwise, GCS discards the auxiliary gradients.\n\nThe experimental results w.r.t. the auxiliary learning-based Multi-Task Optimization PCGrad-Aux and GCS from Table 2 are:\n\n| NYU v2, Primary: Seg | Aux-Head | Aux-Head + PCGrad-Aux | Aux-Head + GCS | Aux-FG-NAS | Aux-FG-NAS + PCGrad-Aux | Aux-FG-NAS + GCS |\n| ------ | ------ | ------ | ------ | ------ | ------ | ------ | \n| mIoU | 34.7 | 35.2 | 35.3 | 36.0 | 36.2 | 36.3 |\n| PAcc | 65.4 | 65.7 | 65.9 | 66.1 | 66.3 | 66.5 |\n\nOur results, as presented above and in Table 2, demonstrate that i\uff09**Our vanilla method surpasses those Multi-Task Optimization methods**, i.e., Aux-FG-NAS vs. Aux-Head + PCGrad-Aux & Aux-Head + GCS. ii) Importantly, **our method is seamlessly incorporable with these Multi-Task Optimization methods**, yielding further improvement in results, as demonstrated by Aux-FG-NAS vs. Aux-FG-NAS + PCGrad-Aux & Aux-FG-NAS + GCS. \n\nWe note that the second point was also acknowledged by Reviewers V6nv, Qzsf, fJaD. Therefore, we focus on comparing our method with other Multi-Task Architecture methods in the subsequent experiments to avoid potential distractions.\nWe thank this discussion again and hope this resolved your concern.\n\nQ2.  **Why \"stitching\" two backbones with different weights and objectives is logical? Are there any supporting theories or references?**\n\nOur method stitches the primary and the auxiliary networks with different weights and objectives, where **the auxiliary networks can be regarded as a \"*loss function*\" to provide gradients, as well as a feature generator to offer features from another view** (though those auxiliary features are pruned out after convergence). Moreover, during the training, **we refrain from imposing any regularization that enforces similarity between the weights of the primary and the auxiliary networks**. This deliberate choice allows us to stitch the primary and auxiliary networks with different weights and objectives, enabling the auxiliary networks to contribute unique \"*losses*\" and \"*features*\" distinct from the primary network.\n\nWe also note that the practice of stitching two backbones with different weights and objectives has a longstanding history in architecture-based MTL [3,4,5,6], where one of the seminal works happened to be termed as *cross-stitching networks* [3].\n\nQ3.  **The marginal improvement of Table 3 and Table 4 given the loss in training efficiency.**\n\nWhile the assessment of the improvement can be subjective, we would like to highlight that **our primary goal is to establish a general-purpose auxiliary-learning framework**. This framework is designed to be seamlessly incorporable with diverse tasks, network backbones, datasets, and various Multi-Task Optimization methods, in a plug-and-play manner. In pursuit of this generality, **we specifically concentrate on the cross-task connections while leaving the network backbone intact**. As acknowledged by Reviewer CQEV (Strength 3), **we have the potential to leverage the task priors, optionally by searching the task backbones, to further enhance the performance**.\n\nMoreover, it is worth noting that in Tables 3 and 4, the improvement of the previous SOTA over the simple Aux-Head baseline is notably less significant than that of our method over the previous SOTA. This observation also underscores the inherent challenges in designing a general-purpose auxiliary learning framework.\n\nAs for the training efficiency, please also kindly refer to the above pseudo codes and Question 2 of Reviewer CQEV."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1619/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700487396578,
                "cdate": 1700487396578,
                "tmdate": 1700488457562,
                "mdate": 1700488457562,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]