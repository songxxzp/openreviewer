[
    {
        "title": "Sufficient conditions for offline reactivation in recurrent neural networks"
    },
    {
        "review": {
            "id": "tFpDyO3Yej",
            "forum": "RVrINT6MT7",
            "replyto": "RVrINT6MT7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8388/Reviewer_NzXs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8388/Reviewer_NzXs"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a training algorithm for continuous-time RNN integration in the presence of noise that relies on an upper bound for the loss that is split into a denoising and a tracking part. It is then shown for a angular and a motion velocity integration task that the noise compensation dynamics induce diffusive reactivations in a quiescent state."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper introduces a novel method of task-optimization for RNNs that rely on an upper bound for the loss that is split into two parts: the first term optimizes for denoising and the second term is optimized for tracking the signal. This is the first paper that discusses the contribution of learning in the presence of noise to have implication to reactivations.\nThrough this framework a new perspective is proposed on reactivations in the brain being the consequence of the presence of noise.\nFinally, this framework provides interpretability to the system that solves an integration task in terms of what part of the dynamics corresponds to denoising the systems and which to integrating instantaneous changes of the state variable.\nThe effectiveness of the approach is demonstrated against a couple of integration tasks highlighting its applicability. The theorems and proofs are sound.\nFurthermore, the paper is well written and the contributions are clearly explained with possible implications of the work for neuroscience.\nThese implications for neuroscience might provide a new perspective on how neural dynamics that implements neural integration might be decomposed into a denoising and an integrating component and how networks that learn in the presence of noise then exhibit reactivations."
                },
                "weaknesses": {
                    "value": "It is unclear what is meant with tuning of the value of $\\tau$. Is this for a fixed noise level? Or is it chosen as the optimal for all used noise levels for training?\n\n\\paragraph{Figures}\nUse some transparency in Figure 1 f and e so that overlapping points are visible. At the moment it is unclear what proportion of the points is hidden behind the first layer and what their distribution is.\n\n\n\\paragraph{Contribution}\nThe the demonstration contribution of this framework could benefit from some additional experiments.\nComparison to training without noise and added noise in quiescent state is missing.\nComparison to other ring attractor/head direction models is missing.\n\nFinally, some of the implications of the proposed framework require more justification.\nThe conclusion that reverse replay could be explained by diffusive reactivations seems like a stretch and should be substantiated better. \nAlso, reactivations such as replay are not the type of reactivations that are found in the networks in the paper (this is admitted in the paper) even though the introduction is discussing those for a large part.\n\n\\paragraph{Noise}\nThe fact that training without noise resulted in erratic output trajectories might be explained by the statistics of the input. \nDoes the used input static reflect best what happens in animal behavior?\nIt would be good to compare the statistics of state sequences and reactivation sequence on the level of the sequences themselves.\nBecause in terms of optimality of exploration erratic trajectories might be more optimal to fully explore, see for example McNamee (2021). But see also Supplementary Figure A.1 (a) vs (e) that seems to show that a network that  has been trained without noise (e) explores a bigger part of the state space than a network that has been trained in the presence of noise (a).\n\nFinally, the claim that even with the addition of noise the failure of exploration could not be corrected (page 8, middle) should be better substantiated with a comparison based on distributions rather than just example trajectories (currently Suppl.Fi. A.1e-f is used as justification).\n\n\nMcNamee, D.C., Stachenfeld, K.L., Botvinick, M.M. and Gershman, S.J., 2021. Flexible modulation of sequence generation in the entorhinal\u2013hippocampal system. Nature neuroscience, 24(6), pp.851-862."
                },
                "questions": {
                    "value": "How is the bias exactly defined for figure 2?\nIn Figure 2, what distribution are random networks sampled from? Further, what does it mean that a random network s trained and tested in the presence of noise (as would be implied in the column $\\text{R}^\\sigma$).\n\n\nWhy is the kernel density estimate the best (or a good) way to compare trained networks?\n\nHow does the final trained network in terms of its parameters compare to the full greedily optimal dynamics?\nHow do different trained networks compare to each other in terms of the reactivation statistics?\n\nIs it truly a ring attractor that solves the angular integration task? On the given time scale it could be the case that the solution is provided by a line attractor that gets mapped to the ring by $D$.\n\nIn Equation (17) is the optimum unique?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8388/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8388/Reviewer_NzXs",
                        "ICLR.cc/2024/Conference/Submission8388/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8388/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698591734329,
            "cdate": 1698591734329,
            "tmdate": 1700690623024,
            "mdate": 1700690623024,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DkpOUTmPJv",
                "forum": "RVrINT6MT7",
                "replyto": "tFpDyO3Yej",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8388/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8388/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer NzXs"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for their review and helpful suggestions. We appreciate that the reviewer found our theory to be novel, providing interpretability, and an interesting new perspective on reactivations. We are glad that the reviewer found the paper to be well-written and our contributions clearly explained. We have addressed the points and questions in the review below:\n\n> It is unclear what is meant with tuning of the value of \\tau. Is this for a fixed noise level? Or is it chosen as the optimal for all used noise levels for training?\n\n**Response:** We choose $\\tau$ for the RNN based on the task to ensure it performs optimally, and it remains the same for all noise levels. The intention is to ensure that the network is faster than the inputs or else it does not react quick enough to new inputs, causing the activity to remain stuck and the performance to be poor. In practice, we do not have to tune $\\tau$ very carefully, we set it to a reasonable value that ensures that the network's dynamics are fast enough to process the task's inputs and perform optimally. We clarify this point in the revised manuscript.\n\n> Use some transparency in Figure 1 f and e so that overlapping points are visible. At the moment it is unclear what proportion of the points is hidden behind the first layer and what their distribution is.\n\n**Response:** We apologize if the plot was not clear. We tried adding transparency to these plots but they did not aid in visualizing the distribution better, due to the interaction with different shades of color that attempt to show the structure of the attractor manifold, and its correspondence with the environment.\n\nNevertheless, this is why we provide the kernel density estimates below! The KDE provides an actual distribution on the data, i.e., a principled visualization of the distributions, rather than requiring visual estimation based on occupied points. We hope that this plot aids the reviewer in comparing the distributions.\n\n> The the demonstration contribution of this framework could benefit from some additional experiments. Comparison to training without noise and added noise in quiescent state is missing. Comparison to other ring attractor/head direction models is missing.\n\n**Response:**  We apologize if this was unclear from the text. We have compared to training without noise and with added noise in quiescent state in Suppl. Fig. A.2e-f. We have shown in this experiment that these trajectories are erratic, and do not properly correct for lack of exploration arising from training without noise. In an additional experiment, we have shown that the point-to-point distances are much higher for these erratic trajectories than even waking behavior, indicating their implausibility.\n\nIt is true that we don't compare to more traditional ring attractor models, with which we have some key differences. The critical distinction is that our model recapitulates statistics of waking navigation, whereas networks with hand-tuned weights, for instance, would only be able to produce a uniform distribution over the attractor manifold in question.\n\n> Finally, some of the implications of the proposed framework require more justification. The conclusion that reverse replay could be explained by diffusive reactivations seems like a stretch and should be substantiated better. Also, reactivations such as replay are not the type of reactivations that are found in the networks in the paper (this is admitted in the paper) even though the introduction is discussing those for a large part.\n\n**Response:** We apologize if our claims were not clear. In our mathematical results, we are only claiming that the steady-state distribution of quiescent activity matches that of waking activity. Specifically, we do not make any comment on the sequential dynamics of quiescent trajectories themselves. Thus, for reverse replay, the only point we make is that for true diffusive rehearsal, reversed trajectories are as probable as forward trajectories. In additional experiments with our RNNs on the head direction task (Suppl. Fig. A.5), we biased the waking moment-to-moment transition structure to be counter-clockwise. We found that quiescent trajectories partially recapitulated the transition structure of active phase trajectories, and maintained their bearing for longer periods, resembling real neural activity more closely. The biased velocities were also reflected during quiescence, although less clearly than waking. We do not yet have theoretical justification for this. However, we note that reversals in the trajectories still remain possible and do occur (Suppl. Fig. A.5)."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8388/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700251775939,
                "cdate": 1700251775939,
                "tmdate": 1700251775939,
                "mdate": 1700251775939,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OThCRx6vlE",
                "forum": "RVrINT6MT7",
                "replyto": "tFpDyO3Yej",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8388/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8388/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Following up with the reviewer"
                    },
                    "comment": {
                        "value": "Dear Reviewer NzXs,\n\nWe believe our response has addressed all of the reviewers' concerns and would be grateful if the reviewer would consider raising their score. As the discussion period is coming to an end soon, we would greatly appreciate a response from the reviewer. If the reviewer has any further questions, we would be happy to address them. We thank the reviewer again for their helpful suggestions and eagerly look forward to a response."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8388/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700500603617,
                "cdate": 1700500603617,
                "tmdate": 1700500603617,
                "mdate": 1700500603617,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FUhWULEVzO",
                "forum": "RVrINT6MT7",
                "replyto": "tFpDyO3Yej",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8388/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8388/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Discussion period coming to an end"
                    },
                    "comment": {
                        "value": "Dear Reviewer NzXs,\n\nThis is a gentle reminder that the discussion period with authors is coming to an end (<20 hours). We believe our responses in the comments and updated manuscript have addressed all of the reviewers' concerns, and we would be grateful if the reviewer would consider raising their score. If the reviewer has any further questions, we would be happy to address them in the short time remaining. We thank the reviewer again for their helpful suggestions and eagerly look forward to a response."
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8388/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700668654231,
                "cdate": 1700668654231,
                "tmdate": 1700668654231,
                "mdate": 1700668654231,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5evnGovbJb",
                "forum": "RVrINT6MT7",
                "replyto": "FUhWULEVzO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8388/Reviewer_NzXs"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8388/Reviewer_NzXs"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the response. I appreciate the additional details and the new version is taking steps in the right direction.  I have raised my score.\n\nSome remaining concerns:\n>  for reverse replay, the only point we make is that for true diffusive rehearsal, reversed trajectories are as probable as forward trajectories.\n\nBut both would be very unlikely compared to diffusion type dynamics.\n\n\n\n> Is it truly a ring attractor that solves the angular integration task?\n\nThe authors claim that \"the networks learned a ring attractor without issue\" but it is unclear how that is \n\nHow do you know that the network has periodic boundary conditions? It seems that the networks has only been run on finite time trials, which could leave open the opportunity of a (long) curved line attractor being mapped onto a ring."
                    }
                },
                "number": 27,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8388/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700690638164,
                "cdate": 1700690638164,
                "tmdate": 1700690638164,
                "mdate": 1700690638164,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "c1V4ocSchE",
            "forum": "RVrINT6MT7",
            "replyto": "RVrINT6MT7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8388/Reviewer_aZpo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8388/Reviewer_aZpo"
            ],
            "content": {
                "summary": {
                    "value": "The authors demonstrate that training network dynamics on an upper bound of the cost function for learning a function of an input stimulus in the presence of network noise results in network dynamics that separate into a denoising component and a signal integration component.  Furthermore, they provide conditions such that, in the absence\tof input, the network will reproduce typical states of the (learned function of the) input stimulus.  Notably, this strictly requires the input noise to be different in the quiescent vs input-driven cases.  The authors demonstrate numerical simulations in which networks reflect these results."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper does a nice job of outlining how the cost function can be deconstructed into two components, the denoising component and the signal integration component.  The idea of replay being a result of signal integration in the presence of noise is an interesting one, and the authors present a simple mechanism whereby this can arise. This is a contribution to an important question in neuroscience.  The theoretical approach, results, and the numerical experiments are well described and supported."
                },
                "weaknesses": {
                    "value": "The paper is potentially confusing to some readers in that the presentation (and the numerical experiments if I have followed) regard training a network with a fixed nonlinearity and trainable weight parameters (i.e. a typical RNN) but the theoretical analysis considers not a fixed nonlinearity, but rather the space of all possible dynamics that could govern the system.  I suspect this may misdirect some readers regarding the basic argument (possibly me as well).  \n\nTraining a regular RNN on the cost function, L, of course, does not guarantee anything about the cost function L_upper, so one must make some additional assumptions about the structure of the network dynamics, namely something akin to demanding that the space of permissible dynamics admits the derived minimum of L_upper.  In the case of the numerical experiments, the authors use ReLU and decompose the input to the nonlinearity into a recurrent part and a signal integration part, which should approximately allow for the decomposition that leads to L_upper.  One needs to add an argument that this decomposition is reasonable for certain classes of RNNs. E.g. If the network is in the active regime, the ReLU doesn't contribute much and then the composition amounts to considering the activity updates from the recurrent part and the input part separately.  One could make a similar argument about sigmoids.  In short, the authors should, I think, be direct about the assumptions on the RNN necessary for their theory. \n\nRegarding the quiescent state, I think the authors are perhaps a little too cavalier with the fact that, strictly speaking, the noise term must change in order to guarantee replay.  As presented at the moment, it feels to this reader like a weakness that is not adequately addressed.  The authors should either supply 1) a reason to believe this sort of change happens in real networks or 2) a demonstration that the deviations are not sufficient to qualitatively change the main result.  There is also the possibility that this is an opportunity for the authors.  The deviations from perfect replay if the noise does not change amount to a prediction, and one which may have computational advantages (e.g. facilitating exploration in terms of a planning algorithm)."
                },
                "questions": {
                    "value": "Last sentence of the introduction:  I think \"ecologically\" should be \"ethnologically\"\n\nQuestions:\n\n- Does this work for more general RNNs (more general nonlinearities, interactions between recurrence and signal integration, etc.)?\n\n- Is there reason to believe the noise distribution changes in the quiescent state?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8388/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8388/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8388/Reviewer_aZpo"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8388/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698696727603,
            "cdate": 1698696727603,
            "tmdate": 1699637044247,
            "mdate": 1699637044247,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "v5Dk3nrhfa",
                "forum": "RVrINT6MT7",
                "replyto": "c1V4ocSchE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8388/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8388/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer aZpo"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for their review and helpful suggestions. We appreciate that the reviewer found our theory and experiments to be interesting, well-described and well-supported. We have addressed the points and questions in the review below:\n\n> The paper is potentially confusing to some readers in that the presentation (and the numerical experiments if I have followed) regard training a network with a fixed nonlinearity and trainable weight parameters (i.e. a typical RNN) but the theoretical analysis considers not a fixed nonlinearity, but rather the space of all possible dynamics that could govern the system. I suspect this may misdirect some readers regarding the basic argument (possibly me as well).\n\n**Response:** We apologize if this was not clear. The idea is that the RNNs trained optimally are approximating the ground-truth optimal solution, given by the greedily optimal dynamics (Section 2.3). It is indeed reasonable for an RNN to implement Eq. 18\u2014according to the literature on RNNs being universal function approximators (Funahashi & Nakamura, 1993). We are thus assuming that our trained RNNs are near the optimum, and our empirical results indicate that they are. Our theoretical results do not make any assumptions about the architecture of the RNN (in fact we have added simulations with GRUs, Suppl. Fig. A.4), only that it is trained to be optimal. Indeed we see the Langevin sampling during quiescence leading to the distributional match. It is certainly possible for suboptimal (or very small) RNNs to fail to align with the theory, the theory only applies for very well-trained networks that are performing near-optimally such as our networks.\n\nFunahashi & Nakamura. Approximation of dynamical systems by continuous time recurrent neural networks. Neural Networks 6 (1993): 801-806.\n\n> Training a regular RNN on the cost function [...] In short, the authors should, I think, be direct about the assumptions on the RNN necessary for their theory.\n\n**Response:** We apologize if this was not clear. We would like to state that our mathematical analysis is a theory for high capacity networks capable of legitimately approximating the optimal solution. RNNs are capable of doing this, as they are universal function approximators (Funahashi & Nakamura, 1993) as long as the activation function is nonlinear. We would like to draw attention to a new experiment we have included with GRUs thanks to the reviewer's suggestion, to highlight the universality of our results. We found that our results do hold even with continuous-time GRUs (Suppl. Fig. A.4), showing that these reactivation phenomena are not unique to a particular network architecture or activation function. We add a discussion about this important point in the revised manuscript.\n\nFunahashi & Nakamura. Approximation of dynamical systems by continuous time recurrent neural networks. Neural Networks 6 (1993): 801-806.\n\n> Regarding the quiescent state, I think the authors are perhaps a little too cavalier with the fact that, strictly speaking, the noise term must change in order to guarantee replay. As presented at the moment, it feels to this reader like a weakness that is not adequately addressed. The authors should either supply 1) a reason to believe this sort of change happens in real networks or 2) a demonstration that the deviations are not sufficient to qualitatively change the main result. There is also the possibility that this is an opportunity for the authors. The deviations from perfect replay if the noise does not change amount to a prediction, and one which may have computational advantages (e.g. facilitating exploration in terms of a planning algorithm).\n\n**Response:** We apologize if there has been a misunderstanding, but the increase in noise variance is **not required** to see reactivation. It is only required for an **exact** distributional match for the Langevin sampling. We demonstrate in Suppl. Fig. A.1 (in the updated manuscript) that reactivation occurs and matches the waking behaviour even when the noise variance remains the same during quiescence. The noise variance merely behaves like a temperature parameter for the sampling, reactivation is not precluded if it stays at the same level. As the reviewer points out, there may indeed be related computational advantages which future work could explore. We thank the reviewer for raising this point and the suggestion for the additional demonstration, which we believe improves our work."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8388/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700251391173,
                "cdate": 1700251391173,
                "tmdate": 1700476379184,
                "mdate": 1700476379184,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "H2wiFxguhb",
                "forum": "RVrINT6MT7",
                "replyto": "c1V4ocSchE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8388/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8388/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Following up with the reviewer"
                    },
                    "comment": {
                        "value": "Dear Reviewer aZpo,\n\nWe believe our response has addressed all of the reviewers' concerns and would be grateful if the reviewer would consider raising their score. As the discussion period is coming to an end soon, we would greatly appreciate a response from the reviewer. If the reviewer has any further questions, we would be happy to address them. We thank the reviewer again for their helpful suggestions and eagerly look forward to a response."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8388/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700500574351,
                "cdate": 1700500574351,
                "tmdate": 1700500574351,
                "mdate": 1700500574351,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lXdlsSqJfp",
                "forum": "RVrINT6MT7",
                "replyto": "c1V4ocSchE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8388/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8388/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Discussion period ending soon"
                    },
                    "comment": {
                        "value": "Dear Reviewer aZpo,\n\nThis is a gentle reminder that the discussion period with authors is coming to an end (<20 hours). We believe our responses in the comments and updated manuscript have addressed all of the reviewers' concerns, and we would be grateful if the reviewer would consider raising their score. If the reviewer has any further questions, we would be happy to address them in the short time remaining. We thank the reviewer again for their helpful suggestions and eagerly look forward to a response."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8388/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700668615657,
                "cdate": 1700668615657,
                "tmdate": 1700668615657,
                "mdate": 1700668615657,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ar1yNMo7a2",
                "forum": "RVrINT6MT7",
                "replyto": "lXdlsSqJfp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8388/Reviewer_aZpo"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8388/Reviewer_aZpo"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the thoughtful responses.  I don't think the authors have addressed my primary concerns, which involve the connection between their empirical training and the analytic bound they consider.  I don't see any arguments that provide reasonable guarantees that the two cost functions under consideration will be approximations of each other without additional assumptions.  \n\nI also understood that changing the noise variance was not required, but this issue is dealt with rather cavalierly in the paper itself and would lead a reasonable reader to think something had been brushed under the rug.  The paper would be strengthened with some more discussion (or examples) of what happens for other choices of noise variance.  The new figure is helpful in this regard.  \n\nThe paper is quite nice and I remain overall of positive opinion about it."
                    }
                },
                "number": 29,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8388/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700709842031,
                "cdate": 1700709842031,
                "tmdate": 1700709842031,
                "mdate": 1700709842031,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QTZ2umIavx",
            "forum": "RVrINT6MT7",
            "replyto": "RVrINT6MT7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8388/Reviewer_pnLC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8388/Reviewer_pnLC"
            ],
            "content": {
                "summary": {
                    "value": "The authors develop a theoretical framework for understand why offline reactivation occurs in recurrent neural networks. In particular, the authors argue that neural noise during awake states is essential for the emergence of faithful and varied reactivation of neural trajectories during quiscience (i.e. in absence of external stimuli)."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- the paper is generally well written\n- the mathematical theory is well presented and interesting (as far as I'm aware it is novel, but I am not certain of this)\n- offline reactivation should be of interest both to machine learning and neuroscience researchers; the paper does make some theoretical and experimental headway into why/how it occurs"
                },
                "weaknesses": {
                    "value": "- perhaps this is not in the scope of the paper, but the authors do not provide any theoretical/numerical support for the functional benefit of offline reactivation. The authors mention other works which demonstrate that it may aid the formation of long-term memories, schema, planning. Do the theoretical/numerical results in this paper support these possibilities, or any one in particular?\n- Relatedly, a key feature of this study is that the RNNs considered are noisy during awake states. It's unclear to me exactly why this is necessary. The authors \"found that even trajectories generated by networks that were not trained in the presence of noise, and also were not driven by noise in the quiescent phase, still generated quiescent activity distributions that corresponded well to the active phase distributions\"; is the point that there's more space exploration with noise, giving rise to a functional benefit? Or that somehow the introduction of noise better captures observed neural data?\n- I found the link between the theoretically optimal solution for update dynamics and the actual application of RNNs somewhat unclear. Are we to take that (presumably backprop-trained) RNNs employ update dynamics similarly to the theoretical solution; is it biologically reasonable for e.g. equation 18 to be implement by an RNN? If so, is it possible to show that they do; e.g. by comparing these recurrent inputs to the optimal solution? If not, are the experimental results relevant to the theory? \n- Relatedly, perhaps this is overly harsh, but I also do not find the result that RNNs without stimuli visit similar states as with stimuli very surprising.\n- The authors suggest an interesting discrepancy in experimental prediction for their noise-based theory of offline reactivation and generative modeling; specifically, that \"while generative models necessarily recapitulate the moment-to-moment transition statistics of sensory data, our approach only predicts that the stationary distribution will be identical\". I am confused by this statement and not sure if it is true. Is that to say that the proposed model would not encode sensory transitions but would simply replicate the overall probability distribution of sensory states? I would find this surprising for an RNN whose recurrent weights are trained on sensory data; that is, I would suppose that the RNN weights would themselves somehow capture transition statistics of the task variables (e.g. maybe this is valuable as a kind of denoising effect on noisy observations). Perhaps I am wrong though"
                },
                "questions": {
                    "value": "- In section 2.1 it may be relatively obvious that p denotes probability, but I would still clarify this\n- In equation 15 it's a bit odd to provide a new definition for L_{noise} having just defined it previousply with the D term involved\n- the delta t is given as 0.02. What's the unit, seconds?\n- I would recommend a limitations section"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8388/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8388/Reviewer_pnLC",
                        "ICLR.cc/2024/Conference/Submission8388/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8388/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698765735375,
            "cdate": 1698765735375,
            "tmdate": 1700394302950,
            "mdate": 1700394302950,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "h1vnHY5MFb",
                "forum": "RVrINT6MT7",
                "replyto": "QTZ2umIavx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8388/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8388/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer pnLC"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their detailed review. We appreciate that the reviewer found our paper to be \"well written\" and the theory \"well presented and interesting\". We have attempted to address the reviewer's concerns and questions below:\n\n> perhaps this is not in the scope of the paper, but the authors do not provide any theoretical/numerical support for the functional benefit of offline reactivation. [...] Do the theoretical/numerical results in this paper support these possibilities, or any one in particular?\n\n**Response:** This is indeed an interesting research question. However, as the reviewer themselves point out, we believe this is out of scope for our current work---our scope here is to explain theoretically the emergence of reactivation and justify this with experiments, not to identify its functional benefits. It is nevertheless an important direction for future work. Many papers have talked about the functional benefits of offline reactivation in other contexts (Mnih et al. 2015; Hayes et al. 2019; Hayes & Kanan 2022), but the reviewer is correct in noting that we would have to concretely demonstrate the benefits of this particular form of reactivation. We acknowledge that this is an immediate direction to be explored and are revising our discussion to discuss the implications of our results in this context.\n\nMnih et al. Human-level control through deep reinforcement learning. Nature 518, 529\u2013533, 2015.\n\nHayes et al. Memory efficient experience replay for streaming learning. International Conference on Robotics and Automation (ICRA), pp. 9769\u20139776. IEEE, 2019.\n\nHayes & Kanan. Online continual learning for embedded devices. arXiv preprint arXiv:2203.10681, 2022.\n\n> Relatedly, a key feature of this study is that the RNNs considered are noisy during awake states. It's unclear to me exactly why this is necessary. The authors \"found that even trajectories generated by networks that were not trained in the presence of noise, and also were not driven by noise in the quiescent phase, still generated quiescent activity distributions that corresponded well to the active phase distributions\"; is the point that there's more space exploration with noise, giving rise to a functional benefit? Or that somehow the introduction of noise better captures observed neural data?\n\n**Response:** Indeed, as the reviewer points out, the noise leads to more exploration. Noise has been shown to improve RNN training (Qin & Vu\u010dini\u0107, 2018). Furthermore, neural activity in the brain is noisy (Destexhe & Rudolph-Lilith, 2012), for example, in the probabilistic nature of synaptic transmission, and in other ways.\n\nIt is critical to note that the match in distributions is across multiple trajectories (multiple initializations), and so does not entirely capture the properties of individual trajectories, such as the amount of exploration.\n\nWe thus carried out an experiment to show the amount of exploration of these noiseless trajectories. Indeed we found that individual trajectories do not explore the space well, and get stuck (Suppl. Fig. A.2c-d). Adding noise during quiescence alone does not fix this problem---it makes the trajectories erratic and implausible (Suppl. Fig. A.2e-f), with much higher point-to-point distances than even waking behavior (Suppl. Fig. A.3). So individual trajectories from noiseless networks do a worse job relative to the noisy trained networks. Noisy training stabilizes noisy quiescent activity, which in turn explores more of the task manifold than noiseless quiescent activity. We clarify this point in the revised manuscript.\n\nQin & Vu\u010dini\u0107. Training Recurrent Neural Networks against Noisy Computations during Inference. 2018 52nd Asilomar Conference on Signals, Systems, and Computers (2018): 71-75.\n\nDestexhe & Rudolph-Lilith. Neuronal Noise. Springer, 2012."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8388/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700251174706,
                "cdate": 1700251174706,
                "tmdate": 1700251174706,
                "mdate": 1700251174706,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZGjjUsHq61",
                "forum": "RVrINT6MT7",
                "replyto": "QTZ2umIavx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8388/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8388/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer pnLC"
                    },
                    "comment": {
                        "value": "> I found the link between the theoretically optimal solution for update dynamics and the actual application of RNNs somewhat unclear. Are we to take that (presumably backprop-trained) RNNs employ update dynamics similarly to the theoretical solution; is it biologically reasonable for e.g. equation 18 to be implement by an RNN? If so, is it possible to show that they do; e.g. by comparing these recurrent inputs to the optimal solution? If not, are the experimental results relevant to the theory?\n\n**Response:** We apologize if this was not clear. The idea is that the RNNs trained optimally (following any optimization scheme) are approximating the ground-truth optimal solution, given by the greedily optimal dynamics (Section 2.3). It is indeed reasonable for an RNN to implement Eq. 18---according to the literature on RNNs being universal function approximators (Funahashi & Nakamura, 1993). We did not show that the network dynamics explicitly align with the optimal solution, though our empirical results indicate that they do. Indeed we see the Langevin sampling during quiescence leading to the distributional match. It is certainly possible for suboptimal (or very small) RNNs to fail to align with the theory, the theory only applies for very well-trained networks that are performing near-optimally such as our networks. We clarify these points in the revised manuscript.\n\nFunahashi & Nakamura. Approximation of dynamical systems by continuous time recurrent neural networks. Neural Networks 6 (1993): 801-806.\n\n> Relatedly, perhaps this is overly harsh, but I also do not find the result that RNNs without stimuli visit similar states as with stimuli very surprising.\n\n**Response:** We believe that there may have been a misunderstanding---our claim is much stronger than merely saying that similar states are visited in the absence of stimuli. Instead, we prove and demonstrate that the networks visit the same states with the same, i.e., **identical steady-state distribution** during quiescence as active behavior. During our biased and unbiased tasks, the same path integrating continuous attractor could theoretically solve both tasks, but then rehearsal would be the same in both conditions. We see that the networks learn different solutions for the biased and unbiased tasks, and that these different solutions produce rehearsals that reflect the task statistics. This is non-trivial, and demonstrating this is, we believe, our main novel contribution. We further clarify this point in the revised manuscript.\n\n> The authors suggest an interesting discrepancy in experimental prediction for their noise-based theory of offline reactivation and generative modeling; specifically, that \"while generative models necessarily recapitulate the moment-to-moment transition statistics of sensory data, our approach only predicts that the stationary distribution will be identical\". I am confused by this statement and not sure if it is true. Is that to say that the proposed model would not encode sensory transitions but would simply replicate the overall probability distribution of sensory states? I would find this surprising for an RNN whose recurrent weights are trained on sensory data; that is, I would suppose that the RNN weights would themselves somehow capture transition statistics of the task variables (e.g. maybe this is valuable as a kind of denoising effect on noisy observations). Perhaps I am wrong though\n\n**Response:** The reviewer is indeed correct that it is **possible** for our networks to capture the transition dynamics (and this is what happens in practice), it is just that our theory does not comment on the dynamics of reactivations. In a new experiment with training solely on counter-clockwise trajectories for the head direction task (Suppl. Fig. A.5), we found that the reactivations recapitulated the transition structure of active phase trajectories, and maintained their bearing for longer periods, resembling real neural activity more closely. This would indeed appear to be due to the RNN weights capturing the transition structure, but is not a prediction of our theory. The biased velocities were reflected during quiescence, but less clearly than during waking. However, reversals in the trajectories still occurred (Suppl. Fig. A.5b)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8388/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700251257860,
                "cdate": 1700251257860,
                "tmdate": 1700476335339,
                "mdate": 1700476335339,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vVT867STlq",
                "forum": "RVrINT6MT7",
                "replyto": "D1kJatcYoa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8388/Reviewer_pnLC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8388/Reviewer_pnLC"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you to the authors for their detailed response and clarifications. I also appreciate the new supplementary figures and analyses. \n\nOverall, I do believe the mathematical analysis is interesting enough for publication, but I would encourage the authors to better link their theory with backprop-trained RNNs and try to present (if only preliminary) emperical results which demonstrate the functional utility of their theory. \n\nI will modify my score accordingly."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8388/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700394263826,
                "cdate": 1700394263826,
                "tmdate": 1700394263826,
                "mdate": 1700394263826,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xLb3ISgGKn",
            "forum": "RVrINT6MT7",
            "replyto": "RVrINT6MT7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8388/Reviewer_xvHM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8388/Reviewer_xvHM"
            ],
            "content": {
                "summary": {
                    "value": "This submission seeks to model the reactivation of brain activity during periods of quiescence using task-optimized network. The paper develops some mathematical formalisms to show that under certain assumptions, recurrent neural networks trained to perform a task develop denoising dynamics. \n\nWhile there are some interesting ideas presented in this submission, overall it feels that the results presented in the current version are preliminary and not surprising."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "\u2014 Understanding the neural mechanisms underlying reactions/replays is an important question.\n\n-- The paper seeks to analyze RNNs trained to perform a class of tasks theoretically, which is a somewhat rare excise in this literature. \n\n\u2014 The mathematical analysis of the loss function used in the training and the connection to the Langevin dynamics, while under somewhat strong assumptions, remain interesting."
                },
                "weaknesses": {
                    "value": "1. A number of relevant studied were not cited and discussed. This include work using RNNs and attractor dynamics to model the replay and theta sequences in the hippocampus, e.g.,\nHopfield, John J. \"Neurodynamics of mental exploration.\" Proceedings of the National Academy of Sciences 107.4 (2010): 1648-1653.\nKang, Louis, and Michael R. DeWeese. \"Replay as wavefronts and theta sequences as bump oscillations in a grid cell attractor network.\" Elife 8 (2019): e46351.\nChu, Tianhao, et al. \"Firing rate adaptation affords place cell theta sweeps, phase precession and procession.\" bioRxiv (2022): 2022-11.\n\nThe paper also misses several pieces work in training RNNs to study the grid cells systems and HD systems.\nCueva, C. J., & Wei, X. X. (2018). Emergence of grid-like representations by training recurrent neural networks to perform spatial localization. ICLR.\nUria, B., Ibarz, B., Banino, A., Zambaldi, V., Kumaran, D., Hassabis, D., Barry, C. and Blundell, C., (2020). The spatial memory pipeline: a model of egocentric to allocentric understanding in mammalian brains. BioRxiv, pp.2020-11.\nCueva, C.J., et al (2020). Emergence of functional and structural properties of the head direction system by optimization of recurrent neural networks.ICLR.\n\n2. The main results are not surprising after considering what we know so far on this topic. Prior work has shown that training the RNNs to perform path integration or angular integration task leads to networks that exhibit attractors dynamics that is similar to continuous attractor models. Furthermore, it is well known that attractor dynamics can perform denoising. Thus it is not surprising that RNNs trained on these previous studied tasks can perform denoising. \n\nIt should also be noted that prior work has further characterize the rate of diffusion along the low-d manifold, e.g., see Fig 6 of the following paper: \nBurak, Y. and Fiete, I.R., 2009. Accurate path integration in continuous attractor network models of grid cells. PLoS computational biology, 5(2), p.e1000291.\n\n\n3. As described in the paper, the trained network follows diffusion dynamics when the stimulus turned off. This is naturally expected because the diffusion of network state on a low-d manifold. This would result in brownian-motion like trajectory. Furthemore, the replay trajectories observed in the hippocampus typically described by a systematic drift towards one direction (e.g., in linear or circular tracks), not diffusive dynamics. Can the authors be more explicit about the experimental data they were modeling?\n\n4. One more point that confuses me\u2014the authors stated \u201cwe found that even trajectories generated by networks that were not trained in the presence of noise, and also were not driven by noise in the quiescent phase, still generated quiescent activity distributions that corresponded well to the active phase distributions. \u201d This seems to argue against the denoising dynamics, and make the usefulness of the mathematical analysis questionable.\n\n5. The mathematical analysis relied on a set of approximations and assumptions, which were not well justified."
                },
                "questions": {
                    "value": "Would it be possible to discuss more explicitly how their results are connected or supported by the empirical data? In particular, how is the \u201cdiffusive reactivation\u201d related to neural data?\n\n\u2028The mathematical analysis relies on a number of assumptions, to the extent that it is difficult to judge whether the conclusion would actually be applicable to the numerical experiments. Can these assumptions be justified or better motivated (beyond the sake of mathematical convenience)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8388/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8388/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8388/Reviewer_xvHM"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8388/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698768859435,
            "cdate": 1698768859435,
            "tmdate": 1700666803071,
            "mdate": 1700666803071,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "drXLP6XYnu",
                "forum": "RVrINT6MT7",
                "replyto": "xLb3ISgGKn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8388/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8388/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xvHM"
                    },
                    "comment": {
                        "value": "We thank the reviewer for reviewing our paper and providing helpful comments that we believe lead us to considerably enrich our work. We are glad that the reviewer found our theoretical approach, which the reviewer has stated is a rare exercise in the literature, to be interesting and addressing an important question. We have provided our responses to the reviewer's comments below:\n\n> A number of relevant studied were not cited and discussed. This include work using RNNs and attractor dynamics to model the replay and theta sequences in the hippocampus [...] The paper also misses several pieces work in training RNNs to study the grid cells systems and HD systems. [...]\n\n**Response:** We thank the reviewer for pointing us to these works. We have cited these in our updated draft. We would also like to stress that the focus of our work is to demonstrate that reactivations are statistical reproductions of waking activity (i.e. the steady-state distribution of reactivations is **identical** to that of waking activity), and not just observed as a result of our networks learning continuous attractors. We formalize and prove this mathematically, and also show this with experiments. We apologize if this was not clear. We hope that our comments here and in the updated draft improve on this.\n\n> The main results are not surprising after considering what we know so far on this topic. Prior work has shown that training the RNNs to perform path integration or angular integration task leads to networks that exhibit attractors dynamics that is similar to continuous attractor models. Furthermore, it is well known that attractor dynamics can perform denoising. Thus it is not surprising that RNNs trained on these previous studied tasks can perform denoising.\n\n**Response:** We apologize if the main message appeared to be the related to the RNNs learning continuous attractors and performing denoising, this is not our intention. In fact, our very starting point is the recognition that continuous attractor networks emerge with training, it is not one of our major claims. The central finding and novel contribution of our manuscript is to explore the conditions under which reactivation, driven by noise, matches the statistics of waking activity.\n\nWith respect to denoising, we believe that one of our novel contributions is to show theoretically how denoising comes to be, and gives rise to statistically faithful reactivations as described below, providing a principled theoretical understanding. We would like to reiterate that our focus is not to show that denoising occurs, although we build on works that explore this idea (Section 2.2--2.3).\n\nMore precisely, our novel contribution is to prove and demonstrate that the steady-state distribution of reactivations is **identical** to that of waking activity, under a well-defined and minimal set of conditions. While this mechanism is related to denoising, which will occur along attractor manifolds, our contribution is to demonstrate that optimal denoising under different task statistics will show rehearsal patterns that reflect those different task statistics. This is the key novel contribution, which we believe is non-trivial in and of itself. We make sure to clearly communicate our findings in the revised manuscript (Section 2.4)."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8388/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700250995918,
                "cdate": 1700250995918,
                "tmdate": 1700250995918,
                "mdate": 1700250995918,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Bc3B8eD66N",
                "forum": "RVrINT6MT7",
                "replyto": "xLb3ISgGKn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8388/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8388/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xvHM"
                    },
                    "comment": {
                        "value": "> The mathematical analysis relied on a set of approximations and assumptions, which were not well justified.\n\nand\n\n> The mathematical analysis relies on a number of assumptions, to the extent that it is difficult to judge whether the conclusion would actually be applicable to the numerical experiments. Can these assumptions be justified or better motivated (beyond the sake of mathematical convenience)?\n\n**Response:** We thank the reviewer for the suggestion, and we have added a section to the Supplementary Material to clarify the key assumptions for our theory. We provide the assumptions and explanations here:\n1. We consider discrete-time approximations of noisy continuous-time RNNs. Using continuous-time RNNs is a common practice in the literature, and we use its discrete-time approximation to be in line with our implementation.\n2. The network must be performing some variant of path integration, by integrating change-based information about environmental state variables to some function of these variables. This condition is often met in natural settings, such as spatial navigation.\n3. We assume that the inputs to the network are drawn from a stationary distribution, which would translate to assuming that the effects of initial conditions on state occupancy statistics are ignored, and also assuming that the behavioral policy remains constant throughout time.\n4. We consider greedy optimization of the loss at every timestep. Greedy optimization is a sensible way of partitioning effort across time in this task: the agent does the best that it can on each timestep, assuming that at each previous timestep the best possible job has been done. In the absence of noise the greedily optimal solution is equivalent to path integration, which is also a **globally** optimal solution.\n5. We assume that the network is performing optimally in the presence of noise. In practice, we train our networks until their loss reaches very low, near-zero values.\n6. We assume that our network dynamics can be decomposed into two terms with different functional dependencies. The first term depends only on the activity while the second term depends on all original dependencies\u2014the activity, the state and the change-based inputs.\n7. For the quiescent state, we assume that the change-based sensory inputs are zero. This is reasonable because for tasks like those we have considered, which involve integrating self-motion cues, $\\frac{\\mathrm{d}s(t)}{\\mathrm{d}t}$ must be zero during periods of quiescence like sleep, where the animal is not moving and hence doesn't receive sensory inputs associated with self-motion.\n8. While we assume that the noise variance is doubled during quiescence to show exact equivalence with Langevin sampling, this is by no means necessary to witness reactivation (Suppl. Fig. A.4). This noise variance is equivalent to a temperature parameter for the sampling.\n\nRNNs trained optimally (following any optimization scheme) are approximating the ground-truth optimal solution, given by the greedily optimal dynamics (Section 2.3). It is indeed reasonable for an RNN to implement Eq. 18---according to the literature on RNNs being universal function approximators (Funahashi & Nakamura, 1993). Our empirical results indicate that the network's dynamics align with this solution. Indeed we see the Langevin sampling during quiescence leading to the distributional match. It is certainly possible for suboptimal (or very small) RNNs to fail to align with the theory, the theory only applies for very well-trained networks that are performing near-optimally such as our networks. Overall, we believe our assumptions are well-justified, and the conclusions of the theory are indeed applicable to our numerical experiments as evidenced by our results.\n\nFunahashi & Nakamura. Approximation of dynamical systems by continuous time recurrent neural networks. Neural Networks 6 (1993): 801-806.\n\nWe hope that our response and updates to the draft have addressed all of the reviewer's concerns, and would be grateful if the reviewer would consider raising their score. If the reviewer has any additional questions, we would be happy to discuss further. We eagerly await the reviewer's response."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8388/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700251077907,
                "cdate": 1700251077907,
                "tmdate": 1700477959316,
                "mdate": 1700477959316,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ingJoItjgk",
                "forum": "RVrINT6MT7",
                "replyto": "xLb3ISgGKn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8388/Reviewer_xvHM"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8388/Reviewer_xvHM"
                ],
                "content": {
                    "title": {
                        "value": "thanks for the clarifications"
                    },
                    "comment": {
                        "value": "I appreciate the authors\u2019 detailed response to my critiques. The response helps clarify some of the concerns. In particular, the key contribution is made more clear.  I have raised my score.\n\nA couple of issues: \n-- while the relevant references were added to the revised version, generally there is a lack of appropriate discussion of these studies in the context.\n\n-- the connection to empirical research/ neuroscience data remains to be strengthened. I don\u2019t feel this point was appropriately addressed in the revised version."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8388/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700666733242,
                "cdate": 1700666733242,
                "tmdate": 1700666839288,
                "mdate": 1700666839288,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]