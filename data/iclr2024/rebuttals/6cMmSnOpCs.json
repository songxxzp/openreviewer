[
    {
        "title": "ScaLearn: Simple and Highly Parameter-Efficient Task Transfer by Learning to Scale"
    },
    {
        "review": {
            "id": "yFbDQXSSp2",
            "forum": "6cMmSnOpCs",
            "replyto": "6cMmSnOpCs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3311/Reviewer_6MN5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3311/Reviewer_6MN5"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new method for task transfer by aggregating the output representations of source task models with some weighted sum, where the weight is either applied on each coordinate or on the entire representation. They claim that this method is more parameter efficient than the previous method which trains a separate adapter for each task, and show that the performance is slightly better."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The idea is simple yet seems to be something that people haven\u2019t tried before.\n- The algorithm both improves parameter efficiency and also the performance, which is quite nice.\n- The paper is well written and the algorithm details are carefully explained."
                },
                "weaknesses": {
                    "value": "- The improvement in the performance is quite marginal. In table 3, it looks like the proposed method is only slightly better than previous algorithms.\n- I believe that the focus of the algorithm is the parameter efficiency. However, I\u2019m not very convinced that this is an important improvement, given that the adapter anyways is already having much fewer parameters than the original models. Isn\u2019t it true that the key bottleneck is the model size itself? \n- It would be good if the authors can include experiments beyond NLP tasks to show the generality of their method."
                },
                "questions": {
                    "value": "- In table 3, the version of the ScaLearn that achieves best performance varies a lot across tasks. Why is this the case? Can you provide some intuition around when would each version work better?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3311/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698460413884,
            "cdate": 1698460413884,
            "tmdate": 1699636280770,
            "mdate": 1699636280770,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xk2nLKnCdT",
                "forum": "6cMmSnOpCs",
                "replyto": "yFbDQXSSp2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3311/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3311/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer to Reviewer 6MN5"
                    },
                    "comment": {
                        "value": "We thank the reviewer for recognizing the novelty and performance of our method, as well as the clarity of our presentation. We appreciate the insightful feedback and address the reviewer's concerns below.\n- **[W1] Performance improvement of ScaLearn**: We would like to emphasize that the performance of ScaLearn is considerably better than the baselines at a much lower or similar parameter count. While the performance on high-resource tasks (QQP, MNLI) is similar to some other methods (since the source adapter can learn the task well), this is considerably different for many other tasks: This is especially the case on SuperGLUE, as shown in Table 3 - our ScaLearn beats the best baseline by ~2 percentage points (!), and this is especially visible on the low-resource tasks - notably CB (90.89 vs. 87.68 for the best baseline, Compacter++) and RTE (78.88 vs. 76.53 for the best baseline, Adapter-m). In addition, on the very different multilingual benchmark HumSet, ScaLearn is the only (!) method that clearly beats any of the single-task learning methods. We attribute this to the simplicity and power of scaling output representations. Furthermore, we also show considerably improved few-shot learning abilities (Section 6.3).\n- **[W2] Significance of parameter-efficiency**: We agree that our method provides strong parameter-efficiency and that the original backbone model typically has more parameters. However, especially since models tend to get larger and larger, it is important to be able to efficiently adapt the model to novel domains, tasks, and languages and mitigate catastrophic interference. The importance of our problem setup and efficiency is also highlighted by reviewer YAiw. This strongly motivates a modular design of such models. In this paradigm, the parameter-efficiency of PEFT (parameter-efficient fine-tuning) has become paramount, especially when scaling up the number of tasks and, thus, modules and corresponding parameter counts [1]. Therefore, this is a crucial aspect of any method within this paradigm. This is especially the case in two-stage MTL methods - and in this regime, ScaLearn, and especially its variations, is highly parameter-efficient while also clearly outperforming the baselines. For an overview of the parameter-efficiency of ScaLearn and all baselines, we refer the reviewer to Figure 1 on page 1 and the parameter-efficiency analysis (Section 6.1).\n- **[W3] Experiments beyond NLP tasks**: We understand the reviewer's interest in applying our method to other contexts beyond NLP. In this work, our focus is on showcasing its strong performance and efficiency in the context of NLP. In doing that, we follow numerous other papers that equally focus on NLP tasks [2, 3, 4]. We have shown the performance of our method on a diverse number of NLP tasks, domains, and languages. Yet, we thank the reviewer for the idea, and it comprises an interesting direction for future research.\n- **[Q1] Variability in ScaLearn\\* performance**: Overall, the performance of the different variations of ScaLearn is rather similar. Moreover, the performance of ScaLearn and ScaLearn++ tends to be highly similar on most tasks (cf. Table 2, 3, and 4), with both performing the best overall. The performance of the variants relying on uniform scaling\u2014ScaLearnUniform and ScaLearnUniform++\u2014also tends to be highly competitive on most tasks and is generally a bit lower than the non-uniform variants, but still very strong. We attribute this to the difference in scaling the output representations element-wise or not (i.e., non-uniform or uniform scaling) and the corresponding difference in learning capacity. For the best performance, we advise using the vanilla ScaLearn or ScaLearn++, and if the parameter count is the main constraint, we advise using ScaLearnUniform++.\n\n[1] Modular Deep Learning. arxiv preprint, 2023.\n\n[2] Few-shot parameter-efficient fine-tuning is better and cheaper than in-context\nlearning. NeurIPS, 2022.\n\n[3] Adapter-Fusion: Non-destructive task composition for transfer learning. EACL, 2021.\n\n[4] Conditionally adaptive multi-task learning: Improving transfer learning in NLP using fewer parameters & less data. ICLR, 2023."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3311/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700468787122,
                "cdate": 1700468787122,
                "tmdate": 1700468787122,
                "mdate": 1700468787122,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "99fH2Lf3D1",
            "forum": "6cMmSnOpCs",
            "replyto": "6cMmSnOpCs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3311/Reviewer_YAiw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3311/Reviewer_YAiw"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a method for transferring previously learned knowledge (in the form of adaptors) from different source tasks to a given target task. The main idea of the paper is to propose a new method that is more parameter-efficient compared to the previous methods. Their method ScaLEARN learns a vector that scales the outputs of the adaptors of the source tasks and then adds these outputs together before passing them to the next layer. They propose some variants of this method that share these activation scaling parameters and across layers, or across the dimension to reduce the parameter count even further. They compare their work with AdaptorFusion methods from 2021."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "S1. The problem being studied in the paper is becoming increasingly important in the scenarios of large language models. \n\nS2. The method reduces the parameter requirement compared to the AdaptorFusion paper. Moreover, some variants of the method (ScaLearnUniform++) are extremely parameter efficient while they perform similarly to adaptor-fusion and the basic method ScaLearn."
                },
                "weaknesses": {
                    "value": "W1. The experimental section is not very strong and is missing some very strong baselines, relevant settings, and analysis. Please refer to the questions below.\n\nW2. The paper is slightly harder to read, the motivation and the analysis on scaling part was not very clear to me until I read section 4 about the method."
                },
                "questions": {
                    "value": "**For Me to Improve My Score (Most to least important)**\n\nQ1: The only modular composition baseline in this paper is AdaptorFusion from 2021, which is quite old now and there have been other works that tackle the same problem from different angles that should act as baselines here. [1] Combining Modular Skills in Multitask Learning, [2] AdapterSoup: Weight Averaging to Improve Generalization of Pretrained Language Models, and [3] Multi-Head Adapter Routing for Cross-Task Generalization. And the baseline methods used in these papers. \n\nQ2: The experimental setup used in the paper can be significantly improved by studying this problem on good seq2seq zero-shot models like T0, or maybe LLaMA family models and then comparing the zero-shot performance, few-shot performance, and the performance obtained via this kind of composition of learned modules. \n\nQ3: How is the classification layer for the target task finetuned? In Table 1 and everywhere in the paper, it seems like you do not count these parameters when counting for the number of trainable parameters. Can you clarify this if the classifier parameters are also learned on each source task then this needs to be clarified throughout the paper. \n\n\n\n**Other Questions**\n\nQ4: The experiments in the paper should add IA3 () as a baseline. IA3 paper shares very high similarity to the proposed method, the ScaLEARN method is like adapting source task adapter modules using IA3 on a downstream task. Hence, for all the experiments, IA3 would be a good baseline as it would learn a lower number of parameters than ScaLearn. However, I don't suspect it to perform better than the presented method as it is leveraging source adaptors.\n\nQ5: At multiple places, the paper talks about how the scaling coefficient does not need to sum to 1 and I agree that for ScaLearn this might be the case however, I am not sure if there is enough evidence in the paper, to claim that this is how it should be for all other methods and this has been talked about at multiple places in the paper as a new finding. It can be that for the other methods having a constraint on summing to 1 might be better than not having it. Hence, this is a method-specific detail and should not be portrayed as a general finding that we should not have a summation constraint. I might have missed something here and I will willing to take this comment back in light of evidence.\n\n\nI looking forward to the rebuttal and will update my score if some of my main concerns are addressed. I really like the simplicity of the method however I would like it to be more rigorously tested against other baselines and experimental setups."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3311/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3311/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3311/Reviewer_YAiw"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3311/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698623863930,
            "cdate": 1698623863930,
            "tmdate": 1700587018633,
            "mdate": 1700587018633,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7Y7id53gfN",
                "forum": "6cMmSnOpCs",
                "replyto": "99fH2Lf3D1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3311/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3311/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for acknowledging the importance, simplicity, and merits of our method. We appreciate the insightful feedback and address each of the concerns below.\n- **[W1] Experimental section and baselines**: We acknowledge the reviewer's emphasis on the importance of a diverse evaluation to demonstrate the merit of our method. We have already included a broad range of strong single-task learning (STL) and multi-task learning (MTL) baselines, and we appreciate the reviewer's perspective on this matter. In response, we have included the PEFT method (IA)^3 [4] and AdapterSoup [2] (adapted to the two-stage MTL framework) in our experiments.\n\nRTable 1: Performance on GLUE using RoBERTa-base.\n| Model                 | MNLI  | QQP   | QNLI  | SST-2 | STS-B | MRPC  | RTE   | CoLA  | Avg.  |\n|-----------------------|-------|-------|-------|-------|-------|-------|-------|-------|-------|\n| Adapter               | 86.50 | 90.18 | 92.25 | 93.65 | 90.23 | 86.64 | 72.89 | 58.28 | 83.83 |\n| ProPETL               | 86.19 | 88.88 | 92.05 | 93.81 | 90.03 | 85.93 | 74.19 | **59.29** | 83.80 |\n| Compacter++           | 85.62 | 88.84 | 91.79 | 93.58 | 89.67 | 87.21 | 72.02 | 58.49 | 83.40 |\n| *(IA)^3*               | 83.78 | 88.37 | 90.57 | 93.35 | 89.93 | 87.11 | 72.56 | 56.57 | 82.78 \n| *AdapterSoup* | 63.47 | 81.63 | 78.00 | 90.75 | 80.17 | 75.00 | 62.09 | 41.06 | 71.52 |\n| ScaLearn              | **86.97** | **90.32** | **92.51** | **93.88** | **90.96** | **87.75** | **82.06** | 58.47 | **85.36** |\n\nRTable 2: Performance on SuperGLUE.\n\n| Model                      | ReCoRD | MultiRC | BoolQ  | WiC    | WSC    | COPA   | CB     | RTE    | Avg.   |\n|----------------------------|--------|---------|--------|--------|--------|--------|--------|--------|--------|\n| Adapter                    | 79.02  | 72.84   | 76.71  | 65.58  | **63.46**  | 70.20  | 84.82  | 72.89  | 73.19  |\n| ProPETL                    | **80.29** | 73.07   | 76.58  | 66.60  | **63.46**  | 70.60  | 84.46  | 74.19  | 73.69  |\n| Compacter++                | 77.69  | 70.44   | 75.88  | 66.46  | **63.46**  | 68.30  | 87.68  | 72.02  | 72.74  |\n| *(IA)^3*                     | 75.27  | 70.32   | 76.31  | **67.07**  | 63.35  | 69.30  | 87.32  | 72.56  | 72.69  |\n| *AdapterSoup* | 64.26 | 33.62 | 68.84 | 58.53 | 63.46 | 52.40 | 70.89 | 57.83 | 58.73 |\n| ScaLearn            | 79.52  | **73.22** | **77.27** | 66.35  | **63.46** | **74.80**  | **90.89**  | **78.88**  | **75.55**  |\n\nRTable 3: Performance on HumSet using XLM-R-base.\n\n| Model                   | Sectors | Pillars 1D | Subpillars 1D | Pillars 2D | Subpillars 2D | Avg.  |\n|-------------------------|---------|------------|---------------|------------|---------------|-------|\n| Adapter                 | 71.38   | 51.02      | 43.26         | 61.43      | 42.46         | 53.91 |\n| ProPETL                 | 71.69   | 49.69      | 41.63         | 60.58      | 39.85         | 52.69 |\n| Compacter++             | 69.97   | 37.37      | 37.76         | 58.13      | 33.10         | 47.26 |\n| *(IA)^3*                  | 70.22   | 45.55      | 40.05         | 58.54      | 39.27         | 50.73 |\n| *AdapterSoup*             | 56.81   | 30.09      | 21.84         | 40.71      | 17.89         | 33.47 |\n| ScaLearn         | **72.36** | **51.63**  | **44.06**     | **61.52**  | **42.81**     | **54.48** |\n\nRTable 1, 2, and 3 demonstrate that our ScaLearn model outperforms the newly introduced baselines.\n\nRegarding (IA)^3, it performs similarly but slightly worse than other STL methods in our setup. We attribute this difference to our different experimental setup, including tasks, dataset sizes, and PLMs, which vary meaningfully from the original (IA)^3 paper [4]. Aside from the original study, hardly any comparisons of (IA)^3 with other PEFT techniques exist; the closest is [5], which is aligned with our findings.\n\nRegarding AdapterSoup, we adapted it to the two-stage MTL framework by adding the target task corpus and, hence, target adapter to the similarity calculation, and also train a new task head on every target task. Still, its comparatively worse performance suggests that calculating weights using sentence similarity is not the best fit for our specific problem setup.\n\nWe incorporated the results of both as an additional baseline in the revised draft. This addition, we believe, further strengthens our evaluation and addresses the reviewer's main concerns, and reflects our commitment to a thorough and inclusive comparative analysis.\n\n[1] Combining parameter-efficient modules for task-level generalisation. EACL, 2023.\n\n[2] AdapterSoup: Weight averaging to improve generalization of pretrained language models. EACL, 2023.\n\n[3] Multi-head adapter routing for data-efficient fine-tuning. arxiv preprint, 2023.\n\n[4] Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning. NeurIPS, 2022.\n\n[5] https://adapterhub.ml/blog/2022/09/updates-in-adapter-transformers-v3-1/#ia3"
                    },
                    "title": {
                        "value": "Results on additional baselines (AdapterSoup and (IA)^3)"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3311/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700469277735,
                "cdate": 1700469277735,
                "tmdate": 1700469335890,
                "mdate": 1700469335890,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IEXkYyhMTA",
                "forum": "6cMmSnOpCs",
                "replyto": "dXmMJGaRzL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3311/Reviewer_YAiw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3311/Reviewer_YAiw"
                ],
                "content": {
                    "title": {
                        "value": "Updating my score"
                    },
                    "comment": {
                        "value": "Hi, I have read the response and I am updating my score to 5. However I still feel experiments with bigger models, better baselines would help the method significantly."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3311/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700586999334,
                "cdate": 1700586999334,
                "tmdate": 1700586999334,
                "mdate": 1700586999334,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "OCBu4WOBCd",
            "forum": "6cMmSnOpCs",
            "replyto": "6cMmSnOpCs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3311/Reviewer_tpyz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3311/Reviewer_tpyz"
            ],
            "content": {
                "summary": {
                    "value": "This article presents a new two-stage multitask learning method called SCALEARN, which achieves knowledge transfer by learning the output representation of scaling the source task. The proposed approach achieves high parameter efficiency and strong performance, and can avoid problems such as task interference and data privacy. The authors conduct extensive experiments on three benchmark datasets, e.g., GLUE, SuperGLUE, and HumSet, using RoBERTa and XLM-R as pre-trained language models. The experimental results demonstrate that ScaLearn and its variants outperform strong baselines on various tasks, and also perform well in a few-shot setting."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- ScaLearn achieves transfer learning on downstream tasks by scaling and combining the output representations of the source task adapter. This approach has high scalability and low parameter overhead. The idea is simple and technically sound.\n- In the third section of the paper, the authors analyze the effects of scaling output representations in transfer learning: scaling output vectors is an effective method for controlling the (partial or full) activation of the knowledge contained in an adapter module and the optimal weights do not necessarily sum up to 1. This provides a new approach for \u201chow to leverage modular knowledge to learn new tasks\u201d. The investigation effectively supports the paper\u2019s claims.\n- Experiments are very extensive and thorough, including a variety of tasks, architectures, datasets (GLUE, SuperGLUE, HumSet), and strong baselines beyond vanilla dropout. Both standard transfer learning and few-shot transfer learning are reported, accompanied by in-depth ablation studies. The experiment and analysis are well organized."
                },
                "weaknesses": {
                    "value": "- Analysis limited to adapter-based methods. Unclear how well it will perform to other PEFT architectures (e.g. Prompt Tuning).\n- To my best knowledge, IA3 [1] achieves stronger fine-tuning performance by scaling the weighted activations in the activation layer using learned vectors. This is similar to your method, but I did not find it in the PEFT baselines you compared.\n- It seems that many source tasks are closely related to each other. I would suggest authors use benchmarks such as CrossFit [2] to do a more large-scale analysis, where the transferring is more challenging as some source tasks can be relatively less related to the target tasks.\n- For the second stage during training, the output representations of multiple source adapters are scaled and combined, which reminds me of MoE (Mixture of Experts), where each source adapter corresponds to an expert. It is a well-known phenomenon that learnable MoE can lead to overfitting and collapse. However, in your method, it seems that this issue does not arise. Could the authors explain the specific reasons behind this?\n\n---\n\n[1] Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning. NeurIPS 2022\n\n[2] CrossFit: A Few-shot Learning Challenge for Cross-task Generalization in NLP. EMNLP2021"
                },
                "questions": {
                    "value": "Please respond to the concerns listed in weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3311/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698671394515,
            "cdate": 1698671394515,
            "tmdate": 1699636280597,
            "mdate": 1699636280597,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2q00wElAe3",
                "forum": "6cMmSnOpCs",
                "replyto": "OCBu4WOBCd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3311/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3311/Authors"
                ],
                "content": {
                    "title": {
                        "value": "General answer to Reviewer tpyz"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their thoughtful feedback and for recognizing the technical soundness and thoroughness of the experiments. We appreciate the constructive feedback and address each of the concerns raised in the review below.\n- **[W1] Focus on adapter-based methods**: We acknowledge the reviewer's observation regarding the focus on adapter-based methods. This focus was intentional, following Pfeiffer et al. [1]. Our method is strongly motivated by our analyses of scaling the output representations of adapters (Section 3). Furthermore, we chose to concentrate on adapters due to the wide availability of pre-trained adapters via AdapterHub [2], which facilitates accessibility and reproducibility in the research community. To the best of our knowledge, such sharing platforms are currently not available for other PEFT architectures. However, these are crucial for two-stage MTL methods since they rely on readily available modules (in our case, adapters). Moreover, we justify the use of adapter modules to enable a fair comparison with the other two-stage MTL baseline - AdapterFusion. Additionally, as suggested by Reviewer YAiw, we added AdapterSoup as an additional adapter-based two-stage MTL baseline. In addition, we understand that other PEFT architectures like LoRA and prompt tuning operate differently, and, hence, these methods are not directly compatible with our method, which scales the output representations of the source modules. While this is a valid limitation of our study, we leave the exploration of expanding our method to other PEFT techniques for future work.\n- **[W3] Use of different benchmarks such as CrossFit [5]**: We agree with the importance of diversity in target and source tasks as well as large-scale analyses. To address diversity, we included SuperGLUE and HumSet in addition to the commonly used GLUE benchmark. SuperGLUE presents considerably more challenging tasks than GLUE, which are also more varied in terms of tasks and corpora sizes. We relied on GLUE and SuperGLUE as we wanted to use benchmarks that are most commonly used within the research community. To include a more distinct benchmark, we added additional experiments on the multilingual, multi-class classification benchmark HumSet, which spans the domain of human crisis response, making it very different from both GLUE and SuperGLUE. Using a different backbone model, XLM-R, the similarly strong results on HumSet showcase our method's applicability across different models, languages, and task formulations In addition, we evaluate our method on the combination of all GLUE and SuperGLUE tasks and show that it also outperforms the baselines at a fraction of the parameter cost when considering a higher number of source tasks. This shows that our method also works across more dissimilar tasks and when scaling up the number of source tasks, too. These challenging and different settings demonstrate the robustness and adaptability across different tasks, domains, and languages of our method. Still, we recognize the potential of including large-scale benchmarks like CrossFit in future work to further validate our method under even more different conditions.\n- **[W4] Comparison to MoEs and overfitting**: We appreciate the insightful comment and acknowledge the conceptual similarity between the scaling in our method and Mixture of Experts (MoEs) models. A key difference between our method and MoEs is that, in our setup, the backbone language model remains frozen during both stages of training. This inherently limits the learning capacity compared to fully fine-tuning the entire model, thereby reducing the risk of overfitting\u2014a common issue with more complex models like MoEs. This effect is also observed by Lian et al. [6] when proposing SSF (Scaling & Shifting your Features): \"*Such improvements in robustness and OOD [out-of-domain] datasets might come from the fact that SSF freezes most of the pre-trained parameters, which maximally preserves the knowledge learned from the large-scale dataset and thus maintains a better generalization ability.*\" Overall, we recognize the conceptual parallels with MoEs. Our approach avoids this phenomenon by keeping the backbone model frozen and, thus, sidestepping issues like overfitting and collapse, ensuring robust and generalizable performance across a variety of tasks.\n\n[1] AdapterFusion: Non-destructive task composition for transfer learning. EACL, 2021.\n\n[2] https://adapterhub.ml/\n\n[3] Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning. NeurIPS, 2022.\n\n[4] https://adapterhub.ml/blog/2022/09/updates-in-adapter-transformers-v3-1/#ia3\n\n[5]  CrossFit: A Few-shot Learning Challenge for Cross-task Generalization in NLP. EMNLP, 2021\n\n[6] Scaling & shifting your features: A new baseline for efficienmodel tuning. NeurIPS, 2022."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3311/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700468545896,
                "cdate": 1700468545896,
                "tmdate": 1700468545896,
                "mdate": 1700468545896,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tWLDVLYEWs",
                "forum": "6cMmSnOpCs",
                "replyto": "3VT4W4ccLX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3311/Reviewer_tpyz"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3311/Reviewer_tpyz"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "I appreciate the authors' responses to some of my questions, which provide additional experimental support. However, the parameter efficiency that you emphasized needs to be better demonstrated on benchmarks that include more tasks. In addition, I still believe that evaluating this method on benchmarks that include more tasks would better demonstrate its robustness and adaptability. Therefore, I maintain the original score unchanged."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3311/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700653414235,
                "cdate": 1700653414235,
                "tmdate": 1700653414235,
                "mdate": 1700653414235,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "pFHczL41FY",
            "forum": "6cMmSnOpCs",
            "replyto": "6cMmSnOpCs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3311/Reviewer_Exzr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3311/Reviewer_Exzr"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a new method for two-stage parameter-efficient multi-task learning called ScaLearn. It builds on prior work that fusing Adapters, and shows that a simpler parameterization of the source-task mixing module works well. It does this by replacing the attention mechanism in (Pfeiffer et al)[https://arxiv.org/abs/2005.00247] with a simpler task-vector or task-scalar parameterization."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is clearly written, and is generally easy to follow. \n - The method is simple, well-motivated, and works well. \n - In my opinion, the method proposed in this paper should have been a baseline in the original [Pfeiffer et al](https://arxiv.org/abs/2005.00247) paper that introduced AdapterFusion. Unfortunately it wasn't, so this paper is valuable in that it shows that attention could be unnecessary to fuse adapters, and simple task-vectors or task-scalars may be sufficient."
                },
                "weaknesses": {
                    "value": "- I don't think this is a glaring weakness, but I do think the paper could benefit from more diverse source/target tasks, especially sequence-generation tasks. It could be possible that the simpler ScaLearn parameterization doesn't work as well for different configurations of source and target tasks. I don't particularly see why *both* GLUE and SuperGLUE had to be included, it might have been better to replace one of them with a different benchmark if resources are/were the constraint. \n - Some paragraphs are very long and hard to parse (e.g. \"Models and Baselines\" on page 6), and could be written in a more organized manner in my opinion."
                },
                "questions": {
                    "value": "- Did the authors try other parameterizations of the fusion module? For example, a low-rank MLP per task instead of a vector per task would be a step towards finding a sweet spot (if it exists) between attention-based fusing and ScaLearn. It's also not clear to me whether a task-vector-per-layer would be better than an MLP-shared-across-layers. This paper essentially shows that what should have been a baseline in [Pfeiffer et al](https://arxiv.org/abs/2005.00247) works very well, which is valuable, but I think expanding the study to include a larger range of parameterizations (starting from ScaLearnUniform++ \u2192 low-rank MLPs \u2192 efficient attention variants) would make this paper much stronger and be a conclusive piece of work on fusing Adapters. Of course this is not necessary to be a solid paper, but definitely worth considering and discussing in the paper in my opinion."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3311/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3311/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3311/Reviewer_Exzr"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3311/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698821171736,
            "cdate": 1698821171736,
            "tmdate": 1699636280385,
            "mdate": 1699636280385,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BqNI6RvMjh",
                "forum": "6cMmSnOpCs",
                "replyto": "pFHczL41FY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3311/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3311/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank Reviewer Exzr for their positive feedback on our method and for acknowledging the clarity of the paper. We appreciate the reviewer\u2019s positive feedback and constructive suggestions. Below, we address each of the comments and concerns in detail.\n\n- **[W1] Selection of source and target tasks**: We agree with the importance of diversity in target and source tasks. We included SuperGLUE for this very purpose, as it presents considerably more challenging tasks than GLUE, which are also more varied in terms of tasks and corpora sizes. We relied on GLUE and SuperGLUE as we wanted to use benchmarks that are most commonly used within the research community. To include a more distinct benchmark, we added additional experiments on the multilingual, multi-class classification benchmark HumSet, which spans the domain of human crisis response, making it very different from both GLUE and SuperGLUE. Using a different backbone model, XLM-R, the similarly strong results on HumSet showcase our method\u2019s applicability across different models, languages, and task formulations. In addition, we evaluate our method on the combination of all GLUE and SuperGLUE tasks and show that it equally outperforms the baselines at a fraction of the parameter cost when considering a higher number of source tasks as well. These challenging and different settings demonstrate the robustness and adaptability across different tasks, domains, and languages of our method.\n- **[W2] Paragraph length and clarity**: We thank the reviewer for highlighting the importance of clarity in our paper. In response to the reviewer's valuable feedback, we have thoroughly revised the \"Models and baselines\" section on page 6 to enhance its readability. Additionally, we have reviewed the entire manuscript to ensure clarity in all sections. We appreciate any further specific feedback on sections that could benefit from increased clarity. \n- **[Q1] Exploration of other parametrizations**: We acknowledge the idea of a broader study encompassing a larger range of parametrizations; however, it would meaningfully deviate from the core contribution of the paper. Our experiments on scaling the output representations of adapters have provided a strong motivation for learning the scaling coefficients to combine different adapter output representations. This has led to our proposed method, ScaLearn, and its variations, showing improved performance and efficiency compared to previous methods."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3311/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700468396031,
                "cdate": 1700468396031,
                "tmdate": 1700468396031,
                "mdate": 1700468396031,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]