[
    {
        "title": "SparseFormer: Sparse Visual Recognition via Limited Latent Tokens"
    },
    {
        "review": {
            "id": "CuSygqfCoR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2346/Reviewer_H2So"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2346/Reviewer_H2So"
            ],
            "forum": "2pvECsmld3",
            "replyto": "2pvECsmld3",
            "content": {
                "summary": {
                    "value": "This paper introduces SparseFormer, an innovative vision transformer designed for efficiency, which encodes images into a select number of sparse tokens in a latent space. The efficacy and computational economy of SparseFormer are showcased through its performance in ImageNet and video classification tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. the SparseFormer model it introduces attains impressive results with a notable reduction in computational cost and latency, highlighting its efficiency and practicality in application.\n2. it is composed with a clear presentation, making it accessible and understandable"
                },
                "weaknesses": {
                    "value": "The experimental validation does not appear to be solid, such as the detection results and the ablation. see details in Question part\nThe novelty of SparseFormer is somewhat constrained, as it does not substantially deviate from existing methods in the field.\nThere is an absence of comparative analysis with other efficiency-oriented techniques, such as token pruning"
                },
                "questions": {
                    "value": "Could you provide insight into why the detection results are not more favorable, especially considering that your Region of Interest (RoI) mechanism appears to be well-suited for detection tasks? \nAdditionally, the paper does not include an ablation study on adjusting the RoI mechanism, which leaves its importance in the proposed method somewhat ambiguous. Could you clarify the necessity of the RoI mechanism within your framework?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2346/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2346/Reviewer_H2So",
                        "ICLR.cc/2024/Conference/Submission2346/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2346/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697380419446,
            "cdate": 1697380419446,
            "tmdate": 1700979255225,
            "mdate": 1700979255225,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2cpiuWSEaB",
                "forum": "2pvECsmld3",
                "replyto": "CuSygqfCoR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2346/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2346/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "__W1. Comparison with token pruning__\n\nThank you for your valuable suggestion.\nWe here compare SparseFormers with one of the most typical and effective token pruning method, ToMe [1], which can be used off-the-shelf to trained models. The throughput is measured on a single NVIDIA A5000 with batch size 32. Note that AugReg [2] models here are pre-trained on ImageNet-21K and we use them as strong baselines.\n\n|      model     |  acc | throughput | FLOPs |\n|:--------------:|:----:|:----------:|--------------|\n| DeiT-S         | 79.8 | 977        | 4.6G        |\n| DeiT-S, ToMe@13    | 78.5 | 1606        | 2.4G        |\n| DeiT-B         | 81.8 | 305       | 17.5G       |\n| DeiT-B, ToMe@13         | 80.0 | 481       | 10.4G       |\n|--------------|----|----------|--------------|\n| IN-21K pretrained AugReg-S         | 81.3 | 977        | 4.6G        |\n| IN-21K pretrained AugReg-S, ToMe@5   | 81.0 | 1065        | 3.7G        |\n| IN-21K pretrained AugReg-S, ToMe@13   | 79.3 | 1606        | 2.4G        |\n| IN-21K pretrained AugReg-B         | 84.6 | 305       | 17.5G       |\n| IN-21K pretrained AugReg-B, ToMe@13         | 82.5 | 481       | 10.4G       |\n|--------------|----|----------|--------------|\n| SparseFormer-T | 81.0 | 1207       | 2.0G       |\n| SparseFormer-S | 82.0 | 824        | 3.8G       |\n| SparseFormer-B | 82.6 | 475        | 7.8G       |\n\nThe table shows that even with ImageNet-21K pre-training, the post-training ToMe with AugReg-B only just matches the same performance as SparseFormer-B, which was trained from scratch on ImageNet-1K.\nThe smaller SparseFormer-T also offers a better trade-off over AugReg-S w/ ToMe.\n\n__W2. Why the detection results are not more favorable__\n\nThis could be because we did not include any form of positional information in the latent token embeddings, as described in Section 4 model configurations. As a result, the attention between these token embeddings is not aware of their positional relationships.\nPossessing positional information is crucial for achieving optimal detection performance, as validated in the original DETR [3].\nThe detection transformer pipeline relies on positional information for accurate localization and to suppress redundant detections by utilizing attention between tokens.\nMeanwhile, the segmentation is simple in sense that we can perform 'classification' on latent tokens that correspond to specific areas in an image, and map this 'classification' back into nearby pixels in the original image space using a simple operator (the location-aware cross-attention we used).\nBesides, using the exact same SparseFormer for the classification and its weights might be a reason, and we expect that introducing more newly initialized transformer blocks could alleviate this gap."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2346/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700674244966,
                "cdate": 1700674244966,
                "tmdate": 1700674244966,
                "mdate": 1700674244966,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vSOwgAYDM7",
                "forum": "2pvECsmld3",
                "replyto": "CuSygqfCoR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2346/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2346/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response (cont'd)"
                    },
                    "comment": {
                        "value": "__W3. Ablation study on the RoI adjusting mechanism__\n\nWe have included the ablation study on the RoI adjuting mechanism from two aspects in Table 3 (b) and Table 3 (c) in the submission paper.\nWe repeat these two tables here.\n\n\n| method    | SF   | ViT/32 | ViT/32* | conv\u00d74 | swin |\n|-----------|------|--------|---------|--------|------|\n| top-1 acc | 81.0 | 72.8   | 74.3    | 79.4   | 79.7 |\n| GFLOPs    | 2.0  | 1.4    | 1.7     | 2.2    | 2.0  |\n\nTable 3 (b) shows the effectiveness of the focusing transformer to adjust RoIs and extract features to build 49 tokens, compared with the dense counterparts to produce 49 tokens.\nAll entries in the table use the same cortex transformer configuration.\nThe 'ViT/32' produces 49 tokens simply by patchifying an image into $32\\times32$ patches, rather than using the focusing transformer. \nThe 'ViT/32*' add two more transformer blocks to 'ViT/32'.\nThe 'conv\u00d74' uses 4 convolution+relu after the early convolution, where each convolution is with stride 2 and doubled output channel to produce 49 tokens.\nThe 'swin' exploit the shifted local attention used in [4] to produce 49 tokens, similar to the 'conv\u00d74'.\n\nThe focusing transformer shows its importance to adjust RoIs to prioritize foregrounds and exclude backgrounds, and therefore enables SparseFormer to perform more accurate recognition with fewer latent tokens, compared with dense counterparts to produce tokens.\n\n| $L_f$ | top-1 | GFLOPs |\n|-------|-------|--------|\n| nil   | 77.8  | 1.6    |\n| 1     | 79.7  | 1.7    |\n| 4     | 81.0  | 2.0    |\n| 8     | 81.0  | 2.5    |\n\nTable 3 (c) studies the number of iteration of the focusing transformer to adjust RoIs, $L_f$.\nThe 'nil' stands for the token RoIs are just learnable parameters of the model and do not adapt to different image content.\nIn other words, the RoI for a token does not adjust at all and keeps the same for all images in the 'nil' entry, and as expected, it is with an inferior result.\nWe can see that the iteration number is vital to the final performance, as the focusing transformer with the insufficient iteration may not effectively adjust tokens to foregrounds.\n\n[1] Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token merging: Your vit but faster. ICLR 2023.\n\n[2] Andreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit, and Lucas Beyer. How to train your vit? data, augmentation, and regularization in vision transformers. Trans. Mach. Learn. Res., 2022,\n\n[3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. ECCV, 2020.\n\n[4] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. CVPR, 2021"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2346/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700674260652,
                "cdate": 1700674260652,
                "tmdate": 1700674260652,
                "mdate": 1700674260652,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5ELJfNOL9C",
            "forum": "2pvECsmld3",
            "replyto": "2pvECsmld3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2346/Reviewer_fz8s"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2346/Reviewer_fz8s"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces the SparseFormer, which modifies the standard Transformer model by using a small number of tokens in latent space to reduce its size and computational complexity."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Provides an alternative sparse paradigm ($i.e.,$) for vision modeling compared to existing Transformers. Reduces computation by operating on limited tokens.\n2. Token ROI adjustment mechanism is effective at focusing on foregrounds.\n3. Visualizations show the model progressively focuses on discriminative regions."
                },
                "weaknesses": {
                    "value": "1. While the paper demonstrates the effectiveness of SparseFormer on classification tasks. The reviewer has concerns about the generalization to more complex scenarios. Appendix A.1 also points out the inferior performance compared to the recent transformer network. The use of specific sparse attention patterns might limit the model's ability to capture certain types of long-range dependencies in the images for downstream tasks. \n2. In addition, the reviewer also has concerns about token ROI. Adjusting token ROIs lacks strong spatial supervision. Performance on dense prediction tasks ($i.e.,$ segmentation tasks) requiring precise localization may suffer. With complex images, the signal will be weak and may not focus on the meaningful pixels."
                },
                "questions": {
                    "value": "Overall, this paper presents a step towards sparse vision architectures by a novel token ROI approach. The reviewer has no further questions, please see the weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethics concern."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2346/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698594899248,
            "cdate": 1698594899248,
            "tmdate": 1699636166844,
            "mdate": 1699636166844,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6P3zPciHpw",
                "forum": "2pvECsmld3",
                "replyto": "5ELJfNOL9C",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2346/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2346/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for reviewing our paper. The concerns are addressed in the following:\n\n__W1. Results in Appendix.1 and generalization to more complex scenarios__\n\nWe admit that the detection performance of SparseFormer is lagging behind DETRs.\nWe suspect that this is because we do not inject any positional information into latent tokens while the detection transformer training pipeline may require strong positional encodings to localize and suppress nearby redudant detections.\nBesides, we directly use the SparseFormer architecture for classification (without the introduction of extra transformer blocks) and its pre-trained weights to perform object detection, and this may not be the optimal design.\n\nWe agree that the sparse attention patterns might limit the long-term and precise positional dependency, which is important to the object detection task.\nHowever, this does not mean that SparseFormer cannot handle complex scenarios.\nAs a fact, the SparseFormer segmentation model performs well on much more complex ADE20K scenarios in Appendix. 2.\nThis is because SparseFormer follows a spatially divide-and-conquer manner when dealing such scenarios, where the location-aware cross-attention operator enables a token to only respond to a specific area.\n\n__W2. Concerns about RoI localization on dense prediction tasks__\n\nWe believe that the SparseFormer does not require the strong explicit spatial RoI localization supervision on dense prediction tasks.\nIndeed, for segmentation tasks, the spatial distribution of classification labels can serve as a coarse localization supervision.\nA token RoI in SparseFormer can be adjusted to focus on a consistent label area by such spatial label distributions and the end-to-end location-aware cross-attention discussed above.\nBesides that, the segmentation task does not necessitate well-localized token RoIs to perform well since the final classification is performed on the dense map, which is projected back from latent tokens and one pixel in the dense map is a mixture of nearby latent tokens with the semantic similarity."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2346/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700674218723,
                "cdate": 1700674218723,
                "tmdate": 1700674218723,
                "mdate": 1700674218723,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bWdnkWan99",
            "forum": "2pvECsmld3",
            "replyto": "2pvECsmld3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2346/Reviewer_fS4e"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2346/Reviewer_fS4e"
            ],
            "content": {
                "summary": {
                    "value": "The authors introduce SparseFormer, which comprises two main components: the Focusing Transformer and the Cortex Transformer. The Focusing Transformer addresses the challenge of extracting image features sparsely, decoding them into latent tokens, and adjusting token regions of interest (RoIs). The Focusing Transformer efficiently extracts image features with a computational complexity of O(N\u00b7P\u00b7C), where N is the number of latent tokens, regardless of the input image size. Evaluated on ImageNet, the authors demonstrated that the proposed method achieved 1.7x faster inference speed compared with Swin-T with small accuracy degradation. It also outperforms ResNet50 with a faster speed."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper is thoroughly motivated and exceptionally well-written. The concept of sparsifying input tokens holds paramount importance for vision transformers (ViTs) owing to the quadratic complexity with respect to sequence length in multi-head self-attention.\n\n2. The authors have designed a functional solution, known as FocusTransformer, which improves upon the Perceiver method by introducing and dynamically adjusting regions of interest (RoIs). Experimental results compellingly demonstrate the effectiveness of this architecture on the ImageNet dataset.\n\n3. The authors have not only illustrated how SparseFormer can reduce computational workload (measured in FLOPs), but they have also empirically shown a significant speedup on a V100 GPU under FP32 precision, further showcasing the efficacy of their proposed approach."
                },
                "weaknesses": {
                    "value": "1. While the authors have put considerable effort into elucidating the disparities between SparseFormer and Perceiver, it remains challenging for me to find a fundamental difference between these two methodologies. In my estimation, the primary distinction appears to be the introduction of the FocusTransformer. However, upon examination of this architecture, I have also observed a clear similarity to DeformableDETR. Consequently, I find it challenging to pinpoint the truly innovative contributions of this paper.\n\n2. The scope of the evaluation in this work appears somewhat limited. The presentation exclusively reports image classification results. However, Vision Transformers (ViTs) have showcased their efficacy across a diverse range of computer vision tasks, including object detection, semantic segmentation, and image generation. A majority of these applications typically demand high-resolution inputs, which makes the efficiency of reducing the number of visual tokens even more critical. My particular interest lies in understanding the applicability of the proposed approach to dense prediction tasks such as segmentation and image generation with diffusion models, given that the FocusTransformer seems to introduce token-level information loss.\n\n3. The section on speed evaluation is extensive, but it may benefit from further solidity. The reliance on the V100 GPU, which is considered somewhat outdated, raises questions in the context of contemporary Deep Neural Network (DNN) inference, where there is a preference for using lower precision formats like INT8 and FP16 with a TensorRT backend. Even though the proposed architecture is light in terms of FLOPs, I am concerned about the potential efficiency of the DeformableDETR-like FocusTransformer when integrated with TensorRT. It would be great if the authors could provide relevant results in this regard."
                },
                "questions": {
                    "value": "Please respond to my questions and concerns in \"Weaknesses\"."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2346/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2346/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2346/Reviewer_fS4e"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2346/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698701736880,
            "cdate": 1698701736880,
            "tmdate": 1700678459462,
            "mdate": 1700678459462,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NlC0iahYTg",
                "forum": "2pvECsmld3",
                "replyto": "bWdnkWan99",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2346/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2346/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thank you for your reviewing our paper. We will address your concerns in the following:\n\n__W1. Similarity to Perceiver and DeformableDETR architectures__    \nFirst, we would like to emphasize again that the aim of SparseFormer is to build a _sparse_ architecture for visual recognition tasks, and many efforts of this paper are put on _how to perform visual recognition with much fewer tokens in a transformer-like architecture_.\nThis distinguishes SparseFormer from either Perceiver or DeformableDETR, where both latters do not attempt to reduce the computation.\n\n__Difference with Perceivers__\n\nWe agree with that the primary distinction lies in the focusing transformer since\nits introduction enables SparseFormer to perform accurate recognition with a highly limited number of tokens.\nFrom Table 3(b) in the paper, where illustrate how 49 tokens are produced, one can see the effectiveness of the focusing transformer comparing with dense counterparts.\n\nAs noted in the paper, Perceivers use a naive cross attention layer to traverse the pixel or feature space in a dense manner $\\mathcal{O}(H\\cdot W\\cdot C)$, where the focusing transformer in our SparseFormer extract a few tokens from an image in a sparse way $\\mathcal{O}(N\\cdot P\\cdot C)$, regardless of the input height and width. Also, the size of latent space is also sparse, the typical Perceiver use 512 tokens with 1024 dimension, resulting in a total 512\\*1024 capacity, which is more than $8\\times$ the SparseFormer-B uses (81 tokens with 768 dimension).    \n\n__Difference with DeformableDETR__\n\nBuilding a sparse architecture necessitates a sparse variant of the basic operators on the image space.\nWe must admit that DeformableDETR and SparseFormer share the similar idea to use bilinear interpolation to extract image features, but the motivation and details differs a lot:\nDeformableDETR aims to achieve the shift invariance in detection transformers (DETRs), where the naive cross and self attention layers in DETRs struggle to handle, while SparseFormer uses bilinear interpolation to efficiently sample image features.\nAlso, deformable attention in DeformableDETR is typically performed on _the highly semantic feature map (e.g., after ResNets)_ and _in a multi-scale manner_, while SparseFormer uses _a single-level early conved feature map_ to perform sampling _in the very beginning of the network_.\nFrom this view, SparseFormer can be seen as a _decoder-only_ architecture for visual modeling except for the early convolution, which much differs from DeformableDETR that requires deformable self attention encoders on dense units upon the backbone.\n\n__W2. Dense prediction tasks and information loss__\n\nSparseFormer can perform dense prediction tasks like object detection and semantic segmentation.\nWe have included experimental results and discuss them in Appendix A.1 and Appendix A.2.\nSpecifically, Table 7 in the Appendix shows the detection results on MS COCO on a simple SparseFormer-S with only an extra linear classifier head and an extra MLP for regression, and the training recipe and loss also following the DETR.\nGiven that no positional information is injected into latent tokens and no multi-scale features are used, SparseFormer can still achieve meaningful detection results with about $0.3\\times$ FLOPs.\nTable 8 in the Appendix illustrates the segmentation performance of SparseFormer on ADE20K.\nOne can see that SparseFormer reaches the result on par with the dense Swin-T w/ UperNet with $0.18\\times$ GFLOPs.\n\nWe deeply understand your concern about information loss introduced by SparseFormer since the latent token capacity (e.g., 81\\*768) is indeed much smaller than the image size (e.g., 3\\*224\\*224).\nHowever, we believe that this is _not_ a significant issue for visual recognition tasks. Natural images contain a lot of pixel-level redundancy, such as backgrounds, which contributes little the final recognition.\nIn fact, our sparse paradigm is exactly based on this redundancy and the proposed SparseFormer learns how to discard the redudancy, or how to perform 'information loss', with the recognition supervision.\nAs for generation tasks like with diffusion models, this information loss issue becomes somewhat critical, and it is appealing to investigate how to incorporate SparseFormers into generation models with the least information loss.\nHowever, this is beyond the scope of this paper."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2346/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700674097066,
                "cdate": 1700674097066,
                "tmdate": 1700674177037,
                "mdate": 1700674177037,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EIpWpdixy4",
                "forum": "2pvECsmld3",
                "replyto": "nY9i1ZC6eP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2346/Reviewer_fS4e"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2346/Reviewer_fS4e"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for providing additional insights. I am pleased to note that many of my concerns regarding latency have been effectively addressed, and as a result, I am inclined to update my score to 5. However, I would like to emphasize the importance of incorporating FP16/INT8 TRT results in future revisions.\n\nConcerning the novelty aspect, I believe that the core innovation of DeformableDETR lies in indexing the feature map based on query-associated coordinates. The choice between bilinear interpolation and deformable attention appears to yield minimal differences in essence. The specific feature maps indexed (lower-level v.s. higher-level, single-scale v.s. multi-scale) do not matter in my opinion. Besides, there is an existing follow-up paper of DeformableDETR, DETR3D that also adopts bilinear interpolation. \n\nFurthermore, to enhance the impact of the paper, I suggest considering the inclusion of more segmentation/detection results within the main body rather than relegating them to the appendix. The findings in Table 8 are promising, and I encourage the authors to provide additional details, such as results on diverse datasets, latency comparisons, and exploration of various model architectures, in their forthcoming submissions. This could significantly strengthen the comprehensiveness and applicability of their work."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2346/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700678442641,
                "cdate": 1700678442641,
                "tmdate": 1700678442641,
                "mdate": 1700678442641,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oLBs64pTrY",
                "forum": "2pvECsmld3",
                "replyto": "bWdnkWan99",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2346/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2346/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We greatly appreciate for your kind reassessment on our work. Till now, we have not managed to deal with the type conversion bug (the issue is already mentioned by others due to customized ops, but not solved, https://github.com/pytorch/TensorRT/issues/2113). We will report FP16/INT8 in future revisions once this bug is resolved.\n\nRegarding the novelty, we agree that our method follows the same idea of indexing feature maps based on query-associated coordinates with Deformable-DETR and its follow-ups.\nBut the important thing is, that we show this query coordinate feature map indexing scheme can lead to sparse vision transformers with much fewer tokens, which is not explored at all to our best knowledge.\nTherefore, we believe that our SparseFormer per se has it novelty in exploring sparse vision architectures.\n\nWe are extremely grateful for your valuable and detailed suggestions on how to enhance the impact, the comprehensiveness, and the applicability of our work, especially for results in Table 8.\nIn future versions, we will reorganize this paper and cover as much experiments as possible per your suggestions."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2346/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700722291373,
                "cdate": 1700722291373,
                "tmdate": 1700722756309,
                "mdate": 1700722756309,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wx8yO1pqPi",
            "forum": "2pvECsmld3",
            "replyto": "2pvECsmld3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2346/Reviewer_SBDj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2346/Reviewer_SBDj"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a sparse paradigm for visual recognition, and introduced a novel backbone named SparseFormer, which has a lower memory footprint and higher throughput compared to dense architectures, especially in the low-compute region.\nExperiments show that the proposed method achieves a low memory and time cost while maintaining high performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed SparseFormer is novel and solid.\n2. While maintain the performance, SparseFormer has a low memory footprint and high throughout.\n3. The experiments are solid."
                },
                "weaknesses": {
                    "value": "None"
                },
                "questions": {
                    "value": "None"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2346/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698754538882,
            "cdate": 1698754538882,
            "tmdate": 1699636166681,
            "mdate": 1699636166681,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gy8cx7eyqF",
                "forum": "2pvECsmld3",
                "replyto": "wx8yO1pqPi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2346/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2346/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We greatly appreciate your positive rating of our paper. Thank you for your reviewing and assessment."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2346/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700674420342,
                "cdate": 1700674420342,
                "tmdate": 1700674420342,
                "mdate": 1700674420342,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]