[
    {
        "title": "Skills-in-Context Prompting:  Unlocking Compositionality in Large Language Models"
    },
    {
        "review": {
            "id": "8fAoF6l6bU",
            "forum": "s1ByDEbpI8",
            "replyto": "s1ByDEbpI8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1466/Reviewer_XFNL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1466/Reviewer_XFNL"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a prompting technique called Skills-in-Context (SKiC) prompting. The motivation for this prompting method is that compositional generalisation is an ability that helps to solve complex problems that are composed of existing skills, and the prompting method explicitly decomposes tasks into sub-skills. The prompting method can be used manually, by having a human construct a prompt for a task, as well as semi-automatically, by prompting LLMs to construct the prompt and having a human verify it. The authors evaluate their prompting technique on a set of reasoning tasks and show it improves over existing prompting techniques, sometimes by a large margin. The authors also do an ablation study where they find that for the semi-automatic method it helps if the prompt is constructed by the same model that is being tested, and an error analysis to find the most common source of errors is required skills for a task that are not in the prompt."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The authors compare their method with a large set of baselines and convincingly show their method is better for the evaluated reasoning tasks.\nThe authors present the interesting finding that it helps for a model to generate its own prompt, even if another model might construct a prompt that is inherently more correct / high-quality.\nThe authors do an error-analysis and identify the most common source of errors."
                },
                "weaknesses": {
                    "value": "This paper is written around the premise that this prompting method unlocks compositionality, but the way the authors use the term is unlike existing literature. Systematic compositionality is the ability to understand and produce novel combinations from known parts. This paper claims the prompting method SKiC *\"teaches LLMs to generalize to harder problems than they have seen and to problems that require innovative compositions of existing knowledge (either in context or inside model weights).\"* and that the authors *\"develop an effective one-stage prompting strategy [..] to unlock the general compositional generalization capability in LLMs\"*. However, the authors do not investigate at all whether the LLMs that are tested are trained on the tasks they give them. For example, the authors call the last-letter concatenation task of concatenating the last letter of 4 words or more *\"out-of-distribution\"*, but a simple Google search reveals an example for 5 words (https://tutorialspoint.com/print-last-character-of-each-word-in-a-string). It seems pretty likely to me that more examples are in the pre-training data of these models. \nFurthermore, the nature of systematic generalisation requires near-perfect generalisation. For models that achieve less than 50% accuracy on some of the tasks, you cannot say they are generalising systematically, because they are not.\nOther aspects of the paper make me think the authors have a different definition of systematic generalisation than most of literature, because they for example claim that Nye. et al (2021) develop the scratchpad method to *\"unlock its potential compositional generalization capabilities\"*, but there is nothing in that paper that indicates this is the case. Scratchpad prompting is developed to allow intermediate steps of computation and with it improves performance on multi-step computation problems.\n\nTo summarise; how do you know that LLMs have not been trained on the tasks you call \"out-of-distribution\"? Without knowing that, you cannot say the models are systematically generalising to unseen tasks. Additionally, when models do not reach near-perfect accuracy, it cannot be called systematic generalisation. I'd rephrame this paper to better reflect what it actually does, which is developing a prompting method that the authors empirically show to be superior to past prompting methods."
                },
                "questions": {
                    "value": "How do you define \"out-of-distribution\"?\n\nI encountered some typos etc. in the text:\n\t- \"Towards this goal, there have been a series of prompting strategies being developed to improve the reasoning and compositionality capabilities.\" paragraph 2 introduction\n\t- \"Put the asked words to a list. For example, put the words in 'apple' to D=['apple']\"; Figure 1\n\t- \"fine-tuneded baselines such as finetuned\" page 6"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1466/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698747827319,
            "cdate": 1698747827319,
            "tmdate": 1699636075532,
            "mdate": 1699636075532,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "MiBzt5f1MB",
                "forum": "s1ByDEbpI8",
                "replyto": "8fAoF6l6bU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1466/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1466/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your assessment and constructive feedback. We think that some concerns might be caused by misunderstanding, which we will explain in detail below.\n\n\n## On whether LLMs are trained on the tasks and the the definition of \u2018out-of-distribution\u2019\nFirst, in this work, we follow the previous work [1,2,3] and focus on the easy-to-hard generalization. Specifically, the in-distribution/seen tasks here means the testing samples are sampled from the same problem size with what have been shown in the context [3]. For example, we demonstrate examples of 2-digit addition, and then test it over samples that are also from 2-digit addition. In contrast, the out-of-distribution/unseen tasks here are defined to be the harder unseen variants of the problem which are with different complexity compared to examples provided in the context.  For example, the testing samples of 5-digit additions are the harder variant of the problem that are not seen in the context examples (2-digit additions) [3]. And we propose the SKiC prompts to improve such easy-to-hard compositional generalization [3] and complex reasoning tasks compared to previous methods.  In some of the tasks, our SKiC actually achieves near-perfect performances on harder cases, which are not demonstrated in the context. \n\nIn terms of the investigation on whether the LLMs have been pre-trained on some of the tasks, even if some of the tasks could be crawled into the pretraining corpus, we believe that they are mostly general and simple examples (e.g., last letters of 4 or 5 words)  rather than the harder cases that we tested on (e.g., last letters of 12 words). This is also demonstrated in the zero-shot performances on the harder cases: for example, the zero-shot performances of ChatGPT on last-letter, addition, multiplication and dynamic programming are quite low (lower than 50% in most of the cases)). With our SKiC prompting, the easy-to-hard generalization capability is significantly boosted to even near-perfect generalization, while other strong prompting methods such CoT and Least-to-Most cannot do so.\n\n[1]Decomposed Prompting: A MODULAR APPROACH FOR SOLVING COMPLEX TASKS\n\n[2]Least-to-Most Prompting Enables Complex Reasoning in Large Language Models\n\n[3] Faith and Fate: Limits of Transformers on Compositionality\n\n\n\n## On the performances which are less than 50% accuracy\n\nOn the tasks which do not reach near-perfect accuracy (i.e., multiplication), they are mainly constrained by the basic skills and the length of context windows. For example, our error analysis on the multiplication tasks  reveals that most of the errors in SKiC prompting are caused by missing the multi-digit addition capability, which can be incorporated as a basic skill in the prompting context. \n\nHowever, it would require an extra context window (over the limit when we perform the 4,5-digit multiplication) to calculate the addition following the adding SKiC prompts. So we did not include them which resulted in 48.5 performances with ChatGPT on 5-digits multiplication. \n \nWhen equipped with such extra skills, the most recent GPT-4 (128k context window) with SKiC can achieve near-perfect generalization. We further check the error cases and find that they are mainly caused by copying errors (copy wrong numbers from previous steps).\n\n| GPT4      | 2     | 3     | 4    | 5    |\n|-----------|-------|-------|------|------|\n| Zero-shot | 99.0  | 59.0  | 4.0  | 0    |\n| SKiC (without addition)      | 100.0 | 92.0 | 88.0 | 68.0 |\n| SKiC (with addition as basic skills)     | 100.0 | 100.0 | 99.0 | 93.0 |"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1466/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700556807868,
                "cdate": 1700556807868,
                "tmdate": 1700558211322,
                "mdate": 1700558211322,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "urMXv5l7wD",
                "forum": "s1ByDEbpI8",
                "replyto": "MiBzt5f1MB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1466/Reviewer_XFNL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1466/Reviewer_XFNL"
                ],
                "content": {
                    "title": {
                        "value": "thanks for the response"
                    },
                    "comment": {
                        "value": "Thank you for your responses, however I remain with my original point that this paper does not address compositional generalisation because it doesn't discuss / investigate whether or not the tasks are part of the training data. Just because zero-shot generalisation is low doesn't mean that the problems aren't part of the training data, and if they are you cannot claim systematic generalisation. To be clear, I don't think this paper does not have merit or doesn't propose a strong method. As I said in my original review, it's a good work \"developing a prompting method that the authors empirically show to be superior to past prompting methods.\" I just disagree with the current framing around compositionality, which is not what it tests, and hence should be re-written to reflect what it is actually about."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1466/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700666328828,
                "cdate": 1700666328828,
                "tmdate": 1700666328828,
                "mdate": 1700666328828,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QLgFfKLmKh",
                "forum": "s1ByDEbpI8",
                "replyto": "uHCHy05Dli",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1466/Reviewer_XFNL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1466/Reviewer_XFNL"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the detailed response"
                    },
                    "comment": {
                        "value": "I thank the authors again for the detailed responses, but I feel like they conflate two things: the contribution of this paper with a prompting method that can help with inherently compositional tasks, and the contribution of this paper with regards to compositional / systematic generalisation of large pre-trained transformers. The former you do not need to convince me, the paper does a good job. \n\nMy problem even after these new results remains with the premise that with this methodology you can be sure that the models are using compositional generalisation. I agree with the authors that compositional generalisation refers to the application of known skills for novel problems. What I disagree with is that you can say with your experiments that the models are using compositional generalisation to generalise without knowing with some certainty that they have not been trained on the tasks. \n\nI agree with the authors that it's unlikely that a sequence of 100 random strings is part of the training data, but I disagree that zero-shot failure and SKiC success means that they are using compositional generalisation. There's many open questions about how these models generalise, and we do not know for sure that they cannot do something in-weights if they cannot do it zero-shot. \n\nI agree that it's difficult to test these kind of questions in the current LLM era where training data is either too large to analyse or not even publicly available, but that is not a reason to resort to methods for which it is unclear what type of generalisation the model is using. It has become an incredibly hard and interesting research problem to figure out what kind of generalisation technqiue these models are using, and I do not believe this prompting method convincingly enough shows that they are compositionally generalising to make the strong claims that are made in the paper. I remain firm believing this paper should be re-written to reflect what it actually does, which is achieve high performance on compositional tasks with a novel prompting technique over other known prompting techniques. Reviewing such rewrite, I would happily recommend acceptance. However, from the current experiments it's difficult to say if they are generalising compositionally or not, at best you can say they might be and it requires further study to be sure."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1466/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700735561546,
                "cdate": 1700735561546,
                "tmdate": 1700735561546,
                "mdate": 1700735561546,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "s66etqBDz6",
            "forum": "s1ByDEbpI8",
            "replyto": "s1ByDEbpI8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1466/Reviewer_52w8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1466/Reviewer_52w8"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a prompting approach to improve compositional generalization in large LMs through an in-context learning approach. The approach is as follows:\n\n- First, provide the model with \"skills\" along with some examples showing how to invoke skills\n- Then, provide some examples of how to compose these skills to solve a task\n- Then with these demonstrations, see if the model can generalize to solving tasks that involve new compositions of tasks.\n\nAs with all prompting approaches, the main question is how much human knowledge / time went into designing the prompt and how generally applicable is the approach. Having said that, from results we find that the proposed approach (SKIC for Skill in-context prompting) improves performance over zero-shot and CoT prompted LMs, on challenging tasks including long digit addition / multiplication"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Lots of experiments of a simple approach, and very good results across the board.\n- The approach seems straightforward and easy to apply.\n- One of the novel aspects in the paper was to use an LM to mine a set of primitive skills from a dataset of demonstrations and then use the same LM to compose these skills. Here, there is much lower human intervention (though there are a few details I couldn't quite get about which experiments use author provided skills vs LM generated skills)."
                },
                "weaknesses": {
                    "value": "I think this work could have explored some of the technical aspects of \"compositional\" skill utilization in LMs beyond simple prompting:\n\n1. What are the limitations of this kind of generalization? Are there some compositions that all models fail at? How do you reconcile this apparent success at compositional generalization with other work that shows poor compositionality in LMs?\n2. Can this be used for semantic parsing tasks where an LM is prompted to discover skills?\n3. Can this be applied to general NLU tasks (e.g. textual entailment) by prompting an LM to discover skills? \n4. Can the LM discover the somewhat \"hand-designed\" skills of \"extracting digits\" / \"list length\" for addition? If not, what skills does the LM discover there?\n\nCurrently, the paper lacks answers to a lot of these technical questions. It's also not clear if this approach is generally applicable. The notion of a skill may not always be very clearly described in terms of natural language. But i'm happy to improve my score if some of these weaknesses are addressed."
                },
                "questions": {
                    "value": "- There is a whole sub-field in language and robotics where LLMs are used to decompose plans into sub-tasks. How do those approaches compare to SkiC?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1466/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698817368982,
            "cdate": 1698817368982,
            "tmdate": 1699636075461,
            "mdate": 1699636075461,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sirRRXsxRW",
                "forum": "s1ByDEbpI8",
                "replyto": "s66etqBDz6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1466/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1466/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the insightful feedback. Hope our clarification below addresses your comments.\n\n## On the limitations of easy-to-hard generalization\n\nPrevious work [1,2,3] introduced step-by-step reasoning and breaking down hard problems to simple problems to improve the easy-to-hard generalization. However, in our work, we make another important discovery that, in order to teach models how to compose skills, it is also crucial to demonstrate the foundational skills and how to ground each of its reasoning steps onto the foundation skills. That is, besides step-by-step reasoning, explicit grounding is another key factor to elicit compositionality and easy-to-hard generalization. We achieve that by proposing SKiC and our SKiC shows significantly better performances compared to previous work in all the experiments. Additionally, we add extra ablation study to highlight our finding (the importance of skill grounding in reasoning steps). We compare SKiC with the setting where (i) we remove the skills but keep the skill grounding in reasoning steps and (ii) we remove the skill grounding in reasoning steps but keep the basic skill introduction in the front. The performance on Dynamic Programming is shown below. Removing both parts would bring in the performance drop, which further indicates the importance of skills and skill grounding in reasoning steps to improve the easy-to-hard generalization.\n\n| ChatGPT | DP (8 numbers) |\n|-------------------|----------------|\n| COT | 72.0 |\n| SKiC | 98.0 |\n| - skills | 94.0 |\n| - skill grounding | 82.0 |\n\n  \n\n[1]Decomposed Prompting: A MODULAR APPROACH FOR SOLVING COMPLEX TASKS\n\n[2]Least-to-Most Prompting Enables Complex Reasoning in Large Language Models\n\n[3]  Faith and Fate: Limits of Transformers on Compositionality"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1466/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700556420411,
                "cdate": 1700556420411,
                "tmdate": 1700556420411,
                "mdate": 1700556420411,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "d9HtUtgHaQ",
                "forum": "s1ByDEbpI8",
                "replyto": "s66etqBDz6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1466/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1466/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### Discover skills for NLI (we use RTE as an example). \nWe first randomly sample two examples from RTE training dataset and generate the predictions.\n    \n```\n\nQ: Please identify whether the premise entails the hypothesis. \npremise: Dana Reeve, the widow of the actor Christopher Reeve, has died of lung cancer at age 44, according to the Christopher Reeve Foundation. \nhypothesis: Christopher Reeve had an accident\n\nChatGPT: The premise and hypothesis are separate statements, and the premise does not entail the hypothesis. The premise provides information about Dana Reeve, the widow of Christopher Reeve, and her cause of death, which is lung cancer. The hypothesis, on the other hand, is about Christopher Reeve having an accident, and it is not directly related to the information provided in the premise.\n\n  \nQ: Please identify whether the premise entails the hypothesis. \npremise: Security forces were on high alert after an election campaign in which more than 1,000 people, including seven election candidates, have been killed. \nhypothesis: Security forces were on high alert after a campaign marred by violence\n\nChatGPT: Yes, the hypothesis is consistent with the premise. The premise states that security forces were on high alert after an election campaign in which more than 1,000 people, including seven election candidates, have been killed. The hypothesis aligns with this by stating that security forces were on high alert after a campaign marred by violence, which corresponds to the information provided in the premise.\n\n```\n\nThen we prompt ChatGPT to discover the skills:\n\n```\nDiscover the skills from above examples to do the NLI tasks:\n\n```\n\nWhich result in the following output:\n\n```\n\nThe basic skills to solve NLI (Natural Language Inference) tasks involve understanding the relationship between the premise and hypothesis statements. Here are the key skills demonstrated in the examples:\n\n-   Context Understanding: Understanding the context and contextually relevant information is key. In both examples, comprehending the context provided in the premises is essential to evaluate the relationship with the hypotheses. This includes understanding details about individuals, events, and the cause-and-effect relationships presented in the text.\n    \n-   Inference Evaluation: The skill to evaluate whether the hypothesis is supported, contradicted, or unrelated to the information in the premise. In the first example, the inference evaluation involves determining that the premise about Dana Reeve's death does not support the hypothesis about Christopher Reeve's accident. In the second example, it involves recognizing that the premise supports the hypothesis about security forces being on high alert due to campaign violence.\n    \n\nThese skills collectively enable a reader or a model to make accurate assessments of the logical connections between statements in NLI tasks.\n\n  \n\n```\n\nWe could summarize basic skills from the LLMs: Context Understanding, Inference Evaluation. And we can further form the SKiC prompts:\n\n```\n\nSkill <Context Understanding>: Understanding the context and contextually relevant information.\n\nFor example,\n\n\u2018Dana Reeve, the widow of the actor Christopher Reeve, has died of lung cancer at age 44, according to the Christopher Reeve Foundation.\u2019\n\nThe sentence provides information about Dana Reeve, the widow of Christopher Reeve, and her cause of death, which is lung cancer.\n\n\u2018Christopher Reeve had an accident\u2019\n\nThe sentence provides information about Christopher Reeve having an accident\n\n  \n\nSkill <Inference Evaluation> The skill to evaluate whether the hypothesis is supported, contradicted, or unrelated to the information.\n\nFor example,\n\nPremise: Dana Reeve\n\nHypothesis: Christopher Reeve having an accident\n\nUsing the skill <Inference Evaluation>, the hypothesis is not related to premise.\n\n\n  \n\nCompose the above skills to solve the NLI tasks.\n\nExample #1:\n\npremise: Dana Reeve, the widow of the actor Christopher Reeve, has died of lung cancer at age 44, according to the Christopher Reeve Foundation.\n\nhypothesis: Christopher Reeve had an accident\n\nAnswer: 1. Using the skill <Context Understanding>, The premise provides information about Dana Reeve, the widow of Christopher Reeve, and her cause of death, which is lung cancer.\n\n2. Using the skill <Context Understanding>, the hypothesis is about Christopher Reeve having an accident.\n\n3.  Using the skill <Inference Evaluation>, the information in hypothesis is not directly related to the information provided in the premise. So the answer is Not entail.\n    \n\n```\n\n  \n\nThe SKiC prompts constructed by utilizing the skills discovered from LLMs could achieve better performances on RTE compared to COT baselines.\n\n| ChatGPT | RTE |\n|---------------------------------|------|\n| COT | 85.2 |\n| SKiC (Skills discovered by LLM) | 89.8 |\n\nWith the extra experiments in semantic parsing and discovering skills for lastletter and NLI, we believe that SKiC is generally applicable."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1466/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700556686970,
                "cdate": 1700556686970,
                "tmdate": 1700557925271,
                "mdate": 1700557925271,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4TjnI1gBhU",
            "forum": "s1ByDEbpI8",
            "replyto": "s1ByDEbpI8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1466/Reviewer_v2fa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1466/Reviewer_v2fa"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a prompting method termed as \"SKiC\" (skills-in-context), which equips the large language models (LLMs) with a set of pre-defined skills by leveraging their in-context learning capabilities. First of all, the method constructs a set of potential skills either from human annotation (distill skills via human) or model generation (distill skills via prompting LLMs). The authors find it is crucial to demonstrate both the skills and the compositional examples within the SKiC prompts. Also, SKiC prompting is shown to be able to solve unseen complex reasoning problems better than previous methods (CoT/Decomp/etc.)."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "**Clarity & Significance**\n\n- The effectiveness of the proposed method is clearly demonstrated through extensive experiments. The empirical observations indicate that the proposed method indeed improves the performance of LLMs across a range of sizes (davinci-003, ChatGPT, GPT4, etc.). The improvement is significant in many tasks, compared to several prior arts.\n- The overall presentation of this paper is clear and easy to follow. The evaluation including ablation studies is comprehensive. The appendix is very detailed."
                },
                "weaknesses": {
                    "value": "**Novelty**\n\nI believe the idea of including skills in context has been proposed in many previous works, which made me unsure about its novelty and whether the contribution of this work is significant. For example, [1] also focuses on the challenge of large number addition/multiplication, and the authors also proposed to include basic skills and their composition in the context. They have also shown significant improvement with in-context skills prompting when there are more digits involved. I'd love to hear from the authors what the novelty of this work is given [1] was proposed and publicly available nearly one year ago.\n\nHowever, many recent works have found using tools can significantly improve performance in math reasoning tasks (e.g., GSM8k and MATH), such as PAL [2] and Program-of-Thought [3], and they have shown that tool-using skills (rather than natural or symbolic language skills) can bring more gain to math reasoning tasks. I understand the main focus of this work is not about tool-using, but the method proposed here is a bit less appealing to me considering the current SoTA can reach 80+ (see [4]), especially when considering [1] already discussed the tool-using case.\n\n**Soundness**\n\nMy main concern is that the skills used in the SKiC are somehow mined from the test set (e.g., you create basic addition/multiplication skills and the compositional examples for GSM8k because you pre-know it is a math reasoning task and preview some of its questions), which might not be the case in practice. This raises the question of how the framework's performance would be affected if the skills were taken from a different set of tasks. For example, will I see any improvement in BBH object counting or penguins on a table task (both are about numeric reasoning) with the SKiC skills you designed for GSM8k? In other words, I guess there is a pre-condition of the success of SKiC, which you can somehow preview the test set of the target tasks, and then you can prepare the needed skills beforehand (either from human annotation or from LLM prompting), which I don't believe is the true definition of \"solving unseen tasks\".\n\nWhile the paper does not explicitly address this issue, it is possible that the performance of the framework could be affected if the skills were taken from a different set of tasks. This is because the skills used in the framework are specific to the domain of the target task test set, and may not be applicable to other domains. For example, if the skills were taken from the Last Letter Concatenation and tested on the GSM8K/MATH tasks, I don't think SKiC can lead to optimal performance.\n\n[1] Teaching Algorithmic Reasoning via In-context Learning\n\n[2] PAL: Program-aided Language Models\n\n[3] Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks\n\n[4] Solving challenging math word problems using gpt-4 code interpreter with code-based self-verification"
                },
                "questions": {
                    "value": "**Impact of the in-context compositional examples**\n\n1. How did you pick the two/N examples in your prompts? I believe the performance will be heavily affected by the choice of these examples. Did you run experiments or have a discussion on their impact?\n2. Does the order of these examples matter? Telling from [1] I think the order is another factor that will affect the performance, but the work does not discuss it.\n\n\n[1] Calibrate Before Use: Improving Few-Shot Performance of Language Models"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1466/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698836623418,
            "cdate": 1698836623418,
            "tmdate": 1699636075402,
            "mdate": 1699636075402,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QSMq4IxZd3",
                "forum": "s1ByDEbpI8",
                "replyto": "4TjnI1gBhU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1466/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1466/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your insightful comments. We humbly think that some concerns might be caused by misunderstanding, which we will clarify in detail below. We hope that our response can clarify the misunderstandings and you can consider our work more favorably.\n\n## On the difference from previous work \u201cTeaching Algorithmic Reasoning via In-context Learning\u201d\n\nThe major goal in [1] is to design algorithm-style prompts to first teach basic math skills like adding two numbers and then decompose hard problems into linear subproblems which could be solved by the usage of previously taught math operations such as multiplication through adding. This is actually similar to [2] which has been discussed in our Section 2.1.\n\nOur work is different and better than [1] in several aspects:\n\n(i) First, beyond algorithmic operation, we emphasize the explicit grounding of general basic skills and teaching models to compose multiple basic and grounded skills to improve the easy-to-hard generalization and the performances on complex tasks. For example, our SKiC for adding two number teach models to compose extract_digits skills, list_length skills, add_two_single_digit_number skills, sub_10 skills and compare_10 skills compare to the algorithmic style prompt in [1] and achieve better performances on adding tasks with ChatGPT as shown below:\n\n| Add | 2 | 3 | 4 | 5 | 6 | 7 |\n|---------------|-----|-----|------|------|------|------|\n| COT | 100 | 100 | 98.5 | 90.0 | 87.5 | 80.0 |\n| Algorithm [1] | 100 | 100 | 98.0 | 94.5 | 91.5 | 90.0 |\n| SKiC | 100 | 100 | 99.0 | 98.0 | 99.0 | 98.5 |\n\n  \n\n(ii) Second, not all the problems can be decomposed linearly and follow [1] to solve. Even though [1] formalize multiplication as a composition of multiple adds (e,g, 11*3 = (11+11) + 11), it is impossible and slow to call multiple adds when it comes to larger numbers like 128*128 (doing 128 additions is infeasible in terms of error propagation and computation cost (significantly longer context window or significant amount of extra calls of basic skills). Instead, our SKiC encourages models to perform multi-digits multiplication by composing single-digit multiplication skill and adding skill in only one stage, which also achieves significantly better performance with one single calls of the foundation model (e.g., ChatGPT) as shown below (when dealing with multiplication between two 3/4/5 digits number, linearly decomposition like [1] would result in exceeding context length.):\n\n| Mul | 2 | 3 | 4 | 5 |\n|---------------|-----|------|------|------|\n| COT | 99 | 54.5 | 13.0 | 2.0 |\n| Algorithm [1] | 100 | 68.0 | 20.0 | 0 |\n| SKiC | 100 | 82.0 | 72.0 | 48.5 |\n\n(iii) Third, our SKiC also encourages LLMs to utilize their internal skills beyond what have been specified in the context to solve complex math reasoning tasks like GSM8K and MATH, which can not be achieved by [1] which are limited to the usage of provided skills. Our proposed SKiC actually enables LLMs to utilize a wider range of internal skills to solve problems as shown in Figure 27, 28, 29, 30, 31, 32, 33 via automatically generating skills from LLM itself and grounding to both mentioned skills and unmentioned skills in examples. The overall performance on GSM8K with ChatGPT also demonstrated the powerfulness of our SKiC compared to algorithm calls in [1].\n\n| ChatGPT | COT | Algo [1] | SKiC |\n|---------|------|----------|------|\n| GSM8K-addition-only SubSet | 78.8 | 84.8 | 90.5 |\n| GSM8K | 74.4 | 79.5 | 87.2 |\n\n  \n  \n  \n\n[1] Teaching Algorithmic Reasoning via In-context Learning\n\n[2] Decomposed Prompting: A MODULAR APPROACH FOR SOLVING COMPLEX TASKS"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1466/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700556215074,
                "cdate": 1700556215074,
                "tmdate": 1700556215074,
                "mdate": 1700556215074,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WitlmsYET4",
                "forum": "s1ByDEbpI8",
                "replyto": "4TjnI1gBhU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1466/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1466/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "One example of the prompt and output for FOLIO task when using our SKiC prompts designed for GSM8K task is\n```\n>>> prompt\n[SKiC: skills for GSM8K]\n[SKiC: examples for GSM8K]\nFollowing the above examples, use your skills (not limited to the skills provided above) to solve the logical inference problem: Given a set of premise, decide whether the conclusion can be derived from the premise.\n\nPremise: [\"All people who regularly drink coffee are dependent on caffeine.\", \"People either regularly drink coffee or joke about being addicted to caffeine.\", \"No one who jokes about being addicted to caffeine is unaware that caffeine is a drug.\", \"Rina is either a student and unaware that caffeine is a drug, or neither a student nor unaware that caffeine is a drug.\", \"If Rina is not a person dependent on caffeine and a student, then Rina is either a person dependent on caffeine and a student, or neither a person dependent on caffeine nor a student. \"]\nConclusion: \"Rina is a person who jokes about being addicted to caffeine or unaware that caffeine is a drug.\"\nUse all your skills (not limited to the skills provided above)  to check: Can the conclusion be drawn based on the premise (answer True, False or Unknown)?\nAnswer:\n\n>>> ChatGPT output:\n<C>\n1. People either regularly drink coffee or joke about being addicted to caffeine. Using the Skill <Or>, this can be expressed as: \"People who do not regularly drink coffee either joke about being addicted to caffeine or are unaware that caffeine is a drug.\"\n2. No one who jokes about being addicted to caffeine is unaware that caffeine is a drug. Using the Skill <Not>, this can be expressed as: \"People who joke about being addicted to caffeine are aware that caffeine is a drug.\"\n3. Rina is either a student and unaware that caffeine is a drug, or neither a student nor unaware that caffeine is a drug. Using the Skill <Or>, this can be expressed as: \"Rina is either a student and unaware that caffeine is a drug, or Rina is neither a student nor unaware that caffeine is a drug.\"\n4. If Rina is not a person dependent on caffeine and a student, then Rina is either a person dependent on caffeine and a student, or neither a person dependent on caffeine nor a student. Using the Skill <If-Then>, this can be expressed as: \"If Rina is not a person dependent on caffeine and a student, then Rina is either a person dependent on caffeine and a student, or neither a person dependent on caffeine nor a student.\"\n5. All people who regularly drink coffee are dependent on caffeine. Using the Skill <All>, this can be expressed as: \"People who regularly drink coffee are dependent on caffeine.\"\n6. Combining the information, we can infer that:\n    i. Rina is either a person dependent on caffeine and a student, or neither a person dependent on caffeine nor a student (from premise 4).\n    ii. If Rina is a person dependent on caffeine and a student, then she is aware that caffeine is a drug (from premise 1).\n    iii. If Rina is neither a person dependent on caffeine nor a student, then she is neither aware that caffeine is a drug nor jokes about being addicted to caffeine (from premise 3).\n    iv. If Rina is not a person dependent on caffeine and a student, then Rina is either a person dependent on caffeine and a student or neither a person dependent on caffeine nor a student (from premise 4).\n    v. If Rina is a person dependent on caffeine and a student, then she jokes about being addicted to caffeine (from premise 2).\n    vi. If Rina is neither a person dependent on caffeine nor a student, then she does not joke about being addicted to caffeine (from premise 3).\n7. Combining the information from steps 6 and 5, we can further infer that:\n    i. If Rina is a person dependent on caffeine and a student, then she jokes about being addicted to caffeine.\n    ii. If Rina is neither a person dependent on caffeine nor a student, then she does not joke about being addicted to caffeine.\n8. Therefore, the conclusion \"Rina is a person who jokes about being addicted to caffeine or unaware that caffeine is a drug\" can be drawn based on the premise.\n9. The answer is True.\n</C>\n### True\n<EOD>\n```\n\nNote that instead of using the skills we provided in the context for GSM8K, the model is actually learnt to use their internal skills like Skill <Or> for this logical inference task."
                    },
                    "title": {
                        "value": "One example of the prompt and output for FOLIO task when using our SKiC prompts designed for GSM8K task"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1466/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700597864352,
                "cdate": 1700597864352,
                "tmdate": 1700598064490,
                "mdate": 1700598064490,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5FJVkjdrrW",
            "forum": "s1ByDEbpI8",
            "replyto": "s1ByDEbpI8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1466/Reviewer_wLQH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1466/Reviewer_wLQH"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new prompting approach for compositional reasoning, which forms exemplars as skills which can be flexibly composed by the LLM. This guides the LLM to ground its reasoning steps on skills which are already available in its knowledge. The method significantly improves performance on compositional generalization, achieving the  state-of-the art."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The idea proposed in the paper is intuitive but effective, and the paper is very well written.\nThe performance of the model is convincing, and the results show impressive performance on both composition over in-context skills and beyond in-context skills."
                },
                "weaknesses": {
                    "value": "Tables 1 and 2 could be better organized in the page."
                },
                "questions": {
                    "value": "1. In the beyond in-context skills setting, does the LLM use skills that are not specified in Figure 16?\n2. Is it possible to replace some of these skills with function calls to Python, for example, to decrease computation errors?\n3. More broadly what would be the pros and cons of letting the model access such \"external\" skills (if it is possible)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1466/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698859626544,
            "cdate": 1698859626544,
            "tmdate": 1699636075334,
            "mdate": 1699636075334,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "aCVAwPPFuJ",
                "forum": "s1ByDEbpI8",
                "replyto": "5FJVkjdrrW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1466/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1466/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1466/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1466/Reviewers",
                    "ICLR.cc/2024/Conference/Submission1466/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission1466/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1466/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your positive assessment and constructive feedback.\n\n  \n\n## On the question about the use of skills in the beyond in-context skills setting\n\n  \n\nYes, in the beyond in-context skills setting, our methods would encourage models to use skills in their pre-trained knowledge that are not provided in the provided context.\n\n  \n\nFor GSM8K, Figures 27 and 28 show the cases where models utilize skills that are not specified in Figure 16 (e.g., \u201cround\u201d and \u201ccompare\u201d)\n\n  \n\nFor MATH, Figure 29, 30, 31, 32, 33 show the examples where models utilize skills that are not specified in context (Figure 17). Also, Table 12 in the Appendix shows the most frequent skills that are from Figure 16 and the most frequent skills that are not specified in Figure 17.\n\n  \n\n## The possibility to replace some of the skills with function calls\n\n  \n\nYes, it is possible to replace several skills with function calls like python which could be used to decrease some computation errors. This is complementary to our proposed methods. But in this work, the major goal is to explore the inherent capabilities of the language model itself. That is, we would like to examine and unlock the inherent abilities of LLMs to compose different skills for complex tasks.\n\n  \n\n## The pros and cons of letting the model access \u2018external\u2019 skills\n\n  \n\nThe pros of using \u2018external\u2019 skills (such as functional / api calls) is that we can even further boost performance by incorporating additional strong basic skills. For example, if we provide an api call of python interpreter as an external basic skill, it can generally reduce the naive computation error involved in math reasoning tasks.\n\n  \n\nThe cons are: \n(i) External calls might bring in extra latency during inferences as LLMs need to call external skills multiple times in multiple stages. As a result, we mainly focus on whether LLM itself could solve tasks in one-stage through our SKiC. \n(ii) It is hard/impossible to enumerate all the needed external skills (external calls) in the context for complex tasks, which would lower down the generalization abilities if the models are taught to rely on provided external calls. So, our SKiC also encourages models to utilize their internal skills which are not provided in the context to solve complex tasks.\n\n  \n\nIn a nutshell, the main focus of this paper is to investigate the inherent capability of compositional generalization by LLM itself. Integrating with external functional calls as additional basic skills is complementary to this work, and we leave it as a future work.\n\n  \n\n### The organization of Table 1 and 2\n\nWe will better distribute Table 1 and 2 in the revised version."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1466/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700556034184,
                "cdate": 1700556034184,
                "tmdate": 1700556034184,
                "mdate": 1700556034184,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]