[
    {
        "title": "HeaP: Hierarchical Policies for Web Actions using LLMs"
    },
    {
        "review": {
            "id": "xmk9BvbTI7",
            "forum": "tcFcKyJgRM",
            "replyto": "tcFcKyJgRM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6350/Reviewer_NxCH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6350/Reviewer_NxCH"
            ],
            "content": {
                "summary": {
                    "value": "The work presents their design of a web action agent with LLM that can follow the natural language instruction and perform actions on the website. The agent consists of a high-level planner and a low-level actor, both of which invoke LLM for concrete output given different input context and prompt. The core idea is to decompose the task to achieve higher performance and generalization capacity."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The design of this method is sound and reasonable. \n\nExhaustive details of the prompt and results analysis are presented."
                },
                "weaknesses": {
                    "value": "The evaluation is based on too few samples: 45 tasks on MIniWob++, 125 examples of two domains on WebArena, 5 distinct tasks with 20 scenarios on Ariline CRM, and 3 website with 10 searches per site on Live Websites.\n\nGiven the fact that human demonstration is collected to form the prompts to the LLM in HEAP, it should be actually evaluated on more diverse websites instead of fewer websites."
                },
                "questions": {
                    "value": "1. Could the author elaborate on the demonstration collection process described in 4.2?\n\n2. What's the purpose of D_{label} on the end of page 4 (Sec 4.2)? How does this dataset utilized in the HEAP?\n\n3. What is the trainable component in the HEAP? e.g. which function / parameters described in Algorithm 1 is learnable? \n\n4. Why the models are not evaluated on the entire benchmarks but only a set of them. \n\n5. What does training size 21 in Table 1 last row mean? The HEAP was shown 21 samples, in which format, to train which part?\n\n6. It really depends on the demonstration collected and the diversity of evaluation cases that whether the benefit claim of \"sample efficient\" and \"generalization\" are sound."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6350/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697759866913,
            "cdate": 1697759866913,
            "tmdate": 1699636700345,
            "mdate": 1699636700345,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HSCfJpLdlT",
                "forum": "tcFcKyJgRM",
                "replyto": "xmk9BvbTI7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6350/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6350/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their feedback! Please find below our response:\n\n**The evaluation is based on too few samples: 45 tasks on MIniWob++, 125 examples of two domains on WebArena, 5 distinct tasks with 20 scenarios on Ariline CRM, and 3 website with 10 searches per site on Live Websites.**\n\n* We provide the most comprehensive evaluation on web tasks, i.e. 4 distinct environments including live websites and totaling ~2500 evaluations. Prior works in this domain [1-5] (in Table 1) evaluate on 1-3 datasets, and none evaluate on live websites.\n* On MiniWoB++, we evaluate all tasks that do not require visual inputs (see Table 4 in Appendix B for detailed breakdown). Each task was evaluated on 50 seeds totalling 2250 (=45*50) episodes or trajectories lasting 2-10 timesteps.\n* We evaluate two distinct domains on WebArena, a SOTA challenging dataset that came out only a couple months ago. We outperform the best performing LLM agent in their paper by a factor of 2x by leveraging hierarchy (see Fig. 4). None of the prior works in Table 1 evaluate on WebArena. In fact, since prior works use large amounts of demonstrations (e.g. 347k, 12k, 2.2M), it\u2019s not possible to use those on WebArena tasks since they don\u2019t have large-scale human demonstrations yet.\n* We can certainly evaluate up to 100 scenarios on Airline CRM, but the confidence intervals are fairly tight.\n\n\n**Given the fact that human demonstration is collected to form the prompts to the LLM in HEAP, it should be actually evaluated on more diverse websites instead of fewer websites.**\n* We reiterate that to the best of our knowledge is this is the most diverse evaluation on web automation tasks compared to prior works [1-5] in this domain.\n* Live websites are challenging since web pages are complex containing ~12k tokens, that we compress down to 2k tokens in text formats for an LLM without loss of performance (see Fig. 6). The purpose of showing live website evaluations is to show the real-world applicability of our method.\n* It is not feasible to do extensive evaluation on the live websites (none of the prior works [1-5] in this domain do this either):\n    * Bot detectors\n    * Safety constraints\n    * Evaluating success\n    * Require visual reasoning that needs multi-modal models\n\n**Could the author elaborate on the demonstration collection process described in 4.2?**\n* Following is the excerpt from Appendix D.1 where we detail this\n    * We collect demonstrations from a human user who is given a conversation context \\phi and performs a sequence of actions in the browser to complete the task.\n    * We design a browser plugin to record webpage DOM d and events such as clicks and types.\n    * Each demonstration is parsed as text by converting the DOM tree into a list of salient web elements like links, buttons, inputs. Each element is represented as text, e.g. <button id=18 title=\"Travelers\">1 Adult />. Action events are parsed as a CLICK <id> or TYPE <id> <val>. \n\n**What's the purpose of D_{label} on the end of page 4 (Sec 4.2)? How does this dataset utilized in the HEAP?**\n* D_label is used to generate examples for both high-level and low-level prompts.\n* Following is the excerpt from Appendix D.2 where we detail this\n    * Once we have a dataset of labeled demonstrations D_{label} we can use that to generate base prompts for both the high-level task planner and low-level policies. \n    * For the task planner, we concatenate the following input\u2013output pairs as in-context examples in the base prompt: {input: [context \\phi, initial state s0]} and {output: sequence of web policy calls (\\pi1, \\psi1), (\\pi2, \\psi2) . . . , (\\piT , \\psiT )}. For each web policy, we search the dataset D_label for all instances of the policy and create a prompt by concatenating examples of {input: [instruction \\psi_t, current state s_t, previous actions a_{t:t\u2212k} ]} and {output: next action a_t}.\n\n**What is the trainable component in the HEAP? e.g. which function / parameters described in Algorithm 1 is learnable?**\n* Since HEAP uses in-context learning, the parameter that can be varied is the input prompt (P_task in Line 7, P_policy in Line 12), specifically the few-shot examples in the prompt.\n\n**Why the models are not evaluated on the entire benchmarks but only a set of them.**\n* On MiniWoB++, we evaluate all tasks that do not require visual inputs.\n* WebArena is a recent and challenging benchmark, and we picked two domains that were easy to collect demonstrations and did not require visual inputs.\n\n**What does training size 21 in Table 1 last row mean? The HEAP was shown 21 samples, in which format, to train which part?**\n* Please see Table 3 which shows the breakup of the 21 examples in HeaP by examples per policy (see row MiniWob++).\n* Each example is a tuple of (context, browser content, previous action, action). The examples are used as part of the prompt that is sent to an LLM to predict the action. See Appendix G.1, Listing 2 for the final prompt with examples."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6350/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700010778916,
                "cdate": 1700010778916,
                "tmdate": 1700010778916,
                "mdate": 1700010778916,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xEIR7Ij9Wt",
            "forum": "tcFcKyJgRM",
            "replyto": "tcFcKyJgRM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6350/Reviewer_4BRb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6350/Reviewer_4BRb"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a framework that learns hierarchical prompts from demonstrations for planning high-level tasks and executing them via a sequence of low-level policies. The approach decomposes complex web tasks into a sequence of high-level subtasks, each of which can be then solved by a sequence of low-level policies. This method learns hierarchical LLM prompts for both levels of tasks and policies. The approach was evaluated on a range of increasingly complex benchmarks and the results show that the proposed approach achieves excellent performance compared with existing approaches."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The approach introduces a novel hierarchical approach to prompt LLMs to perform web tasks. Experimental results on various complex web benchmarking datasets show the superiority of the proposed approach."
                },
                "weaknesses": {
                    "value": "I recommend moving some implementation details like prompts into the main body to help the reader better understand the work."
                },
                "questions": {
                    "value": "Do you label the high-level task plans?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6350/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6350/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6350/Reviewer_4BRb"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6350/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698555713761,
            "cdate": 1698555713761,
            "tmdate": 1699636700229,
            "mdate": 1699636700229,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VnJ3DZPeiB",
                "forum": "tcFcKyJgRM",
                "replyto": "xEIR7Ij9Wt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6350/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6350/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their feedback! Please find below our response: \n\n**I recommend moving some implementation details like prompts into the main body to help the reader better understand the work.**\n* We are happy to do so\n\n**Do you label the high-level task plans?**\n* We do not. The demonstrations collected from the human demonstrations are states and low-level actions. We then use \u201cautolabeling\u201d at each timestep to label the low-level policy that was being executed. Our scripts convert the labeled datasets to prompts for both high-level planners and low-level policies. Finally, we augment prompts with chain-of-thought reasoning."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6350/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700010970786,
                "cdate": 1700010970786,
                "tmdate": 1700010970786,
                "mdate": 1700010970786,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "fgGmSXNhF6",
            "forum": "tcFcKyJgRM",
            "replyto": "tcFcKyJgRM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6350/Reviewer_oBNG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6350/Reviewer_oBNG"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a method HeaP for LLM to perform tasks on the web. It first asks LLM to decompose the task into several steps as a planner, where each step is a call to low-level policies. Then within each low-level policy, LLM is called to predict the next action sequentially. Both the planner and low-level policy execution have prompts constructed automatically by collecting few shot examples from autolabeled human demonstrations. Extensive experiments over several datasets demonstrate the gain over ReAct which does not use hierarchical planning."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is overall easy to read, although some important methodological details like autolabeling and prompt construction are in appendix which makes it hard to read.\n- The experiments are extensive over 4 datasets with many tasks. The gain demonstrated is substantial."
                },
                "weaknesses": {
                    "value": "- The idea of hierarchical planning with a high-level planner and low-level policies using LLM has been explored by many previous robotics works e.g. LLM-planner (https://arxiv.org/pdf/2212.04088.pdf and the line of works they cited). Additionally, PaP (https://aclanthology.org/2022.suki-1.8.pdf) and Parsel (https://arxiv.org/pdf/2212.10561.pdf) have also explored similar ideas of prompting LLM to generate a hierarchical plan but implementing low-level planners with programs. Implementing both high-level and low-level planners with LLM prompting has been explored in Decomposed Prompting (https://openreview.net/pdf?id=_nGgzQjzaRy). Considering these previous works, the novelty of this paper is limited to applying existing ideas to web datasets and potentially the technical details of autolabeling from human demonstrations.\n- The low-level policies are manually defined during autolabeling, making the framework limited in flexibility comparing to previous works that allow LLM to generate decompositions freely. \n- The only LLM prompting baseline compared against is ReAct, which demonstrates the benefits of hierarchical planning. However, such benefits have been demonstrated with the prior works mentioned above."
                },
                "questions": {
                    "value": "n/a"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6350/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6350/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6350/Reviewer_oBNG"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6350/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698777532611,
            "cdate": 1698777532611,
            "tmdate": 1699636700111,
            "mdate": 1699636700111,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "N9jCqGPpqv",
                "forum": "tcFcKyJgRM",
                "replyto": "fgGmSXNhF6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6350/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6350/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the feedback! Please find below our responses:\n\n**The idea of hierarchical planning with a high-level planner and low-level policies using LLM has been explored by many previous robotics works**\n\nThere seems to be misunderstanding on the claimed contributions of this paper. \n* We agree that hierarchical planning is a fundamental concept in AI and has been the bedrock of robotics and decision making at large.\n* We don\u2019t make any universal claims on hierarchical planning with LLMs\n    * Although we note that at the time of writing this paper there is no work (to the best of our knowledge) that uses LLMs for both high and low-level  planning\n* Instead our focus is on enabling LLMs to solve tasks on the web\n   * This is an increasingly important domain, where we present the first results from an LLM based approach that achieves SOTA results with orders of magnitude lower demonstrations (see Table 1). We do so by showing that hierarchical policies are essential for solving web tasks. \nWe provide the most comprehensive evaluation on web tasks, i.e. 4 distinct environments including live websites and totaling ~2500 evaluations. Prior works in this domain [1-5] (in Table 1) evaluate on 1-3 datasets and none evaluate on live websites.\n\nWe are happy to update the writeup to clear any misunderstanding. \n\n**Considering these previous works, the novelty of this paper is limited to applying existing ideas to web datasets and potentially the technical details of autolabeling from human demonstrations.**\n\nThank you for the related works, we will include the relevant ones in the paper. However, we note that:\n* None of them propose hierarchical LLM policies (LLM is only used as the high-level planner)\n* None of the them look at the web domain\n* Some of them don\u2019t even look at planning tasks (e.g. Q&A)\n* None of them propose a method to auto-generate prompts by autolabeling demonstrations\n\nGoing into further details\n* LLM-planner https://arxiv.org/pdf/2212.04088.pdf\n    * Focuses exclusively on vision-language navigation, a very different domain\n    * Moreover, only the high-level planner is LLM, low-level planner is a combination of engineered/learned components\n    * The authors themselves mention that none of the cited works use LLMs for hierarchical planning in the VLN domain.\n* Paap https://aclanthology.org/2022.suki-1.8.pdf\n    * Focuses on defining a hierarchical grammar. Doesn\u2019t use LLMs or train a model.\n* Parsel https://arxiv.org/pdf/2212.10561.pdf\n    * Uses a LLM to decompose a natural task into a bunch of subtasks that is then fed to a codeLM to generate a code\n    * There is no policy / feedback from the real world\n* Decomposed prompting https://openreview.net/pdf?id=_nGgzQjzaRy\n    * Does not look at planning tasks but rather question answering\n    * Requires users to manually annotate low-level policies while we autolabel  \n\n**Why is the web simply not another dataset?**\n\n* Web is an increasingly important and challenging domain for decision making\n* Many fundamentals challenges to creating web agents:\n    * Countless ways to interact with the web, leading to a combinatorially large number of tasks.\n    * Web interfaces differ from one website to another. It is intractable to cover all such variations in the prompt.\n* We achieve SOTA results compared to prior web automation works and do this with orders of magnitude less data (see Table 1). To achieve this, we had to solve a number of problems:\n    * Intractable to cover all tasks and web interface combinations in a single prompt, that we tackle with our hierarchical LLM approach, HeaP\n    * Tedious for users to segment demonstrations into low-level policies, that we automate via autolabellng (see Appx D)\n    * Complex web pages that can have up to 12k tokens, that we compress to 2k without loss of performance (see Fig. 6)\n\n[1] Liu et al.. Reinforcement learning on web interfaces using workflow-guided exploration, ICLR 2018. https://arxiv.org/abs/1802.08802\n\n[2] Humphreys et al., A data-driven approach for learning to control computers, ICML 2022. https://arxiv.org/abs/2202.08137\n\n[3] Gur et al., Understanding HTML with large language models, arXiv 2023 https://arxiv.org/abs/2210.03945\n\n[4] Furuta et al., Multimodal web navigation with instruction fine tuned foundation models. arXiv, 2023. https://arxiv.org/abs/2305.11854\n\n[5] Yao et al.. React: Synergizing reasoning and acting in language models. ICLR 2023. https://arxiv.org/abs/2210.03629"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6350/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700010233824,
                "cdate": 1700010233824,
                "tmdate": 1700010233824,
                "mdate": 1700010233824,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LoP2A2lcfI",
                "forum": "tcFcKyJgRM",
                "replyto": "qUzBZDoglr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6350/Reviewer_oBNG"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6350/Reviewer_oBNG"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the detailed response. I will maintain the current score."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6350/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700673375028,
                "cdate": 1700673375028,
                "tmdate": 1700673375028,
                "mdate": 1700673375028,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ZMHbTgCDhJ",
            "forum": "tcFcKyJgRM",
            "replyto": "tcFcKyJgRM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6350/Reviewer_aRc1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6350/Reviewer_aRc1"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces HeaP, a framework that leverages Large Language Models (LLMs) to decompose complex web tasks into modular sub-tasks. It uses a hierarchical approach, learning high-level task plans and low-level policies from human demonstrations, enabling LLMs to perform web actions effectively. It addresses challenges related to the combinatorially large space of web tasks and variations in web interfaces. Experimental results demonstrate that HeaP outperforms previous methods with significantly fewer training examples on various web tasks and interfaces such as MiniWoB++, WebArena, and a mock airline CRM"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "-Originality: The idea is interesting in the way HeaP leverages hierarchical policies to decompose complex web tasks using a high-level task planner \n into modular  low-level web policies.\n\n-Quality: The paper is quite thorough in its experimental setup as it tests on 4 interesting datasets, including simulated and live websites, to assess the performance of the proposed approach. \n\n-Clarity: The paper is well-written and structured, making it easy for readers to follow and understand the proposed approach\n\n-Significance: The paper addresses a significant challenge in the field of natural language processing and machine learning, which is teaching LLMs to perform web-based tasks which can lead to a huge set of applications"
                },
                "weaknesses": {
                    "value": "- The tasks are not that challenging and the results are very weak relative to how powerful the LLM model used here which is GPT-3.5. For example, it seems that the proposed method struggles with book-flight which is a basic constrained task and therefore this method is very far from being deployed in the real world\n\n- Using closed source methods like GPT-3.5 is expensive. I'd be curious to see how this method would perform with open source methods like Llama and Mistral.\n\n- No code was provided to asses and verify the results as well as understand the low level details of how the method is implemented"
                },
                "questions": {
                    "value": "Please address the weaknesses above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6350/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699278170804,
            "cdate": 1699278170804,
            "tmdate": 1699636700000,
            "mdate": 1699636700000,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dig3OdUmhJ",
                "forum": "tcFcKyJgRM",
                "replyto": "ZMHbTgCDhJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6350/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6350/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their feedback! Please find below our responses:\n\n**The tasks are not that challenging .. seems that the proposed method struggles with book-flight which is a basic constrained task .. method is very far from being deployed in the real world**\n\n_Contextualizing work against existing baselines and benchmarks_\n\n* We rigorously compare our approach against baselines, including prior works and zero vs few-shot and flat vs hierarchy methods. We evaluate tasks across 4 distinct environments including live websites, totaling ~2500 evaluations.\n* We achieve SOTA results compared to prior web automation works [1-5] and do this with two orders of magnitude less data (see Table 1). \n* Live websites are challenging since web pages are complex containing ~12k tokens, that we compress down to 2k tokens in text formats for an LLM without loss of performance (see Fig. 6, Appx F). \n* The purpose of showing live website evaluations is to show the real-world applicability of our method. However, it is not feasible to do extensive evaluation on the live websites (none of the prior works [1-5] do this either) due to:\n   * Bot detectors\n   * Safety constraints\n   * Evaluating success\n   * Require visual reasoning that needs multi-modal models\n\n_On MiniWob++ book-flight task_\n\n* While prior works scored 0.00, 0.87, 0.00, 0.48, 0.00 on this task, our approach gets a success rate of 0.9 (see task-wise performance breakdown in Table 4, Appx B)\n* This task's complexity lies in navigating UI elements such as date grids and applying filters, e.g., \"shortest,\" \"one-way vs. round trip,\" \"cheapest\", over multiple steps with changing pages (see Fig. 8 for illustration)\n* HeaP handles these challenges through dedicated low-level policy prompts like \"choose-date\", \"fill-text,\" that get composed given a high-level task like \"book-flight\"\n\n_On airlinecrm book-flight task_\n\n* Our motivation for building this simulator that we plan to open-source was to simulate longer-horizon tasks with more complex webpages (see Appx E)\n* The book-flight task is connected to a mock database and involves >20 steps (as opposed to ~10 steps in MiniWoB++). \n* The complexity lies in solving more involved subtasks like adding passenger details, payment information, selecting flight schedule while interacting with diverse UI elements like text boxes, selection links, date boxes.\n* We show benefits of both hierarchy and few-shot examples in solving these tasks\n\n_Limitations and Open Challenges_\n\nWe acknowledge there are many limitations and open challenges that need to be solved before our approach can be safely and reliably deployed in the real-world on an open set of tasks. We list these in Limitations (Sec 6) and Broader Impacts (Appx A), in particular,\n\n* **Complex webpages.** Parsing pages containing long tables, databases, etc needs advanced compression techniques such as learning dedicated saliency models. \n* **Multimodal observations.** HeaP is currently unable to handle pages with visual only components since those observations don\u2019t get parsed from the HTML DOM. Leveraging pretrained multi-modal models offer a promising avenue.\n\n* **Verification.** Real-world applications would require a verification module to ensure the successful completion of actions, especially in cases where websites may crash or reset. Using LLMs to verify actions is another promising direction of future work.\n\n* **Error Recovery.**  HeaP may click on a wrong link sending it to a new webpage and must learn to recover from such errors. Learning from incorrect actions either via human feedback or self-verification are interesting directions of future work.\n\n* **Safety.** Finally, action LLMs carry potential for misuse given their execution on open-domain environments, requiring careful security and verification solutions.\n\nWe are happy to include or expand out any points here that the reviewer thinks are important!\n\n[1] Liu et al.. Reinforcement learning on web interfaces using workflow-guided exploration, ICLR 2018. https://arxiv.org/abs/1802.08802.\n\n[2] Humphreys et al., A data-driven approach for learning to control computers, ICML 2022. https://arxiv.org/abs/2202.08137\n\n[3] Gur et al., Understanding HTML with large language models, arXiv 2022 https://arxiv.org/abs/2210.03945\n\n[4] Furuta et al., Multimodal web navigation with instruction fine tuned foundation models. arXiv, 2023. https://arxiv.org/abs/2305.11854\n\n[5] Yao et al.. React: Synergizing reasoning and acting in language models. ICLR 2023. https://arxiv.org/abs/2210.03629"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6350/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700169538102,
                "cdate": 1700169538102,
                "tmdate": 1700169538102,
                "mdate": 1700169538102,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nDtHWWZ9Eb",
                "forum": "tcFcKyJgRM",
                "replyto": "ZMHbTgCDhJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6350/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6350/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We ran evaluations with the llama2-13b model and include results along with common failure modes as Appendix G in the paper. Overall, we find the performance to be lower than gpt-{3,3.5,4}. The drop in performance could be due to a number of reasons such as the model size or the training data on which the models are trained. However, we find that HeaP still outperforms Flat for many tasks. We also observe that the few-shot examples do not improve performance. We think that fine-tuning the model on a larger dataset of demonstrations can help address some of these failure modes and leave that as interesting future work."
                    },
                    "title": {
                        "value": "Llama2 evaluation"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6350/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700507317048,
                "cdate": 1700507317048,
                "tmdate": 1700683799390,
                "mdate": 1700683799390,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]