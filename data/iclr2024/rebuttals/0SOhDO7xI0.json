[
    {
        "title": "DeepDRK: Deep Dependency Regularized Knockoff for Feature Selection"
    },
    {
        "review": {
            "id": "8SxH0lQZcB",
            "forum": "0SOhDO7xI0",
            "replyto": "0SOhDO7xI0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4515/Reviewer_CLhf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4515/Reviewer_CLhf"
            ],
            "content": {
                "summary": {
                    "value": "The authors develop a distribution-free deep learning method for knockoff generation which strikes a balance between FDR and power, called \u201cDeep Dependency Regularized Knockoff\u201d (DeepDRK). In DeepDRK, a \u201cmulti-swapper\u201d adversarial training procedure is proposed to enforce the swap property, while a sliced-Wasserstein-based dependency regularization (together with a novel perturbation technique) is introduced to reduce reconstructability. Experiments on real, synthetic, and semi-synthetic datasets are carried out to show the good performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1) Proposed a distribution-free deep learning method for knockoff generation which strikes a balance between FDR and power.\n2) A DeepDRK pipeline is provided to increase readability.\n3) A number of experimental results are provided on simulated, semi-simulated and real datasets to illustrate the performance of the proposed method."
                },
                "weaknesses": {
                    "value": "1) Though it enjoys theoretical result that a no free lunch situation for selection power when there is exact reconstructability in Appendix B, it seem that there is no theoretical guarantees on the power or explanations for that how the sliced-Wasserstein-based dependency regularization together with a novel perturbation technique introduced to reduce reconstructability can promote selection power.\n2) It is not clear that how to enforce the swap property by the \u201cmulti-swapper\u201d adversarial training procedure.\n3) The motivation behind feature selection is high-dimensional data settings, which in my understanding means that the number of features is larger than the number of examples in the dataset. However, none of the simulated experiments include such scenario. \n\nExamples of writing problems:\n-\u201cSimilar observations can be found in Figure 4.\u201d seems to be \u201cSimilar observations can be found in Figure 5.\u201d in the paragraph \u201cResults\u201d of section 4.4.\n-\u201cAmong them, model-specific ones such as AEknockoff (Liu & Zheng, 2018) Hidden Markov Model (HMM), knockoff (Sesia et al., 2017)\u201d seems to be \u201cAmong them, model-specific ones such as AEknockoff (Liu & Zheng, 2018), Hidden Markov Model (HMM) knockoff (Sesia et al., 2017)\u201d in the first paragraph of section 2.2."
                },
                "questions": {
                    "value": "(1) More explanations for the proposed method striking a balance between FDR and power.\n(2)The diagram of DeepDRK pipeline and code library are given, but the algorithm for training objective (4) is not provided."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4515/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698664880985,
            "cdate": 1698664880985,
            "tmdate": 1699636428194,
            "mdate": 1699636428194,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sjAM7TGKOO",
                "forum": "0SOhDO7xI0",
                "replyto": "8SxH0lQZcB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4515/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4515/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' Response (part 1)"
                    },
                    "comment": {
                        "value": "Thank you for your insightful and constructive feedback on our manuscript. We appreciate the time and effort you invested in reviewing our work. We have carefully considered your comments and have made the following revisions to address the concerns raised above:\n\n## Weaknesses\n\n- Though it enjoys theoretical result that a no free lunch situation for selection power when there is exact reconstructability in Appendix B, it seem that there is no theoretical guarantees on the power or explanations for that how the sliced-Wasserstein-based dependency regularization together with a novel perturbation technique introduced to reduce reconstructability can promote selection power.\n\n**ANSWER**: To the best of our knowledge, theoretical analysis on FDR and power are mostly restricted to parametric designs. Without assuming the knowledge of the feature distribution (i.e. the distribution of $X$), how the SWC dependency regularization and the perturbation affect FDR (Type I error) and power (Type II error) is still an open problem, and we leave it for future research. Intuitively, adding independent components leads to less collinearity and thus improves power, at the cost that the swap property will be violated. Empirically, we observed that by enforcing swap property, the generated knockoff copies are often too close to the original design matrix, resulting in high collinearity so that although Type I error might be controlled, Type II error can be large. In such cases, we aim to sacrifice some FDR control for higher power in return. Experiments show that the SWC dependency regularization and the perturbation will result in slightly larger FDR (still relatively small), and much improved power (see Table 7 and Figure 7).\n\n- It is not clear that how to enforce the swap property by the \u201cmulti-swapper\u201d adversarial training procedure.\n\n**ANSWER**:  Thank you for pointing out this issue. We have added the description of the design of swappers and how they assist the knockoff transformer in achieving the swap property in the second paragraph of Appendix C. To repeat the part that involves the optimization: To generate the subset $B$, we consider drawing samples of $b_j$ from the corresponding $j$-th Gumbel-softmax random variable, and the subset $B$ is defined as $\\\\{j\\in [p] ; b\\_j=1\\\\}$. During optimization, we maximize Eq. (4) w.r.t. to the weights $\\omega_i$ of the swapper $S_{\\omega_i}$ such that the sampled indices, with which the swap is applied, lead to a higher $\\text{SWD}$ in the objective (Eq. (4)). Minimizing this objective w.r.t. $\\tilde{X}_\\theta$ requires the knockoff to fight against the adversarial swaps. Therefore, the swap property is enforced. Compared to DDLK, the proposed DeepDRK utilizes multiple independent swappers. \n\nWe suggest checking Appendix C for the full description of how swappers are defined and used during this optimization.\n\n- The motivation behind feature selection is high-dimensional data settings, which in my understanding means that the number of features is larger than the number of examples in the dataset. However, none of the simulated experiments include such scenario.\n\n**ANSWER**: Thank you for pointing out this part. To the best of our knowledge, existing literature `[1-5]`, both theoretical or methodological, concerns the classical linear regression setting when $n>p$ (or $n>2p$ in the knockoff framework as the knockoff introduces additional $p$ variables to the independent variables) . Extensions to the high dimensional setting are left to be unveiled.\n\nWe also want to mention that the knockoff generation part (e.g. the main objective of DeepDRK), as discussed in `[1]`, can handle high dimensional data as there is no restriction on the generative model part in terms of the dimensionality. Hence, it is not a conflict of the statement in a high dimensional setting to the existing literature. \n\n\n`[1]` Emmanuel Candes, Yingying Fan, Lucas Janson, and Jinchi Lv. Panning for gold:\u2018Model-X\u2019 knockoffs for high dimensional controlled variable selection. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 80(3):551\u2013577, 2018.\n\n`[2]` Yaniv Romano, Matteo Sesia, and Emmanuel Candes. Deep knockoffs. Journal of the American Statistical Association, 115(532):1861\u20131872, 2020.\n\n`[3]` James Jordon, Jinsung Yoon, and Mihaela van der Schaar. Knockoffgan: Generating knockoffs for feature selection using generative adversarial networks. In International Conference on Learning Representations, 2018.\n\n`[4]` Shoaib Bin Masud, Matthew Werenski, James M Murphy, and Shuchin Aeron. Multivariate rank via entropic optimal transport: sample efficiency and generative modeling. arXiv preprint arXiv:2111.00043, 2021.\n\n`[5]` Mukund Sudarshan, Wesley Tansey, and Rajesh Ranganath. Deep direct likelihood knockoffs. Advances in neural information processing systems, 33:5036\u20135046, 2020."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4515/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700425022504,
                "cdate": 1700425022504,
                "tmdate": 1700425022504,
                "mdate": 1700425022504,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "tWbuOUTe2U",
            "forum": "0SOhDO7xI0",
            "replyto": "0SOhDO7xI0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4515/Reviewer_Zneh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4515/Reviewer_Zneh"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes DeepDRK, a new model-X knockoff based methods which adopts a two-stage framework to generate knockoff variables. A ViT (called knockoff transformer in the paper) is trained by minimizing a swap loss plus a dependency regularization loss in the training stage, while its output is further perturb through a row-permuted version of the original covariate to reduce the dependency between knockoffs and original covaraites. Experiments on synthetic, semi-synthetic and real data demonstrates the effectiveness of DeepDRK."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Writing is good and it is easy to follow\n2. The idea of leveraging distribution-free methods while avoiding overfitting is well motivated.\n3. Experiment results are impressive."
                },
                "weaknesses": {
                    "value": "1. Ablation study is not very thorough. \n- The loss in DeepDRK contains five terms, SWD, REx, cosine similarity w.r.t. swappers, SWC and the entry-wise decorrelation term. The necessity of introducing these five losses is under-explored in the paper.\n- The necessity of DRP is unclear. There lacks comparison of $\\tilde{X}_{\\theta}$ and $\\tilde{X}_{\\theta}^{DRP}$ in empirical performance.\n2. Experiments need further analysis and explanation. \n- It is clear that DeepDRK performs better than other baseline methods. But the reason has not been analyzed clearly and adding some intermediate results will be helpful. It is unclear how well the knockoffs generated by DeepDRK following the swap property and avoid overfitting compared to baseline methods.\n- The results w.r.t. the Gaussian mixture seems inconsistent with that in the original DDLK paper (DDLK performs the worst in this paper while it performs better than deep knockoffs and knockoffgan in the original paper)."
                },
                "questions": {
                    "value": "To avoid overfitting, why introducing a post-training perturbation instead of modifying training strategy like early stopping or tuning hypermeters?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4515/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698758874751,
            "cdate": 1698758874751,
            "tmdate": 1699636428099,
            "mdate": 1699636428099,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "I7IHdXD5fn",
                "forum": "0SOhDO7xI0",
                "replyto": "tWbuOUTe2U",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4515/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4515/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' Response (part 1)"
                    },
                    "comment": {
                        "value": "Thank you for your insightful and constructive feedback on our manuscript. We have carefully considered your comments and have made the following revisions to address the concerns raised above:\n\n## Weaknesses\n- The loss in DeepDRK contains five terms, SWD, REx, cosine similarity w.r.t. swappers, SWC and the entry-wise decorrelation term. The necessity of introducing these five losses is under-explored in the paper.\n\n**ANSWER**: Thank you for pointing out this issue. We conducted additional experiments for the ablation study section in Appendix L.3, and summarized them in the \u201cAblation Study Results\u201d section of the general response above. Additionally, we want to point out that the SWD term is designed for imposing the swap property, which serves as a backbone of the model. Therefore, it cannot be removed as this would lead to complete deconstruction of the knockoff learning problem in our case. \n\n- The necessity of DRP is unclear. There lacks comparison of $\\tilde{X}\\_{\\theta}$ and $\\tilde{X}\\_{\\theta}^{DRP}$ in empirical performance.\n\n**ANSWER**: Thank you for pointing out this issue. We added one experiment to investigate the effect of $\\alpha$ to the performance of FDR and power, where $\\alpha$ controls the portion of the introduction of $X\\_{\\text{rp}}$, the perturbed $X$. Results can be found in Figure 7 in Appendix J of the revised manuscript. We verified that decreasing the value of $\\alpha$ increases the power, however, at the cost of increasing the FDR. This is because the permutation inside $\\tilde{X}^{\\text{DRP}}\\_\\theta$ reduces reconstructability and destroys swap property simultaneously. And $\\alpha = 1$ refers to the case without DRP (i.e. $\\tilde{X}\\_\\theta$). Based on our result, we suggest choosing the value $\\alpha$ between 0.4 and 0.5. And all results reported in our paper are based on the choice of $\\alpha$ equals 0.5. \n\n- It is clear that DeepDRK performs better than other baseline methods. But the reason has not been analyzed clearly and adding some intermediate results will be helpful. It is unclear how well the knockoffs generated by DeepDRK following the swap property and avoid overfitting compared to baseline methods.\n\n**ANSWER**: Thank you for pointing out this part. We included a section \"Appendix L.2\" in the revised manuscript for measuring the swap property using three different metrics: mean discrepancy distance with the linear kernel and sliced Wasserstein 1 & 2 distances. This reveals the relationship between the sample level swap property and the performance in feature selection. We summarize this part in the \u201cMeasurement on Swap Property'' of the general response above.\n\nBesides the verification on the swap property, we also clarified that we avoid overfitting via early stopping in the revised manuscript. Details can be found in Appendix I, which is a newly added section that includes the training algorithm of the knockoff transformer. Its effectiveness is proved given the results in Table 3 & 4, compared to other benchmarking models.\n\n- The results w.r.t. the Gaussian mixture seems inconsistent with that in the original DDLK paper (DDLK performs the worst in this paper while it performs better than deep knockoffs and knockoffgan in the original paper).\n\n**ANSWER**: Thank you for pointing out this part. However, we want to point out that the Gaussian mixture experiment considered in our paper is different than that in the original DDLK paper as we considered a lower signal strength (i.e. $\\frac{p}{15\\cdot \\sqrt{n}}\\cdot \\text{Rademacher(0.5)}$ or $\\frac{p}{12.5\\cdot \\sqrt{n}}\\cdot \\text{Rademacher(0.5)}$ for the distribution of $\\beta$). The original DDLK uses $\\frac{p}{1 \\cdot \\sqrt{n}}\\cdot \\text{Rademacher(0.5)}$ as the signal strength, which is larger than ours. The reason we chose the lower signal strength cases is because we found all models perform similarly well on the original case. Descriptions and details concerning this experiment setup can be found in the first paragraph of section 4.3. \n\nAdditionally, we also want to mention that in the references `[1, 2]` listed below, the authors independently identified the underperformance of DDLK on some Mixture of Gaussian data and on some real datasets. Therefore, our results in this paper are not inconsistent with the current literature. \n\n`[1]` Derek Hansen, Brian Manzo, and Jeffrey Regier. Normalizing flows for knockoff-free controlled feature selection. Advances in Neural Information Processing Systems, 35:16125\u201316137, 2022.\n\n`[2]` Shoaib Bin Masud, Matthew Werenski, James M Murphy, and Shuchin Aeron. Multivariate\nrank via entropic optimal transport: sample efficiency and generative modeling. arXiv preprint arXiv:2111.00043, 2021."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4515/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700424322466,
                "cdate": 1700424322466,
                "tmdate": 1700424322466,
                "mdate": 1700424322466,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zp8w7WbTvN",
            "forum": "0SOhDO7xI0",
            "replyto": "0SOhDO7xI0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4515/Reviewer_cU77"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4515/Reviewer_cU77"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposed \u201cDeep Dependency Regularized Knockoff (DeepDRK)\u201d, a distribution-free deep learning method that strikes a balance between FDR and power. It leverages transformer architecture and several loss functions for training to generate Knockoff."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. It introduces knockoff Transformer to generate knockoff with different regularizations. And it uses multi-swappers to ensure to swap property of generated knockoff.\n2. Experimental results show the effectiveness of the proposed method compared to other deep model based knockoff methods."
                },
                "weaknesses": {
                    "value": "1. Some arguments of the proposed method is not validated with corresponding experimental results. For example, \u201cmulti-swapper\u201d is used to better achieve swap property. But there are no experiments to justify the how swap property changes when changing from single swapper to multi-swapper. I think authors should also introduce how to empirically measure the swap property. Since the proposed method relies on regularization to enforce the swap property, which is not guaranteed by design.\n2. The proposed method uses many regularization terms. Some of the regularization terms have ablation studies, but others are not. For example, L_swapper and L_ED are not included. The effect of $\\alpha$ in Eq.~(9) is also not investigated. Moreover, there are four hyperparameters require tuning, making the proposed method hard to tune. \n3. The regularization terms largely come from existing papers; I think authors should better justify what is their contribution on top of existing papers."
                },
                "questions": {
                    "value": "Since most dataset is not very large, but the model size is quite large. Did authors try to change the model size to see how it impacts the performance? Maybe the model could be smaller."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4515/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698889819844,
            "cdate": 1698889819844,
            "tmdate": 1699636427994,
            "mdate": 1699636427994,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fZoQtrBOBo",
                "forum": "0SOhDO7xI0",
                "replyto": "zp8w7WbTvN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4515/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4515/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' Response"
                    },
                    "comment": {
                        "value": "Thank you for your insightful and constructive feedback on our manuscript. We appreciate the time and effort you invested in reviewing our work. We have carefully considered your comments and have made the following revisions to address the concerns raised above:\n\n## Weaknesses\n- Some arguments of the proposed method are not validated with corresponding experimental results. For example, \u201cmulti-swapper\u201d is used to better achieve swap properties. But there are no experiments to justify how swap property changes when changing from single swapper to multi-swapper. I think authors should also introduce how to empirically measure the swap property. Since the proposed method relies on regularization to enforce the swap property, which is not guaranteed by design.\n\n**ANSWER**: Thank you for pointing out. We included the ablation study on all the regularization terms in the Appendix L.3 of the revised manuscript. Please also see the \u201cAblation Study Results\u201d section in the general response above for the complete description of the revised ablation study. We also included the section of Appendix L.2. for measuring the swap property. The summary of this experiment can be found in the \u201cMeasurement on Swap Property\u201d section of the general response above. \n\n\n- The proposed method uses many regularization terms. Some of the regularization terms have ablation studies, but others are not. For example, L_swapper and L_ED are not included. The effect of $\\alpha$ in Eq.~(9) is also not investigated. Moreover, there are four hyperparameters that require tuning, making the proposed method hard to tune.\n\n**ANSWER**: Thank you for pointing out this. As mentioned previously, we completed additional ablation studies considering more terms proposed in this paper, which includes the study of both $\\mathcal{L}\\_\\text{swapper}$ and $\\mathcal{L}\\_\\text{ED}$. Results can be found in Appendix L.3 in the revised manuscript. And the summary can be found in the \"Ablation Study Results\" section of the general response above. Based on the ablation study, we verified that $\\mathcal{L}\\_\\text{ED}$ only affects the model\u2019s training stability, rather than its feature selection performance. As a result, we suggest removing this term if no instability issue is observed during training. And the resulting number of tuning hyperparameters during training is identical with KnockoffGAN.\n\nBesides the ablation study, we also investigated the effect of $\\alpha$ on the performance of FDR and power. Results can be found in Figure 7 in Appendix J of the revised manuscript. We verified that decreasing the value of $\\alpha$ increases the power, however, at the cost of increasing the FDR. This is because the permutation inside $\\tilde{X}^{\\text{DRP}}\\_\\theta$ reduces reconstructability and deteriorates swap property simultaneously. We also want to point out that this hyperparameter appears  after training, which can be adjusted without introducing any training burden.\n\n- The regularization terms largely come from existing papers; I think authors should better justify what is their contribution on top of existing papers. \n\n**ANSWER**: Thank you for bringing up this point. We added one paragraph to the last part of section 3.1.1 to summarize how these terms collaboratively contribute to the guarantee of swap property. We want to point out that these terms are first used in knockoff generation for feature generation in this paper, which improves the effectiveness of the generation of knockoffs at sample level. And the utilization of these regularization terms is only one part of the contribution of this paper. We additionally summarized the overall contribution of our paper in the general response above in threefold.\n\n## Questions\n- Since most dataset is not very large, but the model size is quite large. Did authors try to change the model size to see how it impacts the performance? Maybe the model could be smaller.\n\n**ANSWER**: Thank you for bringing up this point about the model size effect. We prepared new experiments and investigated the effect of the model size (in different hidden dimensions and in different numbers of layers) to the performance in FDR and power. Results can be found in Table 3 & 4 in Appendix K. We observed slightly reduced power when considering smaller models in the number of layers and in the number of hidden dimensions. However, in many cases, since we observed similar performance between DeepDRK (e.g. the current setup) and the reduced models, we think the model size is not a strong influence factor to the feature selection performance. Besides, we also applied early stopping during the model training (stated in Appendix I, a newly added section that includes the training algorithm), which is an effective approach (given the results in Table 3 & 4) to prevent potential overfitting issues. This should also mitigate the effect of the model size to the performance of the model."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4515/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700423522993,
                "cdate": 1700423522993,
                "tmdate": 1700423573284,
                "mdate": 1700423573284,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "tEQX45duoz",
            "forum": "0SOhDO7xI0",
            "replyto": "0SOhDO7xI0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4515/Reviewer_e4Vy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4515/Reviewer_e4Vy"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates the problem of feature selection from the perspective of Model-X knockoff owing to its guarantee of false discovery rate (FDR) control.  Realizing the diminished selection power caused by the swap property that knockoffs need, the authors proposed a  Deep Dependency Regularized Knockoff (DeepDRK), which is a distribution-free deep learning method that strikes a balance between FDR and power.  Experiments on synthetic, semi-synthetic, and real-world data verify the effectiveness of the proposed DeepDRK method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper is well-written and easy to follow.\n2. This paper has a clear motivation for diminished selection power caused by the swap property that knockoffs need.\n3. Comprehensive experiments on synthetic, semi-synthetic, and real-world data are conducted, which verify the effectiveness of the proposed DeepDRK method.\n4. To me, such a distribution-free deep learning method that strikes a balance between FDR and power is new and novel."
                },
                "weaknesses": {
                    "value": "I don't see any major weakness in this work."
                },
                "questions": {
                    "value": "I have no more questions. I am not an expert in this field, but I feel this paper is good from the perspective of general machine learning."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethics review is needed."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4515/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699193544773,
            "cdate": 1699193544773,
            "tmdate": 1699636427874,
            "mdate": 1699636427874,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lWJKkVY5p7",
                "forum": "0SOhDO7xI0",
                "replyto": "tEQX45duoz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4515/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4515/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author's Response"
                    },
                    "comment": {
                        "value": "Thanks a lot for your appreciation of this work. We further improved the manuscript based on reviewers\u2019 comments and suggestions. Please see the general response to all reviewers for details."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4515/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700422658855,
                "cdate": 1700422658855,
                "tmdate": 1700422658855,
                "mdate": 1700422658855,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]