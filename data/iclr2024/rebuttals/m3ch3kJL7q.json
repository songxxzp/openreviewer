[
    {
        "title": "Sentence-level Prompts Benefit Composed Image Retrieval"
    },
    {
        "review": {
            "id": "fbTpOPtXWw",
            "forum": "m3ch3kJL7q",
            "replyto": "m3ch3kJL7q",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1658/Reviewer_NuL6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1658/Reviewer_NuL6"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to learn sentence-level prompts for supervised composed image retrieval task, to handle complex changes in CIR task such as modifications involving multiple objects. The sentence-level prompts are generated from query image and relative caption, to yield precise descriptions of specific elements in the query image that are described in the relative caption. Experimental results demonstrate that the proposed method achieves better results on two public CIR benchmarks including FashionIQ and CIRR."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. It is reasonable to generate sentence-level prompts depending on both reference image and relative caption to enrich the expressivity. \n2. Moreover, the experiments are solid since the authors conduct a thorough comparison with previous methods. \n3. The paper is well-written and the idea is easy to follow."
                },
                "weaknesses": {
                    "value": "1.\tin my comprehension, the sentence-level prompts are actually latent vectors output from the MLP layer, so it is hard to make sure the prompts work as expected as demonstrated in Figure 1(c), i.e., decoupling the multiple objects and attributes of query image, and correctly integrating the process of object removal or attribute modification. \n2.\tIt is difficult to understand the pi\u2019 in prompt alignment loss. Whether each reference image has an auxiliary text prompt? As a result, the Figure 2(a) involves two training stages (ITC loss to optimize p and prompt alignment loss to optimize pi\u2019)? Furthermore, during the optimization of pi\u2019, the text encoder is frozen, so the image encoder learns to align with the frozen text encoder; while in optimization of pi, the text encoder is not frozen, so the image encoder learns to align with the updated text encoder. I find it hard to understand how the prompt alignment loss works and it seems very tricky to achieve a good performance."
                },
                "questions": {
                    "value": "The same as weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1658/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1658/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1658/Reviewer_NuL6"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1658/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698718627136,
            "cdate": 1698718627136,
            "tmdate": 1700727780508,
            "mdate": 1700727780508,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fGPjNI5UGE",
                "forum": "m3ch3kJL7q",
                "replyto": "fbTpOPtXWw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1658/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1658/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for his/her constructive comments and provide our point-wise replies as follows.\n\n>**W1:** How does sentence-level prompts work like in Fig.1(c).\n\n**A1:** Yes, our sentence-level prompts are latent word embeddings. Nevertheless, the sentence-level prompts in our method are a sequence of word embeddings, although not in human-readable form, which is continuous and beneficial for learning. While the purpose of Fig. 1 (c) is to visualize our motivation.\n\nTo further prove whether our method can achieve the effect shown in Fig. 1 (c), we trained SPRC on CIRR and the zero-shot tests on the GeneCIS data demonstrate the efficacy of SPRC in capturing and modifying objects and attributes, i.e., the metrics of Change Attribute, focus attribute, and Change Object in the table below, etc.\n\n\n|                | Focus Attribute | Change Attribute | Focus Object     | Change Object    | Avg. |\n|----------------|-----------------|------------------|------------------|------------------|------|\n|                | R@1,R@2,R@3     | R@1,R@2,R@3      | R@1,R@2,R@3      | R@1,R@2,R@3      |  R@1  |\n| Combiner       | 15.1,27.7,39.8  | 12.1, 22.8, 31.8  | 13.5, 25.4, 36.7 | 15.4, 28.0, 39.6 | 14.0 |\n| SPRC          | 19.9,32.6,42.9  | 16.1, 28.0, 37.7  | 21.8, 35.1, 44.8 | 25.0, 38.5, 48.1 | 20.7 |\n*Table 1: GeneCIS*\n\n>**W2:** How the prompt alignment loss works.\n\n**A2:** Sorry for the confusion. We use the same auxiliary text prompt for all reference images, therefore, '$p_i^\\prime$' should be '$p^\\prime$'. We have revised it in our revised manuscript.\n\nIn our method, two types of loss functions, i.e., ITC loss and alignment loss, are utilized to jointly optimize the sentence-level prompt $p_i$.\n\nSpecifically, the alignment loss indicates calculating the L2 distance between $p_i$ and $p'$.\n\nWe note that during the alignment of $p_i$ to $p'$, the gradient of $p'$ does not need to be computed. \n\nDuring the optimization of  $p'$, the text encoder related to $p'$ is not entirely frozen but is aligned through the text encoder trained by p using an EMA approach. At this stage, the image encoder remains frozen. Therefore, during the optimization of $p'$, we only need to learn $p'$, as the text encoder copies weights via EMA, and the image encoder is frozen.\nHence, the optimization of $p'$ does not affect any parameters of the text encoder, image encoder.\n\nIn summary, the primary role of $p'$ is to act as a constraint for the sentence-level prompt $p_i$, ensuring that $p_i$ can align more closely with the word embedding feature space.\n\nTo verify the effect of the alignment loss, we compared the performance of the model on the CIRR and F-IQ validation sets in Tables 6 and 7 of the supplementary material. The results show that the smaller the weight of the alignment loss, the worse the performance of the model. This further verifies the effectiveness of the alignment loss, and we also provide the performance without the alignment loss as follows:\n\n|                    | Recall@k k=1 | Recall@k k=5 | Recall@k k=10 | Recall@k k=50 | Recall sub@k k=1 | Recall sub@k k=2 | Recall sub@k k=3 | Avg.  |\n|--------------------|--------------|--------------|---------------|---------------|------------------|------------------|------------------|-------|\n| w/o alignment loss | 53.11        | 82.92        | 90.39         | 98.17         | 80.10            | 92.53            | 96.70            | 81.51 |\n| w/ alignment loss  | 54.39        | 84.76        | 91.25         | 97.99         | 81.27            | 93.30            | 97.20            | 83.02 |\n*Table2: Alignment loss on CIRR dataset*\n\n|                    | Dress R@10 | Dress R@50 | Shirt R@10 | Shirt R@50 | Toptee R@10 | Toptee R@50 | Average R@10 | Average R@50 | Avg.  |\n|--------------------|------------|------------|------------|------------|-------------|-------------|--------------|--------------|-------|\n| w/o alignment loss | 46.74      | 70.25      | 53.72      | 73.15      | 56.86       | 77.01       | 52.44        | 73.47        | 62.96 |\n| w/ alignment loss  | 49.18      | 72.43      | 55.64      | 73.89      | 59.35       | 78.58       | 54.92        | 74.97        | 64.85 |\n*Table2: Alignment loss on F-IQ dataset*"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1658/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700191371461,
                "cdate": 1700191371461,
                "tmdate": 1700191371461,
                "mdate": 1700191371461,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "s65wtPQALU",
                "forum": "m3ch3kJL7q",
                "replyto": "fGPjNI5UGE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1658/Reviewer_NuL6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1658/Reviewer_NuL6"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the authors' response. In the response of A2, I am still confused about the whole learning process. Specifically, how many steps are used to train p', and how do you switch the processes of training backbones and p'? It will be better to give a detailed training algorithm."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1658/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700646320604,
                "cdate": 1700646320604,
                "tmdate": 1700646320604,
                "mdate": 1700646320604,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6FIL2jCPur",
                "forum": "m3ch3kJL7q",
                "replyto": "fbTpOPtXWw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1658/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1658/Authors"
                ],
                "content": {
                    "title": {
                        "value": "We thank the reviewer again for the valuable feedback and happy to address any remaining concerns"
                    },
                    "comment": {
                        "value": "As the discussion period draws to a close soon, we extend our sincere gratitude to the reviewer for his/her valuable time and insightful feedback. We value the constructive feedback and hope that our responses have appropriately addressed all the concerns. \n\nWe really appreciate the valuable time to respond to our feedbacks based on the reviewer's comments. Further, we are happy to address any remaining concerns."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1658/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700705444990,
                "cdate": 1700705444990,
                "tmdate": 1700705444990,
                "mdate": 1700705444990,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jzNnfvv5RF",
                "forum": "m3ch3kJL7q",
                "replyto": "fElZfYL5OB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1658/Reviewer_NuL6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1658/Reviewer_NuL6"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the authors' further response. They have addressed most of my concerns. Therefore, I increase the final rating."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1658/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700727796541,
                "cdate": 1700727796541,
                "tmdate": 1700727796541,
                "mdate": 1700727796541,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "GJMEeeEzNb",
            "forum": "m3ch3kJL7q",
            "replyto": "m3ch3kJL7q",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1658/Reviewer_a9pf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1658/Reviewer_a9pf"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a sentence prompt generation based approach to composed image retrieval or CIR. They train their system to learn the generation of such sentence prompts for known target images (in the training set) through two innovative loss functions. Then at inference time their system essentially generates a sentence prompt that along with the original user query enables the user to retrieve the target image with greater accuracy since the sentence prompt has a much more fine-grained description of the target image."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Comprehensive literature survey and good motivation of the problem.\n2. Sound approach based on innovative loss functions.\n3. Good results that exceed the state of the art."
                },
                "weaknesses": {
                    "value": "1. The overall innovation could be seen as modest. However, I am open to being convinced otherwise."
                },
                "questions": {
                    "value": "1. How does your approach do across domains? Is it able to adapt to domain shifts in other words?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1658/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698784151770,
            "cdate": 1698784151770,
            "tmdate": 1699636093811,
            "mdate": 1699636093811,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xAQsUxsZt1",
                "forum": "m3ch3kJL7q",
                "replyto": "GJMEeeEzNb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1658/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1658/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for his/her constructive comments and provide our point-wise replies as follows.\n\n>**W1:**  Innovation moderate; open to persuasion.\n\n**A1:** The key innovations of our research include:\n\n(1) Our work enhances the expression capability and accuracy of CIR by learning sentence-level prompts from reference images and relative captions. This approach is contrasted with the typical late fusion strategies or pseudo-word token generation in existing models, providing an effective solution for complex scenarios such as object removal and attribute modification in CIR.\n\n(2) The effectiveness of our method is not limited to a specific retrieval model; it exhibits consistent superior performance across different models such as CLIP, BLIP, and BLIP-2.\n\n>**W2:** Cross-domain adaptability of SPRC.\n\n**A2:** To verify the domain generalization ability of SPRC, we conducted zero-shot tests on CIRCO, GeneCIS, ImageNet (consists of ImageNet [1] and ImageNet-R [2]). ImageNet consists of samples from several different domains (cartoons, origami, toys, and sculptures), refer to the table below for details.\n\nThe results in the tables below show that the zero-shot performance of our method (pre-trained on CIRR) on the CIRCO is significantly better than the previous zero-shot method SEARLE [3].\n\nOur method also surpasses the previous supervised method Combiner (Baldrati et al., 2022a) on GeneCIS dataset. This clearly shows that our model can adapt to domain shifts.\n\nSimilarly, using CIRR training and conducting zero-shot tests on ImageNet also achieved noticeable improvements against Combiner. \n\nAlthough there is a clear domain difference between Fashion-IQ and other data, our method still obtained considerable performance in zero-shot tests trained on the Fashion-IQ dataset, see Table 1, 3. This result further shows that our model has the ability to handle domain shifts.\n\n\n\n| Model                         | mAP@k=5 | mAP@k=10 | mAP@k=25 | mAP@k=50 |\n|-------------------------------|---------|----------|----------|----------|\n| SEARLE           | 11.68   | 12.73    | 14.33    | 15.12    |\n| SPRC(CIRR)                | 22.86   | 23.63    | 25.56    | 26.55    |\n| SPRC(F-IQ)                | 14.21   | 15.18    | 16.86    | 17.74    |\n*Table 1: CIRCO*\n\n|                | Focus Attribute | Change Attribute | Focus Object     | Change Object    | Avg. |\n|----------------|-----------------|------------------|------------------|------------------|------|\n|                | R@1,R@2,R@3     | R@1,R@2,R@3      | R@1,R@2,R@3      | R@1,R@2,R@3      |  R@1  |\n| Combiner       | 15.1,27.7,39.8  | 12.1, 22.8, 31.8  | 13.5, 25.4, 36.7 | 15.4, 28.0, 39.6 | 14.0 |\n| SPRC.          | 19.9,32.6,42.9  | 16.1, 28.0, 37.7  | 21.8, 35.1, 44.8 | 25.0, 38.5, 48.1 | 20.7 |\n*Table 2: GeneCIS*\n\n|                | Cartoon R@10 | Cartoon R@50 | Toy R@10 | Toy R@50 | Origami R@10 | Origami R@50 | Sculpture R@10 | Sculpture R@50 |\n|----------------|--------------|--------------|---------|---------|--------------|--------------|----------------|----------------|\n| Combiner    | 6.1          | 14.8         | 10.5    | 21.3    | 7.0          | 17.7         | 8.5            | 20.4           |\n| Pic2Word    | 8.0          | 21.9         | 13.5    | 25.6    | 8.7          | 21.6         | 10.0           | 23.8           |\n| SPRC(CIRR)           | 9.8          | 22.9         | 10.6    | 24.5    | 15.4         | 28.0         | 9.8            | 22.6           |\n| SPRC(F-IQ)     | 8.0          | 20.1         | 11.4    | 25.5    | 9.3         | 21.5         | 6.1            | 15.2           |\n*Table 3: ImageNet*\n\n\n\n[1] Deng, Jia, et al. \"Imagenet: A large-scale hierarchical image database.\" 2009 IEEE conference on computer vision and pattern recognition. Ieee, 2009.\n\n[2] Hendrycks, Dan, et al. \"The many faces of robustness: A critical analysis of out-of-distribution generalization.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021.\n\n[3] Baldrati, Alberto, et al. \"Zero-Shot Composed Image Retrieval with Textual Inversion.\" arXiv preprint arXiv:2303.15247 (2023)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1658/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700191367256,
                "cdate": 1700191367256,
                "tmdate": 1700191367256,
                "mdate": 1700191367256,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ok6pnASwt0",
                "forum": "m3ch3kJL7q",
                "replyto": "GJMEeeEzNb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1658/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1658/Authors"
                ],
                "content": {
                    "title": {
                        "value": "We thank the reviewer again for the valuable feedback and happy to address any remaining concerns"
                    },
                    "comment": {
                        "value": "As the discussion period draws to a close soon, we extend our sincere gratitude to the reviewer for his/her valuable time and insightful feedback. We value the constructive feedback and hope that our responses have appropriately addressed all the concerns. \n\nWe really appreciate the valuable time to respond to our feedbacks based on the reviewer's comments. Further, we are happy to address any remaining concerns."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1658/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700705396941,
                "cdate": 1700705396941,
                "tmdate": 1700705396941,
                "mdate": 1700705396941,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CmXQpIuKbA",
            "forum": "m3ch3kJL7q",
            "replyto": "m3ch3kJL7q",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1658/Reviewer_fBxs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1658/Reviewer_fBxs"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes an novel approach for the Composed Image Retrieval (CIR) problem. The idea is to generate a sentence (\"text prompt\") to describe both the input image and the relative caption and it for searching the image database using standard text-to-image retrieval methods. The method is evaluated against 10+ baseline methods against two datasets ( CIRR, Fashion - IQ). The results provide significant top-k recall gains over all the baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The proposed method is technically sound and simple. \n- The paper is easy to follow, experiments are thorough along with ablations (such as prompt length, weight in the loss function etc). \n- Provides SOTA results against 10+ baselines on two public datasets. \n- To be open-sourced."
                },
                "weaknesses": {
                    "value": "- Limited novelty: A recent paper (https://arxiv.org/pdf/2310.09291.pdf) with quite similar methodology and motivations except for a nuanced difference: training-free vs learned sentence level prompts. There is a need for contextualizing these methods together, ideally under the same evaluation framework so that we understand the value of learned sentence level prompts proposed by this paper. \n- Experimental setup: CIRR dataset experiments uses a random split of the training dataset as the test set for evaluations. The results should be reported in the official (hidden) test set instead. Otherwise reported numbers are not comparable to other papers published in this area."
                },
                "questions": {
                    "value": "Q1: Could the database image-caption pairs be enriched with the proposed sentence generation method and used for refining the search?\nQ2: How easy to extend the proposed method for addressing other problem domains or modalities?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1658/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1658/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1658/Reviewer_fBxs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1658/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698908157111,
            "cdate": 1698908157111,
            "tmdate": 1700699695622,
            "mdate": 1700699695622,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6lqkram5oj",
                "forum": "m3ch3kJL7q",
                "replyto": "CmXQpIuKbA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1658/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1658/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for his/her constructive comments and provide our point-wise replies as follows.\n\n>**W1:** Difference with https://arxiv.org/pdf/2310.09291.pdf.\n\n**A1:** Thank you for sharing the CIReVL paper (Karthik et al, (2023). However, we would like to note that CIReVL is a concurrent work with our SPRC, and the arXiv upload date of this paper, i.e., October 13th, is after the submission deadline for ICLR 2024. As suggested, we performed our SPRC on a series of zero-shot tests on the CIRCO test set for comparison with CIReVL (we also discussed the CIReVL in our revised manuscript). The results are as follows:\n\n| Model                         | mAP@k=5 | mAP@k=10 | mAP@k=25 | mAP@k=50 |\n|-------------------------------|---------|----------|----------|----------|\n| CIReVL ViT-L/14         | 18.57   | 19.01    | 20.89    | 21.80    |\n| SPRC ViT-L/14                 | 22.86   | 23.63    | 25.56    | 26.55    |\n*Table 1: CIRCO*\n\n|                | Focus Attribute | Change Attribute | Focus Object     | Change Object    | Avg. |\n|----------------|-----------------|------------------|------------------|------------------|------|\n|                | R@1,R@2,R@3     | R@1,R@2,R@3      | R@1,R@2,R@3      | R@1,R@2,R@3      | R@1  |\n| CIReVL         | 17.9,29.4,40.4  | 14.8, 25.8, 35.8  | 14.6, 24.3, 33.3 | 16.1, 27.8, 37.6 | 15.9 |\n| SPRC          | 19.9,32.6,42.9  | 16.1, 28.0, 37.7  | 21.8, 35.1, 44.8 | 25.0, 38.5, 48.1 | 20.7 |\n*Table 2: GeneCIS*\n\n|                | Cartoon R@10 | Cartoon R@50 | Toy R@10 | Toy R@50 | Origami R@10 | Origami R@50 | Sculpture R@10 | Sculpture R@50 |\n|----------------|--------------|--------------|---------|---------|--------------|--------------|----------------|----------------|\n| Combiner    | 6.1          | 14.8         | 10.5    | 21.3    | 7.0          | 17.7         | 8.5            | 20.4           |\n| Pic2Word    | 8.0          | 21.9         | 13.5    | 25.6    | 8.7          | 21.6         | 10.0           | 23.8           |\n| CIReVL      | 19.2         | 42.8         | 30.2    | 41.3    | 22.2         | 43.1         | 23.4           | 45.0           |\n| SPRC           | 9.8          | 22.9         | 10.6    | 24.5    | 15.4         | 28.0         | 9.8            | 22.6           |\n*Table 3: ImageNet*\n\nThe results in Table 1 show that, when pre-trained on the CIRR dataset and tested for zero-shot performance on the CIRCO dataset, our SPRC significantly outperforms CIReVL across all metrics. The domain differences between the two datasets, CIRR and CIRCO, indicate the generalization ability of our proposed model.\n\nFurthermore, from Table 2, zero-shot test results on the GeneCIS test data show that our SPRC model performs better compared to CLReVL.\n\nAdditionally, while CLReVL outperforms our SPRC model when tested using ImageNet data, i.e., Table 3, SPRC outperforms the baseline model Combiner (Baldrati et al., 2022a).\n\n>**W2:** Results of CIRR are val. or test set?\n\n**A2:** Thanks for the comment. We would like to clarify that all our experiments follow the settings of previous CIR works. For example, the results recorded in Table 2 from our main manuscript are all from the **test set** of CIRR, while the validation set is only used for ablation studies. This is consistent with previous CIR works (Liu et al., 2023b, Liu et al., 2023a). We also emphasized this point in the revised manuscript.\n\n>**Q1:** Potential to enrich the caption.\n\n**A3:**  Actually, the sentence-level prompts in SPRC are implicit word embeddings rather than explicit words; hence, SPRC cannot explicitly generate real text descriptions to enrich the dataset. However, SPRC is orthogonal to CIReVL, which can enrich data. For example, the target captions generated from CIReVL may be utilized as supervision to train SPRC's prompts to obtain better generalization abilities.\n\n>**Q2** Extensibility of SPRC to Other domains/modalities.\n\n**A4:**  As shown in Table 3, when we perform zero-shot inference on ImageNet data, our model outperforms the previous SOTA zero-shot methods, i.e., Pic2Word (Saito et al., 2023) on Cartoon and Origami domains and obtains comparable performance on other domains. This showcases our model's ability to address different domains. We also observed that CIReVL outperforms our model, indicating that the explicit text prompts generated by the CIReVL model are beneficial for processing data across different domains. This potentially complements the sentence-level prompts in our SPRC, which can enable SPRC to achieve better performance across various domains."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1658/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700191349180,
                "cdate": 1700191349180,
                "tmdate": 1700191349180,
                "mdate": 1700191349180,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WYAkU2fOK9",
                "forum": "m3ch3kJL7q",
                "replyto": "CmXQpIuKbA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1658/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1658/Authors"
                ],
                "content": {
                    "title": {
                        "value": "We thank the reviewer again for the valuable feedback and happy to address any remaining concerns"
                    },
                    "comment": {
                        "value": "As the discussion period draws to a close soon, we extend our sincere gratitude to the reviewer for his/her valuable time and insightful feedback. We value the constructive feedback and hope that our responses have appropriately addressed all the concerns. \n\nWe really appreciate the valuable time to respond to our feedbacks based on the reviewer's comments. Further, we are happy to address any remaining concerns."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1658/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700705317080,
                "cdate": 1700705317080,
                "tmdate": 1700705348650,
                "mdate": 1700705348650,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]