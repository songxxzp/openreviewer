[
    {
        "title": "Can Class-Priors Help Single-Positive Multi-Label Learning?"
    },
    {
        "review": {
            "id": "FXpjqr2kWK",
            "forum": "jdynlBj3b0",
            "replyto": "jdynlBj3b0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5153/Reviewer_cikX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5153/Reviewer_cikX"
            ],
            "content": {
                "summary": {
                    "value": "This study introduces an approach for single-positive multi-label learning (SPMLL). The authors present a class-priors estimation technique that aims to align the estimated class-priors with the actual class-priors as training progresses. In addition, an unbiased risk assessment tool is introduced, which is based on the estimated class-priors, and a generalization error bound is provided. Testing on ten MLL benchmark datasets has been conducted to evaluate the performance of this method in comparison to other SPMLL techniques."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper provides theoretical guarantees regarding the convergence of the estimated class priors to ground-truth class priors. Additionally, it claims that the risk minimizer corresponding to the proposed risk estimator will approximately converge to the optimal risk minimizer on fully supervised data. These theoretical insights enhance the credibility of the proposed framework.\n\nWithin the group of other SPMLL techniques experimental results are quite favorable both in terms of average precision as well as predicting the class prior. Attention maps also look quite promising."
                },
                "weaknesses": {
                    "value": "The paper seeks to develop a method for SPMLL, and it's evident that similar efforts have been made in other studies, focusing on the \"single-positive\" label approach. Yet, the rationale for opting for the \"single-positive\" label remains somewhat vague. In real-life situations, it's common to encounter missing labels, but the count of observed labels for each sample can differ, not always being restricted to one. For this study to truly make a difference, it should be benchmarked not just against other SPMLL strategies but also against traditional multi-label learning models that consider multiple positive labels for each sample during training (not just testing). The assumption that each sample contains precisely one positive label for training seems out of sync with practical scenarios. \n\nThe paper should discuss the potential limitations or challenges in generalizing CRISP to different domains or types of data. Real-world scenarios can vary widely, and the effectiveness of the proposed framework in diverse contexts should be explored."
                },
                "questions": {
                    "value": "This is not a question but please explain in the paper how this sentence is related to Section 3.1 \"Note that a method is risk-consistent if the method possesses a classification risk estimator that is equivalent to R(f) given the same classifier (Mohri et al., 2012).\"\n\nAfter rebuttal:\n---------------------------------\nThank you for your response to my initial comments and for conducting the additional comparisons that I suggested. It is encouraging to see that your approach demonstrates superior performance when compared to the additional MLML methods. In light of these new findings, I am pleased to adjust my evaluation of your paper. I am increasing my score by one point."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5153/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5153/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5153/Reviewer_cikX"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5153/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698419418920,
            "cdate": 1698419418920,
            "tmdate": 1700450524607,
            "mdate": 1700450524607,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "82DEtkzPO9",
                "forum": "jdynlBj3b0",
                "replyto": "FXpjqr2kWK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5153/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5153/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate your detailed review and constructive feedback on our manuscript. Your comments have been crucial in improving our research. Regarding your questions, I would like to provide the following explanations:\n\nFor Weaknesses:\n\n1. > The paper seeks to develop a method for SPMLL, and it's evident that similar efforts have been made in other studies, focusing on the \"single-positive\" label approach. Yet, the rationale for opting for the \"single-positive\" label remains somewhat vague. In real-life situations, it's common to encounter missing labels, but the count of observed labels for each sample can differ, not always being restricted to one. For this study to truly make a difference, it should be benchmarked not just against other SPMLL strategies but also against traditional multi-label learning models that consider multiple positive labels for each sample during training (not just testing). The assumption that each sample contains precisely one positive label for training seems out of sync with practical scenarios.\n\n   Thank you for your insightful comments. I would like to clarify the significance and challenges associated with SPMLL problem.\n\n   Firstly, the SPMLL problem has been extensively studied due to its practical relevance in scenarios with extensive instances and label spaces. In reality, large number of instances inherently possess multiple labels. However, accurately annotating every label for each instance is a challenging and labor-intensive task. To enhance efficiency, annotators often annotate only a single positive label for each instance, considerably reducing the burden of annotation. A prime example of this is the ImageNet dataset, which, while being a single-label dataset, contains samples that inherently possess multiple labels [1].\n\n   Secondly, due to the constraint of having only a single positive label per sample, SPMLL presents a more challenging scenario compared to the Multi-Label Learning with Missing Labels (MLML) problem where each example may have multiple positive labels, and the remaining labels are missing. Our supplementary experiments demonstrate that directly applying the state-of-the-art MLML methods [2] to SPMLL results in suboptimal performance. This underscores the necessity for developing specialized approaches tailored for SPMLL. Furthermore, our proposed method is also seamlessly adaptable to the MLML problem, showcasing its versatility.\n\n   Predictive performance of each comparing method on four MLIC datasets in terms of *mAP* (mean \u00b1 std). The best performance is highlighted in bold (the larger the better).\n\n   |       |       VOC        |       COCO       |     NUSWIDE      |       CUB        |\n   | :---: | :--------------: | :--------------: | :--------------: | :--------------: |\n   | LL-R  |   87.784\u00b10.063   |   70.078\u00b10.008   |   48.048\u00b10.074   |   18.966\u00b10.022   |\n   | LL-Cp |   87.466\u00b10.031   |   70.460\u00b10.032   |   48.000\u00b10.077   |   19.310\u00b10.164   |\n   | LL-Ct |   87.054\u00b10.214   |   70.384\u00b10.058   |   47.930\u00b10.010   |   19.012\u00b10.097   |\n   | Crisp | **89.820\u00b10.191** | **74.640\u00b10.219** | **49.996\u00b10.316** | **21.650\u00b10.178** |\n\n   Predictive performance of each comparing method on MLL datasets in terms of *Average Precision* (mean \u00b1 std). The best performance is highlighted in bold (the larger the better).\n\n   |       |      Image      |      Scene      |      Yeast      |     Corel5k     |    Mirflickr    |    Delicious    |\n   | :---: | :-------------: | :-------------: | :-------------: | :-------------: | :-------------: | :-------------: |\n   | LL-R  |   0.605\u00b10.058   |   0.714\u00b10.035   |   0.658\u00b10.006   |   0.268\u00b10.002   |   0.625\u00b10.001   |   0.296\u00b10.004   |\n   | LL-Cp |   0.595\u00b10.031   |   0.735\u00b10.028   |   0.700\u00b10.000   |   0.259\u00b10.004   |   0.621\u00b10.007   |   0.251\u00b10.007   |\n   | LL-Ct |   0.600\u00b10.012   |   0.669\u00b10.052   |   0.629\u00b10.007   |   0.258\u00b10.004   |   0.619\u00b10.004   |   0.253\u00b10.004   |\n   | Crisp | **0.749\u00b10.037** | **0.795\u00b10.031** | **0.758\u00b10.002** | **0.304\u00b10.003** | **0.628\u00b10.003** | **0.319\u00b10.001** |"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5153/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700060844027,
                "cdate": 1700060844027,
                "tmdate": 1700060995684,
                "mdate": 1700060995684,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JH0stiCPnW",
                "forum": "jdynlBj3b0",
                "replyto": "FXpjqr2kWK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5153/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5153/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Predictive performance of each comparing method on MLL datasets in terms of *Ranking Loss* (mean \u00b1 std). The best performance is highlighted in bold (the smaller the better).\n\n   |       |      Image      |      Scene      |      Yeast      |     Corel5k     |    Mirflickr    | Delicious       |\n   | :---: | :-------------: | :-------------: | :-------------: | :-------------: | :-------------: | --------------- |\n   | LL-R  |   0.346\u00b10.072   |   0.155\u00b10.021   |   0.227\u00b10.001   |   0.114\u00b10.001   |   0.123\u00b10.003   | 0.129\u00b10.002     |\n   | LL-Cp |   0.329\u00b10.041   |   0.148\u00b10.017   |   0.215\u00b10.000   |   0.114\u00b10.003   |   0.124\u00b10.003   | 0.160\u00b10.001     |\n   | LL-Ct |   0.327\u00b10.019   |   0.180\u00b10.038   |   0.238\u00b10.001   |   0.115\u00b10.001   |   0.124\u00b10.002   | 0.160\u00b10.000     |\n   | Crisp | **0.164\u00b10.027** | **0.112\u00b10.021** | **0.164\u00b10.001** | **0.113\u00b10.001** | **0.118\u00b10.001** | **0.122\u00b10.000** |\n\n   Predictive performance of each comparing method on MLL datasets in terms of *One Error* (mean \u00b1 std). The best performance is highlighted in bold (the smaller the better).\n\n   |       |      Image      |      Scene      |      Yeast      |     Corel5k     |    Mirflickr    |    Delicious    |\n   | :---: | :-------------: | :-------------: | :-------------: | :-------------: | :-------------: | :-------------: |\n   | LL-R  |   0.597\u00b10.084   |   0.490\u00b10.054   |   0.436\u00b10.087   |   0.715\u00b10.006   |   0.342\u00b10.016   |   0.543\u00b10.041   |\n   | LL-Cp |   0.629\u00b10.043   |   0.450\u00b10.051   |   0.240\u00b10.000   |   0.731\u00b10.016   |   0.357\u00b10.016   |   0.490\u00b10.028   |\n   | LL-Ct |   0.616\u00b10.019   |   0.574\u00b10.074   |   0.552\u00b10.097   |   0.726\u00b10.022   |   0.375\u00b10.012   |   0.475\u00b10.019   |\n   | Crisp | **0.325\u00b10.026** | **0.311\u00b10.047** | **0.227\u00b10.004** | **0.646\u00b10.006** | **0.295\u00b10.009** | **0.402\u00b10.003** |\n\n   In summary, while closely related to MLML, SPMLL poses new challenges with less supervision. Our experiments verify that when compared to MLML techniques, our approach designed specifically for SPMLL performs the best.\n\n   [1] Shankar, V., Roelofs, R., Mania, H., Fang, A., Recht, B., & Schmidt, L. (2020, November). Evaluating machine accuracy on imagenet. In *International Conference on Machine Learning* (pp. 8634-8644). PMLR.\n\n   [2] Kim, Y., Kim, J. M., Akata, Z., & Lee, J. (2022). Large loss matters in weakly supervised multi-label classification. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition* (pp. 14156-14165).\n\n2. > The paper should discuss the potential limitations or challenges in generalizing CRISP to different domains or types of data. Real-world scenarios can vary widely, and the effectiveness of the proposed framework in diverse contexts should be explored.\n\n   In our study, we have indeed gone beyond the four large-scale image datasets commonly used in previous SPMLL research. We have conducted extensive experiments on multiple multi-label learning (MLL) datasets, encompassing a diverse range of domains including images, biology, and text. Specifically, the datasets used in our experiments, include Image, Scene, Yeast, Corel5k, Mirflickr, and Delicious, each with varying numbers of examples, features, and classes, catering to different domains.\n\n   Our experimental results have consistently shown that CRISP outperforms the existing methods across these varied datasets. This not only demonstrates the effectiveness of our method but also substantiates its robust generalization capability across different domains and types of data. For instance, CRISP was able to effectively handle the high dimensionality of the Delicious text dataset with 500 features and 983 classes as well as the biological dataset Yeast with its distinct feature space and class structure.\n\n   We believe these results offer strong evidence of the versatility and adaptability of the CRISP framework. In the revised manuscript, we will includ a more detailed discussion on the potential limitations and challenges when applying CRISP to different domains, along with a comprehensive analysis of the experimental results across these varied datasets."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5153/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700060925069,
                "cdate": 1700060925069,
                "tmdate": 1700123847773,
                "mdate": 1700123847773,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UxvcGsIllF",
                "forum": "jdynlBj3b0",
                "replyto": "FXpjqr2kWK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5153/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5153/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "For Questions:\n\n1. > This is not a question but please explain in the paper how this sentence is related to Section 3.1 \"Note that a method is risk-consistent if the method possesses a classification risk estimator that is equivalent to R(f) given the same classifier (Mohri et al., 2012).\"\n\n   The statement regarding risk consistency does not directly relate to the symbol definitions for the multi-label learning (MLL) problem presented in Section 3.1, but it sets the stage for the upcoming explanation of how our method adheres to this desirable property. Risk consistency is crucial in weakly supervised problems, where the absence of complete ground-truth labels precludes direct optimization of the expected risk $ \\mathcal{R}(h) $ as stated in Equation (1).\n\n   Risk consistency addresses this by transforming the expected risk $\\mathcal R(h)$ into an optimizable risk $\\mathcal R_{consistent}(h)$, which can be optimized using the available weak supervision. For any classifier $h$, it ensures that $\\mathcal R(h)=\\mathcal R_{consistent}(h)$. Our proposed CRISP method is designed to adhere to this crucial property. By introducing this concept early in Section 3.1, we aim to clarify the theoretical underpinnings that make CRISP applicable in the weakly supervised settings of SPMLL.\n\n   We will ensure that the revised manuscript clarifies the relevance of risk consistency to the definitions presented in Section 3.1 and articulates its importance in the broader context of weakly supervised learning.\n\n\nWe have incorporated your suggestions into our revisions and hope that they meet your approval. We are open to any further questions or requests for additional information."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5153/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700060961235,
                "cdate": 1700060961235,
                "tmdate": 1700060961235,
                "mdate": 1700060961235,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "K6wWN3Buu9",
                "forum": "jdynlBj3b0",
                "replyto": "UxvcGsIllF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5153/Reviewer_cikX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5153/Reviewer_cikX"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response to my initial comments and for conducting the additional comparisons that I suggested. It is encouraging to see that your approach demonstrates superior performance when compared to the additional MLML methods. In light of these new findings, I am pleased to adjust my evaluation of your paper. I am increasing my score by one point."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5153/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700450485406,
                "cdate": 1700450485406,
                "tmdate": 1700450485406,
                "mdate": 1700450485406,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "6UTzeLC68x",
            "forum": "jdynlBj3b0",
            "replyto": "jdynlBj3b0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5153/Reviewer_6TNH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5153/Reviewer_6TNH"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on Single-Positive multi-label learning and proposes a framework named CRISP. CRISP estimates the class-priors, and an unbiased risk estimator is derived based on the estimated class-priors. This paper tries to guarantee the estimated class-priors converging to the ground-truth class-priors. Finally, this paper tries to show the effectiveness of CRISP by extensive experiments."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "a. Extensive experiments. I appreciate that this paper provides extensive experiments to show the effectiveness of the proposed method.\n\nb. Nice originality. I am not sure whether this work is the first to focus on the class prior in SPMLL, but it is an interesting track."
                },
                "weaknesses": {
                    "value": "a. The writing of this paper needs to be improved. Specifically, more analysis and descriptions of Theorem 4.2 are necessary.\n\nb. I am concerned about the time cost of the proposed method."
                },
                "questions": {
                    "value": "My main concerns are the following questions:\n\na. It is mentioned that \"This unrealistic assumption will introduce severe biases into the pseudo-labels, further impacting the training of the model supervised by the inaccurate pseudo-labels\". What are the biases? It is necessary to provide more discussions to enrich your motivations.\n\nb. The key of the proposed methods is the threshold. How do you get the optimal threshold in practice, i.e. how do you implement eq.2?\n\nc. What is the time cost of the proposed method? Please discuss more about the time cost of the optimal threshold  and the entire method in theory and experiments.\n\nb. Theorem 4.2 tries to present the convergence of the empirical risk minimizer, but it seems that the empirical risk minimizer does not converge to the true risk minimizer. Please provide more analysis of Theorem 4.2 and more discussions about every component in the upper bound."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5153/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698692633726,
            "cdate": 1698692633726,
            "tmdate": 1699636509648,
            "mdate": 1699636509648,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "J1JeIvx4M0",
                "forum": "jdynlBj3b0",
                "replyto": "6UTzeLC68x",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5153/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5153/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We are deeply grateful for your thoughtful review and insightful suggestions. Your feedback has greatly helped us to improve the quality of our manuscript. Regarding your questions, I would like to provide the following explanations:\n\nFor Weaknesses:\n\n1. > The writing of this paper needs to be improved. Specifically, more analysis and descriptions of Theorem 4.2 are necessary.\n\n   Theorem 4.2 provides an upper bound on the difference between the classifier obtained by the empirical risk minimizer $ \\hat{f}_{sp} $ and the classifier corresponding to the true risk minimizer $ f^* $. This bound consists of four terms, which can be categorized into two groups: the Rademacher complexity terms and the terms of order $ O(1/\\sqrt{n}) $.\n\n   1. **Terms of Order $ O(1/\\sqrt{n}) $**: As the sample size $ n $ grows, these terms reduce, indicating that $ \\hat{f}_{sp} $ converges to the performance of $ f^* $ within a margin of error that becomes progressively smaller with more data.\n   2. **Rademacher Complexity Terms**: The Rademacher Complexity terms in the theorem quantify the complexity of the hypothesis space. These complexity terms denote an intrinsic error bound that persists regardless of sample size, reflecting the capacity of the function class we are choosing from. This intrinsic error is a fundamental aspect of the learning problem and remains even in a fully supervised scenario [1].\n\nTogether, these components suggest that as we gather more data, the classifier $ \\hat f_{sp} $ becomes increasingly accurate, drawing nearer to the performance of the classifier $ f^* $ within an acceptable error tolerance. The empirical risk for $ \\hat{f}_{sp} $ is thus expected to converge to the true risk $ R(f^*) $.\n\n   [1] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of Machine Learning. Adaptive computation and machine learning. MIT Press, 2012. ISBN 978-0-262-01825-8.\n\n2. > I am concerned about the time cost of the proposed method.\n\n   Thank you for your comment regarding the computational efficiency of our proposed method. We understand the importance of time cost in real-world applications and have addressed this concern in two ways in our additional experiments.\n\n   Firstly, we have recorded the time spent on estimating class priors within each epoch and found it to be a relatively minor portion of the total training time. As illustrated in the below table, the time for class-priors estimation is indeed quite short compared to the overall training time for an epoch, ensuring that our method remains practical for use in larger datasets.\n\n   | Dataset | Time of Estimation of the Class-Priors (min) | Whole Training Time of One Epoch (min) |\n   | :-----: | :------------------------------------------: | :------------------------------------: |\n   |   VOC   |                     0.24                     |                  2.19                  |\n   |  COCO   |                     3.47                     |                 27.29                  |\n   |   NUS   |                     6.4                      |                 49.09                  |\n   |   CUB   |                     0.45                     |                  3.89                  |\n\n   Secondly, to further enhance the speed of our algorithm, we have experimented with updating the class priors every few epochs instead of every single one. The variant of our method, denoted as CRISP-3EP, updates the priors every three epochs and our experiments show that this results in a negligible loss in performance, as evidenced by the close metrics between CRISP and CRISP-3EP.\n\n   |           |     VOC      |     COCO     |     NUS      |     CUB      |\n   | :-------: | :----------: | :----------: | :----------: | :----------: |\n   |   CRISP   | 89.820\u00b10.191 | 74.640\u00b10.219 | 49.996\u00b10.316 | 21.650\u00b10.178 |\n   | CRISP-3EP | 89.077\u00b10.251 | 73.930\u00b10.399 | 49.463\u00b10.216 | 19.450\u00b10.389 |\n\n   We will include this discussion in the limitations section in the revised manuscript. We believe these insights could be beneficial to researchers and practitioners using our method in the future."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5153/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700060678945,
                "cdate": 1700060678945,
                "tmdate": 1700060678945,
                "mdate": 1700060678945,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XrXxB947aV",
                "forum": "jdynlBj3b0",
                "replyto": "6UTzeLC68x",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5153/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5153/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "For Questions:\n\n1. > It is mentioned that \"This unrealistic assumption will introduce severe biases into the pseudo-labels, further impacting the training of the model supervised by the inaccurate pseudo-labels\". What are the biases? It is necessary to provide more discussions to enrich your motivations.\n\n   The biases referred to in our paper arise from the discrepancy between the assumed uniform distribution of class-priors and the actual distribution in real-world data. Typically, the class distribution in real-world scenarios is imbalanced, with some classes being more prevalent than others. When pseudo-labels are generated under the assumption of equal class-priors, classes with a naturally lower occurrence rate are overrepresented, while those with a higher occurrence rate are underrepresented.\n\n   The biases resulting from the unrealistic assumption of identical class-priors for pseudo-label generation can indeed lead to the production of incorrect pseudo-labels. This issue is pivotal, as the subsequent model training is supervised by these inaccurate pseudo-labels, which can compound the initial bias into a significant performance degradation.\n\n2. > The key of the proposed methods is the threshold. How do you get the optimal threshold in practice, i.e. how do you implement eq.2?\n\n   In practice, to determine the optimal threshold, we conduct an exhaustive search across the set of outputs generated by the function $ f^j $ for each class. For instance, for a given class $ j $, and a set of instances $ \\boldsymbol{x}_1, \\boldsymbol{x}_2, \\boldsymbol{x}_3 $ in our dataset, we compute the corresponding outputs $ z_1 = f^j(\\boldsymbol{x}_1), z_2 = f^j(\\boldsymbol{x}_2), z_3 = f^j(\\boldsymbol{x}_3) $.\n\n   The optimal threshold $ \\hat{z} $ is then selected by identifying the value of $ z\\in\\\\{z_1, z_2, z_3\\\\} $ that minimizes the objective function specified in Equation (2):\n\n   $$\n   \\hat{z} = \\arg\\min_{z\\in \\\\{z_1, z_2, z_3\\\\}} \\left( \\frac{\\hat{q}_j(z)}{\\hat{q}_j^p(z)} + \\frac{1+\\tau}{\\hat{q}_j^p(z)}\\left( \\sqrt{\\frac{\\log(4/\\delta)}{2n}} + \\sqrt{\\frac{\\log(4/\\delta)}{2n_j^p}} \\right) \\right)\n   $$\n   This approach ensures that we find the optimal threshold that minimizes the given expression, as per Equation (2), across all available output values from the function $f^j$.\n\n3. > What is the time cost of the proposed method? Please discuss more about the time cost of the optimal threshold and the entire method in theory and experiments.\n\n   Please see the response for Weakness 2.\n\n4. > Theorem 4.2 tries to present the convergence of the empirical risk minimizer, but it seems that the empirical risk minimizer does not converge to the true risk minimizer. Please provide more analysis of Theorem 4.2 and more discussions about every component in the upper bound.\n\n   Please see the response for Weakness 1.\n\nWe have incorporated your suggestions into our revisions and hope that they meet your approval. We are open to any further questions or requests for additional information."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5153/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700060779470,
                "cdate": 1700060779470,
                "tmdate": 1700060779470,
                "mdate": 1700060779470,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1WlvhIC2TC",
                "forum": "jdynlBj3b0",
                "replyto": "6UTzeLC68x",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5153/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5153/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviwer,\n\nWe want to express our gratitude to your helpful comments and suggestions, which will be of great importance on the improvement of this work.\n\nWe have made efforts to address questions you raised and improve accordingly. We would like to double check to make sure that we have addressed all your concerns, and would you please let me know if you have any additional questions. Thank you.\n\nBest wishes,\n\nAuthors."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5153/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700452863163,
                "cdate": 1700452863163,
                "tmdate": 1700452863163,
                "mdate": 1700452863163,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "blyW5YeVxt",
                "forum": "jdynlBj3b0",
                "replyto": "6UTzeLC68x",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5153/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5153/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer,\n\nThank you for your valuable and detailed feedback on our manuscript. In response to your concerns, we have made a sincere effort to address and clarify the key issues. \n\nIn our previous response, we have focused on providing a more comprehensive analysis of Theorem 4.2, offering a clearer explanation of its components and implications for the classifier's accuracy. We have also elaborated on the method for determining the optimal threshold, including additional experiments to assess the time efficiency of this process, thereby ensuring its practical applicability in larger datasets. Additionally, in response to your observations regarding certain unclear expressions, we have provided more detailed explanations to ensure clarity and comprehensiveness.\n\nWhile we believe that these revisions address the key issues you have highlighted, we remain open to further guidance and suggestions. Thank you once again for your thoughtful guidance and support.\n\nBest wishes,\n\nAuthors."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5153/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700569251836,
                "cdate": 1700569251836,
                "tmdate": 1700569251836,
                "mdate": 1700569251836,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ADCnB6sRUQ",
            "forum": "jdynlBj3b0",
            "replyto": "jdynlBj3b0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5153/Reviewer_KmCM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5153/Reviewer_KmCM"
            ],
            "content": {
                "summary": {
                    "value": "The target problem of this paper is called single-positive multi-label learning. This is a weakly supervised version of the multi-label classification scenario, where each instance is annotated with only one of the positive labels. The other labels do not necessarily mean negative, but can potentially be positive or negative. The paper proposes a method called CRISP: it alternatively updates the class prior estimate and the multi-label classifier. Theoretically, the paper discusses that the estimated class-prior will converge to the ground-truth class-prior with enough training samples and an estimation error bound for the proposed empirical risk estimator. Experiments show CRISP works better than other methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Estimation error bound is provided for class prior estimation and for the empirical risk estimator.\n- Empirically, class-prior prediction is more accurate compared with others.\n- Multi-label prediction performance is often best for the proposed CRISP method."
                },
                "weaknesses": {
                    "value": "- The paper is motivated by the observation that previous methods have a strong assumption that class priors are assumed to be uniform. It would be interesting to see if the proposed method is still advantageous when class priors are uniform. It would enhance the paper's significance if the authors could demonstrate whether their proposed method retains its advantages even under the condition of uniform class priors.\n- It seems to me that the problem setting of SPMLL is a special case of \"Multi-Label Ranking From Positive and Unlabeled Data\" (CVPR 2016). I wonder if these general methods can be used as a baseline (and if not, what are the weaknesses of using these more general methods?)"
                },
                "questions": {
                    "value": "In addition to the points I wrote in the \"Weaknesses\":\n\n- It would be helpful to explicitly write out the definition of the absolute loss function and the derivation in Eq. 5.\n- I wasn't sure if we end up with an unbiased estimator (even with access to the ground truth class prior), because we have the additional absolute operator in the latter half of Eq. 7. It would be helpful if the paper can clarify."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5153/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5153/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5153/Reviewer_KmCM"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5153/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698725420679,
            "cdate": 1698725420679,
            "tmdate": 1700631989683,
            "mdate": 1700631989683,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qYxEbNj6PZ",
                "forum": "jdynlBj3b0",
                "replyto": "ADCnB6sRUQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5153/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5153/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for taking the time to review the paper and providing valuable feedback. I appreciate your efforts in ensuring the quality of the research. Regarding your concerns, I would like to provide the following explanations:\n\nFor Weaknesses:\n\n1. > The paper is motivated by the observation that previous methods have a strong assumption that class priors are assumed to be uniform. It would be interesting to see if the proposed method is still advantageous when class priors are uniform. It would enhance the paper's significance if the authors could demonstrate whether their proposed method retains its advantages even under the condition of uniform class priors.\n\n   Thank you for your insightful query regarding the performance of our proposed method in scenarios where class priors are uniform. We agree that evaluating our method under this condition would provide a more comprehensive understanding of its robustness and versatility.\n\n   However, unlike multi-class single-label datasets where the number of instances per class can be artificially balanced, multi-label datasets often exhibit complex label correlations that make such balancing more challenging. Existing multi-label datasets typically do not have uniform class priors, and due to the label correlations, balancing the occurrence of one class may inadvertently affect the distribution of other classes, making it impractical to achieve uniformity across all classes simultaneously [1].\n\n   This inherent imbalance in multi-label settings underscores the importance of considering class-prior imbalance in the design of SPMLL methods. Our approach is particularly suited to address these real-world scenarios where uniform class priors are not the norm.\n\n   [1] Wu, T., Huang, Q., Liu, Z., Wang, Y., & Lin, D. (2020). Distribution-balanced loss for multi-label classification in long-tailed datasets. In Proceedings of the 16th European Conference on Computer Vision (ECCV 2020) (pp. 162-178). Springer International Publishing.\n\n2. > It seems to me that the problem setting of SPMLL is a special case of \"Multi-Label Ranking From Positive and Unlabeled Data\" (CVPR 2016). I wonder if these general methods can be used as a baseline (and if not, what are the weaknesses of using these more general methods?)\n\n   I agree that the problem setting of SPMLL is a special case of \"Multi-Label Ranking From Positive and Unlabeled Data\" (It is also called Multi-label learning with missing labels, MLML). We acknowledge the reviewer's suggestion to compare against MLML methods. As recommended, we have added experiments with the state-of-the-art MLML approach [1] on SPMLL datasets. The results show our proposed method still achieves superior performance, as shown in below results:\n\n   Predictive performance of each comparing method on four MLIC datasets in terms of *mAP* (mean \u00b1 std). The best performance is highlighted in bold (the larger the better).\n\n   |       |       VOC        |       COCO       |     NUSWIDE      |       CUB        |\n   | :---: | :--------------: | :--------------: | :--------------: | :--------------: |\n   | LL-R  |   87.784\u00b10.063   |   70.078\u00b10.008   |   48.048\u00b10.074   |   18.966\u00b10.022   |\n   | LL-Cp |   87.466\u00b10.031   |   70.460\u00b10.032   |   48.000\u00b10.077   |   19.310\u00b10.164   |\n   | LL-Ct |   87.054\u00b10.214   |   70.384\u00b10.058   |   47.930\u00b10.010   |   19.012\u00b10.097   |\n   | Crisp | **89.820\u00b10.191** | **74.640\u00b10.219** | **49.996\u00b10.316** | **21.650\u00b10.178** |\n\n   Predictive performance of each comparing method on MLL datasets in terms of *Average Precision* (mean \u00b1 std). The best performance is highlighted in bold (the larger the better).\n\n   |       |      Image      |      Scene      |      Yeast      |     Corel5k     |    Mirflickr    |    Delicious    |\n   | :---: | :-------------: | :-------------: | :-------------: | :-------------: | :-------------: | :-------------: |\n   | LL-R  |   0.605\u00b10.058   |   0.714\u00b10.035   |   0.658\u00b10.006   |   0.268\u00b10.002   |   0.625\u00b10.001   |   0.296\u00b10.004   |\n   | LL-Cp |   0.595\u00b10.031   |   0.735\u00b10.028   |   0.700\u00b10.000   |   0.259\u00b10.004   |   0.621\u00b10.007   |   0.251\u00b10.007   |\n   | LL-Ct |   0.600\u00b10.012   |   0.669\u00b10.052   |   0.629\u00b10.007   |   0.258\u00b10.004   |   0.619\u00b10.004   |   0.253\u00b10.004   |\n   | Crisp | **0.749\u00b10.037** | **0.795\u00b10.031** | **0.758\u00b10.002** | **0.304\u00b10.003** | **0.628\u00b10.003** | **0.319\u00b10.001** |"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5153/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700057316869,
                "cdate": 1700057316869,
                "tmdate": 1700057316869,
                "mdate": 1700057316869,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6Cxgc2kWxS",
                "forum": "jdynlBj3b0",
                "replyto": "ADCnB6sRUQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5153/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5153/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Predictive performance of each comparing method on MLL datasets in terms of *Ranking Loss* (mean \u00b1 std). The best performance is highlighted in bold (the smaller the better).\n\n   |       |      Image      |      Scene      |      Yeast      |     Corel5k     |    Mirflickr    | Delicious       |\n   | :---: | :-------------: | :-------------: | :-------------: | :-------------: | :-------------: | --------------- |\n   | LL-R  |   0.346\u00b10.072   |   0.155\u00b10.021   |   0.227\u00b10.001   |   0.114\u00b10.001   |   0.123\u00b10.003   | 0.129\u00b10.002     |\n   | LL-Cp |   0.329\u00b10.041   |   0.148\u00b10.017   |   0.215\u00b10.000   |   0.114\u00b10.003   |   0.124\u00b10.003   | 0.160\u00b10.001     |\n   | LL-Ct |   0.327\u00b10.019   |   0.180\u00b10.038   |   0.238\u00b10.001   |   0.115\u00b10.001   |   0.124\u00b10.002   | 0.160\u00b10.000     |\n   | Crisp | **0.164\u00b10.027** | **0.112\u00b10.021** | **0.164\u00b10.001** | **0.113\u00b10.001** | **0.118\u00b10.001** | **0.122\u00b10.000** |\n\n   Predictive performance of each comparing method on MLL datasets in terms of *One Error* (mean \u00b1 std). The best performance is highlighted in bold (the smaller the better).\n\n   |       |      Image      |      Scene      |      Yeast      |     Corel5k     |    Mirflickr    |    Delicious    |\n   | :---: | :-------------: | :-------------: | :-------------: | :-------------: | :-------------: | :-------------: |\n   | LL-R  |   0.597\u00b10.084   |   0.490\u00b10.054   |   0.436\u00b10.087   |   0.715\u00b10.006   |   0.342\u00b10.016   |   0.543\u00b10.041   |\n   | LL-Cp |   0.629\u00b10.043   |   0.450\u00b10.051   |   0.240\u00b10.000   |   0.731\u00b10.016   |   0.357\u00b10.016   |   0.490\u00b10.028   |\n   | LL-Ct |   0.616\u00b10.019   |   0.574\u00b10.074   |   0.552\u00b10.097   |   0.726\u00b10.022   |   0.375\u00b10.012   |   0.475\u00b10.019   |\n   | Crisp | **0.325\u00b10.026** | **0.311\u00b10.047** | **0.227\u00b10.004** | **0.646\u00b10.006** | **0.295\u00b10.009** | **0.402\u00b10.003** |\n\n   In conclusion, while MLML methods could be applied to SPMLL, our experiments validate that our approach designed specifically for the SPMLL problem outperforms these more general techniques.\n\n   [1] Kim, Y., Kim, J. M., Akata, Z., & Lee, J. (2022). Large loss matters in weakly supervised multi-label classification. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition* (pp. 14156-14165).\n\nFor Questions:\n\n1. > It would be helpful to explicitly write out the definition of the absolute loss function and the derivation in Eq. 5.\n\n   Thank you for your valuable suggestion to explicitly define the absolute loss function and provide the derivation for Eq. 5. We acknowledge that these additions will enhance the clarity and comprehensiveness of our manuscript.\n\n   The absolute loss function, which is used in our framework, is defined as $l(f^j(x),y_j)=|f^j(x)-y_j|$. And Eq. 5 is derived as:\n$$\n\\begin{aligned}\n\\mathcal{R}(f) &= \\sum_{\\boldsymbol y}p(\\boldsymbol y) \\mathbb{E}_{\\boldsymbol x \\sim p(\\boldsymbol x \\vert \\boldsymbol y)} \\left[ \\sum _{j=1}^c  y_j\\ell(f^j(\\boldsymbol x), 1) + (1 - y_j)\\ell(f^j(\\boldsymbol x), 0) \\right] \\\\\\\\\n& = \\sum _{j=1}^c p(y_j = 1) \\mathbb{E} _{\\boldsymbol x \\sim p(\\boldsymbol x \\vert y_j = 1)}\\left[ \\ell(f^j(\\boldsymbol x), 1) \\right] + p(y_j = 0) \\mathbb{E} _{\\boldsymbol x \\sim p(\\boldsymbol x \\vert y_j = 0)}\\left[ \\ell(f^j(\\boldsymbol x), 0) \\right] \\\\\\\\\n& = \\sum _{j=1}^c p(y_j = 1) \\mathbb{E} _{\\boldsymbol x \\sim p(\\boldsymbol x \\vert y_j = 1)}\\left[ 1 - f^j(\\boldsymbol x) \\right] + (1 - p(y_j = 1)) \\mathbb{E} _{\\boldsymbol x \\sim p(\\boldsymbol x \\vert y_j = 0)}\\left[ f^j(\\boldsymbol x) \\right] \\\\\\\\\n& = \\sum _{j=1}^c p(y_j = 1) \\mathbb{E} _{\\boldsymbol x \\sim p(\\boldsymbol x \\vert y_j = 1)}\\left[ 1 - f^j(\\boldsymbol x) \\right] + \\mathbb{E} _{\\boldsymbol x \\sim p(\\boldsymbol x)}\\left[f^j(\\boldsymbol x)\\right] - p(y_j = 1) \\mathbb{E} _{\\boldsymbol x \\sim p(\\boldsymbol x \\vert y_j = 1)}\\left[f^j(\\boldsymbol x) \\right] \\\\\\\\\n& = \\sum _{j=1}^c p(y_j = 1) \\mathbb{E} _{\\boldsymbol x \\sim p(\\boldsymbol x \\vert y_j = 1)}\\left[ 1 - f^j(\\boldsymbol x) \\right] + \\mathbb{E} _{\\boldsymbol x \\sim p(\\boldsymbol x)}\\left[f^j(\\boldsymbol x)\\right] - p(y_j = 1) \\mathbb{E} _{\\boldsymbol x \\sim p(\\boldsymbol x \\vert y_j = 1)}\\left[f^j(\\boldsymbol x) - 1 + 1 \\right] \\\\\\\\\n& = \\sum _{j=1}^c 2p(y_j = 1) \\mathbb{E} _{\\boldsymbol x \\sim p(\\boldsymbol x \\vert y_j = 1)}\\left[ 1 - f^j(\\boldsymbol x) \\right] + \\left(\\mathbb{E} _{\\boldsymbol x \\sim p(\\boldsymbol x)}\\left[f^j(\\boldsymbol x)\\right] - p(y_j = 1)\\right).\n\\end{aligned}\n$$\n   We ensure that these elements will be meticulously detailed in the revised version of the manuscript."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5153/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700059878485,
                "cdate": 1700059878485,
                "tmdate": 1700059939244,
                "mdate": 1700059939244,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "COLO5dpxQs",
                "forum": "jdynlBj3b0",
                "replyto": "ADCnB6sRUQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5153/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5153/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "2. > I wasn't sure if we end up with an unbiased estimator (even with access to the ground truth class prior), because we have the additional absolute operator in the latter half of Eq. 7. It would be helpful if the paper can clarify.\n\n   Thank you for your query regarding the unbiasedness of our estimator, particularly concerning the incorporation of the absolute value operator in Eq. 7. In our revision, we will clarify that the property of risk consistency with the absolute loss function is contingent upon the condition that $ E_{x\\sim p(x)}[f^j(x)] \\geq p(y_j=1) $. This condition ensures that the expected output of our model for class $ j $ is not less than the ground truth prior probability of that class being the positive label. \n\n   The inclusion of the absolute value serves two purposes in our formulation. On one hand, it ensures that when $ E_{x\\sim p(x)}[f^j(x)] < p(y_j=1) $, the expected value $ E_{x\\sim p(x)}[f^j(x)] $ is coerced to increase, thereby preserving the desirable property of risk consistency for the risk estimator. On the other hand, it aligns the model's output class prior with the true class prior, which is in direct correlation with the initial motivation of our paper\u2014addressing the assumption of uniform class priors in previous methodologies.\n\nWe hope that our revisions have addressed all of your concerns, but please let us know if there is anything else we can do to improve the manuscript. We would be happy to answer any additional questions or provide any further information you may need."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5153/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700060033340,
                "cdate": 1700060033340,
                "tmdate": 1700060033340,
                "mdate": 1700060033340,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tDCF9stlq2",
                "forum": "jdynlBj3b0",
                "replyto": "COLO5dpxQs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5153/Reviewer_KmCM"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5153/Reviewer_KmCM"
                ],
                "content": {
                    "title": {
                        "value": "Comment"
                    },
                    "comment": {
                        "value": "Thank you for answering my questions and for the additional experiments. Currently I do not have any follow-up questions. Since I have no remaining concerns, I plan to raise my score by one step."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5153/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631955372,
                "cdate": 1700631955372,
                "tmdate": 1700631955372,
                "mdate": 1700631955372,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "TKVD6A7aGv",
            "forum": "jdynlBj3b0",
            "replyto": "jdynlBj3b0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5153/Reviewer_BQ4v"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5153/Reviewer_BQ4v"
            ],
            "content": {
                "summary": {
                    "value": "The paper provides a novel class-priors estimator and unbiased risk estimator for the single-positive multi-label learning task. The estimator comes with a convergence guarantee. The paper also shows that the proposed method leads to strong performance on various tasks."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. Strong empirical performance\n2. The proposed method is theoretically principled with a convergence guarantee\n3. The unbiased risk estimator is simple and intuitive"
                },
                "weaknesses": {
                    "value": "1. Clarity of writing. I found the problem setting to be unclear until I finished section 4.  One suggestion would be to add more details about the setup in the preliminary setting e.g. the absolute loss function, before diving into deriving the estimators. Also, changing the order by deriving the risk estimator before the class-priors, may provide a better motivation on why we need to estimate the class-priors."
                },
                "questions": {
                    "value": "1.  The algorithm relies on iteratively estimating the class-prior from f and then using it to update f. Is it possible if there is a failure mode ?\n2.  Would it be possible to extend this type of estimator to a different loss than the absolute loss?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5153/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5153/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5153/Reviewer_BQ4v"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5153/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698808790957,
            "cdate": 1698808790957,
            "tmdate": 1699636509449,
            "mdate": 1699636509449,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "G5tEBCsv0Q",
                "forum": "jdynlBj3b0",
                "replyto": "TKVD6A7aGv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5153/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5153/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for taking the time to review the paper and providing valuable feedback. I appreciate your efforts in ensuring the quality of the research. Regarding your concerns, I would like to provide the following explanations:\n\nFor Weaknesses:\n\n1. > Clarity of writing. I found the problem setting to be unclear until I finished section 4. One suggestion would be to add more details about the setup in the preliminary setting e.g. the absolute loss function, before diving into deriving the estimators. Also, changing the order by deriving the risk estimator before the class-priors, may provide a better motivation on why we need to estimate the class-priors.\n\n   We appreciate your feedback on the clarity of our paper's writing.\n\n   In response to your insightful comments, we will incorporate a more detailed exposition of the setup in the preliminary section, including an in-depth discussion of the absolute loss function. This will be done to lay a clearer foundation before we proceed to the derivation of the estimators.\n\n   Furthermore, we will adjust the order of the sections as you recommended. By deriving the risk estimator prior to introducing the class-priors, we aim to provide stronger motivation and a more logical progression for the need to estimate class-priors. This restructuring is intended to not only clarify the necessity of class-priors estimation within our proposed method but also to facilitate a more intuitive understanding of the method's overall framework.\n\n   We believe that these changes will significantly improve the clarity of the paper and will ensure that the problem setting is comprehensible early in the reading. We are committed to making the necessary revisions to ensure that the final manuscript meets the high standards of clear and logical academic writing.\n\nFor Questions:\n\n1. > The algorithm relies on iteratively estimating the class-prior from f and then using it to update f. Is it possible if there is a failure mode ?\n\n   Our approach initially treats all unknown labels as negative to warm up the model, which is a common practice in existing SPMLL methods. This warming-up step provides a stable starting point, yielding a reasonably effective model before the application of our proposed Crisp method. Upon employing the CRISP framework, we iteratively refine the model through the class-prior estimation technique. In our extensive experimental evaluations, we did not encounter instances of failure.\n\n2. > Would it be possible to extend this type of estimator to a different loss than the absolute loss?\n\n   In our proposed framework, the risk estimator's extension to loss functions beyond absolute loss is indeed feasible, specifically for symmetric loss functions. A symmetric loss function $l$ has the property that for any prediction $f^j(x)$, the sum of the loss for a positive label and the loss for a negative label is constant, i.e., $l(f^j(x),+1)+l(f^j(x),0)=C$, where $C$ is a constant.\n\nWe hope that our revisions have addressed all of your concerns, but please let us know if there is anything else we can do to improve the manuscript. We would be happy to answer any additional questions or provide any further information you may need."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5153/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700056866050,
                "cdate": 1700056866050,
                "tmdate": 1700056866050,
                "mdate": 1700056866050,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]