[
    {
        "title": "Learning Temporal Causal Representation under Non-Invertible Generation Process"
    },
    {
        "review": {
            "id": "wnUjowcQ1E",
            "forum": "5tSLtvkHCh",
            "replyto": "5tSLtvkHCh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2088/Reviewer_uMXb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2088/Reviewer_uMXb"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a model recovering latent causal factors of a time-series, in other words inverting the generating process of sequential data. The key feature of this model is that it utilises temporal context (i.e. to recover latent factors at time t, it uses observations at time t, t-1, ..., t - k for some k) which allows us to overcome the non-injectivity of the generating function. The model is motivated by a theoretical analysis showing that under certain assumptions such an inversion is guaranteed to recover the true latent factors. The numerical experiments demonstrate superior performance of the proposed model in comparison to a number of baselines on synthetic dataset and real-world datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ An interesting model addressing important questions of nonlinear identifiability and disentanglement in temporal data\n+ Thorough theoretical analysis of the proposed model\n+ Experimental comparisons to a number of baseline models"
                },
                "weaknesses": {
                    "value": "I think the presentation could be somewhat improved. The paper is full of technical details of the identifiability theory but I think it would also benefit from a higher-level discussion (maybe using a cartoon or a toy-example) illustrating the intuition behind the assumptions of the theorems. My take home message after reading this paper is that using temporal context enables nonlinear identifiability and disentanglement under certain conditions, but I'd be struggling to explain what these conditions mean and why the temporal context is so crucial."
                },
                "questions": {
                    "value": "Questions to Definition 1:\n- Why are m and \\hat{m} in the subscript of distributions in Eq. (3)? As I understood m is not part of the generative model (we don't need it to generate data from the latent factors) so it shouldn't influence the resulting data distribution.\n- Should the model and data distributions match almost everywhere rather than everywhere?\n- According to this definition, the latent process is identifiable if the model and data distributions match and \\hat{m} = m up to permutations. What about the case when the model and data distributions don't match but \\hat{m} = m up to permutations? (for example, if we could obtain true m-function with the wrong model) Is it an impossible scenario or rather just not in the scope of this paper?\n\nQuestions to Theorem 1:\n- Is it possible to estimate how much temporal context (i.e. the value of \\mu) is required for identifiability in Theorem 1? Or does the result only say that if the inverting function m exists for some \\mu then we can estimate it up to permutations but we don't know how much temporal history we might need to that?\n- (A more speculative question, feel free to ignore if it doesn't make sense.) Do you have an intuition what happens as \\mu -> \\infty? Is every model identifiable in the limit or not necessarily?\n\nQuestion to Section 4:\n- \"To enforce the conditional independence of latent variables, the distribution of p(z_t | x_{t:t\u2212\u03bc}) is constrained by the prior.\" Why does the prior constrain the conditional independencies in the posterior? I guess you refer to ELBO which includes a KL divergence to the prior, but the global maximiser of ELBO is the true posterior distribution, and clearly there are examples of models with independent prior but dependent posterior (e.g. https://en.wikipedia.org/wiki/Interaction_information#Negative_interaction_information)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2088/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698694741320,
            "cdate": 1698694741320,
            "tmdate": 1699636141282,
            "mdate": 1699636141282,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Y1OzwcTQ6J",
                "forum": "5tSLtvkHCh",
                "replyto": "wnUjowcQ1E",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2088/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2088/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer uMXb Part 1"
                    },
                    "comment": {
                        "value": "Dear Reviewer uMXb, we would like to express our deep gratitude for your kind approval and valuable suggestions provided on our paper, as well as for the time you devoted to reviewing it. Below, we have addressed each of your comments and suggestions.\n\n> **W1:** I think the presentation could be somewhat improved. The paper is full of technical details of the identifiability theory but I think it would also benefit from a higher-level discussion (maybe using a cartoon or a toy-example) illustrating the intuition behind the assumptions of the theorems. My take home message after reading this paper is that using temporal context enables nonlinear identifiability and disentanglement under certain conditions, but I'd be struggling to explain what these conditions mean and why the temporal context is so crucial.\n\n\n**A1:** We are grateful for your valuable suggestions which helped better explain our insights.  The identifiability results in our theorem rely on the assumption that there exists a function $\\mathbf{m}$ to recover the information lost caused by non-invertibility from the temporal context. \nThe intuitive insights leveraging the temporal context come from the human visual perception system.  We illustrate our concept with two intuitive examples. \n\n**The perception of 3D world using vidoe**\nThe process of visual perception can be understood as a non-invertible transformation, where a three-dimensional world is projected onto two-dimensional images. This transformation inherently loses some spatial information due to its non-invertible nature. However, by analyzing multiple views of the same object in a video, we can compensate for this loss. Each view offers a unique perspective, providing complementary information that collectively helps in reconstructing a more complete understanding of the object in its original three-dimensional form. \n\n\n**Inference with occlusion**\nSimilarly, consider an example about the inference of the location of a ball that is totally occluded. The occlusion is a non-invertible function. Our humans often have the ability to infer the location of the occluded ball, since we can obtain the information from temporal contexts. \n\n\nTo elaborate on why function $\\mathbf{m}$ exists and what these conditions mean, we propose both an intuitive example and a mathematical example (with a carton) using visual persistence. Please refer to Section 2.3 and Figure 2 in the revised paper for more details. \n\n> **W2(Q1):** Why are m and \\hat{m} in the subscript of distributions in Eq. (3)? As I understood m is not part of the generative model (we don't need it to generate data from the latent factors) so it shouldn't influence the resulting data distribution.\n\n**A2:** We greatly appreciate your suggestions. Although $\\mathbf{m}$ does not explicitly participate in the generation process, it does so implicitly as a property of the mixing function $\\mathbf{g}$. That is why we put $\\mathbf{m}$ into the definition part in the manuscript. However, we do agree that this may cause confusion. Therefore, we have removed $\\mathbf{m}$ from the definition part and added an explanation in the paragraph below Definition 1. \n\n> **W3(Q2):** Should the model and data distributions match almost everywhere rather than everywhere?\n\n**A3:**  Thanks a lot for pointing out this question. By 'everywhere' we would like to say that 'for any value of $\\mathbf{x}_t$'. We have corrected this issue in Definition 1 in the revised version.\n\n> **W4(Q3):** According to this definition, the latent process is identifiable if the model and data distributions match and \\hat{m} = m up to permutations. What about the case when the model and data distributions don't match but \\hat{m} = m up to permutations? (for example, if we could obtain true m-function with the wrong model) Is it an impossible scenario or rather just not in the scope of this paper?\n\n**A4:**   Thanks a lot for your valuable questions. Actually, the aim here is to figure out under what conditions can the process be identifiable. It is totally possible that we get the true process, up to a permutation and element-wise nonlinear function, while the data distributions don't match. Take a example, we have $\\hat{z} = z$ and $\\hat{m} = \\pi \\circ m$. Both $\\hat{z},\\hat{m}$ are versions of permutated truth. However, since the permutations are not matched, the data distributions don't match.  To confirm, it is not in the scope of this paper."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2088/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700317940512,
                "cdate": 1700317940512,
                "tmdate": 1700317940512,
                "mdate": 1700317940512,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eHmaKgFNJZ",
                "forum": "5tSLtvkHCh",
                "replyto": "wnUjowcQ1E",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2088/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2088/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer uMXb Part 2"
                    },
                    "comment": {
                        "value": "> **W6(Q5):** (A more speculative question, feel free to ignore if it doesn't make sense.) Do you have an intuition what happens as \\mu -> \\infty? Is every model identifiable in the limit or not necessarily?\n\n**A6:**  Great insights! Thanks for this question. In our proof, for any given $\\mu$ it will work whether it is infinite or not. Actually, when $\\mu$ is infinite, the temporal context involves more information ideally. We provided an explicit function of $m$ in a toy example if $\\mu$ is infinite.\n\n**Notice:** Due to the issue of markdown for complex equations, we use   $a\\underline{ }(b)$ to represent $a_b$.\n\nLet $\\mathbf{x}\\underline{ }t = \\mathbf{g}(\\mathbf{z}\\underline{ }{(t:t-1)}) = \\frac{2}{3}\\mathbf{z}\\underline{ }t + \\frac{1}{3}\\mathbf{z}\\underline{ }{(t-1)}.$\nGiven an observed sequence with length $\\mu$, the current latent variable can be rewritten as $\\mathbf{z}\\underline{ }t  = \\frac{3}{2}\\mathbf{x}\\underline{ }t - \\frac{1}{2}\\mathbf{z}\\underline{ }{(t-1)} = \\left(-\\frac{1}{2}\\right)^{\\mu+1}\\mathbf{z}\\underline{ }{(t-\\mu-1)} + \\frac{3}{2}\\sum_{i=0}^\\mu \\left(-\\frac{1}{2}\\right)^{i}\\mathbf{x}\\underline{ }{(t-i)}.$\nThereby we can constructing a function $\\mathbf{m}$ as $\\mathbf{m}(\\mathbf{x}\\underline{ }{(t:t-\\mu)})=\\frac{3}{2}\\sum_{i=0}^\\mu \\left(-\\frac{1}{2}\\right)^{i}\\mathbf{x}\\underline{ }{(t-i)}$. When we have an infinite observation sequence, we have $\\lim_{\\mu\\rightarrow\\infty}\\mathbf{m}(\\mathbf{x}\\underline{ }{(t:t-\\mu)}) = \\mathbf{z}\\underline{ }t$. \n\nIn practice, an increasing $\\mu$ will bring more noise, which may affect the model training. So we apply the model selection to determine $\\mu$ in practice.\nPlease refer to Appendix A4.4 in the revised paper for more details.\n\n> **W7(Q6):** \"To enforce the conditional independence of latent variables, the distribution of p(z\\underline{ }t | x\\underline{ }{t:t\u2212\u03bc}) is constrained by the prior.\" Why does the prior constrain the conditional independencies in the posterior? I guess you refer to ELBO which includes a KL divergence to the prior, but the global maximiser of ELBO is the true posterior distribution, and clearly there are examples of models with independent prior but dependent posterior (e.g. https://en.wikipedia.org/wiki/Interaction\\underline{ }information#Negative\\underline{ }interaction\\underline{ }information)\n\n**A7:** We respectfully appreciate your insightful question. Sorry for the inconvenience caused by the unclear writing. \n\nWhen we say \"To uphold the conditional independence assumption\", we didn't mean that the KL divergence enforces entries of $z\\underline{ }t$ to be conditional independent on $\\mathbf{\\hat{z}}\\underline{ }{(t-1:t-\\tau)}$ with each other. Instead, what we want to say is that by minimizing the KL divergence between prior $p(\\mathbf{\\hat{z}}\\underline{ }t|\\mathbf{\\hat{z}}\\underline{ }{(t:t-\\tau)})$ and posterior $q(\\mathbf{\\hat{z}}\\underline{ }t|\\mathbf{x}\\underline{ }{(t:t-\\mu)})$, the posterior is also conditionally independent on $\\mathbf{\\hat{z}}\\underline{ }{(t-1:t-\\tau)}$, since the conditional independence is hard-coded in the calculation of prior. \nWe reorganized the paragraph \"Transition Prior Module\" in Section 4 to make it more clear. \n\nAfter all, many thanks for your effort to enhance the readability of our paper."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2088/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700318637566,
                "cdate": 1700318637566,
                "tmdate": 1700318637566,
                "mdate": 1700318637566,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Fd2rHNJNBa",
                "forum": "5tSLtvkHCh",
                "replyto": "eHmaKgFNJZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2088/Reviewer_uMXb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2088/Reviewer_uMXb"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your detailed reply! I don't have further questions at this stage."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2088/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700494878327,
                "cdate": 1700494878327,
                "tmdate": 1700494878327,
                "mdate": 1700494878327,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ikIy55iARd",
                "forum": "5tSLtvkHCh",
                "replyto": "wnUjowcQ1E",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2088/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2088/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your prompt feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer uMXb,\n\nWe are pleased to hear that your issues are totoally addressed. Thank you for your support and recognition of our work.\n\nBest regards,\n\nAuthors of submission 2088"
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2088/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700553862785,
                "cdate": 1700553862785,
                "tmdate": 1700553862785,
                "mdate": 1700553862785,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "kzfH9XfZ0U",
            "forum": "5tSLtvkHCh",
            "replyto": "5tSLtvkHCh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2088/Reviewer_ur5z"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2088/Reviewer_ur5z"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a novel approach for latent variable identification in a temporal setting. It relaxes the common assumption that the latent representation at time step t can be uniquely determined from the observation at that time step. Instead it assumes that it can be determined from a window of past observations. Their results rely on sufficient variability assumptions very similar to what has been proposed in the literature. They proposed an algorithm based of VAEs and normalizing flows to model the transition model in the latent space. They show experiments on synthetic data, to validate identifiability, and on realistic QA datasets, to assess the usefulness of the learned representation.\n\n### **Review summary**\nAlthough I believe the motivation and the proposition of this paper is very good, I believe this manuscript is not ready for publication. My main concerns are:\n- I am not certain that the there exists a model that actually satisfy the assumptions of this work.\n- The paper presents many math mistakes and imprecision. The terminology used is also wrong at times.\n- The point made in Section 3.3 was already made in a previous work [4]. This work also presents a counter example to Lemma 1 (implying that it is false). \n- The writing quality is low\n- The paper is not well situated in the literature, which makes it hard for a non-expert to understand what is the actual novelty.\n\nI substantiate all of these points below, in the \"Weakness\" section. I sincerely believe this idea has great potential, but too many problems in the execution. For these reasons, I recommend rejection. I very much hope that the authors will take my criticisms seriously and use them to improve their work.\n\n### **Post discussion phase**\n\nLooks like the discussion phase is over. I was hoping to answer the last points raised by the authors, but couldn't do it in the comments, so I decided to share it here:\n\n----\n\nConcrete mathematical example: Well, if you take $f_i$ to be noisy here, what is the corresponding $m$? You provided an $m$ \nonly for the non-noisy case, but your theory assumes noise. And my guess is that the noise is crucial to your proofs (as is often the case in nonlinear ICA).\n\nCounter-example to Lemma 1: Indeed, if you change your definition of disentanglement to having the \"same permutation everywhere\" then the result is correct. But the current phrasing of the Lemma does not suggest this definition, so Example 6 is indeed a counter-example. It's impossible for me to review your modification and make sure it is correct.\n\nThis is a very mathematical paper. I feel like many non-trivial changes to the paper have been done. For instance, the whole section on the model definition has been updated. For some reason, I cannot view the previous versions of the paper, but iirc the mixing function use to take as input z_t and output x_t, correct? This seems to be corroborated by Figure 3 where the StepDecoder has as input only z_t. In the current revision, the model definition allows g to take as input a window of past z_t. This is a non-trivial change in my opinion. What are the repercussions of this change to your proof and the rest of the paper? This is only one of the many changes that this manuscript received. IMO, this version requires a full rereading to make sure everything adds up, i.e. a complete review. That's why I believe this is not something reasonable to ask during a discussion phase."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Relaxing the invertibility of the mixing function in identifiable representation learning is a very important direction and the suggestion that temporal context could be used to infer the latent factors in that case makes a lot of sense intuitively.\n- This suggestion is novel AFAIK.\n- Very few theoretical works of this nature present experiments on realistic data to show the usefulness of their approach, as was done in the present work. This is appreciated."
                },
                "weaknesses": {
                    "value": "### **Is there a mathematically explicit example of model that satisfies the assumptions?**\nThe authors assume a standard data generating process where $z^t$ follows some dynamical process and where $x_t = g(z_t)$. However, they do not assume that the mixing $g$ is injective. Instead, they assume that there exists a map $m$ s.t. $z_t = m(x_{t:t-\\mu})$. This feels like a reasonable assumption, however, I think the authors should provide at least one mathematically concrete example where this assumption holds (specifying what is f, g and m explicitly). This is important, in my opinion, to make sure that the result is not completely vacuous in the sense that there exists no model that satisfies the assumption of the theory. Right now it's not entirely clear to me that such an example exists.\n\n### **Math mistakes, unclear proofs and bad terminology**\n- Beginning of 2.1: wrong definition of surjectivity. What you describe is simply the definition of a function (i.e. it has a unique output). A map $f: A \\rightarrow B$ is surjective if, for all $b \\in B$, there exists $a \\in A$ s.t. $f(a) = b$.\n- In the third assumption of Theorem 1:\n    - What is a \u201ccontinuous manifold\u201d? Can you refer to a definition from a math textbook? You mean a topological manifold? Does it mean the support of $\\hat z$ can have a lower dimension than the ambient space?\n    - requiring that $m, \\hat m, g, \\hat g$ are twice differentiable is clear, but the \u201ci.e.\u201d following is confusing. You could simply get rid of it.\n- I\u2019m confused by the fact that the theorems do not reuse Definition 1 with its notion of \u201cobservational equivalence\u201d. Instead, the theorems start with $x_t = \\hat g(\\hat z_t)$ and $\\hat z_t = \\hat m (x_{t:t-\\mu})$. It certainly implies observational equivalence, but is it equivalent? Should I think of the equalities here as \u201chave equal distribution\u201d, or is it a normal equality? Also the theorem does not refer to the data generating process of section 2.1. This is unclear. \n- Definition 1: It looks like it is implicitly assumed that the random vector x_1, \u2026 x_T has a density (w.r.t. The Lebesgue measure). I think this is also assumed in the proof, equation (17), where the change of variable formula for densities is used (which works only for densities). It\u2019s not clear that the random vector x_1, \u2026 x_T has a density. For example, if dim(x) > dim(z), it won\u2019t be the case. Are you assuming dim(x) = dim(z)? I couldn\u2019t find dim(x) anywhere.\n- Definition 1: The authors seem to include $m$ in the parameters of the generative model. I find this a bit weird since the model is fully specified by f, p, g. No need for m in the parameters.\n- Corollary 1: Usually, corollaries are very simple consequences of a theorem. Here, it doesn\u2019t look like it\u2019s a simple consequence, it actually looks like a generalization. Also, would it be possible to unify Theorem 1 and Corollary 1 in a way that both of these results are special cases? Suggesting because restating almost identical assumptions looks a bit inefficient.\n- Equation (3), should be $\\forall x_{t, t-\\mu} \\in \\mathcal{X}^{\\mu +1}$.\n- Section 3.3: The terminology used here does not align with standard terminology used in topology. For example, what the authors call a \u201ccontinuous domain\u201d or a \u201ccontinuous set\u201d is usually called a \u201cpath-connected\u201d set in topology. Please use existing terminology.\n- In the Jacobian on page 7, what do the \u201c*\u201d mean? Zeros? \n- VAE-based approach: \u201cTo uphold the conditional independence assumption, we aim to minimize the KL divergence between the posterior for each time step, p(\\hat zt|xt:t\u2212\u00b5), and the prior distribution p(\\hat zt|\\hat zt\u22121:t\u2212\u03c4 ).\u201d IMO, this shows a poor understanding of what VAE\u2019s are all about. First, for p(\\hat zt|xt:t\u2212\u00b5), the letter \u201cq\u201d should be used to specify that this is not the \u201cactual\u201d posterior of the model, but a variational approximation. Secondly, saying the KL enforces conditional independence is weird. Conditional independence is hard-coded in your generative model, the KL is just part of your evidence lower bound. It\u2019s not present specifically to enforce or encourage conditional independence. \n\n    \n### **Issues in Section 3.3**\n- The authors rightfully points out that one has to be careful when going from \u201cJacobian is a permutation-scaling matrix\u201d to \u201cthe mapping is a permutation composed with an element-wise transformation\u201d when the domain of the function is not simply $\\mathbb{R}^n$. However, [4] already made that point (see beginning of Section 3.1 and the discussion surrounding what they call \"local\" and \"global\" disentanglement).  \n- Moreover, Example 6 from [4] presents a counterexample to Lemma 1, i.e. an example of function with a path-connected domain where the Jacobian is everywhere a permutation-scaling matrix, but the function is not \u201cdisentangled\u201d, in the sense that it cannot be written as a permutation composed with an element-wise rescaling. This implies that Lemma 1 has to be wrong.\n- I also spent some time reading the proof of Lemma 1 and it is unclear. For example, what is the \u201cn-dimensional axis except 0\u201d? You can also find weird terminologies which makes understanding the argument impossible. This makes me even more confident that Lemma 1 is wrong.\n\n### **Writing is unclear/imprecise**\nThe overall quality of writing was low. I found many sentences that were weirdly formulated. For example: \n- Not sure I understand \u201cNon-invertibility by vision persistence\u201d from the intro. Why does the crashing car example have vision persistence? This was not explained, no?\n- \u201cThus, we assume that there exists a maximum time lag $\\mu$ and an arbitrary nonlinear function $m$...\u201d The word \u201carbitrary\u201d shouldn\u2019t be there.\n- \u201cIn this case, there is information loss in $x_t$ due to the non-invertibility of $g$.\u201d This is imprecise. What is meant by information here? I believe what you mean is that one cannot recover $z^t$ from $x^t$ alone.\n- \u201cWe say latent causal processes are identifiable if observational equivalence can lead to identifiability of the latent variables\u2026\u201d This phrasing is weird. They define identifiability, but use the word \u201cidentifiability\u201d in its definition. This should be rephrased.\n- \u201cDue to the complexity of the non-invertible mixing function, the identifiable representation does not indicate the inference function is identifiable.\u201d I don\u2019t understand this sentence. \n- \u201cwith a function m that satisfies our assumption zt = m(xt:t\u2212\u00b5) in existence\u201d Weird sentence formulation.\n- Figure 3 (c), what is the x-axis?\n\n### **Should make more connections with existing works.**\n- Theorem 1 seems to reuse assumptions very similar to [2], which itself reuses assumptions similar to the line of Aapo Hyvarinen\u2019s group, see for example [3]. I think this resemblance should be highlighted in the text to help the reader understand what is truly novel in the proposed theoretical results. In general, I feel like the results could be contextualized in the literature a bit more.\n- [1] should be cited, as it was among the first work showing identifiability was possible in dynamical latent dynamical systems.\n\n\n\n\n\n\n[1] S. Lachapelle, P. Rodriguez Lopez, Y. Sharma, K. E. Everett, R. Le Priol, A. Lacoste, and S. Lacoste-Julien. Disentanglement via mechanism sparsity regularization: A new principle for nonlinear ICA. In First Conference on Causal Learning and Reasoning, 2022.\n\n[2] W. Yao, G. Chen, and K. Zhang. Temporally disentangled representation learning. In Advances in Neural Information Processing Systems, 2022a.\n\n[3] A. Hyvarinen, H. Sasaki, and R. E. Turner. Nonlinear ica using auxiliary variables and generalized contrastive learning. In AISTATS. PMLR, 2019.\n\n[4] S. Lachapelle, D. Mahajan, I. Mitliagkas and S. Lacoste-Julien. Additive Decoders for Latent Variables Identification and Cartesian-Product Extrapolation. NeurIPS 2023."
                },
                "questions": {
                    "value": "Is the advantage of the proposed algorithm over the baselines on the QA benchmark due to disentanglement and better identifiability? Or is it due to architectural choices? I feel this should be addressed, since the paper is very much centered around disentanglement and identifiability."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2088/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2088/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2088/Reviewer_ur5z"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2088/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698700471783,
            "cdate": 1698700471783,
            "tmdate": 1700778079060,
            "mdate": 1700778079060,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BttXnXvtY5",
                "forum": "5tSLtvkHCh",
                "replyto": "kzfH9XfZ0U",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2088/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2088/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ur5z Part 1"
                    },
                    "comment": {
                        "value": "Dear Reviewer ur5z, we highly appreciate all your time dedicated to reviewing this paper! Your careful reading and valuable suggestions definitely helped improve the paper's quality.  We provided the point-to-point responses below and modified our manuscript accordingly. \n\n> **W1:** Is there a mathematically explicit example of model that satisfies the assumptions? The authors assume a standard data generating process where $z_t$ follows some dynamical process and where $x_t=g(z_t)$. However, they do not assume that the mixing $g$ is injective. Instead, they assume that there exists a map $m$ s.t. $z_t=m(x_{t:t-\\mu})$. This feels like a reasonable assumption, however, I think the authors should provide at least one mathematically concrete example where this assumption holds (specifying what is f, g and m explicitly). This is important, in my opinion, to make sure that the result is not completely vacuous in the sense that there exists no model that satisfies the assumption of the theory. Right now it's not entirely clear to me that such an example exists.\n\n**A1:** We sincerely appreciate your insightful suggestions. For a better understanding of when such a  condition can be satisfied, we add Section 2.3 for a detailed example. A figurative example with a fast-moving ball and a mathematical example are provided in this section. \n\n**Notice:** Due to the issue of markdown for complex equations, we use   $a\\underline{ }(b)$ to represent $a_b$.\n\nHere is a mathematical example to demonstrate the existence of function $\\mathbf{m}$.  Following the concept of visual persistence, let the current observation be a weakened previous observation overlaid with the current image of the object, i.e., $\\mathbf{x}\\underline{ }t = \\mathbf{z}\\underline{ }t + \\frac{1}{2}\\mathbf{x}\\underline{ }{(t-1)} = \\sum_{i=1}^{\\infty} \\left(\\frac{1}{2}\\right)^{i} \\mathbf{z}\\underline{ }{(t-i)}$.  Given an extra observation, the current latent variable can be rewritten as $\\mathbf{z}\\underline{ }t  = \\mathbf{x}\\underline{ }t - \\frac{1}{2}\\mathbf{x}\\underline{ }{(t-1)}$.\nThereby we can easily recover latent variables that cannot be obtained from a single observation, i.e., $\\mathbf{z}\\underline{ }t = \\mathbf{m}(\\mathbf{x}\\underline{ }{(t:t-1)}) = \\mathbf{x}\\underline{ }t - \\frac{1}{2}\\mathbf{x}\\underline{ }{(t-1)}$.\n\n> **W2:**  Beginning of 2.1: wrong definition of surjectivity. What you describe is simply the definition of a function (i.e. it has a unique output). A map $f:A\\rightarrow B$ is surjective if, for all $b\\in B$, there exists $a\\in A$ s.t. $f(a)=b$.\n\n**A2:** We greatly appreciate your pointing out the inaccuracies in our paper. We intended to convey that the range of the mixed function $g$ is $\\mathcal{X}$. However, we acknowledge that such phrasing can increase the difficulty of reading the paper. Therefore, we have revised Section 2.1 and removed the related statement.\n\n> **W3:** In the third assumption of Theorem 1:  What is a \u201ccontinuous manifold\u201d? Can you refer to a definition from a math textbook? You mean a topological manifold? Does it mean the support of $\\hat{z}$\n can have a lower dimension than the ambient space? Requiring that $m,\\hat{m}, g, \\hat{g}$ are twice differentiable is clear, but the \u201ci.e.\u201d following is confusing. You could simply get rid of it.\n\n**A3:** Thank you for your valuable comments which help clarify terminology. We used ''continuous manifold'' to represent a continuous domain. We agree that the ''path-connected domain'' is a better expression and correct it in the revised paper. We also remove the ''i.e.'' part following the sentence in Theorem 1. It makes our theorem much more concise.\n\n> **W4:** I\u2019m confused by the fact that the theorems do not reuse Definition 1 with its notion of \u201cobservational equivalence\u201d. Instead, the theorems start with $x_t=\\hat g(\\hat z_t)$ and $\\hat z_t=\\hat m (x_{t:t-\\mu})$. It certainly implies observational equivalence, but is it equivalent? Should I think of the equalities here as \u201chave equal distribution\u201d, or is it a normal equality? Also the theorem does not refer to the data generating process of section 2.1. This is unclear.\n\n**A4:** Thanks for your insightful suggestions. They definitely improve the readability of our paper. \n- As you mentioned, the condition given in Theorem 1 is observational equivalent to the generating process. We have revised Theorem 1 and reused the definition of observational equivalence in the theorem.\n- The equalities in theorem 1 are normal equalities. When the normal equalities are established everywhere, the equality of distributions is also established. Thus it yields the following proof.\n- When it comes to the true generating process, we are actually talking about the process mentioned in Eq.4."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2088/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700319845601,
                "cdate": 1700319845601,
                "tmdate": 1700319845601,
                "mdate": 1700319845601,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TVlAmldH48",
                "forum": "5tSLtvkHCh",
                "replyto": "kzfH9XfZ0U",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2088/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2088/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ur5z Part 2"
                    },
                    "comment": {
                        "value": "> **W5:** Definition 1: It looks like it is implicitly assumed that the random vector x\\underline{ }1, \u2026 x\\underline{ }T has a density (w.r.t. The Lebesgue measure). I think this is also assumed in the proof, equation (17), where the change of variable formula for densities is used (which works only for densities). It\u2019s not clear that the random vector x\\underline{ }1, \u2026 x\\underline{ }T has a density. For example, if dim(x) > dim(z), it won\u2019t be the case. Are you assuming dim(x) = dim(z)? I couldn\u2019t find dim(x) anywhere.\n\n**A5:** Thanks for your valuable question. The observation $x_t$ is indeed a random vector with a density, and it is also assumed in Eq.18 (originally Eq.17). In my opinion, whether $dim(x)=dim(z)$ doesn't matter, since we never assume there is an equation like $p(z)/|J_g|=p(x)$ when $x=g(z)$. Thus, we do not require jacobian $J_g$ of mixing function $g$ to be a square matrix. That is to say, $dim(x)$ can be different from $dim(z)$.\n\n> **W6:** Definition 1: The authors seem to include $m$ in the parameters of the generative model. I find this a bit weird since the model is fully specified by f, p, g. No need for m in the parameters.\n\n**A6:** We greatly appreciate your suggestions. Although $\\mathbf{m}$ does not explicitly participate in the generation process, it does so implicitly as a property of the mixing function $\\mathbf{g}$. That is why we put $\\mathbf{m}$ into the definition part in the manuscript. However, we do agree that this may cause confusion. Therefore, we have removed $\\mathbf{m}$ from the definition part and added an explanation in the paragraph below Definition 1.\n\n> **W7:** Corollary 1: Usually, corollaries are very simple consequences of a theorem. Here, it doesn\u2019t look like it\u2019s a simple consequence, it actually looks like a generalization. Also, would it be possible to unify Theorem 1 and Corollary 1 in a way that both of these results are special cases? Suggesting because restating almost identical assumptions looks a bit inefficient.\n\n**A7:** Thank you for your insightful suggestions. We used to split them up because they can fit into the two scenarios named \"UG\" and \"UG-TDMP\" separately. We totally agree that a merged version is more clear. In light of your suggestion, we have merged them into Theorem 1 in Section 3.1 in the revised paper. \n\n> **W8:** Equation (3), should be $\\mathbf{x}_{t:t-\\mu} \\in \\mathcal{X}^{\\mu+1}$.\n\n**A8:** Thanks for your correction. We have corrected it in Eq.3.\n\n> **W9:** Section 3.3: The terminology used here does not align with standard terminology used in topology. For example, what the authors call a \u201ccontinuous domain\u201d or a \u201ccontinuous set\u201d is usually called a \u201cpath-connected\u201d set in topology. Please use existing terminology.\n\n**A9:** Thank you very much for your feedback on the readability of our article. The reason we use \u201ccontinuous set\u201d is that we start to consider this issue from a perspective of the property of functions, rather than a perspective of topology. We do agree that a more accurate terminology can better improve our paper. Thus we revise the manuscript accordingly, changing all \"continuous set/domain\" to \"path-connected\" in all theorems, lemmas, explanations, and proof. Thanks for your effort to enhance the readability of our paper again.\n\n> **W10:**  In the Jacobian on page 7, what do the \u201c*\u201d mean? Zeros?\n\n**A10:** Thank you for your pointing out this typo. We have corrected it to zeros.\n\n> **W11:** VAE-based approach: \u201cTo uphold the conditional independence assumption, we aim to minimize the KL divergence between the posterior for each time step, p(\\hat zt|xt:t\u2212\u00b5), and the prior distribution p(\\hat zt|\\hat zt\u22121:t\u2212\u03c4 ).\u201d IMO, this shows a poor understanding of what VAE\u2019s are all about. First, for p(\\hat zt|xt:t\u2212\u00b5), the letter \u201cq\u201d should be used to specify that this is not the \u201cactual\u201d posterior of the model, but a variational approximation. Secondly, saying the KL enforces conditional independence is weird. Conditional independence is hard-coded in your generative model, the KL is just part of your evidence lower bound. It\u2019s not present specifically to enforce or encourage conditional independence.\n\n**A11:** We respectfully appreciate your insightful suggestions. For the variational approximation, we correct the wrong symbol as $q(\\hat z_t|x_{t:t\u2212\u00b5})$ in Section 4. Besides, it is totally right that the conditional independence is hard-coded in our generative model by $\\hat \\epsilon_{it} = f_i^{-1}(\\hat z_{it},\\hat z_{t-1:t-\\tau})$.\n\nIn fact,   we propose to minimize the KL divergence between the posterior distribution  $q(\\hat z_t|x_{t:t-\\mu})$, and a prior distribution $p(\\hat z_t|\\hat z_{t-1:t-\\tau})$, for out conditional independence assumption. By hard-coding the prior distribution to be conditionally independent on $\\hat z_{t-1:t-\\tau}$, we expect the posterior to also be subject to the conditional independence assumption."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2088/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700321095172,
                "cdate": 1700321095172,
                "tmdate": 1700321095172,
                "mdate": 1700321095172,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tYHLgcvqiq",
                "forum": "5tSLtvkHCh",
                "replyto": "kzfH9XfZ0U",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2088/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2088/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ur5z Part 3"
                    },
                    "comment": {
                        "value": "> **W12:** The authors rightfully points out that one has to be careful when going from \u201cJacobian is a permutation-scaling matrix\u201d to \u201cthe mapping is a permutation composed with an element-wise transformation\u201d when the domain of the function is not simply $\\mathbb{R}^n$. However, [4] already made that point (see beginning of Section 3.1 and the discussion surrounding what they call \"local\" and \"global\" disentanglement).\n\n**A12:**  We highly appreciate the sharing of this paper. We had learned a lot from this. Besides, we want to express our sincere gratitude for your recognition of our contribution to this. Please kindly note that the publication date of NeurIPS23 and the submission date of ICLR24 are very close, leading us to miss this good reference. To make the contribution more clear, we have added the citation, included the discussion, and suggested the readers refer to this paper for details. Please kindly refer to the discussion under Lemma 1 in Section 3.2 in the revised version.\n\n> **W13:** Moreover, Example 6 from [4] presents a counterexample to Lemma 1, i.e. an example of function with a path-connected domain where the Jacobian is everywhere a permutation-scaling matrix, but the function is not \u201cdisentangled\u201d, in the sense that it cannot be written as a permutation composed with an element-wise rescaling. This implies that Lemma 1 has to be wrong.\n\n**A13:** After carefully looking into example 6 of [4], we respectfully believe that this example is not a counterexample to Lemma 1. It is because we assumed that the domain of $h$ is path-connected. In this example, $\\mathcal{Z}=\\mathcal{Z}^{(1)} \\cup \\mathcal{Z}^{(2)} \\subseteq \\mathbb{R}^2$ where $\\mathcal{Z}^{(1)} = ${ $\\mathbf{z} \\in  \\mathbb{R}^2 | z_2 \\leq 0 $ } and $\\mathcal{Z}^{(2)} = ${ $\\mathbf{z} \\in  \\mathbb{R}^2 | z_2 \\geq 1 $ }, is the disconnected domain.  Thus, it is not a counterexample to Lemma 1.\n\n> **W14:** I also spent some time reading the proof of Lemma 1 and it is unclear. For example, what is the \u201cn-dimensional axis except 0\u201d? You can also find weird terminologies which makes understanding the argument impossible. This makes me even more confident that Lemma 1 is wrong.\n\n**A14:** Many thanks for your effort to help us to improve readability. We replace the inaccurate terminologies and reorganize the proof to make it more clear. Sorry for the inconvenience caused by the unclear proof. And we do value your effort to enhance the readability of our paper.  Please refer to the proof of Lemma 1 in the revised manuscript on page 18, and kindly let us know if there is anything not clear in our proof."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2088/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700322547884,
                "cdate": 1700322547884,
                "tmdate": 1700440372996,
                "mdate": 1700440372996,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DizhcF4VAB",
                "forum": "5tSLtvkHCh",
                "replyto": "kzfH9XfZ0U",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2088/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2088/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ur5z Part 4"
                    },
                    "comment": {
                        "value": "> **W15:** Not sure I understand \u201cNon-invertibility by vision persistence\u201d from the intro. Why does the crashing car example have vision persistence? This was not explained, no?\n\n**A15:**  Thanks for your insightful question. Actually, all the objects are able to have the vision persistence. But, in the context of high-speed movement, this phenomenon is more obvious. Here, we use this example since there is a motion blur (one special example of vision persistence) caused by high speed movement of crashing car. We have included these explanations in the caption of Figure 1 revised paper. \n\n> **W16:** \u201cThus, we assume that there exists a maximum time lag $\\mu$ and an arbitrary nonlinear function m...\u201d The word \u201carbitrary\u201d shouldn\u2019t be there.\n\n**A16:** Many thanks for your helpful feedback. We remove \u201carbitrary\u201d in the paragraph above Eq.2 in Section 2.1 to make it more clear.\n\n> **W17:** \u201cIn this case, there is information loss in $x\\underline{ }t$ due to the non-invertibility of g.\u201d This is imprecise. What is meant by information here? I believe what you mean is that one cannot recover $z\\underline{ }t$ from $x\\underline{ }t$ alone.\n\n**A17:** Many thanks for your helpful feedback. We replace the original sentence with \"In this case, one cannot recover $\\mathbf{z}_t$ from $\\mathbf{x}_t$ alone due to the non-invertibility of $\\mathbf{g}$.\" in the paragraph above Eq.2 in Section 2.1 to improve the readability.\n\n> **W18:** \u201cWe say latent causal processes are identifiable if observational equivalence can lead to identifiability of the latent variables\u2026\u201d This phrasing is weird. They define identifiability, but use the word \u201cidentifiability\u201d in its definition. This should be rephrased.\n\n**A18:** Thank you for pointing out the issue of circular definition in our work. In light of your suggestion, we rephrase it as \"We say latent causal processes are identifiable if observational equivalence can lead to a version of latent variable $z_t = m(x_{t:t-\\mu})$ up to permutation $\\pi$ and component-wise invertible transformation $T$.\" \n\n> **W19:** \u201cDue to the complexity of the non-invertible mixing function, the identifiable representation does not indicate the inference function is identifiable.\u201d I don\u2019t understand this sentence.\n\n **A19:** Thank you for the feedback. Originally we want to say that since function $g,m$ are not invertible themselves, the identifiability of $g,m$ can not be guaranteed. Meanwhile, the latent variable $z_t$ is identifiable. Thank you for the effort to improve the quality of our writing. We revised the paragraphs at the end of Section 2.2 for better readability.\n\n> **W20:** \u201cwith a function m that satisfies our assumption zt = m(xt:t\u2212\u00b5) in existence\u201d Weird sentence formulation.\n\n **A20:** Thanks for the suggestion. We revise the sentence as \"where a function $\\mathbf{m}$ satisfying $z_t = m(x_{t:t-\\mu})$ exists.\" after Eq.4 in section 3.1.\n\n> **W21:** Figure 3 (c), what is the x-axis?\n\n**A21:** The x-axis in Figure 4 (original Figure 3) is the epochs$/(10^{-1})$\n\n> **W22:** Theorem 1 seems to reuse assumptions very similar to [2], which itself reuses assumptions similar to the line of Aapo Hyvarinen\u2019s group, see for example [3]. I think this resemblance should be highlighted in the text to help the reader understand what is truly novel in the proposed theoretical results. In general, I feel like the results could be contextualized in the literature a bit more.\n\n **A22:** Thank you for pointing out that it's important to emphasize the relationship between these works and highlight their inheritances. We highlight it in Section 3.1, after where Theorem 1 is introduced.\n\n> **W23:** [1] should be cited, as it was among the first work showing identifiability was possible in dynamical latent dynamical systems.\n\n**A23:** Thanks very much for recommending this paper. It is helpful to make the background clearer. We have included this in the introduction of the revised version.  \n\n> **W24(Q1):** Is the advantage of the proposed algorithm over the baselines on the QA benchmark due to disentanglement and better identifiability? Or is it due to architectural choices? I feel this should be addressed, since the paper is very much centered around disentanglement and identifiability.\n\n**A24:** Thanks for this insightful question. In real-world applications, we have no ground truth for computing metrics like MCC. To address this, we use 2 proxy metrics including the disentanglement and the reconstruction ability of the learned representations. In practice, we employ ELBO loss as the metric. We provide results in Appendix A4.2 and Figure A2. The results show that our methods exhibit superior performance in both disentanglement and reconstruction ability compared to baseline methods. It can serve as side evidence to support the advantage of better identifiability."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2088/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700323893815,
                "cdate": 1700323893815,
                "tmdate": 1700323893815,
                "mdate": 1700323893815,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kjKtIMzMdd",
                "forum": "5tSLtvkHCh",
                "replyto": "kzfH9XfZ0U",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2088/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2088/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Could you please let us know whether our responses and updated submission properly addressed your concern?"
                    },
                    "comment": {
                        "value": "Dear Reviewer ur5z,\n\nWe express our sincere gratitude for taking the time to review our manuscript. Your suggestions regarding the theoretical aspects and readability of our article have greatly contributed to improving its quality. We have made detailed revisions to the manuscript and addressed your questions in our response. We hope that our answers have addressed any concerns you had regarding our work. Your feedback is vital to us, and any response would be further appreciated.\n\nMany thanks,\n\nAuthors of submission 2088"
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2088/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700553798273,
                "cdate": 1700553798273,
                "tmdate": 1700553798273,
                "mdate": 1700553798273,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nQjegB1r7s",
                "forum": "5tSLtvkHCh",
                "replyto": "kjKtIMzMdd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2088/Reviewer_ur5z"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2088/Reviewer_ur5z"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for carefully considering my concerns. I apologize for the delayed answered, I thought we would have more discussion time, like last year.\n\nRegarding the explicit model satisfying the assumptions: This example is not transparent to me. What are the functions $g$, $f_i$ and $p_{\\epsilon_i}$ as introduced in (1) here? What is the corresponding $m$?\n\nDensity of x_t: Equation (15) in the new manuscript assumes a density w.r.t. z_t, x_t-1, .... . This is a bit unclear.\n\n\"Please kindly refer to the discussion under Lemma 1 in Section 3.2 in the revised version.\": I'm didn't find the discussion under Lemma 1, I think you meant above.\n\n\"After carefully looking into example 6 of [4], ...\" Oops, it looks like this paper has been updated on arxiv since I wrote my review such that the examples numbering changed. I was referring to Example 6 from this version: https://arxiv.org/abs/2307.02598v1. (in the recent version it's Example 7).\n\nThanks again for considering my concerns seriously. I won't change my score, but I want to reiterate that I very much love the direction proposed in this work. The assumption that the current latent can be inferred when using past observations makes a lot of sense to me. My issues have to do with the execution of the idea + technical details that are not properly treated. I sincerely believe that with some efforts, this work can get well above the acceptance threshold."
                    }
                },
                "number": 27,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2088/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700605892879,
                "cdate": 1700605892879,
                "tmdate": 1700605892879,
                "mdate": 1700605892879,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Z47Cu4SZfJ",
                "forum": "5tSLtvkHCh",
                "replyto": "CWI1XUuSae",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2088/Reviewer_ur5z"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2088/Reviewer_ur5z"
                ],
                "content": {
                    "comment": {
                        "value": "Q1: I think you have to specify f_i and epsilon in your example, otherwise it's incomplete. I'm still left unsure whether the theory is vacuous or not (to be fair, I don't think it is vacuous, but I think it's important to prove it)\n\nQ4: Copy-pasted from my previous message: \"I was referring to Example 6 from this version: https://arxiv.org/abs/2307.02598v1\". Looks like you are talking about the wrong example here.\n\nI'm happy to see that a very large fraction of the paper has been rewritten to integrate the comments of the reviewers. However, I believe this is much more revisions that is reasonable to review during a discussion phase like this one."
                    }
                },
                "number": 31,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2088/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700691332298,
                "cdate": 1700691332298,
                "tmdate": 1700691332298,
                "mdate": 1700691332298,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "2Pw81eNEpu",
            "forum": "5tSLtvkHCh",
            "replyto": "5tSLtvkHCh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2088/Reviewer_5cYn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2088/Reviewer_5cYn"
            ],
            "content": {
                "summary": {
                    "value": "The work studies an identifiability theory of recovering causal latent variables in a non-invertible generation process. The theoretical results show the causal latent variable is identifiable up to permutation and a component-wise transformation under certain conditions. Based on the theoretical study results, the work proposes, CaRiNG, which extends Sequential VAE with a normalizing flow in the latent transition dynamics and an encoder incorporating history information. CaRiNG demonstrates superior performance to baseline methods on synthetic  tasks aligning with the theoretical study. In a real-world experiment setting of understanding traffic dynamics, the proposed approach also demonstrates competitive performance against baseline approaches."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The propose approach is backed by solid theoretical study on the identifiability of latent causal variables; the theoretical study results are also well supported by experiment results under carefully designed synthetic settings."
                },
                "weaknesses": {
                    "value": "1. The proposed approach, CaRiNG, is not significantly different from the original Sequential VAE[1], especially from a probabilistic model perspective. There are also existing VAE works[2, 3] that incorporate normalizing flows. The novelty of CaRiNG as a new approach is rather limited.\n2. The presentation of the work needs improvements. The lack of explicit connections between theoretical study (Sec. 3) and model design (Sec. 4) makes the work less readable. In other words, I would suggest the reader to directly connects their model design choices in Sec. 4 to the conditions of their theoretical results in Sec. 3. Moreover, Sec. 3.3 and Sec. 3.4 are primarily supporting or supplementing the theoretical results in Sec. 3.1 and Sec. 3.2 but not critical to the identifiability theory's presentation or the proposal of CaRiNG. Their positioning in the work is distracting in my personal opinion and much of the detailed discussions in Sec. 3.3 and Sec. 3.4 can be moved to the appendix.\n3. The work repeatedly claims the guarantee of identifiability or guarantee of identifiability under mild conditions. However, their theoretical results also rely on the existing of a function $m$ such that $z_t = m(x_{t:t-\\mu})$. It is not clear if this existence condition can be trivially satisfied, especially in real-world settings, including the work's real-world experiment. Even if such a function exists, it is also not clear how to determine $\\mu$.  \n4. The work studies the proposed approach on only one real-world dataset and relies on QA accuracy as a proxy to indirectly evaluate the model's ability to understand the underlying causality. Even though it is challenging to evaluate the identifiability of causal latent dynamics, experiments on different real-world data and different proxy metrics could provide more convincing results.\n\n[1] Chung, Junyoung, et al. \"A recurrent latent variable model for sequential data.\" Advances in neural information processing systems 28 (2015).\n\n[2] Rezende, Danilo, and Shakir Mohamed. \"Variational inference with normalizing flows.\" International conference on machine learning. PMLR, 2015.\n\n[3] Ziegler, Zachary, and Alexander Rush. \"Latent normalizing flows for discrete sequences.\" International Conference on Machine Learning. PMLR, 2019."
                },
                "questions": {
                    "value": "Apart from the points in *Weaknesses*, I also have the following questions and suggestions:\n1. Sequential VAE can be viewed as a degenerate version of CaRiNG where prior distribution is another Gaussian with non-zero mean and diagonal variance. It is a valid baseline to compare against and the comparison could also help the work better demonstrate the importance of the proposed design changes of CaRiNG from Sequential VAE.\n2. The transition lag $\\tau$ is an important hyper-parameter of the proposed approach. The work includes ablation study results on the choice of $\\tau$ in controlled synthetic setting. It is actually more important to do hyper-parameter search over its values in real-world settings where we do not know the true underlying generative process to avoid model mis-specification."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2088/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2088/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2088/Reviewer_5cYn"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2088/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698737379039,
            "cdate": 1698737379039,
            "tmdate": 1700610823238,
            "mdate": 1700610823238,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CEdXZdJJGD",
                "forum": "5tSLtvkHCh",
                "replyto": "2Pw81eNEpu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2088/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2088/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 5cYn Part 1"
                    },
                    "comment": {
                        "value": "Dear Reviewer 5cYn, we are grateful for your valuable suggestions which helped improve the quality of our paper. In light of your suggestions, we revised our main paper and appendix to better highlight the contribution and explain the details. Below, we provide point-to-point responses.\n\n> **W1:** The proposed approach, CaRiNG, is not significantly different from the original Sequential VAE[1], especially from a probabilistic model perspective. There are also existing VAE works[2, 3] that incorporate normalizing flows. The novelty of CaRiNG as a new approach is rather limited.\n\n**A1:** Thanks for your comments which helped highlight our unique contribution. \n\n**Notice:** Due to the issue of markdown for complex equations, we use   $a\\underline{ }(b)$ to represent $a_b$.\n\n**Technical differences** \nCompared with the original Sequential VAE, we would like to highlight the following differences in our approach:\n1) We established the identifiability theory under the non-invertibility mixing functions, which complements the existing body of the ICA framework. \n2) We model the temporal dynamics of multivariate with a transition function. \n3) We added a prior module to encourage the 'conditional independence' of the learned latent variables. \nPlease kindly note that, our goal is to learn the causal representations with identifiability guarantees, which is different from the conventional VAE-based methods towards better generation ability. \nOur contributions lie in investigating what assumptions we can achieve this identifiability and how to do it in practice. \n\n**Experimental comparison** \nExperimentally, we also compared the identifiability of the learned variables between our methods and one of the advanced SequentialVAE methods, SKD[R1], using the synthetic data.  In light of your advice, we further add the comparisons with the original SequentialVAE method. As shown in Table 1 in the revised paper, our method shows significant improvement over SKD. Interestingly, we find that SequentialVAE method works better than other methods which don't use the temporal context, which also demonstrates the necessity of temporal context to solve the invertibility issue. However, we still find that constraining conditional independence benefits better performance.\n\n**Comparison with other VAE works[2, 3] that incorporate normalizing flows**  Though both our method and some existing works[2, 3] applied the normalizing flow, we highlight the motivations and other model details are different.\n1) We apply normalizing flows to help calculate the prior distribution and further constrain the conditional independence, which is not claimed as our unique contribution. Please refer to Section 4 in the revised version for details.  For existing works, they apply the normalizing flow to obtain the invertible deterministic transformation between two variables in the sequence. \n2) We applied a sequence-to-step encoder, which leverages the temporal context to recover the information lost caused by non-invertibility mixing functions. The existing works[2, 3] only focus on the current observations. \n\nWe have added these discussions in the revised Appendix A6.1 and A6.2. Please kindly let us if you have any further questions. \n\n[R1] Berman N, Naiman I, Azencot O. Multifactor sequential disentanglement via structured koopman autoencoders[J]. arXiv preprint arXiv:2303.17264, 2023."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2088/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700315529854,
                "cdate": 1700315529854,
                "tmdate": 1700315529854,
                "mdate": 1700315529854,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4QPIPVS7SE",
                "forum": "5tSLtvkHCh",
                "replyto": "2Pw81eNEpu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2088/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2088/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 5cYn Part 2"
                    },
                    "comment": {
                        "value": "> **W2:** The presentation of the work needs improvements. The lack of explicit connections between theoretical study (Sec. 3) and model design (Sec. 4) makes the work less readable. In other words, I would suggest the reader to directly connects their model design choices in Sec. 4 to the conditions of their theoretical results in Sec. 3. Moreover, Sec. 3.3 and Sec. 3.4 are primarily supporting or supplementing the theoretical results in Sec. 3.1 and Sec. 3.2 but not critical to the identifiability theory's presentation or the proposal of CaRiNG. Their positioning in the work is distracting in my personal opinion and much of the detailed discussions in Sec. 3.3 and Sec. 3.4 can be moved to the appendix.\n\n**A2:** We highly appreciate your efforts to improve the writing quality of this paper.  In light of your suggestions, we have modified our manuscript and appendix. We believe this version is much more readable than before. \n\nWe reorganized Section 4 and explicitly connected network components and the assumptions in our theorem. \nBesides, we have moved the original Sec. 3.4 into appendix. Instead, we provide some examples to elaborate our problem setting-ups in Section 2.3, which may help with the following questions. \nFor original Sec. 3.3 (now it is Sec. 3.2), we respectfully believe it fills the gaps in existing ICA literature and serves as an important lemma for proving our Theorem 1, which needs to be highlighted in the main submission. We also understand that these sections may not contribute significantly to understanding the algorithm details. Therefore, we have made efforts to streamline the content in the main body and moved some of the content to the appendix.\n\n> **W3:** The work repeatedly claims the guarantee of identifiability or guarantee of identifiability under mild conditions. However, their theoretical results also rely on the existing of a function $m$ such that $z\\underline{ }t=m(x\\underline{ }{(t:t-\\mu)})$. It is not clear if this existence condition can be trivially satisfied, especially in real-world settings, including the work's real-world experiment. Even if such a function exists, it is also not clear how to determine $\\mu$.\n\n**A3:** Thank you for your insightful comments. Please kindly note that compared to models that do not consider non-invertibility, our condition is relatively lenient.\nIt shows our unique contributions to complement the existing body of Nonlinear ICA with non-invertibility. Below, we provide some intuitive and mathematical illustrations to explain why this assumption makes sense and whether this existence condition can be trivially satisfied.\n\n\n**Further illustration with intuitive examples** \nConsider a rapidly moving ball on a two-dimensional plane as described in Figure 2. The horizontal and vertical coordinates of the ball's position at any given moment can be represented by the latent variable $\\mathbf{z}\\underline{ }t\\in \\mathbb{R}^2$. We assume that the ball follows a curved trajectory constrained by the nonlinear function $\\mathbf{f}$  as it moves.\n\nSuppose that we observe the ball with a visual persistence effect, where each observation $\\mathbf{x}\\underline{ }t$ captures several consecutive latent variables as \n$\\mathbf{x}\\underline{ }t = \\mathbf{g}(\\mathbf{z}_{<t})$.\nThe mixing function $\\mathbf{g}$ refers to the weighted sum of the images obtained through multiple exposures, which is what a person ultimately observes as $\\mathbf{x}\\underline{ }t$. In this case, the invertibility of the mapping from $\\mathbf{z}\\underline{ }{t}$ to $\\mathbf{x}\\underline{ }{t}$ is compromised since the current frame also contains the latent information from previous frames. \n\n**Further illustration with mathematical examples** \n Following the concept of visual persistence, let the observation be  $\\mathbf{x}\\underline{ }t = \\mathbf{z}\\underline{ }t + \\frac{1}{2}\\mathbf{x}\\underline{ }{(t-1)} = \\sum_{i=1}^{\\infty} \\left(\\frac{1}{2}\\right)^{i} \\mathbf{z}\\underline{ }{(t-i)}$.  Given an extra observation, the current latent variable can be rewritten as $\\mathbf{z}\\underline{ }t  = \\mathbf{x}\\underline{ }t - \\frac{1}{2}\\mathbf{x}\\underline{ }{(t-1)}$.\nThereby we can easily recover latent variables that cannot be obtained from a single observation, i.e., $\\mathbf{z}\\underline{ }t = \\mathbf{m}(\\mathbf{x}\\underline{ }{(t:t-1)}) = \\mathbf{x}\\underline{ }t - \\frac{1}{2}\\mathbf{x}\\underline{ }{(t-1)}$.\n\nWe have added Section 2.3 in the revised version for the above illustrations.\n\n**How to determine $\\mu$** \nIn practical applications, we can apply the model selection trick to choose an appropriate $\\mu$ in the model implementation. In experiments, we found that the $\\mu$ is not necessarily better when it's larger. As observation length increases, information recovery becomes less efficient, adding noise and impeding model training, though a larger $\\mu$ implies more context information recovery. \n\nWe give a simple example in Appendix A4.4."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2088/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700316448200,
                "cdate": 1700316448200,
                "tmdate": 1700316448200,
                "mdate": 1700316448200,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2HXSxfoylV",
                "forum": "5tSLtvkHCh",
                "replyto": "2Pw81eNEpu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2088/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2088/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 5cYn Part 3"
                    },
                    "comment": {
                        "value": "> **W4:** The work studies the proposed approach on only one real-world dataset and relies on QA accuracy as a proxy to indirectly evaluate the model's ability to understand the underlying causality. Even though it is challenging to evaluate the identifiability of causal latent dynamics, experiments on different real-world data and different proxy metrics could provide more convincing results.\n\n\n**A4:** Thank you very much for your insightful suggestions. In fact, one of the main motivations of our work is to bridge the gap between assumptions made in existing methods and the challenges present in real-world scenarios. We are committed to extending our methodology to real datasets and validating it on a broader range of real-world data. Currently, we are conducting experiments on the Volleyball dataset[R2]. However, due to time constraints and the required tasks such as code migration, we have not yet finished these experiments. We are making every effort to complete this experiment before the deadline.\n\n> **W5(Q1):** Sequential VAE can be viewed as a degenerate version of CaRiNG where prior distribution is another Gaussian with non-zero mean and diagonal variance. It is a valid baseline to compare against and the comparison could also help the work better demonstrate the importance of the proposed design changes of CaRiNG from Sequential VAE.\n\n\n**A5:** We are grateful for your valuable suggestions which made our comparison more complete.  Please refer to **Answer1 Experimental comparison** part for the response. We also include this experiment in Table 1 in the revised version.\n\n> **W6(Q2):** The transition lag $\\tau$ is an important hyper-parameter of the proposed approach. The work includes ablation study results on the choice of \n in controlled synthetic setting. It is actually more important to do hyper-parameter search over its values in real-world settings where we do not know the true underlying generative process to avoid model mis-specification.\n\n **A6:** In light of your suggestion, we provide the results of the ablation study of the real-world experiment (SUTD-TrafficQA dataset) on time lag $\\tau$ here. It's clear that the results are robust to the time lag $\\tau$ because all the ablation results share similar accuracies. We have included this result in Appendix A4.3 and Table A4.\n\n| $\\tau$   | 1     | 2     | 3     |\n| -------- | ----- | ----- | ----- |\n| Accuracy | 41.22 | 41.23 | 41.27 |\n\n[R2] Ibrahim M S, Muralidharan S, Deng Z, et al. A hierarchical deep temporal model for group activity recognition[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 1971-1980."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2088/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700316893508,
                "cdate": 1700316893508,
                "tmdate": 1700316893508,
                "mdate": 1700316893508,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NtfD0TODXs",
                "forum": "5tSLtvkHCh",
                "replyto": "2Pw81eNEpu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2088/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2088/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Could you please let us know whether our responses and updated submission properly addressed your concern?"
                    },
                    "comment": {
                        "value": "Dear Reviewer 5cYn,\n\nThank you for your valuable time dedicated to reviewing our submission and for your insightful suggestions. We've tried our best to address your concerns in the response and updated submission. \nDue to the limited rebuttal discussion period, we eagerly await any feedback you may have regarding these changes. If you have further comments, please kindly let us know--we hope for the opportunity to respond to them.\n\nMany thanks,\n\nAuthors of submission 2088"
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2088/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700553744177,
                "cdate": 1700553744177,
                "tmdate": 1700553744177,
                "mdate": 1700553744177,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gCCvHoYhP3",
                "forum": "5tSLtvkHCh",
                "replyto": "NtfD0TODXs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2088/Reviewer_5cYn"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2088/Reviewer_5cYn"
                ],
                "content": {
                    "title": {
                        "value": "Post-rebuttal Response"
                    },
                    "comment": {
                        "value": "I would like to thank the authors for their hard work and detailed response. The presentation of the work is much better after the significant revision. The distinction and unique contribution of the work is also much more clear from existing works. However, there are still two points I'm not satisfied with. The intuitive and mathematical examples doesn't tell us that if the assumption of in the invertibility theoretical results can be trivially satisfied [w(4)]. Moreover, the ablation study on the time lag $\\tau$ doesn't provide much insights as the results are similar. [w(7)]. It could be due to the assumptions are not satisfied or the evaluation metrics are not good enough and needs further explanation.. I appreciate the authors commitment to future study on more real-world datasets. Overall I'm satisfied with the response. With argument on both sides, I think this is a borderline paper with more room of improvements."
                    }
                },
                "number": 28,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2088/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700610778029,
                "cdate": 1700610778029,
                "tmdate": 1700610778029,
                "mdate": 1700610778029,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "D3g2BKLH3o",
                "forum": "5tSLtvkHCh",
                "replyto": "2Pw81eNEpu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2088/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2088/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Further explaination for abaltion study on real-world data"
                    },
                    "comment": {
                        "value": "Dear Reviewer 5cYn,\n\nWe managed to finish the ablation study on real-world data with different $\\mu$, which is consistent with our intuition. Here is the result, and we will update it in our paper ASAP.\n|$\\mu$|0|3|Full Length|\n|------|------|------|------|\n|acc(%)|38.58|40.67|41.22|\n\nMany thanks,\n\nAuthors of submission 2088"
                    }
                },
                "number": 35,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2088/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700726142070,
                "cdate": 1700726142070,
                "tmdate": 1700726789806,
                "mdate": 1700726789806,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qVXVXGnqOM",
            "forum": "5tSLtvkHCh",
            "replyto": "5tSLtvkHCh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2088/Reviewer_WQAA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2088/Reviewer_WQAA"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the problem of identifying the sources of a data-generating process (similar to ICA) where we assume a temporal scenario, and when the generator from the sources to the observation at a specific time is non-invertible. The authors then assume that the data-generating process is invertible, conditioned on sources from the past, and provide theoretical identifiability results up to permutations and component-wise transformations for three different scenarios. Then, a parametric approach based on variational auto-encoders and normalizing flows is proposed in order to learn the data-generating process, and put the test in synthetic and real-world experiments against previously-proposed approaches."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- The paper addresses an interesting variation of ICA with clear practical usage.\n- The motivation of the paper is quite appealing.\n- The theoretical results look quite impressive and of interest for the community (although I have not looked into the proofs in detail).\n- Empirical results look in principle quite positive and validate the proposed architecture."
                },
                "weaknesses": {
                    "value": "- The presentation of the paper leaves a lot to be desired. \n  - W1. I don't fully understand why this work takes such a confrontational stance with respect to the ICA community. From my perspective, and please correct me if I am wrong, everything the paper does is taking the same ICA framework as [1] (non-linear ICA with z independent given another random variable), and assume that the generation process is invertible _given that same random variable_ (in this case, the previous sources). This is quite commendable and interesting, and complements the existing body of work, rather than being obfuscated on \"non-invertibility\" (which is not completely true).\n  - W2. The manuscript does little effort in providing explanations and justifying certain statements (e.g. the entire paragraph before section 3).\n  - W3. Similarly, the mathematical notation is far from standard, convoluted, sometimes wrong, and unnecessarily unwelcoming. E.g.:\n    - In Eq. 3 $T \\circ \\pi \\circ m$ should be in parentheses.\n    - (I think that) the union symbol $\\cup$ is used in places where the Cartesian product is meant to be (e.g. the continuity condition).\n    - Conditions like those from Eq. 6 are overly convoluted for no reason as, e.g., $v_{k,t}$ being linearly independent could be much simplified by saying that the Hessian has non-zero determinant (i.e. is invertible).\n    - Jargon is non-consistent, e.g., secondary differentiable, second-order differentiable, and second order differentiable. Similarly, normalizing flows are then called normalized flows.\n  - W4. I also find section 4 a bit too convoluted to read, and it takes several reads to understand how exactly looks the network proposed by the authors. My advice would be to try to make more explicit the connections between each network component and the theory/data-generating process.\n\nAbout the experiments:\n- W5. I am surprised that there are no comparisons with iVAE, despite being cited.\n- W6. Number of parameters as well as training times for the real-world experiments seem necessary to me.\n- W7. While real-world results are ok, I find the discussion deceivingly positive, since CMCIR obtains better results on average and beats CaRiNG quite significantly in some individual question types.\n\n\n[1] Khemakhem, I., Kingma, D., Monti, R., & Hyvarinen, A. (2020, June). Variational autoencoders and nonlinear ICA: A unifying framework. In International Conference on Artificial Intelligence and Statistics (pp. 2207-2217). PMLR."
                },
                "questions": {
                    "value": "- Q1. I don't think I understand what is the column \"All\" in Table 2. Is it the mean of the other columns? Because if that is the case, these numbers do not add up."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2088/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2088/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2088/Reviewer_WQAA"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2088/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698865813353,
            "cdate": 1698865813353,
            "tmdate": 1700740477516,
            "mdate": 1700740477516,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KXrZpN19Ag",
                "forum": "5tSLtvkHCh",
                "replyto": "qVXVXGnqOM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2088/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2088/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer WQAA Part 1"
                    },
                    "comment": {
                        "value": "Dear Reviewer WQAA, we greatly appreciate your time dedicated to reviewing this paper and all your effort to improve its writing quality.\nWe have rewritten the manuscript and appendix to make it as clear as possible and provided the point-to-point responses.\n\n> **W1:** I don't fully understand why this work takes such a confrontational stance with respect to the ICA community. From my perspective, and please correct me if I am wrong, everything the paper does is taking the same ICA framework as [1] (non-linear ICA with z independent given another random variable), and assume that the generation process is invertible given that same random variable (in this case, the previous sources). This is quite commendable and interesting, and complements the existing body of work, rather than being obfuscated on \"non-invertibility\" (which is not completely true).\n\n**A1:** We highly appreciate your insightful suggestions which have helped clarify our paper's position and contribution. Besides, we are also grateful that you think our method is commendable and interesting.  In response to your comments, we have made significant modifications to the introduction of our revised paper, ensuring a more coherent and accurate representation of our work. Below are the key aspects of these modifications:\n\n**Clarifying non-confrontational stance** We apologize for the misleading presentation of the confrontational stance. Actually, our work is not intended to be confrontational towards the ICA community. Instead, our aim is to build upon and extend the existing body of work.\n\n**Affirmation of following the ICA framework.** We confirm that our paper is firmly rooted in the nonlinear ICA framework (thank you for pointing it out). Our contribution is in extending this framework to scenarios where temporal dynamics involve a non-invertible mixing function.  This extension implies that the current observation $x\\underline{ }t$ does not fully include all the information about the latent variable $z\\underline{ }t$. To make our contribution clear, we have enriched the introduction with a comprehensive overview of the current nonlinear ICA literature. This also serves to better highlight our method's unique contributions and its role in complementing and enriching the existing body of work.\n\nWe hope that these modifications adequately address your concerns and better articulate the intent and scope of our research."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2088/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700312682053,
                "cdate": 1700312682053,
                "tmdate": 1700312682053,
                "mdate": 1700312682053,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9FDNtT8OkE",
                "forum": "5tSLtvkHCh",
                "replyto": "qVXVXGnqOM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2088/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2088/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer WQAA Part 2"
                    },
                    "comment": {
                        "value": "> **W2:** The manuscript does little effort in providing explanations and justifying certain statements (e.g. the entire paragraph before section 3).\n\n**A2:** Thank you very much for your suggestions on improving the readability of our paper. We have revised the manuscript accordingly. Please kindly refer to our revised paper for the details. Please let us know if you have any concerns about the provided explanations and justification. \n\n**Notice:** Due to the issue of markdown for equations, we use $a\\underline{ }(b)$ to represent $a_b$.\n\n**Modification of Section 2.2** At the end of Section 2.2, we clarified the differences between our Definition 1 and the identifiability definition in existing literature. \nDifferent from the existing literature, we involve $\\mathbf{m}$ in the above definition, since it does so implicitly as a property of the mixing function $\\mathbf{g}$, although it does not explicitly participate in the generation process. Furthermore, the identifiability of $\\mathbf{g}$ is different. In previous nonlinear ICA methods, such as TDRL and iVAE, the mixing function $\\mathbf{g}$ is identifiable.\nHowever, in our case, we cannot find the identifiable mixing function since the information loss is caused by non-invertibility. Instead, we can obtain a component-wise transformation of a permuted version of latent variables $\\mathbf{\\hat{z}}\\underline{ }t = \\mathbf{m}(\\mathbf{x}\\underline{ }{t:t-\\mu})$.\nThe latent causal relations are also identifiable, up to a permutation $\\pi$ and component-wise invertible transformation $T$, i.e., $\\mathbf{\\hat{f}} = T \\circ \\pi \\circ \\mathbf{f}$, once $\\mathbf{z}\\underline{ }t$ is identifiable.\nIt is because, in the time-delayed causally sufficient system, the conditional independence relations fully characterize time-delayed causal relations when we assume no latent causal confounders in the (latent) causal processes.\n\n\n**Further illustration with intuitive examples** \nConsider a rapidly moving ball on a two-dimensional plane as described in Figure 2. The horizontal and vertical coordinates of the ball's position at any given moment can be represented by the latent variable $\\mathbf{z}\\underline{ }t\\in \\mathbb{R}^2$. We assume that the ball follows a curved trajectory constrained by the nonlinear function $\\mathbf{f}$  as it moves.\n\nSuppose that we observe the ball with a visual persistence effect, where each observation $\\mathbf{x}\\underline{ }t$ captures several consecutive latent variables as \n$\\mathbf{x}\\underline{ }t = \\mathbf{g}(\\mathbf{z}_{<t})$.\nThe mixing function $\\mathbf{g}$ refers to the weighted sum of the images obtained through multiple exposures, which is what a person ultimately observes as $\\mathbf{x}\\underline{ }t$. In this case, the invertibility of the mapping from $\\mathbf{z}\\underline{ }{t}$ to $\\mathbf{x}\\underline{ }{t}$ is compromised since the current frame also contains the latent information from previous frames. \n\n**Further illustration with mathematical examples** \nWe also provided a mathematical example to demonstrate the existence of function $\\mathbf{m}$. Following the concept of visual persistence, let the current observation be a weakened previous observation overlaid with the current image of the object, i.e., $\\mathbf{x}\\underline{ }t = \\mathbf{z}\\underline{ }t + \\frac{1}{2}\\mathbf{x}\\underline{ }{(t-1)} = \\sum_{i=1}^{\\infty} \\left(\\frac{1}{2}\\right)^{i} \\mathbf{z}\\underline{ }{(t-i)}$.  Given an extra observation, the current latent variable can be rewritten as $\\mathbf{z}\\underline{ }t  = \\mathbf{x}\\underline{ }t - \\frac{1}{2}\\mathbf{x}\\underline{ }{(t-1)}$.\nThereby we can easily recover latent variables that cannot be obtained from a single observation, i.e., $\\mathbf{z}\\underline{ }t = \\mathbf{m}(\\mathbf{x}\\underline{ }{(t:t-1)}) = \\mathbf{x}\\underline{ }t - \\frac{1}{2}\\mathbf{x}\\underline{ }{(t-1)}$."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2088/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700314007944,
                "cdate": 1700314007944,
                "tmdate": 1700314007944,
                "mdate": 1700314007944,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fDwqb9uX5t",
                "forum": "5tSLtvkHCh",
                "replyto": "qVXVXGnqOM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2088/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2088/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer WQAA Part 3"
                    },
                    "comment": {
                        "value": "> **W3:** Similarly, the mathematical notation is far from standard, convoluted, sometimes wrong, and unnecessarily unwelcoming. E.g.:\n\n>1. In Eq. 3 $T\\circ\\pi\\circ m$ should be in parentheses; \n\n>2. (I think that) the union symbol is used in places where the Cartesian product is meant to be (e.g. the continuity condition) \n\n>3. conditions like those from Eq. 6 are overly convoluted for no reason as, e.g., being linearly independent could be much simplified by saying that the Hessian has non-zero determinant (i.e. is invertible).\n\n>4. Jargon is non-consistent, e.g., secondary differentiable, second-order differentiable, and second order differentiable. Similarly, normalizing flows are then called normalized flows.\n\n**A3:** We sincerely appreciate for pointing out the mathematical notation errors and reader-unfriendly writing in our article. As a result, we have made the following modifications:\n- we put $T\\circ\\pi\\circ m$ in parentheses in Eq. 3;\n- we rewrite Lemma 2 and change the domain of $h$ to $\\mathcal{\\hat{Z}}\\times\\mathcal{C}$, which refers to the Cartesian product;\n- we add an explanation of the sufficiency assumption before the proof of Theorem 1, providing its matrix form in Eqs.12,13 and meaning to make it easier to understand. Please kindly note that $\\mathbb{V}\\underline{ }t$ and $\\mathbb{\\mathring{V}}\\underline{ }t$ are not Hessian matrices, but rather mixed partial derivatives of order 2 and 3;\n- we review the consistency and accuracy of the terminology throughout the entire document and have made consistent changes to \"second-order\" and \"normalizing flow\".\n\nAfter all, thank you for your efforts in helping us improve the quality of our presentation. \n\n> **W4:** I also find section 4 a bit too convoluted to read, and it takes several reads to understand how exactly looks the network proposed by the authors. My advice would be to try to make more explicit the connections between each network component and the theory/data-generating process.\n\n\n**A4:** We are grateful for your valuable advice which helped make the implementation clearer. In light of your advice, we reorganized Section 4 and highlighted the connections between network components and the assumptions in our theorem. Please kindly refer to the revised paper and let us know if you have any further suggestions.\n\n> **W5:** I am surprised that there are no comparisons with iVAE, despite being cited.\n\n**A5:** Thanks for pointing it out.  \nThe reason why we did not compare with iVAE is that, theoretically, iVAE requires additional domain information to achieve identifiability, which is not valid in our setting with stationary transitions and no domain information. We apply this stationary setting for the comparison since it is somehow more difficult than the non-stationary one, since no extra domain information is used.  Given these considerations, we only compare methods that work in stationary scenarios. \n\nWe agree that iVAE is an important work in the literature of ICA, and it is valuable to compare with it. We have included the comparative results in Table 1 in our revised paper, made modifications to Figure 4 (originally Figure 3), and provided relevant explanations in the \"Baseline Methods\" paragraph of Section 5.1.\nPlease note that the input of iVAE requires a domain label. We simply use the time index as the domain label.\n\n> **W6:** Number of parameters as well as training times for the real-world experiments seem necessary to me.\n\n**A6:** We kindly appreciate your suggestions and we provide these details in the following table. Compared with HCRN, we had a similar number of parameters, and needed double the training time.  It is because we apply the normalizing flow to calculate the Jacobian matrix, which introduces the extra computation cost. Please kindly note that, despite more training time, the inference efficiency is totally the same, since the normalizing flow is not used for inference (only for calculating KL loss).  We have included this analysis in Appendix A4.1 and Table A3.\n\n| Method                  | HCRN     | CaRiNG   |\n| ----------------------- | -------- | -------- |\n| Number of Parameters    | 42278786 | 43721954 |\n| Training Time per Epoch | 6min54s  | 13min26s |\n| Inference Time per Epoch| 49s/epoch| 49s/epoch|"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2088/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700314377741,
                "cdate": 1700314377741,
                "tmdate": 1700314377741,
                "mdate": 1700314377741,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zaKwMdwZE6",
                "forum": "5tSLtvkHCh",
                "replyto": "qVXVXGnqOM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2088/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2088/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer WQAA Part 4"
                    },
                    "comment": {
                        "value": "> **W7:**  While real-world results are ok, I find the discussion deceivingly positive, since CMCIR obtains better results on average and beats CaRiNG quite significantly in some individual question types.\n\n**A7:** Thanks for your questions, which help us clarify the different settings with CMCIR. \n\nWe would like to highlight that the experimental setting of CMCIR is a little different from the other baselines.  Specifically, all the other baselines and our method apply a pre-trained ResNet-101 model [R1] as the fine-grained frame feature, and a pre-trained MobileNetv2 [R2] is used as the lightweight CNN to extract.  As mentioned in  CMCIR, \n\"The Swin-L [R3] pre-trained on the ImageNet-22K dataset is used to extract the frame-level appearance features, and the video SwinB [R4] pre-trained on Kinetics-600 is applied to extract the clip-level motion features\". Please note that Swin-L and SwinB are much stronger backbone than ResNet we used.\n\n\n\n> **W8(Q1):** I don't think I understand what is the column \"All\" in Table 2. Is it the mean of the other columns? Because if that is the case, these numbers do not add up.\n\n**A8:** Thanks a lot for your question which enhanced our readability. The column ''ALL'' denotes the weighted average of single-item scores, with the weight being the proportion of this type of question in the test set. We have added an explanation in the caption of Table 2 accordingly.\n\n[R1] He K, Zhang X, Ren S, et al. Deep residual learning for image recognition[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 770-778.\n\n[R2] Sandler M, Howard A, Zhu M, et al. Mobilenetv2: Inverted residuals and linear bottlenecks[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2018: 4510-4520.\n\n[R3] Liu Z, Lin Y, Cao Y, et al. Swin transformer: Hierarchical vision transformer using shifted windows[C]//Proceedings of the IEEE/CVF international conference on computer vision. 2021: 10012-10022.\n\n[R4] Liu Z, Ning J, Cao Y, et al. Video swin transformer[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022: 3202-3211."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2088/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700314858291,
                "cdate": 1700314858291,
                "tmdate": 1700314858291,
                "mdate": 1700314858291,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6p5b1z5UfT",
                "forum": "5tSLtvkHCh",
                "replyto": "zaKwMdwZE6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2088/Reviewer_WQAA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2088/Reviewer_WQAA"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors,\n\nThanks a lot for the extensive changes to the manuscript. I believe that it does indeed improve the readability of the manuscript quite significantly, and the additional examples help gain some intuition on the problem at hand. \n\nI have an additional question: When you say that \"$\\mathbb{V}_t$ is not a Hessian matrix, but rather mixed partial derivatives of order 2;\" Am I getting it wrong, or isn't that precisely a flattened version of the Hessian matrix wrt z, with extra zeroes at the end?\n\nJust as a sidenote, I will decide if I update my score after the end of the rebuttal period."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2088/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700476269662,
                "cdate": 1700476269662,
                "tmdate": 1700476269662,
                "mdate": 1700476269662,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tTR7olE351",
                "forum": "5tSLtvkHCh",
                "replyto": "kHEysfidCK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2088/Reviewer_WQAA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2088/Reviewer_WQAA"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors,\n\nYes, they did clarify it. For me, what you wrote as $\\mathbb{V}_t$ I also call it a Hessian (of $\\eta$ wrt. $z$), but now I understand where was the misunderstanding coming from.\n\nThanks again for the detailed responses!"
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2088/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700599671589,
                "cdate": 1700599671589,
                "tmdate": 1700599671589,
                "mdate": 1700599671589,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EL5ggeoHqK",
                "forum": "5tSLtvkHCh",
                "replyto": "2O3n0f5Mek",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2088/Reviewer_WQAA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2088/Reviewer_WQAA"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors, I have updated my score to reflect my view after the rebuttal period. Thanks again for the fruitful discussion!"
                    }
                },
                "number": 36,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2088/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740528956,
                "cdate": 1700740528956,
                "tmdate": 1700740528956,
                "mdate": 1700740528956,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]