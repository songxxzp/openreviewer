[
    {
        "title": "EXPLORING RAIN-/DETAIL-AWARE REPRESENTATION FOR INSTANCE-SPECIFIC IMAGE DE-RAINING"
    },
    {
        "review": {
            "id": "fC4ZnwgtWd",
            "forum": "pdJXYfJjz9",
            "replyto": "pdJXYfJjz9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7082/Reviewer_rX1q"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7082/Reviewer_rX1q"
            ],
            "content": {
                "summary": {
                    "value": "Summary.\n\nThis paper proposes the CoIC algorithm to train CNN and Transformer based models to be instance-specific, using mixed datasets, for single image rain removal. The CoIC algorithm includes a Context-based Instance-specific Modulation (CoI-M) mechanism and a rain-/detail-aware contrastive learning strategy."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Strengths.\n\nThe paper is generally easy to read and follow.\nThe proposed approach seems to work well for CNN based models.\nExperiments (with tables and figures) are provided to analyze the proposed method.\nThe proposed method seems to work well on the RealInt (Wang et al., 2019), in terms of NIQE.\nThe idea of generating negative examples for both rain and background and modulating the image-specific information into networks sounds interesting."
                },
                "weaknesses": {
                    "value": "Weaknesses/Questions.\n\n\nThe Introduction mentions that there is a long-tail distribution across mixed datasets. As most (if not all) datasets are synthetic, would it be easier/better to synthesize more rain images to handle such an unbalanced distribution? Another question is whether training using a mixed dataset improves the individual performance of one model on each dataset. These answers are not easily found in this paper. \n\nFigure 1(b) shows one observation of this paper that the image is gradually dominated by rain when rain density increases, which, however, sounds superficial. The resulting (from this observation) learning of a joint embedding space for both rain and detail information is based on contrastive learning, while it is not explained why using contrastive learning here as it seems that rain density is the most important factor. Meanwhile, the paper simply says it is distinct from previous instance-discriminative contrastive learning (He et al., 2020) without explanation.\n\nThe Introduction mentions that the embedding space of this paper is not instance-level discriminative, which is hard to understand as the paper aims to learn instance-specific deraining features (If not discriminative enough, how can one model learn the specific information?) The second point it mentions is that the background content is not discriminative, which I do not know whether it is correct, does not explain why it is not suitable for a standard contrastive learning strategy. After I read the method section, it seems to me that the main difference is to construct negative examples based on rain types (selected from another dataset) and Gaussian blurred background. The authors may correct me if I misunderstood but it seems to me that the discussion in the introduction does not match what they do in the method.\n\nThe paper uses Gaussian blur to construct negative exemplars of the background image, which needs further justification as it may not represent the degraded backgrounds. Usually when rain streaks are large or dense, there are more occlusions than blur, which the Gaussian blur may not model.\n\nThe related work section should be expanded. 1. There are certainly several rain removal methods not cited, especially for heavy rain restoration, although they do not combine multiple datasets for training. 2. The modulation is one of the key techniques in the proposed method but there is no literature review on it.\n\nIn Table 1, the performance gain over DRSformer (Chen et al., 2023) is rather limited. The paper needs to show more advantages, otherwise, it seems not worth doing considering the efforts and the performance gains.\n\nThe visual results are not impressive, as we may easily see the remained rain streaks in, e.g., Figure 4, Figure 12. I understand that there are improvements by comparing the results of methods with and without CoIC. However, it seems to me that these visual results are still failure cases (due to obvious rain streaks, in Figure 12, (g) is comparable to or even better than (h)). Combining it with Table 1, really makes others wonder about the necessity of using the proposed approach."
                },
                "questions": {
                    "value": "Please see above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7082/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697531309650,
            "cdate": 1697531309650,
            "tmdate": 1699636835322,
            "mdate": 1699636835322,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Eq5ne9EniW",
                "forum": "pdJXYfJjz9",
                "replyto": "fC4ZnwgtWd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7082/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7082/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer rX1q (Part 1/4)"
                    },
                    "comment": {
                        "value": "**Comment**: We appreciate the effort and valuable questions from the reviewer. We are also glad that the reviewer finds it easy to read and follow our work. Below is our point-to-point response:\n\n> The Introduction mentions that there is a long-tail distribution across mixed datasets. As most (if not all) datasets are synthetic, would it be easier/better to synthesize more rain images to handle such an unbalanced distribution? Another question is whether training using a mixed dataset improves the individual performance of one model on each dataset. These answers are not easily found in this paper.\n\n**As it may be helpful to synthesize more rain images to handle the unbalanced distribution, it is important to consider the potential large domain gap between synthetic rainy images and real-world rainy images.** A key principle in synthesizing rain images is to align the distribution between synthetic and real-world rainy images, especially for real-life applications. Intuitively, real-world rainy images may follow a long-tailed distribution (extremely heavy rainy images are rare). In fact, many researchers are dedicated to synthesizing **realistic images** to enhance deraining ability [1, 2], while paying little attention to balancing the distribution. More importantly, as highlighted by the latest research [2], synthesizing heavy rainy images may also result in more artifacts, which can harm performance. **Therefore, we believe it could be more valuable to synthesize more realistic images to train models.**\n\nAs for the second question, large CNN and Transformer models can easily overfit to an individual dataset, as also noted by the reviewer nGEY. Typically, synthetic training and testing datasets possess a similar rain type. Therefore, it is easy for a model trained on an individual dataset to achieve extremely high PSNR/SSIM metrics on the corresponding testing set. However, **due to heavy overfitting, these models may struggle to handle rainy images from another dataset, thus limiting their applications**. Our analysis in Table 4 verifies that both DGUNet and IDT, when trained on an individual dataset, exhibit the worst performances on RealInt. In this paper, our goal is to learn a comprehensive deraining model that excels at deraining diverse rainy images. Also, we anticipate that a model trained on a mixed dataset could demonstrate better performance than one trained on an individual dataset.\n\n[1]. Hong Wang et al. \"From rain generation to rain removal.\" in CVPR 2021.\n\n[2]. Shen Zheng et al. \"TPSeNCE: Towards Artifact-Free Realistic Rain Generation for Deraining and Object Detection in Rain.\" in WACV 2023."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7082/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700441425597,
                "cdate": 1700441425597,
                "tmdate": 1700441425597,
                "mdate": 1700441425597,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "JypIx7ZrgT",
            "forum": "pdJXYfJjz9",
            "replyto": "pdJXYfJjz9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7082/Reviewer_iZXv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7082/Reviewer_iZXv"
            ],
            "content": {
                "summary": {
                    "value": "Existing de-raining methods tend to overlook the inherent differences between datasets. To address this limitation, this paper develops a rain-/detail-aware contrastive learning strategy to extract a representation, and proposes a Context-based Instance-specific Modulation mechanism, which uses the representation to modulate models. This approach helps existing methods boost the de-raining ability."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper focuses on training de-raining models on amalgamated datasets.\n2. The experiments are sufficient and the results are better than others.\n3. Visualization results on RealInt demonstrate the generalization capability of this method on real scenes."
                },
                "weaknesses": {
                    "value": "1. The main concern is the necessity of the proposed method. In fact, when the deraining model is strong enough, it may be able to learn the differences between datasets. For example, in Table 1, the proposed method has limited performance improvement on DRSformer and DGUNet.\n2. The novelty of the method may be limited. The method of representation extraction based on contrastive learning has been widely explored in the blind super-resolution field [1]. The paper needs to further explain the differences with them.\n[1] Unsupervised Degradation Representation Learning for Blind Super-Resolution. CVPR 2021.\n3. It would be better to show the increase in the number of parameters and changes in inference time brought by the proposed method."
                },
                "questions": {
                    "value": "Please see 'Weaknesses'."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7082/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7082/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7082/Reviewer_iZXv"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7082/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698741129568,
            "cdate": 1698741129568,
            "tmdate": 1699636835175,
            "mdate": 1699636835175,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HqUWdKli0C",
                "forum": "pdJXYfJjz9",
                "replyto": "JypIx7ZrgT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7082/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7082/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer iZXv (Part 1/2)"
                    },
                    "comment": {
                        "value": "Comments: We thank the reviewer for the insightful questions, and we hope to address the concerns with our response below.\n\n> Limited performance improvement in Table 1\n\nThe performance improvement in Table 1 for DGUNet and DRSformer seems marginal, primarily because these models may be capable enough to process synthetic datasets. For instance, DRSformer contains a Mixture of Experts (MoEs) that enables it to tolerate different rains. **However, the potential of the proposed CoIC may be underestimated only based on the results from synthetic datasets.** As suggested by other reviewers, we have added a real-world dataset SPAData [1] to fine-tune the pre-trained DRSformer for another 105k iterations. Below are the quantitative results in terms of PSNR (Details are provided in Table 2):\n\n| Methods            | Rain200L  | Rain200H  | Rain800   | DID-Data  | DDN-Data  | SPAData   |\n| ------------------ | --------- | --------- | --------- | --------- | --------- | --------- |\n| DRSformer w/o CoIC | 39.32     | 29.27     | 28.85     | 34.91     | 33.71     | 45.46     |\n| DRSformer w/ CoIC  | **39.70** | **30.31** | **29.73** | **35.02** | **33.94** | **46.03** |\n\nThe results indicate that even with MoEs, DRSformer directly tuned on SPAData suffers heavy performance drops on Rain200L, Rain200H, and Rain800 datasets. However, **with the help of CoIC, DRSformer could achieve improvements of 1.04dB, 0.88dB, and 0.57dB on Rain200H, Rain800, and the real-world SPAData datasets.** This observation reveals that we could train a comprehensive model targeted at deraining both synthetic and real-world data using CoIC. Additionally, we also obtained a remarkable improvement in deraining ability on RealInt, displayed in Figure 4 and Figure 14. For your convenience, please review the results on our anonymous GitHub pages: https://anonymous.4open.science/r/CoIC-730F/\n\n> Novelty of the method may be limited. The paper needs to further explain the differences with recent contrastive learning-based methods\n\nPlease note that our observation in Figure 1 reveals that learning joint rain-/detail-aware representations may help models learn better on mixed datasets. **This requires learning an encoder capable of discriminating different rain types, as well as perceiving background details. However, rain and background details are coupled in the rainy image, and it is challenging to find a suitable criterion to discriminate rain streaks from mixed datasets**. The contrastive learning method [2] mentioned by the reviewer attempts to learn the **degradation factor** from the degraded image, **where the background details are overlooked.** Moreover, [2] basically assumes that the degradations from two different images are different. This assumption may be inapplicable to mixed rainy datasets, where two images may share the same background (Rain200L and Rain200H) or show a similar rain pattern. Therefore, [2] cannot be applied to extract joint rain-/detail-aware representations from rainy input. In contrast, **we solve this problem by designing negative exemplars that contain different rain patterns and different background details.** To circumvent the issue that there is no well-defined criterion for defining \"different\" rains, we introduce a *rain layer bank* and regard the *most dissimilar rain* from the rain layer bank as different from the current rain. To clearly elaborate on the differences between the proposed contrastive learning strategy and existing strategies, we have provided an in-depth discussion in the section of *Introduction*, which better highlights our contributions.\n\n[1]. Tianyu Wang et al. \"Spatial attentive single-image deraining with a high quality real rain dataset.\" in CVPR 2019.\n\n[2]. Longguang Wang, et al. \"Unsupervised degradation representation learning for blind super-resolution.\" in CVPR 2021."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7082/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700440302741,
                "cdate": 1700440302741,
                "tmdate": 1700440302741,
                "mdate": 1700440302741,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CJiBq3XddP",
                "forum": "pdJXYfJjz9",
                "replyto": "HRm1WtzgZs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7082/Reviewer_iZXv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7082/Reviewer_iZXv"
                ],
                "content": {
                    "title": {
                        "value": "Thanks"
                    },
                    "comment": {
                        "value": "Many Thanks. Fine-tuning a pre-trained DRSformer using SPAData may be not fair. This is my main concern. It may be more fair to train two models (w/o CoIC and w/ CoIC) from scratch on all the datasets."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7082/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700683459874,
                "cdate": 1700683459874,
                "tmdate": 1700683459874,
                "mdate": 1700683459874,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ezdYdW519x",
                "forum": "pdJXYfJjz9",
                "replyto": "1PPuIu7IDH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7082/Reviewer_iZXv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7082/Reviewer_iZXv"
                ],
                "content": {
                    "title": {
                        "value": "Thanks"
                    },
                    "comment": {
                        "value": "Many Thanks. I mainly argue it would be more reasonable and appropriate to train two models (w/o CoIC and w/ CoIC) from scratch on all the datasets. I will carefully make the final decision based on the opinions of other reviewers."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7082/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700738972630,
                "cdate": 1700738972630,
                "tmdate": 1700738972630,
                "mdate": 1700738972630,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "INisDd7Nwl",
            "forum": "pdJXYfJjz9",
            "replyto": "pdJXYfJjz9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7082/Reviewer_v7hb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7082/Reviewer_v7hb"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a context-based instance-specific modulation (CoI-M) method for learning adaptive image de-raining models with mixed datasets. The goal is to exploit the commonalities and discrepancies among datasets for training. This mechanism can efficiently modulates both CNN and Transformer architectures. CoI-M is also verified to improve the performances of existing models when training on mixed datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ As to the paper structure, I think it is clear and easy to follow. \n\n+ The authors claims that images with light rain are primarily characterized by background detail, while heavy rainy images are distinguished more by the rain itself. The statistical analysis in Figure 1 is suitable.\n\n+ The experimental results indicate that the proposed method can further improve performance."
                },
                "weaknesses": {
                    "value": "- It is not clear how to select the positive and negative exemplars. The author should discuss the differences with relevant contrastive learning-based deraining methods [1-2].\n[1] Chen et al, Unpaired deep image deraining using dual contrastive learning, CVPR2022.\n[2] Ye et al, Unsupervised deraining: Where contrastive learning meets self-similarity, CVPR2022\n\n- I suggest the author conduct further validation on the mixed dataset Rain13K.\n\n- The figures included in the manuscript are rendered with small font sizes and low resolution, which makes them challenging for reviewers to examine and comprehend. I recommend revising the figures to meet these requirements to improve the overall quality and accessibility of the visuals in the manuscript.\n\n-----------------------After Rebuttal---------------------------\n\nThank you for your feedback. The rebuttal addressed my concerns well. I have decided to increase my score."
                },
                "questions": {
                    "value": "See the above Weaknesses part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7082/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7082/Reviewer_v7hb",
                        "ICLR.cc/2024/Conference/Submission7082/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7082/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699201141901,
            "cdate": 1699201141901,
            "tmdate": 1700746582538,
            "mdate": 1700746582538,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UuqWZb4eKX",
                "forum": "pdJXYfJjz9",
                "replyto": "INisDd7Nwl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7082/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7082/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer v7hb"
                    },
                    "comment": {
                        "value": "**Comments**: Many thanks for your feedback and constructive suggestions. We are glad that the reviewer find out paper easy to follow and our statistical analysis is suitable. We will carefully address you issues below: \n\n> It is not clear how to select the positive and negative exemplars. Differences between the proposed method and relevant constrastive learning-based deraining methods should be detailed.\n\nWe apologize for not clearly clarifying the differences between the proposed contrastive learning and existing contrastive learning methods. Our findings in Figure 1 suggest that learning joint rain-/detail-aware representations is meaningful for training models on mixed multiple datasets. This implies the need to train an encoder capable of discriminating different rains as well as background details. Hence, **the negative exemplars should contain exemplars with different rains and exemplars with different background details**. However, the recent method DCD-GAN [1] only considers the differences between rain and a clean background, which cannot perceive different rain types as well as background details. Additionally, the unsupervised method NLCL [2] also considers the differences between rain and a clean background. Although [2] introduces location contrastive learning to constrain the restored background to be the same as the ground truth, it cannot extract detail-aware information from the rainy input. Hence, [2] cannot be used to provide joint rain-/detail-aware guidance for adaptive image deraining. To clearly elaborate on the differences between our CoIC and recent contrastive learning methods, **we have added an in-depth discussion in the section of *Introduction***.\n\nWe also trained the unsupervised method DCD-GAN on mixed synthetic datasets for more than 1M iterations. However, it failed to process intricate rains from multiple datasets. Below, we present its PSNR metrics (Details can be found in Table 1):\n\n| Methods          | Rain200L  | Rain200H  | Rain800   | DID-Data  | DDN-Data  |\n| ---------------- | --------- | --------- | --------- | --------- | --------- |\n| DCD-GAN          | 21.64     | 16.04     | 19.52     | 21.28     | 21.60     |\n| DRSformer + CoIC | **39.81** | **30.50** | **29.92** | **35.01** | **33.94** |\n\nIn comparison to the proposed CoIC, DCD-GAN struggles on all synthetic benchmark datasets.\n\n> I suggest the author conduct further validation on the mixed dataset Rain13K.\n\nThank you for this constructive suggestion. Since marginal improvement of CoIC on DRSformer has been observed in Table 1, it may be helpful to train it on another mixed dataset to validate its efficacy. However, as pointed out by reviewer nGEY, the Rain13K dataset could be regarded as one entity. Therefore, to further explore the potential of CoIC, **we have added a real-world dataset SPAData [3] to fine-tune DRSformer**. Surprisingly, **we observe that the tuned model using CoIC can achieve much better performance on both synthetic and real-world SPAData datasets**. We present the quantitative results in terms of PSNR below (Details are provided in Table 2):\n\n| Methods            | Rain200L  | Rain200H  | Rain800   | DID-Data  | DDN-Data  | SPAData   |\n| ------------------ | --------- | --------- | --------- | --------- | --------- | --------- |\n| DRSformer w/o CoIC | 39.32     | 29.27     | 28.85     | 34.91     | 33.71     | 45.46     |\n| DRSformer w/ CoIC  | **39.70** | **30.31** | **29.73** | **35.02** | **33.94** | **46.03** |\n\nMoreover, **the fine-tuned DRSformer with CoIC can efficiently remove complex real rain streaks in RealInt**. We have showcased high-quality real-rain removal results in Figure 4 and Figure 14. For your convenience, please review them on our anonymous GitHub page: https://anonymous.4open.science/r/CoIC-730F/.\n\n> The figures included in the manuscript are rendered with small font size and low resolution, which makes them challenging for reviewers to examine and comprehend. I recommend revising the figures to meet these requirements to improve the overall quality and accessibility of the visuals in the manuscript.\n\nWe apologize for any inconvenience caused by the small font size in the figures, which may have posed readability issues. In the revised manuscript, we have adjusted the figures and increased the font size to enhance the reading experience.\n\n[1]. Xiang Chen, et al. \"Unpaired deep image deraining using dual contrastive learning.\" In CVPR 2022.\n\n[2]. Yuntong Ye, et al. \"Unsupervised deraining: Where contrastive learning meets self-similarity.\" in CVPR 2022.\n\n[3]. Tianyu Wang et al. \"Spatial attentive single-image deraining with a high quality real rain dataset.\" in CVPR 2019."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7082/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700439494613,
                "cdate": 1700439494613,
                "tmdate": 1700439494613,
                "mdate": 1700439494613,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mdSflDdsWQ",
            "forum": "pdJXYfJjz9",
            "replyto": "pdJXYfJjz9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7082/Reviewer_zqPW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7082/Reviewer_zqPW"
            ],
            "content": {
                "summary": {
                    "value": "The paper focuses on tackling the single image deraining problem, wherein the authors introduce instance-specific de-raining models. These models are designed to delve into meaningful representations that capture the distinct characteristics of both the rainy elements and the background components in images affected by rain. Authors propose Context-based Instance-specific Modulation (CoI-M) mechanism which can modulate CNN- or Transformer-based to learn the representations specific to rain details and background. Authors also propose  rain-/detail-aware contrastive learning strategy to help extract joint rain-/detail-aware instance-specific representations. Integrating these modules authors claim that the proposed method can handle multiple different rain datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- proposed Context-based Instance-specific Modulation (CoI-M) mechanism which can modulate CNN- or Transformer-based to learn the representations specific to rain details and background\n     - employed feature extractor E,  and a Global Average Pooling (GAP) operation to capture rich spatial and channel information related to rain and image details.\n- proposed contrastive learning strategy to help extract joint rain-/detail-aware instance-specific representations\n     - to make the encoder discriminate the rain in (x,y) pair of rainy images they use leverage a rain layer bank noted a D_{R}\n     - proposed contrastive learning based loss to learn the difference in representations of different rains and also image representations.\n- Proposed Context-based Instance-aware Modulation (CoI-M) mechanism to modulate features at different layers in CNN, and attention layers in transformer based networks, to learn the embedding space spanned by all instance-specific representations."
                },
                "weaknesses": {
                    "value": "- the comparisons are inadequate. There is rich literature in semi-supervised and continual learning that focuses on representation learning for different types of rain and image representations. Authors failed to compare with the existing methods.\n    -  Memory Oriented Transfer Learning for Semi-Supervised Image Deraining, CVPR 2021.\n        proposes a representation learning based approach where they learn basis vectors (which called memory) to represent the rain and adapt them using them for different datasets to minimize the differences the datasets. The proposed Col-C is also doing similar to this, so it will be easy for the reader to understand the benefits of CoI-C if authors can compare with this method.\n    - Syn2Real Transfer Learning for Image Deraining using Gaussian Processes, CVPR 2020.\n        proposes a Gaussian process based pesudo labeling approach where enable the encoder to learn representation rain and image using generated pesudo label. Note the here in Syn2Real they formulate the joint Gaussian distribution to generated a pesudo label for the unknown or unlabeled image, in way they selecting the k-nearest labeled images and maximizing correlation between unlabeled and labeled  similar type of rain images. Also minimizing correlation between k-nearest farthest labeled images and  unlabeled and labeled  similar type of rain images. Thus Syn2Real approach can be compared to proposed contrastive loss and CoI-C approach, to understand benefits of CoI-C.\n     - Unpaired Deep Image Deraining Using Dual Contrastive Learning, CVPR 2022.\n       proposed a contrastive based approach to learn the representation of rain and image and guide the networks to learn removal network and image generator networks in an unsupervised cyce-GAN approach. Thus, it would be great to see comparisons of CoI-C against this method."
                },
                "questions": {
                    "value": "Please refer weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "not aplicable"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7082/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699218980106,
            "cdate": 1699218980106,
            "tmdate": 1699636834929,
            "mdate": 1699636834929,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VBz0uuSaY1",
                "forum": "pdJXYfJjz9",
                "replyto": "mdSflDdsWQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7082/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7082/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer zqPW"
                    },
                    "comment": {
                        "value": "**Comments**: We appreciate the efforts and valuable suggestions provided by the reviewer. We will address the concerns outlined below:\n\n> the comparisons are inadequate. There is rich literature in semi-supervised and continual learning that focuses on representation learning for different types of rain and image representations. Authors failed to compare with the existing methods.\n\nWe first added the semi-supervised method Syn2Real [1], MOSS [2], and the unsupervised method DCD-GAN [3] to our *Introduction* and literature review in *Related Work*. Since the official training code for MOSS [2] is unavailable, we compare the proposed CoIC with Syn2Real and DCD-GAN. Both Syn2Real and DCD-GAN are trained for more than 1M iterations (much longer than BRN, RCDNet, DGUNet, IDT, and DRSformer). However, we observed that both Syn2Real and DCD-GAN struggle when faced with intricate rainy images from multiple datasets, yielding unsatisfactory results on Rain200L, Rain200H, Rain800, DID-Data, and DDN-Data datasets. We tabulate the PSNR metrics below for direct comparison (Details are in Table 1 of the revised manuscript):\n\n| Methods          | Rain200L  | Rain200H  | Rain800   | DID-Data  | DDN-Data  |\n| ---------------- | --------- | --------- | --------- | --------- | --------- |\n| Syn2Real         | 30.83     | 17.21     | 24.85     | 26.71     | 29.15     |\n| DCD-GAN          | 21.64     | 16.04     | 19.52     | 21.28     | 21.60     |\n| DRSformer + CoIC | **39.81** | **30.50** | **29.92** | **35.01** | **33.94** |\n\nFrom the above results, we observe that learning a comprehensive deraining model on mixed datasets is challenging for semi-supervised and unsupervised methods. The proposed CoIC can efficiently assist models in learning much better on multiple datasets.\n\nAdditionally, we have provided a demo on our anonymous GitHub page: https://anonymous.4open.science/r/CoIC-730F/\n\n[1]. Yasarla, Rajeev, et al. \"Syn2real transfer learning for image deraining using gaussian processes.\" in CVPR 2020.\n\n[2]. Huaibo Huang, et al. \"Memory oriented transfer learning for semi-supervised image deraining.\" in CVPR 2021.\n\n[3]. Xiang Chen, et al. \"Unpaired deep image deraining using dual contrastive learning.\" In CVPR 2022."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7082/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700439769325,
                "cdate": 1700439769325,
                "tmdate": 1700439769325,
                "mdate": 1700439769325,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "g69gjAjqye",
            "forum": "pdJXYfJjz9",
            "replyto": "pdJXYfJjz9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7082/Reviewer_nGEY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7082/Reviewer_nGEY"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an instance-specific de-raining models by exploring representations that characterize both the rain and background components in rainy images. The authors first propose a rain-/detailaware contrastive learning strategy to explore joint rain-/detail-aware representations. Leveraging these representations as instructive guidance, the authors furhter introduce CoI-M to perform layer-wise modulation of CNNs and Transformers."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1, The paper is well-written. \n\n2, The paper may be meaningful to some extent for the deraining community.\n\n3, The results reveal that the proposed method consistently improves previous baselines.\n\n4, The proposed instance-specific method may be useful to other low-level vision tasks, e.g., dehazing, desnowing, super-resolution, etc."
                },
                "weaknesses": {
                    "value": "Although this paper has some strengths, I am still confused by following questions:\n\n1, The proposed techniques seem to be a guidance for deraining, i.e., the authors attempt to constrain an instance-specific representation to guide the deraining.\n\nAlthough this, the reviewer would like to know the increased parameters and FLOPs.\n\n2, Whether the training iterations of the proposed method are the same with the applied method? i.e., BRN, RCDNet, DGUNet, IDT, DRSformer. If not, the comparisons are unfair.\n\n3, The reviewers would like to see the training curve comparisons, e.g., BRN, RCDNet, DGUNet, IDT, DRSformer vs. BRN+ CoIC, RCDNet+ CoIC, DGUNet+ CoIC, IDT+ CoIC, DRSformer+ CoIC.\n\n4, The paper title is the  INSTANCE-SPECIFIC IMAGE DE-RAINING, the reviewer thinks that this is not suitable since the authors also train the deep models on Rain13K. The dataset can be regarded as one entirety. It is hard to reflect the 'INSTANCE'. Whether it has a better title? The reviewer thinks that if the paper does a continual learning for deraining and demonstrates the CoIC can consistently improve the  performance on each dataset, the 'INSTANCE' may be suitable.\n\n5, Deraining methods with high PSNR and SSIM usually tend to overfit to synthetic datasets. Hence, deraining performance would be worse on real datasets. Whether the authors can further solve this problem? \n\n6, Can the rain-aware negative exemplars be images with real rain streaks? Whether the authors consider to explore the real images to participate in training to improve the generalization to real scenes? \n\n7, I have observed that the authors mention the generalization many times. However, only training on synthetic dataset is hard to improve the generalization because of overfitting."
                },
                "questions": {
                    "value": "See Weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7082/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7082/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7082/Reviewer_nGEY"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7082/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699538272290,
            "cdate": 1699538272290,
            "tmdate": 1699636834826,
            "mdate": 1699636834826,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "g59OczpR81",
                "forum": "pdJXYfJjz9",
                "replyto": "g69gjAjqye",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7082/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7082/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer nGEY (Part 1/2)"
                    },
                    "comment": {
                        "value": "**Comment**: We thank the reviewer for the critical and constructive suggestions. We are delighted that you found our paper well-written and meaningful. However, there still exist critical issues that need to be addressed. Here, we will carefully address your concerns and questions:\n\n> About the increased parameters and FLOPs.\n\nAs suggested by the reviewer, we have added a comprehensive analysis of CoIC in terms of the parameters (#P), FLOPs, and testing time. The results are presented in the table below. Generally, **the increased parameters are related to the intrinsic architecture of the models**. **It is noteworthy that the performance improvement is not mainly from the increased parameters**. Note that IDT equipped with CoIC, with fewer increased parameters, can result in much better performance improvement when compared to DRSformer in Table 1, where more parameters are brought by CoIC for DRSformer. However, the changes in FLOPs and inference time (about 30ms) are almost the same for all models, which means that the extra changes are mainly from the encoder but not the modulation process.\n\nWe have also included this analysis in our paper. Please refer to *Appendix A.3* for details.\n\n| Model name | input size | #P w/o CoIC (M) | #$\\Delta$P  (M) | FLOPs w/o CoIC (G) | FLOPs w/CoIC (G) | Time w/o CoIC (ms) | Time w/ CoIC (ms) |\n| ---------- | ---------- | :-------------: | --------------- | ------------------ | ---------------- | ------------------ | ----------------- |\n| BRN        | 512x512    |      0.38       | 0.21            | 392.9              | 393.3            | 332.2$\\pm$38.1     | 364.2$\\pm$33.0    |\n| RCDNet     | 512x512    |      2.98       | 2.11            | 389.0              | 389.5            | 351.6$\\pm$0.2      | 384.8$\\pm$2.2     |\n| DGUNet     | 512x512    |      3.63       | 1.62            | 396.8              | 397.2            | 161.0$\\pm$5.5      | 198.8$\\pm$6.8     |\n| IDT        | 128x128    |      16.42      | 2.53            | 7.3                | 7.6              | 59.8$\\pm$2.7       | 83.2$\\pm$10.5     |\n| DRSformer  | 512x512    |      33.67      | 14.12           | 440.8              | 441.1            | 833.6$\\pm$0.5      | 886.7$\\pm$11.0    |\n\n> Whether the training iterations of the proposed method are the same with the applied model?\n\nWe apologize for missing the training details. In fact, to ensure a fair comparison, all the applied models were trained with consistent iterations and the pixel-fidelity loss. The training details are provided in *Appendix A.5*.\n\n> Comparison on the training curves.\n\nThe training histories are visualized on our anonymous GitHub repository: https://anonymous.4open.science/r/CoIC-730F/. Additionally, we have included the training details and training history in *Appendix A.5*."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7082/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700438467589,
                "cdate": 1700438467589,
                "tmdate": 1700438467589,
                "mdate": 1700438467589,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]