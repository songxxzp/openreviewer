[
    {
        "title": "MMBench: Is Your Multi-modal Model an All-around Player?"
    },
    {
        "review": {
            "id": "b02rc6naj8",
            "forum": "BfMQIJ0nLc",
            "replyto": "BfMQIJ0nLc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission502/Reviewer_1JAs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission502/Reviewer_1JAs"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new benchmark MMBench for multimodal models. Unlike previous benchmarks suffering from scalable or bias problems, MMBench provides a comprehensive evaluation in an automatic ways. This benchmark reveals several drawbacks of current multimodal models, such as limited instruction-following and logic reasoning capabilities."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This work proposes a comprehensive benchmark and conducts extensive experiments on current multi-modal models. Additionally, this paper shows the bias of model's answer of singlechoice questions and proposes circular evaluation to improve the robustness."
                },
                "weaknesses": {
                    "value": "The analysis and observations in this study align with prior research. Additionally, MME[1] highlights issues such as not following instructions, lack of reasoning and limited physical relation perception. Personally I encourage the presentation of novel insights regarding the shortcomings of existing MLLMs or suggestions for improvements.\n\nWhile employing ChatGPT as the choice extractor can eliminate the need for manual ratings, it does introduce reliability concerns. Recent research[2] has revealed successful attacks to GPT by other LLMs, raising safety issues incorporating ChatGPT in evaluations.\n\n[1] https://arxiv.org/pdf/2306.13394.pdf\n[2] https://arxiv.org/pdf/2310.08419.pdf"
                },
                "questions": {
                    "value": "I am curious about the efficiency of this benchmark. As it uses CircularEval and incorporates chatGPT in the evaluation process, will it become much slower than other benchmarks?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission502/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission502/Reviewer_1JAs",
                        "ICLR.cc/2024/Conference/Submission502/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission502/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698367885035,
            "cdate": 1698367885035,
            "tmdate": 1700507501066,
            "mdate": 1700507501066,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "s7d1DavTRc",
                "forum": "BfMQIJ0nLc",
                "replyto": "b02rc6naj8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission502/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission502/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your valuable advice. Here is our response to your questions:\n\n## Q1: Insight brought by MMBench\n\nNumerous insights can be gleaned from the MMBench leaderboard, and we highlight some of the more apparent phenomena observed during the evaluation:\n\n1. The performance of coarse perception is significantly lower than that of fine-grained perception across all models presented in Table 3 of the main paper. This trend is understandable, given that fine-grained perception necessitates the model's ability to precisely identify and comprehend the various objects present in an image. Furthermore, we discovered that enabling trainability of parameters in the vision encoder positively influences the model's fine-grained perception capabilities. This is because it equips the model with the flexibility to modify its vision feature extraction ability during visual-language pre-training and instruction tuning.\n\n2. The performance of cross-instance finegrained perception is much lower than that of single-instance finegrained perception. The leaderboard of MMBench indicates that a majority of pre-training vision-language datasets primarily include descriptions or questions pertaining to individual objects, thereby neglecting to address cross-object relationships. Furthermore, we've observed that incorporating a referring expression comprehension task can effectively enhance the performance of the cross-instance fine-grained perception task.\n\n3. The performance on perception tasks significantly surpasses that on reasoning tasks. Reasoning is a more complex task compared to perception, as it demands the model to not only accurately identify various objects in an image, but also possess the ability to reason about these objects, including their function and social identity. Therefore, enhancing the complexity of the instruction data, rather than posing simple questions about the color and shape of objects, is anticipated to improve this ability.\n\n## Q2: ChatGPT is vulnerable to attack\n\nIn the evaluation pipeline of MMBench, ChatGPT is solely utilized to extract answers from predictions and match them to the available choices, and the accuracy is calculated by some predefined functions based on the matching results, with CircularEval strategy. At present, we haven't identified any potential vulnerabilities that could affect ChatGPT within the evaluation pipeline and cannot think any potential attack approaches. If you have any ideas or suggestions on how to perform the attack, please feel free to propose them directly. We are more than willing to engage in a discussion with you.\n\n## Q3: The efficiency of this benchmark.\n\nWhile exact matching has been utilized in numerous previous benchmarks like VQAv2 and GQA, incorporating ChatGPT into the evaluation pipeline may indeed slow down the process. However, when weighing the tradeoff between robustness and efficiency, using ChatGPT for extraction and matching emerges as the optimal choice. This is because it eliminates the issue of generating a high number of false negative samples, and the time required for evaluation remains within acceptable limits. The following table presents the number of ChatGPT API calles required by each VLM under VanillaEval and CircularEval.\n\n| Model            | VanillaEval GPT Calls | CircularEval GPT Calls |\n| ---------------- | --------------------- | ---------------------- |\n| OpenFlamingov2   | 13                    | 7                      |\n| LLaVA-7B         | 993                   | 1908                   |\n| VisualGLM-6B     | 1051                  | 1990                   |\n| MMGPT-9B         | 143                   | 229                    |\n| InstructBLIP-13B | 69                    | 102                    |\n| mPLUG-Owl-7B     | 701                   | 1510                   |\n| Qwen-VL          | 245                   | 352                    |\n| MiniGPT-4-7B     | 656                   | 1026                   |\n| Idefics-80B      | 23                    | 36                     |\n| PandaGPT         | 1167                  | 1993                   |\n| MiniGPT-4-13B    | 524                   | 1120                   |\n| OpenFlamingo     | 68                    | 57                     |\n| Llama-adapter    | 960                   | 1624                   |\n| Otter-9B         | 0                     | 0                      |\n\nAmong all VLMs, PandaGPT consumes most ChatGPT API calls (\\~ 2000). We find that each answer matching prompt is 500 tokens length on average. Based on the current price of ChatGPT (GPT-3.5, which is 0.001 USD / 1k tokens), the price upper bound for each evaluation is only 1 USD  (\\~0.3 USD for all models on average). In practice, we find that for most models, the evaluation can finish in half an hour, with one single OpenAI key and parallel API calling."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission502/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700209143916,
                "cdate": 1700209143916,
                "tmdate": 1700209143916,
                "mdate": 1700209143916,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SkwAfO3vbd",
                "forum": "BfMQIJ0nLc",
                "replyto": "s7d1DavTRc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission502/Reviewer_1JAs"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission502/Reviewer_1JAs"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your reply. It address my questions and I raise my rating to 6."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission502/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700519884289,
                "cdate": 1700519884289,
                "tmdate": 1700519884289,
                "mdate": 1700519884289,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Nnj067ngbg",
            "forum": "BfMQIJ0nLc",
            "replyto": "BfMQIJ0nLc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission502/Reviewer_g871"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission502/Reviewer_g871"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a new multiple-choice VQA evaluation benchmark for assessing recent multimodal language large models without subjective human evaluation. The benchmark is set up to evaluate the perception and reasoning abilities of these models, such as attribute prediction, OCR, action recognition, social relation, and so on. It currently consists of 2948 single choice questions covering over 20 abilities. It comprehensively reports the performance of recent 18 models including LLaVA, InstructBLIP, Shikra, and etc."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "There are several strengths about this work:\n- The vision-language community certainly needs more objective benchmarks for evaluating recent multimodal models.\n- The proposed VQA benchmark covers a wide array of abilities (over 20).\n- The paper comprehensively tests most recent multimodal models (18 of them)."
                },
                "weaknesses": {
                    "value": "I have several major concerns about dataset collection and evaluation strategies.\n\n> **Dataset Collection and Quality**\n\nAs the major contribution of this paper is the new VQA benchmark, I find the paper did a **poor job in explaining how the samples are generated, collected, and verified**. For example, how did you select images from existing sources? How did the annotator come up with QA pairs based on the images? How did you verify the correctness/relevance of these samples? From the current paper it is really hard to gauge the data quality of the benchmark.\n\nAfter I downloaded the public dev set, I can easily find a lot of VQA samples across categories that can be **solved without looking at the images**. Here are some examples:\u2028\u2028\n\n>**Example 1**\n\nImage: a photo of an African elephant. \n\nQuestion: The African elephant is the () land animal in the world. \n\nOptions: (A) smallest, (B) largest. \n\nCategory: attribute_recognition.\n\n> **Example 2**\n\nImage: a photo of a snow rabbit. \n\nQuestion: Which animal\u2019s skin is also adapted for survival in cold places? \n\nOptions: (A) fantastic leaf-tailed gecko, (B) polar bear. \n\nCategory: physical_property_reasoning.\u2028\u2028\n\n> **Example 3**\n\nImage: a photo of the world map centered on Australia.\n\nQuestion: What direction is Indonesia in Australia?\n\nOptions: (A) northwest, (B) northeast, (C) southwest, (D) southeast.\n\nCategory: spatial_relationship\n\nEven though I cannot attach images to my review, it is clear that these questions can be answered by a human without looking at the images. This makes MMBench more like a QA instead of a VQA benchmark. The authors should discuss how MMBench is collected and why such problematic samples can leak into your dev set. Did you use crowd-sourced or expert annotators? This is an important question to answer especially if MMBench continues to expand -- how do you plan to ensure the quality of collected samples?\n\nFinally, the paper did not discuss the **licensing for collected Internet images**. I also find some images of MMBench containing **watermarks** from existing website such as zhihu.com.\n\n> **Evaluation Strategy**\n\nI am also concerned with the CircularEval strategy. In section 3.1, the paper says *\u201cCircularEval doesn\u2019t necessarily require N x inference cost\u201d* because if the VLM makes a wrong prediction in one pass, the following passes can be dropped. As such, the paper claims this strategy has *\u201can affordable cost\u201c* (Section 3). I find this to be a very misleading statement because the computation cost is indeed O(N) and a \u201cperfect\u201d model will still require N passes. \n\nCould you explain why not doing N passes then report the average accuracy? \n\nHuman performance on MMBench is currently missing. This is important to gauge the overall difficulty of this benchmark.\n\nFinally, answer extraction using LLMs is a standard practice in NLP [1], and thus it is hardly a novel contribution.\n\n> **References**\n\n[1] Large Language Models are Zero-Shot Reasoners. Kojima et al. 2022."
                },
                "questions": {
                    "value": "In addition to my questions listed above in weakness section, I have one extra question regarding **related work**:\n\nCould you explain why MME has *\"a non-rigorous evaluation setting\"* that makes it harder to reveal the real performance gap between VLMs? (Appendix A.1)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Legal compliance (e.g., GDPR, copyright, terms of use)"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "The paper did not discuss the licenses/copyright of their Internet collected images. I can see some images in their dataset including watermark from websites such as zhihu.com."
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission502/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission502/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission502/Reviewer_g871"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission502/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698393156671,
            "cdate": 1698393156671,
            "tmdate": 1699635976794,
            "mdate": 1699635976794,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZVXAlB6syT",
                "forum": "BfMQIJ0nLc",
                "replyto": "Nnj067ngbg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission502/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission502/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your valuable advice. Here is our response to your questions:\n\n## Q1: Quality of the benchmark\n\n### Q1.1: How are these samples collected?\n\nFirst, we recruit a group of college students as volunteers. We then provide necessary training to these volunteers, equipping them with skills to collect images and create relevant questions and answers for each task. Moreover, we provide them with a range of potential sources to gather images and formulate corresponding questions and choices. Table 7 in the paper showcases these potential sources. Following this, the collected Visual Question Answering (VQA) data undergoes a quality inspection pipeline to ensure the creation of a reliable benchmark.\n\n### Q1.2: Quality testing for samples in MMBench\n\nWe conduct thorough quality testing on the collected samples through manual review, with a primary focus on two key aspects:\n1. Ensuring the accuracy and coherence of the questions, without any logical or factual errors.\n2. Verifying the presence of a single correct answer that is pertinent to the question.\nWe filter out any samples that fail to adhere to these aforementioned criteria. \n\n### Q1.3: Question can be answered without the corresponding image\n\nThank you for highlighting this issue. We too identified the same problem after submitting our paper. To determine the number of questions in MMBench that can be answered without an image, we input all the questions into GPT-4 and ensure that it makes reasonable guesses for all questions. We discovered that ~13% of the dev questions and ~10% of the test questions could be answered correctly, which mainly originated from ScienceQA. Subsequently, we removed these questions and used the remaining ones to evaluate all models. As indicated in the table below, the performance of these models remained similar to their performance prior to the image filter, and the rankings of the different models were basically consistent. That suggests that the current MMBench leaderboard remains credible. Furthermore, we plan to release an updated version of MMBench, with the questions that can be answered without images filtered out.\n\n| Model            | Dev   | Dev w/o. GPT-4 | Test  | Test w/o. GPT-4 |\n| ---------------- | ----- | -------------- | ----- | --------------- |\n| Qwen-VL-Chat     | 60.57 | 59.9           | 61.83 | 60.75           |\n| Shikra           | 59.36 | 60             | 60.43 | 60.81           |\n| IDEFICS-80B      | 54.81 | 51.29          | 54.82 | 52.34           |\n| IDEFICS-9B       | 48.37 | 47.03          | 45.52 | 44.8            |\n| mPLUG-Owl        | 48.11 | 48.51          | 46.41 | 46.11           |\n| LLaVA            | 44.5  | 42.77          | 42.21 | 41.74           |\n| InstructBLIP-13B | 44.42 | 43.17          | 43.39 | 43.18           |\n| MiniGPT-4-13B    | 42.53 | 40.5           | 42.54 | 41.62           |\n| LLaMA-Adapter    | 41.24 | 41.09          | 39.63 | 39.63           |\n| VisualGLM        | 38.57 | 37.82          | 33.63 | 33.52           |\n| Qwen-VL          | 38.23 | 36.53          | 32.23 | 31.34           |\n| InstructBLIP     | 35.48 | 33.66          | 35.37 | 34.08           |\n| PandaGPT         | 33.93 | 32.77          | 30.72 | 29.1            |\n| MiniGPT-4        | 32.04 | 30.69          | 29.43 | 28.04           |\n| MMGPT            | 15.21 | 13.66          | 15.98 | 15.26           |\n| OpenFlamingo v2  | 6.7   | 5.45           | 5.72  | 4.61            |\n| OpenFlamingo     | 4.64  | 3.37           | 4.54  | 3.55            |\n\n### Q1.4  Licensing for collected Internet images\n\nAll images within MMBench are available for academic use.  During the collection process, we instruct our annotators to document the original URL of each image. Subsequently, we manually review these URLs individually, eliminating any images that are not suitable for academic use. Moreover, should any authors request the removal of their images from MMBench, we will promptly comply.\n\n### Q1.5 MMBench is a QA benchmark instead of a VQA one\n\nThe conclusion seems overly severe to be considered fair. As demonstrated in Q1.3, a mere 10% of the existing questions can be answered without the aid of images. The remaining majority of questions necessitate the model's accurate comprehension of the image content to provide correct responses."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission502/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700212203196,
                "cdate": 1700212203196,
                "tmdate": 1700212203196,
                "mdate": 1700212203196,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "onkOa0sGUG",
                "forum": "BfMQIJ0nLc",
                "replyto": "Nnj067ngbg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission502/Reviewer_g871"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission502/Reviewer_g871"
                ],
                "content": {
                    "title": {
                        "value": "Reply to authors"
                    },
                    "comment": {
                        "value": "I found the author responses to my major concern (dataset quality / curation) to be vague. \n\n> **Response: ...college students as volunteers. We then provide necessary training to these volunteers, equipping them with skills to collect images and create relevant questions and answers for each task...**\n\nWhat does it mean by **necessary training** and **skills to collect relevant questions**? I do not find details regarding this important information in the updated version of the paper. Please update the paper with the detailed procedure.\n\n> **Response: ..we input all the questions into GPT-4 and ensure that it makes reasonable guesses for all questions. We discovered that ~13% of the dev questions and ~10% of the test questions could be answered correctly, which mainly originated from ScienceQA. ... Furthermore, we plan to release an updated version of MMBench, with the questions that can be answered without images filtered out.**\n\nHow did you use ChatGPT for reasonable guesses? And do you use CircularEval? What is the exact prompt you used for conversing with ChatGPT? I do not find this discussion in the updated paper. Also, given that MMBench is already used in many recent MLLM evaluation, how do you plan to update this benchmark? Do you plan to release a MMBench-2, or simply request all previous works who used the earlier version of MMBench to re-evaluate their models? \n\n> **The author did not discuss the watermark issue in their response.**\n\n> **I found the author's response regarding to CircularEval to be confusing.**\n\nReading the response, I believe the term used in paper (**CircularEval doesn\u2019t necessarily requires N\u00d7 inference cost.**) is just misleading. **Inference cost** usually refers to the cost of model inference. The authors should update the paper to clarify that this has to do with \"ChatGPT calls/querying cost\".\n\n> **Response: sometimes CircularEval even requires less ChatGPT calls compared to VanillaEval. This happens when the heuristic strategy failed to match the VanillaEval prediction with any options, but succeeded in matching one of the CircularEval predictions with a wrong option.**\n\nPlease clarify this with a concrete example. I find this to be confusing.\n\n> **Thank you for clarifying the issues of MME. I think this discussion should go into the updated paper to justify why people should use your benchmark instead of MME.\"**"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission502/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700466388516,
                "cdate": 1700466388516,
                "tmdate": 1700466435780,
                "mdate": 1700466435780,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ThCzqpHEKI",
                "forum": "BfMQIJ0nLc",
                "replyto": "Nnj067ngbg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission502/Reviewer_g871"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission502/Reviewer_g871"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the reply"
                    },
                    "comment": {
                        "value": "I carefully checked the updated paper. I appreciate that the authors took most of reviewers' feedbacks into consideration for improving the draft. I have a remaining question regarding the use of public data sources.\n\n> **Quote from paper: \"Firstly, our data is gathered from the validation sets of these public datasets, not their training sets. Secondly, data samples procured from these public datasets constitute less than 10% of all MMBench data samples.\"**\n\nI think these statements can be misleading -- I saw that LLaVA is one of the data sources for constructing MMBench; however it does not come with an official validation set (unless you are referring to the 30-images LLaVA-Bench). Also, I do not find the actual distribution of data sources in the current draft."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission502/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700638416329,
                "cdate": 1700638416329,
                "tmdate": 1700644354945,
                "mdate": 1700644354945,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "w4kSeGvyrI",
                "forum": "BfMQIJ0nLc",
                "replyto": "ThCzqpHEKI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission502/Reviewer_g871"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission502/Reviewer_g871"
                ],
                "content": {
                    "title": {
                        "value": "Some last comments"
                    },
                    "comment": {
                        "value": "I still think the current evaluation protocol, especially when coupled with both heuristic matching and ChatGPT answer extraction strategy, is not conceptually simple and may lead to unfair comparison. For example, if outputting A/B/C/D is challenging because current VLMs lacks instruction-following abilities, a new VLM/prompt engineer can intentionally choose to ignore the multiple choices and directly output the final answer, thus leaving all answer extraction to the powerful ChatGPT. Also, answer extraction does not seem trivial for this dataset as even the SOTA open-source LLaMA fail to align with human (compared against ChatGPT). \n\nI think for such small-scale (no more than 10K samples) manually-constructed benchmark, the dataset quality is the most important factor because even including a few biased samples could potentially affect the future development of VLMs. While it is not possible for reviewers to perform manual check of the dataset quality, I think the authors should release comprehensive training materials and open-source the platform such that future work can follow the same protocol to collect samples. I will consider this to be a more important scientific contribution than the dataset itself."
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission502/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700674799157,
                "cdate": 1700674799157,
                "tmdate": 1700674799157,
                "mdate": 1700674799157,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "G52PI4dOyi",
                "forum": "BfMQIJ0nLc",
                "replyto": "Nnj067ngbg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission502/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission502/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the update"
                    },
                    "comment": {
                        "value": "# Our Responses: \n\n1. **LLaVA**: \n\n    Apologies for any confusion caused. When we talk about \"the validation sets\", it only applies to datasets that have both \n    \"training\" and \"validation\" splits (like COCO or ScienceQA). For LLaVA, we utilize the images in the full set. We will update the \n    paper accordingly to resolve the confusion.\n\n2. **Data Source Statistics**: \n\n    Since the discussion deadline is approaching, we may not be able to provide the source statistics at this time (some raw data \n    need to be processed to figure out the exact source of each image). We promise that the detailed statistics will be included in the \n    next version of the manuscript. \n\n3. **Answer Extraction**:\n\n    3.1. __ChatGPT-based Answer Extraction__: \n\n    Most VLMs and LLMs are designed to **chat** with human beings: If not finetuned with a sufficient number of multiple-choice \n    problems, most of them can not output the option label for every question perfectly (ie., follow the instruction of solving multiple- \n    choice problems). Our ChatGPT-involved evaluation pipeline is designed to evaluate VLMs solely based on the answer content, \n    and we think that pipeline can be more fair compared to exact matching. \n\n    If we use exact matching during the evaluation, VLMs finetuned on a large number of multiple-choice problems will follow the \n    instructions in MMBench much better, and significantly outperforms VLMs not finetuned on multiple-choice problems. Then it will \n    be difficult to reveal the **real** multi-modal understanding performance gap with the evaluation results. \n\n    3.2. __Poor performance of LLaMA__: \n\n    Actually, the overall performance of Vicuna (v1.0, finetuned based on LLaMA) is not **SOTA**, especially at the current time \n    (2023.11), thus it performs poorly on MMBench answer extraction. In the upcoming version, we will try to utilize more advanced \n    opensource LLMs (XWinLM, UltraLM, Vicuna v1.5, etc.) to see if they can achieve better performance on MMBench answer \n    extraction. If so, we will have a ChatGPT free evaluation pipeline. \n\n4. **Training Materials & Annotating Platform**: \n\n    We plan to release the training materials we used to guide the annotators. Meanwhile, the release of the annotating platform will \n    be more difficult. We will try our best to deliver an open-source version of the annotating platform. However, we cannot make any \n    promise at this time."
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission502/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700732011868,
                "cdate": 1700732011868,
                "tmdate": 1700735352794,
                "mdate": 1700735352794,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "i5R4TGZB8r",
            "forum": "BfMQIJ0nLc",
            "replyto": "BfMQIJ0nLc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission502/Reviewer_EsaC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission502/Reviewer_EsaC"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes an evaluation-only benchmark, named MMBench, for multimodal (vision-and-language) models. The benchmark contains 3k single-choice questions for images that come from existing datasets or newly collected sources, covering 20 different ability dimensions. To evaluate vision-language models on the benchmark, the paper proposes an evaluation pipeline featuring the CircularEval strategy, which tests the VLM for multiple times and requires consistent correct answers, and chatGPT-involved choice extraction, which extracts single-choice answers for the VLM responses that do not follow the instruction format well. Multiple models are evaluated on the benchmark, where Qwen-VL-Chat shows the best performance."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper comes with a relatively big (3k) and well-designed benchmark for VLM evaluation, which is an important contribution.\n2. Evaluation strategies are designed to test the VLMs that cannot generate single-choice answers. ChatGPT is used in this case, with an analysis compared to human evaluation to show that the introduction of ChatGPT does lead to evaluation bias.\n3. The paper is well-written and easy to follow."
                },
                "weaknesses": {
                    "value": "1. More discussions of the 20 different ability dimensions would be favored. How these dimensions are selected can be discussed further. Moreover, in many cases, multiple abilities are entangled with each other in order to correctly answer a question. For example, \u201cHow many apples are there in the image?\u201d as shown in Fig-3 requires both numerical (counting) reasoning and perception (detect apples), which category does this example belong to? \n2. The results are usually \u201cwinner takes all\u201d. As shown in the results in Tab-3, more powerful models are usually stronger in every evaluation dimension. It would be interesting to see more fine-grained analysis, e.g. some models are stronger in dimension A, while another is stronger in dimension B, etc.\n3. Bias analysis. Shortcut/bias has long been a problem in VQA, where language bias and visual context bias are entangled with each other, leading the models to take shortcuts to answer the questions without real understanding. Does this benchmark suffer from similar problems? Some analysis on how the dataset is balanced, as well as visualizations of the distribution for different concepts and how they co-occur with each other would be helpful.\n4. It would be good to have the results for Bard and GPT-4V."
                },
                "questions": {
                    "value": "See weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Discrimination / bias / fairness concerns",
                        "Yes, Responsible research practice (e.g., human subjects, data release)"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "A new dataset is introduced."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission502/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698643810340,
            "cdate": 1698643810340,
            "tmdate": 1699635976706,
            "mdate": 1699635976706,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wFrtwC9aeU",
                "forum": "BfMQIJ0nLc",
                "replyto": "i5R4TGZB8r",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission502/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission502/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your valuable advice. Here is our response to your questions:\n\n## Q1: How these existing 20 ability dimensions are selected\n\n1. Perception and reasoning are two crucial cognitive abilities in humans. Firstly, we refer to the definitions of perception and reasoning in biology, such as the specific aspects they encompass. The part has been thoroughly discussed in Section 2.1 of the main paper.\n\n2. There is currently no consensus in the academic community regarding the specific taxonomy of sub-abilities in perception and reasoning, and there also lacks a comprehensive theoretical framework. Therefore, we gathered a group of experts from the fields of computer vision, natural language processing, and machine learning to discuss the specific ability taxonomy in perception and reasoning. As a result, we obtained 20 ability dimensions included in MMBench, as well as the ability hierarchy ranging from L1 to L3.\n\n3. The existing 20 ability dimensions are not fixed, and we will continuously update the existing ability dimensions to further improve  the ability taxonomy. \n\n## Q2: How to classify a question, if multiple ability dimensions are entangled\n\nAccording to the definition in Page 16, we categorize the \"counting\" ability as \"Object Localization\". In MMBench, we do not consider \"counting\" as a more complex reasoning problem, since for most counting questions, there only exist several objects to be counted. \n\nIf multiple ability dimensions are intertwined, we consistently categorize the current question under the more challenging ability dimension. This is because, in most scenarios, the more difficult ability dimension necessitates the model to possess the capability to solve simpler tasks. By doing so, it can effectively utilize the task outputs as intermediate results to tackle more complex tasks. For instance, all questions in \"Attribute Comparison\" rely on the capability of \"Attribute Recognition\", since we consider \"Attribute Comparison\" as a more complex task, we categorize these questions as \"Attribute Comparison\". \n\n## Q3: The results are usually \u201cwinner takes all\u201d \nTable 3 presents the results of the L-2 ability dimension. Unlike a \"winner takes all\" scenario, these results on L-3 ability dimension offer a nuanced understanding of the model's capabilities. For further insights, please refer to Tables 9 through 14, which provide additional details on the results. For instance, Shikra demonstrates exceptional performance in Action Recognition, while LLaMA-Adapter excels in Attribute comparison. We hypothesize that factors such as data composition, instruction data quality, and instruction template significantly influence a model's performance across different ability dimensions.\n\n## Q4. Bias Analysis\n\n| Model | Overall | CP | FP-S  | FP-C  | AR | LR | RR  |\n| --- | --- | --- | -- | -- | -- | -- | -- |\n| GPT-4v | 75.06   | 82.77 | 71.65 | 66.43 | 77.88 | 69.29 | 74.31 |\n| Qwen-VL-Chat | 60.57   | 79.39 | 66.21 | 48.25 | 59.8  | 32.2  | 43.48 |\n| Shikra  | 59.36 | 76.35 | 57.68 | 58.74 | 57.29 | 26.27 | 58.26 |\n| Bard  | 58.16 | 58.42 | 57.6  | 39.6  | 68.27 | 53.4  | 71.64 |\n| IDEFICS-80B | 54.81   | 64.53 | 58.36 | 44.76 | 65.83 | 23.73 | 46.09 |\n| GPT-4  | 13.23 | 6.41  | 7.84  | 6.99  | 32.16 | 27.96 | 4.34  |\n| GPT-3.5 | 11.17 | 2.7   | 9.9   | 4.9   | 22.11 | 25.42 | 10.43 |\n| Claude-2 | 10.74 | 5.74  | 6.14  | 2.8   | 27.14 | 22.88 | 4.35  |\n\nOne shortcut might be that the model could provide accurate answers even without relying on image information. To assess the necessity of image data for questions in our dataset, we employed some state-of-the-art LLMs to respond to queries in the MMBench-dev. During the evaluation, we design meta prompts to guide the LLMs try their best to make a reasonable guess, even if a question is not theoretically answerable given only the text part. \n\nAs demonstrated in above table, the performance of the text-only GPT-4 model \u2014 prompted solely with question text \u2014 is notably suboptimal. This limitation is particularly evident in responding to queries that require 'perception' abilities. Such questions, often intrinsically linked to visual information, include examples like \"What mood does this image convey?\" or \"Who is this person?\" In contrast, GPT-4 exhibits a degree of proficiency in 'reasoning' tasks, with a notable strength in logical reasoning (LR). The model's performance in LR is comparable to that of other leading open-source models. The effectiveness in LR could be attributed to the model's ability to leverage common sense in answering questions. Notably, some LR data, sourced from ScienceQA, suggest that a portion of these tasks can be effectively addressed using the model's inherent common sense reasoning [1]. In future work, we will meticulously curate our dataset to guarantee that the image data in MMBench is pivotal for addressing every question.\n\n[1] Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission502/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700203858334,
                "cdate": 1700203858334,
                "tmdate": 1700204218746,
                "mdate": 1700204218746,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Xl6oPSsySa",
                "forum": "BfMQIJ0nLc",
                "replyto": "i5R4TGZB8r",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission502/Reviewer_EsaC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission502/Reviewer_EsaC"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "I thank the authors for the response. I appreciate the GPT-4V results, as well as the results for the bias.\n\nThe text-bias result is interesting. In bias results, it's shown that text-only GPT-4 without looking at the images gets 13% accuracy, with a 28% LR (reasoning) performance that is higher than some models with images. It will be interesting to see similar analysis for more models besides GPT-4.\n\n> In Table 1, we report the accuracies of GPT-4v and Bard (only answered questions count: Acc = (Number of questions answered correctly) / (Number of questions answered)).\n\nI believe this metric is misleading and leads to misleading high results for models that produces \"not answerable\" answers for most of the questions. A better metric is to assign random answers for the not answerable questions, then calculate accuracy. For example, Bard refused to answer 31.6% questions, which is not a small percentage.\n\nAdditionally, analysis of those rejected questions, i.e. whether they are ambiguous, low-quality, or some other reasons, would be favorable.\n\nMoreover, the response does not address my concerns in a more detailed analysis on analysis of the 20 different ability axis. As most of the results demonstrate \"winner takes all\". Since the benchmark introduces the 20 dimensions, showing the analysis findings on the 20 dimensions is necessary to show the advantage of reporting 20 numbers over only the overall accuracy. \n\n> please refer to Tables 9 through 14, which provide additional details on the results. For instance, Shikra demonstrates exceptional performance in Action Recognition, while LLaMA-Adapter excels in Attribute comparison. We hypothesize that factors such as data composition, instruction data quality, and instruction template significantly influence a model's performance across different ability dimensions.\n\nWhile the authors provide intensive results in Tab 9-14, the results are less meaningful if no analysis is provided. The analysis provided here is too vague and general to show the necessity for evaluating on different dimensions.\n\nOverall, I thank the authors for their efforts in the rebuttal and the new results. With the remaining concerns, I keep my rating unchanged."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission502/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700670288069,
                "cdate": 1700670288069,
                "tmdate": 1700670288069,
                "mdate": 1700670288069,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "jrP9grLaIZ",
            "forum": "BfMQIJ0nLc",
            "replyto": "BfMQIJ0nLc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission502/Reviewer_Ntup"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission502/Reviewer_Ntup"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to establish a new benchmark, known as MMBench, for evaluating the multi-modal capabilities of VLMs. In comparison to previous benchmarks, MMBench offers a fine-grained assessment of abilities and employs more robust evaluation metrics. This is achieved by incorporating a wider range of evaluation questions. Additionally, MMBench introduces a rigorous CircularEval strategy that ensures models comprehend the questions and provide answers based on understanding rather than guessing. Moreover, MMBench leverages ChatGPT to convert open-form predictions into pre-defined options, mitigating the impact of varying instruction-following capabilities of VLMs. The proposed benchmark evaluates various MLLMs, revealing their capabilities and limitations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The current MLLMs greatly require a fair and reasonable benchmark to assess the strengths and weaknesses of different methods, making the problem addressed in this paper highly significant. The proposed CircularEVAL strategy effectively enhances the robustness of the evaluations."
                },
                "weaknesses": {
                    "value": "The authors should provide results for GPT4 to establish the upper bound of performance within the proposed benchmark.\n\nFor tasks that perform poorly within the current benchmark, the authors should explain why the models exhibit such poor performance. Is it due to inherent issues with the tasks themselves? Additionally, a comparison with the results of GPT4 can be made to analyze the performance shortcomings of the current open-source MLLMs."
                },
                "questions": {
                    "value": "No other questions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission502/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission502/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission502/Reviewer_Ntup"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission502/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698666008384,
            "cdate": 1698666008384,
            "tmdate": 1699635976625,
            "mdate": 1699635976625,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "iCWXN3ayEb",
                "forum": "BfMQIJ0nLc",
                "replyto": "jrP9grLaIZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission502/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission502/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your valuable advice. Here is our response to your questions:\n\n## Q1: Results for closed source VLMs (GPT-4v, Bard, etc.) \n\nCurrently, we have evaluated GPT-4v and Bard on MMBench-Dev, and we will provide the results on MMBench-Test in the final version. Note that due to the strategies set by developers, the closed source VLMs may reject some of the questions: GPT-4v refused to answer 4.2% questions in MMBench-Dev, Bard refused to answer 31.6% questions in MMBench-Dev. In Table 1, we report the accuracies of GPT-4v and Bard (only answered questions count: Acc = (Number of questions answered correctly) / (Number of questions answered)).\n\n| Model    | Overall | CP    | FP-S  | FP-C  | AR    | LR    | RR    |\n| ------------ | ------- | ----- | ----- | ----- | ----- | ----- | ----- |\n| GPT-4v\\*     | 75.06   | 82.77 | 71.65 | 66.43 | 77.88 | 69.29 | 74.31 |\n| Qwen-VL-Chat | 60.57   | 79.39 | 66.21 | 48.25 | 59.8  | 32.2  | 43.48 |\n| Shikra       | 59.36   | 76.35 | 57.68 | 58.74 | 57.29 | 26.27 | 58.26 |\n| Bard\\*       | 58.16   | 58.42 | 57.6  | 39.6  | 68.27 | 53.4  | 71.64 |\n| IDEFICS-80B  | 54.81   | 64.53 | 58.36 | 44.76 | 65.83 | 23.73 | 46.09 |\n\n**Table 1. Model accuracies on MMBench-Dev (all questions). * mean the question numbers used for calculating accuracies may be different from other models.**\n\nIn Table 2, 3, we provide apple-to-apple accuracy comparisons for GPT-4v and Bard, respectively. In those comparisons, the advantages of GPT-4v and Bard are more significant. \n\n| Model    | Overall | CP    | FP-S  | FP-C  | AR    | LR    | RR    |\n| ------------ | ------- | ----- | ----- | ----- | ----- | ----- | ----- |\n| GPT-4v\\*     | 75.06   | 82.77 | 71.65 | 66.43 | 77.88 | 69.29 | 74.31 |\n| Qwen-VL-Chat | 59.46   | 79.39 | 61.42 | 48.25 | 59.8  | 33.33 | 42.2  |\n| Shikra       | 59.1    | 76.35 | 56.3  | 58.74 | 57.29 | 26.32 | 56.88 |\n| IDEFICS-80B  | 53.81   | 64.53 | 53.15 | 44.76 | 65.83 | 24.56 | 46.79 |\n\n**Table 2. Model accuracies on MMBench-Dev (only questions answered by GPT-4v count).**\n\n| Model    | Overall | CP    | FP-S  | FP-C  | AR    | LR    | RR    |\n| ------------ | ------- | ----- | ----- | ----- | ----- | ----- | ----- |\n| Bard\\*       | 58.16   | 58.42 | 57.6  | 39.6  | 68.27 | 53.4  | 71.64 |\n| Qwen-VL-Chat | 56.28   | 80.9  | 62.67 | 36.63 | 53.1  | 34.09 | 35.82 |\n| Shikra       | 52.89   | 74.72 | 57.14 | 48.51 | 48.28 | 20.45 | 40.3  |\n| IDEFICS-80B  | 49.62   | 65.73 | 54.38 | 28.71 | 59.31 | 22.73 | 37.31 |\n\n**Table 3. Model accuracies on MMBench-Dev (only questions answered by Bard count).**\n\n## Q2: Performance Analysis & Comparison with closed source VLMs\n\n1. In Table 1, GPT-4v significantly outperforms the leading open-source model (Qwen-VL-Chat) by over 14% overall accuracy on MMBench-Dev. The improvement is primarily evident in reasoning tasks: \n    1. In Attribute Reasoning questions, GPT-4v outperforms the second best opensource VLM IDEFICS-80B by over 12% Top-1 accuracy.\n    2. In Logic Reasoning questions, GPT-4v outperform1s all opensource VLMs by over 35% Top-1 accuracy\n    3. In Relation Reasoning questions, GPT-4v outperforms all opensource VLMs by over 15% Top-1 accuracy\n     Meanwhile, the gap between opensource VLMs and GPT-4v is not that huge on perceptions tasks (3% - 10% Top-1 accuracy). \n2. Under apple-to-apple comparisons (only questions answered by the closed source VLM count), the advantage of closed source VLMs is more significant. Generally, the perception performance of opensource VLMs is good and comparable with GPT-4v or Bard (under some preception tasks, opensource VLMs even significantly outperforms Bard). However, the reasoning performance of opensource VLMs still lag far behind GPT-4v or Bard. We attribute the superior reasoning capability of GPT-4v or Bard to their potentially larger and more powerful language models. \n3. Most existing opensource VLMs simply map the visual concepts to the language embedding space, and finetune the model with VL instruction data (with a large proportion of questions perception related). To further improve the reasoning capabilities, the developers can: 1. Switch to more powerful language backbones; 2. Design better finetuning algorithms; 3. Built reasoning-related instruction datasets with better quality."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission502/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700203735414,
                "cdate": 1700203735414,
                "tmdate": 1700203735414,
                "mdate": 1700203735414,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]