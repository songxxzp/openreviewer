[
    {
        "title": "LEO: Generative Latent Image Animator for Human Video Synthesis"
    },
    {
        "review": {
            "id": "57fAR2ze7f",
            "forum": "9zHxXaYEgw",
            "replyto": "9zHxXaYEgw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1248/Reviewer_NikU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1248/Reviewer_NikU"
            ],
            "content": {
                "summary": {
                    "value": "This work introduces a diffusion-based method for video generation. The proposed method leverages a flow-based image animator to learn motion representations thus enabling disentangle motion from appearance. An LDM is designed to learn the motion distribution by providing the starting motion \u03b11 as the condition."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This work tries to solve the challenging issue of disentangling motion from appearance. The method is well-motivated and the proposal method is simple to understand.\n2. A Linear Motion Condition (LMC) mechanism is designed in cLMDM to condition the generative process with the first motion code \u03b11.\n3. Qualitative results show the ability to generate long videos and enable disentanglement of motion and appearance."
                },
                "weaknesses": {
                    "value": "1. The author only includes pickup methods for comparison, STOA methods are not included for comparison. Recent methods, such as MoStGAN-V, VDM, Video-LDM, VideoFactory, and Make-A-Video, should be included for comparison.\n\n2. The author should include experiments on more challenging datasets, such as MSR-VTT and UCF101."
                },
                "questions": {
                    "value": "see weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1248/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1248/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1248/Reviewer_NikU"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1248/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698594929723,
            "cdate": 1698594929723,
            "tmdate": 1699636051067,
            "mdate": 1699636051067,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BSvlFSnHcz",
                "forum": "9zHxXaYEgw",
                "replyto": "57fAR2ze7f",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1248/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1248/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response from the Authors"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the feedback, stating that (1) our proposed method is well-motivated and simple, (2) our proposed idea is effective for appearance and motion disentanglment, as well as long video generation.\n\n**Q1. The author only includes pickup methods for comparison, STOA methods are not included for comparison. Recent methods, such as MoStGAN-V, VDM, Video-LDM, VideoFactory, and Make-A-Video, should be included for comparison.**\n\n**A1.** We have included listed works in Related Works in the revised manuscript.\n\nFor MostGAN-V,  we have updated Tab. 1 in revised manuscript and Supplementary Material website on both FaceForensics and CelebV datasets. Results show that our method outperforms MoStGAN-V both qualitatively and quantitatively.\n\nFor VDM, Video-LDM, VideoFactory, and Make-A-Video, **none of the listed methods is officially open-sourced, as well as reported any evaluation results on FaceForensics, CelebV and Taichi datasets.** Qualitativley and quantitatively comaprison with those methods would be difficult. \n\nWe note that our work differs from listed methods in the tasks to solve. We focus on human-centric video generation, while the listed methods mainly target to solve general unconditional video or text-to-video generation problems.  Technically, the listed methods aim to generate very short video clips (e.g., 16 frames before interpolation) to align with text input. They do not propose any techniques for appearance and motion disentanglement, nor for long video generation, which, however, are the main targets in our work.  We think our proposed techniques could be a complementery component in general video generation system. \n\n**Q2. The author should include experiments on more challenging datasets, such as MSR-VTT and UCF101**\n\n**A2.** From our point of view, articulated human video generation is a very challenging task, even large-scale diffusion models can not handle it very well. It requires the model to simultaneously learn the human structure and motion in both spatial and temporal dimensions.\n\nWe have provided quantitative evaluation on UCF101 for unconditonal generation in Appendix A.2 Tab. 5 in revised manuscript. Results show that our method outperforms state-of-the-art w.r.t. FVD. However, from the results, we observe that it is still challenging for LEO in general video generation. We have addressed this limitation in the revised manuscript in Appendix A.4 and will leave it for our future work.\n\nWe argue that MSR-VTT (text-video dataset) and UCF101 (label-video dataset) are well-known for general conditional video generation, especially for label-/text-to-video generation. We think they are not proper datasets to evaluate the contributions of our method as LEO is designed for human-centric data rather than general video data. We focus on appearance and motion disentanglment, as well as long video generation. We have quantitatively and qualitatively evaluated LEO on three widely used human-centric datasets."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1248/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700498009188,
                "cdate": 1700498009188,
                "tmdate": 1700498009188,
                "mdate": 1700498009188,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lUHjYzA3r0",
                "forum": "9zHxXaYEgw",
                "replyto": "BSvlFSnHcz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1248/Reviewer_NikU"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1248/Reviewer_NikU"
                ],
                "content": {
                    "comment": {
                        "value": "In the response, the author claims that the work mainly focuses on human-centric video generation, while the listed methods mainly target solving general unconditional video or text-to-video generation problems. Actually, I do not see much difference between human-centric video generation and general video generation. The challenge of ensuring spatiotemporal coherency lies not only in human-centric video generation but also in general video generation.\n\nSecond, I do not think it's a proper evaluation without real STOA included for comparison. For example, on the TaiChi dataset, LVDM and VideoFusion should be compared. Moreover, VideoFusion reports better performance than the proposed method on this dataset.\n\nHe Y, Yang T, Zhang Y, et al. Latent video diffusion models for high-fidelity long video generation[J]. arXiv preprint arXiv:2211.13221, 2023.\nLuo Z, Chen D, Zhang Y, et al. VideoFusion: Decomposed Diffusion Models for High-Quality Video Generation[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023: 10209-10218."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1248/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700590109621,
                "cdate": 1700590109621,
                "tmdate": 1700590109621,
                "mdate": 1700590109621,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "l4fDhsJbbI",
            "forum": "9zHxXaYEgw",
            "replyto": "9zHxXaYEgw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1248/Reviewer_8SWB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1248/Reviewer_8SWB"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a temporal generative model LEO for synthesizing editable human performance video. The key idea is to represent motions with optical flow maps to disentangle appearances and dynamics. In particular, LEO leverages a latent diffusion model trained for predicting motions in an auto-regressive fashion, and decodes the latent to form flow maps for pixel-space appearance synthesis.\n\nTheir quantitative results show obvious improvement over prior work, with qualitative evidence demonstrating better spatio-temporal coherency."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is well-written and easy to follow, and the proposed solution sounds solid. Particularly:\n- Novel formulation of diffusion-based generative model for optical flow generations, which enables long-term motion generation\n- Explicit disentanglement of the video into appearance (pixel values) and motion (optical flow) that makes LEO better preserve the identity information in the input.\n- Auto-regressive motion generation with careful designs that achieve long-term video generation.\n\nThe quantitative and qualitative evaluations also show significant improvement over prior arts."
                },
                "weaknesses": {
                    "value": "While showing promising results, LEO has some limitations, which are also observed in other baselines:\n- Geometry ambiguity: without any explicit notion of 3D geometry or semantic features, LEO often flips or morphs the limbs from one side to the other. This is particularly obvious in the TaichiHD videos.\n- Temporal coherency: while LEO improves greatly over the other baselines compared in the paper, the appearance can still drift off/morph arbitrarily between frames, especially for videos with occlusion/dis-occlusions or large motions.\n- Limitations and failure cases: these aspects are not presented in the papers and supplementary. Proper discussions on what LEO cannot do well can help the readers to better assess the contribution of the work, and also open up possible future directions."
                },
                "questions": {
                    "value": "Below are the questions I have:\n- How does the proposed LEO compare to the approaches like Siarohin et al. 2019; 2021, where the motions are disentangled into region-based descriptors/flow-field?\n- What are the limitations of LEO? What are the failure cases? It would be great if the paper could show and discuss these topics."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1248/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698617917532,
            "cdate": 1698617917532,
            "tmdate": 1699636050999,
            "mdate": 1699636050999,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "k1SVqrBvcj",
                "forum": "9zHxXaYEgw",
                "replyto": "l4fDhsJbbI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1248/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1248/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response from the Authors"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the positive feedback, and appreciate that reviewer finds (1) our formulation is novel, (2) our appearance and motion disengagement better preserves identity, and (3) our carefully designed method achieves long video generation.\n\n**Q1. How does the proposed LEO compare to the approaches like Siarohin et al. 2019; 2021, where the motions are disentangled into region-based descriptors/flow-field?**\n\n**A1.** Firstly, Siarohin et al. 2019; 2021 proposed leveraging predicted explicit structure representations, i.e., 2D keypoints and region descriptors to generate flow maps. However, such 2D structural representations could not be well-predicted in a self-supervised learning manner. Regions and keypoints are unable to be precisely predicted for large motion, which leads to deformation in faicial structures. Instead, motion code in our method is a per-frame 1D vector. It is able to capture both local and global motion, as well as preserve the faical structure even if in long video generation. We show comparison to Siarohin et al. 2019; 2021 on image animation in updated Supplementary Material website.\n\nSecondly, as region-based descriptor in Siarohin et al. 2021 is a 2D representation. It is unclear how to use such representation to build motion models for unconditional video generation, as well as long video generation. While in our work, motion code can be easily modeled by a standard seq2seq diffusion model. \n\n**Q2. What are the limitations of LEO? What are the failure cases? It would be great if the paper could show and discuss these topics.**\n\n**A2.** In the revised manuscript, we have addressed the limitations of our approach in Appendix A.4. Additionally, we have provided updated failure cases in the Supplementary Material website for further analysis. It is worth noting that, due to the autoregressive nature of our method for long video generation, there are instances where the model may inadvertently repeat the same action in videos of considerable length. To mitigate this issue, we propose a potential solution in the form of introducing random tiny perturbations to the motion code. By incorporating these subtle variations, we aim to enhance diversity and avoid repetitive patterns in the generated videos."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1248/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700497298175,
                "cdate": 1700497298175,
                "tmdate": 1700497298175,
                "mdate": 1700497298175,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Qo2YFuBkQm",
                "forum": "9zHxXaYEgw",
                "replyto": "k1SVqrBvcj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1248/Reviewer_8SWB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1248/Reviewer_8SWB"
                ],
                "content": {
                    "comment": {
                        "value": "I would like to thank the authors for addressing my questions. \n\nReviewer NikU raised the question of insufficient quantitative comparisons against other concurrent video diffusion models, such as VideoFusion and LVDM. In my opinion, this is a valid concern, and comparisons to VideoFusion are possible, given that they also provide results on Taichi. LEO does not necessarily need to perform better than VideoFusion, given how VideoFusion uses a pre-trained image decoder (DALL-E) in their method, but it would be great to see proper comparisons and analysis against it."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1248/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700677243407,
                "cdate": 1700677243407,
                "tmdate": 1700677243407,
                "mdate": 1700677243407,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JYEdVpOc9d",
                "forum": "9zHxXaYEgw",
                "replyto": "rVESgvlJm2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1248/Reviewer_8SWB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1248/Reviewer_8SWB"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the detailed analysis and quick responses, I appreciate it.\n\nI agree with the authors that comparisons with LVDM are neither possible nor necessary as it has not yet been published. The analysis of VideoFusion also seems fair to me, especially considering that (1) LEO and VideoFusion are not designed for the same task (motion-to-video versus text-to-video), and (2) VideoFusion leverages a pre-trained decoder. I think the quantitative comparisons, as they stand now, are already sufficient."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1248/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700721044952,
                "cdate": 1700721044952,
                "tmdate": 1700721044952,
                "mdate": 1700721044952,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "N94ELIVONv",
            "forum": "9zHxXaYEgw",
            "replyto": "9zHxXaYEgw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1248/Reviewer_H4YQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1248/Reviewer_H4YQ"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a method to generate videos by disentangling the synthesis of appearance and motion. To this end, the authors propose a flow-based image animator and a latent motion diffusion model. In particular, the motion synthesis is conditioned on the starting motion code. This formulation allows the model to generate sequences of infinite length by changing the starting frame for each subsequence. The efficacy of the method is evaluated on multiple datasets of humans in motion."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The application of synthesizing videos of arbitrary length is relevant and challenging. \n- The main idea is simple and clearly presented.\n- The quantitative and qualitative results showcase the efficacy of the proposed model over the baselines on the TaichiHD, FaceForensics and CelebV-HQ datasets."
                },
                "weaknesses": {
                    "value": "- It would be nice to see some human-specific baselines, especially since the focus of the paper is on humans, e.g., utilizing skeleton/3DMM guidance.\n- I believe a comparison (or at least discussion) to video-ldm [1] would be beneficial.\n- I am missing a section on the limitations and ethical considerations.\n\n[1] Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models"
                },
                "questions": {
                    "value": "In general, I am positively inclined however I would suggest that the authors address the issues raised in the \"weaknesses\" section, especially regarding the human-specific baselines and the limitations/ethical considerations."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1248/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698838250848,
            "cdate": 1698838250848,
            "tmdate": 1699636050842,
            "mdate": 1699636050842,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mDG3E51Axi",
                "forum": "9zHxXaYEgw",
                "replyto": "N94ELIVONv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1248/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1248/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response from the Authors"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the positive feedback, in particular that (1) our task is challenging, (2) our main idea is simple and clearly presented, and (3) our quantitative and qualitative results showcase the efficacy of the method.\n\n**Q1. It would be nice to see some human-specific baselines, especially since the focus of paper is on humans, e.g., utilizing skeleton/3DMM guidance.**\n\n**A1.**  We compared our method with a 3DMM-based talking head generation method [1], as well as two diffusion-based conditional generation methods using skeleton as guidance [2, 3]. We showed several results on the Taichi and FaceForensics datasets in the updated Supplementary Material website.\n\nUpon analysis, we observed that the results of the diffusion-based methods [2, 3] were significantly influenced by the quality of the extracted poses and skeletons. In contrast, our proposed method exhibited superior visual quality and temporal coherency. Furthermore, when compared to the 3DMM-based method [1], our approach achieved notable improvements in terms of facial details and head motion.\n\n[1] Min et al., StyleTalk: One-shot Talking Head Generation with Controllable Speaking Styles, AAAI 2023\n\n[2] Zhang et al, ControlVideo: Training-free Controllable Text-to-Video Generation, arXiv:2305.13077\n\n[3] Khachatryan et al., Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators, ICCV 2023\n\n**Q2. I believe a comparison or at least discussion to video-ldm would be beneficial.**\n\n**A2.** Video-LDM is a large-scale video generation framework, which extends a pretrained Stable Diffusion with temporal modules in both VAE and 2D UNet. Video features are first extracted by VAE-Encoder,  and then used in spatio-temporal UNet to learn the latent video distribution. In Video-LDM, appearance and motion are not disentangled, and spatio-temporal distribution is jointly learned in 3D UNet. In addition, Video-LDM is only able to generate 16-frame short video clips.\n\nIn our work, we pre-trained an image animator on human face and body. LIA is a flow-based image animator which enables motion transfer from driving video to target image. An important property of proposed framework is that it supports disentagnlment of motion and appearance. Appearance is represented by the input image, and motion is represented by the generated flow maps which are controlled by the motion code in latent space. Our framework is able to produce the same input image performing diverse motion.  Since motion is only a per-frame 1D vector, one major advantage of our work compared to Video-LDM is the low cost of autoressively generating long videos (e.g., ~1 min). \n\n**Q3. I am missing a section on the limitations and ethical considerations.**\n\n**A3.** We have included the discussion of limitations in Appendix A.4, as well as the Section Ethics Statement in the revised manuscript."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1248/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700496654315,
                "cdate": 1700496654315,
                "tmdate": 1700497121041,
                "mdate": 1700497121041,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]