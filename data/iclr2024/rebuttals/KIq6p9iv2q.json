[
    {
        "title": "Towards Perpetually Trainable Neural Networks"
    },
    {
        "review": {
            "id": "rr2RtshtWC",
            "forum": "KIq6p9iv2q",
            "replyto": "KIq6p9iv2q",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7189/Reviewer_m9W8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7189/Reviewer_m9W8"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates the problem of plasticity loss in neural networks trained on nonstationary learning problems. The paper conducted a series of experiments and analyses, identifying different mechanisms of plasticity loss, including saturation of nonlinearities, dead neurons, norm growth, and output sensitivity. Consequently, the paper shows that a combination of layer normalization and L2 regularization is a robust mechanism for mitigating all the aforementioned issues. The approach has been validated in an RL setting and on the WiLDS benchmark."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The writing of this paper is very clear and easy to follow.\n- The authors conduct a fine-grained empirical analysis of plasticity loss (4 different mechanism, 3 different architectures) and provide detailed explanations on how to interpret them. \n-  The authors proposed a training protocol (LN + L2 regularization) for mitigating the plasticity loss."
                },
                "weaknesses": {
                    "value": "The contribution and novelty of the paper are a bit difficult to identify, especially considering previous work such as Lyle et al. 2023 (also cited in this paper), which indicates that factors such as dead units or weight norm may not fully explain the loss of plasticity in learning. Furthermore, normalization and L2 regularization have been studied in the past and are known as ways to prevent plasticity loss (for example, LN was studied in Lyle et al. 2023). Maybe try to extend the current work to a wider RL settings, or focusing on identify newer plasticity loss mechanisms would be better. \n\nSome of the experiments could be questionable or appear to be missing. For example, why is there no L2 regularization used in Figure 6 left (the RL experiments with C51)? Is using a small learning rate equivalent to using a large learning rate? Is using a wider network the same as using a narrower one?"
                },
                "questions": {
                    "value": "Please see weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7189/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7189/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7189/Reviewer_m9W8"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7189/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698421189834,
            "cdate": 1698421189834,
            "tmdate": 1700655803956,
            "mdate": 1700655803956,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vTZQFVN2WY",
                "forum": "KIq6p9iv2q",
                "replyto": "rr2RtshtWC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7189/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7189/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for highlighting the breadth of our empirical evaluations and the efficacy of our proposed protocol. We also appreciate the reviewer\u2019s concerns about the positioning of the paper\u2019s contributions relative to prior work and their relative novelty, and hope that our responses sufficiently address these concerns.\n\n**Concern 1.** The contribution and novelty of the paper are a bit difficult to identify, especially considering previous work such as Lyle et al. 2023 (also cited in this paper), which indicates that factors such as dead units or weight norm may not fully explain the loss of plasticity in learning.\n**AR:** We thank the reviewer for highlighting this. In light of this and other reviews, it is clear that we did not sufficiently emphasize the limitations of previous works and how this work overcomes these limitations, and we hope that we have improved the clarity of the contribution in our revisions. We provide a summary here:\n\n1) We decompose plasticity loss into four relatively independent mechanisms, and show that improvements from mitigation strategies for each mechanism are additive. **The independence of these mechanisms explains the existence of the falsifying examples found by Lyle et al.** For example, mild parameter norm growth does not tend to hurt plasticity, and unit saturation eliminates the potential for parameter growth. As a result, if a population contains some networks with saturated units and others with mild parameter norm growth, one might see a positive correlation between parameter norm and plasticity, whereas in a population without saturating units one would expect a negative correlation. Thus while Lyle et al. (2023) provide a set of negative results showing that no *single* pathology in our framework can fully explain plasticity, this work demonstrates that *combining mitigation strategies for each failure mode can yield significant gains*.\n2) We evaluate a much **more comprehensive spectrum of mitigation strategies** on a significantly broader set of tasks. We perform a thorough evaluation of varying normalization layer combinations and configurations, and further evaluate a range of strategies to control or mitigate pathological loss landscape curvature; \n3) We show that in fact the \u201closs landscape pathologies\u201d identified by Lyle et al. (2023) were largely attributable to the norm of the network outputs (see point 3 below) rather than more subtle properties of the loss landscape. Further, we show that curvature-aware optimization is not sufficient to mitigate plasticity loss, contradicting the implications of Lyle et al. \n4) Our combined strategies exhibit significant performance improvements over the single interventions studied in previous work, which was unable to completely mitigate plasticity loss in the contextual bandit setting studied in Section 3.4 of this paper, and also shows significant robustness to particularly difficult sequential supervised learning tasks in which which individual mitigation strategies exhibit declining performance. While in light of the framework provided in our paper this combination is straightforward, it is notable that to our knowledge no prior work considered the potential synergistic effects of different mitigation strategies. \n\nA read of the literature prior to this paper would suggest a picture of plasticity loss as a collection of nebulous and ill-defined pathologies of the network which can be partially resolved by disparate intervention strategies, without a clear explanation of what failure modes these interventions are rectifying. This work clearly identifies a set of relatively independent mechanisms of plasticity loss, to which interventions can be combined, and shows that a strategy of studying the effect of each mitigation strategy on its targeted pathology in isolation, then combining the most effective approaches, can yield significant gains over prior methods. We believe that this framework will allow for more effective mitigation strategies to be developed in the future, by more accurately targeting the specific mechanisms of plasticity loss which we have outlined.\n\n**Concern 2** Furthermore, normalization and L2 regularization have been studied in the past and are known as ways to prevent plasticity loss (for example, LN was studied in Lyle et al. 2023).\n\n**AR:** In fact, L2 regularization was also studied by Lyle et al., but it did not give a significant benefit to the RL tasks considered there, and it is not obvious from those results that it would give any additional benefit on top of layer normalization. The observation that weight regularization and layer normalization operate on largely independent mechanisms of plasticity loss is crucial to identify them as a mutually beneficial combination."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7189/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700257489286,
                "cdate": 1700257489286,
                "tmdate": 1700257489286,
                "mdate": 1700257489286,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "e7BRTcBb1Z",
                "forum": "KIq6p9iv2q",
                "replyto": "rr2RtshtWC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7189/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7189/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part 2"
                    },
                    "comment": {
                        "value": "**Concern 3** Maybe try to extend the current work to a wider RL settings, or focusing on identify newer plasticity loss mechanisms would be better.\nAR: Re: wider RL settings, given the context of reviewer xE98\u2019s concerns that we evaluated on only a single game, we would like to emphasize that our evaluation included all 57 games from the Atari benchmark. If the reviewer has a particular RL domain that they believe would improve the paper, we would be happy to evaluate on this domain as well. We also emphasize that our evaluation over synthetic nonstationary tasks in Figure 5 is significantly more comprehensive than prior works, including a variety of nonstationarities with varying degrees of smoothness and structure.\n\nRe: newer plasticity loss mechanisms, we emphasize that the following mechanisms and interventions are novel to this paper:\n1. In contrast to prior work, which studied individual mechanisms of plasticity loss, this work identifies **new emergent phenomena which can arise from their interaction.** For example, the interaction between **normalization layers and parameter magnitude** in the context of plasticity loss had not been previously identified and is novel to this work. \n2. This is the first paper to identify the **importance of linearized units** in the loss of plasticity in neural networks (c.f. Appendix D.7 for a more detailed discussion of this pathology).\n3. The **role of regression target magnitude on network plasticity** had not previously been identified. With the addition of the dose-response curves we have provided in Appendix D (Figures 12 and 13), we are the first to demonstrate a direct relationship between the magnitude of the prediction target bias and the ability of the network to adapt to new tasks.\n4. The label smoothing strategy we use to avoid saturation of the softmax output layer in the RL agents which employ the \u2018two-hot\u2019 trick is critical for avoiding plasticity loss in this domain, and had not been identified in prior works, which were unable to eliminate plasticity loss even with this output reparameterization.\n\n**Q1.** Some of the experiments could be questionable or appear to be missing. For example, why is there no L2 regularization used in Figure 6 left (the RL experiments with C51)? \n**AR:** We initially did not include L2 regularization in our experiments due to general consensus in the RL community that this type of regularization tends to slow down learning. We had also observed in prior experiments that weight norms do grow in the atari benchmark, but not to the extent that was necessary to cause performance degradation in the sequential supervised learning tasks. We have included additional results in Appendix D illustrating the effect of layer normalization in a subset of the full Atari benchmark (Figure 15), which confirms that a) weight norm growth is modest compared to the values observed in image classification domains and b) overly aggressive L2 regularization inhibits performance in this benchmark.\n\n**Q2.** Is using a small learning rate equivalent to using a large learning rate? Is using a wider network the same as using a narrower one?\n**AR:** Prior work has noted that wider networks and smaller learning rates tend to be more robust to plasticity loss (Lyle et al., 2023). We conducted a coarse sweep over width and depth for some architectures in the appendix of the original submission (Figure 11). The role of learning rate is also critical: lower learning rates tend to exhibit less plasticity loss, but also slower convergence. Recent related work (Wortsman et al., 2023; \u201cSmall-scale proxies for large-scale Transformer training instabilities\u201d)  also suggests that larger learning rates in smaller models may replicate some instabilities seen in larger models in stationary learning problems. It is possible that analogous results can be obtained in the context of nonstationary training regimes, whereby model scale and learning rate can be calibrated to induce similar failure modes in models of varying sizes. We think this is an exciting area for future research, though it lies outside the scope of this paper to study in full.\n\nWe thank the reviewer for their time and are happy to discuss any additional or remaining concerns in light of our response."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7189/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700257760614,
                "cdate": 1700257760614,
                "tmdate": 1700257760614,
                "mdate": 1700257760614,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Dlg7msfOpG",
                "forum": "KIq6p9iv2q",
                "replyto": "rr2RtshtWC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7189/Reviewer_m9W8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7189/Reviewer_m9W8"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors,\n\nThank you very much for your response and the additional information. I acknowledge that I have reviewed the rebuttal. This additional information makes the paper more complete, and hence I have raised my score to reflect that. However, I still think the contribution and novelty are limited compared to prior work.\n\nBest,"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7189/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700655786638,
                "cdate": 1700655786638,
                "tmdate": 1700655786638,
                "mdate": 1700655786638,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CxUj29RZ8n",
            "forum": "KIq6p9iv2q",
            "replyto": "KIq6p9iv2q",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7189/Reviewer_yXsB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7189/Reviewer_yXsB"
            ],
            "content": {
                "summary": {
                    "value": "The paper describes the ways to overcome degradation of training performance of the neural networks due to loss of plasticity.  The authors identify a number of reasons for the loss of training plasticity: preactivation shift, dead and effectively-linear units, norm growth, and shifts in the target function, and propose the simple ways to mitigate these phenomena.  The authors study the proposed interventions, based upon L2 regularisation and layer normalisation in two settings: deep reinforcement learning and classification"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- It is very important to address the well-documented (as it is well-supported by the references in the paper) phenomenon of the lack of plasticity\n\n- the paper is well motivated and clearly written, however, there are some questions to be resolved on quality and clarity of some particular aspects (see weaknesses below)\n\n- originality: the work builds upon existing well-known methods such as L2 regularisation and layer normalisation, but builds new insight how these remedies could help solve the problem of learning from non-stationary data\n\n- significance: it can help learn from non-linear data streams, both in reinforcement learning and classification scenarios\n\n- the paper discusses in detail the experimental conditions and therefore, addresses reproducibility well"
                },
                "weaknesses": {
                    "value": "Cons:\n- Clarity aspects: it is important to distinguish between the aspect of nonstationarity and continual learning, see Q1; the relative performance metric needs to be clarified, see Q2\n\n- Quality aspects: see Q3-Q6\n\n- Originality aspects: While the paper researches extensively into reasons of loss and plasticity, the remedies are based on well-known methodologies (L2 regularisation, layer normalisation) and therefore the message is mostly limited to identifying the remedies as opposed to novel methodological contributions towards solving this problem (see Q7)"
                },
                "questions": {
                    "value": "Q1. In the introduction, the paper discusses at length the problem of nonstationarity and continual learning at the same time. Although these problems are connected, I think it is important to distinguish between these two. With the problem of nonstationarity alone, one can see the catastrophic forgetting problem entangled with the plasticity loss. The introduction says, for example: \u201cOur empirical evaluations aim to split the philosophical difference between these perspectives. In the following sections, we will evaluate the ability of a learning algorithm to maintain plasticity in a continual supervised learning task, where we fix some image classification dataset (in our case, CIFAR10) and at fixed intervals apply a random transformation to the inputs and labels.\u201d This difference can be illustrated as follows (the example is a toy illustration of a distinction). Imagine that we have trained a model on 180 degrees rotated MNIST symbols. Then we realised that it was wrong and we actually want it to work on non-rotated ones. We decided to continue training from the last checkpoint where we stopped training for 180 degrees rotated MNIST symbols. At the end, we realise that the performance of the model dropped by X per cent due to the problems in Figure 1. In this case, we are happy with catastrophic forgetting (and don\u2019t aim for lifelong learning), but we are not happy with the loss of performance. To address this point, two actions are suggested: (1) disentangling distribution nonstationarity problem in the introduction (2) calculating the baseline of the last task and all tasks performance: in Figure 6, how would the proposed changes affect the performance if we learnt the whole WiLDS dataset in a stationary distribution fashion? How would the proposed changes affect the performance on just the last task?\n\nQ2. In Figure 6, not sure I understand how is the relative performance calculated? Is it calculated for the most recent task, for all tasks, and relative to what?\n\nQ3. Figure 2, top left: should the axes have values? Does it also make sense to report fraction, not the absolute number?\n\nQ4. \u201cMild L2 regularization (we use a value of 1e-5) is sufficient to resolve this issue in all three architectures.\u201d Why this choice of the hyperparameter?\n\nQ5. *\u201cthis analysis will provide a novel explanation of why units die off.\u201d* Although I see the authors point, I\u2019m not sure it\u2019s an entirely correct description of the contribution. Sections 3.1 and 3.2 indeed shed the light on the process of dying out, but they do so through summarising evidence from existing works, i.e. Lin et al, 2016. What is the contribution is that the authors summarise a number of reasons for loss of plasticity and give the pathways towards improving plasticity.  Or am I missing anything?\n\nQ6. \u201cWhile saturated units have received much attention as a factor in plasticity loss, the accumulation of effectively-linear units has not previously been studied in the context of continual learning.\u201d Not sure it fully covers such accumulation in the context of continual learning, as I mentioned in Q1 as the task itself does involve learning in face of nonstationarity but not tackling the question whether and how the previously accumulated knowledge should be used (or discarded). Would it be possible to contrast the added aspects of continual learning to previous works, such as Montufar (2013), in this analysis? \n\nQ7: Does different types of layer normalisation (batch normalisation, layer normalisation provide different impact?) Did the authors compare between those different remedies?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7189/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698691890332,
            "cdate": 1698691890332,
            "tmdate": 1699636853613,
            "mdate": 1699636853613,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Ep3WMuXaRb",
                "forum": "KIq6p9iv2q",
                "replyto": "CxUj29RZ8n",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7189/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7189/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their detailed comments, and for highlighting the importance of the paper topic and insight provided by our analysis. We address specific concerns below:\n\n**Concern 1.** While the paper researches extensively into reasons of loss and plasticity, the remedies are based on well-known methodologies (L2 regularisation, layer normalisation) and therefore the message is mostly limited to identifying the remedies as opposed to novel methodological contributions towards solving this problem (see Q7)\n\n**AR:** We agree that the components of the most effective strategy we identify are already widely known. However, we disagree that this reduces the significance of the contribution: first, as noted in other responses, we emphasize that the primary contribution of this work is the observation that plasticity loss can be decomposed into largely-independent mechanisms, and that mitigation strategies on these mechanisms can be combined. The observation that mechanisms of plasticity loss can be studied in isolation and then combined together significantly reduces the combinatorial complexity of the search for even better plasticity-preserving training algorithms. Second, we note that this unoriginal combination outperforms a number of more \u201cnovel\u201d methods which we evaluated and which have been previously published. Showing that a simple existing method can be adapted to outperform newer more complex approaches is in our opinion an important regularizer in the complexity of machine learning algorithms; indeed, we tried a variety of more \"novel\" approaches (e.g. the normalization strategy in Appendix D, Figure 21) which did not significantly outperform the simple combination of LN + L2 and omitted these from the main paper to simplify the exposition of the work. Finally, the framework we use to select these interventions (the mechanisms presented in Figure 1) can be leveraged to find even more effective solutions: for example, while we use L2 regularization due to its simplicity, it is possible that a more careful variant of weight normalization or regularization towards the initial parameter values may interfere less with convergence speed in single tasks.\n\n**Q1.** Distinguish nonstationarity vs continual learning. With the problem of nonstationarity alone, one can see the catastrophic forgetting problem entangled with the plasticity loss. 2 suggestions: (1) disentangling distribution nonstationarity problem in the introduction (2) calculating the baseline of the last task and all tasks performance in Figure 6, include baseline of training on whole distribution + baseline of training random init each time dataset reshuffled.\n\n**AR:** We use the terms \u201cnonstationary\u201d and \u201ccontinual\u201d learning interchangeably in this work, and the reviewer raises a valid concern that continual learning problems often evaluate the learner on not just forward transfer, but also on backward transfer to previously seen data (or in its absence, catastrophic forgetting). We agree that this is confusing, and will update the manuscript to ensure that when we evaluate methods on forward-looking performance, we use the term \u201cnonstationary learning\u201d. We have also implemented a baseline for the WiLDS benchmark which resets the network parameters each time the dataset changes, which is included in our revisions (Appendix D, Figure 18).\n\n**Q2.** In Figure 6, how is the relative performance calculated?\n**AR:** We normalize by the human scores used widely in the literature and established by Mnih et al., (2015).\n\n**Q3.** Figure 2, top left: should the axes have values? Does it also make sense to report fraction, not the absolute number?\n**AR:** Having reviewed the figure, the only missing value is the x-axis in the middle subplot, which corresponds to training steps. While switching between fraction and absolute number of dead units does not change the overall trend, we agree that since the total number of units isn\u2019t obvious from the figure, it is clearer to set the y-axis ticks to be a fraction. We will make this change in our future revisions.\n\n**Q4.**  \u201cMild L2 regularization (we use a value of 1e-5) is sufficient to resolve this issue in all three architectures.\u201d Why this choice of the hyperparameter?\n**AR:**  The value of 1e-5 was chosen because it was the first value we tried and seemed to work reasonably well. In response to this and other reviews, we ran a more detailed sweep over L2 penalties for the MLP and found that in fact a slightly smaller value of 1e-6 results in higher per-task performance while still avoiding plasticity loss in later tasks, this can be seen in Figure 16 in Appendix D."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7189/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700256979594,
                "cdate": 1700256979594,
                "tmdate": 1700256979594,
                "mdate": 1700256979594,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rjbOnfnWWU",
                "forum": "KIq6p9iv2q",
                "replyto": "CxUj29RZ8n",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7189/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7189/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer yXsB (Part 2)"
                    },
                    "comment": {
                        "value": "**Q5.** \u201cthis analysis will provide a novel explanation of why units die off\u201d but they do so through summarising evidence from existing works[...] Or am I missing anything?\n\n**AR:** We thank the reviewer for highlighting that our initial visualization of the accumulation of dead units was not sufficiently clear. Due to space constraints, we were only able to provide a coarse overview of a nuanced chain of events. In light of this and other reviewers\u2019 comments, we have provided a more detailed visualization of the network trajectory after a task change to illustrate at a step-by-step level how units become saturated. In brief, our explanation consists of 4 observations. \n1) immediately after a task change, the network aims to *increase the predictive entropy* of its outputs. \n2) in practice this is done by *reducing the norm of the features*. \n3) Gradients which reduce the norm of the features have the effect of *reducing the pre-activation value for most inputs*. \n4) If the optimizer state is not reset, then outdated second-order estimates cause large update steps. *Large update steps in a direction which push down pre-activations can result in unit death*, with all pre-activations becoming negative within a handful of steps. While we attempted to provide a summary of this chain of events in the text of Section 3.1, we agree that it was not sufficiently clear. We illustrate this phenomenon in Appendix D where each step in this chain is illustrated. \n\n**Q6.** \u201cWhile saturated units have received much attention as a factor in plasticity loss, the accumulation of effectively-linear units has not previously been studied in the context of continual learning.\u201d Issue with saying \u201ccontinual\u201d rather than \u201cnonstationary\u201d learning. Would it be possible to contrast the added aspects of continual learning to previous works, such as  uMontufar (2013), in this analysis?\n**AR:** Montufar (2013) describes the piecewise linear structure of a neural network assuming ReLU activations, and uses this structure to understand the expressivity of the model. In particular, the focus is on counting the number of linear regions, and providing a specific construction for deep architectures that maximizes the potential number of regions that you get. In this work, **we are interested in learnability rather than expressivity**. We will update the paper to clarify this. In our revisions to Appendix D, we provide a simple demonstration that a completely linearized network can encounter training difficulties (Figures 19 & 20). However, in addition to this example, it is possible to reason more generally about the effect of linearized units on training dynamics with a simple thought experiment, which we include in a child comment. \n\n**Q7:** Do different types of layer normalisation (batch normalisation, layer normalisation provide different impact?) Did the authors compare between those different remedies?\n\n**AR:** We provide one data point on the interaction between layer normalization and batch normalization in Figure 4 of the main paper. However, we also conducted preliminary experiments contrasting the effects of different normalization choices by isolating the effects of mean subtraction and division by standard deviation for both layer and batch normalization, and also studying the interaction between performing different normalization operations on different axes. We did not originally include these in the paper due to space constraints, but we have added a section discussing them and visualizing these results in Figure 21 of Appendix D."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7189/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700257154813,
                "cdate": 1700257154813,
                "tmdate": 1700257216098,
                "mdate": 1700257216098,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ukfrQgSv47",
                "forum": "KIq6p9iv2q",
                "replyto": "CxUj29RZ8n",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7189/Reviewer_yXsB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7189/Reviewer_yXsB"
                ],
                "content": {
                    "title": {
                        "value": "Response on the rebuttal"
                    },
                    "comment": {
                        "value": "Dear authors,\n\nFirst of all, I would like to thank you for a very thorough response to the comments and concerns by multiple reviewers. Sorry for the late response, this is because carefully checking the details of the discussion has taken quite a bit of time.  The revisions of the paper, in my opinion, resulted in a stronger submission. I have checked the responses to myself as well as to other reviewers, and below is my answer to the summary:\n- contribution: the authors stressed that the contribution is not in identifying individual causes of loss of plasticity which have been separately known. In the original review, I mentioned that there is a merit of originality, so I did not actually mean that such empirical analysis of mitigation strategies is not original, it was more a request to clarify the scope, which the authors have done in the revision. As I said in the review, \"the work builds upon existing well-known methods such as L2 regularisation and layer normalisation, but builds new insight how these remedies could help solve the problem of learning from non-stationary data\". The authors further expanded on the thesis stating that these individual causes are found to be largely independent. More precisely, the authors state that *'the work builds upon existing well-known methods such as L2 regularisation and layer normalisation, but builds new insight how these remedies could help solve the problem of learning from non-stationary data'*. The empirical work, together with a careful empirical support of his thesis, would clarify that it is the independence of mechanisms that is the contribution of the paper. However, see follow-up concern 1 which still needs to be clarified.\n- nonstationary vs continual learning: this concern seem to be explained thoroughly in the rebuttal, which alleviates the original concern. \n- relative performance evaluation: this clarifies on the methodology of evaluation. \n-batch vs layer normalisation: this clarifies upon the original concern.\n\n\nFollow-up concern 1:\n- It does indeed now follow from the paper (see, e.g., Figure 3, Figure 16) that combining the plasticity loss mitigation strategies addresses the issue of plasticity loss; however, is there sufficient evidence to claim that \u201cplasticity loss can be decomposed into largely-independent mechanisms\u201d or should the evidence allow to just state as it is said in the paper: \u201cwe find that addressing all mechanisms in conjunction is sufficient to yield negligible loss of plasticity in a variety of synthetic benchmarks.\u201d ? These two claims are different. The second claim looks sufficiently well-supported, while I am not sure about the first one as I am not sure whether it is demonstrated (or how it could be demonstrated that these mechanisms are independent).  I will update the scores if the authors could clarify upon it."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7189/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700563420555,
                "cdate": 1700563420555,
                "tmdate": 1700576623739,
                "mdate": 1700576623739,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "k6JcA4KP6Y",
            "forum": "KIq6p9iv2q",
            "replyto": "KIq6p9iv2q",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7189/Reviewer_xE98"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7189/Reviewer_xE98"
            ],
            "content": {
                "summary": {
                    "value": "Paper presents several failure modes of nonstationary learning, and some solution for mitigating loss of plasticity in neural networks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Paper studies an interesting and important question - why do neural networks lose plasticity and how to mitigate this problem?\n- Includes clear visualizations which summarize experimental findings nicely\n- Thoroughly evaluates a variety of different solutions for solving the plasticity problem -- I especially appreciate the breadth of different solutions that were tried in section 4"
                },
                "weaknesses": {
                    "value": "Sec 1: Abstract/ Introduction\n- Paper claims \"[we] identify four primary mechanisms by which neural networks lose plasticity: unit saturation, preactivation distribution shift, unbounded parameter growth, and loss landscape pathologies induced by the network outputs\". I believe this is overstating the contributions of the paper, as a number of these mechanisms have previously been identified in prior work. \n\nSec 2: Related work\n-  I think section 2.2 (Loss of Plasticity) needs to be much more specific about the contributions in the prior work, and to more clearly delineate how the contributions of this work differs. In its current form, this section only summarizes how prior work characterize the concept of plasticity in a very abstract manner, without mentioning any specific differences between this work and prior work. However, many of the ideas presented in this paper are very similar to ones already existing in prior work (especially Lyle 2021, Kumar 2023a, Abbas 2023), and it would be helpful for understanding the novel contributions of this paper beyond prior work if it were described in more specific terms\n\nSec 3: Failure Modes of Nonstationary Learning\n- Sec 3.1: while prior works has already identified saturated activations to be a reason for loss of plasticity, this paper claims to provide a \"novel explanation of why units die off\". However, it is not clear how the experiments in this section supports any novel explanations. More specifically, Fig 2 show that at the beginning of learning a new task, many network representations have negative dot products with all network inputs, while after a few hundred steps, this leads to many dead neurons. This experiment alone demonstrates that saturated activations is a problem for nonstationary learning, but does not provide insights into why. The authors hypothesize (in text) that this behavior may be related to de reduction in confidence on the new task, followed by an increase in confidence after learning the new task, but no evidence was provided for this explanation. \n- Sec 3.2: in this section, the paper claims that another reason for the loss of plasticity is the linearization of units. However, there are *no experiments* in this section. Therefore, there is no evidence that this phenomenon happens in practice. Furthermore, the paper itself noted \"recovery from this state is possible\", so even if it does happen in practice, the network could potentially quickly recover from this state, making it even more important to empirically understand to what extent this \"plight\" is a problem in practice\n- Sec 3.4: this section, the paper hypothesize that another reason for the loss of plasticity is that \"regression to targets which have a large mean relative to their variance is innately difficult for neural networks\". However, the experiments do not necessarily support this hypothesis. Instead, the experiments show that using distributional losses seem to mitigate loss of plasticity, which is a finding that has already been presented in prior work. While the paper's hypothesis could potentially explain why distributional losses work well, there are a number of other potential explanations, and the paper provide no additional evidence for why the proposed hypothesis is the correct one. \n\nSec 5: Natural Non-stationarities\n- The paper is not specific enough about the exact environments/datasets being evaluated. In particular, the paper says it \"trained on the arcade learning environments\", as well as \"a dataset from the WiLDS benchmark\". However, the arcade learning environment includes over 50 games, and the WiLDS benchmark includes 10 different datasets. For the RL experiment, it is unclear whether the experiment is only trained on one environment (if so, which one?), or if the results are averaged over training runs on many different environments (if so, which ones?). Furthermore, the paper should be more specific about which particular dataset it used from the WiLDS benchmark for its experiment. \n- Assuming the paper only evaluated on 1 RL environment, I believe this, in addition to only 1 distribution shift dataset, is not extensive enough to validate the efficacy of the proposed approach on realistic learning problems\n- The proposed approach is using layer norm with L2 regularization. Why does the RL experiment only compare C51 with layer norm and C51 without layer norm? What about L2 regularization? Even if the L2 regularization doesn't help, or even hurts performance, I think it would \nbe informative to the reader\n\nOverall, I think this paper has some interesting insights. However, in its current, I believe this paper makes a number of claims which are unsubstantiated by the experimental results."
                },
                "questions": {
                    "value": "See weaknesses section for questions"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7189/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7189/Reviewer_xE98",
                        "ICLR.cc/2024/Conference/Submission7189/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7189/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698791045277,
            "cdate": 1698791045277,
            "tmdate": 1700598260973,
            "mdate": 1700598260973,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OSYxPTVyp1",
                "forum": "KIq6p9iv2q",
                "replyto": "k6JcA4KP6Y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7189/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7189/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xE98 (Part 1)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for highlighting the importance of the topic area and thoroughness of our evaluations. We also thank the reviewer for highlighting a number of places in the paper where we did not sufficiently convey the novelty of our findings or their positioning to prior work. We are still working on implementing all of these suggested improvements in the exposition of the text, but a preliminary update to the introduction can be seen in the revised pdf. We also thank the reviewer for highlighting points where we did not provide sufficient experimental evidence to be completely convincing \u2013 in most cases, the requested experiments had either already been completed and had not been deemed important enough to include in the paper, or were straightforward to run during the rebuttal period, and we have included a significant number of supporting results in Appendix D of the revised pdf. We address the reviewer\u2019s concerns point by point. Note **AR** = author response.\n\n\n**Concern 1:**  \"I believe this is overstating the contributions of the paper, as a number of these mechanisms have previously been identified in prior work.\" **AR:** We agree that some pathologies such as dead units have been identified by previous works. What distinguishes our work is the identification that these mechanisms are largely independent, and that their mitigation strategies can be combined. For example, we disentangle preactivation shift from saturated nonlinearities, whereas prior works viewed these as the same phenomenon. However, avoiding dead units by using a nonsaturating nonlinearity does not mitigate plasticity loss caused by preactivation shift, and previous works which studied unit resets and nonsaturating nonlinearities could not account for this more subtle pathology. We have updated Section 1 to express this more explicitly. \n\n**Concern 2:** Section 2.2 needs to be much more specific about the contributions in the prior work, and to more clearly delineate how the contributions of this work differs. \n**AR:** We thank the reviewer for highlighting this pitfall, and will incorporate into our revisions. As an overview, while our work shares many similarities with that of Lyle et al. (we consider overlapping mitigation strategies and use similar benchmarks), we believe that this paper makes a significant contribution to the literature beyond that provided by prior work. In particular: \n\na) We show that **mitigation strategies to independent mechanisms of plasticity loss can be combined** to significantly outperform single interventions. Prior work did not consider that interventions may operate on independent mechanisms and benefit from combination. This insight not only explains the falsification results of Lyle et al., who show that single mechanisms fail to explain plasticity loss in all settings, but also provides a more efficient framework for searching for new plasticity preserving optimization methods. The approach we follow, of isolating individual mechanisms, identifying effective interventions on these mechanisms, and then combining the best of each class, significantly reduces the combinatorial complexity of searching over training improvements.\n\nb) Our combined strategies exhibit significant performance improvements over the single interventions studied by Lyle et al. and also shows significant robustness to particularly difficult sequential supervised learning tasks in which which individual mitigation strategies exhibit declining performance.\n\nc) We provide a finer-grained view into the mechanisms of plasticity loss than that provided by previous works, identifying the importance of controlling for preactivation shift and unbounded norm growth. The benefits of layer normalization have been validated in many prior works, but these works **did not provide an explanation of its benefit**. This mechanistic view also sheds light onto the falsification experiments of Lyle et al., giving intuition behind the inefficacy of weight norm in networks with saturated units.\n\nd) we evaluate a much **more comprehensive spectrum of mitigation strategies** on a significantly broader set of tasks. We perform a thorough evaluation of varying normalization layer combinations and configurations, consider recently proposed interventions such as ReDO (Sokar et al., 2023) and Regenerative Regularization (Kumar et al., 2023) and further evaluate a range of strategies to control or mitigate pathological loss landscape curvature.\n\ne) We show that in fact the \u201closs landscape pathologies\u201d identified by Lyle et al. (2023) were largely attributable to the norm of the network outputs (see point 3 below) rather than more subtle properties of the loss landscape. Further, **we show that curvature-aware optimization is not sufficient to mitigate plasticity loss, contradicting the implications of Lyle et al.**"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7189/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700255964055,
                "cdate": 1700255964055,
                "tmdate": 1700255964055,
                "mdate": 1700255964055,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "12AE76mFUt",
                "forum": "KIq6p9iv2q",
                "replyto": "k6JcA4KP6Y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7189/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7189/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xE98 (Part 2)"
                    },
                    "comment": {
                        "value": "**Concern 3** (Sec 3.2): No experiments on linearization of units. Furthermore\u2026  the network could potentially quickly recover from this state\n\n**AR:** Experiments studying the linearization of units are included in the right hand side of Figure 4, which illustrates how networks trained with non-saturating activations accumulate zombie units in tandem with reduced performance on future tasks, and this accumulation is more pronounced in networks that lose plasticity. While the network could in theory recover from unit linearization, we see in many cases that it does not. Particularly striking is the case of residual networks without layer normalization, where we see that nearly all units are computing linear functions, and for which training curves often never pick up. This can be observed in Figures 19 and 20 in Appendix D. We also note that Appendix C already studies the relationship between plasticity and the variance of the slope of nonlinearities on the preactivation distribution, a quantity which is closely connected to the number of linearized units. We think that further investigation into the effect of these linearized units on optimization dynamics is an exciting direction for future research, potentially drawing on the analysis of Poole et al. and Martens et al. to characterize the feature geometry and gradient dynamics in this network state, and hope that our findings serve as inspiration for further analysis.\n\n**Concern 4 (Sec 3.4):** \"regression to targets which have a large mean relative to their variance is innately difficult for neural networks\" hypothesis not supported by experiments. Show that using distributional losses seems to mitigate loss of plasticity, already known. No justification for why this is the right explanation, and not just that distributional losses are better.\n\n**AR:** The sweep used in Section 3.4 over the two values of gamma provides evidence supporting our claim that it is the higher target mean, rather than some generic benefit of distributional losses, which drives performance gains. Note in particular that the distributional losses with gamma=0 do *not* improve plasticity over regression losses, suggesting that improvements are not coming from uniform superiority of distributional losses. To illustrate the importance of the target offset value at a finer-grained level of detail, we provide dose-response curves for a MLP trained to regress on targets with varying means and then fine-tuned on a new task in Appendix D, Figure 12. In particular, we see a monotone relationship between the magnitude of the pretraining regression problem offset and the error attained on later tasks.\n\n**Concern 5 (Sec. 5)**: For RL: what exact games are being evaluated? Which dataset from WiLDS benchmark? \u201cAssuming the paper only evaluated on 1 RL environment, I believe this, in addition to only 1 distribution shift dataset, is not extensive enough to validate the efficacy of the proposed approach on realistic learning problems\u201d\n\n**AR:** While the arcade learning environment is only a single benchmark, it contains a rich diversity of games which require complementary algorithmic strengths in order to master; we emphasize that **we evaluate on all 57 games**, and so while we agree that more evaluations always provide more information, it was not clear to us what additional information we would expect to gain from an additional environment. If the reviewer has particular domains outside of Atari that they would like to see evaluated, we would be happy to discuss further. We use the iwildcam dataset from the WiLDS benchmark, as was described in Appendix B.\n\n**Concern 6:** Why no L2 regularization in RL experiments?\n\n**AR:**  Our evaluations on the image classification setting revealed that growth of the parameter norm only begins to interfere with learning when it increases by several orders of magnitude (e.g. from O(10^3) to O(10^7+). While the parameter norm of DQN-style agents on the Atari domain is known to grow, it does not increase by five orders of magnitude (c.f. Appendix C for an illustration of parameter growth in the game Seaquest). L2 regularization is known to slow down training in RL and make it difficult for agents to take off, so we did not anticipate seeing any benefits. For completeness, we provide preliminary results for the C51 agent with L2 regularization on a subset of environments from the Atari benchmark in our updates to the paper (we alphabetized the environments and selected every 5th game), which confirm that L2 regularization does not improve performance in the C51 agent.\n\nWe think the narrative around as well as the experimental support for the contributions of the paper have been significantly strengthened by the revisions we have made in response to these comments. If the reviewer has any additional concerns or feels that their original concerns were not sufficiently addressed, we would be eager to discuss further."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7189/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700256224236,
                "cdate": 1700256224236,
                "tmdate": 1700258262795,
                "mdate": 1700258262795,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tTqhyKPbcz",
                "forum": "KIq6p9iv2q",
                "replyto": "12AE76mFUt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7189/Reviewer_xE98"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7189/Reviewer_xE98"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "I appreciate the detailed response from the authors. Many of my questions have been resolved, and I have raised my score. However, I think this paper could benefit from another round of peer review. In the current state, many of the new results are not integrated in the main body of the paper and the related works section has not been updated, and I believe there is much room for improvement in terms of clearly explaining the contributions as well as how the experiments support the claims being made."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7189/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700598245523,
                "cdate": 1700598245523,
                "tmdate": 1700598245523,
                "mdate": 1700598245523,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "guWf16G0nf",
            "forum": "KIq6p9iv2q",
            "replyto": "KIq6p9iv2q",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7189/Reviewer_o35T"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7189/Reviewer_o35T"
            ],
            "content": {
                "summary": {
                    "value": "This paper provides a good, deep analysis of the loss of plasticity. It provides four mechanisms that lead to loss of plasticity. This leads to the proposal of a training protocol that combines layer normalization, l2 regularization, and scale-invariant output parametrization. The effectiveness of these methods is shown in various supervised continual learning tasks and in some naturally arising non-stationary problems."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The paper is well-written and easy to read, for the most part. \n\nThe analysis of loss of plasticity is well done and provides various insights into the phenomenon. The proposed methods are well motivated by these insights, and they seem largely effective on a wide variety of problems."
                },
                "weaknesses": {
                    "value": "Although the paper contains good ideas, there are a couple of major issues that stop me from recommending a full acceptance at the moment.\n\n- **Misleading conclusions**\n    - The paper's conclusion says, \" ... This paper has resolved a critical first step towards robust continual learning algorithms: ensuring that neural networks maintain their ability to learn over time. ...\". The algorithms (Layernorm + L2) they propose maintain plasticity but at the expense of peak performance, so it is unfair to say that the proposed algorithm has \"resolved a critical first step .\"The goal of solving plasticity loss is not to just develop algorithms that maintain a constant level of performance across tasks, but *to develop an algorithm that will maintain the best possible performance for the network.* Consider the extreme case; an algorithm has a learning rate of 0. Clearly, this algorithm maintains plasticity, but it is the worst-performing algorithm. The results in the top-left panel of Figure 3, as well as in Figure 8, show the performance of L2+LayerNorm is worse than the peak of just LayerNorm. L2 is helping maintain plasticity but at the expense of peak performance. I think the conclusion of the paper should be changed to reflect that the proposed algorithms maintain plasticity but sometimes at the expense of peak performance. \n    - In the same vein as the point above, the use of classification loss in a regression problem increases the minimum loss that the network can get to because there will be a residual loss for each data point. Even though we might be able to maintain plasticity with a classification loss, it comes at the expense of the best possible performance. \n    - At many points, the paper says the previously proposed methods \"slow learning in the single-task setting.\" This suggests that the proposed method has no issues in the single-task setting. But again, that is not true. The proposed method can lead to sub-optimal performance in the single-task case (top-left panel of Figure 3, Figure 8)\n- **Missing details of empirical evaluation.** The paper does not provide any details on the number of runs for each experiment, nor does it say what is the shaded region in each plot. It is unclear at the moment if any of the results in the paper have any statistical significance and if they will withstand the test of time."
                },
                "questions": {
                    "value": "- What exactly is scale-invariant output parameterization? This term is used at multiple points in the paper, but it is never explicitly defined. Is it just the conversion to classification loss using the 'two-hot' trick described in section B.3?\n- What is \"smoothing\" on the right side of Figure 3? Similarly, what is \"normalization\"? Is it just layer norm or something else?\n- $\\gamma=0$ fails in Figure 3. The paper attempts to describe what is happening there. But I do not understand why the line for $\\gamma=0$ loses plasticity.\n- Can you please show a zoomed-in image of the blue line ($\\gamma=0.99$) of the bottom left plot on the right side of Figure 3? It seems that the loss is increasing for the blue line, but it is unclear from the current image. And if possible, can you run this experiment for ten times longer? If the loss is increasing, a longer experiment will make it clear.\n- Why wasn't L2 used in the RL experiments with C51? Is it already included in C51, or did it hurt performance?\n- Which figure in the main paper does Figure 8 correspond to?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7189/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7189/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7189/Reviewer_o35T"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7189/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698796897352,
            "cdate": 1698796897352,
            "tmdate": 1699636853353,
            "mdate": 1699636853353,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qRQ2wJoz1a",
                "forum": "KIq6p9iv2q",
                "replyto": "guWf16G0nf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7189/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7189/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer o35T (Part 1)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their thoughtful comments. We appreciate the assessment that our analysis \u201cis well done and provides various insights into the phenomenon\u201d, and that our proposed methods are \u201cwell movitated\u201d and \u201clargely effective\u201d. We now address the reviewer\u2019s concerns. Note: AR = Author Response.\n\n**Concern 1:** sub-optimal performance of weight decay.\n\n**AR:**  We agree with the reviewer on the importance of distinguishing between 'not reducing performance' and 'consistently exhibiting good performance', and that L2 regularization can slow down learning in some domains and architectures. We plan to include a more detailed discussion of tradeoffs between performance on a single task and stability in later revisions. The random label memorization task presents a particularly adversarial realization of this tension, as the primary task amounts to memorizing random noise, and L2 regularization tends to have a smoothing effect and makes it more difficult to memorize noisy functions. However, we also note that ultimately this slowdown does not seem to prevent the network from eventually converging on the task -- it just increases the number of optimizer steps necessary to achieve this. To back up this claim, we have re-run a subset of the experiments originally shown in Figure 8 where we provide a slightly longer training interval between label resets, and this longer interval allows both the regularized and unregularized networks to attain >98% accuracy on the task, which can be seen in Figure 16 of Appendix D.  We have also included a sweep over L2 penalty values and activation functions for the CNN and MLP architectures. These results show that the value of 1e-5 which we defaulted to in the paper is suboptimal for certain architecture-dataset combinations, and in some architectures a different value (1e-6 in this case) can recover the stability benefits while also exhibiting minimal performance degradation relative to the network\u2019s unregularized counterpart. \n\nAs a result, we do think that L2 regularization at a suitable dose is a valid tool for maintaining plasticity. Our perspective can be summarized as follows: **while L2 regularization can slow down training if applied too liberally, in can be titrated to minimally interfere with performance on the current task while still providing benefits to plasticity by containing otherwise-unbounded parameter growth**. We will ensure that this perspective is effectively conveyed in our future revisions.\n\n**Concern 2:** The use of classification loss in a regression problem increases the minimum loss that the network can get to because there will be a residual loss for each data point. Even though we might be able to maintain plasticity with a classification loss, it comes at the expense of the best possible performance.\n\n**AR:** We are not entirely sure what residual the reviewer is referring to: the lower bound on the cross-entropy loss of the target entropy, or approximation error induced by the binning representation. In both cases, we do not believe that this presents a barrier to the network\u2019s ability to match the desired scalar targets. In the case of the lower bound on the cross entropy, we note that this does not influence optimization dynamics. In the case of expressivity of the binning trick, the distributional output representation does not reduce the expressivity of the network, as any scalar value which a regression model can output, provided it is within the bounded range on which the support of the two-hot distribution is defined, can be represented by a corresponding two-hot distribution (see child comment for details).\n\n**Concern 3:** missing evaluation details.\n\n**AR:** Most tasks run with 3-5 seeds per hyperparameter setting; we will specify these details in our revisions. Shaded regions refer to the standard deviation over the training runs exhibiting the listed hyperparameter values."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7189/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700254840739,
                "cdate": 1700254840739,
                "tmdate": 1700254840739,
                "mdate": 1700254840739,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xqhs85FyZy",
                "forum": "KIq6p9iv2q",
                "replyto": "guWf16G0nf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7189/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7189/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answers to Questions"
                    },
                    "comment": {
                        "value": "**Question 1:** What exactly is scale-invariant output parameterization? Not explicitly defined \u2013 just two-hot trick?\n\n**AR:** Yes, the scale-invariant output parameterization we use is the \u201ctwo-hot trick\u201d in the contextual bandit experiments. The C51 agent, however, uses the standard categorical distributional RL update.\n\n**Question 2:** What is \"smoothing\" on the right side of Figure 3? Similarly, what is \"normalization\"? Is it just layer norm or something else?\n**AR:** In figure 3, normalization refers to layer norm, and smoothing refers to the label smoothing approach described in Appx B and in our previous comment.\n\n**Question 3:** Figure 3 concerns about potential uptick. Run for 10x longer?\n**AR:** We have conducted a more expansive sweep over experiment configurations and included the results in Figure 17 in Appendix D. We note that in general, networks trained with layer normalization and weight decay see plateauing loss curves on the probe task which suggest that the network performance has stabilized. These curves plateau at a value comparable to that of the random initialization in most experiment configurations when these three interventions are combined, though there is some variation: some configurations do observe mild plasticity loss and others exhibit positive forward transfer, resulting in slight discrepancies from the loss obtained by a random initialization on the probe tasks. However, in all cases these changes are dwarfed by the plasticity loss incurred by the naive Q-learning algorithm without regularization or normalization layers.  \n\n**Question 4:** Why wasn't L2 used in the RL experiments with C51? Is it already included in C51, or did it hurt performance?\n**AR:** Our evaluations on the image classification setting revealed that growth of the parameter norm only begins to interfere with learning when it increases by several orders of magnitude (e.g. from O(10^3) to O(10^7+). While the parameter norm of DQN-style agents on the Atari domain is known to grow, it does not increase by five orders of magnitude (c.f. Appendix D for an illustration of parameter growth in the game Seaquest). L2 regularization is known to slow down training in RL and make it difficult for agents to take off, so we did not anticipate seeing any benefits. For completeness, we provide preliminary results for the C51 agent with L2 regularization on a subset of environments from the Atari benchmark in our updates to the paper in Appendix D (we alphabetized the environments and selected every 5th game), which confirm that L2 does not provide a significant performance benefit. \n\n**Question 5:** Which figure in the main paper does Figure 8 correspond to?\n**AR:** Figure 8 provides a fine-grained view of the learning curves from a subset of nonstationarities from Figure 6; rather than visualizing only the final loss, it shows the per-task learning curves."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7189/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700254953443,
                "cdate": 1700254953443,
                "tmdate": 1700254953443,
                "mdate": 1700254953443,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Jvw9OogJZ2",
                "forum": "KIq6p9iv2q",
                "replyto": "lt4rlIjMVz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7189/Reviewer_o35T"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7189/Reviewer_o35T"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Authors, thank you for your thorough response. I appreciate your reply; it has reduced many of my original concerns. I've decided to maintain my recommendation of marginal acceptance as the paper still requires significant rewriting to update the claims about L2 regularization and update the related work section."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7189/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700719440373,
                "cdate": 1700719440373,
                "tmdate": 1700719440373,
                "mdate": 1700719440373,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]