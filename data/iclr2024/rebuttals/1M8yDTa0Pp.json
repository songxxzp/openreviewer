[
    {
        "title": "Cross-Model Semi-Supervised Prompt Learning for Vision-Language Models"
    },
    {
        "review": {
            "id": "V1ZkWQzAVI",
            "forum": "1M8yDTa0Pp",
            "replyto": "1M8yDTa0Pp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3945/Reviewer_xki8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3945/Reviewer_xki8"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on learning continuous soft prompts for adapting pre-trained vision-language models in a semi-supervised setting. To make the learned prompts invariant to the different views of a given unlabeled sample, the authors propose a new scheme, i.e., varying the lengths of visual and text prompts attached to these samples. The results show that the proposed method makes an immense gain in image classification performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This paper is well-written and easy to follow.\n\n-  To the best of my knowledge, the paper studied an under-explored problem, semi-supervised prompt learning for VLMs.\n\n- Considering the two branches of VLM, the authors propose to learn multi-modal prompts and vary the lengths of visual and text prompts attached to obtain samples with different views.\n\n- The experiments show the superior effectiveness of XPL compared to the designed baselines."
                },
                "weaknesses": {
                    "value": "My concerns are summarized below:\n\n- The authors illustrate the motivation of XPL with Figure 1(a,b), i.e., a large category-wise performance gap between two models leveraging different numbers of learnable prompts. I am curious about whether the results in Figure 1(a,b) are averaged over several random seeds. In the prompt learning, the random seed has a non-negligible effect on the performance.\n\n- For a more comprehensive comparison, I suggest that the authors add the results of zero-shot CLIP. Besides, these prompt engineering methods (e.g., prompt ensemble) and more advanced prompt learning methods on labeled data should be compared."
                },
                "questions": {
                    "value": "- Whether the results in Figure 1(a,b) are averaged over several random seeds?\n- The results of zero-shot CLIP, prompt engineering methods, and more advanced prompt learning methods should be included."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3945/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3945/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3945/Reviewer_xki8"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3945/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698673910918,
            "cdate": 1698673910918,
            "tmdate": 1699636355228,
            "mdate": 1699636355228,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "R6mKJbUPv7",
                "forum": "1M8yDTa0Pp",
                "replyto": "V1ZkWQzAVI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3945/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3945/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xki8"
                    },
                    "comment": {
                        "value": "We thank Reviewer xki8 for finding the paper well-written and easy to follow. Below are our responses to the specific concerns.\n\n**If Figure 1(a,b) are averaged over several random seeds:** Thanks for the query. All the results given in the paper including the ones shown in Figure 1(a,b) are taken as average over $3$ different random seeds.\n\n**Comparison with zero-shot CLIP and other prompt engineering methods:** . Being one of the first works in multi-modal semi-supervised prompt learning, we carefully designed the baselines for a comprehensive assessment. The Text Prompt Learning (TPL) and Visual Prompt Learning (VPL) baselines exactly follow the CoOp [1] approach, one with only text prompts as in the CoOp paper and the other with only visual prompts. As we found that XPL surpasses these baselines (refer in Figures 3 and 4 of the main paper) by a huge margin, we withheld from adding the zero-shot CLIP values which has been shown to be greatly inferior to CoOp method itself [1]. However, in view of the reviewers comment, we compared the lowest value for XPL obtained in 1-shot (refer Figure 4b in main paper) with the 0-shot CLIP results. As observed in the table below, XPL even for 1-shot surpasses the 0-shot CLIP values by a huge margin.\n\n|  | ImageNet |  Caltech101 | OxfordPets | Flowers102 |UCF-101 | StandfordCars | EuroSAT | DTD | \n| -------- | -------- | -------- | -------- | -------- | -------- |-------- |-------- |-------- |\n|XPL (1-shot)| 66.1 | 92.9 | 87.4 | 87.7 | 69.4 | 65.1 | 80.1 | 53.8 |\n|CoOp (1-shot)| 60.4 | 90.8 | 85.5 | 79.9 | 67.0 | 60.6 | 57.3 | 48.5 |\n|Clip (0-shot)| 59.1 | 85.0 | 84.8 | 66.2 | 60.0 | 56.4 | 39.1 | 42.2 |\n\n[1] Conditional prompt learning for vision-language models, CVPR 2022"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3945/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700601551130,
                "cdate": 1700601551130,
                "tmdate": 1700601763851,
                "mdate": 1700601763851,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "RJ3NLUsjwP",
            "forum": "1M8yDTa0Pp",
            "replyto": "1M8yDTa0Pp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3945/Reviewer_18u3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3945/Reviewer_18u3"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a novel framework for semi-supervised prompt learning tailored for VLMs. Moreover, it showcases the remarkable efficacy of the proposed method on the commonly used 15 datasets. To achieve this, this work is designed with two innovations: i)  the mutual knowledge distillation from the VLMs with different prompt lengths, and ii) the combination and consistency of weak and strong augmentation strategies. In my understanding, this work is actually based on existing basic methods, such as dual student architecture that existed in semi-supervised learning, and the combination of weak and hard augmentation in contrastive learning methods.\nThe advantage of this work is it is the first work to investigate the semi-supervised learning in the VLMs. Moreover, it reveals the importance of cross-model distillation with different prompt lengths, which is interesting. It is also simple and effective on most datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The strengths of this work are as:\n\ni) This work is the first to investigate the semi-supervised learning for the efficient transfer learning (ETL) of VLMs. \n\nii)  The framework of XPL is simple but effective, which showcases great performances on 15 typical datasets. \n\niii) The mutual learning of the models with different prompt lengths looks interesting."
                },
                "weaknesses": {
                    "value": "There are a series of issuses that is required to be solved.\n\ni) I noticed that this work only validated their methods on some simple benchmarks, such as CoOp, and VPT. Is it possible to give more experiments to validate the applicability of your methods on recent works on prompt learning-based few-shot learning?\n\nii) This work lacks the theoretical analysis for why the different prompt lengths is better.  \n\niii) The clarification for the utilization of unlabeled data in \"TPL^u\", \"MPL^u\" is not clear. \n\niv) In Figure 18 of the supplementary, why the MPL is lower than MPL^u in most datasets? \n\nv) Is it possible to provide the comparison for the 10\\% - 50\\% labeled data?\n\nvi) The ablation studies on hyper-parameters are only conducted with EuroSAT is not reasonable, since the randomness in few-shot learning."
                },
                "questions": {
                    "value": "The author is expected to increase more through explanations for their methods, and the experiments listed in the weakness. Moreover, the contribution of this work should be further clarified, especially the difference or significance of their methods with existing basic methods, such as weaker and hard augmentation in contrastive learning and the model distillation with dual students architecture."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3945/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698807879668,
            "cdate": 1698807879668,
            "tmdate": 1699636355154,
            "mdate": 1699636355154,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "h1eDqIfVrE",
                "forum": "1M8yDTa0Pp",
                "replyto": "RJ3NLUsjwP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3945/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3945/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 18u3 (Part 1)"
                    },
                    "comment": {
                        "value": "We thank Reviewer 18u3 for finding our framework simple but effective and the training policy interesting. Below are our responses to the specific concerns.\n\n **Applicability of our methods on recent works on prompt learning:** As ours is the first work in semisupervised prompt learning in VLMs, we designed many possible SSL baselines for learning prompts by leveraging on unlabeled data. We put forth TPL$^u$, VPL$^u$ and MPL$^u$ baselines by carefully culling SSL literature from related fields and showcase the efficacy of learning rich prompts using our framework by comparing with these over 15 datasets [refer Figures $3$ and $4$ of main paper]. For XPL, the main strength lies on efficient transfer learning in VLMs by exploiting the hugely available free knowledge (unlabeled data) available in the wild. However, for the sake of completeness and to address the reviewers concerns regarding the applicability of XPL, we evaluate the generalizability of our approach from base to new classes (Table 2 of main paper and Table 4 in supplementary section) and compare the results with more recent approach CoCoOP [1]. The following tables showcase the supremacy of XPL on the individual datasets.\n\n| EuroSAT | S | U | H |\n -------- | -------- | -------- | -------- |\n|Co-CoOP| 87.49 | 60.04 | 71.21 |\n|XPL| 97.80 | 58.90 | 73.52 |\n\n| UCF-101 | S | U | H |\n -------- | -------- | -------- | -------- |\n|Co-CoOP| 82.33 | 73.45 | 77.64 |\n|XPL| 88.50 | 74.70 | 81.02 |\n\n| Caltech101 | S | U | H |\n -------- | -------- | -------- | -------- |\n|Co-CoOP| 97.96 | 93.81 | 95.84 |\n|XPL| 98.95 | 92.49 | 95.61 |\n\n| Oxford Pets | S | U | H |\n -------- | -------- | -------- | -------- |\n|Co-CoOP| 87.49 | 60.04 | 71.21 |\n|XPL| 97.80 | 58.80 | 73.44 |\n\n| StandfordCars | S | U | H |\n -------- | -------- | -------- | -------- |\n|Co-CoOP| 70.49 | 73.59 | 72.01 |\n|XPL| 74.59 | 71.82 | 73.18 |\n\n| DTD | S | U | H |\n -------- | -------- | -------- | -------- |\n|Co-CoOP| 77.01 | 56 | 64.85 |\n|XPL| 80.18 | 54.60 | 64.96 |\n\n| Flowers102 | S | U | H |\n -------- | -------- | -------- | -------- |\n|Co-CoOP| 94.87 | 71.75 | 81.71 |\n|XPL| 98.24 | 69.87 | 81.66 |\n\n **Analysis for why the different prompt lengths is better.** While prompt learning enables efficient and faster adaptation paradigm, their low capacity may not allow a single prompt learning model to achieve best performances in all cases. As shown in the Figures 1(a) and (b) of the main paper, two models varying in the number of learnable prompts (8 and 16 prompts) exhibit diverse category-wise performance. Some classes in these datasets are better suited to 16 prompts while some show improvement with 8 prompts. Our co-teaching approach exploits these multiple prompt learners to learn complimentary knowledge and thus can complement in providing better semi-supervision to each other. Further, we identify that directly using the same adaptation model to produce confident pseudo-labels for the unlabeled data may miss crucial information for certain categories. As shown in notable works of [2, 3], the discriminative power of a single model is weaker to assign large number of high-quality pseudo labels to the unlabeled data. Similar conjuncture can be observed by using a fix length of prompts as shown in Figure 7 of the paper. Instead, the idea is to construct multiple pseudo-labels of different versions of the same unlabeled data and allow them to complement each other. In our work, we extend this idea further in a multiple prompt learning paradigm. In addition to using different views of the unlabeled samples, the two pathways have different lengths of learnable prompts. Such a co-teaching framework enables better representation learning by not only forcing invariance to different views of the unlabeled data but also enforcing invariance towards prompts of different lengths. To the best of our knowledge, such dual invariance applied to semi-supervised VLMs has not been explored earlier. We have also showcased the effectiveness of our cross-model co-teaching design over several semi-supervised baselines in the main paper and performed another additional baseline in accordance with the different queries of the reviewers.\n \n**Clarification for the utilization of unlabeled data in TPL$^u$, MPL$^u$:** All the $3$ baselines of TPL$^u$, VPL$^u$ and MPL$^u$ are based on naive `Fixmatch` [4] method as the underlying semi-supervised approach. The training on the unlabeled samples is carried out by generating pseudo-labels from the weakly augmented version and using it to reduce the loss across its strongly-augmented counterpart following the `Fixmatch` philosophy."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3945/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700601415305,
                "cdate": 1700601415305,
                "tmdate": 1700601415305,
                "mdate": 1700601415305,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Q8CqB2f3E7",
            "forum": "1M8yDTa0Pp",
            "replyto": "1M8yDTa0Pp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3945/Reviewer_MsJj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3945/Reviewer_MsJj"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a semi-supervised, cross-model prompt learning for vision-language models (VLMs). The key idea of the paper relies on feeding different lengths of soft prompts to two pre-trained VLMs (referred to as primary and auxiliary networks). Given an unlabeled image, the authors create a pair of weakly and strongly augmented versions, pass them to the two networks, and use the confident prediction from one network as a pseudo-label for the other and vice versa. Experimental comparisons with different baselines are presented on several benchmark datasets. The supplementary material contains additional experimental analyses, visualizations and source code."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "+ The paper tackles an interesting and timely topic. With the recent progress made in training powerful VLM models, it is worth studying how to prompt these pre-trained models for different downstream tasks\n+ The paper reads fairly well\n+ Source code is shared in the supplementary for reproducibility"
                },
                "weaknesses": {
                    "value": "Major issues \n\n* The work is very incremental\n\n    The technical novelty of this work is quite limited. The idea of passing a set of augmented versions of unlabeled image data in two different networks and constraining the networks to supervise each other has been explored in several existing works (ex. [1,2]). Furthermore, the idea of deriving visual prompts directly from text prompts using a linear projection (coupling function) has been introduced in previous works (ex. [4]) which the authors fail to cite and discuss. \n\n* Several claims/motivations in the paper are not convincingly justified\n\n    The main claim of this paper is to use prompts of different lengths for the primary and auxiliary networks. However, the authors fail to give a convincing argument as to why that should lead to a better performance. For instance, how did the authors arrive at using N and N/2 long prompts for the primary and auxiliary networks, respectively? why not N and N/3 or N and N/4? \n\n     The ablation experiment presented in Fig 7 of the main paper is counterintuitive to the key message of the paper. On page 5, the authors claim, \"*As the two models with different lengths differ in what they learn, they can complement in generating the supervision of each other*\". However, the results in Fig 7 show that this is not true. For instance, using a model with the same prompt length for the two networks (N=8 or N=16) outperforms a model with different prompt lengths (N=8 for primary and N=4 for auxiliary). If different prompt length is indeed as important as the authors argue, how do they explain these results? \n\n     It can also be noticed from Fig 7 that a model with N=32 for primary and N=8 for auxiliary performs inferior to the baseline model (N=16 for primary and N=8 for auxiliary). Does this mean that if the length difference between the two networks increases, performance gets worse? Where is the threshold for this performance trade-off? This needs a rigorous justification as the merit of the paper heavily relies on this argument.  \n\n    Moreover, the performances of some of the baselines in Fig 7 are worse than the multimodal prompt learning (MPL) baseline. This raises the question of whether the proposed cross-modal approach is indeed better than MPL given its sensitivity to the prompt length.\n  \n* The experimental settings and comparisons are not clearly presented\n\n    There have been several works related to prompt learning in text, vision, or multimodal domains. However, the author's comparison fails to cite most of these works except for CoOP. Which VPL or MPL baselines are used in the paper? Did the authors design their own baselines or use previous works as a baseline (but forgot to cite properly)? What are the experimental settings for these baselines? Why didn't the authors compare with recent works such as Co-CoOP [3]  or MaPLe [4]?\n\n   While I appreciate the extensive comparisons on several datasets, the presented results (quantitative figures) are almost unreadable due to the very small size of the figures. It would be better to either draw bigger figures or use tables instead of figures for a clearer presentation of the experimental results.\n\n* More ablation experiments are needed to justify the merit of the work\n\n    A simple experiment to show the benefit of the cross-model approach would be to use a single model and use the prediction of the weakly augmented input as a pseudo-label for the prediction of the strongly augmented input. However, such a baseline is missing in the paper.\n\n   It is also important to further explore what makes the cross-model approach based on one primary and one auxiliary network work. What if we use one primary and two auxiliary networks with a triplet of augmented inputs (1 weak and 2 strong - one for each auxiliary network) and each auxiliary network supervises the primary network and vice versa?  Does this lead to better supervision of the primary network? These explorations would be important to strengthen this work.\n\nMinor issues\n\n* In the supervised training, why did the authors choose to use only the weakly augmented image?\n\n* The paper needs some re-organizations. Some of the results presented in the supplementary (ex. Appendix B, D, E) should be in the main paper. \n\nReferences\n\n[1] Cross-Model Pseudo-Labeling for Semi-Supervised Action Recognition, CVPR 2022\n\n[2] Semi-Supervised Semantic Segmentation with Cross-Consistency Training, CVPR 2021\n\n[3] Conditional prompt learning for vision-language models, CVPR 2022\n\n[4] MaPLe: Multi-modal Prompt Learning, CVPR 2023"
                },
                "questions": {
                    "value": "Please refer to the questions raised in the \"Weaknesses\" section and try to address them carefully."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3945/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3945/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3945/Reviewer_MsJj"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3945/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699033429786,
            "cdate": 1699033429786,
            "tmdate": 1699636355086,
            "mdate": 1699636355086,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DGvzQVIEMF",
                "forum": "1M8yDTa0Pp",
                "replyto": "Q8CqB2f3E7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3945/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3945/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer MsJj (Part 1)"
                    },
                    "comment": {
                        "value": "We thank Reviewer MsJj for finding our work an interesting and timely topic. Below are our responses to the specific concerns.\n\n**Novelty and Advantage of Co-teaching using prompts of different lengths:** Thanks for the query. To the best of our knowledge, there is no existing Semi-Supervised Learning (SSL) works either in prompt learning or its applications in the multi-modal setting of VLMs. The primary challenges faced are of two folds. 1) The low quality of prompts learned in presence of only a few labeled data with a vast set of unlabeled samples and 2) Exploiting both the text and visual modalities using prompts only, to extract rich representations from unlabeled samples. As mentioned in Section 1 of the main paper, although approaches like [1,2] employ a cross-model representation learning, such paradigms have neither been exploring to tuning prompts nor in the multi-modal context of VLMs. These incur different challenges for learning efficient prompts in a multi-modal setting using frozen VLMs. The main motivation of using a co-training policy is to harness the complementary knowledge across two models having prompt inputs with different lengths. While prompt learning enables efficient and faster adaptation paradigm, their low capacity may not allow a single prompt learning model to achieve best performances in all cases. As shown in the Figures 1(a) and (b) of the main paper, two models varying in the number of learnable prompts (8 and 16 prompts) exhibit diverse category-wise performance. Some classes in these datasets are better suited to 16 prompts while some show improvement with 8 prompts. Our co-teaching approach exploits these multiple prompt learners to learn complimentary knowledge and thus can complement in providing better semi-supervision to each other. Further, we identify that directly using the same adaptation model to produce confident pseudo-labels for the unlabeled data may miss crucial information for certain categories. To the best of our knowledge, such dual invariance applied to semi-supervised VLMs has not been explored earlier. We have also showcased the effectiveness of our cross-model co-teaching design over several semi-supervised baselines in the main paper and performed another additional baseline in accordance with the reviewer's subsequent question."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3945/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700600926484,
                "cdate": 1700600926484,
                "tmdate": 1700600926484,
                "mdate": 1700600926484,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "2PA6LMyOFy",
            "forum": "1M8yDTa0Pp",
            "replyto": "1M8yDTa0Pp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3945/Reviewer_f2tm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3945/Reviewer_f2tm"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces Cross-model Prompt Learning (XPL), a semi-supervised approach for prompt learning in Vision-Language Models (VLMs), aiming to reduce the dependency on large labeled datasets. XPL employs dual pathways with variable soft prompt lengths to utilize unlabeled data for enhancing model performance in low-labeled-data regimes. The approach is validated on 15 datasets, showing that XPL outperforms the supervised baseline, particularly in few-shot classification tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ The method's ability to leverage unlabeled data effectively could substantially reduce the need for large labeled datasets.\n+ The approach is empirically validated across 15 diverse datasets, demonstrating its effectiveness and robustness in various contexts."
                },
                "weaknesses": {
                    "value": "- The approach primarily extends semi-supervised learning (SSL) principles to prompt learning with minimal adaptation, which may suggest that the level of novelty is somewhat constrained.\n- There is a concern regarding the robustness of the learned prompts, as they appear to be highly sensitive to the distribution of the dataset, potentially limiting their applicability in diverse real-world scenarios."
                },
                "questions": {
                    "value": "1. How does the proposed method differ fundamentally from existing SSL applications in prompt learning, and what specific adaptations have been made to tailor this approach to VLMs?\n2. Can the authors provide more insight into how the method would perform on out-of-distribution data or datasets with different characteristics than those tested?\n3. What measures have been taken to ensure that the learned prompts are not overly fitted to the specific datasets used in the experiments?\n4. Conduct additional experiments on out-of-distribution datasets or through domain adaptation challenges to evaluate the robustness and generalizability of the learned prompts."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3945/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699376325523,
            "cdate": 1699376325523,
            "tmdate": 1699636355022,
            "mdate": 1699636355022,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oH6Y76q8nj",
                "forum": "1M8yDTa0Pp",
                "replyto": "2PA6LMyOFy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3945/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3945/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer f2tm (Part 1)"
                    },
                    "comment": {
                        "value": "We thank Reviewer f2tm for acknowledging that our approach could substantially reduce the need for large annotated datasets. Below are our responses to the specific concerns.\n\n**Novelty and difference from existing SSL applications in prompt learning:**  Thanks for the query. To the best of our knowledge there is no existing Semi-Supervised Learning (SSL) work either in prompt learning or its applications in the multi-modal setting of VLMs. The primary challenges faced are of two folds. 1) The low quality of prompts learned in presence of only a few labeled data with a vast set of unlabeled samples and 2) Exploiting both the text and visual modalities using prompts only, to extract rich representations from unlabeled samples. As shown in Figures 1a and b of the main paper, two models leveraging unlabeled data but with different number of learnable prompts exhibit markedly different category-wise performance. Typically (as shown by [$1$,$2$]) in low-labeled data regime, the learned representations tend to lack enough discriminative power for downstream tasks thereby also failing to generate high-quality pseudo-labels. In such a constrained scenario, a combination of multiple models can be of rescue. However, such multiple model approach has not been applied for learning prompts in a semi-supervised setting for large VLMs. Our XPL uniquely combines a cross-model approach to employ a co-training policy for learning quality prompts. This novel prompt learning setup for a large frozen VLM harnesses the complementary knowledge across two models having prompt inputs with different lengths. In addition to using different views of the unlabeled images (vision), the two pathways have different lengths of learnable prompts (language). Such a co-teaching framework enables better representation learning by not only forcing invariance to different views of the unlabeled data but also enforcing invariance towards prompts of different lengths. In view of the different existing conventional SSL approaches, our carefully designed baselines do employ the various standard SSL techniques for prompt learning in VLM. These are illustrated in the following paragraph.\n\nAs ours is the first work in semisupervised prompt learning in VLMs, we designed many possible SSL baselines (namely TPL$^u$, VPL$^u$ and MPL$^u$) by carefully culling SSL literature from related fields and showcase the efficacy of our framework by comparing with these over 15 datasets [refer Figures $3$ and $4$ of main paper]. These $3$ baselines are based on `Fixmatch` [3] as the underlying semi-supervised approach that was used for semisupervised classification of images. As mentioned in Section 4.1, TPL$^u$, VPL$^u$ and MPL$^u$ make use of text prompts, visual prompts and both text and visual prompts respectively. The training on the unlabeled samples is carried out by generating pseudo-labels from the weakly augmented version and using it to reduce the loss across its strongly-augmented counterpart following the `Fixmatch` philosophy and not using multiple models. As can be observed from all the plots in Figures $3$ and $4$ of the main paper, XPL (using multiple models in addition) outperforms all these baselines showing the effectiveness of the cross-model design. In Figure $4(b)$, XPL provides $2.1$% improvement on average even in 1-shot scenario over MPL$^u$(the strongest baseline). Moreover, XPL offers a significant jump of $5$% for the fine-grained DeepWeeds dataset in 1-shot setup. Further, we have also demonstrated the effectiveness of our cross-model XPL over other traditional semi and self supervised approaches - `Pseudo-labeling` [4], which does not employ co-teaching and also over `MoCo` [5] which employs `momentum encoder`(refer Figure $6$ of the main paper). As observed, our XPL outperforms both `PL` and the self-supervised MoCo baselines for all considered datasets across al the scenarios."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3945/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700600698776,
                "cdate": 1700600698776,
                "tmdate": 1700600698776,
                "mdate": 1700600698776,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]