[
    {
        "title": "Understanding the Robustness of Randomized Feature Defense Against Query-Based Adversarial Attacks"
    },
    {
        "review": {
            "id": "nNavznjzgp",
            "forum": "vZ6r9GMT1n",
            "replyto": "vZ6r9GMT1n",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7319/Reviewer_nGVZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7319/Reviewer_nGVZ"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the defense method against query-based black-box attacks by injecting the noise into the middle layers of models. By theoretically analyzing the impact of both the adversarial perturbation and the noise injection on the prediction of the model, this paper tries to understand the impact of the proposed defense on robustness. Compared to the previous defense works that also inject noise into the model, the novelty of this paper is somehow in the noise injection to the feature space, i.e., the middle layer's outputs. Experimental results generally show the robustness improvement of injecting the noise to the feature rather than the input."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed method that injects the noise into the features as the defense against the query-based black-box attack is novel and is empirically shown effective. \n\n2. The adaptive attack is well-considered, which makes the evaluation more comprehensive."
                },
                "weaknesses": {
                    "value": "1. The organization of this paper can be improved. Assumption 1, Figure 1 and 2 are not referred to in the main text. It is confusing on the purpose of presenting the assumption 1 and Figure 1 and 2. \n\n2. The assumption and the theorem are incorrect. Even when the noise is small, the expectation of the randomized model is not necessarily consistent with the original model on the same data point, one simple counter-example is that when the input x is at the decision boundary, a small perturbation can change the prediction, so small noise may change the prediction. Theorem 1 is based on incorrect derivation, Eq. (23) and (24) may be incorrect as the gradient $\\nabla_{h(x)}(L \\cdot g)$ is anisotropic so the multiplication with Gaussian noise should not be an i.i.d. Gaussian noise. In addition, the assumption of the proof is the value of v and $\\mu$ are small, so the approximation holds, but in the experiments, the value of $v$ is not present, and the value of $\\mu$ is as large as 0.3, which is not negligible.\n\n3. The correctness of Theorem 1 is not fully evaluated. The observation based on Theorem 1 is that the ratio $v/\\mu$ is a factor of the robustness, if we fix the input x, then it is the only factor that affects the robustness. In Table 3, it is observed that the robustness is not strictly correlated to the ratio, this is reasonable since the inputs are changing during the multi-step perturbation. The correctness of the influence of the ratio can be verified by trying one-step perturbation so that the input x is kept the same, which is missing in this paper.\n\n4. The evaluation of the decision-based attack is insufficient and the results are not good. It seems the proposed method only works on RayS, and the results on DeiT and ViT are not presented."
                },
                "questions": {
                    "value": "1. Please verify if the Eq. (23) and the Eq. (24) are correct.\n\n2. Please verify that assumption 1 is correct and that the theorems and experiments are strictly following this assumption.\n\n3. I am curious about the impact of the ratio on the robustness when the gradients are fixed. Can you present the experimental results if possible?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7319/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7319/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7319/Reviewer_nGVZ"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7319/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698199409862,
            "cdate": 1698199409862,
            "tmdate": 1699636875062,
            "mdate": 1699636875062,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QrJuA0dgOB",
                "forum": "vZ6r9GMT1n",
                "replyto": "nNavznjzgp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7319/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7319/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the valuable comments!"
                    },
                    "comment": {
                        "value": "Please see our responses to your comments below:\n\n**Q1: Assumption 1, Figure 1 and 2 are not referred to in the main text. It is confusing on the purpose of presenting the assumption 1 and Figure 1 and 2.**\n\nA: Thank you for the observation. We have made incorrect references for Figures 1 and 2, when including the Supplementary File within the Main paper. Please see our clarifications below:\n- \u2018Figure 3\u2019 in Section 3.3 should be \u2018Figure 1\u2019, \n- \u2018Figure 5\u2019 in Section 3.5 should be \u2018Figure 2\u2019, \n- The discussion for Assumption 1 is immediately above its declaration. It allows us to ensure that small noise diffuses but does not shift the prediction, or adversarial samples of the original model are adversarial samples of the randomized model. \nWe have updated this discussion in the revised submission.\n\n**Q2: The assumption and the theorem are incorrect. Even when the noise is small, the expectation of the randomized model is not necessarily consistent with the original model on the same data point, one simple counter-example is that when the input x is at the decision boundary, a small perturbation can change the prediction, so small noise may change the prediction.**\n\nA: As discussed in Section 3.2,  given Assumption 1, adversarial samples of the original model are adversarial samples of the randomized model, on expectation. In other words, at the decision boundary, while a specific sample of the small Gaussian noise may change the prediction, but expectedly, it should be the same prediction as that of the original model (across several samples of the small Gaussian noise). This assumption is typically made in other relevant randomization works, including the evaluated RND defense.\n\n**Q3: Theorem 1 is based on incorrect derivation, Eq. (23) and (24) may be incorrect as the gradient\u00a0$\\nabla_{h(x)}(L \\cdot g)$\u00a0is anisotropic so the multiplication with Gaussian noise should not be an i.i.d. Gaussian noise.**\n\nA: Thank you for the comment. Please see our response below, confirming that it is **still indeed an i.i.d. Gaussian noise**.\n\nFor a random vector $x$ following $\\mathcal{N}(\\mu, \\Sigma)$, we have $Mx\\sim\\mathcal{N}(M\\mu, M\\Sigma M^T)$ which is still Gaussian noise. In Equation 23, since $\\delta_1 - \\delta_2\\sim\\mathcal{N}(0, 2\\nu I)$ and $\\nabla_{h(x)}(L \\cdot g)2\\nu I \\nabla_{h(x)}(L \\cdot g)^T=2\\nu ||\\nabla_{h(x)}(L \\cdot g)||^2_2$, we have $\\nabla_{h(x)}(L \\cdot g)(\\delta_1 - \\delta_2)\\sim\\mathcal{N}(0, 2\\nu ||\\nabla_{h(x)}(L \\cdot g)||^2_2)$.  Since the noise added by the attack and the defense at each application are independent, $\\nabla_{h(x)}(L \\cdot g)(\\delta_1 - \\delta_2)$ follows i.i.d Gaussian noise.\n\nA similar derivation is applied to Equation 24.\n\n**Q4: In addition, the assumption of the proof is the value of \\nu and \\mu\u00a0are small, so the approximation holds, but in the experiments, the value of\u00a0\\nu\u00a0is not present, and the value of\u00a0\\mu\u00a0is as large as 0.3, which is not negligible.**\n\nA: In our experiments, as indicated (e.g., Tables 1 and 2), the selected $\\nu$  corresponds to a small drop in accuracy (1-2%). These $\\nu$ values are still very small, with magnitude less than 0.016. Our experiments show that the user can control the degree of clean accuracy loss for their application when selecting $\\nu$, and typically, the user will not use a large $\\nu$ value that will cause significant degradation in clean accuracy.\n\nOn the other hand, $\\mu$ is the step size that the attacker uses to search for the correct perturbed direction. For Square attack, the maximum square size is 0.3, however, the added noise is still bounded by the $\\ell_{\\infty}$ ball of radius 0.05. For NES $\\mu$ is the step size of finite difference method, which should be small so the attacker can obtain a good approximation for the gradient; when $\\mu$ is too large, this approximation becomes unreliable. In the experiments, we run the attacks with small exploration step size, in particular for Square attack 1 and 0.005 for $\\ell_{2}$ and $\\ell_{\\infty}$ NES attacks, respectively. Note that, these values are still small, to ensure gradient approximation is reliable. These exact values have also been used in previous works, including RND and SND."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7319/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700499538391,
                "cdate": 1700499538391,
                "tmdate": 1700501199275,
                "mdate": 1700501199275,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AZs8Mr8qmm",
                "forum": "vZ6r9GMT1n",
                "replyto": "gjze89gLxZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7319/Reviewer_nGVZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7319/Reviewer_nGVZ"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the detailed response. The response addresses my concerns in Q1 and Q3, while the rest of my concerns are not addressed well.\n\nFor Q2, it is possible that the majority of the noise-perturbed inputs lie in the original class. To validate this assumption, you may need to provide some theoretical analysis of the convexity of the decision boundary.\n\nFor Q4, it is hard to justify how small of $\\mu$ and $\\nu$ is enough for the approximation. It would be helpful if the authors could provide some theoretical analysis to quantify the error w.r.t. the magnitude of $\\mu$ and $\\nu$, otherwise, the qualitative analysis is not convincing.\n\nFor Q5, the authors claim that the proposed method does not work well only on RayS/CIFAR10 with VGG19 and ResNet50 networks, it seems like the main text and the rebuttal still do not present the ViT and DeiT's results on CIFAR10 as evidence.\n\nMy main concern is still on the validation of the assumption since this paper focuses on providing some theoretical guarantee to the defense, which largely relies on the assumption. If the assumption is not strictly satisfied, even if other papers have relied on it, the main contribution would be largely affected. A valid theoretical analysis of the correctness of the assumption would raise my rating."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7319/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700698164050,
                "cdate": 1700698164050,
                "tmdate": 1700698164050,
                "mdate": 1700698164050,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WqHYV6dDOf",
            "forum": "vZ6r9GMT1n",
            "replyto": "vZ6r9GMT1n",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7319/Reviewer_s19G"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7319/Reviewer_s19G"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates a well-known defense against black-box adversarial attacks (both score-based and decision-based) which involves adding a random noise to the input. The paper argues that the robustness-accuracy trade-off of such defense can be improved by adding noise to the intermediate layer instead. Theoretical and empirical analyses are provided to support this claim.\n\n---\n\n## Comment After Rebuttal\n\nOnce again, thank you so much for acknowledging and addressing my concerns! I appreciate your efforts.\n\nBased on the new results (counting one successful attack query as a successful attack), there seems to be a minimal improvement from adding noise at the feature vs at the input. However, the result does show that the defense is effective in this difficult practical setting (~40% of samples are still correctly classified after 10k queries), and this convinces me that there are applications where this type of defense can be successfully applied.\n\nI would really appreciate it if the author(s) could include this type of results and discussion (along with other suggestions during this rebuttal period) in the next revision of the paper. After reading the other reviewers' comments, I have no further concerns and have decided to adjust my rating from 5 to 6."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "### Quality\n\nThe experiments are thorough, and the metrics are well-designed. Many models, datasets, and attack algorithms are included in the experiments. I like that the other baseline defense like AAA is also included. I also appreciate the comprehensive background section.\n\nThe paper also takes into account the nuance of picking the appropriate noise variance; they nicely solve this issue using the notion of the robustness-accuracy trade-off and pick the variance $\\nu$ that results in a small fixed drop in the clean accuracy."
                },
                "weaknesses": {
                    "value": "### Disadvantages of randomized models\n\nI understand that the paper focuses on randomized models, but in practice, randomized models can be unattractive for two reasons:\n\n1. Its output is stochastic and unpredictable. For users or practitioners, what a randomized model entails is the fact that all predictions have some unaccounted chance of being wrong. One may argue that it is possible to average the outputs across multiple runs, but doing so would reduce the robustness and just converge back to the deterministic case (and with increased costs).\n2. **Definition of a successful attack**. This has to do with how a successful attack is defined on page 4: \u201c\u2026adversarial attacks are successful if the obtained adversarial example can fool the randomized model in the *majority* of its applications on the example.\u201d I argue that in security-sensitive applications (e.g., authentication, malware detection, etc.), it is enough for the model to be fooled *once*. The randomness enables the attacker to keep submitting the same adversarial examples until by chance, the model misclassifies it.\n\nI believe that these practical disadvantages limit the significance of this line of work.  \n\n### First-order Taylor approximation\n\nAll the analysis in the paper uses the first-order Taylor approximation. First of all, this assumption should be stated more clearly. More importantly, I am not convinced that this approximation is good especially when the added noise or the perturbation is relatively large. Neural networks are generally highly nonlinear so I wonder if there is a way to justify this assumption better. An empirical evaluation of the approximation error would make all the analyses more convincing.\n\n### Method for picking the intermediate layer\n\nFirst, I wonder which intermediate layer is picked for all the results in Section 4. Do you pick the best one empirically or according to some metric? It will also be good to clearly propose a heuristic for picking the intermediate layer and measure if or how much the heuristic is inferior to the best possible choice.\n\n### Loss-maximizing attacks\n\nOne of the big questions for me is whether the attack is related to the fact that most of the black-box attacks try to **minimize the perturbation magnitude**. Because of the nature of these attacks, all the attack iterations stay very close to the decision boundary, and hence, they perform particularly poorly against the noise addition defense. In other words, these attacks are never designed for a stochastic system in the first place so they will inevitably fail.\n\nThe authors have taken some steps to adapt the attacks for the randomized defense, mimicking obvious modifications that the real adversary might do (EoT and Appendix D.4). I really like these initiatives and also wonder if there are other obvious alternatives. One that comes to mind is to use attacks that **maximize loss given a fixed $\\epsilon$ budget**. These attacks should not have to find a precise location near the decision boundary which should, in turn, make it less susceptible to the randomness.\n\nThis actually does NOT mean that the randomness is not beneficial. Suppose that the loss-maximizing attack operates by estimating gradients (via finite difference) and just doing a projected gradient descent. One way to conceptualize the effect of the added noise is a noisy gradient, i.e., turning gradient descent into *stochastic* gradient descent (SGD). SGD convergence rate is slowed down with larger noise variance so the adversary will have to either use more iterations or uses more queries per step to reduce the variance. Either way the attack becomes more costly. I suggest this as an alternative because it directly tests the benefits of the added noise without exploiting the fact that the distance-minimizing attacks assume deterministic target models.\n\n### Additional figures\n\nI have suggestions on additional figures that may help strengthen the paper.\n\n1. **Scatter plot of the robustness vs the ratio in Theorem 1.** The main claim of the paper is that the quantity in Theorem 1 positively correlates with the failure rate of the attack (and so the robustness). There are some figures that show the distribution of this quantity, but the figure that will help empirically verify this message is to plot it against the robustness (perhaps average over the test samples). Then, a linear fit and/or an empirical correlation coefficient can also be shown. Personally, this plot more clearly confirms the theoretical result than the density plot (e.g., Figure 2, etc.) or Table 6. I also think that $\\nu$ should not be fixed across layers/models and should be selected to according to the clean accuracy.\n2. **Scatter plot of the clean accuracy vs the ratio in Eq. (14)**. Similar to the first suggest, I would like to see an empirical confirmation for both of these theoretical analysis.\n3. **Robustness-accuracy trade-off plot**. This has been an important concept for evaluating any adversarial defense. I would like to see this trade-off with varying $\\nu$ as well as varying intermediate layers. The full characterization of the trade-off should also help in choosing the best intermediate layer, instead of just considering a few fixed values of $\\nu$.\n\n### Originality\n\nOne other weakness of the paper is the originality/novelty of the method. The most important contribution of this paper is the analysis on the gradient norm (i.e., sensitivity of the model) of benign and perturbed samples. The proposal to add noise to the intermediate layer instead of the input in itself is relatively incremental. However, the theoretical analysis does seem particularly strong to me, even though it does build up a nice intuition of the scheme. This is a minor weakness to me personally, and I would like to see more empirical results, as suggested earlier, rather than additional theoretical analyses.\n\n### Other minor issues\n\n- Eq. (7): the RHS is missing $L(...,y)$.\n- 2nd paragraph on page 7: I think it was slightly confusing the first time I read it. It was not immediately clear to me that this is about Eq. (14) and what \u201cproduct of this value and \u2026\u201d refers to."
                },
                "questions": {
                    "value": "1. The paper mentions that the attack success rate is measured by \u201cmajority.\u201d There\u2019s an issue that I have already mentioned above, but I would like to know how many trials are used to compute this majority for the reported robustness in the experiments. If some variance can be reported too, that would be great. \n2. Section 3.3: from my understanding, the ratios plotted in Figure 3 involve both $\\nu$ and $\\nabla_{h(x)}(L \\circ g)$. I wonder how $\\nu$ is picked here. Is it picked like in Section 3.5 where the accuracy drop is fixed at some threshold (e.g., 1%, 2%)?\n3. Section 4.1: it is mentioned that Qin et al. [2021] and Byun et al. [2021] are included in the comparison, but they never show up again. I wonder if the results are really missing or if these two schemes are basically the noise addition in the input.\n4. Generally, I see a larger improvement for VGG and ViT vs ResNet-50 and DeiT. Is there any explanation or intuition the authors can provide to better understand this observation?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7319/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7319/Reviewer_s19G",
                        "ICLR.cc/2024/Conference/Submission7319/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7319/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698705512862,
            "cdate": 1698705512862,
            "tmdate": 1700869132046,
            "mdate": 1700869132046,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "f5G8PpgXNs",
                "forum": "vZ6r9GMT1n",
                "replyto": "WqHYV6dDOf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7319/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7319/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the valuable comments!"
                    },
                    "comment": {
                        "value": "Please see our responses to your comments below:\n\n**Q1: randomized output is stochastic and unpredictable... a randomized model entails all predictions have some unaccounted chance of being wrong. average the outputs across multiple runs, but doing so would reduce the robustness and just converge back to the deterministic case (and with increased costs).**\n\nA: Randomized models have been extensively studied in the literature as an effective way to defend against adversarial attacks (Liu et al. 2017; He et al. 2019; Cohen et al. 2019; Salman et al. 2019; Byun et al. 2021; Qin et al. 2021). Indeed, since randomization may cause unpredictable predictions, existing works require training and model ensembles, both of which are expensive, to ensure minimal impact of the noise (i.e., cause a decrease in clean accuracy), while achieving high robustness. In contrast, our defense, and also RND, allow the user to control how much random noise to add to ensure a minimal drop in clean accuracy (using a small test set). For example, our defense achieves high robustness in Tables 1 and 2 against several attacks while only causing 1-2% accuracy drops in performance. Please note that defense approaches such as adversarial training cause a significant drop in clean accuracy (e.g., 10% or more in ViT (Mo, Yichuan, et al)) to achieve good robustness. \n\nMo, Yichuan, et al. \"When adversarial training meets vision transformers: Recipes from training to architecture.\"\u00a0NeurIPS 2022.\n\n**Q2: Definition of a successful attack. I argue that in security-sensitive applications (e.g., authentication, malware detection, etc.), it is enough for the model to be fooled\u00a0once**\n\nA: Our empirical experiments focus on evaluating whether the attack can truly find adversarial examples. For non-randomized models, when an attack arrives at the decision that $x$ is an adversarial example, $x$ unquestionably is on the other side of the decision boundary. However, for a randomized model, $x$ could be still on the correct side of the decision boundary, but the added randomization shifts it to the other side of the decision boundary; thus, in principle, $x$ is still not an adversarial example, and if we use 1 single application for evaluation, an attack could be lucky or a randomized defense could be unlucky. Consequently, for a fair evaluation of the effectiveness of a defense, we forward $x$ multiple times and decide that it is an adversarial example if the majority of the results say so, as seen in our paper.\n\nNevertheless, we understand that in the case where we ignore fair evaluation, the attack is allowed to be lucky and just has to fool the defense once, as given in the example in the comment. Therefore, we also provide the experiments when forwarding $x$ only once, as below. As we can observe, our defense is still effective against the attacks.\n\n*Robustness with a single trial, 10k queries, 1%-accuracy noise scale, ImageNet*\n|  |       | Square  | NES     |\n|--------------|-------|---------|---------|\n| ViT          | Base | 0 | 7.3 |\n|              | Input | 44.0| 47.0 |\n|              | Feature | 44.8| 42.5 |\n\n**Q3: All the analysis in the paper uses the first-order Taylor approximation. An empirical evaluation of the approximation error would make all the analyses more convincing.**\n\nA: Thank you for your suggestion. We include the histogram of the approximation error of ViT on ImageNet in Section F.1 (Supplementary). As we can observe, the error is relatively small.\n\n**Q4: First, I wonder which intermediate layer is picked for all the results in Section 4. empirically or according to some metric?**\n\nWe pick the penultimate layers of the base models for randomization, in all experiments. We have shown with theoretical analysis in Figure 1 (main) and empirical analysis in Table 12 (supplement), that the randomizing at a deeper layer consistently achieves the best robustness; for example, randomizing the last layer in VGG or ViT achieves 8-10% improvement over the previous layer. We have already revised the paper (Section 4.1) to clarify this discussion accordingly."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7319/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700499842055,
                "cdate": 1700499842055,
                "tmdate": 1700499842055,
                "mdate": 1700499842055,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tG4sSLSf9n",
                "forum": "vZ6r9GMT1n",
                "replyto": "WqHYV6dDOf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7319/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7319/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Continued rebuttal comments!"
                    },
                    "comment": {
                        "value": "**Q5: The authors have taken some steps to adapt the attacks for the randomized defense, mimicking obvious modifications that the real adversary might do (EoT and Appendix D.4). I really like these initiatives and also wonder if there are other obvious alternatives. One that comes to mind is to use attacks that\u00a0maximize loss given a fixed\u00a0$\\eps$\u00a0budget. These attacks should not have to find a precise location near the decision boundary which should, in turn, make it less susceptible to randomness.**\n\n**This actually does NOT mean that the randomness is not beneficial. Suppose that the loss-maximizing attack operates by estimating gradients (via finite difference) and just doing a projected gradient descent. One way to conceptualize the effect of the added noise is a noisy gradient, i.e., turning gradient descent into\u00a0*stochastic*\u00a0gradient descent (SGD). SGD convergence rate is slowed down with larger noise variance so the adversary will have to either use more iterations or more queries per step to reduce the variance. Either way, the attack becomes more costly. I suggest this as an alternative because it directly tests the benefits of the added noise without exploiting the fact that the distance-minimizing attacks assume deterministic target models.**\n\nA: Thank you for your appreciation of our analysis and indeed a great observation regarding the fixed budget. In fact, NES, Square attack, and SignHunt, all of which are evaluated in our paper, work in the principle of maximizing loss given a budget. Specifically, NES approximates the gradient directly and applies projected gradient descent, while Square attack and SignHunt search for the adversarial sample at the boundary of the $\\ell_p$ ball.\n\n**Q6: additional figures: Scatter plot of the robustness vs the ratio in Theorem 1. Scatter plot of the clean accuracy vs the ratio in Eq. (14). Robustness-accuracy trade-off plot**\n\nThank you for your suggestions. In fact, the robustness averaged over the test samples at each layer, corresponding to Figures 1 and 2, are provided in Table 12 (Supplementary). We can observe an increase in robustness as the ratio becomes higher at deeper layers. We have updated these Figures, in the revised paper, with the performance results created in Table 12, for a better explanation.\n\nAs discussed in Section 4, our defense allows the user to choose $\\nu$ that results in a \u201ccontrolled\u201d decrease in clean accuracy, using a small test set. For example, we provided the results of varying $\\nu$ in Tables 1 and 2 that correspond to 1%, 2%, 3% and 4%. drops in clean accuracy. In general, increasing $\\nu$, or more randomness, will lead to a decrease in clean accuracy but also an increase in robustness, as observed in these tables. In addition, we provide the robustness (Accuracy under Attack) when varying $\\nu$ (corresponding varying drops in clean accuracy) on CIFAR10/VGG19/Square Attack setting:\nAuA = [56.2, 58.4, 62.3, 62.8, 63.5]\nAcc = [95.33, 94.94, 94.55, 93.58, 92.09]\n\n**Q7: The most important contribution is the analysis on the gradient norm (i.e., sensitivity of the model) of benign and perturbed samples. The proposal to add noise to the intermediate layer instead of the input in itself is relatively incremental. However, the theoretical analysis does seem particularly strong to me, even though it does build up a nice intuition of the scheme. This is a minor weakness to me personally, and I would like to see more empirical results, as suggested earlier**\n\nA:  Thank you for appreciating our work. Here, we summarize our responses to the earlier questions:\n- Evaluation with a single application on the successful query (answered in Q2, added to the revised paper)\n- Error analysis of first-order approximation (answered in Q3, added to the revised paper)\n- Performance at different layers with the same drop in accuracy (answered in Q6, provided in Table 12 of original submission)\n- Robustness-Accuracy tradeoff (answered in Q6 with additional experiments)\nDifferent from existing randomized-feature defenses, which additionally require adversarial expensive training and ensemble, our work opens the possibility of randomizing the latent layers at **inference time** without these hassles (being lightweight and plug-and-play) and with a strong theoretical foundation. In addition, unlike randomizing the input, such as RND, our defense gives the user protection against non-continuous input, such as graph data; however, this is beyond the scope of our paper and deserves another independent study."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7319/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700499910915,
                "cdate": 1700499910915,
                "tmdate": 1700500009246,
                "mdate": 1700500009246,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Io33gQZdzW",
                "forum": "vZ6r9GMT1n",
                "replyto": "CPAkGsUzk2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7319/Reviewer_s19G"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7319/Reviewer_s19G"
                ],
                "content": {
                    "title": {
                        "value": "Reviewer s19G's response to the author rebuttal"
                    },
                    "comment": {
                        "value": "I really appreciate the author\u2019s clarification of my questions. My concerns/questions from Q3-7 are satisfactorily addressed. I have some responses and clarification for Q1 and Q2.\n\n**Practical motivation (Q1/2)**\n\nI am aware of the other prior works that also rely on randomness to combat query-based attacks, but I am not convinced that it is practically useful for the reasons I outlined in my review. It may, of course, be *scientifically* interesting, but I see this line of problem as *practically motivated*.\n\nI also did not mention anything about clean accuracy; I understand that to improve security, some trade-off must be made. I know that randomized defense usually has a much better clean accuracy than adversarial training. My first issue is about the unpredictability or the randomness itself which entails the security Implication in the second issue. \n\nPlease allow me to rephrase the core issue I\u2019m raising. The adversarial attack usually matters to security-sensitive applications (e.g., malware, self-driving cars) where *the system should not fail even once*. By allowing the system to be non-deterministic, I argue that we have increased that probability (e.g., the attacker just has to query the model repeatedly, and by chance, it will fail at some point!). On the other hand, if I use adversarial training, I may take a hit on clean accuracy, but at least, I can systematically study where my model fails and manage that risk rather than leaving things to chance.\n\nThis issue is inherent to any randomized defense, and I don\u2019t see a way to reconcile this. In other words, I have not found a way to practically justify this line of defense. That said, I\u2019m more than happy to be swayed by your argument on this matter.\n\n**What I think is a good evaluation scheme**\n\nWith the above reasoning, I would not call the evaluation by averaging over multiple queries \u201cfair\u201d at all. If anything, it is *unfair* because it\u2019s more computationally costly and defeats the purpose of randomness. What\u2019s considered \u201creasonable\u201d to me is the attacker should be allowed to query $N$ times (including running the attack), and if *any* one of these $N$ queries leads to a successful attack, then the defender loses on this sample. This is what I expect practitioners or industry to do! Choosing an appropriate $N$ depends on applications, and it can be anywhere from 1 to ~10k. So I would report on multiple values of $N$ in that range.\n\nNote that what I propose here is different from letting the attacker complete the attack first and then evaluate only the final adversarial example in one query. It is unclear to me what is the exact procedure the authors used for the new result so I\u2019d appreciate a clarification."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7319/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700512574258,
                "cdate": 1700512574258,
                "tmdate": 1700512574258,
                "mdate": 1700512574258,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7qnnh0XRFk",
            "forum": "vZ6r9GMT1n",
            "replyto": "vZ6r9GMT1n",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7319/Reviewer_NLrk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7319/Reviewer_NLrk"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to defend against black-box attacks by adding noise to intermediate features at test time. It is empirically validated effective against both score-based and decision-based attacks. The authors also provide theoretical insights on the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The idea is straightforward, lightweight, and can be plugged into all existing defenses like adversarial training.\n2. It is great to see the theoretical analysis for the defense method.\n3. The paper is well-organized and easy to follow.\n4. The authors do comprehensive experiments to study the effectiveness of the proposed method."
                },
                "weaknesses": {
                    "value": "1. The motivation to inject feature noise is not clear compared to injecting input noise. \"Unlike previous randomized defense approaches that solely rely on empirical evaluations to showcase effectiveness\" is not correct, since RND also provides lots of theoretical analysis as the authors acknowledged in Sec. 2.3. The results are not significantly better than RND, but injecting feature noise requires a careful choice of the layer.\n\n2. The idea of injecting noise into hidden features is not novel, seeing Parametric Noise Injection: Trainable Randomness to Improve Deep Neural Network Robustness against Adversarial Attack, CVPR 2019. Although this is for defending against white-box attacks, adopting it for black-box attacks does not seem a significant contribution.\n\n3. Does the proposed method have an advantage against AAA in defending score-based attacks? AAA is not designed for decision-based attacks, where the authors use AAA for comparison."
                },
                "questions": {
                    "value": "Response to rebuttal: The authors provide a strong rebuttal and a good revision of the paper. My Q1 and Q3 have been well addressed, making me raise my score to 6. Although the method differs from the CVPR 2019 paper in Q2, the novelty is weak, i.e., perturbing feature to defend has been explored for a long time."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7319/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7319/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7319/Reviewer_NLrk"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7319/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698811776649,
            "cdate": 1698811776649,
            "tmdate": 1700766176573,
            "mdate": 1700766176573,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wuUIgfRxk4",
                "forum": "vZ6r9GMT1n",
                "replyto": "7qnnh0XRFk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7319/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7319/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the valuable comments!"
                    },
                    "comment": {
                        "value": "Please see our responses to your comments below:\n\n**Q1: The motivation to inject feature noise is not clear compared to injecting input noise. \"Unlike previous randomized defense approaches that solely rely on empirical evaluations to showcase effectiveness\" is not correct, since RND also provides lots of theoretical analysis as the authors acknowledged in Sec. 2.3. The results are not significantly better than RND, but injecting feature noise requires a careful choice of the layer.**\n\nA: Thank you for the suggestion. In the above sentence, we referred to the lack of rigorous theoretical analysis of the robustness against black-box, query-based attacks when randomizing the internal features of the model. Similar to our work on the feature space, RND indeed provides a theoretical analysis on the input space. We have already revised the paper (Section 1) to clarify this claim accordingly. \n\nAs we can observe in Tables 1 and 2, our randomized feature defense consistently outperforms RND (in several cases, the improved robustness of our defense is almost 10-20% more than that of RND) for all evaluated attacks, except NES, where RND occasionally performs better than our method (in these cases, RND\u2019s slightly better robustness of ~2-3%). \n\nFinally, regarding the comment that \u201cour method requires careful choice of the layer\u201d:\n* We have shown with theoretical analysis in Figure 1 (main) and empirical analysis in Table 12 (supplement), that the randomizing at a deeper layer consistently achieves the best robustness; for example, randomizing the last layer in VGG or ViT achieves 8-10% improvement over the previous layer.\n* In fact, all our experiments randomize only the penultimate layers of the models. Consequently, the complexity of using our method is equivalent to that of RND. We already provided clarification on this claim (Section 4.1) in the revised submission.\n* Please note that an advantage of our defense over RND is that randomizing an internal layer makes the method suitable for discrete data, such as graphs. However, this is beyond the scope of our paper and deserves an independent study.\n\n**Q2: 1. The idea of injecting noise into hidden features is not novel, seeing Parametric Noise Injection: Trainable Randomness to Improve Deep Neural Network Robustness against Adversarial Attack, CVPR 2019. Although this is for defending against white-box attacks, adopting it for black-box attacks does not seem a significant contribution.**\n\nA: Although injecting noise to the features is previously proposed (such as in the mentioned paper), these methods require additional training to tune the noise, which is computationally expensive, especially when dealing with large and complex datasets. In contrast, our defense, and also RND, are lightweight, plug-and-play and can be applied to any pretrained model without additional model training. Our defense is effective against black-box attacks because it is designed to fool the querying process of these attacks, as seen in our theoretical and empirical analysis. \n\n**Q3: Does the proposed method have an advantage against AAA in defending score-based attacks? AAA is not designed for decision-based attacks, where the authors use AAA for comparison.**\n\nA:  We already provided the experiments, comparing AAA to our defenses on score-based attacks in Section E.3 (Supplementary). The experiments show that that our defense improves the robustness and has comparable results to AAA on score-based attacks. However, for decision-based attacks, as provided in Section 4.3, our defense has superior performance. From a practical perspective, a defense should work well against many types of query-based black-box attacks, and these experiments show that our defense has more practical utility than AAA."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7319/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700497979550,
                "cdate": 1700497979550,
                "tmdate": 1700501091119,
                "mdate": 1700501091119,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "aOmMX4ZFJt",
            "forum": "vZ6r9GMT1n",
            "replyto": "vZ6r9GMT1n",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7319/Reviewer_pa1M"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7319/Reviewer_pa1M"
            ],
            "content": {
                "summary": {
                    "value": "This paper showed that adding noises to some parts of models could protect the models from query-based attacks. The authors derived proofs to show that their method (adding noises) theoretically provided robustness to the models. Besides, they experimented this method with several datasets (Imagenet and CIFAR10) and models' architectures (i.e., ResNet50, VGG19, DeiT and ViT)."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- The paper has a strong theoretical proof to show that the method can effectively provide robustness.\n- The experiments are strong because the authors used Imagenet and CIFAR10 to show that their method and generalize in small and large datasets. Also, they tried with several models' architectures."
                },
                "weaknesses": {
                    "value": "- I understand that the paper focuses on black-box attacks, but in the experiment section, the authors may try evaluating models with white-box attacks as well.\n- Please check the parentheses in equation (7)."
                },
                "questions": {
                    "value": "- In page 5, can you please give a reason for this sentence \"We can observe that these ratios become higher when the data are perturbed toward the adversarial samples. In other words, the randomized model is more robust during the attack.\"?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7319/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698814394601,
            "cdate": 1698814394601,
            "tmdate": 1699636874662,
            "mdate": 1699636874662,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QLERds31gI",
                "forum": "vZ6r9GMT1n",
                "replyto": "aOmMX4ZFJt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7319/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7319/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the valuable comments!"
                    },
                    "comment": {
                        "value": "Please see our responses to your comments below:\n\n**Q1: I understand that the paper focuses on black-box attacks, but in the experiment section, the authors may try evaluating models with white-box attacks as well.**\n\nA: Our work focuses on studying the effect of randomized models on black-box attacks, which are more practical in MLaaS system. In these systems, the adversary rarely has access to the trained model or model\u2019s architecture to perform white-box attacks. Although our analysis does not cover white-box attacks, in Section E.4 we still conduct experiments for white-box attacks on randomized feature defense. The experimental results on C&W and PGD show that our defense can also boost the model\u2019s robustness against white-box attacks, which demonstrates the broad utility of our method beyond black-box attacks.\n\n**Q2: Please check the parentheses in equation (7).**\n\nA: Thank you for the comment. We have fixed the typo in the revised submission.\n\n**Q3: In page 5, can you please give a reason for this sentence \"We can observe that these ratios become higher when the data are perturbed toward the adversarial samples. In other words, the randomized model is more robust during the attack.\"?**\n\nA: In our theoretical analysis, we show that the robustness to black-box attacks is controlled by three terms: the scale of noise vector added by the attack $\\mu$ and the defense $\\nu$, and the ratio of the norm of the gradient with respect to the feature where noise is added and the input $\\frac{||\\nabla_h(L\\circ g)||}{||\\nabla_h(L\\circ g)||}$. While the scales of the noise vectors are fixed during an attack, the last term is dependent on the input and where in the model the defense is performed.  During the attack, the input is perturbed and thus changes the above ratio if noise is injected into hidden layers of the model. Figure 1 illustrates that the ratio increases, especially for deeper randomized layers, when the attack happens, implying that randomized feature defense has a higher chance than input defense to fool black-box attacks."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7319/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700497609973,
                "cdate": 1700497609973,
                "tmdate": 1700497609973,
                "mdate": 1700497609973,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gfhUJUNymt",
                "forum": "vZ6r9GMT1n",
                "replyto": "QLERds31gI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7319/Reviewer_pa1M"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7319/Reviewer_pa1M"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your answers and clarification!"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7319/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700630634714,
                "cdate": 1700630634714,
                "tmdate": 1700630634714,
                "mdate": 1700630634714,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]