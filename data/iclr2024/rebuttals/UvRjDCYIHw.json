[
    {
        "title": "Double Equivariance for Inductive Link Prediction for Both New Nodes and New Relation Types"
    },
    {
        "review": {
            "id": "QpexexEyKu",
            "forum": "UvRjDCYIHw",
            "replyto": "UvRjDCYIHw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8054/Reviewer_1Qir"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8054/Reviewer_1Qir"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates inductive link prediction for both new entities and new relations. It proposes an inductive structural double equivariant architecture that decomposes a knowledge graph into subgraphs containing different relations and encodes and aggregates them in the same way to eliminate the use of relation embeddings. The paper also constructs two datasets based on OpenEA and Wikidata5M. Extensive experimental results demonstrate the strong performance of ISDEA."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "S1. This paper addresses an important and challenging task.\n\nS2. It introduces a novel framework that avoids reliance on relationship embeddings.\n\nS3. Good reproducibility - the paper provides code and detailed experimental settings."
                },
                "weaknesses": {
                    "value": "W1. The proposed framework requires significant preprocessing and expensive encoding costs. This may be attributed to three factors: preprocessing costs, encoding for each relation, and separate scoring for each candidate entity.\n\nW2. The 1 vs. 50 evaluation poses a risk as negative samples obtained from negative sampling are mostly easily distinguishable. This setup may not be sufficient to cover real-world scenarios.\n\nW3. While ISDEA appears suitable for relation prediction, its performance on node prediction is not very good."
                },
                "questions": {
                    "value": "Q1. As shown in Table 1(b), ISDEA's performance is not good and, in some datasets, even receives the lowest scores. Can you explain the reasons for this?\n\nQ2. I am concerned about the efficiency of the proposed framework. Could you report training and inference times on some datasets?\n\nQ3. It should be clarified that the multilingual KGs in the OpenEA library share the same schema. So many of the relations in these KGs overlap."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8054/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8054/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8054/Reviewer_1Qir"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8054/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698457457965,
            "cdate": 1698457457965,
            "tmdate": 1699636995873,
            "mdate": 1699636995873,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yBoviQWVkc",
                "forum": "UvRjDCYIHw",
                "replyto": "QpexexEyKu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8054/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8054/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## Official Response to Reviewer 1Qir\n\nWe are grateful to the reviewer 1Qir for recognizing the significance of our work. Your thoughtful questions and constructive feedback are highly valued. Please refer to the Common Reviewer Response for the most important revisions in our submission. Here are our detailed responses to your comments: \n\n**Comment 1. The proposed framework requires significant preprocessing and expensive encoding costs.**\n\n**A1:** In Appendix F.2 and G, we have discussed the limitation of our proposed ISDEA architecture due to its computation cost. The primary source of complexity in ISDEA is the process of enumerating across all relations and computing distances between every pair of nodes. However, we believe separate scoring for each candidate entity is commonly used by existing literatures such as DistMult (Yang et al., ICLR 2015), RGCN (Schlichtkrull et al., ESWC 2018), GraIL (Teru et al., ICML 2020), etc..\n\nTo address this, as discussed in the Common Reviewer Response, we have developed ISDEA+, which improves the time complexity by $R$ over ISDEA (where $R$ is the number of relation types). In our experiments, we observe ISDEA+ is between 20x to 120x faster than ISDEA (old).\n\n**Comment 2. Only using 50 negative samples may not be sufficient to cover real-world scenarios.**\n\n**A2:** The choice was purely due to computational constraints of our proposed ISDEA (old) since we wanted at least 5 runs for each experiment. We have simply followed the procedure proposed by GraIL (Teru et al., ICML 2020), which is among the first works to apply GNNs for inductive link prediction over new nodes (as GraIL, we perform evaluation against 50 random negative samples). This choice allows some balance between computational cost and meaningful performance insights. \n\nIn our rebuttal, we introduced an improved version of ISDEA, named ISDEA+. This enhancement allows us to handle larger negative samples during evaluation. Here are the results we have managed to gather in the limited time available. In particular, we rerun the node prediction task on all tasks of PediaTypes with 500 random negative node samples, and collected the Hits@10 results in the following table:\n\n| Models     | EN-FR          | FR-EN          | EN-DE          | DE-EN          | DB-WD           | WD-DB          | DB-YG          | YG-DB           |\n|------------|----------------|----------------|----------------|----------------|-----------------|----------------|----------------|-----------------|\n| NBFNet     | $65.73\\pm3.17$ | $73.36\\pm1.84$ | $58.93\\pm2.89$ | $32.15\\pm3.14$ | $33.75\\pm2.87$  | $61.64\\pm1.36$ | $34.07\\pm1.49$ | $51.00\\pm1.90$ |\n| InGram     | $67.42\\pm2.42$ | $48.76\\pm8.10$ | $46.84\\pm4.07$ | $54.08\\pm1.35$ | $24.87\\pm10.38$ | $49.41\\pm5.41$ | $15.69\\pm4.11$ | $23.73\\pm7.12$  |\n| DEq-InGram | $76.33\\pm1.27$ | $64.84\\pm6.17$ | $59.67\\pm2.43$ | $66.30\\pm2.42$ | $35.70\\pm12.10$ | $65.55\\pm4.04$ | $24.11\\pm4.73$ | $37.76\\pm12.01$ |\n| ISDEA+     | $68.27\\pm3.62$ | $50.76\\pm2.57$ | $66.83\\pm0.48$ | $53.52\\pm0.67$ | $56.97\\pm0.85$  | $65.75\\pm0.78$ | $20.72\\pm0.35$ | $26.67\\pm1.76$  |\n\n\nFrom these results, we can observe that even with an enlarged number of negative samples, we still observe the same model behaviors as negative sample size 50. The NBFNet model is still able to achieve relatively good performance in the node prediction task while its model architecture is not able to capture the relationship among the new relation types. Both of our proposed models, ISDEA+ (and DEq-InGram), still achieve better performance than the baseline NBFNet and InGram in most scenarios.\n\n**Comment 3. The performance of ISDEA on node prediction is not good.**\n\n**A3:** Please see new results of ISDEA+ in Section 5.1 of the revised manuscript, which now outperforms DEq-InGram in nearly all scenarios. The improvement in performance of DEq-InGram over InGram confirms our theory that double equivariance allows neural networks to perform zero-shot double inductive link prediction tasks.\n\nPS: The node prediction performance aligns with findings from Jambor et al., EACL 2021, where a model's success in node prediction tasks may not fully reflect its capabilities in predicting the type of link, but simply that of being able to predict homogeneous links (in a way agnostic to its type). For instance, RMPI and NBFNet have good performance at predicting nodes but significantly worse performance at predicting edge types."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8054/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700426152403,
                "cdate": 1700426152403,
                "tmdate": 1700429055650,
                "mdate": 1700429055650,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ROf5J2Y7Fl",
                "forum": "UvRjDCYIHw",
                "replyto": "QpexexEyKu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8054/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8054/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Comment 4. What is the training and inference times for ISDEA, and how is the efficiency of the architecture?**\n\n**A4:** Here we present a run time comparison for ISDEA+, ISDEA, DEq-InGram, InGram and NBFNet on the node prediction task on EN-FR in our PediaTypes dataset, and a much larger transductive FB15K-237 dataset. Due to varied mini-batch training sizes for different models, we report the training time per minibatch step (i.e., per gradient update step) for the training time comparison. Here are the results:\n\n**Training and inference on the EN-FR in PediaTypes**\n- 4962 entities, 122 relations, 30876 train triplets, 3326 validation triplets, 2485 test triplets\n\n| Model      | Training Time per Minibatch Step | Test Inference Time | Max GPU Memory Usage |\n|------------|----------------------------------|---------------------|----------------------|\n| ISDEA+     | 0.05s                            | 6.53s               | 1.9GB                |\n| ISDEA      | 2.56s                            | 359.45s             | 9.8GB                |\n| Deq-InGram | 0.09s                            | 10.39s              | 3.0GB                |\n| InGram     | 0.09s                            | 5.60s               | 3.0GB                |\n| NBFNet     | 0.02s                            | 5.14s               | 5.5GB                |\n\n**Training and inference on the transductive FB15k-237 dataset**\n- 14541 entities, 237 relations, 272115 train triplets, 17535 validation triplets, 20466 test triplets\n\n| Model      | Training Time per Minibatch Step  | Test Inference Time | Max GPU Memory Usage |\n|------------|-----------------------------------|---------------------|----------------------|\n| ISDEA+     | 0.08s                             | 27.60s              | 5GB                  |\n| ISDEA      | N/A                               | N/A                 | OOM                  |\n| Deq-InGram | 0.57s                             | 99.67s              | 13GB                 |\n| InGram     | 0.57s                             | 62.60s              | 13GB                 | \n| NBFNet     | 0.07s                             | 62.23s              | 14GB                 |\n\nWe can see that ISDEA+ significantly improves ISDEA, and is among the fastest models with least memory consumption. \n\n\n**Comment 5. It should be clarified that multilingual KGs in the OpenEA library share the same schema. So many of the relations in these KGs overlap.**\n\n**A5:** We appreciate your observation on this aspect. Indeed, the multilingual KGs within the OpenEA library consist of several pairs of knowledge graphs, each representing similar domains but in different languages. For example, entities like \u201chttp://dbpedia.org/resource/E678522\u201d in English and \u201chttp://fr.dbpedia.org/resource/E415873\u201d in French represent the same concept, although with different linguistic labels. It is accurate that these KGs predominantly use English for relation labels, which causes an overlap in relations. However, in our experimental setup, we treat relations as if they were in different languages and do not leverage this overlapping information during model training. Moreover, in other KGs used in our PediaTypes and WikiTopics datasets, such overlapping relations are not present. We have included this clarification in the revised submission, in the description of our datasets. Thank you for pointing this out.\n\n\n# Summary \n\nWe want to thank the reviewer for their time and insightful suggestions! We hope our answers can address your concerns well.\n\nWe also prudently ask you to reconsider our work if the concerns are addressed. With the new zero-shot meta-learning results and the significant performance boost of our new method ISDEA+, we hope the reviewer will reconsider their score. \n\nOverall, we believe our work makes important contributions in inductive learning on knowledge graphs, and proposes a new paradigm and novel architecture ISDEA+. Thank you for your efforts again!\n\n**References**\n\n[1] Yang et al., \"Embedding entities and relations for learning and inference in knowledge bases.\" ICLR 2015\n\n[2] Schlichtkrull et al., \"Modeling Relational Data with Graph Convolutional Networks.\" ESWC 2018\n\n[3] Teru et al. \"Inductive Relation Prediction by Subgraph Reasoning.\" ICML 2020\n\n[4] Jambor et al. \"Exploring the limits of few-shot link prediction in knowledge graphs.\" EACL 2021"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8054/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700426190580,
                "cdate": 1700426190580,
                "tmdate": 1700427476425,
                "mdate": 1700427476425,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Tzu9FWlcV5",
            "forum": "UvRjDCYIHw",
            "replyto": "UvRjDCYIHw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8054/Reviewer_VGiN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8054/Reviewer_VGiN"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a theoretical framework for inductive link prediction over multi-relational graphs (knowledge graphs) where both entities and relations are unseen at test time. The framework includes the concepts of double permutation equivariance (to node permutation and edge type permutation) and its slight relaxation of distributionally double equivariance (to incorporate another existing model into the framework). Further, the authors introduce the first GNN implementation of the proposed framework \u2013 ISDEA as a double equivariant model, and DEq-InGram as a distributionally-double equivariant version of InGram. Experimentally, the authors devise a handful of new datasets and run experiments on relation prediction $(i, ?, k)$ and node prediction $(i, r, ?)$ tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "**S1.** Overall, I think it is a solid work that lays important theoretical foundations for the hardest of inductive link prediction tasks - dealing with both new entities and relations at test time requires more effort beyond learning relation embeddings. This is highly relevant for modern graph learning tasks, especially in low-data regimes without input node features.\n\n**S2.** The experimental agenda is convincing - a handful of newly proposed datasets with relation prediction and entity prediction tasks. Perhaps the experimental section could have been even stronger if the evaluation was performed on all nodes/relations in the inference graph instead of 50 random negatives, but the authors acknowledge it is the scalability issues of the ISDEA model (not the framework in general) that are likely to be addressed in the future work."
                },
                "weaknesses": {
                    "value": "The following ones are not the critical weaknesses but rather several discussion points I\u2019d invite the authors to elaborate on: \n\n**W1.** The formalization in Section 2 assumes the existence of bijections (nodes-to-nodes, relations-to-relations) in training and test graphs. Basically, the framework posits the double equivariance only when training and test graphs have exactly the same number of nodes and edge types - which practically does not happen very often. On the other hand, the constructed datasets PediaTypes and WikiTopics all have different numbers of nodes and relations at training and test time (so there is no bijection possible). Could you please comment on the seeming discrepancy between the theory and what is measured in the experiments? \n\n**W2.** Section 5.2: \u201c_relatively easier task of node prediction_\u201d - I do not quite agree with this statement. The results might suggest the task is easier simply because you take 50 random negatives among _thousands_ of nodes in the inference graph, so those negatives are likely to be _easy_ negatives. On the other hand, the number of relations in the datasets is 50-150 in PediaTypes and <50 in WikiTopics, so the negative relation samples are likely to be harder. It was found that evaluation on small number of negative entities overestimates the performance, so I would hypothesize the numbers (and task impression) would change when the architecture would scale to ranking all nodes in the inference graph."
                },
                "questions": {
                    "value": "**Q1.** What are the input features to standard GNN architectures reported in the experiments under GraphConv / GAT / GIN? Initialization of nodes with all ones or with random vectors? \n\n**Q2.** Since DEq-InGram is distributionally double equivariant (by means of averaging several runs with different random relation vectors initializations), would averaging NBFNet results across several runs with random relation initialization count as distributionally double equivariant as well?\n\n**Q3.** The distributionally double equivariant idea posits equivariance in expectation, of which the easiest implementation is averaging over several runs (if we talk about drawing samples of relation vectors). Drawing parallels to group-equivariant CNNs, it is possible to achieve equivariance via augmentations such as frame averaging. I wonder if any such \u201caugmentation\u201d or frame averaging is possible within the double equivariance framework. If so, it might be a good idea to clearly state in the paper that distributionally-double equivariance is different from frame averaging"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8054/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698644911464,
            "cdate": 1698644911464,
            "tmdate": 1699636995767,
            "mdate": 1699636995767,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FCuZ1HKK2f",
                "forum": "UvRjDCYIHw",
                "replyto": "Tzu9FWlcV5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8054/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8054/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## Official Response to Reviewer VGiN\n\nWe are grateful to reviewer VGiN for seeing the significance of our work. Your insightful questions and valuable feedback are deeply appreciated. Please refer to the Common Reviewer Response for the most important revisions in our submission. Below, we provide comprehensive responses to your comments:\n\n**Comment 1. The theorem of double equivariant representation seems to only apply to the graphs with the same number of nodes and edges, however, it is not common in real-world graphs and is also not the setting considered in the experiments section. How to explain the discrepancy between the theory and experiments? What is the goal of the experiments?**\n\n**A1:** Thank you for the insightful question, which allows us to clarify our method (we added a comment to the revised submission). In all our WikiTopics datasets the training and test datasets have **different** number of relations (Figure 6 in the Appendix F.1.3). ISDEA+ works with a different number of relations between train and test. There is no need to have a bijective mapping between the relations in training and test. GNNs are node-equivariant and work on test graphs with varying numbers of nodes. ISDEA+ is double (relation+node) equivariant and works on test graphs with varying numbers of nodes and relation types. Isomorphic pairs are used to help us understand the property of ISDEA+, but ISDEA+ can be applied to non-isomorphic graphs. In standard graph tasks, a GNN tends to make similar embeddings for nodes that have similar structure (they need not be isomorphic) as long as doing that does not negatively affect the prediction task. And we showcase ISDEA+ also has such capabilities.\n\n**Comment 2. The statement of \u201cnode prediction task is relatively easy\u201d might be invalid. It can depend on the negative sample size. The current negative sample size of 50 is not enough to showcase the statement.**\n\n**A2:** Thank you for raising this important point! That is true. We will qualify our point, which is essentially similar to Jambor et al., EACL 2021, where a model's success in node prediction tasks may not fully reflect its capabilities in predicting the type of link, but simply that of being able to predict homogeneous links (in a way agnostic to its type). For instance, RMPI and NBFNet have good performance at predicting nodes but significantly worse performance at predicting edge types. \n\nIn our rebuttal, we introduced an improved version of ISDEA (old), named ISDEA+. This enhancement allows us to handle larger negative samples during evaluation. Here are the results we have managed to gather in the limited time available. In particular, we rerun the node prediction task on all tasks of PediaTypes with 500 random negative node samples, and collected the Hits@10 results in the following table:\n\n\n| Models     | EN-FR          | FR-EN          | EN-DE          | DE-EN          | DB-WD           | WD-DB          | DB-YG          | YG-DB           |\n|------------|----------------|----------------|----------------|----------------|-----------------|----------------|----------------|-----------------|\n| NBFNet     | $65.73\\pm3.17$ | $73.36\\pm1.84$ | $58.93\\pm2.89$ | $32.15\\pm3.14$ | $33.75\\pm2.87$  | $61.64\\pm1.36$ | $34.07\\pm1.49$ | $51.00\\pm1.90$ |\n| InGram     | $67.42\\pm2.42$ | $48.76\\pm8.10$ | $46.84\\pm4.07$ | $54.08\\pm1.35$ | $24.87\\pm10.38$ | $49.41\\pm5.41$ | $15.69\\pm4.11$ | $23.73\\pm7.12$  |\n| DEq-InGram | $76.33\\pm1.27$ | $64.84\\pm6.17$ | $59.67\\pm2.43$ | $66.30\\pm2.42$ | $35.70\\pm12.10$ | $65.55\\pm4.04$ | $24.11\\pm4.73$ | $37.76\\pm12.01$ |\n| ISDEA+     | $68.27\\pm3.62$ | $50.76\\pm2.57$ | $66.83\\pm0.48$ | $53.52\\pm0.67$ | $56.97\\pm0.85$  | $65.75\\pm0.78$ | $20.72\\pm0.35$ | $26.67\\pm1.76$  |\n\n\nFrom these results, we can observe that even with an enlarged number of negative samples, we still observe the same model behaviors as negative sample size 50. The NBFNet model is still able to achieve relatively good performance in the node prediction task while its model architecture is not able to capture the relationship among the new relation types. Both of our proposed models, ISDEA+ (and DEq-InGram), still achieve better performance than the baseline NBFNet and InGram in most scenarios.\n\n**Comment 3. What are the input features to standard GNN architectures reported in the experiments under GraphConv / GAT / GIN?**\n\n**A3:** Thank you for this important question. We discussed this at the bottom of page 5: since our graphs in PediaTypes and WikiTopics do not have node features, we initialize them as all-one features to feed into all model architectures."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8054/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700426029090,
                "cdate": 1700426029090,
                "tmdate": 1700428625819,
                "mdate": 1700428625819,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "vuur3BQDuF",
            "forum": "UvRjDCYIHw",
            "replyto": "UvRjDCYIHw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8054/Reviewer_Uo9n"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8054/Reviewer_Uo9n"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces the task of  \"doubly inductive link prediction'\", where the objective is to be able to make inductive prediction on both novel nodes and novel relation types, which are not encountered during training. This is a highly challenging task, especially because the authors do not allow the use of any additional context regarding the unknown relations.   The authors propose a general framework ISDEA to generate \"double permutation-equivariant\" representations and further explore ways to augment the existing InGram architecture with \"distributionally double equivariant positional embeddings\". Two new real-world datasets are proposed for benchmarking \"doubly inductive link prediction\" and experiments are carried out to validate the theoretical findings."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- **Problem and setup**: Inductive link prediction is a very important task and authors generalise this task to also predict novel relation types. The paper provides an approach for modeling equivariant representations of nodes and relations. \n- **Motivation and study**: A clear motivation, including the study of different architectures.\n- **Benchmarking**: New benchmarking datasets are introduced and assessed against prior methods, establishing a new context."
                },
                "weaknesses": {
                    "value": "- **Presentation and formal writing**: The writing of the paper is problematic and concepts are often unclear:\n  - The text is very repetitive and contains many redundancies (i.e., the contribution of the paper is highlighted three times in the first page with paraphrased sentences), but when it comes to formal definitions, it does not make a rigorous treatment (see below).\n  - Figure 1: This is crowded and does not explain much to me: why are the relations typed using conjunctions at this point? How does the logical description given in the beginning of page 3 in any way correspond to this figure?\n  - Multigraph: Authors seem to suggest a multigraph is more general than a knowledge graph. It is unclear to me what authors specifically mean by this? If they mean a directed, multi-relational graph then this is nothing more than a knowledge graph. Heterogenous networks are special instances with single relation types allowed between nodes etc.\n  - Doubly inductive: The naming is somewhat problematic, because the inductive prediction is either on the relation or on one of the entities at a time, but not both according to Def 1. \n   - Isomorphic triplets: The definition of multigraph isomorphism and triplet isomorphism is a very odd one. I have no idea why, e.g., (Hans, Grand $\\land$ Father, Bob) in train and (Hanna, Granny $\\land$ Mother) should be considered isomorphic (and at this point we still do not know the role of logical conjunction in defining the relations). This is essential because everything builds on this notion of \"isomorphism\" which is completely unjustified.\n  - The paper is very hard to parse in general: in many cases, the statements of the results appear ambiguous to me, including the ones in the appendix.\n\n- **New architectures**: The new architectures introduced in the paper appear to be somewhat incremental. IDSEA is a variant of DSS-GNN operating on relation-induced subgraphs, whereas DEq-InGram is a simple modification of InGram with bagging.\n\n- **Empirical findings**: IDSEA seems to perform consistently worse than DEq-InGram in the task node prediction on PediaTypes, which does not seem to match what the theory suggests and is not being discussed in the paper. \n\n- **Train and test distribution**: The paper predominantly focuses on scenarios where the train and test graphs share a similar distribution. However, there exists a range of tasks involving unseen nodes and relations where the distribution significantly differs between the training and testing phases.  Further experimental validation on these tasks is required."
                },
                "questions": {
                    "value": "Please refer to my review for clarifications and some more questions  here:\n\n- In the experiments, why do the authors not compare with standard relational GNNs such as RGCN, CompGCN, NBFNets, etc?\n\n- What are the differences between ISDEA, DEq-InGram, and InGram in terms of their runtime?\n\n- Since both DEq-InGram and InGram produce distributionally double equivariant representations, why is there a substantial performance gap between these models on both datasets?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8054/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698753503277,
            "cdate": 1698753503277,
            "tmdate": 1699636995652,
            "mdate": 1699636995652,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Y8S74768T5",
                "forum": "UvRjDCYIHw",
                "replyto": "vuur3BQDuF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8054/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8054/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## Official Response to Reviewer Uo9n\nWe express our gratitude to reviewer Uo9n. Your feedback is greatly appreciated. Please refer to the Common Reviewer Response for the most important revisions in our submission. We believe there was a misunderstanding due to an unintended interpretation of Figure 1, which cascaded throughout the paper making it hard to understand. Our revised submission changes Figure 1 to hopefully clear that source of misunderstanding (we also modified the text to further clarify our work). We have also made architectural changes to ISDEA (now called ISDEA+ in the revision) to make it significantly more scalable (e.g., from between 20x to 120x faster than the original ISDEA in our experiments). Please let us know if the revised version addresses the reviewer's concerns. Below we consider all questions point-by-point:\n\n**Comment 1. Writing. Introduction.**\n\n**A1:** Thank you for the feedback. We have revised the introduction to make it less repetitive.\n\n**Comment 2. Figure 1 is hard to understand, what does the relation conjunction mean and how does it correspond to the logical induction statements in the top of page 3?**\n\n**A2:** Great feedback! Thank you! In the revised version we have changed Figure 1 to help clarify the task. Now it shows the graph being trained solely on the Sports Wikipedia KG and tested on the Corporations Wikipedia KG. Note that the relations observed on the domain Sports are all different from the relations on the domain Corporations. This pre-trained zero-shot task better illustrates our task. We also point the reviewer to the updated benchmark task in Section 5.2, where we describe our newly proposed pre-trained zero-shot meta-learning task. We relegated the logical connections to Appendix B, where we discuss the connection between ISDEA+ and universally quantified Horn clauses. We removed the connection to UQER formulas from the main paper to improve clarity.\n\n**Comment 3. Why claim multigraphs are more general than knowledge graphs?**\n\n**A3:** In our revised submission we describe the graphs simply as knowledge graphs and wrote a note that the approach could be adapted to multilayer networks and similar types of graphs.\n\n**Comment 4. The naming of \u201cDoubly inductive task\u201d is problematic since in test the prediction is either on one of the relations or one of the entities.**\n\n**A4:** Thank you for the feedback. We believe there is a misunderstanding of the term \"doubly inductive link prediction task\" (Definition 2.1). The inductive setting of our task aims to predict missing links in test graphs with both completely new nodes and completely new relation types, as seen in our PediaType and WikiTopic datasets (and illustrated in Figure 1). We have debated whether the term *fully inductive* would be more adequate, but we can see that also leading to even further potential misunderstandings. The task is expressed mathematically that the set of training and test nodes are disjoint, $\\mathcal{V}^{(te)} \\cap \\mathcal{V}^{(tr)} = \\emptyset$, and that the set of training and test relation types are disjoint $\\mathcal{R}^{(te)} \\cap \\mathcal{R}^{(tr)} = \\emptyset$. Perhaps the misunderstanding originates from our last sentence in Definition 2.1, \u201cwe aim to predict both missing relations for the given head and tail nodes $(i, ?, j)$ and missing nodes for a given relation $(i, k, ?)$.\", which we now removed in the revision to avoid this misunderstanding.\n\n**Comment 5. The definition of multigraph isomorphism and triplet isomorphism is unclear, why (Hans, Grand \u2227 Father, Bob) in train and (Hanna, Granny \u2227 Mother) should be considered isomorphic? And what does the conjunction between relation types mean?**\n\n**A5:** Thank you for pointing out this source of confusion! We have now changed Figure 1 to avoid this confusion in the revised submission. Figure 1 now describes a zero-shot task between training on the Sports wikipedia KG (KG restricted to the domain Sports) and inductively predicting links (zero-shot, no retraining, no side-information) on the Corporations wikipedia KG (KG restricted to the domain Corporations). The definition of isomorphism in the context of our theory involves a bijective mapping between the nodes and relation types of two KGs, such that the same structural relationships are preserved. Figure 1 illustrates a Sports KG and a Corporations KG that do not share relation types. In our datasets, training and test KGs even have different numbers of relation types. A double equivariant model like ISDEA+ automatically deals with distinct relation types and distinct numbers of relation types between train and test."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8054/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700425776028,
                "cdate": 1700425776028,
                "tmdate": 1700427660319,
                "mdate": 1700427660319,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aw5cqDul9E",
                "forum": "UvRjDCYIHw",
                "replyto": "vuur3BQDuF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8054/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8054/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Comment 6. The new architectures introduced in the paper are somewhat incremental, ISDEA is a variant of DSS-GNN operating on relation-induced subgraphs, whereas DEq-InGram is a simple modification of InGram with bagging.**\n\n**A6:** Thank you for your feedback. It seems that there is a misunderstanding. The original DSS-GNN is not double equivariant, since it is a method for creating representations for bags of subgraphs over a single group equivariance (node permutations). Moreover, we have now made the method ISDEA+ between 20x and 120x faster, which took significant effort. We also show that double equivariance is a key property for the task of zero-shot link prediction without side information. Inspired by one of Reviewer\u2019s VGiN questions, we added new experimental results to the main paper, where we expanded our proposed benchmark to better explain the power of double equivariant models in a self-supervised pre-trained zero-shot meta-learning task. Please refer to ''Common Reviewer Response'' and for Section 5.2 and Appendix F.1.3 in the revised submission.\n\nThere is also a small misunderstanding about DEq-InGram: We perform Monte Carlo averaging, not bootstrapping aggregation (the averaging is not over bootstrapping). This averaging provably makes the model double equivariant, which significantly improves its performance. The new results show that ISDEA+ is still significantly more consistent and better than even DEq-InGram. We have made this point more salient in the text.\n\n**Comment 7. Why does ISDEA perform consistently worse than DEq-InGram in the task node prediction on PediaTypes? Does it contradict the theory?**\n\n**A7:** Please see new results of ISDEA+ in Section 5.1 of the revised manuscript, which now outperforms DEq-InGram in nearly all scenarios. The improvement in performance of DEq-InGram over InGram confirms our theory that double equivariance allows neural networks to perform zero-shot double inductive link prediction tasks.\n\nPS: The node prediction performance aligns with findings from Jambor et al., EACL 2021, where a model's success in node prediction tasks may not fully reflect its capabilities in predicting the type of link, but simply that of being able to predict homogeneous links (in a way agnostic to its type). For instance, RMPI and NBFNet have good performance at predicting nodes but significantly worse performance at predicting edge types.\n\n**Comment 8. In experiments, the training graphs and test graphs share similar distributions. How will it perform if it is not the case?**\n\n**A8:** Thank you for your question. We are uncertain what the reviewer means by \"similar distributions\". Assuming the reviewer is referring to the distribution of graphs, the WikiTopics dataset presents a scenario where we intentionally train the model on graphs from one domain and test it on graphs from a completely different domain, such as training on Art and testing on Sports. Arts and Sports not only have nearly no common relation types, they also have a different number of relation types. Hence, their distribution should be different. Please let us know if this answers your concerns.\n\n**Comment 9. Why not compare with standard relational GNNs such as RGCN, CompGCN, NBFNet, etc?**\n\n**A9:** We thank the reviewer for pointing out these works, but again there seems to be a misunderstanding that we hope will be cleared with the new Figure 1 in the revised submission. Please note that RGCN, CompGCN and NBFNet cannot perform the task where they are trained on, say, Arts domains, and as asked to zero-shot predict links on, say, Sport domains. Please refer to the second paragraph in Section 4 and Appendix E for the detailed distinction between the GNN-based inductive approaches and our method. We have added CompGCN in our related work section in the appendix.\n\nThe doubly inductive link prediction task (Definition 2.1) involves both entirely new nodes and new relation types in the test graph. In contrast, the mentioned papers concentrate on different tasks. Specifically, RGCN and CompGCN were initially designed for transductive tasks, while NBFNet is designed for inductive link prediction for only new nodes. Following InGram, we modified the SOTA relational GNN model NBFNet with minimal changes to handle unseen relation types at test time, whose results are shown in Table 1 and Appendix F.1.2. The results demonstrate that our proposed method significantly outperforms the adaptation of NBFNet used by InGram, highlighting the unique challenges and efficiency of our approach in this task setting."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8054/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700425817219,
                "cdate": 1700425817219,
                "tmdate": 1700428689266,
                "mdate": 1700428689266,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "a8frpNt67q",
                "forum": "UvRjDCYIHw",
                "replyto": "vuur3BQDuF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8054/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8054/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Comment 10. What are the differences between ISDEA, DEq-InGram, and InGram in terms of their runtime?**\n\n**A10:** Here we present a run time comparison for ISDEA+, ISDEA, DEq-InGram, InGram and NBFNet on the node prediction task on EN-FR in our PediaTypes dataset, and a much larger transductive FB15K-237 dataset. Due to varied mini-batch training sizes for different models, we report the training time per minibatch step (i.e., per gradient update step) for the training time comparison. Here are the results:\n\n**Training and inference on the EN-FR in PediaTypes**\n\n-   4962 entities, 122 relations, 30876 train triplets, 3326 validation triplets, 2485 test triplets\n   \n| Model | Training Time per Minibatch Step | Test Inference Time | Max GPU Memory Usage |\n|------------|----------------------------------|---------------------|----------------------|\n| ISDEA+ | 0.05s | 6.53s | 1.9GB |\n| ISDEA(old) | 2.56s | 359.45s | 9.8GB |\n| Deq-InGram | 0.09s | 10.39s | 3.0GB |\n| InGram | 0.09s | 5.60s | 3.0GB |\n| NBFNet | 0.02s | 5.14s | 5.5GB |\n\n**Training and inference on the transductive FB15k-237 dataset**\n\n-   14541 entities, 237 relations, 272115 train triplets, 17535 validation triplets, 20466 test triplets\n    \n| Model | Training Time per Minibatch Step | Test Inference Time | Max GPU Memory Usage |\n|------------|-----------------------------------|---------------------|----------------------|\n| ISDEA+ | 0.08s | 27.60s | 5GB |\n| ISDEA(old) | N/A | N/A | OOM |\n| Deq-InGram | 0.57s | 99.67s | 13GB |\n| InGram | 0.57s | 62.60s | 13GB |\n| NBFNet | 0.07s | 62.23s | 14GB |\n\nWe can see that ISDEA+ significantly improves ISDEA, and is among the fastest model with least memory consumption.\n\n**Comment 11. Since both DEq-InGram and InGram produce distributionally double equivariant representations, why is there a substantial performance gap between these models on both datasets?**\n\n**A11:** There appears to be a misunderstanding. InGram is a distributionally double equivariant representation (as per Lemma 3.2) and is sensitive to permutations. In contrast, DEq-InGram, is an (approximate) double-equivariant representation constructed based on Theorem 2.8, which is designed to be (approximately) permutation invariant. This difference in handling permutations contributes to the substantial performance gap between the two models. These results underscore the significance of double equivariant representations in doubly inductive link prediction tasks. Still, the new ISDEA+ consistently outperforms InGram and DEq-InGram in the revised submission.\n\n\n# Summary\n\nWe want to thank the reviewer for their time and insightful suggestions! We hope our answers can address your concerns well.\n\nWe also prudently ask you to reconsider our work if the concerns are addressed. With the new zero-shot meta-learning results and the significant performance boost of our new method ISDEA+, we hope the reviewer will reconsider their score.\n\nOverall, we believe our work makes important contributions in inductive learning on knowledge graphs, and proposes a new paradigm and novel architecture ISDEA+; we would appreciate your reconsideration on this point. Thank you for your efforts again!\n\n**References**\n\n[1] Jambor et al. \"Exploring the limits of few-shot link prediction in knowledge graphs.\" EACL 2021"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8054/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700425887712,
                "cdate": 1700425887712,
                "tmdate": 1700427437071,
                "mdate": 1700427437071,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "uVDV4jGLUZ",
            "forum": "UvRjDCYIHw",
            "replyto": "UvRjDCYIHw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8054/Reviewer_ZXyk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8054/Reviewer_ZXyk"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to address the so-called doubly inductive link prediction task, where both new nodes and new relation types can be found solely in test time. To this end, author proposes two different models, ISDEA and DEq-InGram, which all abides by the equivariance requirement. Finally, experiment results show the new method beats baseline empirically."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The result of the paper seems sound, the author provides the reader with many theorems and proofs for its theory and they seem plausible to me.\n\nThe experiments are good, including many baselines, and the empirical result shows that the new method is in general better (though it falls behind the baseline in some settings)."
                },
                "weaknesses": {
                    "value": "The design of ISDEA is very straightforward, however, it is purely brutal force and has very high complexity. I have checked the statistics of the dataset used for experiment evaluation and found these two newly crafted datasets are significantly smaller than commonly used datasets, like FB15k or even its subset FB15k-237. I believe one major motivation for the setting for inductive learning is to allow for scalability towards a larger knowledge graph, yet the model design seems to be in the opposite direction."
                },
                "questions": {
                    "value": "What is the largest knowledge graph that can be computed by ISDEA, for example, with GPU memory of 32 GB?\n\n\nCan the isomorphism requirement be reduced to some WL test to reduce the complexity yet maintain decent empirical results?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed.",
                        "Yes, Privacy, security and safety"
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8054/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8054/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8054/Reviewer_ZXyk"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8054/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698839887952,
            "cdate": 1698839887952,
            "tmdate": 1699636995456,
            "mdate": 1699636995456,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "I9M0bBA7S9",
                "forum": "UvRjDCYIHw",
                "replyto": "uVDV4jGLUZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8054/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8054/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## Official Response to Reviewer ZXyk\n\nWe appreciate your efforts and insightful comments! We would like to extend our sincere thanks to reviewer ZXyk for acknowledging the theoretical framework and comprehensive empirical results presented in our work. Your insightful inquiries and constructive feedback are immensely valuable to us. Please refer to the Common Reviewer Response for the most important revisions in our submission. To address your concerns, we provide point-by-point responses below.\n\n**Comment 1. The design of ISDEA is purely brute force and has very high complexity.**\n\n**A1:** Please see the new architecture of ISDEA (denoted ISDEA+) in our revised submission. The primary source of computation complexity in the old ISDEA is the process of enumerating across all $R$ relations and computing distances between every pair of nodes. To address this, as discussed in the Common Reviewer Response, we have developed ISDEA+, which improves the time complexity by $R$ over ISDEA (where $R$ is the number of relation types). In our experiments, we observe ISDEA+ is between 20x to 120x faster than ISDEA (old) .\n\n**Comment 2. The newly crafted datasets are significantly smaller than commonly used datasets, such as FB15K and its subset FB15K-237.**\n\n**A2:** Thank you for your question! It seems there's a bit of confusion regarding the size of the datasets. Our datasets, PediaTypes and WikiTopics, were created by selectively sampling from the OpenEA library (Sun et al., VLDB 2020) and WikiData-5M (Wang et al., TACL 2021). This was done specifically to test the model's proficiency in doubly inductive link prediction. While the transductive knowledge graph completion dataset FB15K-237 has a greater number of triplets compared to our datasets, the inductive versions of commonly used datasets like FB15K-237, WN18RR, and NELL995, which are employed in models like GraIL (refer to Table 13 in Teru et al., ICML 2020) and NBFNet (see Table 11 in Zhu et al., NeurIPS 2021), are generally smaller than our graphs. For instance, as illustrated in Appendix F.1.3, each graph in our WikiTopics dataset comprises 10,000 nodes, with a minimum of 12,516 and a maximum of 80,269 training triplets. In contrast, the inductive version of FB15K-237 used in NBFNet (Table 11 in Zhu et al., NeurIPS 2021) contains only between 5,226 and 33,916 training triplets, which is significantly fewer than our datasets. \n\nIn the revised version, we have added new zero-shot meta-learning experiments over larger graphs formed by combining up to three domains in training. These new zero-shot meta-learning experiments amount to up to training graphs with ~150,000 triplets.\n\n**Comment 3.  What is the largest knowledge graph that can be computed by ISDEA, for example, with GPU memory of 32 GB?**\n\n**A3:** Thank you for the great questions! We use the commonly-used transductive FB15K-237 KG to test the memory consumptions for our model along with run time comparison. Here is the result:\n\n**Training and inference on the transductive FB15k-237 dataset**\n- 14541 entities, 237 relations, 272115 train triplets, 17535 validation triplets, 20466 test triplets\n\n| Model      | Training Time per Minibatch Step  | Test Inference Time | Max GPU Memory Usage |\n|------------|-----------------------------------|---------------------|----------------------|\n| ISDEA+     | 0.08s                             | 27.60s              | 5GB                  |\n| ISDEA      | N/A                               | N/A                 | OOM                  |\n| Deq-InGram | 0.57s                             | 99.67s              | 13GB                 |\n| InGram     | 0.57s                             | 62.60s              | 13GB                 | \n| NBFNet     | 0.07s                             | 62.23s              | 14GB                 |\n\nIndeed, we have found that the original implementation of ISDEA (old) encountered an out of memory (OOM)  issue when training on this graph, while the new architecture of ISDEA+ can be trained with only 5GB max GPU memory usage, significantly better than InGram, DEq-InGram, and NBFNet. Due to time constraints, we did not test on larger graphs, but the model should be able to manage a much larger dataset on a 32GB GPU. Overall, we find that ISDEA+ is among the fastest models with least memory consumption."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8054/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700425626790,
                "cdate": 1700425626790,
                "tmdate": 1700426884152,
                "mdate": 1700426884152,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]