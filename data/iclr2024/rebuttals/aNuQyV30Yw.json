[
    {
        "title": "An Image is Worth Multiple Words: Learning Object Level Concepts using Multi-Concepts Prompts Learning"
    },
    {
        "review": {
            "id": "WwFAhRCbxh",
            "forum": "aNuQyV30Yw",
            "replyto": "aNuQyV30Yw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1277/Reviewer_wHAP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1277/Reviewer_wHAP"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces the Multi-Concept Prompt Learning (MCPL) framework for simultaneously learning multiple prompts from one scene in order to address the challenge of managing multiple concepts in scenes with multiple objects. The authors conducted a motivational study to investigate the limitations of existing prompt learning methods in multi-concept settings and found that object-level learning and editing without manual intervention remains challenging. To enhance prompt-object level correlation, the authors propose regularization techniques including Attention Masking (AttnMask) and Prompts Contrastive Loss (PromptCL). Experimental results demonstrate that the MCPL framework enables enhanced precision in object-level concept learning, synthesis, editing, quantification, and understanding of relationships between multiple objects."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tA novel task of the Multi-Concept Prompt Learning (MCPL) framework, which enables simultaneous learning of multiple prompts from one scene. This approach addresses the challenge of learning multiple concepts within multi-object scenes, which has not been previously explored.\n\n2.\tEnhanced Object-Level Concept Learning: The proposed MCPL framework demonstrates enhanced precision in object-level concept learning, synthesis, editing, quantification, and understanding of relationships between multiple objects. This is validated through extensive quantitative analysis and evaluation of learned object-level embeddings.\n\n3.\tThe paper proposes several regularization techniques to enhance the accuracy of prompt-object level correlation. These techniques restrict prompt learning to relevant regions, facilitate disentanglement of prompt embeddings, and the use of descriptive adjective words to bind each learnable prompt. These effective techniques contribute to learning object-level information under image-level supervision."
                },
                "weaknesses": {
                    "value": "1.\tThe aim of this paper is to learn and compose multiple concepts in the same scene. However, the demonstrations to prove the composing ability are insufficient. In almost all demonstrations the concepts are composed in the same string as training examples without any changes and only some editing demonstrations are available. Upon the interaction between the two concepts is changed, the effects seem to be worse.\n\n2.\tSome writing mistakes exist in the paper. In the top right of Figure 3, the labeling of \u201con\u201d and \u201cunder\u201d should be reversed. On page 6, \u201cThe\u201d in \u201cTherefore The contrastive loss\u201d should be lowercase in line 8 of the paragraph before \u201cImplementation details\u201d.\n\n3.\tThe experiment is a little confusing. In the section \u201cBaselines and experiments\u201d on page 7, the author presents four learning methods to compare their effectiveness. However, the author seems not to explain the meaning of the first setting called textural Inversion applied to unmasked multi-concept images and the subsequent experiments don\u2019t contain the effects of the first two settings. The detail and comparison for a setting called \u201cMCPL-diverse\u201d are also unavailable.\n\n4.\tThe paper constructs a new dataset to evaluate the proposed framework for multi-concept learning. However, I see all provided examples contain only two distinct concepts so I doubt the generalization of this method."
                },
                "questions": {
                    "value": "Please see the weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1277/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698219739876,
            "cdate": 1698219739876,
            "tmdate": 1699636054466,
            "mdate": 1699636054466,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YlhqlYh4Dd",
                "forum": "aNuQyV30Yw",
                "replyto": "WwFAhRCbxh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1277/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1277/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Respond to R4 (wHAP)"
                    },
                    "comment": {
                        "value": "We are pleased to note that the reviewer **recognises the novel task** of learning multiple concepts within multi-object scenes under image-level supervision, which has **not been previously explored** and the proposed Multi-Concept Prompt Learning (MCPL) framework in conjunction with proposed regularisation techniques **effectively tackles such challenge**. Specifically, we are gratified that the reviewer:\n\n* Confirmed our observations from the motivational study that **existing method has limitations in object-level learning and editing without manual intervention**;\n* Agreed that our **extensive quantitative analysis and evaluation** show that the proposed approaches effectively enhanced precision in object-level concept learning, synthesis, editing and quantification.\n* Acknowledged **our proposed regularisation terms effective techniques contribute to learning object-level information under image-level supervision2**;\n\nWe are also thankful for the reviewer\u2019s identification of certain weaknesses and questions in our work. We have taken these points into careful consideration, performed extensive additional experiments and provided clarifications as follows.\n\n---\n\n > _\u2026 the demonstrations to prove the composing ability are insufficient..upon the interaction between the two concepts is changed._\n\n> _\u2026 However, I see all provided examples contain only two distinct concepts so I doubt the generalization of this method._\n\n* We thank the reviewer for raising this important experiment. Both \u2018composing capability\u2019 and \u2018learning more than two concepts' are covered in Figure 11-12 of Section A.2, which involves **learning three or more concepts from a single image and composing a subset in a new scene**. \n* **Both tasks involve composing with strings differ from training**, our method generally learns targeted concepts and showed decent composing capability, outperforming Textural Inversion and DreamBooth, and was closer to **Break-A-Scene**, which represents the **state-of-the-art and a performance upper-bound in masked-based multi-concept learning**. **Unlike BAS, our approach neither uses segmentation masks as input nor updates model parameters.**\n\n---\n\n > _\u2026 The detail and comparison for a setting called \u201cMCPL-diverse\u201d are also unavailable._\n\n* We are thankful for the reviewer\u2019s question and interest in \u201cMCPL-diverse\u201d \u2014 a powerful but under-explored variant of our method in the original submission. We perform new experiments and provide clarifications as follows.\n* **MCPL-diverse v.s. MCPL-one with segmentation masks**: we add new qualitative results in Figures 13 and 14 in Section A.3 for the task of learning per-image different concepts. In both examples, as **MCPL-diverse is specially designed for such tasks, it consistently outperforms MCPL-one**.\n* MCPL-diverse has also been found powerful in more competitive tasks of learning more than three concepts from a single image, following R2(RxXV)\u2019s suggestion on comparing competitive baseline Break-A-Scene, which represents the **state-of-the-art and a performance upper-bound in masked-based multi-concept learning**. **Unlike BAS, MCPL neither uses segmentation masks as input nor updates model parameters.**\n* While R2 noted it's somewhat **unfair to compare directly with BAS due to its segmentation inputs**, we add Figure 12 of Section A.2 for this ambitious comparison. In Figure 12, we evaluated both MCPL-one and MCPL-diverse, the latter employing random crops akin to BAS\u2019s \u2019union sampling\u2019 strategy. The random crops strategy leads to each image containing different multiple concepts. In such case, **we found MCPL-diverse is a perfect fit, outperforming MCPL-one and comparable to the ambitious BAS baseline.**\n\n---\n\n > _... not to explain the meaning of the first setting called textural Inversion applied to unmasked multi-concept images and the subsequent experiments don\u2019t contain the effects of the first two settings._\n\n* We thank the reviewer for identifying this clarification issue! The first setting called \u2018textural Inversion applied to unmasked multi-concept images was a very initial baseline we explored. It was designed to assess the learning of combined concepts (i.e. encoding two concepts in one prompt embedding). This baseline was evaluated by the t-SNE visualisation and because it was **less comparable to other experiments** hence has been presented in Figures 26 and 27 in Appendix A.5. \n* We apologise for the above confusion and revised the entire experiments section to further clarify all experiment setups.\n\n---\n\n > _Some writing mistakes exist in the paper. In the top right of Figure 3, the labelling of \u201con\u201d and \u201cunder\u201d should be reversed. On page 6, \u201cThe\u201d in \u201cTherefore The contrastive loss\u201d should be lowercase in line 8 of the paragraph before \u201cImplementation details\u201d._\n\n* We thank the reviewer for the thoughtful reviews and for pointing out those writing mistakes, we updated Figure 3 and corrected the writing mistake in the revised submission."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1277/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700711230455,
                "cdate": 1700711230455,
                "tmdate": 1700742193138,
                "mdate": 1700742193138,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "RNOEKnk97X",
            "forum": "aNuQyV30Yw",
            "replyto": "aNuQyV30Yw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1277/Reviewer_cRiH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1277/Reviewer_cRiH"
            ],
            "content": {
                "summary": {
                    "value": "To tackle the issue of learning multiple individual concepts simltaneously, in this paper, the authors propose MCPL. The authors first conduct preliminary studies to show that vanilla MCPL to jointly learn multiple concepts is feasible, but it is not adequate to learn correlations between objects and locate corresponding concepts. To tackle this issue, the authors propose three techniques: AttnMask, PromptCL, and PromptCL with Bind adj.. The proposed full MCPL-one can correctly recognize and localize different concepts."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed prompts contrastive loss as well as Bind adj. can effectively regularize the attention maps regarding concepts to localize onto correct position. And learning with AttnMask can also largely refine the attention mask boundary, thus reducing false positive attention values. All these methods benefit to textural inversion. \n\n2. Extensive experimental results illustrate that the proposed MCPL can effectively generate attention masks for corresponding concepts, which benefits to textural inversion task. \n\n3. The description of method and experiment section is polished and easy to understand."
                },
                "weaknesses": {
                    "value": "The main concern is the analysis between MCPL-diverse mentioned in preliminary study and the full version of MCPL-one. The authors could provide visualization of generated natural concepts from MCPL-diverse as well as corresponding generated segmentation masks to support the observation."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1277/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1277/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1277/Reviewer_cRiH"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1277/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698689260829,
            "cdate": 1698689260829,
            "tmdate": 1699636054392,
            "mdate": 1699636054392,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xcn1CbSRqm",
                "forum": "aNuQyV30Yw",
                "replyto": "RNOEKnk97X",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1277/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1277/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Respond to R3 (cRiH)"
                    },
                    "comment": {
                        "value": "We are pleased to note that the reviewer **recognises the challenge of learning multiple individual concepts simultaneously from a single image** and finds our work to be **well-motivated with detailed results**. Specifically, we are gratified that the reviewer:\n\n* Confirmed our proposed **three regularisation techniques**, Attention Masking, Prompts Contrastive Loss, and Bind Adjectives, **are effective and would benefit relevant works such as textural inversion**;\n* Agreed that our **extensive experiments** show that the proposed MCPL effectively creates attention masks for relevant concepts.\n* Acknowledged **the description of the method and experiment section is polished and easy to understand**;\n\nWe are also thankful for the reviewer\u2019s question and interest in **\u201cMCPL-diverse\u201d \u2014 a powerful but under-explored variation of our method in the original submission**. We perform new experiments and provide clarifications as follows\n\n > _\u2026 analysis between the MCPL-diverse mentioned in the preliminary study and the full version of MCPL-one. The authors could provide visualization of generated natural concepts from MCPL-diverse as well as corresponding generated segmentation masks to support the observation._\n\n* **MCPL-diverse v.s. MCPL-one with segmentation masks**: we add new qualitative results in Figures 13 and 14 in Section A.3 for the task of learning per-image different concepts. In both examples, as **MCPL-diverse is specially designed for such tasks, it consistently outperforms MCPL-one**.\n* MCPL-diverse has also been found powerful in more competitive tasks of learning more than three concepts from a single image, following **R2(RxXV)**\u2019s suggestion on comparing competitive baseline **Break-A-Scene**, which represents the **state-of-the-art and a performance upper-bound in masked-based multi-concept learning**. **Unlike BAS, our MCPL neither uses segmentation masks as input nor updates model parameters.**\n* While R2 noted it's somewhat **unfair to compare directly with BAS due to its relying on segmentation inputs**, we add Figure 12 of Section A.2 for this ambitious comparison. In the right example of Figure 12, we evaluated both MCPL-one and MCPL-diverse, the latter employing random crops akin to BAS\u2019s \u2019union sampling\u2019 strategy. The random crops strategy leads to each image containing different multiple concepts. In such case, **we found MCPL-diverse is a perfect fit, outperforming MCPL-one and comparable to the ambitious BAS baseline.**"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1277/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700710588180,
                "cdate": 1700710588180,
                "tmdate": 1700741727601,
                "mdate": 1700741727601,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "TJ0boi9ZRI",
            "forum": "aNuQyV30Yw",
            "replyto": "aNuQyV30Yw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1277/Reviewer_RxXV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1277/Reviewer_RxXV"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a Multi-Concept Prompt Learning (MCPL) method that extracts multiple prompts from single images under the stable diffusion framework. A motivation study is first provided to demonstrate the current limitation. The proposed method is based on Textual Inversion and incorporates multiple regularisations (including Attention Masking, Prompts Contrastive Loss, and Bind Adjective) to disentangle multiple objects. The concepts are learned from a new dataset and evaluated by two designed protocols, followed by application visualizations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "-  To achieve multi-concept extractions from a single image, the proposed method leverages novel regularisation losses without relying on any groundtruth object segmentation\n-  Overall, the paper is clearly structured and easy to follow. The motivational study introduces the problem and the current limitation in a systematic way\n-  The experiments are well-designed with both real-world categories and out-of-domain biomedical images involved during the evaluation\n-  Comprehensive analysis is conducted with t-SNE visualizations and embedding similarity evaluation."
                },
                "weaknesses": {
                    "value": "- Some recent works (such as Break-A-Scene) on similar tasks could also be mentioned in the related work section. On the other hand, though it would be a bit unfair to directly compare with Break-A-Scene (due to its given segmentation inputs), it could still be interesting to treat its performance as an upper bound and comment on how a good segmentation mask would affect the learned concepts.\n- More implementation details could be added, particularly on prompt initialization. It seems a bit unclear how to initialize all learnable embeddings by the same word, \u201cphoto\u201d in a random manner. Besides, one may be curious about how the number of prompts is determined, especially for MCPL-all.\n- It would be better to discuss the limitations of current work in the last section and point out some future improvement directions."
                },
                "questions": {
                    "value": "- It seems that all learned concepts (nouns) are associated with a pre-defined adjective description in the prompt (such as \u201ca green *\u201d). Are these adjectives playing roles in disentangling the concepts? What if the initial prompt is provided in the form \u201cA * and a @\u201d?\n- I wonder if there are any examples of cases when the number of objects in the image is mismatched with the number of learnable concepts"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1277/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1277/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1277/Reviewer_RxXV"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1277/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698785552329,
            "cdate": 1698785552329,
            "tmdate": 1699636054313,
            "mdate": 1699636054313,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yVOzLld19V",
                "forum": "aNuQyV30Yw",
                "replyto": "TJ0boi9ZRI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1277/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1277/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Respond to R2 (RxXV) --- Part 1"
                    },
                    "comment": {
                        "value": "We are pleased to note that the reviewer **recognises the challenge of multi-concept extractions from a single image** and finds our work to be **well-motivated with detailed results**. Specifically, we are gratified that the reviewer:\n\n* Confirmed our method **leverages novel regularisation losses without relying on any groundtruth object segmentation**;\n* Agreed that our **paper is clearly structured and easy to follow**. The **motivational study introduces the problem and the current limitation in a systematic way**.\n* Acknowledged that **experiments are well-designed and analysis are comprehensive**;\n\nWe are also thankful for the reviewer\u2019s identification of certain weaknesses and questions in our work. We have taken these points into careful consideration, performed extensive additional experiments and provided clarifications as follows.\n\n---\n\n > _W.1 - \u201cSome recent works (such as Break-A-Scene) on similar tasks could also be mentioned in the related work section. On the other hand, though it would be a bit unfair to directly compare with Break-A-Scene (due to its given segmentation inputs), it could still be interesting to treat its performance as an upper bound and comment on how a good segmentation mask would affect the learned concepts.\u201d_\n\n* We wholeheartedly concur and are thankful for the reviewer highlighting this significant recent reference, which had previously been overlooked in our work. In response, we have **conducted extensive experiments to incorporate Break-A-Scene into all our quantitative analyses and visual comparisons**. This inclusion enriches our study and provides a more comprehensive understanding of our method in the context of current advancements\n* Specifically, we have updated Figures 5, 7, 8, and 16-23 to compare against **Break-A-Scene as a performance upper bound** in our 16 concepts experiments. **This expansion includes an additional 160 single GPU runs, approximately 2 million pairwise similarities, and extensive human-in-the-loop pre- and post-processing for integration with Break-A-Scene**, as detailed in Section A.1. Moreover, Figures 11-12 in Section A.2 provide a **visual comparison with Break-A-Scene as well as other baselines**.\n* It is worth noting **BAS requires segmentation masks as input and employs separate segmentation models to produce masked objects**, hence integration BAS involves extensive human-in-the-loop pre- and post-processing efforts, as detailed in Section A.1. In contrast, **our method is mask-free at learning and employs its own AttnMask to generate masked objects.**\n* The following tables highlight new results in Figure 8, our fully regularised method (MCPL-one+CL+Mask) shows **competitive performance against BAS on natural images**. For the **out-of-domain (OOD) medical dataset, BAS leads in the DINOv1 space, but we're comparable in others**. This is **due to our less precise object masks compared to BAS's human-assisted MedSAM segmentation**, shown in Figures 5 and 6. \n\n**Object-level fidelity learned concepts referencing to \"ground truth\" (natural images)**\n\n| exp_names           | bert          | clip          | dinov1        | dinov2        |\n|---------------------|---------------|---------------|---------------|---------------|\n| MCPL-one+CL+Mask    | 0.271 \u00b1 0.078 | 0.821 \u00b1 0.050 | 0.534 \u00b1 0.112 | 0.991 \u00b1 0.006 |\n| BAS                 | 0.276 \u00b1 0.078 | 0.823 \u00b1 0.050 | 0.552 \u00b1 0.097 | 0.995 \u00b1 0.030 |\n\n**Object-level fidelity learned concepts referencing to \"ground truth\" (medical images)**\n\n| exp_names           | bert          | clip          | dinov1        | dinov2        |\n|---------------------|---------------|---------------|---------------|---------------|\n| MCPL-one+CL+Mask    | 0.262 \u00b1 0.072 | 0.792 \u00b1 0.037 | 0.407 \u00b1 0.081 | 0.983 \u00b1 0.009 |\n| BAS                 | 0.267 \u00b1 0.079 | 0.823 \u00b1 0.043 | 0.560 \u00b1 0.080 | 0.987 \u00b1 0.021 |\n\n* **The new t-SNE results in** Figure 7 demonstrate that the learned embeddings from both the mask-based \u2019ground truth\u2019 and BAS show less disentanglement compared to ours, attributable to their lack of a specific disentanglement objective, such as the PromptCL loss in MCPL.\n* We've reflected on all the mentioned points in our revision, including ensuring proper citation of Break-A-Scene throughout the paper."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1277/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700710073211,
                "cdate": 1700710073211,
                "tmdate": 1700740993505,
                "mdate": 1700740993505,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8A2wVxysYk",
                "forum": "aNuQyV30Yw",
                "replyto": "TJ0boi9ZRI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1277/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1277/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Respond to R2 (RxXV) --- Part 2"
                    },
                    "comment": {
                        "value": "> _W.3 - \u201cIt would be better to discuss the limitations of current work in the last section and point out some future improvement directions.\u201d_\n\n* We appreciate the reviewer's valuable suggestion! We've included a discussion on limitations and future work in the '**Limitation and Conclusion**' section, a copy of which is provided below for your convenience in reviewing.\n* We found our method to suffer from the following limitations: \n    * (1) **Imperfect Masking**: Our reliance on image-level text descriptions, instead of segmentation masks, grants flexibility in exploring unknown concepts but results in less precise object boundary optimization. Future research could use our AttnMask as an input prompt to segmentation models for mask refinement. \n    * (2) **Composition Capability**: MCPL\u2019s composition strength is weaker than BAS, as MCPL doesn\u2019t update model parameters, unlike BAS. Integrating MCPL with weight optimization methods like BAS or DreamBooth may enhance performance, albeit at higher computational costs, which is a potential direction for future work. \n    * (3) **Evaluation Metrics**: Current quantification methods in this field (e.g. TI, DB, CD, BAS, and P2P), predominantly rely on prompt/embedding similarity due to the absence of more effective quantification mechanisms without known ground truth. This indicates a need for developing better evaluation metrics in future research. \n    * (4) Our method **relies on adjectives** serving as textual descriptors (e.g., colour) to differentiate between multiple concepts. While human-machine interaction using purely linguistic descriptions is generally preferred, challenges arise when two concepts are very similar and lack distinct visual cues in the image. In such cases, our method may struggle, and Break-A-Scene currently offers the best solution.\t\t\t\t\t\n\n---\n\n> _Q.2 - I wonder if there are any examples of cases when the number of objects in the image is mismatched with the number of learnable concepts_\n\n* We showcase examples in Figure 11-12 of Section A.2, which involves: 1) learning three concepts from a single image and composing a subset in a new scene. 2) more challenging, entails learning six concepts from one image and composing 1 to 3 concepts in a novel scene.\n* In all **\u201cmismatched tasks\u201d**, our method generally learns targeted concepts and showed decent composing capability, **outperforming Textural Inversion and DreamBooth, and was closer to BAS.**\n* We conclude the **'Bind Adjectives' regularisation directs our model to the correct location**. This is evidenced by the **performance diminishes when adjectives are removed**, as seen in the right example of Figure 12. \n\n---\n\n > _Q.1 - It seems that all learned concepts (nouns) are associated with a pre-defined adjective description in the prompt (such as \u201ca green *\u201d). Are these adjectives playing roles in disentangling the concepts? What if the initial prompt is provided in the form \u201cA * and a @\u201d?_\n\n* We add Figure 15 in section A.4 for an ablation study on the impact of adjective words. **Adjective words are crucial in linking each prompt to the correct region**; without them, the model may struggle for regional guidance, **akin to the role of masks in Break-A-Scene**.  \n* Figure 38 highlights how each regularisation technique enhances object-level prompt-concept correlation in MCPL. The left example shows **reduced accuracy in attention and masks without Bind Adjectives**.\n* The aforementioned Figure 12-right results reinforce the significance of adjectives in maintaining model performance.\n\n---\n\n > _W.2 - More implementation details could be added, particularly on prompt initialization. It seems a bit unclear how to initialize all learnable embeddings by the same word, \u201cphoto\u201d in a random manner. _\n\n* It\u2019s a writing mistake, thank you for pointing it out! **To clarify, all learnable embeddings are initialised using the encoding of pseudowords like \u2018*\u2019 or '@', rather than random initialization.** We intended to convey that 'during training, sentences/phrases are constructed from randomly selected text templates y derived from CLIP ImageNet'. This has been corrected in the revised manuscript.\"\n\n---\n\n > _W.2 - Besides, one may be curious about how the number of prompts is determined, especially for MCPL-all._\n\n* For MCPL-all, we learn all prompts/words in the sentence."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1277/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700710385886,
                "cdate": 1700710385886,
                "tmdate": 1700741369151,
                "mdate": 1700741369151,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "N7K0FLDvfd",
            "forum": "aNuQyV30Yw",
            "replyto": "aNuQyV30Yw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1277/Reviewer_BYgx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1277/Reviewer_BYgx"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a method to associate new words into concepts. The proposes three techniques: attention masking, prompts contrastive loss, and bind adjective. The applications they adopted is the image synthesis / editing when replacing some of the original concepts in a sentence, with a different concept (i.e. image editing over disentangled concepts). They also claim to introduce a novel dataset for this application."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- They claimed to release code and dataset upon publication.\n - The paper targets important research areas."
                },
                "weaknesses": {
                    "value": "- The paper is not very clear to read. The idea seems to be straightforward, but the description of the method is a bit ambiguous. I have to read multiple times to make sure I understand the method accurately.\n - In experiments, the authors show multiple interesting qualitative results. However, there are very little quantitative results, and it is very hard to compare with other methods and understand the contribution of this effort."
                },
                "questions": {
                    "value": "Attention masks and contrastive loss on different concepts seems to be a widely used method. It would be great if the authors can explain a bit more about the novelty of their work."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1277/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1277/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1277/Reviewer_BYgx"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1277/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699259972679,
            "cdate": 1699259972679,
            "tmdate": 1699636054257,
            "mdate": 1699636054257,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8QU1bMyfN3",
                "forum": "aNuQyV30Yw",
                "replyto": "N7K0FLDvfd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1277/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1277/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Respond to R1 (BYgx) - Part 1"
                    },
                    "comment": {
                        "value": "We are pleased to note the reviewer's agreement that our work **targets critical research areas** in text-guided learning of multiple prompts from a single scene **without ground-truth object segmentation**. We are also thankful for the confirmation of our **contribution in introducing a novel dataset**.\n\nFurthermore, we acknowledge and appreciate the **reviewer's questions regarding novelty, quantitative evaluation and clarity in our manuscript**. We recognise that these issues may stem from misunderstandings. To address these concerns, we have provided detailed responses to each question below, aiming to clarify any ambiguities and enhance the overall comprehensibility of our work.\n\n---\n\n > _Q.1 - Attention masks and contrastive loss on different concepts seem to be a widely used method. It would be great if the authors can explain a bit more about the novelty of their work._\n\nWe appreciate the reviewer's inquiry and are pleased to clarify our contributions as follows:\n\n* Firstly, we highlight the consensus among all reviewers on **our work targets on important research areas** of **mask-free text-guided learning of multiple prompts from a single scene**. \n* We introducing Multi-Concept Prompt Learning (MCPL), **the first** to achieve multi-concept learning **without relying on object masks as input**. This novelty has been recognised by all the other reviewers.\n* Our approach **differs to the leading mask-based method**, Break-A-Scene (BAS) as follows:\n    * BAS learns multiple concepts from images paired with object-level masks but we don\u2019t use a mask;\n    * BAS updates both textural embeddings and model weights but we don\u2019t update model weights; \n* We also summarise **other key contributions** our work have made:\n    * The **motivational study** introduces the problem and the current limitation with vanilla MCPL. Addressing this, we propose **three innovative regularisation techniques**: Attention Masking, Prompts Contrastive Loss, and Bind Adjectives, to disentangle multiple object concepts and enhance the accuracy of object-level concept learning.\n    * **Extensive experiments and analysis** demonstrate superior performance with our method against competitive baselines, delivering results comparable to the leading mask-based method, Break-A-Scene. This **highlights the potential of our mask-free approach**. We also provide detailed visualisations to showcase the practical applications of our method.\n    * We have compiled a **new dataset** for comprehensive evaluation. This dataset involves both in-domain natural images and out-of-domain biomedical images, **setting up a new benchmark for the field**.\n    * Overall, our approach not only enhances current methodologies but also **paves the way for novel applications**, such as facilitating knowledge discovery through natural language-driven interactions between humans and machines.\n* All of the above clarifications and contributions have been reflected in our revised manuscript."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1277/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700709117704,
                "cdate": 1700709117704,
                "tmdate": 1700740552799,
                "mdate": 1700740552799,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lI9KAuPOeM",
                "forum": "aNuQyV30Yw",
                "replyto": "N7K0FLDvfd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1277/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1277/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Respond to R1 (BYgx) - Part 2"
                    },
                    "comment": {
                        "value": "> _W.2 - However, there are very little quantitative results, and it is very hard to compare with other methods and understand the contribution of this effort._\n\n* We are grateful to the reviewer for highlighting a potential limitation in our study. As our work is **pioneering** in the field of multi-concept learning **without ground-truth object segmentation**, we acknowledge the **scarcity of directly comparable baselines**.\n* **Our original experiments were extensive**, taking about 80 days on a single GPU, a fact recognised by other reviewers. However, following R2's advice, we **added Break-A-Scene (BAS) as an additional benchmark**. We perform **extensive new experiments resulting in an extra seven pages in the revised version.** We have updated Figures 5, 7, 8, 16-23, adding Figures 10-12, and detailing in Sections A.1 and A.2 for a comprehensive comparison. This involved 160 more single GPU runs and around 2 million pairwise comparisons.\n* This thorough approach ensures our approach is **quantitatively compared** against **three baselines** (Textural Inversion, MCPL, and BAS) and a **qualitative comparison** against an **additional five baselines** (Figure 11-12), comprehensively covering the scope of the field.\n* It worth noting **BAS requires segmentation masks as input and employs separate segmentation models to produce masked objects**, hence integration BAS involves extensive human-in-the-loop pre- and post-processing efforts, as detailed in Section A.1. In contrast, **our method is mask-free at learning and employed its own AttnMask to generate masked objects.**\n* In the following results (Figure 8 in table format), our fully regularised method (MCPL-one+CL+Mask) shows **competitive performance against BAS on natural images**. For the **out-of-domain (OOD) medical dataset, BAS leads in the DINOv1 space, but we're comparable in others**. This is **due to our less precise object masks compared to BAS's human-assisted MedSAM segmentation**, shown in Figures 5 and 6.\n\n**Object-level fidelity learned concepts referencing to \"ground truth\" (natural images)**\n\n| exp_names           | bert          | clip          | dinov1        | dinov2        |\n|---------------------|---------------|---------------|---------------|---------------|\n| MCPL-all            | 0.235 \u00b1 0.076 | 0.742 \u00b1 0.050 | 0.311 \u00b1 0.115 | 0.983 \u00b1 0.010 |\n| MCPL-one            | 0.235 \u00b1 0.073 | 0.760 \u00b1 0.065 | 0.388 \u00b1 0.143 | 0.978 \u00b1 0.014 |\n| MCPL-all+CL         | 0.247 \u00b1 0.081 | 0.756 \u00b1 0.051 | 0.346 \u00b1 0.115 | 0.983 \u00b1 0.010 |\n| MCPL-all+CL+Mask    | 0.251 \u00b1 0.083 | 0.756 \u00b1 0.057 | 0.362 \u00b1 0.128 | 0.982 \u00b1 0.012 |\n| MCPL-one+CL         | 0.265 \u00b1 0.089 | 0.791 \u00b1 0.061 | 0.467 \u00b1 0.128 | 0.986 \u00b1 0.010 |\n| MCPL-one+CL+Mask    | 0.271 \u00b1 0.078 | 0.821 \u00b1 0.050 | 0.534 \u00b1 0.112 | 0.991 \u00b1 0.006 |\n| BAS                 | 0.276 \u00b1 0.078 | 0.823 \u00b1 0.050 | 0.552 \u00b1 0.097 | 0.995 \u00b1 0.030 |\n\n**Object-level fidelity learned concepts referencing to \"ground truth\" (medical images)**\n\n| exp_names           | bert          | clip          | dinov1        | dinov2        |\n|---------------------|---------------|---------------|---------------|---------------|\n| MCPL-all            | 0.240 \u00b1 0.065 | 0.787 \u00b1 0.046 | 0.349 \u00b1 0.069 | 0.977 \u00b1 0.011 |\n| MCPL-one            | 0.250 \u00b1 0.069 | 0.776 \u00b1 0.044 | 0.329 \u00b1 0.062 | 0.982 \u00b1 0.010 |\n| MCPL-all+CL         | 0.251 \u00b1 0.083 | 0.785 \u00b1 0.040 | 0.339 \u00b1 0.062 | 0.982 \u00b1 0.011 |\n| MCPL-all+CL+Mask    | 0.264 \u00b1 0.065 | 0.785 \u00b1 0.045 | 0.353 \u00b1 0.068 | 0.982 \u00b1 0.010 |\n| MCPL-one+CL         | 0.247 \u00b1 0.070 | 0.791 \u00b1 0.047 | 0.376 \u00b1 0.095 | 0.981 \u00b1 0.012 |\n| MCPL-one+CL+Mask    | 0.262 \u00b1 0.072 | 0.792 \u00b1 0.037 | 0.407 \u00b1 0.081 | 0.983 \u00b1 0.009 |\n| BAS                 | 0.267 \u00b1 0.079 | 0.823 \u00b1 0.043 | 0.560 \u00b1 0.080 | 0.987 \u00b1 0.021 |\n\n* **The new t-SNE results in** Figure 7 demonstrate that the learned embeddings from both the mask-based \u2019ground truth\u2019 and BAS show less disentanglement compared to ours, attributable to their lack of a specific disentanglement objective, such as the PromptCL loss in MCPL.\n\n---\n\n > _W.1 - The paper is not very clear to read. The idea seems to be straightforward, but the description of the method is a bit ambiguous\u2026_\n\n* We thank the reviewer for pointing out the clarity issue in our manuscript. In response, we have **simplified certain descriptions in the methods section** to enhance clarity.\n* We also acknowledge our methods section combined method descriptions with preliminary motivational experiments, and this design might have disrupted the flow of the method description. To address this, we have **reorganised the methods section, including adjustments to Figure 4**, to ensure a more coherent and seamless presentation of our methods."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1277/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700709710567,
                "cdate": 1700709710567,
                "tmdate": 1700740790990,
                "mdate": 1700740790990,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]