[
    {
        "title": "Spawrious: A Benchmark for Fine Control of Spurious Correlation Biases"
    },
    {
        "review": {
            "id": "16Hgrs2pA5",
            "forum": "W0zgCR6FIE",
            "replyto": "W0zgCR6FIE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5951/Reviewer_d4Gu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5951/Reviewer_d4Gu"
            ],
            "content": {
                "summary": {
                    "value": "The paper studied the spurious correlation (SC) problem. To study this problem, the paper introduces the Spawrious dataset. Unlike previous SC benchmarks that only contain one-to-one SCs, the Spawrious benchmark introduces new many-to-many SCs that jointly consider spurious correlations and domain generalization problems. The images are generated using Stable Diffusion v1.4. The experimental results show that many group robustness methods struggle with the new benchmark."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The paper evaluated many (11) group robustness methods (Table 3) on the newly introduced dataset.\n* The discussion on potential ethical concerns about using Stable Diffusion to generate training images (Appendix C) is appreciated.\n* The paper is well-written."
                },
                "weaknesses": {
                    "value": "### Major Concerns\n\n**[Benchmark W2D]**: Although the paper evaluates many group robustness methods in Table 3, none of them is designed to handle both correlation shift and domain shift. Why not evaluate the W2D method (Huang et al. 2022) that is designed to handle two shifts, which is the main focus of Spawrious?\n\n**[More comprehensive benchmark of architecture]**: Wenzel et al. [1] did a comprehensive evaluation of OOD generalization. One of the conclusions is that architectures (such as Deit, Swin, and ViT) play a key role in improving OOD robustness. Although the paper benchmarks many group robustness methods and compares two architectures (Appendix D), I think it is necessary to see more results in terms of different neural architectures on the new benchmark based on conclusions in [1].\n\n### Minor Concerns\n\n**[More results of foundation models]**: To show that this Spawrious is really challenging, I think we evaluate the performance of foundation models (e.g., CLIP [2] with zero-shot transfer) pretrained on web-scale datasets.\n\nI wonder why the authors argued that ImageNet-W (Li et al, 2023) is synthetic (Section 2, page 3). The watermark shortcut in ImageNet-W naturally exists in the real-world ImageNet dataset.\n\n### References\n\n[1] Florian Wenzel, Andrea Dittadi, Peter Vincent Gehler, Carl-Johann Simon-Gabriel, Max Horn, Dominik Zietlow, David Kernert, Chris Russell, et al., \u201cAssaying Out-Of-Distribution Generalization in Transfer Learning,\u201d in NeurIPS, 2022.\n\n[2] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, et al., \u201cLearning Transferable Visual Models From Natural Language Supervision,\u201d in ICML, 2021."
                },
                "questions": {
                    "value": "In the rebuttal, I expect the authors to address my concerns:\n\n1. Add results of W2D.\n2. Add more results by using different architectures.\n3. Add results of CLIP or other foundation models to better demonstrate how challenging the benchmark is."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5951/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698734996524,
            "cdate": 1698734996524,
            "tmdate": 1699636634683,
            "mdate": 1699636634683,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ffPfqStK3z",
                "forum": "W0zgCR6FIE",
                "replyto": "16Hgrs2pA5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5951/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5951/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the constructive feedback that improves the quality of our submission! We address your comments below.\n\n> Add results of W2D.\n\nThank you for highlighting this method to us; we have since collected results for W2D and added the results to Table 3 in the revised submission. Averaged across all Spawrious challenges, W2D ranks 6th out of 12 methods.\n\n> Add more results by using different architectures.\n\nThank you for this suggestion! We have since included results in the appendix for the effects of using the Vit-B/16 architecture instead of the ResNet-50. The results we included in the initial submission remain the strongest. \n\n> Add results of CLIP or other foundation models to demonstrate better how challenging the benchmark is.\n\nThank you for suggesting an interesting direction for investigation! We shall return to you with the results of fine-tuning a head applied to a CLIP feature extractor on the Spawrious challenges. \n\nWe believe there is some slight confusion \u2013 the problem setup of domain generalization assumes some distribution shift between training and test domains. Consequently,  training methods addressing this problem have only access to training domains that differ from the test domains. \nFor a fair evaluation of such methods, the models must be trained on training data that strictly differs from the test data. In the case of CLIP, the training set is unknown and vast, consisting of internet-crawled data. Hence, the CLIP model may have already been trained on {class, background} combinations similar to the ones in the test data. \nHowever, this result may interest the OOD community, and we will attempt to gather results for the next revision."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5951/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700399454674,
                "cdate": 1700399454674,
                "tmdate": 1700411182071,
                "mdate": 1700411182071,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Qk4qh1BvQa",
                "forum": "W0zgCR6FIE",
                "replyto": "ffPfqStK3z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5951/Reviewer_d4Gu"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5951/Reviewer_d4Gu"
                ],
                "content": {
                    "comment": {
                        "value": "### Results of W2D\nI appreciate the authors for adding the results of W2D, which addressed my concerns.\n\n### Results of different architectures\nI appreciate the authors' efforts in adding the ViT-B/16 architecture. However, it still lacks the comprehensiveness to cover more architectures, such as Deit, Swin, and different sizes of these architectures. Thus, my concern on this is not fully addressed.\n\n### Results of foundation models\nI appreciate the authors' promise to add the results of CLIP. However, I wonder whether the results will be added during the rebuttal.\n\nI appreciate the explanations on the domain generalization setup. However, the purpose of evaluating foundation models is to see if the proposed Spawrious challenge is sufficiently challenging to fail foundation models. Otherwise, if the domains can be easily seen through web-crawled images, then I am afraid that combining domain generalization with spurious correlation (i.e., the key contribution of Spawrious compared to existing benchmarks) may not be realistic since the former can be easily addressed by models trained on web-crawled data (e.g., CLIP) whereas the latter remains unaddressed."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5951/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700718314444,
                "cdate": 1700718314444,
                "tmdate": 1700718314444,
                "mdate": 1700718314444,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "l2689cvrtq",
                "forum": "W0zgCR6FIE",
                "replyto": "16Hgrs2pA5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5951/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5951/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Results of different architectures"
                    },
                    "comment": {
                        "value": "> I appreciate the authors' efforts in adding the ViT-B/16 architecture. However, it still lacks the comprehensiveness to cover more architectures, such as Deit, Swin, and different sizes of these architectures. Thus, my concern on this is not fully addressed.\n\nSorry, we were just in the process of posting more architecture results and it took us a bit more time than we anticipated.\n \nHere are more test accuracy results for the hardest challenge (M2M-Hard) trained with ERM; for comparison, we also include the ResNet50 results from our initial submission.\n\n| Architecture | M2M Hard  |\n|--------------|-----------|\n| [Swin-B-4](https://huggingface.co/timm/swin_base_patch4_window7_224.ms_in22k_ft_in1k) | 67.07%   |\n| [DeiT-III-B-16](https://huggingface.co/timm/deit3_base_patch16_224.fb_in22k_ft_in1k) | 77.41%   |\n| [Beit-B-16](https://huggingface.co/timm/beit_base_patch16_224.in22k_ft_in22k_in1k) | 66.79%   |\n| [LeViT-128s](https://huggingface.co/timm/levit_128s.fb_dist_in1k) | 45.41%   |\n| [Eva-B-14](https://huggingface.co/timm/eva02_base_patch14_448.mim_in22k_ft_in22k_in1k) | 68.02%   |\n| [ViT-B-16](https://pytorch.org/vision/main/models/generated/torchvision.models.vit_b_16.html) | 30.20%    |\n| ResNet50     | 58.70%    |\n\n\nAfter the rebuttal, we will add results for all other challenges too (ie results for {all challenges} x {all architectures}).\n\n\n> I appreciate the authors' promise to add the results of CLIP. However, I wonder whether the results will be added during the rebuttal.\n\nAgain, our sincere apologies for letting you wait. Instead of the original, by now outdated CLIP checkpoint, we fine-tune via ERM the state-of-the-art [SigLIP-14 model](https://huggingface.co/timm/ViT-SO400M-14-SigLIP-384) [1] pre-trained on 400 million images (like CLIP) of the WebLI dataset. For comparison, we also include the ResNet50 results from our initial submission.\n\n| Architecture          | O2O Easy       | O2O Medium      | O2O Hard       | M2M Easy       | M2M Medium      | M2M Hard       |\n|-----------------------|----------------|-----------------|----------------|----------------|-----------------|----------------|\n| [SigLip](https://huggingface.co/timm/ViT-SO400M-14-SigLIP-384)   | 60.32         | 72.77           | 61.42          | 73.58          | 49.56           | 57.51          |\n| ResNet50              | 77.49  | 76.60  | 71.32 | 83.80  | 53.05   | 58.70  |\n\n\n\n[1]: [Sigmoid Loss for Language Image Pre-Training, Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, Lucas Beyer, ICCV'23 Oral.](https://arxiv.org/abs/2303.15343)\n\n\n> I appreciate the explanations on the domain generalization setup. However, the purpose of evaluating foundation models is to see if the proposed Spawrious challenge is sufficiently challenging to fail foundation models. Otherwise, if the domains can be easily seen through web-crawled images, then I am afraid that combining domain generalization with spurious correlation (i.e., the key contribution of Spawrious compared to existing benchmarks) may not be realistic since the former can be easily addressed by models trained on web-crawled data (e.g., CLIP) whereas the latter remains unaddressed.\n\nAgreed, based on the above results, we conclude that the Spawrious challenges remain sufficiently challenging, even for foundation models pre-trained on web-crawled data."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5951/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740680892,
                "cdate": 1700740680892,
                "tmdate": 1700741314333,
                "mdate": 1700741314333,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ri1ehFQYxw",
            "forum": "W0zgCR6FIE",
            "replyto": "W0zgCR6FIE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5951/Reviewer_evLb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5951/Reviewer_evLb"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a new image classification benchmark dataset called \"Spawrious\" that addresses the problem of spurious correlations in classifiers. Spawrious contains both one-to-one (O2O) and many-to-many (M2M) spurious correlations between classes and backgrounds in images. The dataset is carefully designed to meet six specific desiderata and is generated using text-to-image and image captioning models, resulting in ~152k high-quality images.\n\nExperimental results show that even state-of-the-art group robustness methods struggle with the Spawrious dataset, especially in challenging scenarios (Hard-splits) where accuracy remains below 73%. Model misclassifications reveal a reliance on irrelevant backgrounds, highlighting the significant challenge posed by the dataset. Experimental results demonstrate the difficulty of the dataset and the limitations of current group robustness techniques."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The strengths of the paper can be summarized as follows:\n\nNovel Benchmark Dataset: The paper introduces a novel benchmark dataset, Spawrious, which contains a wide range of spurious correlations, including both one-to-one and many-to-many relationships. This dataset offers three difficulty levels (Easy, Medium, and Hard) for evaluating the robustness of classifiers against spurious correlations. The dataset consists of approximately 152,064 high-resolution images of 224 \u00d7 224 pixels. The dataset's size and quality make it a valuable resource for testing and probing classifiers' reliance on spurious features.\n\nExperimental evaluation: The paper explores different model architectures and robustness methods, evaluating their performance on the dataset, revealing that larger architectures can sometimes improve performance but the gains are inconsistent across methods. The experimental results demonstrate that state-of-the-art methods struggle to perform well on the Spawrious dataset, particularly in the most challenging scenarios (Hard-splits) where accuracy remains below 73%. This highlights the dataset's effectiveness in pushing the boundaries of current classifier robustness. The paper provides evidence for the reliance of models on spurious features through an analysis of model misclassifications. \n\nOverall, the strengths of the paper lie in its creation of a challenging benchmark dataset and the empirical evidence it provides about the limitations of state-of-the-art methods in handling spurious correlations in image classification, stimulating the need for future research and developments in this domain."
                },
                "weaknesses": {
                    "value": "t would have been better if the paper included empirical results on some well known domain generalization datasets (using the same methods as the ones in Table 3). \nBy comparing between the accuracy of various methods on multiple such datasets, the case could be made stronger for the paper introducing a strong benchmark for spurious correlations. One such dataset could be: FOCUS: Familiar Objects in Common and Uncommon Settings\n\nHowever, the paper is not cited. Nor is any such comparison provided. \n\nMoreover, no details are provided for any filtering of the model generated images. There should be some human study in the paper which shows what percentage of images generated by the diffusion models are aligned with the the prompts. If the images generated are not aligned with the prompt, the dataset cannot be trusted to contain the variety of spurious correlations."
                },
                "questions": {
                    "value": "Did the authors conduct any crowd study to show that the images generated by the diffusion model follow the intent of the prompt? If not, it is hard to say whether the generated data actually contains the different images in different backgrounds and can be useful for research."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5951/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698781336079,
            "cdate": 1698781336079,
            "tmdate": 1699636634584,
            "mdate": 1699636634584,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Bety7fcqjq",
                "forum": "W0zgCR6FIE",
                "replyto": "ri1ehFQYxw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5951/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5951/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the constructive feedback that improves the quality of our submission! We address your comments below.\n\n> Did the authors conduct any crowd study to show that the images generated by the diffusion model follow the intent of the prompt? If not, it is hard to say whether the generated data actually contains the different images in different backgrounds and can be useful for research.\n\nWe have since sent out 480 random samples of our dataset to human volunteers who have been asked to decide on the image prompt alignment, and have found that 97.2 % of the dataset is clean. Additional details are provided in Appendix H of the updated submission.\n\n> By comparing between the accuracy of various methods on multiple such datasets, the case could be made stronger for the paper introducing a strong benchmark for spurious correlations. One such dataset could be: FOCUS: Familiar Objects in Common and Uncommon Settings. However, the paper is not cited. Nor is any such comparison provided.\n\nThanks for pointing this out! We admire the effort by the authors of the FOCUS work to curate a dataset with unfamiliar class-background combinations. While they examine an image classifier\u2019s OOD performance on their dataset, they do not attempt to introduce spurious correlations between class and background in their benchmark, nor do they attempt to balance the datasets by class and group. Hence, Spawrious provides a fundamentally different contribution to the OOD community. We will cite the FOCUS dataset and add this comparison to the revision."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5951/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700399149875,
                "cdate": 1700399149875,
                "tmdate": 1700410831539,
                "mdate": 1700410831539,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "IjkmIU3g2I",
            "forum": "W0zgCR6FIE",
            "replyto": "W0zgCR6FIE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5951/Reviewer_GoTk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5951/Reviewer_GoTk"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a new benchmark for assessing algorithms for training models to be robust to spurious signals. The benchmark uses a text-to-image model to generate inputs under different specifications, i..e input text, which allows one to control the difficulty of the task. Because of this text-based control, the benchmark has a many-to-many spurious signal set, which can be completely reversed between training and testing---a challenging condition for current algorithms. The primary task is dog classification. This paper then tests several approaches for training a model to be robust to spurious correlations, and finds that Mixup does particularly well for many-to-many spurious correlation and Just-train-twice is the best performing for One-to-One spurious settings."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "Overall, I enjoyed reading this paper, and think it was well executed. Here I discuss some of these key strengths. \n\n- **Nice Dataset Design**: I particularly enjoyed the use of the image-to-text models here for performing dataset design. I think this type of dataset design is going to be increasingly common for various settings. Essentially the design here it to use a text-conditioned generative model to create toy datasets where the data generation process is carefully controlled to induce various proportions of features of interest. This is approach was also used in the Instructpix2pix paper. \n\n- **Scale of Empirical Assessments**: The coverage in algorithms here is also quite substantial since this literature is quite active. To my count, the authors test 10 methods across 6 settings, which is a substantial amount of work, and commendable."
                },
                "weaknesses": {
                    "value": "I have two weaknesses with this work, but they don't factor into my rating. \n\n- **Failures of Dataset Design**: My first issue is about how to verify whether the output of the text-to-image model matches and satisfies **all** conditions or features specified in the prompt. The authors discuss this issue in Appendix F. The authors attempt a manual and an automatic filtering process. However, both of these also might be susceptible to failure in different ways.\n\n- **Insights**: Table 1 is a very compelling result for mixup, and it points at trying to better understand its properties theoretically w.r.t. to spurious signals. Given the already wide scope of this paper, it is unable to delve into explaining the effectiveness of various methods."
                },
                "questions": {
                    "value": "- In Table 3, how is this average computed? Also, does it make sense to report an average if it includes a combination of different settings/environments?\n\n- Section E of the appendix is very interesting. Did you also happen to compute the saliency maps for the mixup models on these same inputs? Would be interesting to compare both of them."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5951/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698860652197,
            "cdate": 1698860652197,
            "tmdate": 1699636634480,
            "mdate": 1699636634480,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Jfpnv0MxWh",
                "forum": "W0zgCR6FIE",
                "replyto": "IjkmIU3g2I",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5951/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5951/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the constructive feedback that improves the quality of our submission! We address your comments below.\n\n> The authors attempt a manual and an automatic filtering process. However, both of these also might be susceptible to failure in different ways.\n\nThis is a valid point, and we appreciate your insight. Our automatic filtering process may fail due to errors from the captioning model, and the human filtering part might fail when we do not reach out to a variety of human volunteers to assess the alignment of the image-prompt alignment. We have since sent out 480 random samples of our dataset to human volunteers who have been asked to decide on the image prompt alignment and have found that 97.2 % of the dataset is clean. Additional details are provided in Appendix H of the updated submission.\n\n> In Table 3, how is this average computed? Also, does it make sense to report an average if it includes a combination of different settings/environments?\n\nThis average is computed as the average of the row, giving the average Spawrious benchmark performance for a given optimization algorithm.\n\n> Section E of the appendix is very interesting. Did you also happen to compute the saliency maps for the mixup models on these same inputs? Would be interesting to compare both of them.\n\nThank for raising an interesting direction of investigation! We are currently running a training loop for ERM and Mixup models, and will present saliency maps of their misclassifications, in order to investigate reasons for Mixup\u2019s superior test performance."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5951/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700399122053,
                "cdate": 1700399122053,
                "tmdate": 1700405263335,
                "mdate": 1700405263335,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Np47Xl2ijI",
                "forum": "W0zgCR6FIE",
                "replyto": "Jfpnv0MxWh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5951/Reviewer_GoTk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5951/Reviewer_GoTk"
                ],
                "content": {
                    "title": {
                        "value": "Clarifies my concerns"
                    },
                    "comment": {
                        "value": "The new human study done to check prompt alignment is quite helpful and allays my concern. I'll be keeping my score as is.\n\nI have some additional questions about JTT vs Mixup: There is quite a drastic difference between the one-to-one performance and the Many-to-Many performance, which is somewhat surprising. For example, in the hard setting. JTT is 7 percent better than the closest performing method, which is not even Mixup. However, Mixup is effect for the Many-to-Many setting. Is it because the first classifier in JTT can only capture one 'group'/'environment' in the many-to-many setting?\n\nAre there works that have studied Mixup and JTT theoretically or tried to show what it is that makes these approaches effective (or ineffective) in different settings? If this is the first time the one-to-one  vs many-to-many effectiveness of these approaches is being studied then that'll be interesting future work, but I wonder if there is enough known already about how these approaches confer robustness to spurious signals."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5951/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700499721640,
                "cdate": 1700499721640,
                "tmdate": 1700499721640,
                "mdate": 1700499721640,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dKANuh6JkG",
            "forum": "W0zgCR6FIE",
            "replyto": "W0zgCR6FIE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5951/Reviewer_9Fk3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5951/Reviewer_9Fk3"
            ],
            "content": {
                "summary": {
                    "value": "Previous benchmarks testing robustness to spurious correlations faced problems, such as over-saturation and a lack of many-to-many (M2M) social correlations. The authors introduce Spawrious-{O2O, M2M}-{Easy, Medium, Hard}, an image classification benchmark with 152k high-quality images. State-of-the-art group robustness methods struggle with Spawrious, especially on the hard splits, achieving less than 73% accuracy using an ImageNet pre-trained ResNet50. Model misclassifications expose dependencies on spurious backgrounds, underscoring the dataset's significant challenge."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The authors do a good job of elaborating six desiderata for a spurious correlations benchmark including multiple training environments, photo-realism and high fidelity backgrounds. This can act as general guidelines that future works in this area can build upon.\n- The authors formally present the O2O and M2M spurious correlation settings, which helps make their contribution clear. See questions for some clarifications on these.\n- Overall the paper is well written (Figure 3 is particularly nice) and addresses an outstanding concern of insufficient benchmarks for the spurious correlation/distribution shift community. Though, there are some other recent benchmarks (like PUG) that do the same (see Weaknesses), and addressing some of the points below (especially the hardness of M2M setting) would help distinguish this work from those."
                },
                "weaknesses": {
                    "value": "- Comparison with/discussion on some other relevant spurious correlation benchmarks that also use a synthetic/combinatorial construction pipeline like the PUG dataset is missing.\n- Some understanding of the hardness of the proposed benchmark would be relevant. Presumably there is some optimal re-weighting function for this dataset. How does the JTT assigned weights compare with this? \n- Explanation/discussion of why MixUp does better than other baselines in M2M setting would be helpful.\n- Performance of some other recent baselines like RWY (uses group info), BR-DRO/LfF (does not use group info) is missing.\n\nOverall, this paper makes an attempt towards a useful and much needed SC benchmark, but falls slightly short of building some understanding of the distinguishing characteristics of the proposed dataset -- it is unclear how M2M is different from O2O but with superclass/superattribute labels. It is possible that I missed some details. Therefore, I would be happy to consider raising my score after the authors have had a chance to respond to my questions."
                },
                "questions": {
                    "value": "- In the M2M case, there is still a one-to-one relationship between disjoint subgroups of classes and attributes. I did not fully understand why it is M2M, since it can still be thought of as O2O with respect to labels and attributes at a higher granularity? I imagined that the M2M case would involve overlapping subsets of classes and attributes.\n- Why is the correlation completely flipped in the M2M case only, and not O2O case?\n- In Figure 2c and 2d why is the correlation flipped and not randomized, i.e., zero correlation, which is typical of test distribution on existing datasets like waterbirds (unless you are looking at only the worst group as the test set)?\n- What weights were used for group DRO, since the test set has correlations that do not appear at all in the training set, so theoretically the weights are infinite in this case?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5951/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699074047013,
            "cdate": 1699074047013,
            "tmdate": 1699636634359,
            "mdate": 1699636634359,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7dZWgKr7uq",
                "forum": "W0zgCR6FIE",
                "replyto": "dKANuh6JkG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5951/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5951/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply by authors"
                    },
                    "comment": {
                        "value": "Thank you for the constructive feedback that improves the quality of our submission! We address your comments below.\n\n\n> Comparison with/discussion on some other relevant spurious correlation benchmarks that also use a synthetic/combinatorial construction pipeline like the PUG dataset is missing.\n\nWe appreciate you highlighting the PUG dataset to us and have added a reference to PUG in the revised submission.\nFirstly, the PUG dataset was released long after (>6 months) our benchmark has been publicly released and already adopted in several DG works. Unfortunately, the PUG paper missed mentioning our benchmark. \nSecondly, to answer your actual question, PUG does not introduce spurious correlations between class and background in their benchmark, and neither do they attempt to balance the datasets by class and group. In this regard, we feel that PUG addresses a different problem and Spawrious directly contributes to the OOD community.\nPlease let us know if there are other synthetic construction pipelines you have in mind and we will make sure to cite these.\n\n> Some understanding of the hardness of the proposed benchmark would be relevant. \n\nThanks for pointing this out! The hardness of the challenges differ due to the variation of the background features for a given location. For example, while the Jungle and Dirt location seems to have little variation in the features observed, with forestry and dirt paths, the Mountain background varies substantially. Further, there doesn\u2019t seem to be much overlap between Mountain and Snow background features with other locations, while Jungle and Dirt often overlap with each other in grassy dirt scenes. We will clarify this further in the revision. \n\n> Presumably there is some optimal re-weighting function for this dataset. How does the JTT assigned weights compare with this?\n\nThe optimal (reweighting) importance weights can be inferred from Table 2 in our initial submission. We will add the JTT assigned weights to next revision.\n\n> Explanation/discussion of why MixUp does better than other baselines in M2M setting would be helpful.\n\nThank you for pointing this out! MixUp reduces spurious correlations between background and class because a lot of the background information from two images being interpolated may be lost while edges and curves remain in greater detail, rendering the backgrounds to be less predictive of the class and the dog features more predictive. We will add this explanation to the next revision. \n\n> Performance of some other recent baselines like RWY (uses group info), BR-DRO/LfF (does not use group info) is missing.\n\nSpawrious is designed to be class-balanced in O2O and both class- and location-balanced in M2M. Hence, re-weighting algorithms like RWY are not applicable. In other words, RWY rebalances the dataset for each class by its size in each group, but Spawrious classes are already balanced in each group. We will clarify this in the next revision, thanks for asking this.\n\nWe acknowledge that there are missing baselines from our benchmark, while curating those we believe are representative of the literature at large, such as IRM for group info optimization and JTT for group agnostic optimization. We intended to capture a wide variety of baselines to demonstrate the utility of our dataset. We look forward to extending our benchmark results to other methods (such as BrDRO) within a reasonable time frame. \n\nUnfortunately, we could not find any repository for the LfF code. BrDRO is exciting to look into, however, the documentation provided is sparse, making it challenging to evaluate it quickly within the rebuttal time. We already have evaluated JTT, another method that does not use group info. \n\n\n> Overall, this paper makes an attempt towards a useful and much needed SC benchmark, but falls slightly short of building some understanding of the distinguishing characteristics of the proposed dataset -- it is unclear how M2M is different from O2O but with superclass/superattribute labels. It is possible that I missed some details. Therefore, I would be happy to consider raising my score after the authors have had a chance to respond to my questions.\n\nWe agree that the community lacks a SC benchmark, and thank you for your recognition. M2M is qualitatively different from the O2O case because the spurious feature is no longer fully predictive of the class. In the O2O case, setting the spurious correlation to 1 would render the background features equally as predictive of the class as the dog features. In the M2M case, setting the spurious correlation to 1 renders the background predictive of 2 classes but insufficient to distinguish between the two classes. In our dataset, we set the O2O spurious correlation to less than 1, so the dog features are marginally more predictive than the background features. We set the M2M spurious correlation to 1 because the dog features are already much more predictive than the background features."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5951/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700399061552,
                "cdate": 1700399061552,
                "tmdate": 1700411294145,
                "mdate": 1700411294145,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "krYY323nLI",
                "forum": "W0zgCR6FIE",
                "replyto": "7dZWgKr7uq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5951/Reviewer_9Fk3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5951/Reviewer_9Fk3"
                ],
                "content": {
                    "title": {
                        "value": "Response to Rebuttal"
                    },
                    "comment": {
                        "value": "I thank the authors for their detailed response. Thank you for acknowledging PUG including that in your discussion. \n\n- I understand that some SC baselines may not be easy to implement in the given time frame, but would encourage adding some discussion on why these baselines (LfF/BR-DRO/JTT/LISA etc.) may or may not fail in the M2M setting (in addition to some of the empirical observations already in the paper). This can motivate the need for algorithmic interventions specifically for M2M that current algorithms may lack, and can also expose some overfitting of these algorithms to O2O setting. \n\n- Regarding the final question about differences between M2M and O2O, it seems in the M2M case, the spurious correlation is less fatal on target since it is not fully predictive of a single class on source. Then, why does ERM perform worse in M2M vs O2O. Can the authors please explain? It is possible I missed something."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5951/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700675773164,
                "cdate": 1700675773164,
                "tmdate": 1700675773164,
                "mdate": 1700675773164,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qEPYrLxEta",
                "forum": "W0zgCR6FIE",
                "replyto": "awczQfWN9F",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5951/Reviewer_9Fk3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5951/Reviewer_9Fk3"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors"
                    },
                    "comment": {
                        "value": "Thank you for the clarification. It would be great if you can include the above two discussion points in the paper (at least in the appendix), since it gives weight to the exposition and helps the community understand why the different proposed algorithms may or may not fail in capturing M2M. \n\nOverall I am OK with the presented arguments, though they are sometimes anecdotal and vague (for example the description of the dog classification through the decision tree lens). At the same time, I understand that this was probably only meant as a motivation.  I will certainly consider raising my score after discussion with other reviewers."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5951/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700687709367,
                "cdate": 1700687709367,
                "tmdate": 1700687709367,
                "mdate": 1700687709367,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]