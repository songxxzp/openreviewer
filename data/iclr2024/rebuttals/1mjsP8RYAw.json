[
    {
        "title": "Unsupervised Fact Verification by Language Model Distillation"
    },
    {
        "review": {
            "id": "IUmLC4Exat",
            "forum": "1mjsP8RYAw",
            "replyto": "1mjsP8RYAw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6308/Reviewer_kcpN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6308/Reviewer_kcpN"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes an unsupervised framework for unsupervised fact verification. Specifically, three losses are designed to encourage the two models adopted in this framework to produce high-quality features for claim-fact matching. The authors conduct experiments on the FEVER dataet. Experimental results show that the proposed method can achieve good performance on the FEVER dataset."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Experimental results on the FEVER dataset show the advantage of the proposed method. The improvement seems very significant.\n\n2. The unsupervised manner of conducting fact verification is encouraged and useful."
                },
                "weaknesses": {
                    "value": "1. Though the experiments show the effectiveness of the method, I do not get how the framework solves the cold starting. For the scoring module, the claim embeddings from the LM are very different from those of Knowledge from the knowledge model. Then how does the framework pick the top-k evidence at the beginning? How does $L_{distill}$ work at the early iterations? This is an important prerequisite that should be clearly stated in the paper. \n\n2. Why does the paper only evaluate the effect of each loss on a smaller T5 model? Considering the best performance reported in the work is based on Transformer-XL, ablation studies based on it are desired.\n\n3. The annotation in the methodology section makes me really confused. For example but not limited to:\n\n    (1) In Section 3.1, what does the V and each $v_i$ mean? I cannot get it until I read through the whole methodology section. \n\n    (2) The use of subscript and superscript is messed up. I think embedding is presented as $X_F$ in Section 3.2, but it becomes $X^F$ in Section 3.3.\n    \n    (3) Some annotations are not really necessary, e.g., $N_i$ in Equation 3 are not used anywhere else in the paper.\n\n    I encourage the authors to make Section 3 more concise and clear by better formula presenting."
                },
                "questions": {
                    "value": "Please refer to the questions above in the weakness. Especially please make it clear how the model works at the beginning when the representations of the LM and the knowledge model are significantly different."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6308/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6308/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6308/Reviewer_kcpN"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6308/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698736455106,
            "cdate": 1698736455106,
            "tmdate": 1700762293678,
            "mdate": 1700762293678,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FgUdrjnqa1",
                "forum": "1mjsP8RYAw",
                "replyto": "IUmLC4Exat",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6308/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6308/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responding to Reviewer kcpN"
                    },
                    "comment": {
                        "value": "We thank you for your time and for providing constructive feedback. We have provided a high-level overview of major concerns in the general rebuttal and address more targeted questions in this comment.\n\n## Additional Experiments on Loss Function Ablation Study\n\nWe have updated Table 3 to include the loss function ablation experiments on the best performing backbone (GPT-2-XL) as well as a different, smaller backbone (T5-Small). This allows for a comprehensive understanding of the most critical components of the loss function across two different backbone settings. We consistently observed that the distillation loss is the most relevant component of the loss function, and the contrastive and scoring loss components allow for further improvement in the quality of the learned embeddings.\n\n## Clarification on Distillation Loss and Cold Start\n\nWe appreciate your attention to the issue of cold start and your request for clarification on how our framework addresses it. Our method indeed takes the cold start problem into consideration, where during the early training stages, the knowledge model provides completely dissimilar embeddings to triplets in comparison to the claim embedding given by the language model. As you correctly pointed out, we tackle this with the $\\mathcal{L}\\_{distill}$ component in the loss function. We would like to clarify how this works by providing a fine-grained explanation of $\\mathcal{L}\\_{distill}$. \n\nThe $\\mathcal{L}\\_{distill}$ is defined in Section 3.4 as follows:\n\n$\n\\begin{equation}\n{\\mathcal{L}\\_{distill}} = \\sum\\_{j\\in F^{+}} || \\frac{F^{KM}\\_j}{||F^{KM}\\_j||_2} - \\frac{F^{LM}\\_j}{||F^{LM}\\_j||_2} ||_p.\n\\end{equation}\n$\n\nIn the above equation, we have two main elements: $F^{KM}\\_j$ and $F^{LM}\\_j$, corresponding to the embeddings obtained from the knowledge model and the language model, respectively, for each fact, $j$, in the positive set $F^{+}$. To obtain $F^{LM}_j$, we generate embeddings for the verbalised fact triplets in the knowledge graph using a language model (e.g. GPT-2-XL). These embeddings capture the fact's textual representation and provide a way to compare it to the structured knowledge from the knowledge graph. This way of obtaining triplets embeddings from sequence-based models is standard in the state-of-the-art (see [1] for a recent example). These embeddings are computed only once during data preprocessing and stored to be used during training. \n\nTherefore, during training, the $\\mathcal{L}\\_{distill}$ encourages the knowledge model to embed the triplets in the feature space as close as possible to the corresponding verbalised triplet embedding from the language model. As shown in the ablation study in Table 3, this loss component is critical for the performance of our method, as without it the model can not solve the cold start. Moreover, this is consistent with our findings at the end of Section 3.4, where we state that weighting more $\\mathcal{L}\\_{distill}$ than $\\mathcal{L}\\_{contrastive}$ or $\\mathcal{L}\\_{intra}$ is the best setting for optimal pre-training results.\n\nTo make this more clear, we have extended our explanation in the Claim-Fact Distillation Loss paragraph in Section 3.4. \n\n## Notation Typos and Simplification of Section 3\n\nWe appreciate you pointing out these typos and have made another pass through the work to correct errors. In particular, we have made the notation in Section 3 consistent throughout the paper, fixed notation typos and made it more clear and concise when possible.\n\nThank you again for helping us improve this work.\n\nPlease let us know if you would like to see additional experiments or have further feedback.\n\n## References\n\n[1] Lu et al., \u201cKELM: Knowledge Enhanced Pre-Trained Language Representations with Message Passing on Hierarchical Relational Graphs\u201d. ICLR, 2022."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6308/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700246516841,
                "cdate": 1700246516841,
                "tmdate": 1700246516841,
                "mdate": 1700246516841,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "doSiX2dZiB",
            "forum": "1mjsP8RYAw",
            "replyto": "1mjsP8RYAw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6308/Reviewer_GMHM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6308/Reviewer_GMHM"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes an unsupervised method called SFAVEL for fact verification. The method aim is to distil knowledge from a language model into a knowledge model so that they produce similar vectors for fact in a formal form and in a natural language form. Besides to make the knowledge model distinguish facts, contrastive learning is employed. The paper shows that SFAVEL remarkably outperforms SoTA fact-verification models on the FEVER dataset (about 8% label accuracy higher than the best model in the literature)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The experimental result of SFEVEL on the FEVER dataset is remarkable."
                },
                "weaknesses": {
                    "value": "First of all, the paper is confusing in using the term \"unsupervised\". The proposed method SFAVEL is unsupervised because it is for learning a knowledge model. However, the fact-verification model reported in section 4 is supervised. The model uses SFAVEL for mapping fact / claim to vectors and then uses a classifier trained in a supervised learning manner. \n\nSecondly, it is unclear about what is the used \"linear probe\". Fig 4b shows that the linear probe takes top evidence as input. But then how can we verify the input claim if we use only evidence (and their scores)? E.g. how knowing \"Obama was born in Hawaii\" and \"Hawaii is in the US\"  can reject a claim without knowing what the claim is? \n\nThirdly, although the performance of the proposed model is remarkable, it is unclear why there's such a big gap between it and the existing models in the literature. What are cases that the proposed model can solve but the others can? Does the model find some crucial factors that the others miss? \n\nLast but not least, the proposed SFAVEL is for learning a knowledge model. But it is unclear whether that knowledge model is useful for other fact-verification cases (like on other FEVER dataset -- FEVER 2.0 for example). Also, whether that knowledge model is also useful for other downstream task requiring fact?"
                },
                "questions": {
                    "value": "In contrastive learning loss, e.g. eq 5, the denominator includes the numerator. However, it's not the case in eq 6. Why is that? What is the impact of excluding the numerator out of the denominator? \n\nIn section 4.2, what would happen if more data are used (rather than only 1, 5, 10%)?\n\nIn table 3, as transformer-XL is used in the end, why the ablation is for T5-small instead?\n\n\n-------\nThe score is updated after reading the authors' response."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6308/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6308/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6308/Reviewer_GMHM"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6308/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698789054905,
            "cdate": 1698789054905,
            "tmdate": 1700647389152,
            "mdate": 1700647389152,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6NSi6pwzqG",
                "forum": "1mjsP8RYAw",
                "replyto": "doSiX2dZiB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6308/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6308/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responding to Reviewer GMHM (Part 1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the helpful comments, and would like to clarify the concerns raised. We have provided a high-level overview of major concerns in the general rebuttal and address more targeted questions in this comment.\n\n# Response to Weaknesses:\n\n## Experiments on Additional Datasets\n\nAs this is a common question between some reviewers, we have provided experiments on additional datasets in the general rebuttal. In particular, we benchmark SFAVEL's unsupervised pre-trained model in the FB15k-237 dataset, which is another common benchmark for fact checking tasks. When compared against state-of-the-art methods, our approach is able to achieve +5.3%, +3.6% and +6.3% for Hits@1, Hits@3 and Hits@10 for fact ranking, respectively.\n\n## On Performance Gap Between Previous Methods and SFAVEL\n\nIn our work, we leverage self-supervised learning to exclusively learn and understand the underlying structure of the claim-fact alignments, leading to high-quality embeddings that can be used in further fact-related downstream tasks, as shown in our new experiments. This is different to the approach taken by previous works (e.g. [1, 2, 3], to mention some), which aim to learn models for both feature representation and prediction at the same time, in a supervised setting. This leads to models that focus on learning specific and potentially spurious patterns associated with the labels, without encouraging the model to learn rich, meaningful and general representations of the data. For this reason, SFAVEL is able to learn intrinsic properties of the data that attain better transferability.\n\n## Clarification on Terminology: Proposed Unsupervised Pre-training Method vs Supervised Linear Probe\n\nWe would like to emphasise that the main contribution of our work is the unsupervised (self-supervised) pre-training framework. The design of our method is motivated by the goal of learning high-quality claim-fact embeddings useful for downstream fact verification tasks. The embeddings obtained with SFAVEL can be used as input for training any downstream model on different tasks related to fact checking. This is a standard practice in self-supervised learning (see e.g. [4, 5]). \n\nIn our experiments we use a linear probe, which is just a linear projection layer, as an example of a downstream model using SFAVEL\u2019s claim embedding and facts pooled embedding as input for claim classification. This is required for the case of FEVER, as the task is to classify claims, but it is not for FB15k-237, where only ranking of claims given a fact is important. On the latter, we do not need a supervised stage, and only use the ranking given by SFAVEL directly for evaluation. The aim of this experiment is two-fold: first, we show the generalizability of our unsupervised pre-training method on fact verification tasks with different nature, and second we obtain state-of-the-art performance with a simple linear layer classifier relying only on pre-trained SFAVEL features, showing the quality of the learned embeddings. \n\nWe have now clarified this with the following changes: we added \u201cPretraining\u201d in the paper title, clarified the optionality of the downstream task in Figure 1, and added more details in the explanation for the Linear Probe description in Section 4.1."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6308/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700247358299,
                "cdate": 1700247358299,
                "tmdate": 1700247358299,
                "mdate": 1700247358299,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cqTCmzm6YX",
            "forum": "1mjsP8RYAw",
            "replyto": "1mjsP8RYAw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6308/Reviewer_6mXX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6308/Reviewer_6mXX"
            ],
            "content": {
                "summary": {
                    "value": "The paper motivates the necessity of fact-verification specifically in an unsupervised way. Given that the recent works have focused on NLIs, this work focuses on a pre-training objective that includes claim-fact distillation loss, intra-sample contrastive loss, and scoring loss. These losses are determined based on the positive and negative samples and their embeddings from the knowledge-base conditioned on the claim. The overall goal is to pre-train this model in the context of available knowledge base to verify facts. The results show that the model performs well on FEVER dataset. \n\nConcerns of this work: \n1. Size of the models: Given the aspect of large language models where the sizes are in billions, the evaluation is performed on smaller models. Is there some conclusion that can be made with these smaller models instead of just mentioning that the results are good?\n2. Datasets: The work has been evaluated only on one single dataset which begs the question of generalizability. Some works such as [1, 2] have evaluated on other datasets such as UKP and FEVER 2 etc. How does this work compare to those? This is particularly necessary because of the use of Wikidata5m knowledge-base that is used. If it is outside the context of Wikipedia, how can this approach work for other knowledge-bases?\n3. Comparisons to other approaches: The top-k fact retrieval seems to play an important role, given that the recall in number of facts is improved based on the ranking your approach has, is it a fair comparison to other approaches that work on probably the only retrieved fact? If K=1 then the dev numbers are not comparable to any of the approaches mentioned in the paper. \n4. Self-supervision: There is a strong assumption that there is availability of a knowledge base - Wikidata5m and since Fever is derived from it, the losses are carried between facts from Wikidata and Fever claims. Would be good to clarify why the authors think this is self-supervised? \n\n\n[1]: Incorporating External Knowledge for Evidence-based Fact Verification\n[2]: Retrieval-augmented generation for knowledge-intensive nlp tasks"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper describes a novel approach for fact-verification\n2. Results show significant gains in comparison to state of the art approaches"
                },
                "weaknesses": {
                    "value": "1. Generalizability of the approach given other knowledge bases\n2. Self-supervision is an ambitious claim\n3. Fever is the only dataset used"
                },
                "questions": {
                    "value": "In the Summary"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6308/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6308/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6308/Reviewer_6mXX"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6308/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699329466144,
            "cdate": 1699329466144,
            "tmdate": 1699636693318,
            "mdate": 1699636693318,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ikc17wxSct",
                "forum": "1mjsP8RYAw",
                "replyto": "cqTCmzm6YX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6308/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6308/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responding to Reviewer 6mXX"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful and detailed comments on our work. We have provided a high-level overview of major additions in the general rebuttal and hope to address more targeted concerns in this comment.\n\n## Experiments on Additional Datasets\n\nAs this is a common question between some reviewers, we have provided experiments on additional datasets in the general rebuttal. In particular, we benchmark SFAVEL's unsupervised pre-trained model in the FB15k-237 dataset, which is another common benchmark for fact checking tasks. When compared against state-of-the-art methods, our approach is able to achieve +5.3%, +3.6% and +6.3% for Hits@1, Hits@3 and Hits@10 for fact ranking, respectively.\n\n## Experiments on Larger Language Models\n\nAs this is a common question between some reviewers, we have provided experiments on larger language models in the general rebuttal. In summary, we have added experiments with GPT-2-XL (1.5B parameters) as the distilled language model and updated the paper accordingly.\n\n## Clarification on Comparisons to Other Approaches\n\nThe use of top-k retrieval in our approach is motivated by the need to consider multiple pieces of evidence when verifying a claim in real-world applications, and we acknowledge this is a critical factor in our (and all recent) fact verification approach. Given the complexity of some of the claims in the benchmarked datasets (FEVER, FB15k-237), it is not possible to only use single-fact reasoning to predict whether a claim is true or false. For this reason, all state-of-the-art approaches we compare against have a (multiple) evidence retrieval stage (see e.g. [1, 2, 3] for some examples). Therefore, we would like to kindly emphasise that with respect to the top-k retrieval mechanism, the comparison of our method against others is fair and consistent with the literature.\n\n## Clarification on Self-supervision\n\nWe would like to clarify that the main contribution of our work is the unsupervised (self-supervised) pre-training framework. The design of our method is motivated by the goal of learning high-quality claim-fact embeddings useful for downstream fact verification tasks. The embeddings obtained with SFAVEL can be used as input for training any downstream model on different tasks related to fact checking. This is a standard practice in self-supervised learning (see e.g. [4, 5]). \n\nIn our experiments we use a linear probe, which is just a linear projection layer, as an example of a downstream model using SFAVEL\u2019s claim embedding and facts pooled embeddings as the input for claim classification. This is required for the case of FEVER, as the task is to classify claims, but it is not for FB15k-237, where only ranking of claims given a fact is important. On the latter, we do not need a supervised stage, and only use the ranking given by SFAVEL directly for evaluation. The aim of this experiment is two-fold: first, we show the generalizability of our unsupervised pre-training method on fact verification tasks with different nature, and second we obtain state-of-the-art performance with a simple linear layer classifier relying on pre-trained SFAVEL features, showing the quality of the learned embeddings. We have now clarified this with the following changes: we added \u201cPretraining\u201d in the paper title, clarified the optionality of the downstream task in Figure 1, and detailed the purpose for the Linear Probe in the description in Section 4.1.\n\nWe appreciate your feedback and will consider these points in our discussions and future work.\n\nPlease let us know if you would like to see additional experiments, and thank you again for helping us improve this work.\n\n## References\n\n[1] Krishna et al., \"ProoFVer: Natural Logic Theorem Proving for Fact Verification\". TACL, 2021.\n\n[2] Zhong et al., \u201cReasoning Over Semantic-Level Graph for Fact Checking\u201d. ACL, 2020.\n\n[3] Chen et al., \u201cGERE: Generative Evidence Retrieval for Fact Verification\u201d. SIGIR, 2022.\n\n[4] Bouniot et al., \u201cProposal-Contrastive Pretraining for Object Detection from Fewer Data\u201d. ICLR, 2023.\n\n[5] Hamilton et al., \u201cUnsupervised Semantic Segmentation by Distilling Feature Correspondences\u201d. ICLR, 2022."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6308/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700245567815,
                "cdate": 1700245567815,
                "tmdate": 1700245567815,
                "mdate": 1700245567815,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "liaKzzBmaT",
            "forum": "1mjsP8RYAw",
            "replyto": "1mjsP8RYAw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6308/Reviewer_1Hnq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6308/Reviewer_1Hnq"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a new (contrastive) loss to train models for unsupervised fact verification. This allows to check claims without having to collect annotations, instead relying on unsupervised claim-fact alignment."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The paper is very well written and well motivated.\n* The results presented in the paper are impressive, outperforming FEVER SOTA even for supervised approaches.\n* The authors compare the approach on 7 different models, including a variety of small to medium size models.\n* The paper contains good ablation experiments, in particular analysing the different components of the loss on a small model."
                },
                "weaknesses": {
                    "value": "* No large models were included, the biggest model tested has 250M parameters. There is no strict definition of LLM, but the authors may overpromise in their title/intro when no model with more than 1B parameters is included.\n* The increase over the SOTA may be exaggerated, given that most of the systems the paper compares to are several years old, and do not include the latest generation of models. (This is not strictly a weakness, but context worth mentioning.)"
                },
                "questions": {
                    "value": "* Have you considered including larger language models (given that the title mentions \"Large Language Models\")?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6308/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6308/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6308/Reviewer_1Hnq"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6308/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699462846090,
            "cdate": 1699462846090,
            "tmdate": 1700751943296,
            "mdate": 1700751943296,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hXymdhb2Dx",
                "forum": "1mjsP8RYAw",
                "replyto": "liaKzzBmaT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6308/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6308/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responding to Reviewer 1Hnq"
                    },
                    "comment": {
                        "value": "Firstly we would like to thank the reviewer for their thoughtful comments and support of our work. We have provided a high-level overview of major additions in the general rebuttal and would like to address more targeted questions in this comment.\n\nRegarding Large Language Models (LLMs), we would like to point out that we do not state our method is targeting LLMs in particular, but more generally any Language Model (LM). However, we agree that assessing the performance of our unsupervised pre-training methodology by distilling from even larger models is an interesting experiment given the current landscape in terms of language model sizes. \n\nTherefore, we have added an additional experiment in Table 2, where we utilise GPT-2-XL as the teacher/distilled language model. Despite not existing a strict definition of LLM, any language model with at least 1B parameters is usually deemed as \u201clarge\u201d, and GPT-2-XL contains 1.5B parameters, which effectively makes it a LLM. To run the experiment, we utilise the same hyperparameter settings as the initial experiments.\n\nAs a result of this new experiment, we see a 90.32% label accuracy in the FEVER dev dataset. From this experiment we can conclude that our distillation method is still beneficial on larger models, and still provides performance improvements on the downstream task. \n\nFinally, we added additional experiments on the FB15k-237 dataset to showcase the applicability of our method in a different setting.\n\nThank you again for the helpful feedback and support of our work!"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6308/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700244670769,
                "cdate": 1700244670769,
                "tmdate": 1700244670769,
                "mdate": 1700244670769,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "70PslTkAtE",
                "forum": "1mjsP8RYAw",
                "replyto": "hXymdhb2Dx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6308/Reviewer_1Hnq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6308/Reviewer_1Hnq"
                ],
                "content": {
                    "title": {
                        "value": "Reply to authors"
                    },
                    "comment": {
                        "value": "Thank you for addressing the questions, and I appreciate the new experiments.\n\nI enjoyed reading the paper, and still think it's good -- so my recommendation stays the same at accept."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6308/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733334495,
                "cdate": 1700733334495,
                "tmdate": 1700733334495,
                "mdate": 1700733334495,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PKkxHxsx5P",
            "forum": "1mjsP8RYAw",
            "replyto": "1mjsP8RYAw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6308/Reviewer_bggr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6308/Reviewer_bggr"
            ],
            "content": {
                "summary": {
                    "value": "This paper focused on unsupervised fact verification --- verifying a claim based on a trustworthy knowledge base without a requirement of direct supervision. The author proposed to train a scoring model on top of embeddings produced by a pre-trained language model to determine whether a given claim can be aligned to a fact from knowledgebase. The scoring model is trained by leveraging positive and negative examples constructed based on triples from a knowledge graph. The author conducted experimental evaluations on FEVER, and showed that their method can yield a significant improvement over SOTA."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The author's idea of leveraging a knowledge graph to produce positive and negative examples of unlabeled claims to train a scoring model is creative. \n2. The paper is very well-structured, and easy to follow.\n3. The experiments presented promising results on FEVER (~8% improvement on accuracy), and the method can work on a broad set of language models."
                },
                "weaknesses": {
                    "value": "1. The technique proposed in the paper does not seem to be generalizable. Specifically, the positive and negative examples constructed through triples from knowledge graph are too simple, which makes this method difficult to generalize to more complicated claims. Specifically, triples can only represent who did what, while in reality, a claim can be who did what at where on when for why.  Any wrong information about these factors can make a claim false. While it is not clear to me how the current method can learn a model that can be effectively aware of some more fine-grained factual differences.\n2. The experimental setup is limited. The evaluations are only based on FEVER, which is not convincing. FEVER is created through Wikipedia, and Wikipedia information is closer to triples, which is bias to author's method and training process. At least, an experiment to show the effectiveness of this method on other fact verification dataset would be very helpful.\n3. Ranking may not be the best problem formulation for fact verification. For claim verification, it is important to help people decide whether they should believe the claim or not. Now the author formulates this problem as a ranking problem, which is not very useful from a fact verification perspective. it is not clear what does it mean to the user that a claim can find a piece of evidence with 0.9 score."
                },
                "questions": {
                    "value": "1. How this method is different from a ranking/retrieval problem? Is fact verification equivalent to ranking/retrieval?\n2. How would this method work on other datasets that are not created based on Wikipedia?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6308/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6308/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6308/Reviewer_bggr"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6308/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699591241203,
            "cdate": 1699591241203,
            "tmdate": 1699636693061,
            "mdate": 1699636693061,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qSBlaMLtNC",
                "forum": "1mjsP8RYAw",
                "replyto": "PKkxHxsx5P",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6308/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6308/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responding to Reviewer bggr"
                    },
                    "comment": {
                        "value": "We appreciate your thoughtful and detailed comments on our work. We have provided a high-level overview of major concerns in the general rebuttal and address more targeted questions in this comment.\n\n## Experiments on Additional Datasets\n\nAs this is a common question between some reviewers, we have provided experiments on additional datasets in the general rebuttal. In particular, we benchmark SFAVEL's unsupervised pre-trained model in the FB15k-237 dataset, which is another common benchmark for fact checking tasks. When compared against state-of-the-art methods, our approach is able to achieve +5.3%, +3.6% and +6.3% for Hits@1, Hits@3 and Hits@10 for fact ranking, respectively.\n\n## Clarification on Generation of Negative Instances\n\nWe understand your concern regarding the generalizability of our proposed negative examples construction technique and its simplicity. As you correctly pointed out, the individual triples used in our experiments capture relatively simple relationships, typically involving subject-predicate-object structures. While it is true that these triples might not encompass all the complexity of natural language claims, the effectiveness of such negative triplet generation procedures has been demonstrated in previous works (see e.g. [1, 2, 3], to mention a few). In this regard, one of our main contributions is to demonstrate the potential of leveraging structured knowledge from knowledge graphs to improve unsupervised pre-training for fact verification. Despite the fact that real-world claims can involve various aspects such as \"who did what, where, when, and why\", we argue that breaking down these statements into unitary components can be an effective strategy allowing to capture fine-grained factual differences. As an example, for the claim \u201cBob Ross created ABC drama The Joy of Painting\u201d (see Section A.1 of the Appendix), our model is capable of obtaining a set of facts that when used together allows the model to perform multi-level reasoning.\n\n## Relation Between Ranking/Retrieval and Fact Verification\n\nYour comment raises a valid concern about the choice of problem formulation, specifically the use of \u201cranking\u201d in the context of fact verification, which may not appear intuitive in this context at a first glance. However, ranking is a crucial intermediate step in state-of-the-art fact verification machine learning approaches (see [1] for a comprehensive overview of relevance of ranking in fact verification). While our approach does provide a score for each fact, such as 0.9, it is intended as a measure of the relevance and credibility of the evidence. Regarding the difference between retrieval and ranking, the former aims to find relevant data points (i.e. facts), while ranking goes further by ordering them based on relevance or quality. In the context of fact verification, ranking helps users prioritise evidence, which is crucial when dealing with a large volume of potentially relevant information, as is the case for the benchmarked datasets. Your feedback has been instrumental in highlighting this aspect, and we will consider it for future improvements to our work.\n\nPlease let us know if you would like to see additional experiments, and once again, thank you for the helpful and constructive feedback to improve our work!\n\n## References\n\n[1] Guo et al., \u201cA Survey on Automated Fact-Checking\u201d. TACL, 2022.\n\n[2] Jiang et al., \u201cDon\u2019t Mess with Mister-in-Between: Improved Negative Search for Knowledge Graph Completion\u201d. EMNLP, 2023.\n\n[3] Komatani et al., \u201cKnowledge Graph Completion-based Question Selection for Acquiring Domain Knowledge through Dialogues\u201d. IUI, 2021.\n\n[4] Zhang et al., \u201cSimple and automated negative sampling for knowledge graph embedding\u201d. VLDB, 2021."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6308/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700244458609,
                "cdate": 1700244458609,
                "tmdate": 1700244458609,
                "mdate": 1700244458609,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]