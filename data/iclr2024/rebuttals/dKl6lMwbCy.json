[
    {
        "title": "Peering Through Preferences: Unraveling Feedback Acquisition for Aligning Large Language Models"
    },
    {
        "review": {
            "id": "VeYH9t42VA",
            "forum": "dKl6lMwbCy",
            "replyto": "dKl6lMwbCy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3202/Reviewer_KCah"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3202/Reviewer_KCah"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the two types of feedback, collecting ratings versus rankings, from both human annotators and AI as an annotator. The authors analyzed both types of collected feedback, observing general inconsistency, and also used them to train reward models, finding that the choice of feedback protocol affects the effectiveness of the reward model (where the trends hold across both human and AI feedback)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This paper studies an important problem of understanding the effect of different kinds of human feedback and how they can be used in the training pipeline. \n- The findings, which apply to both human and AI annotations could be useful for informing how people design feedback protocols in the future."
                },
                "weaknesses": {
                    "value": "In general, the specific details surrounding experimental design were not as well justified, making it difficult to assess the applicability of the findings more broadly. For example, \n- Why were Dolly, User-orient, and SuperNI selected as the tasks of interest? \n- What was the prompt provided for AI annotation? Given the subjectivity of the task, what instructions were given to the crowdworkers when asked to provide ratings / rankings? This is important to justify because the text mentions that crowd workers perceived a response to be \u201cdull\u201d, though it\u2019s not clear what kind of metric crowd workers are / should be using.\n- Additionally, the generalizability of the results may be limited by the choice of model in the various experiments: (1) only Alpaca-7b was tested in terms of generating candidate responses, (2) only GPT-3.5-Turbo was evaluated as an AI annotator, (3) only LORA Alpaca-7b was selected as the reward model, and (4) win-rate was computed only against DaVinci-003. It would be helpful for the authors to clarify why those models were selected in each part of the paper."
                },
                "questions": {
                    "value": "- How would results in Figure 3 differ across tasks?\n- In Section 2.2 and 2.3, it would be helpful for authors to add references for each to help the reader get a sense of where the protocols and models have been used in prior work.\n- Some typos, e.g., missing link in the first paragraph of Section 3 and \u201cthe humans 6k instances of annotations\u201d."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3202/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3202/Reviewer_KCah",
                        "ICLR.cc/2024/Conference/Submission3202/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3202/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698517738117,
            "cdate": 1698517738117,
            "tmdate": 1700576819608,
            "mdate": 1700576819608,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "I5qi4tarSc",
                "forum": "dKl6lMwbCy",
                "replyto": "VeYH9t42VA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3202/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3202/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer KCah"
                    },
                    "comment": {
                        "value": "We are motivated to find that the reviewer finds our work (a) important for understanding the effect of feedback protocols, (b) widely applicable to both human and AI, and (c) useful for informing people how people design feedback protocols.\n\n**Q:** Why were Dolly, User-orient, and SuperNI selected as the tasks of interest?\n\n**A:** We thank the reviewer for their insightful question on our dataset setup. As mentioned in Section 3, the instructions are designed to present a collection of queries that could potentially be posed to a text-based AI assistant in real-world scenarios. We clarify that Dolly, User-Orient, and SuperNI are all human-written instruction datasets. Specifically, Dolly and User-orient are open-ended instructions and responses while we select a subset of SuperNI datasets that cover a wide range of public NLP datasets with long responses (Table 11). We also point that there is no consensus within the community about the datasets to be used for feedback acquisition and subsequent reward modeling. Hence, we focused on high quality datasets that are used for instruction tuning. \n\n**Q:** Additionally, the generalizability of the results may be limited by the choice of model in the various experiments.\n\n**A:** We thank the reviewer for their query regarding our experimental setup. \n\n- While it is indeed possible to change various components of the setup, the critique holds for any LLM paper as LLMs are rapidly evolving and increasing in number. Our choices reflect (a) strong models and practices which have had numerous pick ups by the community at large (e.g., GPT-3.5-Turbo, Alpaca-7B), (b) finite compute and labeling budget available for conducting the experiments in an academic setting.\n- In our experiments, we generated $30$K candidate responses and got them annotated by humans and AI which costs ~$2000 USD in total. Although it would be interesting to generate candidate responses from other models, the subsequent annotation is time-consuming and out of our academic budget. \n- We also clarify that the win-rate against davinci-003 has been a de facto evaluation metric for many influential works on LLM alignment [1,2,3,4] We will mention this argument in the revised paper.\n\nFollowing up on the reviewer\u2019s suggestions, we do run additional experiments for (a) using GPT-3.5-Turbo for consistency analysis and (b) using LORA Alpaca-7B as a reward model.\n\n(a) **GPT-3.5-Turbo for Consistency Analysis**\n\nBy default, we use GPT-3.5-Turbo with temperature = 0. Here, we perform an inconsistency analysis on 500 comparisons between the ratings and rankings feedback for (i) GPT-3.5-Turbo at temperature = 0.5, (ii) GPT-3.5-Turbo-0613, (iii) GPT-4. Here are the results:\n\n| Model                                          | Inconsistency |\n|------------------------------------------------|---------------|\n| ChatGPT-3.5-Turbo-Temperature=0 (Ours) | 58%           |\n| ChatGPT-3.5-Turbo-Temperature=0.5              | 56%           |\n| ChatGPT-3.5-Turbo-0613                         | 54%           |\n| GPT-4                                          | 50%           |\n\nThe standard error for the above numbers is 4% with 95% confidence. We find that all the model choices suffer from the inconsistency problem. This indicates that our results are not restricted by the choice of the AI annotator, and are indeed observable in different models too. \n\n(b) **Choice of the reward model**\n\n- We clarify that there is no clear consensus on the \u201cright\u201d choice of the reward model. We believe that any good language understanding model could act as a reward model. Prior works [1,6] use decoder-only architecture for the reward model. These models are normally at a scale of 10-100 billion parameters and harder to train in the compute efficient manner without engineering tricks. Hence, we chose Alpaca-7B in our setup, and LoRA is just a method to finetune it in a parameter efficient manner. \n- On the other hand, [5,7] have used BERT transformer encoder models, which usually have less parameters, say 500M-1B parameters. Finetuning BERT models achieve very good performance on many NLP tasks. Hence, we repeat train reward models on the Roberta-large architecture.\n- Specifically, we finetune a robert-large model on the ratings data using the regression loss. In addition, we train another roberta-large model on the rankings data using the negative log likelihood loss. We use these two finetuned models to perform Best-of-64 policy. Subsequently, we use GPT-3.5-Turbo to provide rating and ranking feedback for the rating/ranking Best-of-64 policy.\n\nWe present the results below:"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3202/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700369581666,
                "cdate": 1700369581666,
                "tmdate": 1700369581666,
                "mdate": 1700369581666,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EFCorzIQkM",
                "forum": "dKl6lMwbCy",
                "replyto": "VeYH9t42VA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3202/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3202/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal Reminder"
                    },
                    "comment": {
                        "value": "Thanks again for your insightful feedback on our work! We've carefully worked to address your comments/questions and would like to note that the end of the discussion phase is coming soon. Are there any further questions or concerns we should discuss?"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3202/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700504434360,
                "cdate": 1700504434360,
                "tmdate": 1700504434360,
                "mdate": 1700504434360,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QJLn9zxvpr",
                "forum": "dKl6lMwbCy",
                "replyto": "EFCorzIQkM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3202/Reviewer_KCah"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3202/Reviewer_KCah"
                ],
                "content": {
                    "title": {
                        "value": "Unaddressed point"
                    },
                    "comment": {
                        "value": "Thanks to the authors for providing additional experiments with more models. I believe there was one of the bullet points that remains unaddressed from my original review: \"What was the prompt provided for AI annotation? Given the subjectivity of the task, what instructions were given to the crowdworkers when asked to provide ratings / rankings? This is important to justify because the text mentions that crowd workers perceived a response to be \u201cdull\u201d, though it\u2019s not clear what kind of metric crowd workers are / should be using.\""
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3202/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700512057519,
                "cdate": 1700512057519,
                "tmdate": 1700512057519,
                "mdate": 1700512057519,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ADma4QaSSH",
                "forum": "dKl6lMwbCy",
                "replyto": "VeYH9t42VA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3202/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3202/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Update on the revised paper"
                    },
                    "comment": {
                        "value": "Hi,\n\nWe wanted to highlight that we have uploaded the revised paper with the new results and setups. Addressing your suggestions, we have added Section E.2 (variation in AI annotator), Section N (RoBERTA-Large), Section L (Task-specific results), Section I (AI prompts), Section J (human UI screenshot), Section P (qualitative examples).\n\nPlease let us know if there are any further questions."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3202/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700532998672,
                "cdate": 1700532998672,
                "tmdate": 1700533122096,
                "mdate": 1700533122096,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vEwBFz12RS",
                "forum": "dKl6lMwbCy",
                "replyto": "ADma4QaSSH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3202/Reviewer_KCah"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3202/Reviewer_KCah"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for following up with additional details! I appreciate the authors' continued hard work at improving their submission. I have raised my score accordingly."
                    }
                },
                "number": 26,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3202/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700576869538,
                "cdate": 1700576869538,
                "tmdate": 1700576869538,
                "mdate": 1700576869538,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3U2UU2ZBlR",
            "forum": "dKl6lMwbCy",
            "replyto": "dKl6lMwbCy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3202/Reviewer_ofRF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3202/Reviewer_ofRF"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates two feedback protocols (rating and ranking) for the alignment and evaluation of LLMs. It collects AI feedback based on these two settings and uses them to train reward models. The reward models and the LLMs with the best-of-n policies are then evaluated on the annotations of humans and ChatGPT. It conducts a detailed analysis of the characteristics of the collected annotations. It reveals evaluation inconsistencies in which feedback protocols used in alignment algorithms have an advantage over other feedback protocols during evaluation."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* This paper draws attention to feedback inconsistency where the ratings and rankings disagree with each other for the 60% comparison in both humans and AI.\n* This paper investigates the influence of different feedback protocols on reward functions. It sheds light on how we should collect feedback.\n* This paper collects human feedback and AI feedback and conducts a detailed analysis from different aspects."
                },
                "weaknesses": {
                    "value": "* This paper does not explore the influence of feedback protocols on common alignment methods (such as RLHF[1], RRHF[2], RLAIF[3], etc.). The alignment in this paper just applies the reward models to select the best out of n generation, which is only affected by the performance of reward models.\n* The evaluation inconsistency seems straightforward: the performance of reward models will be affected by the format of input data. It is better to convert the rating feedback to the ranking format first and then use it to train an NLL reward model (just like the ranking feedback) and then compare the performance.\n\t\t\n\t\t\n[1] Training language models to follow instructions with human feedback\n[2] Rank Responses to Align Language Models with Human Feedback without Tears\n[3] Constitutional AI: Harmlessness from AI Feedback"
                },
                "questions": {
                    "value": "* Can you explore how the feedback protocol affects the reinforcement learning finetuning for model alignment, such as RLHF?\n* Will the collected feedback be released?\n* Is there any calibration on the rating scores? For example, detail the meaning of each score (1-7) in the prompt for AI feedback and instruction for human annotation to make sure that the annotators can fully understand the principle of evaluation."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3202/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3202/Reviewer_ofRF",
                        "ICLR.cc/2024/Conference/Submission3202/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3202/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698771629841,
            "cdate": 1698771629841,
            "tmdate": 1700540645716,
            "mdate": 1700540645716,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LLtTZU9P4G",
                "forum": "dKl6lMwbCy",
                "replyto": "3U2UU2ZBlR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3202/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3202/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ofRF"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their helpful feedback.\n\n**Q:** This paper does not explore the influence of feedback protocols on common alignment methods (such as RLHF[1], RRHF[2], RLAIF[3], etc.). The alignment in this paper just applies the reward models to select the best out of n generation, which is only affected by the performance of reward models.\n\n**A:** We thank the reviewer for their thoughtful question. Firstly, we highlight that the best-of-n (rejection sampling) is also a common method in RLHF literature for aligning LLMs. As mentioned in Section 2.3, we choose rejection sampling as due to simplicity, simplicity, and robust performance. For instance, [1] finds that the win-rate of the Best-of-n policy outperforms the base model by 10% absolute points on AlpacaEval.  We do not perform PPO since it requires atleast 8 80GB GPUs according to the AlpacaFarm implementation, which is outside our academic budget. We attempted to train PPO at our compute scale by reducing the effective batch sizes and using LoRA models but it did not run stably. Following the reviewer\u2019s feedback, we performed an additional experiment with rejection sampling finetuning (RSFT) used in three highly cited LLM alignment papers, including LLaMA 2 [2,3,4]. \nSpecifically, our setup is as follows:\n\n1. We prompt Alpaca-7B with 5K instructions from Alpaca-52K data.\n2. We generate 64 responses for every instruction.\n3. We use our rating and ranking reward model to select the best response from the 64 responses.\n4. We finetune two Alpaca-7B models with the 5K instruction-responses data. \n(a) One where the responses are chosen from the rating reward model.\n(b) Second where the responses are chosen from the ranking reward model.\n5. Post finetuning, we sample a single response from the finetuned Alpaca-7B with 553 evaluation instructions\n6. We calculate the win-rate against DaVinci003 using the rating and ranking protocol using ChatGPT.\n\nHere are the results for this experiment where baseline is the base Alpaca-7B model:\n| Win-rate against Davinci-003 | Baseline | RSFT (Rating) | RSFT (Ranking) |\n|------------------------------|----------|---------------|----------------|\n| Ranking Evaluation           | 36.9%     | 42.0%          | **43.3%**           |\n| Rating Evaluation            | 41.9%    | **44.0%**         | 43.0%           |\n\nWe find that the evaluation inconsistency persists under this alignment algorithm too. It indicates that the choice of feedback protocol for the evaluation favors the same feedback protocol used for training the reward models and subsequently finetuning the base LLM. We will add this result in the updated paper. We hope that this experiment answers the reviewer\u2019s comment on the generalizability of the evaluation protocol to other algorithms.\n\n[1] AlpacaFarm: https://arxiv.org/abs/2305.14387 \\\n[2] RRHF: https://arxiv.org/abs/2304.05302 \\\n[3] LLaMA2: https://arxiv.org/pdf/2307.09288.pdf \\\n[4] Constitutional AI: https://arxiv.org/pdf/2212.08073.pdf \\\n[5] AlpacaFarm Code: https://github.com/tatsu-lab/alpaca_farm#running-reference-methods"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3202/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700198142982,
                "cdate": 1700198142982,
                "tmdate": 1700208908213,
                "mdate": 1700208908213,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Yue4pbqCkh",
                "forum": "dKl6lMwbCy",
                "replyto": "3U2UU2ZBlR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3202/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3202/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ofRF"
                    },
                    "comment": {
                        "value": "**Q:** The evaluation inconsistency seems straightforward: the performance of reward models will be affected by the format of input data. It is better to convert the rating feedback to the ranking format first and then use it to train an NLL reward model (just like the ranking feedback) and then compare the performance.\n\n**A:**\n\n - In our work, we established that the feedback data obtained from humans and AI suffers from inconsistency problems. We believe that our work is unique as it shows that this has a direct impact on the alignment and evaluation of LLMs through empirical evidence.\n- The choice of reward models was naturally made to reflect the nature of the feedback protocol. Specifically, the *rating* protocol provides us with a scalar score hence the regression model makes sense, and the *ranking* protocol does not provide us with scalar scores hence we use a negative log likelihood (NLL) objective function. \n- We observe that 57.4% of the pairwise comparisons are considered \u201cequal\u201d when we convert the rating data into the ranking format (Row 1 of Table 2(a)). In practice, this means that we need to filter almost 60% of the rating data if we want to train a ranking format reward model. This is not the most optimal way to use the rating data when we know that a better regression method exists that can utilize the complete data. Hence, we train a regression reward model in our original setup. \n\nHowever, we still perform the experiment as requested by the reviewer. \n1. We train two reward models, of roberta-large architecture, on (a) ranking data with NLL loss and (b) rating data converted to ranking with NLL loss.\n2. We perform Best-of-n sampling with n = 64\n3. We evaluate the model\u2019s win-rate against davinci-003 with a rating and ranking evaluation scheme. \n\nWe present the results below:\n\n| Win-rate against davinci-003 | Best-of-64 (Ranking) | Best-of-64 (Rating Data Ranking Format) |\n|------------------------------|----------------------|-----------------------------------------|\n| Relative Eval                | **47.3%**                | 45%                                     |\n| Absolute Eval                | 42.0%                | **46%**                                     |\n\nWe highlight that the evaluation inconsistency is still evident when the data format used for training the reward models is kept identical. This indicates that our finding is not confounded by the reward model\u2019s data format but the nature of the feedback protocol data itself i.e., rating and ranking. We will update this result in the paper.\n\n**Q:**  Will the collected feedback be released?\n\n**A:** Yes, we will release the data upon acceptance. To support the claim, we have uploaded the AI feedback data in the supplementary material. \n\n**Q:** Is there any calibration on the rating scores?\n\n**A:** \n\n- We clarify that we collect the rating data on a scale of 1-7 from AI and humans where {1=Very poor, 2=Poor, 3=Below Average,4=Average,5=Above Average,6=Good,7=Excellent}. \n- We provide an annotation guideline in the prompt to the AI explaining the various quality dimensions along which the rating score decision must be based along with two in-context examples \u2013 one for low score scenario and second for the high score scenario. In addition, we perform rating data collection for each instance in a new session so that the LLM maintains its same calibration/understanding for the rating scores throughout the annotation process. We will add the prompt to the revised paper.\n- We provide the same annotation guidelines as AI to humans. Differently from AI, the humans can (re)calibrate themselves as the annotation progresses. As we ask four workers to annotate the same instance, the small differences caused by the worker\u2019s calibration will be averaged out. Specifically, the feedback data will not be biased towards a specific annotator's calibration, instead reflect the average belief about the response quality under the rating/ranking feedback protocol. We will add the UI screenshot of the human annotator interface in the revised paper."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3202/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700198317617,
                "cdate": 1700198317617,
                "tmdate": 1700208928238,
                "mdate": 1700208928238,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RIAx1YsRRI",
                "forum": "dKl6lMwbCy",
                "replyto": "3U2UU2ZBlR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3202/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3202/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal Reminder"
                    },
                    "comment": {
                        "value": "Thanks again for your insightful feedback on our work! We've carefully worked to address your comments/questions and would like to note that the end of the discussion phase is coming soon. Are there any further questions or concerns we should discuss?"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3202/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700504417275,
                "cdate": 1700504417275,
                "tmdate": 1700504417275,
                "mdate": 1700504417275,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dLeTeXMfaq",
                "forum": "dKl6lMwbCy",
                "replyto": "A8mhvtYvvg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3202/Reviewer_ofRF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3202/Reviewer_ofRF"
                ],
                "content": {
                    "title": {
                        "value": "Thank authors for their detailed response."
                    },
                    "comment": {
                        "value": "Their rebuttal has solved most of my concerns, so I would like to raise my score to 6.  I have some extra questions about the additional experiments:\n* For *\"57.4% of the pairwise comparisons are considered \u201cequal\u201d when we convert the rating data into the ranking format \"*, how do you convert the data here? I think that with the ratings there can be more ranking examples. For example, *n* ratings can be converted into *n(n-1)/2* pairwise ranking.\n* Do you think that the calibration changes of rating during the annotation progress will also happen in the ranking protocol? Or will the ranking be more stable than the rating (because there are only two options instead of 7 in the rating)? Will this difference affect the feedback quality?"
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3202/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700540622764,
                "cdate": 1700540622764,
                "tmdate": 1700540622764,
                "mdate": 1700540622764,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BF6Qj5P66v",
                "forum": "dKl6lMwbCy",
                "replyto": "UHAvNXRBcq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3202/Reviewer_ofRF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3202/Reviewer_ofRF"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the further discussion"
                    },
                    "comment": {
                        "value": "I have no more concern now."
                    }
                },
                "number": 27,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3202/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700687355804,
                "cdate": 1700687355804,
                "tmdate": 1700687355804,
                "mdate": 1700687355804,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "OYRyVQEE9f",
            "forum": "dKl6lMwbCy",
            "replyto": "dKl6lMwbCy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3202/Reviewer_gmKJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3202/Reviewer_gmKJ"
            ],
            "content": {
                "summary": {
                    "value": "This work provides an intriguing analysis of the issue of the feedback inconsistency problem, where human or model-generated evaluations can be inconsistent across different evaluation protocols (e.g., output A is better than B in pairwise evaluation while if the rating of A is lower than the rating of B). They collect 6k human feedback data under the ratings and rankings protocols, as well as model-generated feedback. They found that inconsistencies are prevalent both in human and AI evaluations. They further conduct quantitative and qualitative analyses to understand the potential factors of these inconsistencies. While such preference or rating data is essential for recent RLHF approaches as well as evaluation for open-ended generations, it is still unclear whether such data is reliable or what kind of factors affect the overall rating. This work provides an interesting analysis of this important area, and sheds light on several underexplored issues. I have several questions and concerns (e.g., inconsistencies from prior findings, validity of the final experimental), overall it has positive and good scientific contributions to ICLR."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This paper provides an in-depth analysis of feedback acquisition by humans and AI, based on 6,000 human ranking and rating annotations for model responses, as well as model predictions. \n- Their analysis reveals the prevalence of feedback inconsistency issues and also provides an in-depth analysis of why it arises.\n- They also found that which type of feedback data a model is trained on has strong effects on the evaluation."
                },
                "weaknesses": {
                    "value": "I found this paper quite interesting, but the paper reports a set of many different findings, and sometimes their findings are inconsistent with the previous work. Having more detailed discussions on key findings can make this paper stronger (detailed below). I am also not fully convinced by the results of Section 5. Below, I detailed those points. \n\n**Lack of detailed discussions on findings and detailed annotation setup** \n\nParticularily Section 3 and 4 report various interesting phenomena, but some of them lack detailed explanations. For instance, \n\n- Section 3.1 length distributions: despite multiple papers reporting length of responses has a positive correlation with rating, the authors claim there's no difference (\"we find that there is no discernible difference between the average length and average number of unique tokens of the preferred and unpreferred response in the rankings feedback collected from the humans and AI.\"). I wonder if the authors have any insights into this. \n- Section 4.2 Qualitative analysis: the authors say that they sampled a few inconsistent instances and asked annotators to provide explanations for the inconsistencies. I think this is an important and inconsistent analysis and the annotation protocols should be precisely documented. If the claim \"the differences in the preferences of the humans while annotating for different feedback protocols played a significant role in their decision making\" is based on 2-3 instances, the claim may not be fully satisfied. \n\n**The findings of \"Alignment and Evaluation\" section**\n\nI think the findings of Best-of-n policies outperform SFT have been already reported in prior work, and evaluation inconsistency is somewhat predictable given the discussion of inconsistencies. While the second part (inconsistencies) can be novel, I am confused about the descriptions of the results. To my understanding, the finding is if we use a ranking model for Best-of-n ranking we can get higher rates when the same ranking protocol is used during evaluations. For me, it's not really surprising as the reward model is trained on the feedback data and reranks n responses at inference time, so if a model is trained on pairwise feedback data it learns to choose the response preferred in the pairwise setup. It'd be interesting if you could use the feedback data during training (e.g., PPO) and see if the trends remain as well. Yet, I am overall confused with the descriptions in these paragraphs and feel free to correct me if"
                },
                "questions": {
                    "value": "- Could you prvovide the details of human annotation process of Qualitative analysis?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3202/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698827475045,
            "cdate": 1698827475045,
            "tmdate": 1699636268197,
            "mdate": 1699636268197,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "X5Tnycvg73",
                "forum": "dKl6lMwbCy",
                "replyto": "OYRyVQEE9f",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3202/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3202/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer gmKJ"
                    },
                    "comment": {
                        "value": "We are excited to find that the reviewer finds our work (a) interesting in an important area, (b) sheds light on unexplored issues,  (c) and with positive and good scientific contributions to the ICLR community. Here, we address your comments in detail.\n\n**Q:** Section 3.1 length distributions\n\n**A:** \n- We thank the reviewer for their pertinent question. In Section 3.1, we mention that the max length of the Alpaca generated response is 128. This avoids humans to rank very large responses (>128 tokens).  In Table 6 and 7, we show that the human and AI is indeed not biased towards longer responses or towards the number of unique words in our setup. However, the results might differ with the changes in the feedback collection setup. We further identify the distinctions between our setup and the previous ones to further understand such differences. \n- [1] mentioned verbosity bias examined verbosity bias when using LLMs to collect feedback data by constructing a repetitive list attack in which responses with a list were prepended with a paraphrased version of the list (generated by GPT-4) which contains no new information. For example, model A has two pointer responses and model B has the same two pointers + their paraphrased versions (which does not add any new information), the annotators prefer model B response. We believe that this setting largely differs from our setting since we consider two different responses with no further intervention such as a repetition attack. Furthermore, [1] conducts their experiment on just 23 model responses [3] whereas we perform 2K responses. We will add this discussion in our updated paper.\n- Similarly, [2] talks about verbosity bias in preference labeling from humans and AI for creative writing tasks. In our work, we consider a broad range of instructions sourced from 3 datasets as discussed in Table 1. In addition, they conduct their experiments on just 300 responses whereas we perform our analysis with 2K responses. We will also include this discussion in our paper. \n\n[1] LLM-as-judge: https://arxiv.org/pdf/2306.05685.pdf \\\n[2] Verbosity Bias: https://arxiv.org/pdf/2310.10076.pdf \\\n[3] Data: https://github.com/lm-sys/FastChat/blob/main/fastchat/llm_judge/data/mt_bench/reference_answer/gpt-4.jsonl\n\n\n**Q:** Section 4.2: Qualitative Analysis\n\n**A:** \n\nWe thank the reviewer for their insightful question. We agree that it is important to understand the cause of inconsistencies quantitatively and qualitatively. Here, we describe our qualitative analysis for more clarity:\n\n1. In Section 4.2, we show that the feedback data collected from humans and AI is highly inconsistent and analyze their behaviours.\n2. Further, we quantify the closeness with the model responses from the lens of rating and ranking feedback protocol. Overall, this analysis suggested that the perceived closeness between the quality of the responses leads to more inconsistent feedback. \n3. To investigate this further, we conduct another small scale experiment. Specifically, we sample a few consistent and inconsistent responses, and then ask the annotators to provide their rating/ranking feedback along with their natural language explanation. We agree with the reviewer that such experiments should be better documented. As a result, we will include the human annotation interface in the updated paper. We believe that our small scale experimental setup is a good design under a finite budget for a qualitative study. \n4. Post-feedback collection, we study the human responses and natural language explanations for both inconsistent (Table 9 and 10) and consistent responses (Table 11 and 12). \n5. We notice that the natural language explanations from humans do explain their choice well. However, the quality aspects that they focus on changes with the feedback protocol. \n6. Specifically we look at the inconsistent feedback example in Table 9. In the ranking setup, the annotators perceived Response 2 as more clever (\u201cmaking a riddle..\u201d) or correct (\u201cmakes more sense..\u201d) over Response 1. Whereas, in the rating setup, the Response 1 gets higher average score as the annotators focussed on the answer\u2019s faithfulness to the task instruction (\u201cResponse 2 does not meet the task structure requirements\u201d..)\n7. To provide more evidence, we will update the revised paper with more qualitative examples in the Appendix."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3202/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700288069775,
                "cdate": 1700288069775,
                "tmdate": 1700352787894,
                "mdate": 1700352787894,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lm5dSdO191",
                "forum": "dKl6lMwbCy",
                "replyto": "OYRyVQEE9f",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3202/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3202/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal Reminder"
                    },
                    "comment": {
                        "value": "Thanks again for your insightful feedback on our work! We've carefully worked to address your comments/questions and would like to note that the end of the discussion phase is coming soon. Are there any further questions or concerns we should discuss?"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3202/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700504391554,
                "cdate": 1700504391554,
                "tmdate": 1700504391554,
                "mdate": 1700504391554,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vANdbqXVu0",
                "forum": "dKl6lMwbCy",
                "replyto": "OYRyVQEE9f",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3202/Reviewer_gmKJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3202/Reviewer_gmKJ"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your response."
                    },
                    "comment": {
                        "value": "Dear authors,\n\nThank you so much for your response. I appreciate your detailed response, but I think the analyses could have been improved, or the descriptions are still not clear to me.\n\nRe: length bias\n\nRegarding the length issue, now I think the different finding might be coming from a much shorter maximum length---the max length of 128 seems to be really short for instruction-following queries. While I am not super familiar with [1], [2], [3] and their evaluation setups might be different, several papers analyzing the relationship between human/model preferences and length on the setup similar to this work e.g., Singhal et al. (2023) or Wang et al. (2023) set maximum length much higher. In my opinion, if we want to draw some conclusions on the length bias in human/model evaluations, we should set the maximum length longer.\n\nAlso I am confused by the sentece.\n\nThis avoids humans to rank very large responses (>128 tokens).\n\nDo you mean that you set the maximum tokens to be 128 as it's going to be expensive if we ask human to rank long sequences?\n\nRe: qualitative analysis\n\nCould you clarify the exact number you sampled, the overall statistics of qualitatively analyzed data, and representative trends? For instance, as in Xu et al. (2023), you can meta-analyze the human explanations to understand what common cases are.\n\nwe sample a few consistent and inconsistent responses, and then ask the annotators to provide their rating/ranking feedback along with their natural language explanation. We agree with the reviewer that such experiments should be better documented. As a result, we will include the human annotation interface in the updated paper. We believe that our small scale experimental setup is a good design under a finite budget for a qualitative study.\n\nI agree that it is difficult to scale up qualitative analysis, but due to the subjective nature of qualitative analysis, qualitative analysis should be carefully designed and a paper should provide clear details of the experimental designs to make it reproducible. For me, it's hard to draw conclusions without knowing the exact details and stats of this analysis.\n\n\nOverall, I think the initial motivation is interesting and I support the paper, while I felt the analysis could have been improved, and the findings may not be fully applicable to more general settings (e.g., larger LMs, longer sequences, different way of RLHF, such as PPO). Therefore, I keep my score (6)."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3202/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700515640175,
                "cdate": 1700515640175,
                "tmdate": 1700515873191,
                "mdate": 1700515873191,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rCpDYle85U",
            "forum": "dKl6lMwbCy",
            "replyto": "dKl6lMwbCy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3202/Reviewer_7SsY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3202/Reviewer_7SsY"
            ],
            "content": {
                "summary": {
                    "value": "This paper studied the two kinds of protocols used for collecting preference data: ratings and rankings. The authors found an inconsistency problem where in the preferences inferred from ratings and rankings significantly disagree 60% for both human and AI annotators. Their subsequent analysis identifies various facets of annotator biases that explain this phenomena such as human annotators would rate denser responses higher while preferring accuracy during pairwise judgments, for a particular comparison instance. Finally, they also found that the choice of the feedback protocol has a sharp influence on the evaluation of the aligned LLMs in the form of evaluation inconsistency. This highlights a challenge in designing robust evaluation protocols that mirror real-world performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This paper studied an interesting question: the significance of Ratings versus Rankings in collecting and evaluating preference feedback. To me, this represents an essential and timely investigation.\n- The findings in the paper could shed light on this question and stimulate further research.\n- The paper is well structured and written. The experiments are extensive and carefully designed."
                },
                "weaknesses": {
                    "value": "I liked this paper a lot but I do have several comments:  \n\n- In Table 2, the authors showed that when converting the rating to ranking, there is a huge disagreement between the two different protocols. One flaw is that a proper baseline is missing here. For example, if you annotate the same examples using another batch of human or sample from the same model (GPT-3.5-Turbo) using different temperature, what would be the disagreement? I would be curious to see such a baseline as it also correlates with the main conclusion of this paper.\n\n- This is not necessarily a weakness but given what observed in the paper, it would be great to see if the authors could take one step further to verify the root cause of the difference and try potential ways to combine the advantages of the two annotation protocols. To me, Figure 3 shows that the ranking-based protocol is more effective and robust (it yield better win rate with ranking-based evaluation protocol and smaller gap when changing the evaluation protocol). This makes sense, as in the rating protocol, the model/human only see each individual answer thus the rating is likely to be less calibrated. Also, seen from the examples in section H, the ranking protocol is likely to introduce some systematic biases. Here is my proposal: can we try the third protocol that instruct the model/human to give both ratings and rankings for either a pair of responses or a list of responses. My guess is that rating annotated in such a way will be better calibrated and the rankings will less likely to be biased."
                },
                "questions": {
                    "value": "see weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3202/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3202/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3202/Reviewer_7SsY"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3202/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699310069046,
            "cdate": 1699310069046,
            "tmdate": 1701063762751,
            "mdate": 1701063762751,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kgLbt5FvTz",
                "forum": "dKl6lMwbCy",
                "replyto": "rCpDYle85U",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3202/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3202/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 7SsY"
                    },
                    "comment": {
                        "value": "We are excited to find that the reviewer really liked our work and found it (a) essential and timely, (b) well structured and written, and (c) extensive and carefully designed. \n\n**Q:** Annotate the same examples using another batch of human or sample from the same model (GPT-3.5-Turbo) using different temperature, what would be the disagreement? \n\n**A:** We thank the reviewer for their insightful question. Firstly, we provide further clarifications on collecting the feedback data from the humans.\n\n1. During the feedback acquisition, we create two exclusive groups of humans \u2013 one that provides the rating feedback and ranking feedback.\n2. For the rating protocol, four humans rate an individual response for a given instruction. \n3. Similarly, four humans rank a pair of responses for a given instruction under the ranking protocol.\n4. As described in the Agreement Analysis (3.1), we consider three annotations as the predicted human preferences and the fourth annotation as the human gold label. \n5. Table 2(b) indicates consistency analysis of the predicted human preferences i.e., randomly selecting 3 annotators out of 4 annotators. \n6. To provide further analysis as requested by the reviewer, we calculate the consistency evaluation for a different batch of randomly selected 3 annotators four times. \n7. We find that the standard deviation of the inconsistency analysis for four splits of the humans evaluators is **1.6%**. This indicates that the inconsistency analysis is robust to the batch of humans used for annotation. \n\nTo provide more evidence for the same model using a different temperatures \u2013 0, 0.2 and 0.5 for 1000 comparisons. We find that the standard deviation in the disagreements between these temperatures is **2.7%**.  This indicates that the inconsistency analysis is also robust to the different model temperatures used for generating feedback data. We will add these statistics in our updated paper.\n\n**Q: Root cause of the difference**\n\n**A:**\n\nWe agree that the root cause of the difference between the feedback protocols is quite important. Towards this, we highlight our quantitative and qualitative findings in the Section 4.2:\n\n1. We show that the feedback data collected from humans and AI is highly inconsistent and analyze their behaviours.\n2. Further, we  quantify the closeness with the model responses from the lens of rating and ranking feedback protocol. Overall, this analysis suggested that the perceived closeness between the quality of the responses leads to more inconsistent feedback. \n3. To investigate this further, we conduct another small scale experiment. Specifically, we sample a few consistent and inconsistent responses, and then ask the annotators to provide their rating/ranking feedback along with their natural language explanation. We agree with the reviewer that such experiments should be better documented. As a result, we will include the human annotation interface in the updated paper. We believe that our small scale experimental setup is a good design under a finite budget for a qualitative study. \n4. Post-feedback collection, we study the human responses and natural language explanations for both inconsistent (Table 9 and 10) and consistent responses (Table 11 and 12). \n5. We notice that the natural language explanations from humans do explain their choice well. However, the quality aspects that they focus on changes with the feedback protocol. \n6. Specifically we look at the inconsistent feedback example in Table 9. In the ranking setup, the annotators perceived Response 2 as more clever (\u201cmaking a riddle..\u201d) or correct (\u201cmakes more sense..\u201d) over Response 1. Whereas, in the rating setup, the Response 1 gets higher average score as the annotators focussed on the answer\u2019s faithfulness to the task instruction (\u201cResponse 2 does not meet the task structure requirements\u201d..)\n7. To provide more evidence, we will update the revised paper with more qualitative examples in the Appendix."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3202/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700369129293,
                "cdate": 1700369129293,
                "tmdate": 1700369129293,
                "mdate": 1700369129293,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gNbyJpbodC",
                "forum": "dKl6lMwbCy",
                "replyto": "rCpDYle85U",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3202/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3202/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal Reminder"
                    },
                    "comment": {
                        "value": "Thanks again for your insightful feedback on our work! We've carefully worked to address your comments/questions and would like to note that the end of the discussion phase is coming soon. Are there any further questions or concerns we should discuss?"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3202/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700504351340,
                "cdate": 1700504351340,
                "tmdate": 1700504351340,
                "mdate": 1700504351340,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "I953mpBzUx",
                "forum": "dKl6lMwbCy",
                "replyto": "rCpDYle85U",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3202/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3202/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Update on the revised paper"
                    },
                    "comment": {
                        "value": "Hi,\n\nWe wanted to highlight that we have uploaded the revised paper with the new results and setups. Addressing your suggestions, we have added Section E (variation with annotators), Section I (AI prompt), Section J (human UI screenshot), and Section F (pairwise rating), Section P (more qualitative examples). \n\nPlease do let us know if there are any further questions. If all your questions have been resolved, we would be grateful if you would consider raising your score."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3202/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700532725413,
                "cdate": 1700532725413,
                "tmdate": 1700533090669,
                "mdate": 1700533090669,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]