[
    {
        "title": "Counting Graph Substructures with Graph Neural Networks"
    },
    {
        "review": {
            "id": "PcTIOT9Jl8",
            "forum": "qaJxPhkYtD",
            "replyto": "qaJxPhkYtD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8631/Reviewer_QJcy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8631/Reviewer_QJcy"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on the representation power of GNNs and their ability to count graph substructures. The analysis enables the design of a generic GNN architecture that achieves good performance in various tasks, including subgraph detection, subgraph counting, graph classification, and logP prediction. The paper also provides both theoretical and experimental evidence of the ability of GNN to count graph substructures in graph-level representation learning tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper studies an important problem: the ability to recognize subgraphs is crucial in graph applications.\n\n2. The authors provide detailed deductions of how to design the proposed GNN to identify different numbers of graph substructures."
                },
                "weaknesses": {
                    "value": "1. Writing quality: The presentation and the layout of the paper need to be improved substantially. Equation spacing/referencing is not correct with editing hints unremoved. \nThe paper is hard to follow. A clear introduction/navigation at the beginning of each section is lacking. This is particularly the case in the methodology part, which confuses the reader.\n \n2. Experiments: The experiments show that the proposed GNN has good capability of substructure counting. However, this may not lead to better performance on downstream tasks. There are many benchmark datasets commonly used for node-level and graph-level tasks. Testing on more datasets will better demonstrate how counting substructures can help improve the performance of other tasks.\n \n3. For the regression task on ZINC, the method proposed in \u2018Graph Neural Networks with Local Graph Parameters\u2019 accepted by NeurIPS 2021 seems to have a better result, which is not included in this paper."
                },
                "questions": {
                    "value": "See the weaknesses above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8631/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8631/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8631/Reviewer_QJcy"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8631/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698663477867,
            "cdate": 1698663477867,
            "tmdate": 1700659607357,
            "mdate": 1700659607357,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "j2yMSBJBwg",
                "forum": "qaJxPhkYtD",
                "replyto": "PcTIOT9Jl8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8631/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8631/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer's comments"
                    },
                    "comment": {
                        "value": "> Writing quality: The presentation and the layout of the paper need to be improved substantially. Equation spacing/referencing is not correct with editing hints unremoved. The paper is hard to follow. A clear introduction/navigation at the beginning of each section is lacking. This is particularly the case in the methodology part, which confuses the reader.\n\nWe would like to thank the reviewer for their suggestion. We have substantially improved the paper presentation in the revised manuscript. We would be happy to implement further changes that the reviewer may suggest after reviewing our revised manuscript.\n\n> Experiments: The experiments show that the proposed GNN has good capability of substructure counting. However, this may not lead to better performance on downstream tasks. The model was tested on two datasets only, which is not sufficient. There are many benchmark datasets commonly used for node-level and graph-level tasks. Testing on more datasets will better demonstrate how counting substructures can help improve the performance of other tasks.\n\nWe would like to thank the reviewer for their comment. We want to point out that we are testing our approach in 5 different datasets. Two synthetically generated datasets, to test the main scope of the paper (counting substructures), and 3 real benchmark datasets (ZINC, REDDIT-B, REDDIT-M) to assess the performance and generality of our approach in downstream tasks. We are also adding experiments with the full ZINC dataset in the revised version and would be happy to include additional datasets that the reviewer suggests. However, we believe that the current set of experiments is sufficient to show the effectiveness and generality of our approach. Note that the contribution of our work is not only practical but has a strong analytical component which we prefer to maintain. \n\n> The proposed model relies on the outcomes from other subgraph recognition studies as the ground truth for training. This limits its applicability as its performance may be affected by these upstream models.\n\nWe would like to respectfully correct the reviewer in this statement. Our proposed model is completely independent of other subgraph recognition studies. We do not use any subgraph information from other models during training, validation, or testing.\n\n> For the regression task on ZINC, the method proposed in \u2018Graph Neural Networks with Local Graph Parameters\u2019 accepted by NeurIPS 2021 seems to have a better result, which is not included in this paper.\n\nThank you for bringing this work to our attention, which we cite and include in the revised version of the paper. However, it is not true that this method achieves better performance. In fact, the best performance for the regression task on ZINC, which is reported in \u2018Graph Neural Networks with Local Graph Parameters\u2019 is MAE 0.1353 \u00b1 0.01, whereas our proposed approach achieves MAE 0.110 \u00b1 0.005."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8631/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700387619850,
                "cdate": 1700387619850,
                "tmdate": 1700387619850,
                "mdate": 1700387619850,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "isbvl6KXuN",
                "forum": "qaJxPhkYtD",
                "replyto": "PcTIOT9Jl8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8631/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8631/Authors"
                ],
                "content": {
                    "title": {
                        "value": "We would like to hear your thoughts on our response and revised manuscript"
                    },
                    "comment": {
                        "value": "Dear reviewer,\n\nWe very much appreciate your feedback and suggestions. Your input was important in improving our paper. We believe that the writing of our manuscript has been significantly improved. We have also added one more dataset and comparisons with \u2018Graph Neural Networks with Local Graph Parameters\u2019, as requested. We also clarified that our approach does not depend on other subgraph methods. \n\nWe would really appreciate it if you could find time to share your thoughts on our response and the revised manuscript."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8631/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700623258832,
                "cdate": 1700623258832,
                "tmdate": 1700624695864,
                "mdate": 1700624695864,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "J7toPXVFA4",
                "forum": "qaJxPhkYtD",
                "replyto": "j2yMSBJBwg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8631/Reviewer_QJcy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8631/Reviewer_QJcy"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the response"
                    },
                    "comment": {
                        "value": "The rebuttal has addressed my concerns. I appreciate the authors' effort in including the full ZINC dataset in evaluation and improving the writing clarity in the revision. I have raised my rating."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8631/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700659922280,
                "cdate": 1700659922280,
                "tmdate": 1700659922280,
                "mdate": 1700659922280,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WcaJ9pFjdZ",
            "forum": "qaJxPhkYtD",
            "replyto": "qaJxPhkYtD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8631/Reviewer_K7V4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8631/Reviewer_K7V4"
            ],
            "content": {
                "summary": {
                    "value": "This work studies the substructure counting power of a message-passing neural networks (MPNN) with random input. Concretely, the function class of interest is the expectation of outputs of standard MPNNs with random node features. They prove that this model can count up to 7 cycles, which is more powerful than regular MPNN with constant input. Moreover, the model is shown be equivalent to a deterministic graph filtering with constant input, where the filter is a hadamard product of polynomial filters. The equivalent filtering model has a better empirical performance over tasks such as substructure counting and molecular property prediction."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Though it is known that random node feature injection can improve expressive power, it is interesting to see such analysis in an average meaning (thus perserving equivariance), and the connection to the corresponding graph filtering."
                },
                "weaknesses": {
                    "value": "- The comparison to other baselines on cycle counting and ZINC is insufficient.\n- I feel like the sentence in the abstract \"However, their ability to count substructures, which play a crucial role in biological and social networks, remains uncertain\" may be confusing in the sense that, the counting ability of regular MPNN is pretty clear as shown in [1].\n\n\n[1] Zhengdao Chen, Lei Chen, Soledad Villar, and Joan Bruna. Can graph neural networks count substructures? Advances in neural information processing systems, 33:10383\u201310395, 2020."
                },
                "questions": {
                    "value": "- I was wondering if large cycles (such as 8 cycles) can also be written as hadamard product of $S^k$ and thus can be expressed by the model? I know the construction of such function is pretty combinatorial, so probably the question is open. But it is interesting as 7 cycle is also the counting limitation of 2-FWL."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8631/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8631/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8631/Reviewer_K7V4"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8631/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698713278663,
            "cdate": 1698713278663,
            "tmdate": 1699637080148,
            "mdate": 1699637080148,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jm0n3bDvSS",
                "forum": "qaJxPhkYtD",
                "replyto": "WcaJ9pFjdZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8631/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8631/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer's comments"
                    },
                    "comment": {
                        "value": "> The comparison to other baselines on cycle counting and ZINC is insufficient.\n\nNote that our approach is generic and we try to keep the comparisons between more general approaches for fairness. Nevertheless, we have compared the performance of our approach against 25 different baselines in our experiments. We are also including 5 more baselines from \u2018Graph Neural Networks with Local Graph Parameters\u2019 on ZINC in the revised manuscript. However, we would be happy to include any further comparisons/discussions that the reviewer may suggest.\n\n\n> I feel like the sentence in the abstract \"However, their ability to count substructures, which play a crucial role in biological and social networks, remains uncertain\" may be confusing in the sense that, the counting ability of regular MPNN is pretty clear as shown in [1].\n\nWe agree with the reviewer that [1] has provided significant insights into the counting ability of message-passing GNNs. [1] studies the counting capabilities of message-passing GNNs with either constant or symmetric (structural) input signals. In that case, the counting capacity of message-passing GNNs is upper bounded by the 1-FWL test. However, our approach shows that random input signals can better exploit the power of message-passing operations and improve their counting capacity beyond 1-FWL. As a result, the ability of message-passing GNNs to count substructures is indeed very clear with symmetric input signals, thanks to [1], however when the input is not symmetric this research question is still open. \n\nTo avoid possible confusion we rephrase this part to 'However, there are still open questions regarding their ability to count and list substructures, which play a crucial role in biological and social networks.\"\n\n\n> I was wondering if large cycles (such as 8 cycles) can also be written as hadamard product of $\\mathbf{S}^k$ and thus can be expressed by the model? I know the construction of such function is pretty combinatorial, so probably the question is open. But it is interesting as 7 cycle is also the counting limitation of 2-FWL.\n\nIn our new results, we can prove that 8 cycles can be written using Hadamard products of $\\mathbf{S}^k$ and Khatri-Rao products of $\\mathbf{S}^k$. These operations can actually be computed by message-passing GNNs with random inputs as discussed in Section 6. As a result, message-passing GNNs with random inputs can produce equivariant representations that are more powerful than the 2-FWL test for certain graph classes. Regarding the model in (14) (in the revised manuscript), we have not proven yet whether it is bounded by 2-FWL test or not. Please see the general response and Section 6 in the revised manuscript for more details."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8631/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700386198192,
                "cdate": 1700386198192,
                "tmdate": 1700428556998,
                "mdate": 1700428556998,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KRTIcftscu",
                "forum": "qaJxPhkYtD",
                "replyto": "jm0n3bDvSS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8631/Reviewer_K7V4"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8631/Reviewer_K7V4"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response"
                    },
                    "comment": {
                        "value": "I appreciate authors' efforts of providing more counting ability analysis and experiments during rebuttal. I keep my original rate."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8631/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700443905955,
                "cdate": 1700443905955,
                "tmdate": 1700443905955,
                "mdate": 1700443905955,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DNpenbwHOq",
            "forum": "qaJxPhkYtD",
            "replyto": "qaJxPhkYtD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8631/Reviewer_4Tmr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8631/Reviewer_4Tmr"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the representation power of GNNs in terms of counting graph substructures, including cycles, quasi-cliques, and connected components. The authors employ tools from tensor algebra and stochastic processes, and they demonstrate that with appropriate activation and normalization functions, GNNs can generate permutation equivariant node representations that capture statistical moments of the GNN output distribution."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The exploration of the graph substructure counting ability in the context of GNNs constitutes a novel contribution to this field. This particular property is closely tied to the representational power of GNNs and is critical for numerous real world graph modalities like social networks, making it a pertinent and valuable topic for discussion.\n\n2. The theoretical analysis presented in the paper is extensive and commendable, shedding light on various aspects of the subject matter.\n\n3. The empirical experiments conducted in the paper aligns well with theoretical findings and demonstrates the effectiveness of proposed method."
                },
                "weaknesses": {
                    "value": "1. Many powerful, expressive, and well-adopted  GNN baselines are missed. For instance, GNN-AK[1], GrphGPS[2], CIN [3], ....\n\n2. The technique of encoding substructures in graphs is widely used in GNNs to improve the expressiveness, such as EGO [1], GNN-AK [1] and NGNN [4].  I think the comparison with these substructure-based GNNs should be discussed.\n\n[1] Lingxiao Zhao, Wei Jin, Leman Akoglu, and Neil Shah. From stars to subgraphs: Uplifting any GNN with local structure awareness. In International Conference on Learning Representations,2022.\n\n[2] Ramp\u00e1\u0161ek, L., Galkin, M., Dwivedi, V. P., Luu, A. T., Wolf, G., & Beaini, D. (2022). Recipe for a general, powerful, scalable graph transformer. Advances in Neural Information Processing Systems, 35, 14501-14515.\n\n[3] Bodnar, C., Frasca, F., Otter, N., Wang, Y., Lio, P., Montufar, G. F., & Bronstein, M. (2021). Weisfeiler and lehman go cellular: Cw networks. Advances in Neural Information Processing Systems, 34, 2625-2640.\n\n[4] Zhang, M., & Li, P. (2021). Nested graph neural networks. Advances in Neural Information Processing Systems, 34, 15734-15747.\n\n[5]  Beatrice Bevilacqua, Fabrizio Frasca, Derek Lim, Balasubramaniam Srinivasan, Chen Cai, Gopinath Balamurugan, Michael M Bronstein, and Haggai Maron. Equivariant subgraph aggregation networks. In International Conference on Learning Representations, 2022."
                },
                "questions": {
                    "value": "1. When performing graph regression task on dataset ZINC, does the 10000 training graphs, 1000 validation graphs and 1000 test graphs are randomly sampled or they are from the ZINC-100K? For a more fair comparison, can this experiment implemented on whole ZINC dataset?\n\n2. Proposition 5.2 show that Moment-GNN can improve expressivity over 1-FWL. What about the upper-bound on its' expressive power?  Is it bounded by 3-WL like subgraph GNNs [1]?\n\n[1] Frasca, F., Bevilacqua, B., Bronstein, M., & Maron, H. (2022). Understanding and extending subgraph gnns by rethinking their symmetries. Advances in Neural Information Processing Systems, 35, 31376-31390."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8631/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698789934385,
            "cdate": 1698789934385,
            "tmdate": 1699637080003,
            "mdate": 1699637080003,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "14HvzxELuF",
                "forum": "qaJxPhkYtD",
                "replyto": "DNpenbwHOq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8631/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8631/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer's comments"
                    },
                    "comment": {
                        "value": "> Many powerful, expressive, and well-adopted GNN baselines are missed. For instance, GNN-AK[1], GrphGPS[2], CIN [3], ....\n\nWe would like to thank the reviewer for bringing these works to our attention. We have cited and added discussions on your suggested papers in the revised manuscript.\n\n> The technique of encoding substructures in graphs is widely used in GNNs to improve the expressiveness, such as EGO [1], GNN-AK [1] and NGNN [4]. I think the comparison with these substructure-based GNNs should be discussed.\n\nWe agree with the reviewer that comparison with these substructure methods is important and is discussed in the revised manusript. The basic difference between our approach and [1], [4] is that we learn to process substructures without any prior information or additional algorithmic help, whereas [1], [4] predefine the subgraphs of interest and resort to non-message passing mechanisms to identify them. After identifying these subgraphs they perform subgraph message-passing to learn local representations that are used to extract global graph embeddings. Both approaches have pros and cons. For instance, our proposed method is more generic as it is purely data-driven. However, it does not utilize the help of other algorithms or prior knowledge as [1], [4] which can be valuable in many tasks.\n\n> When performing graph regression task on dataset ZINC, does the 10000 training graphs, 1000 validation graphs and 1000 test graphs are randomly sampled or they are from the ZINC-100K? For a more fair comparison, can this experiment implemented on whole ZINC dataset?\n\nThey are not randomly sampled. The dataset and the training/testing/validation splits are downloaded from the Benchmarking Graph Neural Networks repository (https://github.com/graphdeeplearning/benchmarking-gnns). Thank you for your suggestion. We are currently testing our method on the whole ZINC dataset, which is also downloaded from the same repository. Without fine tuning our current test MAE is $0.040\\pm 0.002$ among 6 different runs.\n\n> Proposition 5.2 show that Moment-GNN can improve expressivity over 1-FWL. What about the upper-bound on its' expressive power? Is it bounded by 3-WL like subgraph GNNs [1]?\n\nWe would like to thank the reviewer for motivating us to look deeper into the expressivity of our approach. In our new results, we can prove that message-passing GNNs with random inputs can count 8-node cycles and 4-node cliques. These operations however do not admit a closed-form expression with purely Hadamard product operations but they also involve diagonals of the tensor Tucker model. As a result, the expressivity of message-passing GNNs with random inputs and the expectation operator in a hidden layer is not bounded by the 3-WL test. However, it could be the case that Moment-GNN (as defined in (14) in the revised manuscript) is indeed bounded by the 3-WL test, but we haven't proven it yet. Please see the general response and Section 6 in the revised manuscript for more details."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8631/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700385933633,
                "cdate": 1700385933633,
                "tmdate": 1700428581740,
                "mdate": 1700428581740,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EUZDrwtYKy",
                "forum": "qaJxPhkYtD",
                "replyto": "DNpenbwHOq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8631/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8631/Authors"
                ],
                "content": {
                    "title": {
                        "value": "We would like to hear your thoughts on our response and revised manuscript"
                    },
                    "comment": {
                        "value": "Dear reviewer,\n\nWe would like to express our gratitude for your appreciation of our work. Your comments have been instrumental in deriving new results and enhancing our paper. We believe we have addressed all of your comments and would be delighted to hear your thoughts on our response and revised manuscript."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8631/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700622668340,
                "cdate": 1700622668340,
                "tmdate": 1700622668340,
                "mdate": 1700622668340,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Y0DN6Gubuw",
                "forum": "qaJxPhkYtD",
                "replyto": "EUZDrwtYKy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8631/Reviewer_4Tmr"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8631/Reviewer_4Tmr"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for these responses"
                    },
                    "comment": {
                        "value": "Thank you for your comprehensive response, and I have no further questions."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8631/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700679533310,
                "cdate": 1700679533310,
                "tmdate": 1700679533310,
                "mdate": 1700679533310,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TjlabJmnYC",
                "forum": "qaJxPhkYtD",
                "replyto": "DNpenbwHOq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8631/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8631/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review"
                    },
                    "comment": {
                        "value": "Thank you for appreciating our work and providing valuable comments. We are glad to answer all of your concerns."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8631/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700683106777,
                "cdate": 1700683106777,
                "tmdate": 1700683331465,
                "mdate": 1700683331465,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5Bi1MUxjOG",
            "forum": "qaJxPhkYtD",
            "replyto": "qaJxPhkYtD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8631/Reviewer_jgwt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8631/Reviewer_jgwt"
            ],
            "content": {
                "summary": {
                    "value": "This paper studied the problem of detecting/counting graph structures such as cycles, cliques, and connected components, using MPNNs with random initial node features. They considered taking the high-order moment of the neural network output to obtain deterministic node representations. In this case, the authors proved that with the increase of the order, the resulting GNN can express more and more graph structures, resulting in higher expressivity. Experiments demonstrate the expressive power of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The theoretical results of this paper are rigorous and correct (although proving it is straightforward given prior work).\n2. The proposed method is quite interesting. Prior to this work, researchers mainly improved the expressive power of GNNs by sacrificing the computational costs, e.g., using higher-order GNNs. While in this paper, the proposed GNNs only have linear complexity. On the other hand, using random node initialization has been proposed in prior work. However, unlike prior work, this paper achieves equivariance by using an expectation in the final layer (although there may be other weaknesses, see below). The authors showed promising theoretical results for this architectural design."
                },
                "weaknesses": {
                    "value": "1. This paper is poorly written. Please carefully polish the paper in the rebuttal period. Several problems include: (1) the word \"equation\" is redundant in many places; (2) many definitions are unclear. For example, what do you mean by \"x is anonymous\" in page 4, and what do you mean by \"stationary random vector\"? (3) the paper even exposes the author name \"Charilaos\" in page 6. Please fix it. (4) What is role of the characteristic function in page 3? Why is it unrelated to x?\n\n2. Regarding the theoretical results:\n   - The authors only proved positive results for the expressive power of GNNs using high-order moments, which are incomplete. Does the GNNs fail without high-order moments? In other words, are the theoretical results tight? For example, can the GNNs count 6-cycle using only 2-order moment, and can the GNNs count 7-cycle using only 3-order moment?\n   - I do not think Theorem 4.1 is meaningful. The theorem uses a GNN to fit the number of connected components of a *single* graph, which is just equivalent to fit a constant if I understand correctly.\n\n3. Regarding the experiments:\n   - The proposed method relies crucially on taking the expectation in the final layer. How do you implement this in your experiments? Will a large number of samples be needed?\n   - I found from the results that the GNNs can even count 8-cycles perfectly but the current theory did not prove it. What is the number of moment order required in your experiments?\n\n   Please give more details in your paper."
                },
                "questions": {
                    "value": "See the box above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8631/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8631/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8631/Reviewer_jgwt"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8631/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699525554951,
            "cdate": 1699525554951,
            "tmdate": 1700660458141,
            "mdate": 1700660458141,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SA57SuhBLu",
                "forum": "qaJxPhkYtD",
                "replyto": "5Bi1MUxjOG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8631/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8631/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer's comments (1/2)"
                    },
                    "comment": {
                        "value": "> The theoretical results of this paper are rigorous and correct (although proving it is straightforward given prior work).\n\nWe want to thank the reviewer for appreciating our work. It is worth mentioning that our proofs are quite complex, involving the analysis of multiple layers of GNNs with nonlinearities in between (see Section 5 and Appendix F). To the best of our knowledge, this is one of the few attempts that analyze multiple GNN layers. \n\n> This paper is poorly written. Please carefully polish the paper in the rebuttal period. Several problems include: (1) the word \"equation\" is redundant in many places;\n\nThank you for your suggestion. We have significantly improved the paper writing in the revised version. Unfortunately the word \"equation\" was generated automatically from the ICLR template whenever the command \\eqref was used, which explains the repetition. We have addressed this issue in the revised version.\n\n> many definitions are unclear. For example, what do you mean by \"x is anonymous\" in page 4, and what do you mean by \"stationary random vector\"?\n\nBy \"anonymous x,\" we refer to node features that are completely uninformative, i.e., they possess no information about the nodes or the graph structure. In other words, they are agnostic to the graph and the node identities. In the revised manuscript, we changed \"anonymous\" to \"uninformative.\" The term \"stationary random vector\" is well-defined in probability theory and refers to the characteristics of the distribution of the input. We have removed this term in the revised manuscript to avoid confusion.\n\nPlease see our answer to the next comment for further explanations.\n\n> What is role of the characteristic function in page 3? Why is it unrelated to x?\n\nThe short answer is that input x is a random vector which can be fully described by its characteristic function, the same way a random vector can be fully described by its density function (e.g., Gaussian function). The characteristic function can also be viewed as the Fourier Transform of the density function. Since x can be completely described by its characteristic function it is also completely related to x.\n\nWe understand that there is a confusion regarding input x and its properties and therefore have updated the first part of Section 3, which now reads:\n\nThe goal of our paper is to answer the research question in 1.1 and characterize the expressive power of message-passing GNNs in terms of counting important substructures of the graph. To do that we study  Equation 2, when the input is a random vector $\\mathbf{x}\\in\\mathbb{R}^N$. Our work focuses on the ability of message-passing GNN operations to generate expressive features, therefore the input $\\mathbf{x}$ should be completely uninformative, i.e., structure and identity agnostic. To ensure that $\\mathbf{x}=\\left[x_1,x_2,\\dots,x_N\\right]^T$ is agnostic with respect to the structure of the graph, we assume that $x_i$, $i=1,\\dots,N$ are independent random variables. To ensure that $x_i$s are identity agnostic, we assume that they are identically distributed and satisfy $\\mathbb{E}\\left[ x_i\\right]= 0$ $\\mathbb{E}\\left[ x_i^p\\right]= 1$, for $i=1,\\dots,N$, and $p\\geq 2\\in\\mathbb{Z}$. Overall, the elements of $\\mathbf{x}$ are independent and identically distributed (i.i.d.), with joint characteristic function $\\phi_{\\mathbf{x}}\\left(t_1,\\dots,t_N\\right)=\\prod_{i=1}^N \\left(e^{jt_i}-jt_i\\right)$, and the moments of the joint distribution of $\\mathbf{x}$ satisfy: \n\n$\\mathbb{E}\\left[\\mathbf{x}\\right]=\\mathbf{0},\\quad\n    \\mathbb{E}\\left[\\mathbf{x}\\mathbf{x}^T\\right]=\\mathbf{I},\\quad\n   \\mathbb{E}\\left[\\mathbf{x}\\circ\\mathbf{x}\\circ\\mathbf{x}\\right]=\\underline{\\mathbf{I}},\\quad\n    \\mathbb{E}\\left[\\mathbf{x}\\circ\\mathbf{x}\\circ\\dots\\circ\\mathbf{x}\\right]=\\underline{\\mathbf{I}}$"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8631/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700384581351,
                "cdate": 1700384581351,
                "tmdate": 1700385967946,
                "mdate": 1700385967946,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Pp07nmecI2",
                "forum": "qaJxPhkYtD",
                "replyto": "5Bi1MUxjOG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8631/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8631/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer's comments (2/2)"
                    },
                    "comment": {
                        "value": "> The authors only proved positive results for the expressive power of GNNs using high-order moments, which are incomplete. Does the GNNs fail without high-order moments? In other words, are the theoretical results tight? For example, can the GNNs count 6-cycle using only 2-order moment, and can the GNNs count 7-cycle using only 3-order moment?\n\nThe point the reviewer is raising is important and is worth further clarification. Our results are sufficient but not necessary. For example, we cannot guarantee that 7-node cycles cannot be computed using the 3rd-order moments. However, it is important to note that the question we are addressing in this paper is not whether high-order moments of certain graph filters can count substructures, but whether message-passing GNNs can count substructures. High-order moments are not the goal but rather the tool we are using to analyze the counting abilities of GNNs.\n\nFor instance, a 2-layer GNN with square nonlinearities implicitly or explicitly computes 4th-order moments, and a 3-layer GNN with square nonlinearities computes 6th-order moments. To make things more interesting, GNNs with analytic pointwise nonlinearities, such as the hyperbolic tangent, can linearly combine infinitely high-order information in their output with just a single layer. As a result, the question the reviewer is posing is indeed very interesting, but more from a graph theory perspective, rather than GNN analysis.\n\n> I do not think Theorem 4.1 is meaningful. The theorem uses a GNN to fit the number of connected components of a single graph, which is just equivalent to fit a constant if I understand correctly.\n\nWe respectfully disagree with the reviewer's comment. Theorem 4.1 demonstrates that message-passing GNN operations are able to generate powerful node embeddings that, when summed across nodes, enable the computation of the number of connected components in a graph. Considering that message-passing GNNs with constant or structural inputs are provably unable to generate this information, we find the significance of Theorem 4.1 noteworthy.\n\nHowever, we agree with the reviewer that our other theorems prove more powerful results. In particular, we establish that message-passing GNNs can learn how to count cycles and quasi-cliques for any graph. As a result, our other theorems not only characterize the expressive power of GNNs but also their generalization ability.\n\n\nIn summary, Theorem 4.1 is crucial in describing the expressive power of GNNs and is applicable to graphs observed during training. On the other hand, all the other theorems characterize both the expressive and generalization power of GNNs and are also applicable to graphs that were not observed during training. Parts of this discussion appear in the revised manuscript.\n\n> The proposed method relies crucially on taking the expectation in the final layer. How do you implement this in your experiments? Will a large number of samples be needed?\n\nIn fact the proposed approach **does not** necessarily rely on taking the expectation in the final layer. One can either implement the architecture with the expectation or use the equivalent closed-form expression of Equation 14. In our implementation, we employ the equivalent closed-form expression. Since we are using a single Moment GNN layer followed by GIN layers, the exact expressions are described in Equation 65 of the Appendix. Detailed discussion about this topic can be found in Proposition 5.1, Remark 5.1, and Section 7.1 of the revised manuscript. \n\n> I found from the results that the GNNs can even count 8-cycles perfectly but the current theory did not prove it. What is the number of moment order required in your experiments?\n\nPlease see the general comment.\n\nOur new results, which are presented in Section 6 of the revised manuscript, prove that message-passing operations can actually count 8-node cycles. These results however do not directly apply to the implementation we used in our experiments. In the experiments, the moment order is a hyperparameter which is selected between 3-5."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8631/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700384954089,
                "cdate": 1700384954089,
                "tmdate": 1700384954089,
                "mdate": 1700384954089,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "J4K0RbzbQB",
                "forum": "qaJxPhkYtD",
                "replyto": "5Bi1MUxjOG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8631/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8631/Authors"
                ],
                "content": {
                    "title": {
                        "value": "We would like to hear your thoughts on our response and revised manuscript"
                    },
                    "comment": {
                        "value": "Dear reviewer,\n\nWe would like to thank you for your comments and constructive feedback. Your review helped our paper to improve significantly.\n\nThe writing of the revised manuscript has been significantly improved and the definitions are now super clear. We also believe that we have addressed your concerns regarding our theoretical results. Regarding experiments, we also clarified how our model is implemented in practice without the need to compute expectations.\n\nAs a result, we would be very glad if you could share your thoughts on our response and revised manuscript. We would be happy to answer any further questions you may have."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8631/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700623882377,
                "cdate": 1700623882377,
                "tmdate": 1700623882377,
                "mdate": 1700623882377,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Bz7OcWFjiP",
                "forum": "qaJxPhkYtD",
                "replyto": "Pp07nmecI2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8631/Reviewer_jgwt"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8631/Reviewer_jgwt"
                ],
                "content": {
                    "title": {
                        "value": "Thank you"
                    },
                    "comment": {
                        "value": "Thank you for your thorough reply. Several of my questions have been addressed. However, I am still not convinced by the significance of Theorem 4.1. Since there is only one graph, fitting the connected component should be trivial. Can you explain why you said that \"Considering that message-passing GNNs with constant or structural inputs are provably unable to generate this information\"?\n\nAnother weakness is in the implementation. You said that you use the equivalent closed-form expression of Equation 14 instead of taking expectation. However, this is quite computationally expensive (unlike the standard MPNN). It requires at least $\\Omega(n^3)$ computational costs. It is not clear what is the advantage of using your GNN design in practice.\n\nFinally, it seems that you have added many new results in the updated paper. I should acknowledge that **I may not have enough time to carefully check all the new results again**, but after a coarse check it seems that the writing quality has been improved."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8631/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700638977407,
                "cdate": 1700638977407,
                "tmdate": 1700638977407,
                "mdate": 1700638977407,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Pps67sSDy2",
                "forum": "qaJxPhkYtD",
                "replyto": "5Bi1MUxjOG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8631/Reviewer_jgwt"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8631/Reviewer_jgwt"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the prompt reply."
                    },
                    "comment": {
                        "value": "I believe both of my concerns on Theorem 4.1 and practical implementation are addressed. I tend towards raising my score. As a minor comment, please link each Appendix section to some place in the main text."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8631/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700660392685,
                "cdate": 1700660392685,
                "tmdate": 1700660500998,
                "mdate": 1700660500998,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]